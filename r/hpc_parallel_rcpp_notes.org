
https://stackoverflow.com/questions/6807068/why-is-my-recursive-function-so-slow-in-r
http://dirk.eddelbuettel.com/code/rcpp/html/Misc_2fibonacci_8r-example.html
https://www.r-bloggers.com/2018/10/optimize-your-r-code-using-memoization/
https://bookdown.org/content/d1e53ac9-28ce-472f-bc2c-f499f18264a3/releasememory.html

https://topepo.github.io/caret/parallel-processing.html
https://bookdown.org/rdpeng/RProgDA/profiling-and-benchmarking.html
https://towardsdatascience.com/random-forest-in-r-f66adf80ec9
https://www.guru99.com/r-random-forest-tutorial.html
https://bookdown.org/rdpeng/rprogdatascience/profiling-r-code.html
https://adv-r.hadley.nz/perf-measure.html
https://www.r-bloggers.com/2018/10/optimize-your-r-code-using-memoization/
https://wgpages.netlify.app/python202302/python-13-pandas/
https://bookdown.org/content/d1e53ac9-28ce-472f-bc2c-f499f18264a3/releasememory.html#use-rm
https://stackoverflow.com/questions/11357602/r-caching-memoise-for-environments
https://www.r-bloggers.com/2021/07/caching-the-results-of-functions-of-your-r-package/
https://www.r-bloggers.com/2018/10/optimize-your-r-code-using-memoization/
https://arxiv.org/abs/1409.5827



https://towardsdatascience.com/parallelization-caveats-in-r-1-the-basics-multiprocessing-and-multithreading-performance-eb584b7e850e

https://lorentzen.ch/index.php/2022/02/19/avoid-loops-in-r-really/

https://topepo.github.io/caret/parallel-processing.html



https://www.futureverse.org/packages-overview.html
https://www.r-bloggers.com/2020/05/performance-optimization-in-r-parallel-computing-and-rcpp/

https://quantifyinghealth.com/fibonacci-function-in-r/?swcfpc=1
https://www.r-bloggers.com/2020/05/performance-optimization-in-r-parallel-computing-and-rcpp/

https://quantifyinghealth.com/fibonacci-function-in-r/?swcfpc=1
https://callr.r-lib.org/
https://cran.r-project.org/web/packages/future.apply/vignettes/future.apply-1-overview.html
https://future.callr.futureverse.org/

important to look at: https://future.batchtools.futureverse.org/

https://quantifyinghealth.com/fibonacci-function-in-r/?swcfpc=1


https://github.com/sparklyr/sparklyr#distributed-r

https://cran.r-project.org/web/packages/future/vignettes/future-1-overview.html

https://privefl.github.io/blog/a-guide-to-parallelism-in-r/

https://henrikbengtsson.github.io/future-tutorial-user2022/index.html

https://bookdown.org/rdpeng/rprogdatascience/parallel-computation.html
https://medium.com/civis-analytics/programming-with-futures-in-r-401e906db384#:~:text=In%20plan(%E2%80%9Cmultisession%E2%80%9D),from%20the%20current%20R%20process

https://www.futureverse.org/

https://journal.r-project.org/archive/2021/RJ-2021-048/index.html

https://www.jottr.org/2023/01/10/progressr-0.13.0/



https://github.com/HenrikBengtsson/future
https://future.futureverse.org/
https://github.com/HenrikBengtsson/doFuture/issues/39

https://tune.tidymodels.org/articles/extras/optimizations.html

https://learn.microsoft.com/en-us/machine-learning-server/r/how-to-revoscaler-distributed-computing-foreach
https://stackoverflow.com/questions/37636463/how-to-use-reduce-function-in-r-parallel-computing
https://stackoverflow.com/questions/55506788/how-to-transform-a-for-loop-in-a-foreach-loop-in-r

https://bookdown.org/rdpeng/RProgDA/profiling-and-benchmarking.html
https://bookdown.org/rdpeng/rprogdatascience/profiling-r-code.html
https://bookdown.org/rdpeng/rprogdatascience/data-analysis-case-study-changes-in-fine-particle-air-pollution-in-the-u.s..html#loading-and-processing-the-raw-data
https://cran.r-project.org/web/packages/rsimsum/vignettes/C-plotting.html
https://cran.r-project.org/web/packages/rsimsum/vignettes/A-introduction.html
https://github.com/ecophilina/herring-birds/blob/master/scripts/02-get-data.R
https://adv-r.hadley.nz/perf-measure.html
https://rstudio.github.io/profvis/index.html
https://cran.r-project.org/web/packages/bench/readme/README.html
https://www.tidyverse.org/blog/2018/06/bench-1.0.1/

https://jstaf.github.io/hpc-r/parallel/

https://www.r-tutor.com/gpu-computing

https://adv-r.hadley.nz/rcpp.html
https://gallery.rcpp.org/
http://rstudio.github.io/profvis/index.html
https://bookdown.org/rdpeng/rprogdatascience/parallel-computation.html#example-bootstrapping-a-statistic
http://zevross.com/blog/2019/02/12/dramatically-speed-up-your-r-purrr-functions-with-the-furrr-package/
https://furrr.futureverse.org/
https://www.tidyverse.org/blog/2020/10/furrr-0-2-0/
https://cran.r-project.org/web/packages/foreach/vignettes/foreach.html
https://www.blasbenito.com/post/02_parallelizing_loops_with_r/
https://docs.alliancecan.ca/wiki/R#install.packages.28.29
https://yxue-me.com/post/2019-05-12-a-glossary-of-parallel-computing-packages-in-r-2019/
https://dept.stat.lsa.umich.edu/~jerrick/courses/stat701/notes/parallel.html
https://docs.csc.fi/support/tutorials/parallel-r/
https://bioinfomagician.wordpress.com/2013/11/25/mpi-tutorial-for-r-rmpi/
https://help.rc.ufl.edu/doc/R_MPI_Example

https://medium.com/civis-analytics/programming-with-futures-in-r-401e906db384#:~:text=In%20plan(%E2%80%9Cmultisession%E2%80%9D),from%20the%20current%20R%20process.

https://towardsdatascience.com/getting-started-with-parallel-programming-in-r-d5f801d43745
https://nceas.github.io/oss-lessons/parallel-computing-in-r/parallel-computing-in-r.html
https://yxue-me.com/post/2019-05-12-a-glossary-of-parallel-computing-packages-in-r-2019/
https://subscription.packtpub.com/book/big-data-and-business-intelligence/9781784394004/1/ch01lvl1sec09/the-r-parallel-package
https://www.r-bloggers.com/2017/08/implementing-parallel-processing-in-r/
https://www.stat.umn.edu/geyer/3701/notes/parallel.html
https://www.stat.umn.edu/geyer/8054/notes/parallel.html
  https://blog.esciencecenter.nl/parallel-r-in-a-nutshell-4391d45b5461
https://dept.stat.lsa.umich.edu/~jerrick/courses/stat701/notes/parallel.html
https://bookdown.org/rdpeng/rprogdatascience/parallel-computation.html
https://docs.alliancecan.ca/wiki/R#doParallel_and_foreach
https://www.r-bloggers.com/2016/01/strategies-to-speedup-r-code/
https://www.datacamp.com/tutorial/r-tutorial-apply-family


https://www.r-bloggers.com/2022/02/avoid-loops-in-r-really/

https://spark.rstudio.com/

https://jstaf.github.io/hpc-r/parallel/

https://github.com/tidyverse/multidplyr
https://cran.r-project.org/web/packages/multidplyr/index.html
https://cran.r-project.org/web/packages/multidplyr/vignettes/multidplyr.html


https://dcgerard.github.io/advancedr/09_future.html
https://www.futureverse.org/packages-overview.html
https://github.com/HenrikBengtsson/future/issues/146
https://yxue-me.com/post/2019-05-12-a-glossary-of-parallel-computing-packages-in-r-2019/
https://grantmcdermott.com/ds4e/parallel.html
https://sparklyr.ai/
https://spark.rstudio.com/
https://docs.alliancecan.ca/wiki/Apache_Spark/en
https://therinspark.com/

https://future.batchtools.futureverse.org/
https://future.callr.futureverse.org/
https://cran.r-project.org/web/packages/future.apply/vignettes/future.apply-1-overview.html
https://callr.r-lib.org/

post in https://stackoverflow.com/questions/42393658/what-are-the-performance-differences-between-for-loops-and-the-apply-family-of-f

https://blogs.rstudio.com/ai/posts/2021-03-10-forecasting-time-series-with-torch_1/

Start large (4GB) on a test script. Then:
While the script is running, check how much memory is used in real time by typing: sstat yourjobID.batch --format="JobID,MaxRSS"
Or
When the script is done running, check how much was used by typing: sacct -o MaxRSS -j yourjobID
If you check the slurm.out file and you’re getting “oom-kill” errors, you need to request more memory
If you’re using less than you asked for, it’s beneficial to reduce the memory in --mem or --mem-per-cpu (this way your job will get scheduled sooner)
Resubmit your job with your new estimate.


Independent repeats of computations (e.g. bootstrapping)

No communication needed between computations.


(a) Start up M ‘worker’ processes, and do any initialization needed on the workers.
(b) Send any data required for each task to the workers.
(c) Split the task into M roughly equally-sized chunks, and send the chunks (including the R
code needed) to the workers.
(d) Wait for all the workers to complete their tasks, and ask them for their results.
(e) Repeat steps (b–d) for any further tasks.
(f) Shut down the worker processes.


look at mclapply

fib <- function(n) {
  if(n <= 1) {
    return(n)
  } else {
    return(fib(n-1) + fib(n-2))
  }
}
nterms=4
for(i in 0:(nterms-1)) {
  fib <- print(recurse_fibonacci(i))
}

Let's create a numerical vector `a` of length `1e6` filled with random numbers from the uniform distribution:

```{r}
a <- runif(1e6)
```

We want to create a new numerical vector `b` of the same length filled with the cosines of `a`.

We are often taught not to use for loops (which have been perceived as extremely inefficient in R), but to instead use a vectorized approach:

```{r}
b <- sapply(a, cos)

cos(a[1]) == b[1]
```

Let's try the for loop anyway:

```{r}
## It is very important to instantiate the output
## vector at once and not grow it at each iteration
c <- vector("numeric", 1e6)

for (i in seq_along(a)) {
  c[i] <- cos(a[i])
}

## Verifications
cos(a[1]) == c[1]
all.equal(b, c)
```

Let's do some benchmarking:

```{r}
## Reinstantiate an empty vector
c <- vector("numeric", 1e6)

mark(
  b <- sapply(a, cos),
  for (i in seq_along(a)) {
    c[i] <- cos(a[i])
  },
  check=F
)
```

:::{.note}

When comparing code, people suggest to focus on the `min` and `median` values rather than the `mean`.

:::

:::{.note}

Loops aren't always the worst option after all.

:::

Let's now use the `future` package.

First of all, let's verify that the `multisession` and `multicore` plans do launch multiple processes:

```{r}
## Id of current R process
Sys.getpid()

plan(sequential)

fd <- future({
  cat(Sys.getpid(), "\n")
})

fd

capture.output(d <- value(fd))

## Note that the current R process hasn't changed
Sys.getpid()

plan(multisession)

fd <- future({
  cat(Sys.getpid(), "\n")
})

capture.output(d <- value(fd))

Sys.getpid()

plan(multicore)

fd <- future({
  cat(Sys.getpid(), "\n")
})

capture.output(d <- value(fd))

Sys.getpid()

plan(sequential)

fd <- future({
  cat(Sys.getpid(), "\n")
})

capture.output(d <- value(fd))
```

Now, let's look at timing.

With the `sequential` plan, the timing should be similar to our code:

```{r}
library(future)
plan(sequential)

a <- runif(1e7)
b <- sapply(a, cos)

cos(a[1]) == b[1]

d %<-% {
  sapply(a, cos)
}

all.equal(b, d)
```

Now, let's try the `multisession` plan:

```{r}
plan(multisession)

mark(
  b <- sapply(a, cos),
  b %<-% {
    sapply(a, cos)
  },
  check=F,
  memory=F
)

fd <- future({
  cat(Sys.getpid(), "\n")
})

capture.output(d <- value(fd))
```

:::{.note}

speedup 25

:::

And with the `multicore` plan:

```{r}
plan(multicore)

mark(
  b <- sapply(a, cos),
  b %<-% {
    sapply(a, cos)
  },
  check=F,
  memory=F
)

fd <- future({
  cat(Sys.getpid(), "\n")
})

capture.output(d <- value(fd))
```

:::{.note}

speedup 80

:::

try print instead of cat
