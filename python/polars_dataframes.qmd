---
title: The world of data frames
author: Marie-Hélène Burle
---

:::{.def}

Let's talk about data frames, how they came to the world of programming, how pandas had the monopoly for many years in Python, and how things are changing very quickly at the moment.

:::

## Tabular data

Many fields of machine learning, data science, and humanities rely on tabular data where:

- columns hold variables and are homogeneous (same data type)—you can think of them as vectors,
- rows contain observations and can be heterogeneous.

Early computer options to manipulate such data were limited to [spreadsheets](https://en.wikipedia.org/wiki/Spreadsheet) (e.g. Microsoft Excel).

Dataframes (data frames or DataFrames) are two dimensional objects that brought tabular data to programming.

## Early history of data frames

After data frames emerged in S, then R, they were added to Python with the library [pandas](https://pandas.pydata.org/) in 2008:

```{dot}
//| echo: false
//| fig-height: 250px

strict graph {
  
bgcolor="transparent"
graph [fontname="Inconsolata, sans-serif"]
node [fontname="Inconsolata, sans-serif", fontsize=15]

y1 [label=1990, shape=plaintext, group=g1, group=g1]
y2 [label=2000, shape=plaintext, group=g1, group=g1]
y3 [label=2008, shape=plaintext, group=g1]

l1 [label="S programming language", href="https://en.wikipedia.org/wiki/S_(programming_language)", shape=plaintext, group=g2, fontcolor="#5592FD"]
l2 [label="R", href="https://en.wikipedia.org/wiki/R_(programming_language)", shape=plaintext, group=g2, fontcolor="#5592FD"]
l3 [label="pandas (Python)", href="https://en.wikipedia.org/wiki/pandas_(software)", shape=plaintext, group=g2, fontcolor="#5592FD"]

{rank=same; y1 l1}

y1 -- y2 -- y3
l1 -- l2 -- l3 [style=invis]

}
```

After which, pandas remained *the* Python data frame library for a long time.

## Issues with pandas

[Wes McKinney](https://wesmckinney.com/)—the author of pandas—himself [has complaints about pandas](https://wesmckinney.com/blog/apache-arrow-pandas-internals/):

- internals too far from the hardware,
- no support for memory-mapped datasets,
- poor performance in database and file ingest / export,
- lack of proper support for missing data,
- lack of memory use and RAM management transparency,
- weak support for categorical data,
- complex `groupby` operations awkward and slow,
- appending data to a DataFrame tedious and costly,
- limited and non-extensible type metadata,
- eager evaluation model with no query planning,
- slow and limited multicore algorithms for large datasets.

## A rich new field

Over the past few years, there has been an explosion of faster alternatives.

### Parallel computing

The Python [global interpreter lock (GIL)](https://en.wikipedia.org/wiki/Global_interpreter_lock) gets in the way of multi-threading, but several libraries allow the use of Python on multiple cores:

- [Ray](https://github.com/ray-project/ray)
- [Dask](https://github.com/dask/dask)
- [Apache Spark](https://github.com/apache/spark).

[Fugue](https://github.com/fugue-project/fugue/) provides a unified interface for distributed computing that works with all three libraries.

To use data frames on multiple cores, Dask and Spark have APIs for pandas and [Modin](https://docs.ray.io/en/latest/ray-more-libs/modin/index.html) provides a drop-in replacement for pandas in all three libraries.

### Accelerators

[RAPIDS](https://rapids.ai/) brings data frames on the GPUs with the [cuDF library](https://github.com/rapidsai/cudf) and integration with pandas is easy.

### Lazy out-of-core

[Vaex](https://github.com/vaexio/vaex) exists as an alternative to pandas.

### SQL

[Structured query language (SQL)](https://en.wikipedia.org/wiki/SQL) handles [relational databases](https://en.wikipedia.org/wiki/Relational_database), but the distinction between SQL and data frame software is getting increasingly blurry with most libraries now able to handle both.

[DuckDB](https://github.com/duckdb/duckdb) is a very fast and popular option with good integration with pandas.

Many additional options such as [dbt](https://github.com/dbt-labs/dbt-core) and the [snowflake snowpark Python API](https://github.com/snowflakedb/snowpark-python) exist, although integration with pandas is not always as good.

### Polars

The new memory standard is [Apache Arrow](https://arrow.apache.org/) and the most efficient library making use of it is [Polars](https://pola.rs/).

Most libraries are developing an integration with Polars, lodging it nicely in the Python ecosystem.

## Best data frame strategy

For maximum efficiency, the best strategy currently seems to be:

- Single machine&nbsp; ➔ &nbsp;use Polars
- Cluster &emsp;&emsp; ➔ &nbsp;use Polars + [fugue](https://github.com/fugue-project/fugue/) ([example benchmark](https://medium.com/fugue-project/benchmarking-pyspark-pandas-pandas-udfs-and-fugue-polars-198c3109a226), [documentation of Polars integration](https://fugue-tutorials.readthedocs.io/tutorials/integrations/backends/polars.html))
- GPUs available&nbsp; ➔ &nbsp;use Polars + [RAPIDS](https://rapids.ai/) library [cuDF](https://github.com/rapidsai/cudf) ([Polars integration coming soon](https://pola.rs/posts/polars-on-gpu/))
- Cluster with GPUs&nbsp; ➔ &nbsp;use Polars + fugue + RAPIDS

No matter the scenario, Polars is better than pandas and you should use it instead.
