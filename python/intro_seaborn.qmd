---
title: "seaborn: a brief introduction"
author: Marie-Hélène Burle
bibliography: python.bib
csl: ../diabetologia.csl
---

:::{.def}

[seaborn](https://seaborn.pydata.org/), at its core, is a statistical visualization library for tabular data[@Waskom2021]. It is based on [matplotlib](https://matplotlib.org/), but its higher-level, declarative, and easy interface makes it ideal for [exploratory data analysis (EDA)](https://en.wikipedia.org/wiki/Exploratory_data_analysis). It comes with demo datasets and statistical tools.

:::

## Getting data

[seaborn](https://seaborn.pydata.org/) comes with a number of [datasets](https://github.com/mwaskom/seaborn-data) used in its documentation. They are convenient to play with the library. In this section however, we use [snow survey data for the province of British Columbia (Canada)](https://www2.gov.bc.ca/gov/content/environment/air-land-water/water/water-science-data/water-data-tools/snow-survey-data).

The data is tabular and stored in a CSV file. We read it into a Polars DataFrame:

```{python}
#| echo: false

import polars as pl
file_path = "data/allmss_archive.csv"
```

```{.python}
import polars as pl

file_path = "/project/def-sponsor00/data/allmss_archive.csv"
```

### Troubleshooting data reading

[As we saw earlier](intro_df.qmd), you read tabular data in a Polars DataFrame with:

```{.python}
df = pl.read_csv(file_path)
```

However, here, we get the following error:

```
ComputeError: could not parse `35.0` as dtype `i64` at column ' Snow Depth cm' (column number 5)
```

With, conveniently, the following suggestions:

```
You might want to try:
- increasing `infer_schema_length` (e.g. `infer_schema_length=10000`),
- specifying correct dtype with the `schema_overrides` argument
- setting `ignore_errors` to `True`,
- adding `35.0` to the `null_values` list.n
```

If you want to be sure to read the data without error, you can use `ignore_errors=True`, but this will drop the problematic rows and you will loose data. This is the solution of last resort.

A better solution here is to set the [schema](https://docs.pola.rs/user-guide/lazy/schemas/) for the problematic variable manually:

```{.python}
df = pl.read_csv(file_path, schema_overrides={" Snow Depth cm": pl.Float64})
```

But oops. We now get the following error:

```
ComputeError: could not parse `760.0` as dtype `i64` at column ' Water Equiv. mm' (column number 6)
```

:::{.exo}

:::{.yourturn}

Your turn:

:::

Fix the new problem to read in the data.

```{python}
#| echo: false

df = pl.read_csv(
	file_path,
	schema_overrides={" Snow Depth cm": pl.Float64, " Water Equiv. mm": pl.Float64}
)
```

:::

## Data exploration and transformation

Here is our DataFrame:

```{python}
print(df)
```

### First cleaning

Let's create a cleaned DataFrame with relevant columns, the date into a proper format, etc.:

```{python}
snow_data = df.select(
	pl.col("Snow Course Name").str.to_titlecase().alias("station"),
	pl.col(" Elev. metres").alias("elevation"),
	pl.col(" Date of Survey").str.to_date("%Y/%m/%d").alias("date"),
	pl.col(" Snow Depth cm").alias("snow"),
	pl.col(" Water Equiv. mm").alias("water")
)

print(snow_data)
```

We can explore the data a bit:

```{python}
print(snow_data.columns)
```

```{python}
snow_data.glimpse()
```

```{python}
print(snow_data.describe())
```

```{python}
n_stations = snow_data.select("station").n_unique()
print(f"There are {n_stations} stations.")
```

```{python}
list_stations = snow_data.get_column("station").unique().to_list()
print(list_stations)
```

### Stations selection

Let's select subsets of snow pillow stations located in the Southern Coastal Mountains of British Columbia. They exclude stations in the Interior (Okanagan, Kamloops, Kootenays, Cariboo, Rockies, Columbias) and Northern BC. This is where a decrease in snowpack due to global warming is the most likely to be detectable since the temperatures are not very cold and a few degrees of warming is enough to go from snow to rain.

:::{.note}

I used Gemini 3 to split the list of stations into mountain ranges. This was very convenient since the data doesn't include coordinates (that would have allowed to filter by latitude and longitude) nor information about the location of the stations.

:::

Vancouver Island Ranges:

```{python}
island_stations = [
    "Burman Lake", "Dickson Lake", "East Creek", "Forbidden Plateau", "Green Mountain",
    "Heather Mountain", "Heather Mountain Upper", "June Lake", "Labour Day Lake",
    "Loch Lomond", "Lyford Mountain", "Mount Cokely", "Mount Wells", "Newcastle Ridge",
    "Sproat Lake", "Tennent Lake", "Upper Quinsam", "Upper Thelwood Lake",
    "Wolf River Lower", "Wolf River Middle", "Wolf River Upper"
]
```

Lower Mainland & Fraser Valley:

```{python}
vancouver_stations = [
    "Alouette Lake", "Black Mountain (Cypress Provincial Park)", "Burwell Lake (North Shore)",
    "Coquitlam Lake", "Dog Mountain (Mount Seymour)", "Grouse Mountain", "Hollyburn", "Hope",
    "Mount Saint Anne (Sunshine Coast/Sechelt)", "Mount Seymour",
    "Orchid Lake (Stave/Lower Mainland)", "Palisade Lake (Capilano Watershed)",
    "Stave Lake", "Steelhead", "Upper Stave River", "Wahleach Lake"
]
```

Sea-to-Sky, Garibaldi & Bridge River:

```{python}
s2s_stations = [
    "Bralorne", "Bralorne (Upper)", "Bridge Glacier Lower", "Callaghan Creek", "Diamond Head",
    "Downton Lake Upper", "Duffey Lake", "Garibaldi Lake", "McGillivray Pass", "Mount Cook",
    "Mount Penrose", "Mount Sheba", "Powell River", "Powell River Lower", "Powell River Upper",
    "Shalalth", "Shovelnose Mountain", "Tenquille Lake", "Toba River",
    "Tyaughton Creek (North)", "Whistler Mountain"
]
```

Cascades & Coquihalla (Southern Border Ranges):

```{python}
cascades_stations = [
    "Blackwall Peak", "Boston Bar Creek (Lower)", "Boston Bar Creek (Upper)", "Great Bear",
    "Klesilkwa", "Lightning Lake", "Nahatlatch River", "New Tashme", "Sumallo River",
    "Sumallo River West", "Sunday Summit", "Tashme"
]
```

```{python}
selected_data = snow_data.filter(
    pl.col("station").is_in(
        island_stations + vancouver_stations + s2s_stations + cascades_stations
    )
).with_columns(
    pl.when(pl.col("station").is_in(island_stations)).then(pl.lit("Vancouver Island"))
    .when(pl.col("station").is_in(vancouver_stations)).then(pl.lit("Lower Mainland"))
    .when(pl.col("station").is_in(s2s_stations)).then(pl.lit("Sea to Sky"))
    .otherwise(pl.lit("Cascades"))
    .alias("range")
)
```

### Convert to seasons

Snow pillow data go from October ^1st^ of one year to September ^30th^ of the following year. Let's bin the data into snow seasons:

```{python}
season_data = selected_data.with_columns(
	pl.col("date").dt.offset_by("3mo").dt.year().alias("season")
)

print(season_data)
```

### Select balanced panel

[`polars.DataFrame.pivot`](https://docs.pola.rs/api/python/stable/reference/dataframe/api/polars.DataFrame.pivot.html) turns a long-format DataFrame to a wide-format one.

```{python}
stations_years = season_data.pivot(
    "season",
    index="station",
    values="snow",
    sort_columns=True,
    aggregate_function="mean"
)

print(stations_years)
```

This clearly shows that the data was not collected on all the stations for all years. If we want to see trends over time, this is not good, particularly since not all stations are at the same elevation (if early years mostly had low elevations stations and recent years have more stations in the alpine, this would give a strong fake signal).

We need to select a period and a set of stations that are monitored throughout that period. This is called a balanced panel.

```{python}
def n_valid_in_period(df, period_start, period_end):
    test_seasons = range(period_start, period_end + 1)

    # Filter data for the test seasons
    test_seasons_subset = df.filter(pl.col("season").is_in(test_seasons))

    # Count seasons per station
    station_counts = test_seasons_subset.group_by("station").agg(
        pl.col("season").n_unique().alias("n_seasons")
    )

    # Filter stations with data for all test seasons
    valid_stations = station_counts.filter(pl.col("n_seasons") == len(test_seasons))

    n_valid = len(valid_stations)

    return (n_valid, period_start, period_end)
```

```{python}
years = season_data.select(pl.col("date").dt.year())
min_year = years.min().item()
max_year = years.max().item()
print(f"Year range: {min_year} to {max_year}")
```

```{python}
list_n_valid = []

for i in range(min_year, max_year + 1):
    for j in range(min_year, max_year + 1):
        if j - i > 9:         # we want a period of at least 10 years
            list_n_valid.append(n_valid_in_period(season_data, i, j))
```

We want at least 5 stations, so let's clean the list from all `n_valid < 5`:

```{python}
non_nil_list_n_valid = [i for i in list_n_valid if i[0] >= 5]
```

```{python}
balanced_panels = pl.DataFrame(
    non_nil_list_n_valid,
    orient="row",
    schema=["n_valid", "period_start", "period_end"]
).with_columns(
    (pl.col("period_end") - pl.col("period_start")).alias("period_length")
).sort(
    "period_length", "n_valid", descending=True
)
```

```{python}
print(balanced_panels)
```

:::{.exo}

:::{.yourturn}

Your turn:

:::

Create a DataFrame of balanced panels similar to `balanced_panels` but in which all periods include the present.

Call it `current`.

```{python}
#| echo: false

balanced_current = balanced_panels.filter(pl.col("period_end") == 2026)
```

:::

To print more rows than the default, you can use [`polars.Config.set_tbl_rows`](https://docs.pola.rs/api/python/dev/reference/api/polars.Config.set_tbl_rows.html):

```{python}
with pl.Config(tbl_rows=100):
    print(balanced_current)
```

48 seasons for 8 stations is not bad, so let's select the period from 1978 to the present.

```{python}
subset_df = season_data.filter(
    pl.col("season").is_between(1978, 2026)
)

print(subset_df)
```

```{python}
max_n_seasons = subset_df.group_by("station").agg(
    pl.col("season").n_unique().alias("n_seasons")
).select("n_seasons").max().item()

subset_stations = subset_df.group_by("station").agg(
    pl.col("season").n_unique().alias("n_seasons")
).filter(pl.col("n_seasons") == max_n_seasons).get_column("station").to_list()

print(subset_stations)
```

```{python}
subset_data = subset_df.filter(pl.col("station").is_in(subset_stations))

print(subset_data)
```

### ADD ENSO data

[El Niño–Southern Oscillation (ENSO)](https://en.wikipedia.org/wiki/El_Ni%C3%B1o%E2%80%93Southern_Oscillation) 

Based on NOAA ONI data. Year listed is the "Season Year" (approx Jan-Apr of that year). e.g. El Niño 1997-98 -> Season 1998

<!-- ```{python} -->
<!-- # all events -->

<!-- el_nino_years = [ -->
<!-- 	1940, 1941, 1942, 1947, 1952, 1954, 1958, 1959, 1964, 1966, 1969, 1970, 1973, 1977, 1978, -->
<!-- 	1980, 1983, 1987, 1988, 1992, 1995, 1998, 2003, 2005, 2007, 2010, 2015, 2016, 2019, 2020, 2024 -->
<!-- ] -->

<!-- la_nina_years = [ -->
<!-- 	1939, 1943, 1945, 1946, 1950, 1951, 1955, 1956, 1957, 1962, 1965, 1968, 1971, 1972, 1974, 1975, 1976, 1984, -->
<!-- 	1985, 1989, 1996, 1999, 2000, 2001, 2006, 2008, 2009, 2011, 2012, 2017, 2018, 2021, 2022, 2023, 2025, 2026 -->
<!-- ] -->
<!-- ``` -->

<!-- all events, selected period only: -->
```{python}
el_nino_years = [
	1978, 1980, 1983, 1987, 1988, 1992, 1995, 1998, 2003,
    2005, 2007, 2010, 2015, 2016, 2019, 2020, 2024
]

la_nina_years = [
	1984, 1985, 1989, 1996, 1999, 2000, 2001, 2006, 2008, 2009,
    2011, 2012, 2017, 2018, 2021, 2022, 2023, 2025, 2026
]
```

<!-- ```{python} -->
<!-- # strong events -->

<!-- el_nino_years = [ -->
<!-- 	1941, 1942, 1958, 1966, 1973, 1978, 1980, 1983, 1987, -->
<!-- 	1988, 1992, 1995, 1998, 2003, 2007, 2010, 2016, 2024 -->
<!-- ] -->

<!-- la_nina_years = [ -->
<!-- 	1939, 1943, 1950, 1951, 1955, 1956, 1962, 1971, 1974, -->
<!-- 	1976, 1989, 1999, 2000, 2008, 2011, 2012, 2021, 2022 -->
<!-- ] -->
<!-- ``` -->

<!-- ```{python} -->
<!-- # strong events, selected period only -->

<!-- el_nino_years = [ -->
<!-- 	1978, 1980, 1983, 1987,	1988, 1992, 1995, 1998, 2003, 2007, 2010, 2016, 2024 -->
<!-- ] -->

<!-- la_nina_years = [ -->
<!-- 	1989, 1999, 2000, 2008, 2011, 2012, 2021, 2022 -->
<!-- ] -->
<!-- ``` -->

```{python}
subset_enso = subset_data.with_columns(
	pl.when(pl.col("season").is_in(el_nino_years)).then(pl.lit("El Niño"))
	.when(pl.col("season").is_in(la_nina_years)).then(pl.lit("La Niña"))
	.otherwise(pl.lit("Neutral"))
	.alias("enso")
)

print(subset_enso)
```

## Plotting

Let's load the seaborn library:

```{python}
import seaborn as sns
```

### Relation plots

You could xxx

```{python}
season_means = subset_enso.group_by(
	pl.col("season"),
	maintain_order=True
).agg(
	pl.col("snow").mean().alias("mean_snow")
)
```

```{python}
sns.relplot(
	data=season_means,
	kind="line",
	x="season",
	y="mean_snow"
).figure.set_size_inches(9, 4)
```

But it isn't necessary with seaborn which is a statistically oriented plotting framework:

```{python}
sns.relplot(
	data=subset_enso,
	kind="line",
	x="season",
	y="snow"
).figure.set_size_inches(9, 4)
```

### Regression plots

`regplot`

```{python}
sns.regplot(
	data=subset_enso,
	x="season",
	y="snow"
).figure.set_size_inches(9, 4)
```

`lmplot`

```{python}
sns.lmplot(
	data=subset_enso,
	x="season",
	y="snow",
    hue="enso"
).figure.set_size_inches(9, 4)
```

```{python}
el_nino_seasons = subset_enso.filter(
    pl.col("enso") == "El Niño"
)
```

```{python}
sns.relplot(
	data=el_nino_seasons,
	kind="line",
	x="season",
	y="snow"
).figure.set_size_inches(9, 4)
```

```{python}
sns.regplot(
	data=el_nino_seasons,
	x="season",
	y="snow"
).figure.set_size_inches(9, 4)
```

:::{.exo}

:::{.yourturn}

Your turn:

:::

Make the same plot with only the stations below 1100m to see whether climate change is affecting low elevations more strongly.

```{python}
#| echo: false

elevations = sorted(subset_enso.get_column("elevation").unique().to_list())
```

```{python}
#| echo: false

low_stations = el_nino_seasons.filter(
    pl.col("elevation") <= 1100
)
```

<!-- ```{python} -->
<!-- sns.regplot( -->
<!-- 	data=low_stations, -->
<!-- 	x="season", -->
<!-- 	y="snow" -->
<!-- ).figure.set_size_inches(9, 4) -->
<!-- ``` -->

:::

### Boxplots

```{python}
sns.boxplot(
	data=subset_enso,
	x="enso",
	y="snow",
	order=["La Niña", "Neutral", "El Niño"]
).figure.set_size_inches(9, 9)
```
