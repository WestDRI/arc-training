---
title: "seaborn: a brief introduction"
author: Marie-Hélène Burle
bibliography: python.bib
csl: ../diabetologia.csl
---

:::{.def}

[seaborn](https://seaborn.pydata.org/), at its core, is a statistical visualization library for tabular data[@Waskom2021]. It is based on [matplotlib](https://matplotlib.org/), but its higher-level, declarative, and easy interface makes it ideal for [exploratory data analysis (EDA)](https://en.wikipedia.org/wiki/Exploratory_data_analysis). It comes with demo datasets and statistical tools.

:::

## Getting data

[seaborn](https://seaborn.pydata.org/) comes with a number of [datasets](https://github.com/mwaskom/seaborn-data) used in its documentation. They are convenient to play with the library. In this section however, we use [snow survey data for the province of British Columbia (Canada)](https://www2.gov.bc.ca/gov/content/environment/air-land-water/water/water-science-data/water-data-tools/snow-survey-data). I am interested in assessing whether global warming is causing a decrease in the snowpack height in this part of the world.

The data is tabular and stored in a CSV file. We read it into a Polars DataFrame:

```{python}
#| echo: false

import polars as pl
file_path = "data/allmss_archive.csv"
```

```{.python}
import polars as pl

file_path = "/project/def-sponsor00/data/allmss_archive.csv"
```

### Troubleshooting data reading

[As we saw earlier](intro_df.qmd), you read tabular data in a Polars DataFrame with:

```{.python}
df = pl.read_csv(file_path)
```

However, here, we get the following error:

```
ComputeError: could not parse `35.0` as dtype `i64` at column ' Snow Depth cm' (column number 5)
```

With, conveniently, the following suggestions:

```
You might want to try:
- increasing `infer_schema_length` (e.g. `infer_schema_length=10000`),
- specifying correct dtype with the `schema_overrides` argument
- setting `ignore_errors` to `True`,
- adding `35.0` to the `null_values` list.n
```

If you want to be sure to read the data without error, you can use `ignore_errors=True`, but this will drop the problematic rows and you will loose data. This is the solution of last resort.

A better solution here is to set the [schema](https://docs.pola.rs/user-guide/lazy/schemas/) for the problematic variable manually:

```{.python}
df = pl.read_csv(file_path, schema_overrides={" Snow Depth cm": pl.Float64})
```

But oops. We now get the following error:

```
ComputeError: could not parse `760.0` as dtype `i64` at column ' Water Equiv. mm' (column number 6)
```

:::{.exo}

:::{.yourturn}

Your turn:

:::

Fix the new problem to read in the data.

```{python}
#| echo: false

df = pl.read_csv(
	file_path,
	schema_overrides={" Snow Depth cm": pl.Float64, " Water Equiv. mm": pl.Float64}
)
```

:::

## Data exploration and transformation

Here is our DataFrame:

```{python}
print(df)
```

### First cleaning

Let's create a cleaned DataFrame with relevant columns, the date into a proper format, etc.:

```{python}
snow_data = df.select(
	pl.col("Snow Course Name").str.to_titlecase().alias("station"),
	pl.col(" Elev. metres").alias("elevation"),
	pl.col(" Date of Survey").str.to_date("%Y/%m/%d").alias("date"),
	pl.col(" Snow Depth cm").alias("snow"),
	pl.col(" Water Equiv. mm").alias("water")
)

print(snow_data)
```

We can explore the data a bit:

```{python}
print(snow_data.columns)
```

```{python}
snow_data.glimpse()
```

```{python}
print(snow_data.describe())
```

```{python}
n_stations = snow_data.select("station").n_unique()
print(f"There are {n_stations} stations.")
```

```{python}
list_stations = snow_data.get_column("station").unique().to_list()
print(list_stations)
```

### Stations selection

Let's select subsets of snow pillow stations located in the Southern Coastal Mountains of British Columbia. They exclude stations in the Interior (Okanagan, Kamloops, Kootenays, Cariboo, Rockies, Columbias) and Northern BC. This is where a decrease in snowpack due to global warming is the most likely to be detectable since the temperatures are not very cold and a few degrees of warming is enough to go from snow to rain.

:::{.note}

I used Gemini 3 to split the list of stations into mountain ranges. This was very convenient since the data doesn't include coordinates (that would have allowed to filter by latitude and longitude) nor information about the location of the stations.

:::

```{python}
# Vancouver Island Ranges:
island_stations = [
    "Burman Lake", "Dickson Lake", "East Creek", "Forbidden Plateau", "Green Mountain",
    "Heather Mountain", "Heather Mountain Upper", "June Lake", "Labour Day Lake",
    "Loch Lomond", "Lyford Mountain", "Mount Cokely", "Mount Wells", "Newcastle Ridge",
    "Sproat Lake", "Tennent Lake", "Upper Quinsam", "Upper Thelwood Lake",
    "Wolf River Lower", "Wolf River Middle", "Wolf River Upper"
]

# Lower Mainland & Fraser Valley:
vancouver_stations = [
    "Alouette Lake", "Black Mountain (Cypress Provincial Park)", "Burwell Lake (North Shore)",
    "Coquitlam Lake", "Dog Mountain (Mount Seymour)", "Grouse Mountain", "Hollyburn", "Hope",
    "Mount Saint Anne (Sunshine Coast/Sechelt)", "Mount Seymour",
    "Orchid Lake (Stave/Lower Mainland)", "Palisade Lake (Capilano Watershed)",
    "Stave Lake", "Steelhead", "Upper Stave River", "Wahleach Lake"
]

# Sea-to-Sky, Garibaldi & Bridge River:
s2s_stations = [
    "Bralorne", "Bralorne (Upper)", "Bridge Glacier Lower", "Callaghan Creek", "Diamond Head",
    "Downton Lake Upper", "Duffey Lake", "Garibaldi Lake", "McGillivray Pass", "Mount Cook",
    "Mount Penrose", "Mount Sheba", "Powell River", "Powell River Lower", "Powell River Upper",
    "Shalalth", "Shovelnose Mountain", "Tenquille Lake", "Toba River",
    "Tyaughton Creek (North)", "Whistler Mountain"
]

# Cascades & Coquihalla (Southern Border Ranges):
cascades_stations = [
    "Blackwall Peak", "Boston Bar Creek (Lower)", "Boston Bar Creek (Upper)", "Great Bear",
    "Klesilkwa", "Lightning Lake", "Nahatlatch River", "New Tashme", "Sumallo River",
    "Sumallo River West", "Sunday Summit", "Tashme"
]
```

Now we can filter the data for all these stations:

```{python}
selected_data = snow_data.filter(
    pl.col("station").is_in(
        island_stations + vancouver_stations + s2s_stations + cascades_stations
    )
).with_columns(
    pl.when(pl.col("station").is_in(island_stations)).then(pl.lit("Vancouver Island"))
    .when(pl.col("station").is_in(vancouver_stations)).then(pl.lit("Lower Mainland"))
    .when(pl.col("station").is_in(s2s_stations)).then(pl.lit("Sea to Sky"))
    .otherwise(pl.lit("Cascades"))
    .alias("range")
)
```

### Convert to seasons

Snow pillow data go from October 1^st^ of one year to September 30^th^ of the following year. Let's add a `season` column:

```{python}
season_data = selected_data.with_columns(
	pl.col("date").dt.offset_by("3mo").dt.year().alias("season")
)

print(season_data)
```

### Select balanced panel

Our data is in long format. It is the format we want for plotting and analyses. The wide format has use cases too. For instance, it allows to easily see whether data was collected for all stations at all times.

[`polars.DataFrame.pivot`](https://docs.pola.rs/api/python/stable/reference/dataframe/api/polars.DataFrame.pivot.html) turns a long-format DataFrame to a wide-format one:

```{python}
stations_years = season_data.pivot(
    "season",
    index="station",
    values="snow",
    sort_columns=True,
    aggregate_function="mean"
)

print(stations_years)
```

This clearly shows that the data was not collected on all stations for all years. If we want to see trends over time, this is not good, particularly since not all stations are at the same elevation (if early years mostly had low elevations stations and recent years have more stations in the alpine, we would get a totally fake signal).

We need to select a period and a set of stations that are monitored throughout that period. This is called a balanced panel.

Let's define a function that returns the number of stations that have data collected for each season between 2 particular seasons:

```{python}
def n_valid_in_period(df, period_start, period_end):
    test_seasons = range(period_start, period_end + 1)

    # Filter data for the test seasons
    test_seasons_subset = df.filter(pl.col("season").is_in(test_seasons))

    # Count seasons per station
    station_counts = test_seasons_subset.group_by("station").agg(
        pl.col("season").n_unique().alias("n_seasons")
    )

    # Filter stations with data for all test seasons
    valid_stations = station_counts.filter(pl.col("n_seasons") == len(test_seasons))

    n_valid = len(valid_stations)

    return n_valid, period_start, period_end
```

We can test our function:

```{python}
print(n_valid_in_period(season_data, 1998, 2001))
```

This means that 29 stations have data collected every season between 1998 and 2001.

Now we need to know the earliest and latest seasons in our dataset:

```{python}
seasons = season_data.select(pl.col("season"))
min_season = seasons.min().item()
max_season = seasons.max().item()
print(f"Seasons range: {min_season} to {max_season}")
```

We can create a list the number of stations present in the whole period for each period spanning at least 10 seasons:

```{python}
list_n_valid = []

for i in range(min_season, max_season + 1):
    for j in range(min_season, max_season + 1):
        if j - i > 9:         # we want a period of at least 10 seasons
            list_n_valid.append(n_valid_in_period(season_data, i, j))
```

We want at least 5 stations, so let's clean the list from all `n_valid < 5`:

```{python}
selected_list_n_valid = [i for i in list_n_valid if i[0] >= 5]

print(selected_list_n_valid)
```

Now we can turn this new list of tuples into a DataFrame, add a column with the length of the period, and sort it by decreasing period length:

```{python}
balanced_panels = pl.DataFrame(
    selected_list_n_valid,
    orient="row",
    schema=["n_valid", "period_start", "period_end"]
).with_columns(
    (pl.col("period_end") - pl.col("period_start")).alias("period_length")
).sort(
    "period_length", "n_valid", descending=True
)

print(balanced_panels)
```

If there is any trend such as a decrease in the snow pack height due to global warming, we want to include data up to the present.

:::{.exo}

:::{.yourturn}

Your turn:

:::

Create a DataFrame of balanced panels similar to `balanced_panels` but in which all periods include the present.

Call it `current`.

```{python}
#| echo: false

balanced_current = balanced_panels.filter(pl.col("period_end") == 2026)
```

:::

To print more rows than the default, you can use [`polars.Config.set_tbl_rows`](https://docs.pola.rs/api/python/dev/reference/api/polars.Config.set_tbl_rows.html):

```{python}
with pl.Config(tbl_rows=100):
    print(balanced_current)
```

48 seasons for 8 stations is not bad, so let's select the period from 1978 to the present and create a new DataFrame called `subset_df`:

```{python}
subset_df = season_data.filter(
    pl.col("season").is_between(1978, 2026)
)

print(subset_df)
```

Now we want to get a list of the stations that have data for all seasons between 1978 and the present:

```{python}
max_n_seasons = subset_df.group_by("station").agg(
    pl.col("season").n_unique().alias("n_seasons")
).select("n_seasons").max().item()

subset_stations = subset_df.group_by("station").agg(
    pl.col("season").n_unique().alias("n_seasons")
).filter(pl.col("n_seasons") == max_n_seasons).get_column("station").to_list()

print(len(subset_stations))  # should return 8
print(subset_stations)
```

Finally, we filter data for those stations:

```{python}
subset_data = subset_df.filter(pl.col("station").is_in(subset_stations))

print(subset_data)
```

`subset_data` is a balanced panel of 8 stations between 1978 and 2026. That's the data we will plot and play with.

### Add ENSO data

The Pacific coast is strongly affected by [El Niño–Southern Oscillation (ENSO)](https://en.wikipedia.org/wiki/El_Ni%C3%B1o%E2%80%93Southern_Oscillation)—an unpredictable pseudo-cycle of sea-surface temperature changes that lead to atmospheric oscillations. El Niño years tend to be drier and warmer, so bad for the snowpack (and skiers!) while La Niña tend to be wetter and colder (so great for non-skiers but bad for the snowpack and skiers).

Below a list of El Niño and La Niña years for our period of interest. It comes from the [National Oceanic and Atmospheric Administration](https://en.wikipedia.org/wiki/National_Oceanic_and_Atmospheric_Administration) Oceanic Niño Index (ONI) [data](https://www.cpc.ncep.noaa.gov/products/analysis_monitoring/ensostuff/ONI_v5.php). Years that don't fall under either El Niño or La Niña are considered "neutral".

This tells a simplified story because not all El Niño or La Niña years are the same: there are strong, moderate, and weak events in which the patterns observed during El Niño or La Niña have different intensities.

<!-- ```{python} -->
<!-- # all events -->

<!-- el_nino_years = [ -->
<!-- 	1940, 1941, 1942, 1947, 1952, 1954, 1958, 1959, 1964, 1966, 1969, 1970, 1973, 1977, 1978, -->
<!-- 	1980, 1983, 1987, 1988, 1992, 1995, 1998, 2003, 2005, 2007, 2010, 2015, 2016, 2019, 2020, 2024 -->
<!-- ] -->

<!-- la_nina_years = [ -->
<!-- 	1939, 1943, 1945, 1946, 1950, 1951, 1955, 1956, 1957, 1962, 1965, 1968, 1971, 1972, 1974, 1975, 1976, 1984, -->
<!-- 	1985, 1989, 1996, 1999, 2000, 2001, 2006, 2008, 2009, 2011, 2012, 2017, 2018, 2021, 2022, 2023, 2025, 2026 -->
<!-- ] -->
<!-- ``` -->

<!-- all events, selected period only: -->
```{python}
el_nino_years = [
	1978, 1980, 1983, 1987, 1988, 1992, 1995, 1998, 2003,
    2005, 2007, 2010, 2015, 2016, 2019, 2020, 2024
]

la_nina_years = [
	1984, 1985, 1989, 1996, 1999, 2000, 2001, 2006, 2008, 2009,
    2011, 2012, 2017, 2018, 2021, 2022, 2023, 2025, 2026
]
```

<!-- ```{python} -->
<!-- # strong events -->

<!-- el_nino_years = [ -->
<!-- 	1941, 1942, 1958, 1966, 1973, 1978, 1980, 1983, 1987, -->
<!-- 	1988, 1992, 1995, 1998, 2003, 2007, 2010, 2016, 2024 -->
<!-- ] -->

<!-- la_nina_years = [ -->
<!-- 	1939, 1943, 1950, 1951, 1955, 1956, 1962, 1971, 1974, -->
<!-- 	1976, 1989, 1999, 2000, 2008, 2011, 2012, 2021, 2022 -->
<!-- ] -->
<!-- ``` -->

<!-- ```{python} -->
<!-- # strong events, selected period only -->

<!-- el_nino_years = [ -->
<!-- 	1978, 1980, 1983, 1987,	1988, 1992, 1995, 1998, 2003, 2007, 2010, 2016, 2024 -->
<!-- ] -->

<!-- la_nina_years = [ -->
<!-- 	1989, 1999, 2000, 2008, 2011, 2012, 2021, 2022 -->
<!-- ] -->
<!-- ``` -->

We can add a column for ENSO data to our DataFrame (note the use of `polars.when`, `polars.then`, `polars.otherwise`, `polars.is_in`, and `polars.lit` in this snippet):

```{python}
subset_enso = subset_data.with_columns(
	pl.when(pl.col("season").is_in(el_nino_years)).then(pl.lit("El Niño"))
	.when(pl.col("season").is_in(la_nina_years)).then(pl.lit("La Niña"))
	.otherwise(pl.lit("Neutral"))
	.alias("enso")
)

print(subset_enso)
```

## Plotting

Let's load the seaborn library:

```{python}
import seaborn as sns
```

There are several ways to plot in seaborn. In this course, we will use the functions that plot across [`seaborn.FacedGrid`](https://seaborn.pydata.org/generated/seaborn.FacetGrid.html#seaborn.FacetGrid).

### Relationship plots

[`seaborn.relplot`](https://seaborn.pydata.org/generated/seaborn.relplot.html#seaborn.relplot) displays relationships between dependent and independent variables.

#### Scatter plots

To display the relationship between the height of snow and the season, you could first calculate the mean height of the snowpack for each season:

```{python}
season_means = subset_enso.group_by(
	pl.col("season"),
	maintain_order=True
).agg(
	pl.col("snow").mean().alias("mean_snow")
)
```

Then plot those means against the seasons:

```{python}
sns.relplot(
	data=season_means,
	x="season",
	y="mean_snow"
).figure.set_size_inches(8, 4)
```

In Jupyter, the plots will be displayed automatically. Outside Jupyter, you need to import `matplotlib.pyplot` and run `matplotlib.pyplot.show()`:

```{.python}
import matplotlib.pyplot as plt
plt.show()
```

#### Line plots

By default `kind` is set to `scatter` and uses under the hood the function [`season.scatterplot`](https://seaborn.pydata.org/generated/seaborn.scatterplot.html#seaborn.scatterplot).

You can change it to `line` and it will use instead the function [`seaborn.lineplot`](https://seaborn.pydata.org/generated/seaborn.lineplot.html#seaborn.lineplot):

```{python}
sns.relplot(
	data=season_means,
	kind="line",
	x="season",
	y="mean_snow"
).figure.set_size_inches(8, 4)
```

But calculating the mean manually as we just did isn't necessary with seaborn which is a statistically oriented plotting framework. You can directly plot the height of the snowpack against the seasons and seaborn will calculate and plot the mean and 95% confidence interval around the mean:

```{python}
sns.relplot(
	data=subset_enso,
	kind="line",
	x="season",
	y="snow"
).figure.set_size_inches(8, 4)
```

We can see that there is a great deal of year to year variation in the snowpack height, but it doesn't look like there is much of a trend in the height of the snowpack over time.

To assess this further, we can use regression plots.

### Regression plots

[`seaborn.lmplot`](https://seaborn.pydata.org/generated/seaborn.lmplot.html#seaborn.lmplot) plots the data and a linear regression model fit:

```{python}
sns.lmplot(
	data=subset_enso,
	x="season",
	y="snow"
).figure.set_size_inches(8, 4)
```

Note that you can overlay multiple plots, as long as they share the same axis, by running them in the cell Jupyter cell or, if outside Jupyter, calling them before calling `matplotlib.pyplot.show()`. In our case, the result is very busy and not great however:

```{python}
sns.relplot(
	data=subset_enso,
	kind="line",
	x="season",
	y="snow"
).figure.set_size_inches(8, 4)

sns.lmplot(
	data=subset_enso,
	x="season",
	y="snow"
).figure.set_size_inches(8, 4)
```

You can get the regression lines across various categories by passing a categorical variable to the `hue` argument:

```{python}
sns.lmplot(
	data=subset_enso,
	x="season",
	y="snow",
    hue="enso"
).figure.set_size_inches(8, 4)
```

It looks like there is a downward trend for El Niño years.

:::{.exo}

:::{.yourturn}

Your turn:

:::

Create relationship and regression plots for the El Niño data only.

They should look like the following:

```{python}
#| echo: false

el_nino_seasons = subset_enso.filter(
    pl.col("enso") == "El Niño"
)

sns.relplot(
	data=el_nino_seasons,
	kind="line",
	x="season",
	y="snow"
).figure.set_size_inches(7, 4)
```

```{python}
#| echo: false

sns.lmplot(
	data=el_nino_seasons,
	x="season",
	y="snow"
).figure.set_size_inches(7, 4)
```

:::


:::{.exo}

:::{.yourturn}

Your turn:

:::

Now make the same regression plot with only the stations below 1100m to see whether climate change is affecting low elevations more strongly.

Does it look like they are?

```{python}
#| echo: false

elevations = sorted(subset_enso.get_column("elevation").unique().to_list())
```

```{python}
#| echo: false

low_stations = el_nino_seasons.filter(
    pl.col("elevation") <= 1100
)
```

<!-- ```{python} -->
<!-- sns.lmplot( -->
<!-- 	data=low_stations, -->
<!-- 	x="season", -->
<!-- 	y="snow" -->
<!-- ).figure.set_size_inches(8, 4) -->
<!-- ``` -->

:::

### Categorical data

Categorical data can be displayed thanks to [`season.catplot`](https://seaborn.pydata.org/generated/seaborn.catplot.html).

#### Categorical scatterplots

By default, `catplot` displays a cloud of markers for each category:

```{python}
sns.catplot(
	data=subset_enso,
	x="enso",
	y="snow",
	order=["La Niña", "Neutral", "El Niño"]
).figure.set_size_inches(8, 4)
```

#### Boxplots

[Boxplots](https://en.wikipedia.org/wiki/Box_plot) are a good way to display differences between groups. They can be draw in seaborn by passing `box` to the `kind` argument (the default is `strip`):

```{python}
sns.catplot(
	data=subset_enso,
	x="enso",
	y="snow",
    kind="box",
	order=["La Niña", "Neutral", "El Niño"]
).figure.set_size_inches(8, 4)
```

#### Violin plots

For violin plots, pass `violin` to `kind`:

```{python}
sns.catplot(
	data=subset_enso,
	x="enso",
	y="snow",
    kind="violin",
	order=["La Niña", "Neutral", "El Niño"]
).figure.set_size_inches(8, 4)
```

### Distributions

[`seaborn.displot`](https://seaborn.pydata.org/generated/seaborn.displot.html#seaborn.displot) allows to display distribution plots.

#### Histograms

By default `displot` draws histograms (the default for the `kind` argument is `hist` and it uses [`seaborn.histplot`](https://seaborn.pydata.org/generated/seaborn.histplot.html) under the hood):

```{python}
sns.displot(
    data=subset_enso,
    x="snow",
    hue="enso"
).figure.set_size_inches(8, 4)
```

We can see that there is more data in the left bins for El Niño and more in the right bins for La Niña but it is not the best plot to tell our story.

#### Kernel density estimation

[Kernel density estimation (KDE)](https://en.wikipedia.org/wiki/Kernel_density_estimation) applies kernel smoothing to the probability density estimation. You can plot them by passing `kde` to the `kind` argument (it uses [`seaborn.kdeplot`](https://seaborn.pydata.org/generated/seaborn.kdeplot.html) under the hood).

The value for `cut` sets how far the evaluation grid extends past the extreme data points. Its default is `3`. Setting it to `0` prevents it from drawing negative height of snow by going below zero:

```{python}
sns.displot(
    data=subset_enso,
    x="snow",
    hue="enso",
    kind="kde",
    cut=0
).figure.set_size_inches(8, 4)
```

This is even worse than the histogram to show tell our story. It is not a great plot with our data.

#### eCDF plots

[Empirical cumulative distribution functions (eCDF)](https://en.wikipedia.org/wiki/Empirical_distribution_function) display the fraction of observations that are smaller or equal to the values on the x axis.

This can be achieved by passing the value `ecdf` in the `kind` argument of the `displot` (which uses [`seaborn.ecdfplot`](https://seaborn.pydata.org/generated/seaborn.ecdfplot.html) under the hood):

```{python}
sns.displot(
    data=subset_enso,
    x="snow",
    hue="enso",
    kind="ecdf"
).figure.set_size_inches(8, 4)
```

This is a much better way to display this data as it clearly tells the story: we can see that La Niña have a higher snowpack than neutral years which are in turn better than El Niño years.
