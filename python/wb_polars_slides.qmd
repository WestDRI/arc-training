---
title: DataFrames on steroids with Polars
frontpic: img/logo_polars.png
frontpicwidth: 45%
frontpicmargintop: 40px
frontpicmarginbottom: 40px
noshadow: noshadow
author: Marie-Hélène Burle
date: 2024-05-14
date-format: long
execute:
  freeze: auto
  cache: true
  error: true
  echo: true
format:
  revealjs:
    embed-resources: true
    theme: [default, ../revealjspolars.scss]
    logo: /img/favicon_sfudrac.png
    highlight-style: ayu
    code-line-numbers: false
    template-partials:
      - ../title-slide.html
    pointer:
      color: "#b5111b"
      pointerSize: 32
    link-external-newwindow: true
    footer: <a href="wb_polars.html"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="rgb(153, 153, 153)" class="bi bi-arrow-90deg-up" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4.854 1.146a.5.5 0 0 0-.708 0l-4 4a.5.5 0 1 0 .708.708L4 2.707V12.5A2.5 2.5 0 0 0 6.5 15h8a.5.5 0 0 0 0-1h-8A1.5 1.5 0 0 1 5 12.5V2.707l3.146 3.147a.5.5 0 1 0 .708-.708l-4-4z"/></svg>&nbsp;Back to webinar page</a>
    auto-stretch: false
revealjs-plugins:
  - pointer
---

# Background

## Tabular data {.center}

Many fields of machine learning and data science rely on tabular data where

- columns hold variables and are homogeneous (same data type)
- rows contain observations and can be heterogeneous

Early computer options to manipulate such data were limited to [spreadsheets](https://en.wikipedia.org/wiki/Spreadsheet)

Dataframes (data frames or DataFrames) are two dimensional objects that brought tabular data to programming

## Early history of dataframes {.center}

```{dot}
//| echo: false
//| fig-height: 350px

strict graph {
  
bgcolor="transparent"
graph [fontname="Inconsolata, sans-serif"]
node [fontname="Inconsolata, sans-serif", fontsize=15]

y1 [label=1990, shape=plaintext, group=g1, group=g1]
y2 [label=2000, shape=plaintext, group=g1, group=g1]
y3 [label=2008, shape=plaintext, group=g1]

l1 [label="S programming language", href="https://en.wikipedia.org/wiki/S_(programming_language)", shape=plaintext, group=g2, fontcolor="#5592FD"]
l2 [label="R", href="https://en.wikipedia.org/wiki/R_(programming_language)", shape=plaintext, group=g2, fontcolor="#5592FD"]
l3 [label="Pandas (Python)", href="https://en.wikipedia.org/wiki/Pandas_(software)", shape=plaintext, group=g2, fontcolor="#5592FD"]

{rank=same; y1 l1}

y1 -- y2 -- y3
l1 -- l2 -- l3 [style=invis]

}
```

The world was simple ... but slow. Another problem: high memory usage \

## Issues with Pandas {.center}

[Wes McKinney](https://wesmckinney.com/) (pandas creator) himself [has complaints about it](https://wesmckinney.com/blog/apache-arrow-pandas-internals/):

>• Internals too far from “the metal” \
>• No support for memory-mapped datasets \
>• Poor performance in database and file ingest / export \
>• Warty missing data support \
>• Lack of transparency into memory use, RAM management \
>• Weak support for categorical data \
>• Complex groupby operations awkward and slow \
>• Appending data to a DataFrame tedious and very costly \
>• Limited, non-extensible type metadata \
>• Eager evaluation model, no query planning \
>• “Slow”, limited multicore algorithms for large datasets

# Improving performance

## Parallel computing {.center}

Python [global interpreter lock (GIL)](https://en.wikipedia.org/wiki/Global_interpreter_lock) gets in the way of multi-threading

Libraries such as [Ray](https://github.com/ray-project/ray), [Dask](https://github.com/dask/dask), and [Apache Spark](https://github.com/apache/spark) allow use of multiple cores and bring dataframes on clusters

Dask and Spark have APIs for Pandas and [Modin](https://docs.ray.io/en/latest/ray-more-libs/modin/index.html) makes this even more trivial by providing a drop-in replacement for Pandas on Dask, Spark, and Ray

[fugue](https://github.com/fugue-project/fugue/) provides a unified interface for distributed computing that works on Spark, Dask, and Ray

## Accelerators {.center}

[RAPIDS](https://rapids.ai/) brings dataframes on the GPUs with the [cuDF library](https://github.com/rapidsai/cudf)

Integration with pandas is easy

## Lazy out-of-core {.center}

[Vaex](https://github.com/vaexio/vaex) exists as an alternative to pandas (no integration)

## SQL {.center}

*[Structured query language (SQL)](https://en.wikipedia.org/wiki/SQL) handles [relational databases](https://en.wikipedia.org/wiki/Relational_database), but the distinction between SQL and dataframe software is getting increasingly blurry with most libraries now able to handle both*

[DuckDB](https://github.com/duckdb/duckdb) is a very fast and popular option with good integration with pandas

Many additional options such as [dbt](https://github.com/dbt-labs/dbt-core) and the [snowflake snowpark Python API](https://github.com/snowflakedb/snowpark-python) exist, although integration with pandas is not always as good

# Arrives Polars

## Comparison with Pandas {.center}

| | Pandas | Polars |
|--|--|--|
| Available for | Python | Rust, Python, R, NodeJS |
| Written in | Cython | Rust |
| Multithreading | Some operations | Yes (GIL released) |
| Index | Rows are indexed | Integer positions are used |
| Evaluation | Eager only | Lazy and eager |
| Query optimizer | No | Yes |
| Out-of-core | No | Yes |
| [SIMD](https://en.wikipedia.org/wiki/Single_instruction,_multiple_data) vectorization | Yes | Yes |
| Data in memory | With [NumPy](https://github.com/numpy/numpy) arrays | With [Apache Arrow](https://github.com/apache/arrow) arrays |
| Memory efficiency | Poor | Excellent |
| Handling of missing data | Inconsistent | Consistent, promotes type stability |

## Polars integration with other tools {.center}

*As good as Pandas' (except for cuDF, still in development)*

- *With NumPy:* see [the documentation](https://docs.pola.rs/user-guide/expressions/numpy/), the [from_numpy](https://docs.pola.rs/py-polars/html/reference/api/polars.from_numpy.html) and [to_numpy](https://docs.pola.rs/py-polars/html/reference/dataframe/api/polars.DataFrame.to_numpy.html) functions, [the development progress of this integration](https://github.com/pola-rs/polars/issues/14334), and [performance advice](https://kevinheavey.github.io/modern-polars/performance.html#numpy-can-make-polars-faster)

- *Parallel computing:* with [Ray](https://github.com/ray-project/ray/issues/26131) thanks to [this setting](https://docs.ray.io/en/latest/data/api/data_context.html#ray.data.DataContext); with Spark, Dask, and Ray thanks to [fugue](https://github.com/fugue-project/fugue/)

- *GPUs:* with the [cuDF library](https://github.com/rapidsai/cudf) from [RAPIDS](https://rapids.ai/) (in development)

- *SQL:* with [DuckDB](https://github.com/duckdb/duckdb)

The list is growing fast

## Benchmarks {.center}

Comparisons between Polars and distributed (Dask, Ray, Spark) or GPU (RAPIDS) libraries aren't the most pertinent since they can be used in *combination with* Polars and the benefits can be combined

It makes most sense to compare Polars with another library occupying the same "niche" such as Pandas or Vaex

## Benchmarks {.center}

The net is full of benchmarks with consistent results: Polars is 3 to 150 times faster than Pandas

Pandas is trying to fight back: v 2.0 came with optional Arrow support instead of NumPy, then [it became the default engine](https://dataalgo.medium.com/pandas-2-0-ditches-numpy-for-pyarrow-what-you-need-to-know-cbba4cb60249), but performance remains way below that of Polars (e.g. in [DataCamp benchmarks](https://www.datacamp.com/tutorial/high-performance-data-manipulation-in-python-pandas2-vs-polars), [official benchmarks](https://pola.rs/posts/benchmarks/), many blog posts for [whole scripts](https://medium.com/@asimandia/benchmarking-performance-polars-vs-vaex-vs-pandas-f1c889dccc12) or [individual tasks](https://medium.com/cuenex/pandas-2-0-vs-polars-the-ultimate-battle-a378eb75d6d1))

As for Vaex, [it seems twice slower](https://medium.com/@asimandia/benchmarking-performance-polars-vs-vaex-vs-pandas-f1c889dccc12) and [development has stalled over the past 10 months](https://github.com/vaexio/vaex)

The only framework performing better than Polars in some benchmarks is [datatable](https://github.com/h2oai/datatable) (derived from the R package [data.table](https://cran.r-project.org/web/packages/data.table/index.html)), but it hasn't been developed for 6 months—a sharp contrast with the fast development of Polars

# Getting started

## Installation {.center}

Personal computer:

```{.bash}
python -m venv ~/env                  # Create virtual env
source ~/env/bin/activate             # Activate virtual env
pip install --upgrade pip             # Update pip
pip install polars          		  # Install Polars
```
<br>
Alliance clusters (polars wheels are available, always prefer wheels when possible):

```{.bash}
python -m venv ~/env                  # Create virtual env
source ~/env/bin/activate             # Activate virtual env
pip install --upgrade pip --no-index  # Update pip from wheel
pip install polars --no-index		  # Install Polars from wheel
```

## Syntax {.center}

The package is [well documented](https://docs.pola.rs/)

[Kevin Heavey](https://github.com/kevinheavey) wrote [Modern Polars](https://kevinheavey.github.io/modern-polars/) following the model of the [Modern Pandas](http://tomaugspurger.net/posts/modern-8-scaling/) book. This is a great resource, although getting a little outdated for the scaling chapter since Polars is evolving so fast

Overall, the syntax feels somewhat similar to [R's dplyr](https://cran.r-project.org/web/packages/dplyr/index.html) from [the tidyverse](https://www.tidyverse.org/)

## Table visualization {.center}

While Pandas comes with [internal capabilities](https://pandas.pydata.org/docs/user_guide/style.html) to make publication ready tables, Polars [integrates very well](https://posit-dev.github.io/great-tables/blog/polars-styling/) with [great-tables](https://github.com/posit-dev/great-tables)

# The bottom line

## A rich new field {.center}

After years with *the one* Python option (Pandas), there is currently this exuberant explosion of faster alternatives for dataframes

It might seem confusing and overwhelming, but in fact, the picture seems quite simple

For now, the new memory standard seems to be Apache Arrow and the most efficient library making use of it is Polars

## Best performance strategy for software {.center}

The best strategy thus seems to be at the moment:

- *Single machine:* Polars

- *Cluster:* Polars + [fugue](https://github.com/fugue-project/fugue/) ([example benchmark](https://medium.com/fugue-project/benchmarking-pyspark-pandas-pandas-udfs-and-fugue-polars-198c3109a226), [documentation of integration](https://fugue-tutorials.readthedocs.io/tutorials/integrations/backends/polars.html))

- *GPUs available:* Polars + [RAPIDS](https://rapids.ai/) library [cuDF](https://github.com/rapidsai/cudf) ([integration coming soon](https://pola.rs/posts/polars-on-gpu/))

- *SQL:* Polars + [DuckDB](https://github.com/duckdb/duckdb) ([documentation of integration](https://duckdb.org/docs/guides/python/polars.html))

- Or combination of the above (if cluster with GPUs, etc.)

As so many libraries are developing an integration with Polars, it is becoming hard to still find reasons to use Pandas

## Performance tips {.center}

- Read [the migration guide](https://docs.pola.rs/user-guide/migration/pandas/#selecting-data): it will help you write Polars code rather than "literally translated" Pandas code that runs, but doesn't make use of Polars' strengths. The differences in style mostly come from the fact that Polars runs in parallel

- *Execution:* lazy where possible

- *File format:* [Apache Parquet](https://arrow.apache.org/docs/python/parquet.html)
