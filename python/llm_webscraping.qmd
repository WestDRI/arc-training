---
title: Webscraping with an LLM
author: Marie-Hélène Burle
---

:::{.def}

The internet is a trove of information. A lot of it is publicly available and thus suitable for use in research. Extracting that information and putting it in an organized format for analysis can however be extremely tedious.

Some websites have an API that makes it easy to extract information. These are websites that were built with the intention of being scraped (e.g. sites that contain databases, museums, art collections, etc.). When this is the case, this is definitely the way to go. Most websites however do not contain an API that can be queried.

Web scraping tools allow to automate parts of that process and Python is a popular language for the task.

Of note, an increasing number of websites use JavaScript to add cookies, interactivity, etc. to websites. This makes them a lot harder to scrape and require more sophisticated tools.

In this section, we will scrape a simple site that does not contain any JavaScript using the package [Beautiful Soup](https://beautiful-soup-4.readthedocs.io/en/latest/).

We will use an LLM to help us in this process.

:::

## Background information

### HTML and CSS

[HyperText Markup Language](https://en.wikipedia.org/wiki/HTML) (HTML) is the standard markup language for websites: it encodes the information related to the formatting and structure of webpages. Additionally, some of the customization can be stored in [Cascading Style Sheets](https://en.wikipedia.org/wiki/CSS) (CSS) files.

HTML uses tags of the form:

```{.html}
<some_tag>Your content</some_tag>
```

Some tags have attributes:

```{.html}
<some_tag attribute_name="attribute value">Your content</some_tag>
```

:::{.example}

Examples:

:::

Site structure:

- `<h2>This is a heading of level 2</h2>`
- `<p>This is a paragraph</p>`

Formatting:

- `<b>This is bold</b>`
- `<a href="https://some.url">This is the text for a link</a>`

### Web scrapping

Web scraping is a general term for a set of tools which allow for the extraction of data from the web automatically.

While most of the data on the internet is publicly available, it is illegal to scrape some sites and you should always look into the policy of a site before attempting to scrape it. Some sites will also block you if you submit too many requests in a short amount of time, so remember to scrape responsibly.

## Example for this workshop

We will use [a website](https://trace.tennessee.edu/utk_graddiss/index.html) from the [University of Tennessee](https://www.utk.edu/) containing a database of PhD theses from that university which uses the [Digital Commons Network](https://network.bepress.com/).

Our goal is to scrape data from this site to produce a dataframe with the date, major, and advisor for each dissertation. Scraping any other site which uses the [Digital Commons Network](https://network.bepress.com/) can be done following a similar workflow.

### Let's look at the site

First of all, let's have a close look at the website we want to scrape to think carefully about what we want to do. Before starting to write code, it is always a good idea to think about what you are trying to achieve with your code.

To see how the website is built, you can open the web inspector. Most browsers allow to right-click on an element and select "inspect" in the menu. Alternatively, Chrome opens the web inspector with F12 and Firefox and Edge with Ctrl+Shift+I (Windows, Linux) or Command+Option+I (macOS).

To create a dataframe with the data for all the dissertations on that first page, we need to do two things:

- Step 1: from the [dissertations database first page](https://trace.tennessee.edu/utk_graddiss/index.html), we want to scrape the list of URLs for the dissertation pages.

- Step 2: once we have the URLs, we want to scrape those pages too to get the date, major, and advisor for each dissertation.

### Let's try to scrape the data

:::{.llm}

Write Python code to scrape the following website: https://trace.tennessee.edu/utk_graddiss/index.html.

As the end result, I want to have a polars dataframe with the following columns:

- Date
- Major of the thesis
- Advisor (PI)

:::

```{python}
import requests
from bs4 import BeautifulSoup
import polars as pl
from urllib.parse import urljoin

BASE_URL = "https://trace.tennessee.edu/utk_graddiss/index.html"

def get_dissertation_data():
	# Get main page and extract dissertation URLs
	main_page = requests.get(BASE_URL)
	soup = BeautifulSoup(main_page.content, "html.parser")

	# Extract individual dissertation links
	links = [urljoin(BASE_URL, a["href"])
			 for a in soup.select(".article-listing a")]

	data = []

	for link in links[:10]:  # Reduced to 10 for testing; remove slice for full 100
		try:
			# Get individual dissertation page
			page = requests.get(link)
			page_soup = BeautifulSoup(page.content, "html.parser")

			# Extract required fields
			date = page_soup.select_one("#publication_date p").text.strip()
			major = page_soup.select_one("#department p").text.strip()
			advisor = page_soup.select_one("#advisor1 p").text.strip()

			data.append({
				"Date": date,
				"Major": major,
				"Advisor": advisor
			})

		except Exception as e:
			print(f"Error processing {link}: {str(e)}")

	return pl.DataFrame(data)

# Usage
df = get_dissertation_data()
print(df)
```

:::{.exo}

:::{.yourturn}

Your turn:

:::

- Did it work? Go to the site and verify the data.

- What is the problem?

:::

:::{.llm}

Write Python code to scrape the following website: https://scholarworks.utrgv.edu/do/discipline_browser/articles?discipline_key=438.

As the end result, I want to have a polars dataframe with the following columns:

- The defense date
- The department
- Both committee members

:::
