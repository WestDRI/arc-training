---
title: Why JAX?
author: Marie-Hélène Burle
---

:::{.def}

There are many excellent and popular deep learning frameworks already (e.g. [PyTorch](https://pytorch.org/)), so why did Google—already behind the successful [TensorFlow](https://www.tensorflow.org/) project—start developing [JAX](https://jax.readthedocs.io/en/latest/index.html)?

In this section, we will look at the advantages brought by JAX, namely speed and extremely flexible automatic differentiation.

:::

## A relatively new project

It is clear that JAX is not a widely adopted project yet.

### Trends of Google searches

![As of October 16, 2023.](img/googletrends_jax_tf_pt.png)

<br>

### Trends of Stack Overflow tags

![As of October 16, 2023.](img/sotrends_jax_tf_pt.png)

<br>

So why learn JAX?

## JAX is faster

JAX traces pure Python functions into an intermediate language that can be transformed and [JIT compiled](https://en.wikipedia.org/wiki/Just-in-time_compilation) into a high-level optimized code which is passed to the [XLA compiler](https://github.com/openxla/xla) for optimization across the available hardware ([CPUs](https://en.wikipedia.org/wiki/Central_processing_unit), [GPUs](https://en.wikipedia.org/wiki/Graphics_processing_unit), or [TPUs](https://en.wikipedia.org/wiki/Tensor_Processing_Unit)).

This series of internal steps—which starts with the constraint that JAX only works reliably with pure Python functions (functions with no side effects)—makes JAX extremely fast. Training models with JAX can be substantially faster than with TensorFlow or PyTorch, although the size of the speedup depends on the size of the model, the type of GPUs, and other factors.

## More flexible differentiation

[Automatic differentiation (autodiff or AD)](https://en.wikipedia.org/wiki/Automatic_differentiation) is the evaluation by computer programs of the partial derivatives of functions. It is a key part of deep learning since training a model consists of updating its weights and biases to decrease some loss function thanks to various gradient-based optimizations.

Several implementations have been developed by different teams over time. [This post](https://www.stochasticlifestyle.com/engineering-trade-offs-in-automatic-differentiation-from-tensorflow-and-pytorch-to-jax-and-julia/) by [Chris Rackauckas](https://chrisrackauckas.com/) summarizes the trade-offs of the various strategies.

Removing [Julia](https://julialang.org/) (which is very interesting: Julia has a lot to offer in the field of AD) and PyTorch stale attempt at JIT compilation, Chris Rackauckas' post can be summarized this way:

```{dot}
//| echo: false
//| fig-height: 450px

strict digraph {
  
bgcolor="transparent"
graph [fontname="Inconsolata, sans-serif"]
node [fontname="Inconsolata, sans-serif", fontsize=15]

01 [label="Autodiff method", shape=underline, group=g1, group=g1, fontcolor=gray55, color=gray55]
1 [label="Static graph\nand XLA", shape=plaintext, group=g1, group=g1]
2 [label="Dynamic graph", shape=plaintext, group=g1]
4 [label="Dynamic graph\nand XLA", shape=plaintext, group=g1]
5 [label="Non-standard\ninterpretation\nand XLA", shape=plaintext, group=g1]

02 [label="Framework", shape=plaintext, group=g2, shape=underline, fontcolor=gray55, color=gray55]
a [label="TensorFlow", shape=plaintext, group=g2, shape=oval, color=darkorange4, fontcolor=darkorange4]
b [label="PyTorch", shape=plaintext, group=g2, shape=oval, color=chocolate, fontcolor=chocolate]
d [label="TensorFlow2", shape=plaintext, group=g2, shape=oval, color=darkorange4, fontcolor=darkorange4]
e [label="JAX", shape=plaintext, group=g2, shape=oval, color=deepskyblue3, fontcolor=deepskyblue3]

03 [label=Advantage, shape=underline, group=g3, fontcolor=gray55, color=gray55]
7 [label="Optimized\nautodiff", shape=plaintext, fontcolor=darkolivegreen, group=g3]
8 [label="Convenient", shape=plaintext, fontcolor=darkolivegreen, group=g3]
9 [label="Convenient", shape=plaintext, fontcolor=darkolivegreen, group=g3]
10 [label="Flexible and\noptimized\nautodiff", shape=plaintext, fontcolor=darkolivegreen, group=g3]

04 [label=Disadvantage, shape=underline, group=g4, fontcolor=gray55, color=gray55]
A [label="Inconvenient", shape=plaintext, fontcolor=darkorchid2, group=g4]
B [label="Can't optimize\nautodiff", shape=plaintext, fontcolor=darkorchid2, group=g4]
D [label="Dynamism doesn't\nplay well\nwith XLA", shape=plaintext, fontcolor=darkorchid2, group=g4]
E [label="Requires an\nintermediate\nrepresentation", shape=plaintext, fontcolor=darkorchid2, group=g4]

{rank=same; 01 02 03 04}
{rank=same; 1 a 7 A}
{rank=same; 2 b 8 B}
{rank=same; 4 d 9 D}
{rank=same; 5 e 10 E}

01 -> 02 -> 03 -> 04 [style=invis]
1 -> a -> 7 -> A [style=invis]
2 -> b -> 8 -> B [style=invis]
4 -> d -> 9 -> D [style=invis]
5 -> e -> 10 -> E [style=invis]

01 -> 1 [style=invis]
1 -> 2 -> 4 -> 5 [color=gray55]
02 -> a -> b -> d -> e [style=invis]
03 -> 7 -> 8 -> 9 -> 10 [style=invis]
04 -> A -> B -> D -> E [style=invis]

}
```

<br>

TensorFlow initial approach with computational graphs in a domain-specific language was unintuitive, inconvenient, and hard to debug. PyTorch came with dynamic graphs—an approach so much more convenient that it marked the beginning of the decline of TensorFlow. However xxxx. TensorFlow2 tried xxxx, but xxx.

This leaves room for new strategies. Julia offers several promising approaches, but implementations are not straightforward and projects are not always mature. It is an exciting avenue for developers, not necessarily an easy one for end users. [JAX](https://jax.readthedocs.io/en/latest/index.html) is another attempt at bringing both optimization and flexibility to autodiff. With Google behind it, it is a new but fast growing project.



<!-- http://sotagtrends.com/?tags=jax+pytorch+tensorflow&relative=true -->
<!-- https://flax.readthedocs.io/en/latest/philosophy.html -->
<!-- https://www.it-jim.com/blog/jax-can-it-beat-pytorch-and-tensorflow/ -->
<!-- https://softwareengineering.stackexchange.com/questions/40454/what-is-a-closure -->
<!-- https://stackoverflow.com/questions/36636/what-is-a-closure -->
<!-- https://stackoverflow.com/questions/220658/what-is-the-difference-between-a-closure-and-a-lambda/36878651#36878651 -->


<!-- NumPy on accelerators (GPUs/TPUs) -->
<!-- very flexible autodiff, but with additional acceleration thanks to the domain-specific XLA compiler -->
<!-- fuse functions so that intermediate results don't have to be written in memory -->
