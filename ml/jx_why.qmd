---
title: Why JAX?
author: Marie-Hélène Burle
---

:::{.def}



:::

Many excellent and popular deep learning frameworks already.

Moreover, JAX is new and not widely adopted yet.

<script type="text/javascript" src="https://ssl.gstatic.com/trends_nrtr/3461_RC01/embed_loader.js"></script> <script type="text/javascript"> trends.embed.renderExploreWidget("TIMESERIES", {"comparisonItem":[{"keyword":"/g/11bwp1s2k3","geo":"","time":"2009-12-31 2023-10-11"},{"keyword":"/g/11gd3905v1","geo":"","time":"2009-12-31 2023-10-11"},{"keyword":"/g/11t6my1_gw","geo":"","time":"2009-12-31 2023-10-11"}],"category":0,"property":""}, {"exploreQuery":"date=2009-12-31%202023-10-11&q=%2Fg%2F11bwp1s2k3,%2Fg%2F11gd3905v1,%2Fg%2F11t6my1_gw&hl=en-CA","guestPath":"https://trends.google.com:443/trends/embed/"}); </script>

![](img/googletrends_jax_tf_pt.png)

So why bother learning it?

TensorFlow on the decline, supplanted by PyTorch due to an inconvenient autodiff system. PyTorch is great but struggles with optimization during the Autodiff Process.

[There is a great post](https://www.stochasticlifestyle.com/engineering-trade-offs-in-automatic-differentiation-from-tensorflow-and-pytorch-to-jax-and-julia/) by [Chris Rackauckas](https://chrisrackauckas.com/) on the trade-offs of various autodiff strategies over time. Here is a simplified summary (removing [Julia](https://julialang.org/)'s autodiff strategies and PyTorch's attempt at JIT compilation):

```{dot}
//| echo: false
//| fig-height: 450px

strict digraph {
  
bgcolor="transparent"
graph [fontname="Inconsolata, sans-serif"]
node [fontname="Inconsolata, sans-serif", fontsize=15]

01 [label="Autodiff method", shape=underline, group=g1, group=g1, fontcolor=gray55, color=gray55]
1 [label="Static graph\nand XLA", shape=plaintext, group=g1, group=g1]
2 [label="Dynamic graph", shape=plaintext, group=g1]
4 [label="Dynamic graph\nand XLA", shape=plaintext, group=g1]
5 [label="Non-standard\ninterpretation\nand XLA", shape=plaintext, group=g1]

02 [label="Framework", shape=plaintext, group=g2, shape=underline, fontcolor=gray55, color=gray55]
a [label="TensorFlow", shape=plaintext, group=g2, shape=box]
b [label="PyTorch", shape=plaintext, group=g2, shape=box]
d [label="TensorFlow2", shape=plaintext, group=g2, shape=box]
e [label="jax", shape=plaintext, group=g2, shape=box]

03 [label=Advantage, shape=underline, group=g3, fontcolor=gray55, color=gray55]
7 [label="Optimized\nautodiff", shape=plaintext, fontcolor=darkolivegreen, group=g3]
8 [label="Convenient", shape=plaintext, fontcolor=darkolivegreen, group=g3]
9 [label="Convenient", shape=plaintext, fontcolor=darkolivegreen, group=g3]
10 [label="Flexible and\noptimized\nautodiff", shape=plaintext, fontcolor=darkolivegreen, group=g3]

04 [label=Disadvantage, shape=underline, group=g4, fontcolor=gray55, color=gray55]
A [label="Inconvenient", shape=plaintext, fontcolor=darkorchid2, group=g4]
B [label="Can't optimize\nautodiff", shape=plaintext, fontcolor=darkorchid2, group=g4]
D [label="Dynamism doesn't\nplay well\nwith XLA", shape=plaintext, fontcolor=darkorchid2, group=g4]
E [label="Only works on\npure functions", shape=plaintext, fontcolor=darkorchid2, group=g4]

{rank=same; 01 02 03 04}
{rank=same; 1 a 7 A}
{rank=same; 2 b 8 B}
{rank=same; 4 d 9 D}
{rank=same; 5 e 10 E}

01 -> 02 -> 03 -> 04 [style=invis]
1 -> a -> 7 -> A [style=invis]
2 -> b -> 8 -> B [style=invis]
4 -> d -> 9 -> D [style=invis]
5 -> e -> 10 -> E [style=invis]

01 -> 1 [style=invis]
1 -> 2 -> 4 -> 5 [color=gray55]
02 -> a -> b -> d -> e [style=invis]
03 -> 7 -> 8 -> 9 -> 10 [style=invis]
04 -> A -> B -> D -> E [style=invis]

}
```

http://sotagtrends.com/?tags=jax+pytorch+tensorflow&relative=true
https://flax.readthedocs.io/en/latest/philosophy.html
https://www.it-jim.com/blog/jax-can-it-beat-pytorch-and-tensorflow/
https://softwareengineering.stackexchange.com/questions/40454/what-is-a-closure
https://stackoverflow.com/questions/36636/what-is-a-closure
https://stackoverflow.com/questions/220658/what-is-the-difference-between-a-closure-and-a-lambda/36878651#36878651


NumPy on accelerators (GPUs/TPUs)
very flexible autodiff, but with additional acceleration thanks to the domain-specific XLA compiler
fuse functions so that intermediate results don't have to be written in memory
