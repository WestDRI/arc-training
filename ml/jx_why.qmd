---
title: Why JAX?
author: Marie-Hélène Burle
---

:::{.def}

There are many excellent and popular deep learning frameworks already (e.g. [PyTorch](https://pytorch.org/)). So why did Google—already behind the successful [TensorFlow](https://www.tensorflow.org/) project—start developing [JAX](https://jax.readthedocs.io/en/latest/index.html)?

In this section, we will look at the advantages brought by JAX, namely speed and flexible automatic differentiation.

:::

## A relatively new project

It is clear that JAX is not a widely adopted project yet.

### Trends of Google searches

![As of October 16, 2023.](img/googletrends_jax_tf_pt.png)

<br>

### Trends of Stack Overflow tags

![As of October 16, 2023.](img/sotrends_jax_tf_pt.png)

<br>

So why JAX?

## JAX is fast

JAX was built with performance in mind. Its speed relies on design decisions at all levels.

- **Default data type**

:::{.ind}

Like [PyTorch](https://pytorch.org/)—a popular deep learning library—JAX uses float32 as its default data type. This level of precision is perfectly suitable for deep learning and increases efficiency (by contrast, [NumPy](https://numpy.org/) defaults to float64).

:::

- **[JIT compilation](https://en.wikipedia.org/wiki/Just-in-time_compilation)**

JIT compilation combines computations, avoids the allocation of memory to temporary objects, and more generally optimizes code for the XLA.

- **Accelerators**

:::{.ind}

The same code can run on [CPUs](https://en.wikipedia.org/wiki/Central_processing_unit) or on accelerators ([GPUs](https://en.wikipedia.org/wiki/Graphics_processing_unit) and [TPUs](https://en.wikipedia.org/wiki/Tensor_Processing_Unit)).

:::

- **XLA optimization**

:::{.ind}

[XLA (Accelerated Linear Algebra)](https://github.com/openxla/xla) is a domain-specific compiler for linear algebra that takes JIT-compiled JAX programs and optimizes them for the available hardware (CPUs, GPUs, or TPUs). 

:::

- **Asynchronous dispatch**

:::{.ind}

Computations are executed on the accelerators [asynchronously](https://en.wikipedia.org/wiki/Asynchrony_(computer_programming)).

:::

- **Vectorization, data parallelism, and sharding**

:::{.ind}

All levels of shared and distributed memory parallelism are supported in JAX.

:::

## Flexible differentiation

[Automatic differentiation (autodiff or AD)](https://en.wikipedia.org/wiki/Automatic_differentiation) is the evaluation by computer programs of the partial derivatives of functions. It is a key part of deep learning since training a model consists of updating its weights and biases to decrease some loss function thanks to various gradient-based optimizations.

Several implementations have been developed by different teams over time. [This post](https://www.stochasticlifestyle.com/engineering-trade-offs-in-automatic-differentiation-from-tensorflow-and-pytorch-to-jax-and-julia/) by [Chris Rackauckas](https://chrisrackauckas.com/) summarizes the trade-offs of the various strategies.

Removing [Julia](https://julialang.org/) (which is very interesting: Julia has a lot to offer in the field of AD) and PyTorch stale attempt at JIT compilation, Chris Rackauckas' post can be summarized this way:

```{dot}
//| echo: false
//| fig-height: 450px

strict digraph {
  
bgcolor="transparent"
graph [fontname="Inconsolata, sans-serif"]
node [fontname="Inconsolata, sans-serif", fontsize=15]

01 [label="Autodiff method", shape=underline, group=g1, group=g1, fontcolor=gray55, color=gray55]
1 [label="Static graph\nand XLA", shape=plaintext, group=g1, group=g1]
2 [label="Dynamic graph", shape=plaintext, group=g1]
4 [label="Dynamic graph\nand XLA", shape=plaintext, group=g1]
5 [label="Non-standard\ninterpretation\nand XLA", shape=plaintext, group=g1]

02 [label="Framework", shape=plaintext, group=g2, shape=underline, fontcolor=gray55, color=gray55]
a [label="TensorFlow", shape=plaintext, group=g2, shape=oval, color=darkorange4, fontcolor=darkorange4]
b [label="PyTorch", shape=plaintext, group=g2, shape=oval, color=chocolate, fontcolor=chocolate]
d [label="TensorFlow2", shape=plaintext, group=g2, shape=oval, color=darkorange4, fontcolor=darkorange4]
e [label="JAX", shape=plaintext, group=g2, shape=oval, color=deepskyblue3, fontcolor=deepskyblue3]

03 [label=Advantage, shape=underline, group=g3, fontcolor=gray55, color=gray55]
7 [label="Optimized\nautodiff", shape=plaintext, fontcolor=darkolivegreen, group=g3]
8 [label="Convenient", shape=plaintext, fontcolor=darkolivegreen, group=g3]
9 [label="Convenient", shape=plaintext, fontcolor=darkolivegreen, group=g3]
10 [label="Flexible and\noptimized\nautodiff", shape=plaintext, fontcolor=darkolivegreen, group=g3]

04 [label=Disadvantage, shape=underline, group=g4, fontcolor=gray55, color=gray55]
A [label="Inconvenient", shape=plaintext, fontcolor=darkorchid2, group=g4]
B [label="Can't optimize\nautodiff", shape=plaintext, fontcolor=darkorchid2, group=g4]
D [label="Dynamism doesn't\nplay well\nwith XLA", shape=plaintext, fontcolor=darkorchid2, group=g4]
E [label="Requires an\nintermediate\nrepresentation", shape=plaintext, fontcolor=darkorchid2, group=g4]

{rank=same; 01 02 03 04}
{rank=same; 1 a 7 A}
{rank=same; 2 b 8 B}
{rank=same; 4 d 9 D}
{rank=same; 5 e 10 E}

01 -> 02 -> 03 -> 04 [style=invis]
1 -> a -> 7 -> A [style=invis]
2 -> b -> 8 -> B [style=invis]
4 -> d -> 9 -> D [style=invis]
5 -> e -> 10 -> E [style=invis]

01 -> 1 [style=invis]
1 -> 2 -> 4 -> 5 [color=gray55]
02 -> a -> b -> d -> e [style=invis]
03 -> 7 -> 8 -> 9 -> 10 [style=invis]
04 -> A -> B -> D -> E [style=invis]

}
```

<br>

TensorFlow initial approach with computational graphs in a domain-specific language was unintuitive, inconvenient, and hard to debug. PyTorch came with dynamic graphs—an approach so much more convenient that it marked the beginning of the decline of TensorFlow. However xxxx. TensorFlow2 tried xxxx, but xxx.

This leaves room for new strategies. Julia offers several promising approaches, but implementations are not straightforward and projects are not always mature. It is an exciting avenue for developers, not necessarily an easy one for end users. [JAX](https://jax.readthedocs.io/en/latest/index.html) is another attempt at bringing both optimization and flexibility to autodiff. With Google behind it, it is a new but fast growing project.
