---
title: The PyTorch API
aliases:
  - pytorch.html
author: Marie-Hélène Burle
---

:::{.def}

[PyTorch](https://pytorch.org/) is a free and open-source machine learning and scientific computing framework based on [Torch](http://torch.ch/). While Torch uses a scripting language based on [Lua](https://en.wikipedia.org/wiki/Lua_(programming_language)), PyTorch has a [Python](https://www.python.org/) and a [C++](https://isocpp.org/) interface.

Created by [Meta AI](https://en.wikipedia.org/wiki/Meta_AI) (formerly Facebook, Inc.) in 2017, it is now a project of [The Linux Foundation](https://en.wikipedia.org/wiki/Linux_Foundation).

PyTorch is widely used in academia and research. Part of its popularity stems from the fact that the Python interface is truly pythonic in nature, making it easier to learn than other popular frameworks such as [TensorFlow](https://www.tensorflow.org/).

The PyTorch API is vast and complex. This section links to the key components to get you started.

:::

## Domain-specific libraries

PyTorch is a large framework with domain-specific libraries:

- [TorchVision](https://pytorch.org/vision/stable/index.html) for computer vision,
- [TorchText](https://pytorch.org/text/stable/index.html) for natural languages,
- [TorchAudio](https://pytorch.org/audio/stable/index.html) for audio and signal processing.

These libraries contain standard datasets and utilities specific to the data in those fields.

## Loading data

[torch.utils.data](https://pytorch.org/docs/stable/data.html) contains everything you need create data loaders (iterables that present the data to a model).

## Building models

[torch.nn](https://pytorch.org/docs/stable/nn.html) contains the elements you need to build your model architecture and chose a [loss function](https://pytorch.org/docs/stable/nn.html#loss-functions).

## Training

Training a model consists of optimizing the model parameters.

[torch.autograd](https://pytorch.org/docs/stable/autograd.html) contains the tools for automatic differentiation (to compute the gradients, that is the tensors containing the partial derivatives of the error with respect to the parameters of the functions in the model) and [torch.optim](https://pytorch.org/docs/stable/optim.html) contains optimization algorithms that can be used for gradient descent.
