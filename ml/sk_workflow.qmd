---
title: Sklearn workflow
author: Marie-Hélène Burle
---

:::{.def}

Scikit-learn has a very clean and consistent API, making it very easy to use: a similar workflow can be applied to most techniques.

*This code was modified from [Matthew Greenberg](https://science.ucalgary.ca/mathematics-statistics/contacts/matthew-greenberg).*

:::

## Load packages

```{python}
from sklearn.datasets import fetch_california_housing, load_breast_cancer
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, accuracy_score

import pandas as pd

import matplotlib
from matplotlib import pyplot as plt

import numpy as np

from collections import Counter
```

## Load and explore the data

```{python}
cal_housing = fetch_california_housing()
type(cal_housing)
```

Let's look at the attributes of `cal_housing`:

```{python}
dir(cal_housing)
```

```{python}
cal_housing.feature_names
```

```{python}
print(cal_housing.DESCR)
```

```{python}
X = cal_housing.data
y = cal_housing.target
X.shape, y.shape
```

While not at all necessary, we can turn this bunch object into a more familiar data frame to explore the data further:

```{python}
cal_housing_df = pd.DataFrame(cal_housing.data, columns=cal_housing.feature_names)
```

```{python}
cal_housing_df.head()
```

```{python}
cal_housing_df.tail()
```

```{python}
cal_housing_df.info()
```

```{python}
cal_housing_df.describe() 
```

We can even plot it:

```{python}
plt.hist(y)
```

```{python}
b_cancer = load_breast_cancer()
b_cancer.feature_names
```

```{python}
b_cancer.target_names
```

```{python}
X = b_cancer.data
y = b_cancer.target
X.shape, y.shape
```

```{python}
X, y = fetch_california_housing(return_X_y=True)
X = X[:, :-2]
X.shape, y.shape
```

## Create a model and fit it

```{python}
model = LinearRegression().fit(X, y)
```

:::{.note}

This is equivalent to:

```{.python}
model = LinearRegression()
model.fit(x, y)
```

First, we create an instance of the class `LinearRegression`, then we call `.fit()` on it to fit the model.

:::

```{python}
model.coef_
```

:::{.note}

Trailing underscores indicate that an attribute is estimated. `.coef_` here is an estimated value.

:::

```{python}
model.coef_.shape
```

```{python}
model.intercept_
```

We can now get our predictions:

```{python}
y_hat = model.predict(X)
```

And calculate some measures of error:

- Sum of squared errors

```{python}
np.sum((y - y_hat)**2)           # 
```

- Mean squared error

```{python}
mean_squared_error(y, y_hat)
```

:::{.note}

MSE could also be calculated with `np.mean((y - y_hat)**2)`.

:::

```{python}
mean_absolute_percentage_error(y, y_hat)
```

Index of minimum value:

```{python}
model.coef_.argmin()
```

Index of maximum value:

```{python}
model.coef_.argmax()
```

```{python}
cal_housing = fetch_california_housing()
cal_housing.feature_names[4]
```

```{python}
print(cal_housing.DESCR)
```

```{python}
XX = np.concatenate([np.ones((len(X), 1)), X], axis=1)

beta = np.linalg.lstsq(XX, y, rcond=None)[0]
intercept_, *coef_ = beta

intercept_, model.intercept_
```

```{python}
np.allclose(coef_, model.coef_)
```

:::{.note}

This means that the two arrays are equal element-wise, within a tolerance.

:::

```{python}
X_test = np.random.normal(size=(10, X.shape[1]))
X_test.shape
```

```{python}
y_test = X_test @ coef_ + intercept_
y_test
```

```{python}
model.predict(X_test)
```

```{python}
model.fit(X, y).predict(X_test)
```

```{python}
model = RandomForestRegressor()
model.fit(X, y).predict(X_test)
```

```{python}
X, y = load_breast_cancer(return_X_y=True)
X.shape, y.shape, set(y)
```

```{python}
Counter(y)
```

```{python}
model = LogisticRegression(max_iter=10000)
y_hat = model.fit(X, y).predict(X)
np.mean(y_hat == y)
```

```{python}
accuracy_score(y, y_hat)
```

```{python}
def sigmoid(x):
  return 1/(1 + np.exp(-x))

x = np.linspace(-10, 10, 100)
plt.plot(x, sigmoid(x), lw=3)
plt.title("The Sigmoid Function $\\sigma(x)$")
```

```{python}
y_pred = 1*(sigmoid(X @ model.coef_.squeeze() + model.intercept_) > 0.5)
assert np.all(y_pred == model.predict(X))

np.allclose(
    model.predict_proba(X)[:, 1],
    sigmoid(X @ model.coef_.squeeze() + model.intercept_)
)
```

```{python}
def make_spirals(k=20, s=1.0, n=2000):
    X = np.zeros((n, 2))
    y = np.round(np.random.uniform(size=n)).astype(int)
    r = np.random.uniform(size=n)*k*np.pi
    rr = r**0.5
    theta = rr + np.random.normal(loc=0, scale=s, size=n)
    theta[y == 1] = theta[y == 1] + np.pi
    X[:,0] = rr*np.cos(theta)
    X[:,1] = rr*np.sin(theta)
    return X, y

X, y = make_spirals()
cmap = matplotlib.colormaps["viridis"]

a = cmap(0)
a = [*a[:3], 0.3]
b = cmap(0.99)
b = [*b[:3], 0.3]

plt.figure(figsize=(7,7))
ax = plt.gca()
ax.set_aspect("equal")
ax.plot(X[y == 0, 0], X[y == 0, 1], 'o', color=a, ms=8, label="$y=0$")
ax.plot(X[y == 1, 0], X[y == 1, 1], 'o', color=b, ms=8, label="$y=1$")
plt.title("Spirals")
plt.legend()
```

```{python}
model = LogisticRegression()
y_hat = model.fit(X, y).predict(X)
accuracy_score(y, y_hat)
```

```{python}
u = np.linspace(-8, 8, 100)
v = np.linspace(-8, 8, 100)
U, V = np.meshgrid(u, v)
UV = np.array([U.ravel(), V.ravel()]).T
U.shape, V.shape, UV.shape
```

:::{.note}

`np.ravel` returns a contiguous flattened array.

:::

```{python}
W = model.predict(UV).reshape(U.shape)
W.shape
```

```{python}
plt.pcolormesh(U, V, W)
```

```{python}
model = KNeighborsClassifier(n_neighbors=5)
y_hat = model.fit(X, y).predict(X)
accuracy_score(y, y_hat)
```

```{python}
u = np.linspace(-8, 8, 100)
v = np.linspace(-8, 8, 100)
U, V = np.meshgrid(u, v)
UV = np.array([U.ravel(), V.ravel()]).T
U.shape, V.shape, UV.shape
```

```{python}
W = model.predict(UV).reshape(U.shape)
W.shape
```

```{python}
plt.pcolormesh(U, V, W)
```

```{python}
fig, axes = plt.subplots(2, 4, figsize=(9.8, 5))
fig.suptitle("Decision Regions")

u = np.linspace(-8, 8, 100)
v = np.linspace(-8, 8, 100)
U, V = np.meshgrid(u, v)
UV = np.array([U.ravel(), V.ravel()]).T

ks = np.arange(1, 16, 2)

for k, ax in zip(ks, axes.ravel()):
  model = KNeighborsClassifier(n_neighbors=k)
  model.fit(X, y)
  acc = accuracy_score(y, model.predict(X))
  W = model.predict(UV).reshape(U.shape)
  ax.imshow(W, origin="lower", cmap=cmap)
  ax.set_axis_off()
  ax.set_title(f"$k$={k}, acc={acc:.2f}")
```
