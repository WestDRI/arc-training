---
title: Deep learning with JAX
description: A deep learning course with &nbsp;[![](img/JAX_logo.svg.png){width="4%" fig-alt="noshadow"}](https://jax.readthedocs.io/)
title-block-banner: true
---

:::{.topdef}

Faster than [TensorFlow](https://www.tensorflow.org/) and [PyTorch](https://pytorch.org/), [JAX](https://jax.readthedocs.io/) is the new open source Google deep learning framework. Based on [Python](https://www.python.org/) and [NumPy](https://numpy.org/), it uses the [XLA](https://www.tensorflow.org/xla) compiler for linear algebra and [just-in-time compilation](https://en.wikipedia.org/wiki/Just-in-time_compilation). It runs on accelerators ([GPUs](https://en.wikipedia.org/wiki/Graphics_processing_unit) and [TPUs](https://en.wikipedia.org/wiki/Tensor_Processing_Unit)) and provides a powerful and flexible automatic differentiation engine.

This course will introduce the basic concepts of deep learning and will get you started building and training neural networks with a modern toolkit:

- [PyTorch](https://pytorch.org/), [TensorFlow](https://www.tensorflow.org/), and [Hugging Face](https://huggingface.co/) to load datasets
- [Flax](https://flax.readthedocs.io/en/latest/index.html) (an open source Python neural network library built on top of [JAX](https://jax.readthedocs.io/)) to build neural networks
- [JAX](https://jax.readthedocs.io/) and [Flax](https://flax.readthedocs.io/en/latest/index.html) to calculate gradients and optimize the networks parameters
- [JAX](https://jax.readthedocs.io/) and [TensorBoard](https://www.tensorflow.org/tensorboard/) for profiling

:::

<br>
[[Start course âž¤](jx_why.qmd)]{.topinline}
