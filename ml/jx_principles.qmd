---
title: JAX principles
author: Marie-Hélène Burle
---


```{python}
import jax.numpy as jnp
from jax import grad, vmap, pmap, jit
```

Composable transformations:

- `grad` transform: turn a loss function (e.g. `mse_loss`) and turn it into a function that computes the gradients
- `jit` transform: just-in-time compilation
- `vmap` transform: work in a **v**ectorized fashion across elements of a batch
- `pmap` transform: work in **p**arallel across processing units

Python → tracer values → Intermediate Representation (IR) → autodiff


```{dot}
//| echo: false
//| fig-height: 800px

strict digraph {

bgcolor="transparent"
node [fontname="Inconsolata, sans-serif"]

"tracer\nvalues" [shape=rectangle]
"jit\ncompilation" [shape=rectangle]
"Accelerated\nLinear Algebra\n(XLA)" [shape=rectangle]
"transforms" [shape=rectangle]

CPU [shape=octagon]
GPU [shape=octagon]
TPU [shape=octagon]

"Python code\nwith only pure\nfunctions"-> "tracer\nvalues" [dir=none]
"tracer\nvalues" -> "Intermediate\nRepresentation\n(IR)"
"Intermediate\nRepresentation\n(IR)" -> "jit\ncompilation" [dir=none]
"jit\ncompilation" -> "High Level\nOptimized code\n(HLO)"
"High Level\nOptimized code\n(HLO)" -> "Accelerated\nLinear Algebra\n(XLA)" [dir=none]

"Accelerated\nLinear Algebra\n(XLA)" -> CPU [shape=doubleoctagon]
"Accelerated\nLinear Algebra\n(XLA)" -> GPU
"Accelerated\nLinear Algebra\n(XLA)" -> TPU

"Intermediate\nRepresentation\n(IR)" -> "transforms" [dir=both, minlen=3]

{rank=same; "Intermediate\nRepresentation\n(IR)" "transforms"}

}
```


