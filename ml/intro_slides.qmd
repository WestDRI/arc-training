---
title: Introduction to machine learning
frontlogo: /img/sfudrac.png
author: Marie-Hélène Burle
execute:
  error: true
  echo: true
format:
  revealjs:
    embed-resources: true
    theme: [default, ../revealjs.scss]
    logo: /img/sfudrac_logo.png
    highlight-style: monokai
    code-line-numbers: false
    template-partials:
      - ../title-slide.html
    pointer:
      color: "#b5111b"
      pointerSize: 32
    link-external-newwindow: true
    footer: <a href="intro.html"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="rgb(153, 153, 153)" class="bi bi-arrow-90deg-up" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4.854 1.146a.5.5 0 0 0-.708 0l-4 4a.5.5 0 1 0 .708.708L4 2.707V12.5A2.5 2.5 0 0 0 6.5 15h8a.5.5 0 0 0 0-1h-8A1.5 1.5 0 0 1 5 12.5V2.707l3.146 3.147a.5.5 0 1 0 .708-.708l-4-4z"/></svg>&nbsp;Back to the course</a>
revealjs-plugins:
  - pointer
---

# Definitions

---

### Artificial intelligence (AI)

Any human-made system mimicking animal intelligence. This is a large and very diverse field.

### Machine learning (ML)

A subfield of AI that can be defined as computer programs whose performance at a task improves with experience.

### Deep learning (DL)

A subfield of ML using artificial neural networks with two or more hidden layers.

# Why has ML become so popular?

## ML allows to achieve previously impossible tasks

:::{.example}

Let's take the example of image recognition:

:::

In typical computing, a programmer writes code that gives a computer detailed instructions of what to do.

Coding all the possible ways—pixel by pixel—that an image can represent, say, a dog is an impossibly large task: there are many breeds of dogs, the image can be a picture, a blurred picture, a drawing, a cartoon, the dog can be in all sorts of positions, wearing clothes, etc.

There just aren't enough resources to make the traditional programming approach able to create a computer program that can identify a dog in images.

By feeding a very large number of dog images to a neural network however, we can train that network to recognize dogs in images that it has never seen (without explicitly programming how it does this!).

## Old concept ... new computing power

The concept is everything but new: [Arthur Samuel](https://en.wikipedia.org/wiki/Arthur_Samuel_(computer_scientist)) came up with it in 1949 and built a self-learning Checkers-playing program in 1959.

::::{.columns}

:::{.column width="60%"}

Machine learning consists of feeding vast amounts of data to algorithms to strengthen *pathways*, so the excitement for the approach became somewhat dormant due to the lack of computing power and the lack of training data at the time.

The advent of powerful computers, GPUs, and massive amounts of data have brought the old concept to the forefront.

:::

:::{.column width="7%"}

:::

:::{.column width="33%"}

![From [xkcd.com](https://xkcd.com/)](https://imgs.xkcd.com/comics/machine_learning.png){fig-alt="noshadow"}

:::

::::

# How does it work?

. . .

<br>
It depends on the type of learning.

## Supervised learning

- *Regression* is a form of supervised learning with continuous outputs
- *Classification* is supervised learning with discrete outputs

Supervised learning uses training data in the form of example input/output pairs.

### Goal

Find the relationship between inputs and outputs.

## Unsupervised learning

Clustering, social network analysis, market segmentation, PCA ... are all forms of unsupervised learning.

Unsupervised learning uses unlabelled data.

### Goal

Find structure within the data.

## Reinforcement learning

The algorithm explores by performing random actions and these actions are rewarded or punished (bonus points or penalties).

This is how algorithms learn to play games.

# Let's explore the case of supervised learning

## Decide on an architecture

![](img/diag_01.png){fig-alt="noshadow"}

The architecture won't change during training.

The type of architecture you choose (e.g. CNN, Transformer) depends on the type of data you have (e.g. vision, textual). The depth and breadth of your network depend on the amount of data and computing resource you have.

## Set some initial parameters

![](img/diag_02.png){fig-alt="noshadow"}

You can initialize them randomly or get much better ones through transfer learning.

While the parameters are also part of the model, those will change during training.

## Get some labelled data

![](img/diag_03.png){fig-alt="noshadow"}

When we say that we need a lot of data for machine learning, we mean "lots of labelled data" as this is what gets used for training models.

## Make sure to keep some data for testing

![](img/diag_04.png){fig-alt="noshadow"}

Those data won't be used for training the model. Often people keep around 20% of their data for testing.

## Pass data and parameters through the architecture

![](img/diag_05.png){fig-alt="noshadow"}

The train data are the inputs and the process of calculating the outputs is the forward pass.

## The outputs of the model are predictions

![](img/diag_06.png){fig-alt="noshadow"}

## Compare those predictions to the train labels

![](img/diag_07.png){fig-alt="noshadow"}

Since our data was labelled, we know what the true outputs are.

## Calculate train loss

![](img/diag_08.png){fig-alt="noshadow"}

The deviation of our predictions from the true outputs gives us a measure of training loss.

## Adjust parameters

![](img/diag_09.png){fig-alt="noshadow"}

The parameters get automatically adjusted to reduce the training loss through the mechanism of backpropagation. This is the actual training part.

This process is repeated many times. Training models is pretty much a giant for loop.

## From model to program

![](img/diag_10.png){fig-alt="noshadow"}

Remember that the model architecture is fixed, but that the parameters change at each iteration of the training process.

---

While the labelled data are key to training, what we are really interested in is the combination of architecture + final parameters.

![](img/diag_11.png){fig-alt="noshadow"}

---

When the training is over, the parameters become fixed. Which means that our model now behaves like a classic program.

![](img/diag_12.png){fig-alt="noshadow"}

## Evaluate the model

![](img/diag_13.png){fig-alt="noshadow"}

We can now use the testing set (which was never used to train the model) to evaluate our model: if we pass the test inputs through our program, we get some predictions that we can compare to the test labels (which are the true outputs).

This gives us the test loss: a measure of how well our model performs.

## Use the model

![](img/diag_14.png){fig-alt="noshadow"}

Now that we have a program, we can use it on unlabelled inputs to get what people ultimately want: unknown outputs.

This is when we put our model to actual use to solve some problem.
