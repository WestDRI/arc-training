---
title: Deep learning with JAX
author: Marie-Hélène Burle
---

:::{.def}

JAX is perfect for developing deep learning models:

- it deals with multi-dimensional arrays,
- it is extremely fast,
- it is optimized for accelerators,
- and it is capable of flexible automatic differentiation.

JAX is however not a DL library. While it is possible to create neural networks directly in JAX, it makes more sense to use libraries built on JAX that provide the toolkit necessary to build and train neural networks.

:::

## Deep learning workflow 

Training a neural network from scratch requires a number of steps:

```{dot}
//| echo: false

digraph {

bgcolor="transparent"
node [fontname="Inconsolata, sans-serif", color=gray55]
edge [color=gray55]

"Load\ndataset" -> "Define\narchitecture" -> Train -> Test -> "Save\nmodel"

}
```
<br>
Pretrained models can also be used for [feature extraction](https://en.wikipedia.org/wiki/Feature_engineering) or [transfer learning](https://en.wikipedia.org/wiki/Transfer_learning).

## Deep learning ecosystem for JAX

Here is a classic ecosystem of libraries for deep learning with JAX:

- **Load datasets**

  There are already good tools to load datasets (e.g. [torchvision](https://github.com/pytorch/vision), [TensorFlow datasets](https://github.com/tensorflow/datasets), [Hugging Face datasets](https://github.com/huggingface/datasets), [Grain](https://github.com/google/grain)), so JAX did not worry about creating its own implementation.

- **Define network architecture**

  Neural networks can be build in JAX from scratch, but a number of packages built on JAX provide the necessary toolkit. [Flax](https://flax.readthedocs.io/en/latest/index.html) is the option recommended by the JAX developers and the one we will use in this course.

- **Train**

  The package [CLU (Common Loop Utils)](https://github.com/google/CommonLoopUtils?tab=readme-ov-file) is a set of helpers to write shorter training loops. [Optax](https://github.com/google-deepmind/optax) provides loss and optimization functions. [Orbax](https://github.com/google/orbax) brings checkpointing utilities.

- **Test**

  Testing a model is easy to do directly in JAX.

- **Save model**

  Flax provides methods to save a model.

<br>
To sum up, here is an ecosystem of libraries to use JAX for neural networks:<br><br>

```{dot}
//| echo: false
//| fig-width: 700px

digraph {

bgcolor="transparent"
node [fontname="Inconsolata, sans-serif", color=gray55]
edge [color=gray55]

load [label="Load data", shape=plaintext, group=g1]
nn [label="Define network", shape=plaintext, group=g1]
train [label="Train\nOptimize\nCheckpoint", shape=plaintext, group=g1]
test [label="Test", shape=plaintext, group=g1]
save [label="Save model", shape=plaintext, group=g1]

PyTorch [fontcolor=darkorange4]
TensorFlow [group=g2, fontcolor=darkorange4]
"Hugging Face" [fontcolor=darkorange4]
Grain [fontcolor=darkorange4]
flax1 [label=Flax, group=g2, fontcolor="#669900"]
flax2 [label=Flax, fontcolor="#669900"]

{rank=same; load PyTorch TensorFlow "Hugging Face" Grain}
load -> PyTorch -> TensorFlow -> "Hugging Face" -> Grain [style=invis]

{rank=same; nn flax1}
nn -> flax1 [style=invis]

jax1 [label=<<FONT COLOR="deepskyblue3">JAX</FONT><BR/><FONT COLOR="#336699">CLU</FONT><BR/><FONT COLOR="#669999">Optax</FONT><BR/><FONT COLOR="#6699ff">Orbax</FONT>>]

jax2 [label="JAX", fontcolor=deepskyblue3]

{PyTorch TensorFlow "Hugging Face" Grain} -> flax1 -> jax1 -> jax2 -> flax2

load -> nn -> train -> test -> save [style=invis]

}
```

<br>
When working from pretrained models, Hugging Face also provides [a great API to download from thousands of pretrained models](https://huggingface.co/models).

## How to get started?

A common approach is to start from one of [the example projects](https://flax.readthedocs.io/en/latest/examples/index.html) and use it as a template.
