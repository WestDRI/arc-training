---
title: Bayesian inference in
frontpic: img/logo_jax.png
frontpicwidth: 45%
frontpicmargintop: 40px
frontpicmarginbottom: 40px
noshadow: noshadow
author: Marie-Hélène Burle
date: 2025-02-25
date-format: long
execute:
  error: true
  echo: true
format:
  revealjs:
    <!-- embed-resources: true -->
    theme: [default, ../../revealjsjax.scss]
    logo: /img/favicon_sfudrac.png
    highlight-style: ayu
    code-line-numbers: false
    template-partials:
      - /title-slide.html
    pointer:
      color: "#b5111b"
      pointerSize: 32
    link-external-newwindow: true
    footer: <a href="wb_bayesian.html"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="rgb(94,152,246)" class="bi bi-arrow-90deg-up" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4.854 1.146a.5.5 0 0 0-.708 0l-4 4a.5.5 0 1 0 .708.708L4 2.707V12.5A2.5 2.5 0 0 0 6.5 15h8a.5.5 0 0 0 0-1h-8A1.5 1.5 0 0 1 5 12.5V2.707l3.146 3.147a.5.5 0 1 0 .708-.708l-4-4z"/></svg>&nbsp;Back to webinar page</a>
    auto-stretch: false
revealjs-plugins:
  - pointer
---

[statistical inference](https://en.wikipedia.org/wiki/Statistical_inference)
[probability distribution](https://en.wikipedia.org/wiki/Probability_distribution)

[Markov chain Monte Carlo (MCMC)](https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo)
[Hamiltonian Monte Carlo (HMC)](https://en.wikipedia.org/wiki/Hamiltonian_Monte_Carlo)
No-U-Turn sampler (NUTS), a variant of HMC and Stan's default MCMC engine

[probabilistic programming language (PPL)](https://en.wikipedia.org/wiki/Probabilistic_programming) explained simply [in this (now outdated) blog post](https://medium.com/dunnhumby-data-science-engineering/what-are-probabilistic-programming-languages-and-why-they-might-be-useful-for-you-a4fe30c4d409)

# On probabilities

## Two interpretations {.center}

- Frequentist
- Bayesian

## Frequentist {.center}

Frequentist approach to probabilities: assigns probabilities to the long-run frequency of events

For events for which there is no long-run, you imagine alternative realities and consider the frequency of occurrences in all those realities

The frequentist approach is computationally simpler and faster and returns summary statistics

## Bayesian {.center}

Bayesian approach: assigns probabilities to our beliefs

It is a measure of believability in an event
There is always some level of uncertainty

The belief is associated with the observer. Different people have different beliefs about the probability of an event occurring because they possess different information

This is the instinctive way to think about probabilities

The Bayesian approach is computationally more cumbersome and returns the probabilities of possible outcomes
Only became possible with the advent of powerful computers and new algorithms such as [Markov chain Monte Carlo (MCMC)](https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo)

## Which approach to choose? {.center}

As the number of instances of evidence N tends towards infinity, frequentist and Bayesian approaches tend to converge towards an objective truth

For small N, the inference is unstable and frequentist estimates have large variance and confidence intervals
Bayesian approaches are more informative because they return probabilities

In short, the Bayesian approach is particularly useful for small data

# Bayesian computing

## Algorithms {.center}

xxx: but problem xxx

HMC
NUTS

Requires autodiff

## Software {.center}

Since the Bayesian approach is computationally intensive, the choice of framework, language, and compiler is important

## Initial software {.center}

[WinBUGS](https://en.wikipedia.org/wiki/WinBUGS) replaced by [OpenBUGS](https://en.wikipedia.org/wiki/OpenBUGS), written in [Component Pascal](https://en.wikipedia.org/wiki/Component_Pascal), then [JAGS](https://en.wikipedia.org/wiki/Just_another_Gibbs_sampler), written in C++

## Stan {.center}

Stan: a probabilistic programming language

but it is a domain-specific language
which means that you have to learn the syntax to write a Stan script (that you can then call from R or Python via RStan and PyStan)
PyMC: Python

[Stan](https://en.wikipedia.org/wiki/Stan_(software)), see also [website](https://mc-stan.org/) and [paper](https://journals.sagepub.com/doi/abs/10.3102/1076998615606113)
[brms](https://cran.r-project.org/web/packages/brms/index.html) in R doesn't require learning Stan but only works for simple models

Stan has reverse-mode automatic differentiation

<!-- ## Stan algorithms {.center} -->

<!-- gradient-based MCMC algorithms for Bayesian inference: -->
<!-- - Hamiltonian Monte Carlo (HMC) -->
<!-- - No-U-Turn sampler (NUTS), a variant of HMC and Stan's default MCMC engine -->

<!-- stochastic gradient-based variational algorithms for approximate Bayesian inference: -->
<!-- - Automatic Differentiation Variational Inference -->
<!-- - Pathfinder: Parallel quasi-Newton variational inference -->

<!-- gradient-based optimization algorithms for penalized maximum likelihood estimation: -->
<!-- - Limited-memory BFGS (Stan's default optimization algorithm) -->
<!-- - Broyden–Fletcher–Goldfarb–Shanno algorithm -->

<!-- - Laplace's approximation for classical standard error estimates and approximate Bayesian posteriors -->

## PPLs based on DL frameworks {.center}

Since HMC and NUTS require autodiff, many software have emerged in recent years, following the explosion of deep learning

(The Stan developers wrote their own autodiff system)

:::{.example}

[Pyro](https://github.com/pyro-ppl/pyro) based on [PyTorch](https://github.com/pytorch/pytorch)

[Edward](https://github.com/blei-lab/edward), then [Edward2](https://github.com/google/edward2) as well as [TensorFlow Probability](https://github.com/tensorflow/probability) based on [TensorFlow](https://github.com/tensorflow/tensorflow)

:::

## Enters JAX {.center}

> Had JAX existed when we started coding Stan in 2011, we would’ve used that rather than rolling our own autodiff system.

Bob Carpenter, one of Stan creators (https://statmodeling.stat.columbia.edu/2024/09/25/stan-faster-than-jax-on-cpu/)

JAX is designed natively for GPU coding and parallelism. Stan is not

## What is JAX? {.center}

[JAX](https://github.com/jax-ml/jax) is

See [our introductory course](https://mint.westdri.ca/ai/top_jx) and [webinar](https://mint.westdri.ca/ai/jx/wb_jax)

---

```{dot}
//| echo: false
//| fig-height: 600px

strict digraph {

node [fontname="Inconsolata, sans-serif"]
edge [color=gray55]
bgcolor="transparent"

tracer  [label=Tracing, shape=rectangle, color=darkviolet, fontcolor=darkviolet]
jit [label=" Just-in-time \n(JIT)\ncompilation", shape=rectangle, color=chocolate, fontcolor=chocolate]
xla [label="Accelerated\n Linear Algebra \n(XLA)", shape=rectangle, color=deeppink3, fontcolor=deeppink3]
transform [label=" Transformations ", shape=rectangle, color=chocolate, fontcolor=chocolate]

CPU [shape=octagon, color=darkslategray4, fontcolor=darkslategray4]
GPU [shape=octagon, color=darkslategray4, fontcolor=darkslategray4]
TPU [shape=octagon, color=darkslategray4, fontcolor=darkslategray4]


py [label="Pure Python\nfunctions", color=gray50, fontcolor=gray50]
jaxpr [label="Jaxprs\n(JAX expressions)\nintermediate\nrepresentation\n(IR)", color=gray30, fontcolor=gray30]
hlo [label="High-level\noptimized (HLO)\nprogram", color=gray10, fontcolor=gray10]

py -> tracer [dir=none]
tracer -> jaxpr
jaxpr -> jit [dir=none]
jit -> hlo
hlo -> xla [dir=none]

xla -> CPU [shape=doubleoctagon]
xla -> GPU
xla -> TPU

jaxpr -> transform [dir=both, minlen=3]
{rank=same; jaxpr transform}

}
```

---

```{dot}
//| echo: false
//| fig-height: 600px

strict digraph {

node [fontname="Inconsolata, sans-serif"]
edge [color=gray55]
bgcolor="transparent"

tracer  [label=Tracing, shape=rectangle, color=darkviolet, fontcolor=darkviolet]
jit [label=" Just-in-time \n(JIT)\ncompilation", shape=rectangle, color=chocolate, fontcolor=chocolate]
xla [label="Accelerated\n Linear Algebra \n(XLA)", shape=rectangle, color=deeppink3, fontcolor=deeppink3]
transform [label="Vectorization\nParallelization\n   Differentiation  ", shape=rectangle, color=chocolate, fontcolor=chocolate]

CPU [shape=octagon, color=darkslategray4, fontcolor=darkslategray4]
GPU [shape=octagon, color=darkslategray4, fontcolor=darkslategray4]
TPU [shape=octagon, color=darkslategray4, fontcolor=darkslategray4]


py [label="Pure Python\nfunctions", color=gray50, fontcolor=gray50]
jaxpr [label="Jaxprs\n(JAX expressions)\nintermediate\nrepresentation\n(IR)", color=gray30, fontcolor=gray30]
hlo [label="High-level\noptimized (HLO)\nprogram", color=gray10, fontcolor=gray10]

py -> tracer [dir=none]
tracer -> jaxpr
jaxpr -> jit [dir=none]
jit -> hlo
hlo -> xla [dir=none]

xla -> CPU [shape=doubleoctagon]
xla -> GPU
xla -> TPU

jaxpr -> transform [dir=both, minlen=3]
{rank=same; jaxpr transform}

}
```

## New backends using JAX {.center}

[Edward2](https://github.com/google/edward2) and [TensorFlow Probability](https://github.com/tensorflow/probability) can now use JAX as a backend

## PyMC {.center}

PyMC is a PPL based on [PyTensor](https://github.com/pymc-devs/pytensor). rely on building a static graph

[pymc-extras](https://github.com/pymc-devs/pymc-extras) is a testing ground for novel algorithms, distributions, and methods

## NumPyro {.center}

[NumPyro](https://github.com/pyro-ppl/numpyro) based on [Pyro](https://github.com/pyro-ppl/pyro) but uses NumPy and JAX

## Blackjax {.center}

Not a PPL but a library of samplers

<!-- ## Annex {.center} -->

<!-- [List of currently available PPLs](https://en.wikipedia.org/wiki/Probabilistic_programming#List_of_probabilistic_programming_languages) -->
