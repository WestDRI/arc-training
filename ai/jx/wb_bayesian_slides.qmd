---
title: Bayesian inference in
frontpic: img/logo_jax.png
frontpicwidth: 45%
frontpicmargintop: 40px
frontpicmarginbottom: 40px
noshadow: noshadow
author: Marie-Hélène Burle
date: 2025-02-25
date-format: long
execute:
  error: true
  echo: true
format:
  revealjs:
    <!-- embed-resources: true -->
    theme: [default, ../../revealjsjax.scss]
    logo: /img/favicon_sfudrac.png
    highlight-style: ayu
    code-line-numbers: false
    template-partials:
      - /title-slide.html
    pointer:
      color: "#b5111b"
      pointerSize: 32
    link-external-newwindow: true
    footer: <a href="wb_bayesian.html"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="rgb(94,152,246)" class="bi bi-arrow-90deg-up" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4.854 1.146a.5.5 0 0 0-.708 0l-4 4a.5.5 0 1 0 .708.708L4 2.707V12.5A2.5 2.5 0 0 0 6.5 15h8a.5.5 0 0 0 0-1h-8A1.5 1.5 0 0 1 5 12.5V2.707l3.146 3.147a.5.5 0 1 0 .708-.708l-4-4z"/></svg>&nbsp;Back to webinar page</a>
    auto-stretch: false
revealjs-plugins:
  - pointer
---

[Bayes' theorem](https://en.wikipedia.org/wiki/Bayes%27_theorem)
[statistical inference](https://en.wikipedia.org/wiki/Statistical_inference)
[probability distribution](https://en.wikipedia.org/wiki/Probability_distribution)

# On probabilities

## Two interpretations {.center}

- Frequentist
- Bayesian

## Frequentist {.center}

Frequentist approach to probabilities: assigns probabilities to the long-run frequency of events

For events for which there is no long-run, you imagine alternative realities and consider the frequency of occurrences in all those realities

The frequentist approach is computationally simpler and faster and returns summary statistics

## Bayesian {.center}

Bayesian approach: assigns probabilities to our beliefs

It is a measure of believability in an event
There is always some level of uncertainty

The belief is associated with the observer. Different people have different beliefs about the probability of an event occurring because they possess different information

This is the instinctive way to think about probabilities

The Bayesian approach is computationally more cumbersome and returns the probabilities of possible outcomes
Only became possible with the advent of powerful computers and new algorithms such as [Markov chain Monte Carlo (MCMC)](https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo)

## Which approach to choose? {.center}

As the number of instances of evidence N tends towards infinity, frequentist and Bayesian approaches tend to converge towards an objective truth

For small N, the inference is unstable and frequentist estimates have large variance and confidence intervals
Bayesian approaches are more informative because they return probabilities

In short, the Bayesian approach is particularly useful for small data

# Bayesian computing

## Algorithms {.center}

[Markov chain Monte Carlo (MCMC)](https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo), but problem xxx

[Hamiltonian Monte Carlo (HMC)](https://en.wikipedia.org/wiki/Hamiltonian_Monte_Carlo)
No-U-Turn sampler (NUTS), a variant of HMC and Stan's default MCMC engine

Require require burdensome calculations of derivatives solved by [automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation)

## Probabilistic Programming Language {.center}

Since the Bayesian approach is computationally intensive, the choice of framework, language, and compiler is important

[Probabilistic programming language (PPL)](https://en.wikipedia.org/wiki/Probabilistic_programming), explained simply [in this (now outdated) blog post](https://medium.com/dunnhumby-data-science-engineering/what-are-probabilistic-programming-languages-and-why-they-might-be-useful-for-you-a4fe30c4d409), are xxx

## First PPLs {.center}

Rely on [Gibbs sampling](https://en.wikipedia.org/wiki/Gibbs_sampling)

- [WinBUGS](https://en.wikipedia.org/wiki/WinBUGS) replaced by [OpenBUGS](https://en.wikipedia.org/wiki/OpenBUGS), written in [Component Pascal](https://en.wikipedia.org/wiki/Component_Pascal)

- later [JAGS](https://en.wikipedia.org/wiki/Just_another_Gibbs_sampler), written in C++

## Stan {.center}

[Stan](https://en.wikipedia.org/wiki/Stan_(software)), see also [website](https://mc-stan.org/) and [paper](https://journals.sagepub.com/doi/abs/10.3102/1076998615606113), is a domain-specific language

You write a Stan script that you can then call from R, Python, or the shell via RStan, PyStan, etc.

Also used as the backend for [brms](https://cran.r-project.org/web/packages/brms/index.html) in R which doesn't require learning Stan but only works for simple models

Stan has reverse-mode automatic differentiation

<!-- ## Stan algorithms {.center} -->

<!-- gradient-based MCMC algorithms for Bayesian inference: -->
<!-- - Hamiltonian Monte Carlo (HMC) -->
<!-- - No-U-Turn sampler (NUTS), a variant of HMC and Stan's default MCMC engine -->

<!-- stochastic gradient-based variational algorithms for approximate Bayesian inference: -->
<!-- - Automatic Differentiation Variational Inference -->
<!-- - Pathfinder: Parallel quasi-Newton variational inference -->

<!-- gradient-based optimization algorithms for penalized maximum likelihood estimation: -->
<!-- - Limited-memory BFGS (Stan's default optimization algorithm) -->
<!-- - Broyden–Fletcher–Goldfarb–Shanno algorithm -->

<!-- - Laplace's approximation for classical standard error estimates and approximate Bayesian posteriors -->

## PPLs based on DL frameworks {.center}

Since HMC and NUTS require autodiff, many software have emerged in recent years, following the explosion of deep learning

(The Stan developers wrote their own autodiff system)

:::{.example}

[Pyro](https://github.com/pyro-ppl/pyro) based on [PyTorch](https://github.com/pytorch/pytorch)

[Edward](https://github.com/blei-lab/edward), then [Edward2](https://github.com/google/edward2) as well as [TensorFlow Probability](https://github.com/tensorflow/probability) based on [TensorFlow](https://github.com/tensorflow/tensorflow)

:::

## Enters JAX {.center}

> Had JAX existed when we started coding Stan in 2011, we would’ve used that rather than rolling our own autodiff system.

Bob Carpenter, one of Stan creators (https://statmodeling.stat.columbia.edu/2024/09/25/stan-faster-than-jax-on-cpu/)

JAX is designed natively for GPU coding and parallelism. Stan is not

## What is JAX? {.center}

[JAX](https://github.com/jax-ml/jax) is a library for Python that:

- makes use of the extremely performant [XLA compiler](https://en.wikipedia.org/wiki/Accelerated_Linear_Algebra)
- runs on accelerators (GPUs/TPUs)
- provides automatic differentiation
- uses [just-in-time compilation](https://en.wikipedia.org/wiki/Just-in-time_compilation)
- allows batching and parallelization

:::{.fragment}

[⇒ perfect tool for Bayesian statistics]{.emph}

:::

<br>
[*See [our introductory JAX course](https://mint.westdri.ca/ai/top_jx) and [webinar](https://mint.westdri.ca/ai/jx/wb_jax) for more details*]{.small}

---

```{dot}
//| echo: false
//| fig-height: 650px

strict digraph {

node [fontname="Inconsolata, sans-serif"]
edge [color=gray55]
bgcolor="transparent"

tracer  [label=Tracing, shape=rectangle, color=darkviolet, fontcolor=darkviolet]
jit [label=" Just-in-time \n(JIT)\ncompilation", shape=rectangle, color=chocolate, fontcolor=chocolate]
xla [label="Accelerated\n Linear Algebra \n(XLA)", shape=rectangle, color=deeppink3, fontcolor=deeppink3]
transform [label=" Transformations ", shape=rectangle, color=chocolate, fontcolor=chocolate]

CPU [shape=octagon, color=darkslategray4, fontcolor=darkslategray4]
GPU [shape=octagon, color=darkslategray4, fontcolor=darkslategray4]
TPU [shape=octagon, color=darkslategray4, fontcolor=darkslategray4]


py [label="Pure Python\nfunctions", color=gray50, fontcolor=gray50]
jaxpr [label="Jaxprs\n(JAX expressions)\nintermediate\nrepresentation\n(IR)", color=gray30, fontcolor=gray30]
hlo [label="High-level\noptimized (HLO)\nprogram", color=gray10, fontcolor=gray10]

py -> tracer [dir=none]
tracer -> jaxpr
jaxpr -> jit [dir=none]
jit -> hlo
hlo -> xla [dir=none]

xla -> CPU [shape=doubleoctagon]
xla -> GPU
xla -> TPU

jaxpr -> transform [dir=both, minlen=3]
{rank=same; jaxpr transform}

}
```

---

```{dot}
//| echo: false
//| fig-height: 650px

strict digraph {

node [fontname="Inconsolata, sans-serif"]
edge [color=gray55]
bgcolor="transparent"

tracer  [label=Tracing, shape=rectangle, color=darkviolet, fontcolor=darkviolet]
jit [label=" Just-in-time \n(JIT)\ncompilation", shape=rectangle, color=chocolate, fontcolor=chocolate]
xla [label="Accelerated\n Linear Algebra \n(XLA)", shape=rectangle, color=deeppink3, fontcolor=deeppink3]
transform [label="Vectorization\nParallelization\n   Differentiation  ", shape=rectangle, color=chocolate, fontcolor=chocolate]

CPU [shape=octagon, color=darkslategray4, fontcolor=darkslategray4]
GPU [shape=octagon, color=darkslategray4, fontcolor=darkslategray4]
TPU [shape=octagon, color=darkslategray4, fontcolor=darkslategray4]


py [label="Pure Python\nfunctions", color=gray50, fontcolor=gray50]
jaxpr [label="Jaxprs\n(JAX expressions)\nintermediate\nrepresentation\n(IR)", color=gray30, fontcolor=gray30]
hlo [label="High-level\noptimized (HLO)\nprogram", color=gray10, fontcolor=gray10]

py -> tracer [dir=none]
tracer -> jaxpr
jaxpr -> jit [dir=none]
jit -> hlo
hlo -> xla [dir=none]

xla -> CPU [shape=doubleoctagon]
xla -> GPU
xla -> TPU

jaxpr -> transform [dir=both, minlen=3]
{rank=same; jaxpr transform}

}
```

## What is JAX? {.center}

<iframe width="900" height="600" src="https://docs.jax.dev/en/latest/index.html"></iframe>

## New backends using JAX {.center}

[Edward2](https://github.com/google/edward2) and [TensorFlow Probability](https://github.com/tensorflow/probability) can now use JAX as a backend

## PyMC {.center}

Library for Python

PyMC is a PPL based on [PyTensor](https://github.com/pymc-devs/pytensor). rely on building a static graph

[pymc-extras](https://github.com/pymc-devs/pymc-extras) is a testing ground for novel algorithms, distributions, and methods

## NumPyro {.center}

Library for Python

[NumPyro](https://github.com/pyro-ppl/numpyro) based on [Pyro](https://github.com/pyro-ppl/pyro) but uses NumPy and JAX

## Blackjax {.center}

Not a PPL but a library of samplers for Python

<!-- ## Annex {.center} -->

<!-- [List of currently available PPLs](https://en.wikipedia.org/wiki/Probabilistic_programming#List_of_probabilistic_programming_languages) -->
