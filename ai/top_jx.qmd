---
title: Faster deep learning with JAX
description: An introductory course to [![](img/JAX_logo.svg.png){width="1.7em" fig-alt="noshadow"}](https://jax.readthedocs.io/)
title-block-banner: true
---

:::{.topdef}

Faster than [TensorFlow](https://www.tensorflow.org/) and [PyTorch](https://pytorch.org/), [JAX](https://jax.readthedocs.io/) is the new open source Google deep learning framework. With a [NumPy](https://numpy.org/)-like API, it uses [asynchronous dispatch](https://en.wikipedia.org/wiki/Asynchrony_(computer_programming)), [just-in-time compilation](https://en.wikipedia.org/wiki/Just-in-time_compilation), and the [XLA](https://www.tensorflow.org/xla) compiler for linear algebra to run on accelerators ([GPUs](https://en.wikipedia.org/wiki/Graphics_processing_unit) and [TPUs](https://en.wikipedia.org/wiki/Tensor_Processing_Unit)). It also provides a powerful and flexible automatic differentiation engine.

This course will introduce the basics of JAX functioning—its strengths and its constraints—and get you started accelerating your array computations. Finally, we will see how to use [Flax](https://flax.readthedocs.io/en/latest/index.html) and [Optax](https://optax.readthedocs.io/en/latest/) to build neural networks with JAX.

:::

<br>
[[Start course ➤](jx_why.qmd)]{.topinline}
