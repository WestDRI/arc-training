---
title: Accelerated array computing and flexible differentiation with
frontpic: img/logo_jax.png
frontpicwidth: 45%
frontpicmargintop: 40px
frontpicmarginbottom: 40px
author: Marie-Hélène Burle
date: 2024-04-16
date-format: long
execute:
  error: true
  echo: true
format:
  revealjs:
    # embed-resources: true
    theme: [default, ../revealjs.scss]
    logo: /img/logo_sfudrac.png
    highlight-style: ayu
    code-line-numbers: false
    template-partials:
      - ../title-slide.html
    pointer:
      color: "#b5111b"
      pointerSize: 32
    link-external-newwindow: true
    footer: <a href="wb_jax.html"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="rgb(153, 153, 153)" class="bi bi-arrow-90deg-up" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4.854 1.146a.5.5 0 0 0-.708 0l-4 4a.5.5 0 1 0 .708.708L4 2.707V12.5A2.5 2.5 0 0 0 6.5 15h8a.5.5 0 0 0 0-1h-8A1.5 1.5 0 0 1 5 12.5V2.707l3.146 3.147a.5.5 0 1 0 .708-.708l-4-4z"/></svg>&nbsp;Back to webinar page</a>
    auto-stretch: false
revealjs-plugins:
  - pointer
---

# Context

## What is JAX? {.center}



## Why JAX? {.center}



## Relation to NumPy {.center}



# How does JAX work?

## Overall functioning

```{dot}
//| echo: false
//| fig-height: 550px

strict digraph {

bgcolor="transparent"
node [fontname="Inconsolata, sans-serif"]
edge [color=gray55]

tracer  [label=Tracing, shape=rectangle, color=darkviolet, fontcolor=darkviolet]
jit [label=" Just-in-time \n(JIT)\ncompilation", shape=rectangle, color=chocolate, fontcolor=chocolate]
xla [label="Accelerated\n Linear Algebra \n(XLA)", shape=rectangle, color=deeppink3, fontcolor=deeppink3]
transform [label=" Transformations ", shape=rectangle, color=chocolate, fontcolor=chocolate]

CPU [shape=octagon, color=darkslategray4, fontcolor=darkslategray4]
GPU [shape=octagon, color=darkslategray4, fontcolor=darkslategray4]
TPU [shape=octagon, color=darkslategray4, fontcolor=darkslategray4]


py [label="Pure Python\nfunctions", color=gray50, fontcolor=gray50]
jaxpr [label="Jaxpr\n(JAX expression)\nintermediate\nrepresentation\n(IR)", color=gray30, fontcolor=gray30]
hlo [label="High-level\noptimized (HLO)\nprogram", color=gray10, fontcolor=gray10]

py -> tracer [dir=none]
tracer -> jaxpr
jaxpr -> jit [dir=none]
jit -> hlo
hlo -> xla [dir=none]

xla -> CPU [shape=doubleoctagon]
xla -> GPU
xla -> TPU

jaxpr -> transform [dir=both, minlen=3]

{rank=same; jaxpr transform}

}
```

## Tracing {.center}



## JIT compilation {.center}



## Transformations {.center}



# Parallel computing

## Asynchronous dispatch {.center}



## Vectorization {.center}



## Data parallelism {.center}



## Sharding {.center}



## Multi-host communication {.center}

JAX does not have the ability to scale things up to the level of multi-node clusters

However you can use [mpi4jax](https://github.com/mpi4jax/mpi4jax) extension

# Examples
