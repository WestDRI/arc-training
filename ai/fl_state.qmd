---
title: Stateless models
author: Marie-Hélène Burle
---

:::{.def}

Deep learning models can be split into two big categories depending on the deep learning frameworks used to train them: stateful and stateless models.

What does this mean and where does Flax stand?

:::

## Stateful models

In frameworks such as [PyTorch](https://pytorch.org/) or the [Julia](https://julialang.org/) package [Flux](https://fluxml.ai/), model parameters and optimizer state are stored within the model instance. Instantiating a PyTorch model allocates memory for the model parameters. The model can then be described as *stateful*.

During the forward pass, only the inputs are passed through the model. The output depends on the inputs and on the state of the model.

## Stateless models

JAX JIT compilation requires that functions be without side effects since side effects are only executed once, during tracing.

Updating model parameters and optimizer state thus cannot be done as a side-effect. The state cannot be part of the model instance—it needs to be explicit, that is, separated from the model. During instantiation, no memory is allocated for the parameters. During the forward pass, the state will pass, along with the inputs, through the model. The model is thus *stateless* and the constrains of pure functional programming are met (inputs lead to outputs without external influence or side effects).

Frameworks based on JAX such as [Flax](https://flax.readthedocs.io/en/latest/index.html) as well as the Julia package [Lux](https://lux.csail.mit.edu/) (a modern rewrite of Flux with explicit model parameters) follow this approach.


