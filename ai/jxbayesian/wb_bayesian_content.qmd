---
title: Bayesian inference in JAX
author: Marie-Hélène Burle
---

:::{.def}

*Content from [the webinar slides](wb_bayesian_slides.qmd) for easier browsing.*

:::

## On probabilities

### Two interpretations of probabilities

:::{layout="[50, 46]"}

:::{.col1}

<center>**Frequentist**</center>

![from <https://www.kdnuggets.com/2023/05/bayesian-frequentist-statistics-data-science.html>](img/frequentist_stats.png){fig-alt="noshadow"}

:::

:::{.col2}

<center>**Bayesian**</center>

![from <https://vitalflux.com/bayesian-machine-learning-applications-examples/>](img/bayesian_stats.png){fig-alt="noshadow"}

:::

:::

### Frequentist

[Frequentist approach](https://en.wikipedia.org/wiki/Frequentist_probability) to probabilities: assigns probabilities to the long-run frequency of events.

It doesn't assign probabilities to non-random variables such as hypotheses or parameters.

Instead, the probability is assigned to the limit of the relative frequencies of events in infinite trials and we can assign a probability to the fact that a new random sample would produce a confidence interval that contains the unknown parameter.

This is not how we intuitively think and the results are hard to interpret. This approach is also often artificially constrained and limits the integration of various forms of information.

It is however computationally simple and fast: samples are randomly selected from the sample space and it returns test statistics such as p-values and confidence intervals. This is why it was the dominant approach for a long time: we knew how to do it.

### Bayesian

[Bayesian approach](https://en.wikipedia.org/wiki/Bayesian_statistics): assigns probabilities to our beliefs about an event.

Based on [Bayes' theorem](https://en.wikipedia.org/wiki/Bayes%27_theorem) of conditional probabilities which allows to calculate the probability of a cause given its effect:

$$ P(A \vert X) = \frac{P(X \vert A) P(A)}{P(X)} $$

where:

- [$P(A)$ is the *prior probability* of $A$—our belief about event $A$]{.emph}.
- $P(X)$ is the *marginal probability* of event $X$ (some observed data).
- $P(X \vert A)$ is the *likelihood* or conditional probability of observing $X$ given $A$.
- [$P(A \vert X)$ is the *posterior probability*—our updated belief about $A$ given the data]{.emph}.

### Which approach to choose?

Bayesian statistics:

- is more intuitive to the way we think about the world (easier to interpret),
- allows for the incorporation of prior information and diverse data,
- is more informative as it provides a measure of uncertainty (returns probabilities),
- is extremely valuable when there is little data (the inference is unstable and frequentist estimates have large variance and confidence intervals).

But beyond extremely simple examples, Bayesian inference is mathematically extremely arduous.

It is also much more computationally heavy and only became possible to apply widely with the advent of powerful computers and new algorithms such as [Markov chain Monte Carlo (MCMC)](https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo).

## Bayesian computing

### Algorithms

A Bayesian approach to statistics often leads to posterior probability distributions that are too complex or too highly dimensional to be studied by analytical techniques.

[Markov chain Monte Carlo (MCMC)](https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo) is a class of sampling algorithms which explore such distributions.

Different algorithms move in different ways across the N-dimensional space of the parameters, accepting or rejecting each new position based on its adherence to the prior distribution and the data.

The sequence of accepted positions constitute the *traces*.

### PPL

[Probabilistic programming language (PPL)](https://en.wikipedia.org/wiki/Probabilistic_programming#Probabilistic_programming_languages), explained simply [in this (a bit outdated) blog post](https://medium.com/dunnhumby-data-science-engineering/what-are-probabilistic-programming-languages-and-why-they-might-be-useful-for-you-a4fe30c4d409), are computer languages specialized in creating probabilistic models and making inference.

Model components are first-class primitives.

They can be based on a general programming language (e.g. Python, Julia) or domain specific.

### First Bayesian PPLs

Relied on [Gibbs sampling](https://en.wikipedia.org/wiki/Gibbs_sampling):

- [WinBUGS](https://en.wikipedia.org/wiki/WinBUGS) replaced by [OpenBUGS](https://en.wikipedia.org/wiki/OpenBUGS), written in [Component Pascal](https://en.wikipedia.org/wiki/Component_Pascal),
- [JAGS](https://en.wikipedia.org/wiki/Just_another_Gibbs_sampler), written in C++.

*BUGS = Bayesian inference Using Gibbs Sampling \
JAGS = Just Another Gibbs Sampler*

### Stan

[Stan](https://en.wikipedia.org/wiki/Stan_(software)) (see also [website](https://mc-stan.org/) and [paper](https://journals.sagepub.com/doi/abs/10.3102/1076998615606113)) is a domain-specific language.

Stan scripts can be executed from R, Python, or the shell via RStan, PyStan, etc.

Also used as the backend for the R package [brms](https://cran.r-project.org/web/packages/brms/index.html) which doesn't require learning Stan but only works for simple models.

Relies on No-U-Turn sampler (NUTS), a variant of [Hamiltonian Monte Carlo (HMC)](https://en.wikipedia.org/wiki/Hamiltonian_Monte_Carlo) (see also [HMC paper](https://arxiv.org/abs/1701.02434)).

HMC and variants require burdensome calculations of derivatives. Stan solved that by creating its own reverse-mode [automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation) engine.

Superior to Gibbs sampler ➔ made Stan a very popular PPL for years.

### PPLs based on DL frameworks

Since HMC and NUTS require autodiff, many Python PPLs have emerged in recent years, following the explosion of deep learning.

Examples:

- [Pyro](https://github.com/pyro-ppl/pyro) based on [PyTorch](https://github.com/pytorch/pytorch),
- [Edward](https://github.com/blei-lab/edward), then [Edward2](https://github.com/google/edward2) as well as [TensorFlow Probability](https://github.com/tensorflow/probability) based on [TensorFlow](https://github.com/tensorflow/tensorflow).

### Enters JAX

> Had JAX existed when we started coding Stan in 2011, we would’ve used that rather than rolling our own autodiff system.

:::{.right}

*[Bob Carpenter](https://xcelab.net/rm/), one of Stan's creators, in [a recent blog post](https://statmodeling.stat.columbia.edu/2024/09/25/stan-faster-than-jax-on-cpu/).*

:::

### What is JAX?

[JAX](https://github.com/jax-ml/jax) is a library for Python that:

- makes use of the extremely performant [XLA compiler](https://en.wikipedia.org/wiki/Accelerated_Linear_Algebra),
- runs on accelerators (GPUs/TPUs),
- provides automatic differentiation,
- uses [just-in-time compilation](https://en.wikipedia.org/wiki/Just-in-time_compilation),
- allows batching and parallelization.

[⇒ perfect tool for Bayesian statistics.]{.emph}

*See [our introductory JAX course](/ai/top_jx.qmd) and [webinar](/ai/jx/wb_jax.qmd) for more details.*

```{dot}
//| echo: false
//| fig-height: 650px

strict digraph {

node [fontname="Inconsolata, sans-serif"]
edge [color=gray55]
bgcolor="transparent"

tracer  [label=Tracing, shape=rectangle, color=darkviolet, fontcolor=darkviolet]
jit [label=" Just-in-time \n(JIT)\ncompilation", shape=rectangle, color=chocolate, fontcolor=chocolate]
xla [label="Accelerated\n Linear Algebra \n(XLA)", shape=rectangle, color=deeppink3, fontcolor=deeppink3]
transform [label=" Transformations ", shape=rectangle, color=chocolate, fontcolor=chocolate]

CPU [shape=octagon, color=darkslategray4, fontcolor=darkslategray4]
GPU [shape=octagon, color=darkslategray4, fontcolor=darkslategray4]
TPU [shape=octagon, color=darkslategray4, fontcolor=darkslategray4]

py [label="Pure Python\nfunctions", color=gray50, fontcolor=gray50]
jaxpr [label="Jaxprs\n(JAX expressions)\nintermediate\nrepresentation\n(IR)", color=gray30, fontcolor=gray30]
hlo [label="High-level\noptimized (HLO)\nprogram", color=gray10, fontcolor=gray10]

py -> tracer [dir=none]
tracer -> jaxpr
jaxpr -> jit [dir=none]
jit -> hlo
hlo -> xla [dir=none]

xla -> CPU [shape=doubleoctagon]
xla -> GPU
xla -> TPU

jaxpr -> transform [dir=both, minlen=3]
{rank=same; jaxpr transform}

}
```

```{dot}
//| echo: false
//| fig-height: 650px

strict digraph {

node [fontname="Inconsolata, sans-serif"]
edge [color=gray55]
bgcolor="transparent"

tracer  [label=Tracing, shape=rectangle, color=darkviolet, fontcolor=darkviolet]
jit [label=" Just-in-time \n(JIT)\ncompilation", shape=rectangle, color=chocolate, fontcolor=chocolate]
xla [label="Accelerated\n Linear Algebra \n(XLA)", shape=rectangle, color=deeppink3, fontcolor=deeppink3]
transform [label="Vectorization\nParallelization\n   Differentiation  ", shape=rectangle, color=chocolate, fontcolor=chocolate]

CPU [shape=octagon, color=darkslategray4, fontcolor=darkslategray4]
GPU [shape=octagon, color=darkslategray4, fontcolor=darkslategray4]
TPU [shape=octagon, color=darkslategray4, fontcolor=darkslategray4]

py [label="Pure Python\nfunctions", color=gray50, fontcolor=gray50]
jaxpr [label="Jaxprs\n(JAX expressions)\nintermediate\nrepresentation\n(IR)", color=gray30, fontcolor=gray30]
hlo [label="High-level\noptimized (HLO)\nprogram", color=gray10, fontcolor=gray10]

py -> tracer [dir=none]
tracer -> jaxpr
jaxpr -> jit [dir=none]
jit -> hlo
hlo -> xla [dir=none]

xla -> CPU [shape=doubleoctagon]
xla -> GPU
xla -> TPU

jaxpr -> transform [dir=both, minlen=3]
{rank=same; jaxpr transform}

}
```

```{dot}
//| echo: false
//| fig-height: 650px

strict digraph {

node [fontname="Inconsolata, sans-serif"]
edge [color=gray55]
bgcolor="transparent"

tracer  [label=Tracing, shape=rectangle, color=darkviolet, fontcolor=darkviolet]
jit [label="jit", shape=rectangle, color=chocolate, fontcolor=chocolate]
xla [label="Accelerated\n Linear Algebra \n(XLA)", shape=rectangle, color=deeppink3, fontcolor=deeppink3]
transform [label="vmap\npmap\ngrad", shape=rectangle, color=chocolate, fontcolor=chocolate]

CPU [shape=octagon, color=darkslategray4, fontcolor=darkslategray4]
GPU [shape=octagon, color=darkslategray4, fontcolor=darkslategray4]
TPU [shape=octagon, color=darkslategray4, fontcolor=darkslategray4]

py [label="Pure Python\nfunctions", color=gray50, fontcolor=gray50]
jaxpr [label="Jaxprs\n(JAX expressions)\nintermediate\nrepresentation\n(IR)", color=gray30, fontcolor=gray30]
hlo [label="High-level\noptimized (HLO)\nprogram", color=gray10, fontcolor=gray10]

py -> tracer [dir=none]
tracer -> jaxpr
jaxpr -> jit [dir=none]
jit -> hlo
hlo -> xla [dir=none]

xla -> CPU [shape=doubleoctagon]
xla -> GPU
xla -> TPU

jaxpr -> transform [dir=both, minlen=3]
{rank=same; jaxpr transform}

}
```

### JAX idiosyncrasies

JAX is sublanguage of Python requiring pure functions instead of Python's object-oriented style.

It has [other quirks](https://docs.jax.dev/en/latest/notebooks/Common_Gotchas_in_JAX.html).

The only one you really need to understand for use in PPLs is the pseudorandom number generation.

### PRNG keys

Traditional [pseudorandom number generators](https://en.wikipedia.org/wiki/Pseudorandom_number_generator) are based on nondeterministic state of the OS. This is slow and problematic for parallel executions.

JAX relies on an explicitly-set random state called a *key*:

```{.python}
from jax import random
key = random.key(18)
```

Each key can only be used for one random function, but it can be split into new keys:

```{.python}
key, subkey = random.split(key)
```

:::{.note}

The first key can't be used anymore. We overwrote it with a new key to ensure we don't accidentally reuse it.

:::

We can now use `subkey` in random functions in our code (and keep `key` to generate new subkeys as needed).

### JAX use cases

<iframe width="900" height="600" src="https://docs.jax.dev/en/latest/index.html#ecosystem" data-external="1"></iframe>

### New JAX backends

New JAX backends are getting added to many PPLs.

[Edward2](https://github.com/google/edward2) and [TensorFlow Probability](https://github.com/tensorflow/probability) can now use JAX as backend.

[PyMC](https://github.com/pymc-devs/pymc) relies on building a static graph. It is based on [PyTensor](https://github.com/pymc-devs/pytensor) which provides JAX compilation *([PyTensor](https://github.com/pymc-devs/pytensor) is a fork of [aesara](https://github.com/aesara-devs/aesara), itself a fork of [Theano](https://github.com/Theano/Theano))*.

### NumPyro

[NumPyro](https://github.com/pyro-ppl/numpyro) is a library based on [Pyro](https://github.com/pyro-ppl/pyro) but using [NumPy](https://numpy.org/) and JAX.

<iframe width="760" height="500" src="https://num.pyro.ai/en/stable/" data-external="1"></iframe>

### Blackjax

*Not a PPL but a library of MCMC samplers built on JAX.*

Can be used directly if you want to define your own log-probability density functions or can be used with several PPLs to define your model (make sure to translate it to a log-probability function).

Also provides building blocks for experimentation with new algorithms.

<iframe width="900" height="600" src="https://blackjax-devs.github.io/blackjax/index.html#hello-world" data-external="1"></iframe>

### Example Blackjax sampler: HMC

<iframe width="900" height="600" src="https://blackjax-devs.github.io/blackjax/autoapi/blackjax/mcmc/hmc/index.html" data-external="1"></iframe>

### Example Blackjax sampler: NUTS

<iframe width="900" height="600" src="https://blackjax-devs.github.io/blackjax/autoapi/blackjax/mcmc/nuts/index.html" data-external="1"></iframe>

### Which tool to choose?

All these tools are in active development (JAX was released and started shaking the field in 2018). Things are fast evolving. Reading blogs of main developers, posts on [Hacker News](https://en.wikipedia.org/wiki/Hacker_News), discourse forums, etc. helps to keep an eye on evolutions in the field.

[This recent conversation between Bob Carpenter (Stan core developer) and Ricardo Vieira (PyMC core developer) in the PyMC discourse forum](https://discourse.pymc.io/t/user-experience-python-vs-r-pymc-vs-stan-pytensor-vs-jax/16426) is interesting.

A lot of it also comes down to user preferences.

## Resources

### Bayesian computing

Some good resources to get started with Bayesian computing:

- the book [Probabilistic Programming & Bayesian Methods for Hackers](https://dataorigami.net/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/) by [Cameron Davidson-Pilon](https://github.com/CamDavidsonPilon) provides a code-based (using PyMC) and math-free introduction to Bayesian methods for the real beginner,
- [several resources on the PyMC website](https://www.pymc.io/projects/docs/en/stable/learn.html) including [intro Bayesian with PyMC](https://www.pymc.io/projects/docs/en/latest/learn/core_notebooks/pymc_overview.html),
- [NumPyro tutorials](https://num.pyro.ai/en/stable/tutorials/).

*More advanced: tutorials from Blackjax [Sampling Book Project](https://blackjax-devs.github.io/sampling-book/).*

### From Stan to a JAX-based PPL

The code to the classic Bayesian textbook [Statistical Rethinking](https://xcelab.net/rm/) by [Richard McElreath](https://www.eva.mpg.de/ecology/staff/richard-mcelreath/) got translated by various people to modern JAX-based PPLs and might help you transition from Stan:

<iframe width="900" height="400" src="https://xcelab.net/rm/" data-external="1"></iframe>
