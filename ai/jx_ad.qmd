---
title: Automatic differentiation
author: Marie-Hélène Burle
---

:::{.def}



:::

Considering the function `f`:

```{.python}
f = lambda x: x**3 + 2*x**2 - 3*x + 8
```

We can create a new function `dfdx` that computes the gradient of `f` w.r.t. `x`:

```{.python}
from jax import grad

dfdx = grad(f)
```

`dfdx` returns the derivatives

```{.python}
print(dfdx(1.))
```

```
4.0
```

## Composing transformations

Transformations can be composed:

```{.python}
print(jit(grad(f))(1.))
```

```
4.0
```

```{.python}
print(grad(jit(f))(1.))
```

```
4.0
```

## Forward and reverse modes

Other autodiff methods:

- Reverse-mode vector-Jacobian products: `jax.vjp`
- Forward-mode Jacobian-vector products: `jax.jvp`

## Higher-order differentiation

With a single variable, the `grad` function calls can be nested:

```{.python}
d2fdx = grad(dfdx)   # function to compute 2nd order derivatives
d3fdx = grad(d2fdx)  # function to compute 3rd order derivatives
...
```

With several variables:

- `jax.jacfwd` for forward-mode
- `jax.jacrev` for reverse-mode
