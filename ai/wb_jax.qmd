---
title: Accelerated array computing and flexible differentiation with JAX
author: Marie-Hélène Burle
---

:::{.def}

[JAX](https://jax.readthedocs.io/) is an open source Python library for high-performance array computing and flexible automatic differentiation.

High-performance computing is achieved by [asynchronous dispatch](https://en.wikipedia.org/wiki/Asynchrony_(computer_programming)), [just-in-time compilation](https://en.wikipedia.org/wiki/Just-in-time_compilation), the [XLA compiler for linear algebra](https://www.tensorflow.org/xla), and full compatibility with accelerators ([GPUs](https://en.wikipedia.org/wiki/Graphics_processing_unit) and [TPUs](https://en.wikipedia.org/wiki/Tensor_Processing_Unit)).

Automatic differentiation uses [Autograd](https://github.com/hips/autograd) and works with complex control flows (conditions, recursions), second and third-order derivatives, forward and reverse modes. This makes JAX ideal for machine learning and neural network libraries such as [Flax](https://flax.readthedocs.io/en/latest/) are built on it.

This webinar will give an overview of JAX's principles and functioning.

:::

*Coming up in spring 2024.*
<!-- [Slides](wb_jax_slides.qmd){.btn .btn-outline-primary} [(Click and wait: this reveal.js presentation may take a little time to load.)]{.inlinenote} -->
<!-- <br><br> -->

<!-- {{< video https://www.youtube.com/embed/ >}} -->
