---
title: Loading datasets
author: Marie-HÃ©lÃ¨ne Burle
bibliography: fl.bib
---

:::{.def}

Neither JAX nor Flax implement methods to load datasets since [PyTorch](https://github.com/pytorch/pytorch), [TensorFlow](https://github.com/tensorflow/datasets), and [Hugging Face](https://github.com/huggingface/datasets) already provide great APIs for this.

Let's use one of the most classic of all deep learning datasetsâ€”the MNIST [@lecun2010mnist]â€”to see how these APIs work.

:::

## Hugging Face Datasets

The [Datasets](https://github.com/huggingface/datasets) library from ðŸ¤— is a lightweight, framework-agnostic, and easy to use API to download datasets from the [Hugging Face Hub](https://huggingface.co/datasets). It uses [Apache Arrow](https://arrow.apache.org/)'s efficient caching system, allowing large datasets to be used on machines with small memory [@lhoest-etal-2021-datasets].

### Search dataset

Go to the [Hugging Face Hub](https://huggingface.co/datasets) and search through thousands of open source datasets provided by the community.

### Inspect dataset

You can get information on a dataset before downloading it.

Load the dataset builder for the dataset you are interested in:

```{python}
from datasets import load_dataset_builder
ds_builder = load_dataset_builder("mnist")
```

Get a description of the dataset:

```{python}
ds_builder.info.description
```

Get information on the features:

```{python}
ds_builder.info.features
```

### Download dataset and load in session

```{python}
from datasets import load_dataset

ds = load_dataset("mnist")
ds
```

:::{.note}

You need to have the [Pillow](https://python-pillow.org/) package installed for this to work since the MNIST is an image dataset.

:::

Let's explore our dataset dictionary:

```{python}
len(ds)
```

```{python}
ds.keys()
```

```{python}
ds['train']
```

```{python}
len(ds['train'])
```

```{python}
ds['train'][0]
```

```{python}
len(ds['train'][0])
```

```{python}
ds['train'][0].keys()
```

```{python}
ds['train'][0]['image']
```

```{python}
ds['train'][0]['label']
```

### Convert to JAX object

We need to convert our dataset to a JAX `Array` object:

```{.python}
dsj = ds.with_format("jax")
dsj
```

Printing `dsj` looks the same, but:

```{.python}
dsj['train'][0]
```

```
{'image': Array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
           0,   0],
        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
           0,   0],
       ...,
        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
           0,   0]], dtype=uint8),
 'label': Array(5, dtype=int32)}
```

```{.python}
dsj['train'][0]['image']
```

```
Array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
          0,   0],
       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
          0,   0],
       ...,
       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
          0,   0]], dtype=uint8)
```

```{.python}
dsj['train'][0]['label']
```

```
Array(5, dtype=int32)
```

```{.python}
dsj['train']['label'][:10]
```

```
Array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4], dtype=int32)
```

We can shuffle the data:

```{.python}
ds_shuffled = dsj.shuffle(seed=123)
ds_shuffled['train']['label'][:10]
```

```
Array([4, 4, 4, 1, 7, 8, 5, 2, 8, 3], dtype=int32)
```

For normalization and more complex operations, you will need the Hugging Face [transformers](https://github.com/huggingface/transformers) package.

## PyTorch

If you are familiar with PyTorch DataLoaders, this is an equally great option:

```{.python}
import torch
from torchvision import datasets, transforms

transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.1307,), (0.3081,))
])

train_data = datasets.MNIST(
    '~/projects/def-sponsor00/data',
    train=True, download=True, transform=transform)

test_data = datasets.MNIST(
    '~/projects/def-sponsor00/data',
    train=False, transform=transform)

# create DataLoaders
train_loader = torch.utils.data.DataLoader(
    train_data, batch_size=20, shuffle=True)

test_loader = torch.utils.data.DataLoader(
    test_data, batch_size=20, shuffle=False)
```

You can find more details in [our PyTorch course](https://mint.westdri.ca/ai/pt_mnist).

## TensorFlow Datasets

For those familiar with TensorFlow, here is an example from the Flax [Quick start](https://flax.readthedocs.io/en/latest/quick_start.html):

```{.python}
import tensorflow_datasets as tfds
import tensorflow as tf

def get_datasets(num_epochs, batch_size):
  """Load MNIST train and test datasets into memory."""
  
  train_ds = tfds.load('mnist', split='train')
  test_ds = tfds.load('mnist', split='test')

  # normalize train set
  train_ds = train_ds.map(
      lambda sample: {'image': tf.cast(sample['image'], tf.float32) / 255.0,
                      'label': sample['label']})
  # normalize test set
  test_ds = test_ds.map(
      lambda sample: {'image': tf.cast(sample['image'], tf.float32) / 255.0,
                      'label': sample['label']})
  
  # create shuffled dataset by allocating a buffer size of 1024
  # to randomly draw elements from
  train_ds = train_ds.repeat(num_epochs).shuffle(1024)

  # group into batches of batch_size and skip incomplete batch,
  # prefetch the next sample to improve latency
  train_ds = train_ds.batch(batch_size, drop_remainder=True).prefetch(1)

  # create shuffled dataset by allocating a buffer size of 1024
  # to randomly draw elements from
  test_ds = test_ds.shuffle(1024)

  # group into batches of batch_size and skip incomplete batch,
  # prefetch the next sample to improve latency
  test_ds = test_ds.batch(batch_size, drop_remainder=True).prefetch(1)

  return train_ds, test_ds
```

There is nothing wrong with using utilities from different libraries! Pick and choose the tools that serve your needs best.
