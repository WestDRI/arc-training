---
title: Loading datasets with Hugging Face
author: Marie-Hélène Burle
bibliography: fl.bib
---

:::{.def}

Neither JAX nor Flax implement methods to load datasets since [PyTorch](https://github.com/pytorch/pytorch), [TensorFlow](https://github.com/tensorflow/datasets), and [Hugging Face](https://github.com/huggingface/datasets) already provide great APIs for this. In this section, we will see how to load and pre-process a dataset using Hugging Face.

:::

The [Datasets](https://github.com/huggingface/datasets) library from Hugging Face is a lightweight, framework-agnostic, and easy to use API to download datasets from the [Hugging Face Hub](https://huggingface.co/datasets). It uses [Apache Arrow](https://arrow.apache.org/)'s efficient caching system, allowing large datasets to be used on machines with small memory [@lhoest-etal-2021-datasets].

Let's use one of the most classic of all deep learning datasets, the MNIST [@lecun2010mnist].

## Search dataset

Go to the [Hugging Face Hub](https://huggingface.co/datasets) and search through thousands of open source datasets provided by the community.

## Inspect dataset

You can get information on a dataset before downloading it.

Load the dataset builder for the dataset you are interested in:

```{python}
from datasets import load_dataset_builder
ds_builder = load_dataset_builder("mnist")
```

Get a description of the dataset:

```{python}
ds_builder.info.description
```

Get information on the features:

```{python}
ds_builder.info.features
```

## Download dataset and load in session

```{python}
from datasets import load_dataset

ds = load_dataset("mnist")
ds
```

:::{.note}

You need to have the [Pillow](https://python-pillow.org/) package installed for this to work since the MNIST is an image dataset.

:::

Let's explore our dataset dictionary:

```{python}
len(ds)
```

```{python}
ds.keys()
```

```{python}
ds['train']
```

```{python}
len(ds['train'])
```

```{python}
ds['train'][0]
```

```{python}
len(ds['train'][0])
```

```{python}
ds['train'][0].keys()
```

```{python}
ds['train'][0]['image']
```

```{python}
ds['train'][0]['label']
```

## Convert to JAX object

We need to convert our dataset to a JAX `Array` object:

```{.python}
dsj = ds.with_format("jax")
dsj
```

Printing `dsj` looks the same, but:

```{.python}
dsj['train'][0]
```

```
{'image': Array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
           0,   0],
        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
           0,   0],
       ...,
        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
           0,   0]], dtype=uint8),
 'label': Array(5, dtype=int32)}
```

```{.python}
dsj['train'][0]['image']
```

```
Array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
          0,   0],
       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
          0,   0],
       ...,
       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
          0,   0]], dtype=uint8)
```

```{.python}
dsj['train'][0]['label']
```

```
Array(5, dtype=int32)
```


```{.python}
for epoch in range(epochs):
        for batch in dsj["train"].iter(batch_size=32):
            x, y = batch["image"], batch["label"]
            ...
```

## Process the data

:::{.info}

## Which library to use to load datasets?

There are many equally great options to load datasets. If you are familiar with PyTorch DataLoaders, this is an equally great option:

```{.python}

```

For those familiar with TensorFlow, here is an example from the Flax [Quick start](https://flax.readthedocs.io/en/latest/quick_start.html):

```{.python}
import tensorflow_datasets as tfds
import tensorflow as tf

def get_datasets(num_epochs, batch_size):
  """Load MNIST train and test datasets into memory."""
  
  train_ds = tfds.load('mnist', split='train')
  test_ds = tfds.load('mnist', split='test')

  # normalize train set
  train_ds = train_ds.map(
      lambda sample: {'image': tf.cast(sample['image'], tf.float32) / 255.0,
                      'label': sample['label']})
  # normalize test set
  test_ds = test_ds.map(
      lambda sample: {'image': tf.cast(sample['image'], tf.float32) / 255.0,
                      'label': sample['label']})
  
  # create shuffled dataset by allocating a buffer size of 1024
  # to randomly draw elements from
  train_ds = train_ds.repeat(num_epochs).shuffle(1024)

  # group into batches of batch_size and skip incomplete batch,
  # prefetch the next sample to improve latency
  train_ds = train_ds.batch(batch_size, drop_remainder=True).prefetch(1)

  # create shuffled dataset by allocating a buffer size of 1024
  # to randomly draw elements from
  test_ds = test_ds.shuffle(1024)

  # group into batches of batch_size and skip incomplete batch,
  # prefetch the next sample to improve latency
  test_ds = test_ds.batch(batch_size, drop_remainder=True).prefetch(1)

  return train_ds, test_ds
```

:::
