---
title: Data augmentation
author: Marie-Hélène Burle
bibliography: jxai.bib
csl: diabetologia.csl
---

:::{.def}

Key to deep learning is data augmentation. This section explains what it is and what techniques we should use in our example.

:::

:::{.callout-note collapse="true"}

## Minimal necessary code from previous sections

```{.python}
base_dir = "<path-of-the-nabirds-dir>"
```

:::{.notenoit}

To be replaced by actual path: in our training cluster, the `base_dir` is at `/project/def-sponsor00/nabirds`:

```{.python}
base_dir = '/project/def-sponsor00/nabirds'
```

:::

```{python}
#| echo: false

base_dir = 'nabirds'
```

```{python}
import os
import polars as pl
import imageio.v3 as iio
import grain.python as grain

metadata = pl.read_parquet("metadata.parquet")
metadata_train = metadata.filter(pl.col("is_training_img") == 1)
cleaned_img_dir = os.path.join(base_dir, "cleaned_images")

class NABirdsDataset:
    """NABirds dataset class."""

    def __init__(self, metadata_file, data_dir):
        self.metadata_file = metadata_file
        self.data_dir = data_dir

    def __len__(self):
        return len(self.metadata_file)

    def __getitem__(self, idx):
        path = os.path.join(self.data_dir, self.metadata_file.get_column("path")[idx])
        img = iio.imread(path)
        species = self.metadata_file.get_column("species")[idx].replace("_", " ")
        subcategory = self.metadata_file.get_column("subcategory")[idx]
        if subcategory is not None:
            subcategory = subcategory.replace("_", " ")
        photographer = self.metadata_file.get_column("photographer")[idx].replace("_", " ")
        element = {
            "img": img,
            "species": species,
            "subcategory": subcategory,
            "photographer": photographer,
        }

        return element

nabirds_train = NABirdsDataset(metadata_train, cleaned_img_dir)
```

:::

## What is data augmentation?

Training deep learning models requires vast amounts of labelled data. The more diverse and numerous the data, the better the models will perform on new, unseen data.

A number of labelled datasets exist, but they are only so big. Moreover, for specific applications, you will need to fine-tune models on specific data for which there might not be any labelled data. And labelling data is costly and time-consuming. In some cases, it might even be impossible because little unlabelled data exist (think for instance of rare tumours).

Data augmentation is the artificial creation of new data by modifying the existing data in small ways. It is very powerful to avoid overfitting, particularly where datasets are small (if you were lucky enough to have a huge dataset, you wouldn't need to bother with data augmentation which would actually create an additional computationally costly step with extremely little benefit).

Xu *et al.* performed a comprehensive survey of image augmentation techniques for deep learning in 2023 [-@Xu_2023], but the field is evolving very fast with new techniques coming out all the time.

## Augmentation libraries

Data augmentation being such a classic technique, many libraries offer sets of augmentation tools. Among the very popular, we can list:

- [AlbumentationsX](https://github.com/albumentations-team/AlbumentationsX),
- [TorchVision transforms](https://docs.pytorch.org/vision/0.8/transforms.html),
- [skimage.transform](https://scikit-image.org/docs/0.25.x/api/skimage.transform.html) from [scikit-image](https://github.com/scikit-image/scikit-image).

There are others.

Amarù *et al.* created a curated repository of libraries for data augmentation in computer vision [-@jimaging9100232].

In this course, we will use [PIX](https://github.com/google-deepmind/dm_pix), a library built for JAX which provides low-level, JAX-native image processing primitives that can be directly jitted and vmapped.

*We are shifting paradigm here and moving from the CPU to the GPU. This is where JAX comes in.*

## Augmentation techniques

There are many techniques:

- Geometric transformations (flips, rotations, scaling, crops...).
- Color space transformations (brightness, contrast, gamma, hue, saturation, grayscale conversion, channel shuffling...).
- Noise transformations (Gaussian noise, blurring...).
- Occlusive transformations (erasing parts of the image).
- Mixing images (various techniques mixing images and appropriately applying the same treatment to labels).

## Choosing techniques

### How many to use?

The size of your dataset dictates how aggressively you should augment.

| Dataset size | Strategy | Recommended count |
| :--- | :--- | :--- |
| **Tiny (<1k images)** | **Heavy augmentation.** You should be worried about overfitting, so you need to create more artificial data | **4-6 techniques** |
| **Medium (1k - 100k)** | **Standard augmentation.** Balance variety with training speed | **3-4 techniques** |
| **Large (>100K images)** | **Light augmentation.** | **1-2 techniques**  |
| **Massive (>1M images)** | **No augmentation**. The data itself provides enough diversity. Augmentation will only slow down training. | **0 technique** |

<br>Our dataset contains about 50K images (including the evaluation set). This puts us in the medium category and a standard approach with **3 or 4 techniques** should be reasonable.

### Which ones to use?

The choice depends on the problem.

In our case, we are dealing with fine-grained birds identification. Colours are critical for species differentiation, so we don't want to mess with that. This means that playing with hue, solarization, color jitter, or gray scale could be a bad idea as it might invalidate the labels (making one species actually look like another). Transformations that do not preserve the aspect ratio (squashing, warping) could be bad too as they might change the shape of discriminant features.

Vertical flips or 90° rotations would not be very useful as they wouldn't produce realistic data ...

There are a lot of more advanced techniques out there (CutMix, SnapMix, TransMix, Attention Drop), but let's start with easier ones. For geometric techniques, let's do **random crops**, **horizontal flips**, and slight **rotations**.

We can also do some photometric augmentation as long as they don't invalidate the labels: brightness and contrast. If we want the model to be able to identify black and white pictures, we definitely need to remove the colour dependence by using a technique turning images to gray (with some probability and level of colour removal). If, on the other hand, we are only interested in having a model able to identify colour images, we want to stay away from this.

Let's consider for instance random **gamma adjustments**. They simulate different exposure levels and will improve the model's robustness and performance by making it less sensitive to variations in lighting conditions.

### Choosing the parameters

Picking the right bounds for each type of data augmentation involves balancing dataset diversity against image realism. If the range is too narrow, you don't get much benefit, if it's too wide, you might destroy critical features or create unrealistic images that confuse the model.

#### Default range

Check the industry-standards (look at the literature, ask an LLM, etc.).

For random crops, we don't want to go too hard or we will crop out the distinguishing features of the birds.

For gamma, for most computer vision tasks (natural images, object detection, classification), the industry-standard starting point is 0.8 to 1.2.

This range simulates subtle lighting variations—like a cloud passing over the sun or a slight difference in camera exposure—without washing out the image or making it too dark to see details. This should be good for us.

#### Domain specific ranges

You might want to adjust the values based on your specific data type.

For instance, you can increase the gamma range for OCR (document analysis) because scanned documents often have wildly varying contrast and because text usually remains legible even under extreme gamma.

None of this applies to our example.

#### Visual sanity check

Never set augmentation parameters blindly. Visualize some tests to ensure the data you are using to train is still reasonable (and to make sure that you aren't messing something up and getting totally absurd results!).

Let's test various gamma values on the first 4 images in the training set:

```{python}
import dm_pix as pix
import PIL.Image as Image
import numpy as np
import matplotlib.pyplot as plt
import jax.numpy as jnp
```

Let's write a helper function that applies PIX deterministic [adjust_gamma](https://dm-pix.readthedocs.io/en/latest/api.html#adjust-gamma) function to a JAX array image, then covert it to a PIL image that we can display:

```{python}
def apply_gamma(img, gamma):
    """
    Apply gamma transformation to a JAX array image
    then turn it back into a PIL image for display.
    """
    new_img = pix.adjust_gamma(
        image=img,
        gamma=gamma
    )
    pil_img = Image.fromarray(
        np.asarray(new_img * 255.).astype(np.uint8), 'RGB'
    )
    gamma = round(gamma, 1)
    return pil_img, gamma
```

This next function will take a NumPy image from our Dataset class, turn it to a JAX array, apply our helper function `apply_gamma`, and display the result:

```{python}
def show_tests(img, gamma_range):
    """
    Turn the image into a JAX array,
    run our apply_gamma function on it
    (apply gamma then turn back to PIL image),
    plot the PIL image.
    """
    jnp_img = jnp.array(img, dtype=jnp.float32) / 255.
    pics = []
    gammas = []

    for i in gamma_range:
        pics.append(apply_gamma(jnp_img, i)[0])
        gammas.append(apply_gamma(jnp_img, i)[1])

    fig, axes = plt.subplots(3, 4, figsize=(8, 6))
    axes = axes.flatten()

    for i, ax in enumerate(axes):
        ax.imshow(pics[i])
        ax.axis('off')
        ax.set_title(f'Gamma = {gammas[i]}', fontsize=9)
    plt.tight_layout()
    plt.show()
```

Now we can apply it to a few images to get an idea of the effect and ensure nothing weird is going on. This will help us catch a coding mistake that could ruin the whole training process:

```{python}
show_tests(nabirds_train[0]['img'], np.linspace(0.1, 3, 12))
```

0.4 to 1.2 seem ok.

```{python}
show_tests(nabirds_train[1]['img'], np.linspace(0.1, 3, 12))
```

0.4 to 2.2 seem reasonable for this image.

```{python}
show_tests(nabirds_train[2]['img'], np.linspace(0.1, 3, 12))
```

0.4 to 2.5 seem ok for this one.

```{python}
show_tests(nabirds_train[3]['img'], np.linspace(0.1, 3, 12))
```

0.4 to 1.4 seem reasonable here.

For training, we will use [random gamma](https://dm-pix.readthedocs.io/en/latest/api.html#random-gamma) as one of our transformation (we don't want the deterministic function here since we want different transformations randomly applied at each epoch) with the min and max values set a bit more broadly than what is standard as our images seem to handle it OK. Let's go with a min and max of 0.4 and 1.2 respectively.

#### Validation check

You can adjust the probabilities and magnitudes of the various augmentation techniques you chose based on the validation performance you get during training.

You can train a small version of your model (or for fewer epochs) and check how the validation loss improves with various variations of your augmentation strategy.

I recently gave a [webinar on MLflow](/ai/mlops/wb_mlflow.qmd) which would be the perfect tool for this kind of comparison.

## Applying our augmentation strategy

