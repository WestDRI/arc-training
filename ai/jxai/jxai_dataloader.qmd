---
title: Data loader
author: Marie-Hélène Burle
---

:::{.def}



:::

:::{.callout-note collapse="true"}

## Minimal necessary code from previous sections

```{.python}
base_dir = "<path-of-the-nabirds-dir>"
```

:::{.note}

To be replaced by proper path.

:::

```{python}
#| echo: false

base_dir = "nabirds"
```

```{python}
import os
import polars as pl
import imageio.v3 as iio

metadata = pl.read_parquet("metadata.parquet")
metadata_train = metadata.filter(pl.col("is_training_img") == 1)

class NABirdsDataset:
    """NABirds dataset class."""
    def __init__(self, metadata_file, data_dir):
        self.metadata_file = metadata_file
        self.data_dir = data_dir
    def __len__(self):
        return len(self.metadata_file)
    def __getitem__(self, idx):
        path = os.path.join(
            self.data_dir,
            self.metadata_file.get_column('path')[idx]
        )
        img = iio.imread(path)
        id = self.metadata_file.get_column('id')[idx].replace('_', ' ')
        photographer = self.metadata_file.get_column('photographer')[idx].replace('_', ' ')
        element = {
            'image': img,
            'id': id,
            'photographer': photographer,
        }
        return element

cleaned_img_dir = os.path.join(base_dir, "cleaned_images")

nabirds_train = NABirdsDataset(
    metadata_train,
    cleaned_img_dir
)
```

:::

## Goal of a DataLoader

We can access elements of our Dataset class (as we did in the previous section) with:

```{python}
for i, element in enumerate(nabirds_train):
    print(element['image'].shape)
    if i == 3:
        break
```

And we can return the next object by creating an iterator from of iterable dataset:

```{python}
next(iter(nabirds_train))
```

:::{.note}

We get all those 0 because we padded images with black.

:::

But all this is extremely limited.

DataLoaders feed data to the model during training. They handle batching, shuffling, sharding across machines, the number of epochs, etc.

## Grain DataLoaders

Creating a DataLoader with the JAX AI stack is done [Grain](https://github.com/google/grain), but it can just as well be done using [PyTorch](https://github.com/pytorch/pytorch), [TensorFlow Datasets](https://github.com/tensorflow/datasets), [Hugging Face Datasets](https://github.com/huggingface/datasets), or any method you are used to. That's the advantage of the modular philosophy that the stack relies on. Grain has the advantage of being extremely efficient and not relying on a huge set of dependencies such as PyTorch or TensorFlow.

In Grain, a DataLoader requires 3 components:

- A data source: 
- Transforms:
- Sampler:

## Data source

We already have that: it is our instance of Dataset class that we called `nabirds_train`.

## Transformations

We need to split the data into batches. Batches can be defined with the  `grain.Batch` method as a DataLoader transformation.

Let's use batches of 32 with `grain.Batch(batch_size=32)`.

:::{.callout-note collapse="true"}

## How to choose the batch size?

The batch size is a crucial hyperparameter: it impacts your training speed, model stability, and final accuracy.

### Default strategy

If you are unsure where to start, **use a batch size of 32**.

32 is small enough to provide a regularizing effect (helping the model generalize) but large enough to benefit from parallel processing on GPUs.

### Standard values

Always use powers of 2 (32, 64, 128, 256) because GPUs and CPUs are optimized for binary operations, and this aligns memory allocation efficiently.

|  | **Small batch size**  | **Large batch size**  |
| :--- | :--- | :--- |
| **Training speed** | Slower: less efficient use of GPU | Faster: maximizes GPU throughput |
| **Generalization** | Better: the "noise" in the gradient helps the model escape sharp local minima | Worse: can lead to overfitting |
| **Convergence** | Noisy training curve: loss fluctuates | Smoother training curve: stable descent |
| **Memory usage** | Low: good for limited VRAM | High: risk of OOM |

### Tuning the batch size

#### Ceiling

Your maximum batch size is dictated by your GPU memory (VRAM).

If you hit an [out of memory (OOM) error](https://en.wikipedia.org/wiki/Out_of_memory), you need to back down to the the previous successful power of 2 (this is your hardware maximum).

#### Performance

Just because you *can* fit a batch size of 4096 doesn't mean you *should*.

If training is stable but slow, double to 64, then double again to 128. You can increase the batch size to the hardware maximum to speed up epochs.

If the model overfits or diverges, try reducing the batch size. The "noisy" updates act like regularization, preventing the model from memorizing the data too perfectly.

### Advanced techniques

- **Gradient accumulation:**

If you need a batch size of 64 for stability but your GPU only fits 16, you can use *gradient accumulation*. You process 4 mini-batches of 16, accumulate the gradients, and update the weights once. This mathematically simulates a batch size of 64.

- **Dynamic batching:**

Some advanced training regimes start with a small batch size to stabilize early training and increase it over time to speed up convergence (similar to learning rate decay).

### Learning rate

If you change your batch size significantly, you should adjust your learning rate.

A rule that works well until you get to very large batch sizes is to double the learning rate when you double the batch size.

:::

## Samplers

### Sequential sampler

Grain comes with a [basic sequential sampler](https://google-grain.readthedocs.io/en/latest/_autosummary/grain.samplers.SequentialSampler.html).

```{python}
import grain.python as grain

nabirds_train_seqsampler = grain.SequentialSampler(
    num_records=4
)
```

```{python}
for record_metadata in nabirds_train_seqsampler:
    print(record_metadata)
```

### Index sampler

Grain [index sampler](https://google-grain.readthedocs.io/en/latest/_autosummary/grain.samplers.IndexSampler.html) xxx


```{python}
nabirds_train_isampler = grain.IndexSampler(
    num_records=200,
    shuffle=True,
    seed=0
)
```

```{python}
for i, record_metadata in enumerate(nabirds_train_isampler):
  print(record_metadata)
  if i == 3:
      break
```

## DataLoaders

We now have our source, samplers, and transformation, so we can build experiment with Grain DataLoaders.

### With sequential sampler

```{python}
nabirds_train_dl = grain.DataLoader(
    data_source=nabirds_train,
    sampler=nabirds_train_seqsampler,
    worker_count=0
)
```


```{python}
import matplotlib.pyplot as plt

fig = plt.figure(figsize=(8, 8))

for i, element in enumerate(nabirds_train_dl):
    ax = plt.subplot(2, 2, i + 1)
    plt.tight_layout()
    ax.set_title(
        f'Element {i}\nIdentification: {element['id']}\nPicture by {element['photographer']}',
        fontsize=9
    )
    ax.axis('off')
    plt.imshow(element['image'])

plt.show()
```

:::{.note}

Notice that, unlike [last time we displayed some images](jxai_preprocess.qmd#display-a-sample), we aren't looping through our Dataset (`nabirds_train`) anymore, but through our DataLoader (`nabirds_train_dl`).

Because we set the number of records to 4 in the sampler, we don't have to break the loop.

:::

### With index sampler

```{python}
nabirds_train_dl = grain.DataLoader(
    data_source=nabirds_train,
    sampler=nabirds_train_isampler,
    worker_count=0
)
```



```{python}
fig = plt.figure(figsize=(8, 8))

for i, element in enumerate(nabirds_train_dl):
    ax = plt.subplot(2, 2, i + 1)
    plt.tight_layout()
    ax.set_title(
        f'Element {i}\nIdentification: {element['id']}\nPicture by {element['photographer']}',
        fontsize=9
    )
    ax.axis('off')
    plt.imshow(element['image'])
    if i == 3:
        plt.show()
        break
```

### Adding batching

```{python}
nabirds_train_dl = grain.DataLoader(
    data_source=nabirds_train,
    sampler=nabirds_train_isampler,
    worker_count=0,
    operations=[
        grain.Batch(batch_size=32)
    ]
)
```

```{python}
for i, element in enumerate(nabirds_train_dl):
    print(element['image'].shape)
    if i == 0:
        plt.show()
        break
```

As you can see, the batch size got added as an extra dimension.
