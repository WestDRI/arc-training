---
title: DataLoaders
author: Marie-Hélène Burle
bibliography: jxai.bib
csl: diabetologia.csl
---

:::{.def}

A critical part of deep learning is the loading of data to the model during the training loops.

DataLoaders handle the choice of which sample to load and in what order; they optimize the process in parallel by managing workers; they set several hyperparameters such as batch size and number of epochs.

In this section we explore DataLoaders with the [Grain](https://github.com/google/grain) library [-@grain2023github].

:::

:::{.callout-note collapse="true"}

## Minimal necessary code from previous sections

```{.python}
base_dir = "<path-of-the-nabirds-dir>"
```

:::{.notenoit}

To be replaced by actual path: in our training cluster, the `base_dir` is at `/project/def-sponsor00/nabirds`:

```{.python}
base_dir = '/project/def-sponsor00/nabirds'
```

:::

```{python}
#| echo: false

base_dir = 'nabirds'
```

```{python}
import os
import polars as pl
import imageio.v3 as iio
import grain.python as grain

metadata = pl.read_parquet("metadata.parquet")
metadata_train = metadata.filter(pl.col("is_training_img") == 1)
metadata_val = metadata.filter(pl.col("is_training_img") == 0)
cleaned_img_dir = os.path.join(base_dir, "cleaned_images")

class NABirdsDataset:
    """NABirds dataset class."""
    def __init__(self, metadata_file, data_dir):
        self.metadata_file = metadata_file
        self.data_dir = data_dir

    def __len__(self):
        return len(self.metadata_file)

    def __getitem__(self, idx):
        path = os.path.join(self.data_dir, self.metadata_file.get_column('path')[idx])
        img = iio.imread(path)
        class_id = self.metadata_file.get_column('class_id')[idx]
        species = self.metadata_file.get_column('species')[idx].replace('_', ' ')
        subcategory = self.metadata_file.get_column('subcategory')[idx]
        if subcategory is not None:
            subcategory = subcategory.replace('_', ' ')
        photographer = self.metadata_file.get_column('photographer')[idx].replace('_', ' ')
        element = {
            'img': img,
            'class_id': class_id,
            'species': species,
            'subcategory': subcategory,
            'photographer': photographer,
        }

        return element

nabirds_train = NABirdsDataset(metadata_train, cleaned_img_dir)
nabirds_val = NABirdsDataset(metadata_val, cleaned_img_dir)
```

:::

## Goal of DataLoaders

We can access elements of our Dataset class (as we did in the previous section) with:

```{python}
for i, element in enumerate(nabirds_train):
    print(element['img'].shape)
    if i == 3:
        break
```

And we can return the next object by creating an iterator from of iterable dataset:

```{python}
next(iter(nabirds_train))
```

But all this is extremely limited.

DataLoaders feed data to the model during training. They handle batching, shuffling, sharding across machines, the number of epochs, etc. All things that would be very inconvenient to implement from scratch.

## Grain DataLoaders

The JAX AI stack includes the [Grain](https://github.com/google/grain) library [-@grain2023github] to create DataLoaders, but it can also be done using [PyTorch](https://github.com/pytorch/pytorch), [TensorFlow Datasets](https://github.com/tensorflow/datasets), [Hugging Face Datasets](https://github.com/huggingface/datasets), or any method you are used to. That's the advantage of the modular philosophy that the stack relies on. Grain is extremely efficient and does not rely on a huge set of dependencies as PyTorch and TensorFlow do.

In Grain, a DataLoader requires 3 components:

- A data source
- Transforms
- A sampler

## Data source

We already have that: our data sources are our instances of Dataset class `nabirds_train` and `nabirds_val` for training and validation respectively.

## Transformations

We need to split the data into batches. Batches can be defined with the  `grain.Batch` method as a DataLoader transformation.

Let's use batches of 32 with `grain.Batch(batch_size=32)`.

:::{.callout-note collapse="true"}

## How to choose the batch size?

The batch size is a crucial hyperparameter: it impacts your training speed, model stability, and final accuracy.

### Default strategy

If you are unsure where to start, **use a batch size of 32**.

32 is small enough to provide a regularizing effect (helping the model generalize) but large enough to benefit from parallel processing on GPUs.

### Standard values

Always use powers of 2 (32, 64, 128, 256) because GPUs and CPUs are optimized for binary operations, and this aligns memory allocation efficiently.

|  | **Small batch size**  | **Large batch size**  |
| :--- | :--- | :--- |
| **Training speed** | Slower: less efficient use of GPU | Faster: maximizes GPU throughput |
| **Generalization** | Better: the "noise" in the gradient helps the model escape sharp local minima | Worse: can lead to overfitting |
| **Convergence** | Noisy training curve: loss fluctuates | Smoother training curve: stable descent |
| **Memory usage** | Low: good for limited VRAM | High: risk of OOM |

### Tuning the batch size

#### Ceiling

Your maximum batch size is dictated by your GPU memory (VRAM).

If you hit an [out of memory (OOM) error](https://en.wikipedia.org/wiki/Out_of_memory), you need to back down to the the previous successful power of 2 (this is your hardware maximum).

#### Performance

Just because you *can* fit a batch size of 4096 doesn't mean you *should*.

If training is stable but slow, double to 64, then double again to 128. You can increase the batch size to the hardware maximum to speed up epochs.

If the model overfits or diverges, try reducing the batch size. The "noisy" updates act like regularization, preventing the model from memorizing the data too perfectly.

### Advanced techniques

- **Gradient accumulation:**

If you need a batch size of 64 for stability but your GPU only fits 16, you can use *gradient accumulation*. You process 4 mini-batches of 16, accumulate the gradients, and update the weights once. This mathematically simulates a batch size of 64.

- **Dynamic batching:**

Some advanced training regimes start with a small batch size to stabilize early training and increase it over time to speed up convergence (similar to learning rate decay).

### Learning rate

If you change your batch size significantly, you should adjust your learning rate.

A rule that works well until you get to very large batch sizes is to double the learning rate when you double the batch size.

:::

## Samplers

There are 2 types of Grain samplers: sequential and index samplers.

### Sequential sampler

Grain comes with a [basic sequential sampler](https://google-grain.readthedocs.io/en/latest/_autosummary/grain.samplers.SequentialSampler.html).

```{python}
import grain.python as grain

nabirds_train_seqsampler = grain.SequentialSampler(
    num_records=4
)

for record_metadata in nabirds_train_seqsampler:
    print(record_metadata)
```

### Index sampler

Grain [index sampler](https://google-grain.readthedocs.io/en/latest/_autosummary/grain.samplers.IndexSampler.html) is the one you should use as it allows for global shuffling of the dataset, setting the number of epochs, etc.

```{python}
nabirds_train_isampler = grain.IndexSampler(
    num_records=200,
    shuffle=True,
    seed=0
)

for i, record_metadata in enumerate(nabirds_train_isampler):
  print(record_metadata)
  if i == 3:
      break
```

## Let's experiment with DataLoaders

### With sequential sampler

```{python}
nabirds_train_dl = grain.DataLoader(
    data_source=nabirds_train,
    sampler=nabirds_train_seqsampler,
    worker_count=0
)
```

We can plot images in this sequential DataLoader:

```{python}
import matplotlib.pyplot as plt

fig = plt.figure(figsize=(8, 9))

for i, element in enumerate(nabirds_train_dl):
    ax = plt.subplot(2, 2, i + 1)
    plt.tight_layout()
    ax.set_title(
        f"""
        Species: {element['species']}
        Additional information: {element['subcategory']}
        Picture by {element['photographer']}
        """,
        fontsize=9,
        linespacing=1.5
    )
    ax.axis('off')
    plt.imshow(element['img'])

plt.show()
```

:::{.note}

Notice that, unlike [last time we displayed some images](jxai_preprocess.qmd#display-a-sample), we aren't looping through our Dataset (`nabirds_train`) anymore, but through our DataLoader (`nabirds_train_dl`).

Because we set the number of records to 4 in the sampler, we don't have to break the loop.

:::

### With index sampler

```{python}
nabirds_train_dl = grain.DataLoader(
    data_source=nabirds_train,
    sampler=nabirds_train_isampler,
    worker_count=0
)
```

Let's plot these as well:

```{python}
fig = plt.figure(figsize=(8, 9))

for i, element in enumerate(nabirds_train_dl):
    ax = plt.subplot(2, 2, i + 1)
    plt.tight_layout()
    ax.set_title(
        f"""
        Species: {element['species']}
        Additional information: {element['subcategory']}
        Picture by {element['photographer']}
        """,
        fontsize=9,
        linespacing=1.5
    )
    ax.axis('off')
    plt.imshow(element['img'])
    if i == 3:
        plt.show()
        break
```

### Adding batch sizes

You can add the batch size in the `operations` argument of `grain.DataLoader`:

```{python}
nabirds_train_dl = grain.DataLoader(
    data_source=nabirds_train,
    sampler=nabirds_train_isampler,
    worker_count=0,
    operations=[
        grain.Batch(batch_size=32)
    ]
)
```

## 

```{python}
print(nabirds_train[0]['img'].dtype)
```

```{python}
def normalize(img):
    # Image preprocessing matches the one of pretrained ViT
    mean = np.array([0.5, 0.5, 0.5], dtype=np.float32)
    std = np.array([0.5, 0.5, 0.5], dtype=np.float32)
    img = img.astype(np.float32) / 255.0
    return (img - mean) / std
```

```{python}
def get_transform(fn):
    def wrapper(batch):
        batch['img'] = [
            fn(img) for img in batch['img']
        ]
        # batch['class_id'] = [
            # label for label in batch['class_id']
        # ]
        return batch
    return wrapper
```

```{python}
train_normalize = get_transform(normalize)
```

```{python}
train_normalized = train_normalize(nabirds_train)
```

## The DataLoaders we will use

Now that we have played with various DataLoaders to understand how they work, here are the DataLoaders we will actually use to train the model:

```{python}
seed = 123
train_batch_size = 32
val_batch_size = 2 * train_batch_size

# Train set sampler:
train_sampler = grain.IndexSampler(
    len(train_normalized),
    shuffle=True,                      # We shuffle the training set
    seed=seed,
    shard_options=grain.NoSharding(),  # No sharding for a single-device setup
    num_epochs=1
)

# Train set DataLoader:
train_loader = grain.DataLoader(
    data_source=train_normalized,
    sampler=train_sampler,
    worker_count=4,        # Number of child processes to parallelize the transformations
    worker_buffer_size=2,  # Output batches to produce in advance per worker
    operations=[
        grain.Batch(train_batch_size, drop_remainder=True)
    ]
)
```

We can also create the sampler and DataLoader for the validation set:

```{python}
# Validation set sampler:
val_sampler = grain.IndexSampler(
    len(nabirds_val),
    shuffle=False,                     # We don't shuffle the validation set
    seed=seed,
    shard_options=grain.NoSharding(),
    num_epochs=1
)

# Validation set DataLoader:
val_loader = grain.DataLoader(
    data_source=nabirds_val,
    sampler=val_sampler,
    worker_count=4,
    worker_buffer_size=2,
    operations=[
        grain.Batch(val_batch_size)
    ]
)
```
