---
title: The JAX AI stack
author: Marie-Hélène Burle
---

:::{.def}

*Content from [the course slides](jxai_jaxstack_slides.qmd) for easier browsing.*

:::

## What is JAX?

JAX is a high-performance accelerator-oriented array computing library for Python developed by Google. It allows composition, JIT-compilation, transformation, and automatic differentiation of numerical programs.

It provides NumPy-like and lower-level APIs.

It also requires strict functional programming.

## Why JAX?

### Fast

- **Default data type suited for deep learning**

  Like [PyTorch](https://pytorch.org/), uses `float32` as default. This level of precision is suitable for deep learning and increases efficiency (by contrast, [NumPy](https://numpy.org/) defaults to `float64`).

- **[JIT compilation](https://en.wikipedia.org/wiki/Just-in-time_compilation)**

- The same code can run on [CPUs](https://en.wikipedia.org/wiki/Central_processing_unit) or on **accelerators** ([GPUs](https://en.wikipedia.org/wiki/Graphics_processing_unit) and [TPUs](https://en.wikipedia.org/wiki/Tensor_Processing_Unit))

- **[XLA (Accelerated Linear Algebra)](https://github.com/openxla/xla) optimization**

- **Asynchronous dispatch**

- **Vectorization, data parallelism, and sharding**

  All levels of shared and distributed memory parallelism are supported.

### Great AD

```{dot}
//| echo: false
//| fig-height: 450px

strict digraph {

bgcolor="transparent"
graph [fontname="Inconsolata, sans-serif"]
node [fontname="Inconsolata, sans-serif", fontsize=15]

01 [label="Autodiff method", shape=underline, group=g1, group=g1, fontcolor=gray55, color=gray55]
1 [label="Static graph\nand XLA", shape=plaintext, group=g1, group=g1]
2 [label="Dynamic graph", shape=plaintext, group=g1]
4 [label="Dynamic graph\nand XLA", shape=plaintext, group=g1]
5 [label="Pseudo-dynamic\nand XLA", shape=plaintext, group=g1]

02 [label="Framework", shape=underline, group=g2, fontcolor=gray55, color=gray55]
a [label="TensorFlow", shape=oval, group=g2, color=darkorange4, fontcolor=darkorange4]
b [label="PyTorch", shape=oval, group=g2, color=chocolate, fontcolor=chocolate]
d [label="TensorFlow2", shape=oval, group=g2, color=darkorange4, fontcolor=darkorange4]
e [label="JAX", shape=oval, group=g2, color=deepskyblue3, fontcolor=deepskyblue3]

03 [label=Advantage, shape=underline, group=g3, fontcolor=gray55, color=gray55]
7 [label="Mostly\noptimized AD", shape=plaintext, fontcolor=darkolivegreen, group=g3]
8 [label="Convenient", shape=plaintext, fontcolor=darkolivegreen, group=g3]
9 [label="Convenient", shape=plaintext, fontcolor=darkolivegreen, group=g3]
10 [label="Convenient and\nmostly optimized AD", shape=plaintext, fontcolor=darkolivegreen, group=g3]

04 [label=Disadvantage, shape=underline, group=g4, fontcolor=gray55, color=gray55]
A [label="Manual writing of IR", shape=plaintext, fontcolor=darkorchid2, group=g4]
B [label="Limited AD optimization", shape=plaintext, fontcolor=darkorchid2, group=g4]
D [label="Disappointing speed", shape=plaintext, fontcolor=darkorchid2, group=g4]
E [label="Pure functions", shape=plaintext, fontcolor=darkorchid2, group=g4]

{rank=same; 01 02 03 04}
{rank=same; 1 a 7 A}
{rank=same; 2 b 8 B}
{rank=same; 4 d 9 D}
{rank=same; 5 e 10 E}

01 -> 02 -> 03 -> 04 [style=invis]
1 -> a -> 7 -> A [style=invis]
2 -> b -> 8 -> B [style=invis]
4 -> d -> 9 -> D [style=invis]
5 -> e -> 10 -> E [style=invis]

01 -> 1 [style=invis]
1 -> 2 -> 4 -> 5 [color=gray55]
02 -> a -> b -> d -> e [style=invis]
03 -> 7 -> 8 -> 9 -> 10 [style=invis]
04 -> A -> B -> D -> E [style=invis]

}
```

[summarized from [a blog post](https://www.stochasticlifestyle.com/engineering-trade-offs-in-automatic-differentiation-from-tensorflow-and-pytorch-to-jax-and-julia/) by [Chris Rackauckas](https://chrisrackauckas.com/)]{.figure-caption}

### Close to the math

Considering the function `f`:

```{.python}
f = lambda x: x**3 + 2*x**2 - 3*x + 8
```

We can create a new function `dfdx` that computes the gradient of `f` w.r.t. `x`:

```{.python}
from jax import grad

dfdx = grad(f)
```

`dfdx` returns the derivatives:

```{.python}
print(dfdx(1.))
```

```
4.0
```

### Forward and reverse modes

- reverse-mode vector-Jacobian products: `jax.vjp`
- forward-mode Jacobian-vector products: `jax.jvp`

### Higher-order differentiation

With a single variable, the `grad` function calls can be nested:

```{.python}
d2fdx = grad(dfdx)   # function to compute 2nd order derivatives
d3fdx = grad(d2fdx)  # function to compute 3rd order derivatives
...
```

With several variables, you have to use the functions:

- `jax.jacfwd` for forward-mode,
- `jax.jacrev` for reverse-mode.

## How does it work?

```{dot}
//| echo: false
//| fig-height: 600px

strict digraph {

node [fontname="Inconsolata, sans-serif"]
bgcolor="transparent"

tracer  [label=Tracing, shape=rectangle, color=darkviolet, fontcolor=darkviolet]
jit [label="Transformation", shape=rectangle, color=chocolate, fontcolor=chocolate]
xla [label="Accelerated\n Linear Algebra \n(XLA)", shape=rectangle, color=deeppink3, fontcolor=deeppink3]
transform [label=" Transformations ", shape=rectangle, color=chocolate, fontcolor=chocolate]

CPU [shape=octagon, color=darkslategray4, fontcolor=darkslategray4]
GPU [shape=octagon, color=darkslategray4, fontcolor=darkslategray4]
TPU [shape=octagon, color=darkslategray4, fontcolor=darkslategray4]


py [label="Pure Python\nfunctions", color=gray50, fontcolor=gray50]
jaxpr [label="Jaxprs\n(JAX expressions)\nintermediate\nrepresentation\n(IR)", color=gray30, fontcolor=gray30]
hlo [label="High-level\noptimized (HLO)\nprogram", color=gray10, fontcolor=gray10]

py -> tracer [dir=none]
tracer -> jaxpr
jaxpr -> jit [dir=none]
jit -> hlo
hlo -> xla [dir=none]

xla -> CPU [shape=doubleoctagon]
xla -> GPU
xla -> TPU

jaxpr -> transform [dir=both, minlen=3]
{rank=same; jaxpr transform}

}
```

```{dot}
//| echo: false
//| fig-height: 600px

strict digraph {

node [fontname="Inconsolata, sans-serif"]
bgcolor="transparent"

tracer  [label=Tracing, shape=rectangle, color=darkviolet, fontcolor=darkviolet]
jit [label=" Just-in-time \n(JIT)\ncompilation", shape=rectangle, color=chocolate, fontcolor=chocolate]
xla [label="Accelerated\n Linear Algebra \n(XLA)", shape=rectangle, color=deeppink3, fontcolor=deeppink3]
transform [label="Vectorization\nParallelization\n   Differentiation  ", shape=rectangle, color=chocolate, fontcolor=chocolate]

CPU [shape=octagon, color=darkslategray4, fontcolor=darkslategray4]
GPU [shape=octagon, color=darkslategray4, fontcolor=darkslategray4]
TPU [shape=octagon, color=darkslategray4, fontcolor=darkslategray4]


py [label="Pure Python\nfunctions", color=gray50, fontcolor=gray50]
jaxpr [label="Jaxprs\n(JAX expressions)\nintermediate\nrepresentation\n(IR)", color=gray30, fontcolor=gray30]
hlo [label="High-level\noptimized (HLO)\nprogram", color=gray10, fontcolor=gray10]

py -> tracer [dir=none]
tracer -> jaxpr
jaxpr -> jit [dir=none]
jit -> hlo
hlo -> xla [dir=none]

xla -> CPU [shape=doubleoctagon]
xla -> GPU
xla -> TPU

jaxpr -> transform [dir=both, minlen=3]
{rank=same; jaxpr transform}

}
```

```{dot}
//| echo: false
//| fig-height: 600px

strict digraph {

node [fontname="Inconsolata, sans-serif"]
bgcolor="transparent"

tracer  [label=Tracing, shape=rectangle, color=darkviolet, fontcolor=darkviolet]
jit [label="jax.jit", shape=rectangle, color=chocolate, fontcolor=chocolate]
xla [label="Accelerated\n Linear Algebra \n(XLA)", shape=rectangle, color=deeppink3, fontcolor=deeppink3]
transform [label="jax.vmap\njax.pmap\njax.grad", shape=rectangle, color=chocolate, fontcolor=chocolate]

CPU [shape=octagon, color=darkslategray4, fontcolor=darkslategray4]
GPU [shape=octagon, color=darkslategray4, fontcolor=darkslategray4]
TPU [shape=octagon, color=darkslategray4, fontcolor=darkslategray4]


py [label="Pure Python\nfunctions", color=gray50, fontcolor=gray50]
jaxpr [label="Jaxprs\n(JAX expressions)\nintermediate\nrepresentation\n(IR)", color=gray30, fontcolor=gray30]
hlo [label="High-level\noptimized (HLO)\nprogram", color=gray10, fontcolor=gray10]

py -> tracer [dir=none]
tracer -> jaxpr
jaxpr -> jit [dir=none]
jit -> hlo
hlo -> xla [dir=none]

xla -> CPU [shape=doubleoctagon]
xla -> GPU
xla -> TPU

jaxpr -> transform [dir=both, minlen=3]
{rank=same; jaxpr transform}

}
```

## JAX for AI

### Not itself a DL library

```{dot}
//| echo: false

strict digraph {

bgcolor="transparent"
node [fontname="Inconsolata, sans-serif"]

jx [label="JAX", fontcolor="#9D22B2", color="#9D22B2"]

dl [label="Deep learning", fontcolor="#5E98F6", shape=plaintext]
ll [label="LLMs", fontcolor="#5E98F6", shape=plaintext]
op [label="Optimizers", fontcolor="#5E98F6", shape=plaintext]
so [label="Solvers", fontcolor="#5E98F6", shape=plaintext]
pp [label="Probabilistic\nprogramming", fontcolor="#5E98F6", shape=plaintext]
pm [label="Probabilistic\nmodeling", fontcolor="#5E98F6", shape=plaintext]
ph [label="Physics\nsimulations", fontcolor="#5E98F6", shape=plaintext]

{ll so ph} -> jx [dir=back, color="#5E98F6"]
jx -> {pp dl pm op} [color="#5E98F6"]

}
```

### A sublanguage ideal for DL

```{dot}
//| echo: false

strict digraph {

bgcolor="transparent"
node [fontname="Inconsolata, sans-serif"]

jx [label="JAX", fontcolor="#9D22B2", color="#9D22B2"]

dl [label="Deep learning", fontcolor="#5E98F6", shape=plaintext]
ll [label="LLMs", fontcolor=gray55, shape=plaintext]
op [label="Optimizers", fontcolor="#5E98F6", shape=plaintext]
so [label="Solvers", fontcolor=gray55, shape=plaintext]
pp [label="Probabilistic\nprogramming", fontcolor=gray55, shape=plaintext]
pm [label="Probabilistic\nmodeling", fontcolor=gray55, shape=plaintext]
ph [label="Physics\nsimulations", fontcolor=gray55, shape=plaintext]

{ll so ph} -> jx [dir=back, color=gray55]
jx -> {pp pm} [color=gray55]
jx -> {dl op} [color="#5E98F6"]

}
```

## The JAX AI stack

A modular approach:

![from the [JAX AI Stack website](https://docs.jaxstack.ai/en/latest/index.html)](img/jx_ecosystem.png){fig-alt="noshadow"}
