---
title: Model and training strategy
author: Marie-Hélène Burle
bibliography: jxai.bib
csl: diabetologia.csl
---

:::{.def}



:::

:::{.callout-note collapse="true"}

## Minimal necessary code from previous sections

```{.python}
base_dir = "<path-of-the-nabirds-dir>"
```

:::{.notenoit}

To be replaced by actual path: in our training cluster, the `base_dir` is at `/project/def-sponsor00/nabirds`:

```{.python}
base_dir = '/project/def-sponsor00/nabirds'
```

:::

```{python}
#| echo: false

base_dir = 'nabirds'
```

```{python}
import os
import polars as pl
import imageio.v3 as iio
import grain.python as grain

metadata = pl.read_parquet("metadata.parquet")
metadata_train = metadata.filter(pl.col("is_training_img") == 1)
cleaned_img_dir = os.path.join(base_dir, "cleaned_images")

class NABirdsDataset:
    """NABirds dataset class."""

    def __init__(self, metadata_file, data_dir):
        self.metadata_file = metadata_file
        self.data_dir = data_dir

    def __len__(self):
        return len(self.metadata_file)

    def __getitem__(self, idx):
        path = os.path.join(self.data_dir, self.metadata_file.get_column("path")[idx])
        img = iio.imread(path)
        species = self.metadata_file.get_column("species")[idx].replace("_", " ")
        subcategory = self.metadata_file.get_column("subcategory")[idx]
        if subcategory is not None:
            subcategory = subcategory.replace("_", " ")
        photographer = self.metadata_file.get_column("photographer")[idx].replace("_", " ")
        element = {
            "img": img,
            "species": species,
            "subcategory": subcategory,
            "photographer": photographer,
        }

        return element

nabirds_train = NABirdsDataset(metadata_train, cleaned_img_dir)
```

:::

## Our strategy

### Technique



### Architecture

Options that would make sense for our example include ResNet, EfficientNet, and ViT.

#### ResNet

[ResNet](https://en.wikipedia.org/wiki/Residual_neural_network), or residual network, is a type of architecture in which the layers are reformulated as learning residual functions with reference to the layer inputs. This allows for deeper (and thus more performant) networks [@he2015deepresiduallearningimage]. This is the oldest of the options that make sense for us, but it is also the most robust.

![from @he2015deepresiduallearningimage](img/resnet.jpg){fig-alt="noshadow" width="70%"}

[ResNet-50](https://huggingface.co/microsoft/resnet-50) is available from [Hugging Face](https://huggingface.co/) and has become a classic CNN for image classification.

#### EfficientNet

[EfficientNet](https://en.wikipedia.org/wiki/EfficientNet) is a family of newer computer vision CNNs from Google that uses a compound coefficient to uniformly scale depth, width, and resolution of networks and achieves better accuracy with fewer parameters than other CNNs [@tan2020efficientnetrethinkingmodelscaling]. This makes them easier to train on fewer resources and can lead to better results. Tuning them is however harder than the more robust ResNet family.

![from @tan2020efficientnetrethinkingmodelscaling](img/efficientnet.jpg){fig-alt="noshadow"}

There are variations for different image sizes sizes, all available in Hugging Face. For instance:

- [EfficientNet b0](https://huggingface.co/google/efficientnet-b0) for images of size 224x224
- [EfficientNet b2](https://huggingface.co/google/efficientnet-b2) for images 260x260
- [EfficientNet b3](https://huggingface.co/google/efficientnet-b3) for images 300x300
- [EfficientNet b7](https://huggingface.co/google/efficientnet-b7) for images 600x600

#### ViT

While the other options were CNN, [ViT](https://en.wikipedia.org/wiki/Vision_transformer), or vision transformer, is a transformer architecture (initially created for NLP tasks) applied to computer vision tasks [@dosovitskiy2021imageworth16x16words]. This is a more recent technique that attains excellent results while training substantially fewer computational resources.

![from @dosovitskiy2021imageworth16x16words](img/vit.jpg){fig-alt="noshadow"}

[ViT](https://huggingface.co/docs/transformers/en/model_doc/vit) is available in Hugging Face.

Which one to choose depends on the available hardware, libraries in the framework you want to use, and other practical considerations. If time permits, this is a good case of [experiment tracking](/ai/mlops/wb_mlflow) with [MLflow](https://github.com/mlflow/mlflow).

### Pre-trained weights


### Choice of library


### Strategy summary

Category | Answer
-- | -----
Technique | Transfer learning
Architecture | EfficientNet-B2 (EfficientNet-B0 or ResNet-50 are other reasonable options)
Pre-trained weights | ImageNet
Library | 

## Implementation

[Flax](https://github.com/google/flax) is the neural network library in the JAX AI stack.

```{python}
import jax
import jax.numpy as jnp
from flax import nnx
from flax.nnx import bridge
from transformers import FlaxViTForImageClassification

class NABirdsViT(nnx.Module):
    def __init__(self, num_classes, *, rngs: nnx.Rngs):
        # 1. Load the pre-trained Linen model structure & weights from Hugging Face
        hf_model = FlaxViTForImageClassification.from_pretrained(
            "google/vit-base-patch16-224",
            num_labels=num_classes,
            ignore_mismatched_sizes=True # Necessary to overwrite the 1000-class head
        )

        # 2. Bridge the Linen module to NNX
        # 'hf_model.module' is the underlying Linen module
        # 'hf_model.params' contains the pre-trained weights
        self.backbone = bridge.ToNNX(hf_model.module, rngs=rngs)

        # 3. Initialize and Load Weights
        # ToNNX requires a lazy initialization to structure the variables
        dummy_input = jnp.zeros((1, 3, 224, 224)) # ResNet expects NCHW by default in HF
        self.backbone.lazy_init(dummy_input)

        # Extract the empty NNX state
        _, backbone_state = nnx.split(self.backbone)

        # Copy weights from the loaded HF model into the NNX state
        # We define a helper to recursively copy the dictionary structure
        def copy_weights(target_state, source_dict):
            for key, value in source_dict.items():
                if isinstance(value, dict) or hasattr(value, 'items'):
                    copy_weights(target_state[key], value)
                else:
                    # Assign the weight to the NNX Variable
                    target_state[key].value = value

        # HF stores weights in .params and batch stats in .batch_stats (if applicable)
        # We merge them to match the structure expected by the bridge
        full_linen_vars = {**hf_model.params}
        if hasattr(hf_model, 'batch_stats'):
            full_linen_vars['batch_stats'] = hf_model.batch_stats

        copy_weights(backbone_state, full_linen_vars)

        # Update the bridge with the loaded weights
        nnx.update(self.backbone, backbone_state)

    def __call__(self, x):
        # HF ResNet expects NCHW format (channels first)
        # If your data is NHWC (standard for JAX), transpose it:
        # x = jnp.transpose(x, (0, 3, 1, 2))

        # Run the bridged model
        logits = self.backbone(x).logits
        return logits
```

Initialize the model:

```{python}
rngs = nnx.Rngs(params=0, dropout=1)
model = NABirdsViT(num_classes=405, rngs=rngs)
```

Test forward pass:

```{python}
dummy_img = jax.random.normal(jax.random.key(0), (1, 3, 224, 224))
logits = model(dummy_img)
print(f"Output shape: {logits.shape}") # (1, 555)
```

<!-- Forward pass: -->

<!-- ```{python} -->
<!-- x = jax.random.normal(jax.random.key(0), (1, 224, 224, 3)) -->
<!-- logits = model(x) -->

<!-- print(f"Logits shape: {logits.shape}") # (1, 555) -->
<!-- print("Model initialized and pre-trained weights loaded via NNX bridge.") -->
<!-- ``` -->
