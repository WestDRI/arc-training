---
title: Data preprocessing
author: Marie-Hélène Burle
---

:::{.def}



:::

:::{.callout-note collapse="true"}

## Minimal necessary code from previous sections

```{.python}
base_dir = "<path-of-the-nabirds-dir>"
```

:::{.note}

To be replaced by actual path.

:::

```{python}
#| echo: false

base_dir = "nabirds"
```

```{python}
import os
import polars as pl
import imageio.v3 as iio
import matplotlib.pyplot as plt
import matplotlib.patches as patches

img_dir = os.path.join(base_dir, "images")

bb_file = os.path.join(base_dir, "bounding_boxes.txt")
classes_translation_file = os.path.join(base_dir, "classes_fixed.txt")
class_labels_file = os.path.join(base_dir, "image_class_labels.txt")
img_file = os.path.join(base_dir, "images.txt")
photographers_file = os.path.join(base_dir, "photographers_fixed.txt")
sizes_file = os.path.join(base_dir, "sizes.txt")
train_test_split_file = os.path.join(base_dir, "train_test_split.txt")

bb = pl.read_csv(
    bb_file,
    separator=" ",
    has_header=False,
    new_columns=["UUID", "bb_x", "bb_y", "bb_width", "bb_height"]
)

classes = pl.read_csv(
    class_labels_file,
    separator=" ",
    has_header=False,
    new_columns=["UUID", "class"]
)

classes_translation = pl.read_csv(
    classes_translation_file,
    separator=" ",
    has_header=False,
    new_columns=["class", "id"]
)

img_paths = pl.read_csv(
    img_file,
    separator=" ",
    has_header=False,
    new_columns=["UUID", "path"]
)

photographers = pl.read_csv(
    photographers_file,
    separator=" ",
    has_header=False,
    new_columns=["UUID", "photographer"]
)

sizes = pl.read_csv(
    sizes_file,
    separator=" ",
    has_header=False,
    new_columns=["UUID", "img_width", "img_height"]
)

train_test_split = pl.read_csv(
    train_test_split_file,
    separator=" ",
    has_header=False,
    new_columns=["UUID", "is_training_img"]
)

classes_metadata = (
    classes.join(classes_translation, on="class")
)

metadata = (
    bb.join(classes_metadata, on="UUID")
    .join(img_paths, on="UUID")
    .join(photographers, on="UUID")
    .join(sizes, on="UUID")
    .join(train_test_split, on="UUID")
)

metadata_train = metadata.filter(pl.col("is_training_img") == 1)

class NABirdsDataset:
    """NABirds dataset class."""
    def __init__(self, metadata_file, data_dir):
        self.metadata = metadata_file
        self.data_dir = data_dir
    def __len__(self):
        return len(self.metadata)
    def __getitem__(self, idx):
        img_path = os.path.join(
            self.data_dir,
            self.metadata.get_column('path')[idx]
        )
        img = iio.imread(img_path)
        img_id = self.metadata.get_column('id')[idx].replace('_', ' ')
        img_photographer = self.metadata.get_column('photographer')[idx].replace('_', ' ')
        img_bb_x = self.metadata.get_column('bb_x')[idx]
        img_bb_y = self.metadata.get_column('bb_y')[idx]
        img_bb_width = self.metadata.get_column('bb_width')[idx]
        img_bb_height = self.metadata.get_column('bb_height')[idx]
        sample = {
            'image': img,
            'id': img_id,
            'photographer': img_photographer,
            'bbx' : img_bb_x,
            'bby' : img_bb_y,
            'bbwidth' : img_bb_width,
            'bbheight' : img_bb_height
        }
        return sample

nabirds_train = NABirdsDataset(
    metadata_train,
    img_dir
)
```

:::

## Needed transformations

Raw data seldom works without being transformed.

It is a classic normalization technique to divide the pixel values of an image by 255: the color values range from 0 to 255; we bring them in the 0-1 range. This improves performance and stability in training.

We should also turn our image into a JAX array.

We also should get rid of the parts of the images that are outside of the bounding boxes containing the birds.

Finally, a neural network will need images of the same size and our images come in all sorts of sizes.

## Transforms

Deep learning frameworks provide Transforms—classes that modify your images. You can also write your own. Here, as in the Dataset section, we will use [Grain](https://github.com/google/grain), part of the JAX AI stack, but you can use the framework of your choice, even when you use JAX.

Let's first write a Transform that will normalize our images and turn them to a JAX array of dtype Float32:

```{python}
import grain.python as grain
import jax.numpy as jnp

class NormAndCast(grain.MapTransform):
    """Transform class to normalize and cast images to float32."""
    def map(self, element):
        element['image'] = jnp.array(element['image'], dtype=jnp.float32) / 255.0
        return element
```

Here is another Transform that crops our images at the bounding boxes:

```{python}
class BbCrop(grain.MapTransform):
    """Transform class to crop images to their bounding boxes."""
    def map(self, element):
        img = element['image']
        bbx = element['bbx']
        bby = element['bby']
        bbwidth = element['bbwidth']
        bbheight = element['bbheight']
        img_cropped = img[bby:bby+bbheight, bbx:bbx+bbwidth]
        element['image'] = img_cropped
        return element
```

We also need to resize the images to the same size.

what size to chose?

```{python}
from skimage.transform import resize
import numpy as np

target = (224, 224)

class PaddingResize(grain.MapTransform):
    """Transform class to resize images to a given size with padding to avoid distortion."""
    def map(self, element):
        img = element['image']
        h, w, _ = img.shape
        target_h, target_w = target

        # Calculate the scaling factor to fit the image inside the box
        scale = min(target_h / h, target_w / w)

        # Calculate the new dimensions of the image
        new_h = int(h * scale)
        new_w = int(w * scale)

        # Resize the image to these new dimensions
        img_resized = resize(img, (new_h, new_w), anti_aliasing=True)

        # Create a black canvas (zeros) of the target size
        out_img = np.zeros((target_h, target_w, img.shape[2]), dtype=img_resized.dtype)

        # Place the resized image in the center of the canvas
        y_offset = (target_h - new_h) // 2
        x_offset = (target_w - new_w) // 2
        out_img[y_offset:y_offset+new_h, x_offset:x_offset+new_w] = img_resized

        element['image'] = out_img
        return element
```

We can combine Transforms together very easily with Grain (no need of a Compose class with Grain as with TorchVision):

```{python}
transformations = [NormAndCast(), BbCrop(), PaddingResize()]
```

:::{.callout-tip collapse="true"}

## Equivalent using PyTorch

Custom transformations can also be written in PyTorch. Here is an example for a simple normalization and casting of images:

```{.python}
class NormAndCastPyTorch(object):
    """Transform class to normalize and cast images to float32."""
    def __call__(self, sample):
        sample['image'] = jnp.array(sample['image'], dtype=jnp.float32) / 255.0
        return sample
```

To combine multiple Transforms, you use [`torchvision.transforms.Compose`](https://docs.pytorch.org/vision/0.8/transforms.html#torchvision.transforms.Compose), one of the [TorchVision Transforms](https://docs.pytorch.org/vision/0.8/transforms.html) (or you can use the newer [transforms v2](https://docs.pytorch.org/vision/main/auto_examples/transforms/plot_transforms_getting_started.html)).

In PyTorch, you would apply the Transforms as you instantiate your Dataset class since the Dataset class has a `transform` argument in which to pass them:

```{.python}
nabirds_norm_train_pytorch = NABirdsDataset(
    metadata_train,
    os.path.join(base_dir, img_dir),
    transform=NormAndCastPyTorch()
)
```

With Grain, you pass the combined Transforms as an argument of the DataLoader class, as we will see later.

:::
