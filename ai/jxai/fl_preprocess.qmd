---
title: Preprocessing data
bibliography: fl.bib
csl: diabetologia.csl
author:
  - Marie-Hélène Burle
  - Code adapted from JAX's [Implement ViT from scratch](https://docs.jaxstack.ai/en/latest/JAX_Vision_transformer.html)
---

:::{.def}

This section covers an example of the second step of a classic workflow: preprocessing the data.

:::

## Context

There are many tools and options. In this example, we use [TorchVision](https://github.com/pytorch/pytorch) to transform and augment images and [Grain](https://github.com/google/grain) to create data loaders.

```{dot}
//| echo: false
//| fig-width: 700px

digraph {

bgcolor="transparent"
node [fontname="Inconsolata, sans-serif", color=gray55, fontsize="18pt"]
edge [color=gray55]

load [label="Load data", shape=plaintext, group=g1, fontcolor=gray55]
proc [label="Process data", shape=plaintext, group=g1]
nn [label="Define architecture", shape=plaintext, group=g1, fontcolor=gray55]
pretr [label="Pre-trained model", shape=plaintext, group=g1, fontcolor=gray55]
opt [label="Hyperparameters", shape=plaintext, group=g1, fontcolor=gray55]
train [label="Train", shape=plaintext, group=g1, fontcolor=gray55]
cp [label="Checkpoint", shape=plaintext, group=g1, fontcolor=gray55]

pt [label=torchdata, fontcolor=gray55, color=gray55]
tfds [label=tfds, group=g2, fontcolor=gray55, color=gray55]
dt [label=datasets, fontcolor=gray55, color=gray55]

gr [label=grain, fontcolor=orangered3, color=orangered3]
tv [label=torchvision, fontcolor=orangered3, color=orangered3]

tr [label=transformers, fontcolor=gray55, color=gray55]

fl1 [label=flax, group=g2, fontcolor=gray55, color=gray55]
fl2 [label=flax, group=g2, fontcolor=gray55, color=gray55]

oa [label=optax, group=g2, fontcolor=gray55, color=gray55]

jx [label=jax, group=g2, fontcolor=gray55, color=gray55]

ob [label=orbax, group=g2, fontcolor=gray55, color=gray55]

{rank=same; gr load tv tr}
gr -> load -> tv -> tr [style=invis]

{rank=same; fl1 proc pretr}
fl1 -> proc -> pretr [style=invis]

{rank=same; jx fl2 opt}
fl1 -> proc -> pretr [style=invis]

{pt tfds dt} -> load [color=gray55]
{gr tv} -> proc [color=orangered3]
fl1 -> nn [color=gray55]
pretr -> nn [dir=none]
tr -> pretr [color=gray55]
oa -> opt [color=gray55]
jx -> fl2 -> train [color=gray55]
ob -> cp [color=gray55]

load -> proc -> nn -> opt -> train -> cp [dir=none]

}
```

:::{.callout-note collapse="true"}

## Minimal necessary code from previous section

```{python}
from datasets import load_dataset
import matplotlib.pyplot as plt

train_size = 5 * 750
val_size = 5 * 250

train_dataset = load_dataset("food101",
                             split=f"train[:{train_size}]")

val_dataset = load_dataset("food101",
                           split=f"validation[:{val_size}]")

labels_mapping = {}
index = 0
for i in range(0, len(val_dataset), 250):
    label = val_dataset[i]["label"]
    if label not in labels_mapping:
        labels_mapping[label] = index
        index += 1

inv_labels_mapping = {v: k for k, v in labels_mapping.items()}

def display_datapoints(*datapoints, tag="", names_map=None):
    num_samples = len(datapoints)

    fig, axs = plt.subplots(1, num_samples, figsize=(20, 10))
    for i, datapoint in enumerate(datapoints):
        if isinstance(datapoint, dict):
            img, label = datapoint["image"], datapoint["label"]
        else:
            img, label = datapoint

        if hasattr(img, "dtype") and img.dtype in (np.float32, ):
            img = ((img - img.min()) / (img.max() - img.min()) * 255.0).astype(np.uint8)

        label_str = f" ({names_map[label]})" if names_map is not None else ""
        axs[i].set_title(f"{tag} Label: {label}{label_str}")
        axs[i].imshow(img)
```

:::

## Load packages

Packages necessary for this section:

```{python}
# general array manipulation
import numpy as np

# for image transformation and augmentation
from torchvision.transforms import v2 as T

# to create data loaders
import grain.python as grain
```

## Data normalization and augmentation

Let's preprocess our images to match the methods used in the [vision transformer (ViT)](https://en.wikipedia.org/wiki/Vision_transformer) introduced by Dosovitskiy A et al. [-@dosovitskiy2021imageworth16x16words] and [implemented in JAX](https://github.com/google-research/vision_transformer/). This will be useful when we fine tune this model with the Food dataset in another section.

The preprocessing involves normalization and random augmentation (to prevent overfitting) with [TorchVision](https://github.com/pytorch/pytorch):

```{python}
img_size = 224

def to_np_array(pil_image):
  return np.asarray(pil_image.convert("RGB"))

def normalize(image):
    # Image preprocessing matches the one of pretrained ViT
    mean = np.array([0.5, 0.5, 0.5], dtype=np.float32)
    std = np.array([0.5, 0.5, 0.5], dtype=np.float32)
    image = image.astype(np.float32) / 255.0
    return (image - mean) / std

tv_train_transforms = T.Compose([
    T.RandomResizedCrop((img_size, img_size), scale=(0.7, 1.0)),
    T.RandomHorizontalFlip(),
    T.ColorJitter(0.2, 0.2, 0.2),
    T.Lambda(to_np_array),
    T.Lambda(normalize),
])

tv_test_transforms = T.Compose([
    T.Resize((img_size, img_size)),
    T.Lambda(to_np_array),
    T.Lambda(normalize),
])

def get_transform(fn):
    def wrapper(batch):
        batch["image"] = [
            fn(pil_image) for pil_image in batch["image"]
        ]
        # map label index between 0 - 4
        batch["label"] = [
            labels_mapping[label] for label in batch["label"]
        ]
        return batch
    return wrapper

train_transforms = get_transform(tv_train_transforms)
val_transforms = get_transform(tv_test_transforms)

train_dataset = train_dataset.with_transform(train_transforms)
val_dataset = val_dataset.with_transform(val_transforms)
```

## Data loaders

We use [Grain](https://github.com/google/grain) to create efficient data loaders:

```{python}
seed = 12
train_batch_size = 32
val_batch_size = 2 * train_batch_size

train_sampler = grain.IndexSampler(
    len(train_dataset),
    shuffle=True,
    seed=seed,
    shard_options=grain.NoSharding(),
    num_epochs=1,
)

val_sampler = grain.IndexSampler(
    len(val_dataset),
    shuffle=False,
    seed=seed,
    shard_options=grain.NoSharding(),
    num_epochs=1,
)

train_loader = grain.DataLoader(
    data_source=train_dataset,
    sampler=train_sampler,
    worker_count=4,
    worker_buffer_size=2,
    operations=[
        grain.Batch(train_batch_size, drop_remainder=True),
    ]
)

val_loader = grain.DataLoader(
    data_source=val_dataset,
    sampler=val_sampler,
    worker_count=4,
    worker_buffer_size=2,
    operations=[
        grain.Batch(val_batch_size),
    ]
)
```

## Inspect batches

```{python}
train_batch = next(iter(train_loader))
val_batch = next(iter(val_loader))

print(
    "Training batch info:",
      train_batch["image"].shape,
      train_batch["image"].dtype,
      train_batch["label"].shape,
      train_batch["label"].dtype
)

print(
    "Validation batch info:",
      val_batch["image"].shape,
      val_batch["image"].dtype,
      val_batch["label"].shape,
      val_batch["label"].dtype
)
```

Display the first three training and validation items:

```{python}
display_datapoints(
    *[(train_batch["image"][i], train_batch["label"][i]) for i in range(3)],
    tag="(Training) ",
    names_map={
        k: train_dataset.features["label"].names[v]
               for k, v in inv_labels_mapping.items()
    }
)

display_datapoints(
    *[(val_batch["image"][i], val_batch["label"][i]) for i in range(3)],
    tag="(Validation) ",
    names_map={
        k: val_dataset.features["label"].names[v]
               for k, v in inv_labels_mapping.items()
    }
)
```
