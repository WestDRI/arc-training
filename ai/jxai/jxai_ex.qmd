---
title: Our deep learning example
author: Marie-Hélène Burle
lightbox: true
---

:::{.def}

This course is structured around one concrete deep learning example which we introduce in this section.

:::

## The problem

We want to train a model to perform a fine-grained vision classification task in which the distinguishing features between classes are subtle.

## The data

We will use the [NABirds dataset](https://dl.allaboutbirds.org/nabirds) from the [Cornell Lab of Ornithology](https://www.birds.cornell.edu/home/).

## Our strategy

Before embarking on a deep learning project, it is crucial to think about the overall strategy to follow for optimum performance.

In particular, it is important to consider what should run on CPUs vs GPUs and what is part of the preliminary work and what constitutes the actual training.

Here is our plan [(click on the image to enlarge)]{.emph}:

![](img/strategy.png)

<br>It is split in 2 phases:

### Phase 1: preparation (CPU)

We will do the preliminary, deterministic data cleaning of the images on CPUs because:

- Doing it as a transformation during the training loop (so a Transform on our DataLoader) would make no sense—that work would be repeated at each epoch while we only need to do it once. We are much better off doing it once and writing the outputs to file.

- Writing to file is actually a task done by CPUs, so sending the data to GPUs and back to save it to files is actually a very inefficient workflow.

*For this first part, we don't use JAX. Instead, we use classic NumPy arrays. Trying to use JAX, JIT-compilation, of GPUs for this initial part would actually be a lot less efficient.*

### Phase 2: training (GPU)

After that, we move to the GPUs for the training loop (which includes data augmentation transformations—these are random and happening with some probability at each epoch, so they must be part of the loop. We aren't saving any of the transformed images to file at that point.)

*For that part, we use JAX and we JIT compile. This is where we use the heavy lifting of compilation, JAX, and GPUs.*

<!-- ```{mermaid} -->
<!-- %%| echo: false -->

<!-- --- -->
<!-- config: -->
<!--   theme: 'default' -->
<!--   themeVariables: -->
<!--     cScale0: '#8db14e' -->
<!--     cScale1: '#008b8b' -->
<!-- --- -->

<!-- timeline -->
<!--         section Data preparation <br> (on CPU) <br> Use NumPy for arrays -->
<!--           Metadata cleaning : Create cleaned copies of metadata files if needed -->
<!--                             : Gather needed metadata in a DataFrame -->
<!--           Data deterministic transformations <br> Use multiprocessing : Create cleaned copies of data if needed -->
<!--         section Model training <br> (on GPU) <br> Use JAX for arrays <br> JIT compile <br> Use vmap -->
<!--           DataLoader : Create sampler (shuffling, sharding, number of epochs) -->
<!--                      : Batching -->
<!--           Data augmentation : Define augmentation function -->
<!--                             : JIT compile augmentation function -->
<!--           Training loop : Move data to GPU -->
<!--                         : Convert NumPy arrays to JAX arrays -->
<!--                         : Apply augmentations -->
<!--                         : Train -->
<!-- ``` -->
