---
title: The JAX AI stack
frontpic: img/jaxstack.png
frontpicwidth: 50%
frontpicmargintop: 40px
frontpicmarginbottom: 40px
noshadow: noshadow
author: Marie-Hélène Burle
execute:
  freeze: auto
  cache: true
  error: true
  echo: true
format:
  revealjs:
    <!-- embed-resources: true -->
    theme: [default, ../../revealjsjax.scss]
    logo: /img/favicon_sfudrac.png
    highlight-style: vim-dark
    code-line-numbers: false
    template-partials:
      - /title-slide.html
    pointer:
      color: "#b5111b"
      pointerSize: 32
    link-external-newwindow: true
    footer: <a href="jxai_jaxstack.qmd"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="rgb(153, 153, 153)" class="bi bi-arrow-90deg-up" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4.854 1.146a.5.5 0 0 0-.708 0l-4 4a.5.5 0 1 0 .708.708L4 2.707V12.5A2.5 2.5 0 0 0 6.5 15h8a.5.5 0 0 0 0-1h-8A1.5 1.5 0 0 1 5 12.5V2.707l3.146 3.147a.5.5 0 1 0 .708-.708l-4-4z"/></svg>&nbsp;Back to the course</a>
    auto-stretch: false
revealjs-plugins:
  - pointer
---

## What is JAX? {.center}

:::{.fragment}

Library for Python developed by Google

:::

:::{.fragment}

Key data structure: Array

:::

:::{.fragment}

Composition, transformation, and differentiation of numerical programs

:::

:::{.fragment}

Compilation for CPUs, GPUs, and TPUs

:::

:::{.fragment}

NumPy-like and lower-level APIs

:::

:::{.fragment}

Requires strict functional programming

:::

# Why JAX?

## Fast {.center}

- **Default data type suited for deep learning**

  Like [PyTorch](https://pytorch.org/), uses float32 as default. This level of precision is suitable for deep learning and increases efficiency (by contrast, [NumPy](https://numpy.org/) defaults to float64)

- **[JIT compilation](https://en.wikipedia.org/wiki/Just-in-time_compilation)**

- The same code can run on [CPUs](https://en.wikipedia.org/wiki/Central_processing_unit) or on **accelerators** ([GPUs](https://en.wikipedia.org/wiki/Graphics_processing_unit) and [TPUs](https://en.wikipedia.org/wiki/Tensor_Processing_Unit))

- **[XLA (Accelerated Linear Algebra)](https://github.com/openxla/xla) optimization**

- **Asynchronous dispatch**

- **Vectorization, data parallelism, and sharding**

  All levels of shared and distributed memory parallelism are supported

## Great AD {.center}

```{dot}
//| echo: false
//| fig-height: 450px

strict digraph {

bgcolor="transparent"
graph [fontname="Inconsolata, sans-serif"]
node [fontname="Inconsolata, sans-serif", fontsize=15]

01 [label="Autodiff method", shape=underline, group=g1, group=g1, fontcolor=gray55, color=gray55]
1 [label="Static graph\nand XLA", shape=plaintext, group=g1, group=g1]
2 [label="Dynamic graph", shape=plaintext, group=g1]
4 [label="Dynamic graph\nand XLA", shape=plaintext, group=g1]
5 [label="Pseudo-dynamic\nand XLA", shape=plaintext, group=g1]

02 [label="Framework", shape=underline, group=g2, fontcolor=gray55, color=gray55]
a [label="TensorFlow", shape=oval, group=g2, color=darkorange4, fontcolor=darkorange4]
b [label="PyTorch", shape=oval, group=g2, color=chocolate, fontcolor=chocolate]
d [label="TensorFlow2", shape=oval, group=g2, color=darkorange4, fontcolor=darkorange4]
e [label="JAX", shape=oval, group=g2, color=deepskyblue3, fontcolor=deepskyblue3]

03 [label=Advantage, shape=underline, group=g3, fontcolor=gray55, color=gray55]
7 [label="Mostly\noptimized AD", shape=plaintext, fontcolor=darkolivegreen, group=g3]
8 [label="Convenient", shape=plaintext, fontcolor=darkolivegreen, group=g3]
9 [label="Convenient", shape=plaintext, fontcolor=darkolivegreen, group=g3]
10 [label="Convenient and\nmostly optimized AD", shape=plaintext, fontcolor=darkolivegreen, group=g3]

04 [label=Disadvantage, shape=underline, group=g4, fontcolor=gray55, color=gray55]
A [label="Manual writing of IR", shape=plaintext, fontcolor=darkorchid2, group=g4]
B [label="Limited AD optimization", shape=plaintext, fontcolor=darkorchid2, group=g4]
D [label="Disappointing speed", shape=plaintext, fontcolor=darkorchid2, group=g4]
E [label="Pure functions", shape=plaintext, fontcolor=darkorchid2, group=g4]

{rank=same; 01 02 03 04}
{rank=same; 1 a 7 A}
{rank=same; 2 b 8 B}
{rank=same; 4 d 9 D}
{rank=same; 5 e 10 E}

01 -> 02 -> 03 -> 04 [style=invis]
1 -> a -> 7 -> A [style=invis]
2 -> b -> 8 -> B [style=invis]
4 -> d -> 9 -> D [style=invis]
5 -> e -> 10 -> E [style=invis]

01 -> 1 [style=invis]
1 -> 2 -> 4 -> 5 [color=gray55]
02 -> a -> b -> d -> e [style=invis]
03 -> 7 -> 8 -> 9 -> 10 [style=invis]
04 -> A -> B -> D -> E [style=invis]

}
```

[&emsp;&emsp;Summarized from [a blog post](https://www.stochasticlifestyle.com/engineering-trade-offs-in-automatic-differentiation-from-tensorflow-and-pytorch-to-jax-and-julia/) by [Chris Rackauckas](https://chrisrackauckas.com/)]{.small}

## Close to the math {.center}

Considering the function `f`:

```{.python}
f = lambda x: x**3 + 2*x**2 - 3*x + 8
```

We can create a new function `dfdx` that computes the gradient of `f` w.r.t. `x`:

```{.python}
from jax import grad

dfdx = grad(f)
```

`dfdx` returns the derivatives:

```{.python}
print(dfdx(1.))
```

```
4.0
```

## Forward and reverse modes {.center}

- reverse-mode vector-Jacobian products: `jax.vjp`
- forward-mode Jacobian-vector products: `jax.jvp`

## Higher-order differentiation {.center}

With a single variable, the `grad` function calls can be nested:

```{.python}
d2fdx = grad(dfdx)   # function to compute 2nd order derivatives
d3fdx = grad(d2fdx)  # function to compute 3rd order derivatives
...
```

With several variables, you have to use the functions:

- `jax.jacfwd` for forward-mode,
- `jax.jacrev` for reverse-mode.

## How does it work? {.center}

```{dot}
//| echo: false
//| fig-height: 600px

strict digraph {

node [fontname="Inconsolata, sans-serif"]
bgcolor="transparent"

tracer  [label=Tracing, shape=rectangle, color=darkviolet, fontcolor=darkviolet]
jit [label="Transformation", shape=rectangle, color=chocolate, fontcolor=chocolate]
xla [label="Accelerated\n Linear Algebra \n(XLA)", shape=rectangle, color=deeppink3, fontcolor=deeppink3]
transform [label=" Transformations ", shape=rectangle, color=chocolate, fontcolor=chocolate]

CPU [shape=octagon, color=darkslategray4, fontcolor=darkslategray4]
GPU [shape=octagon, color=darkslategray4, fontcolor=darkslategray4]
TPU [shape=octagon, color=darkslategray4, fontcolor=darkslategray4]


py [label="Pure Python\nfunctions", color=gray50, fontcolor=gray50]
jaxpr [label="Jaxprs\n(JAX expressions)\nintermediate\nrepresentation\n(IR)", color=gray30, fontcolor=gray30]
hlo [label="High-level\noptimized (HLO)\nprogram", color=gray10, fontcolor=gray10]

py -> tracer [dir=none]
tracer -> jaxpr
jaxpr -> jit [dir=none]
jit -> hlo
hlo -> xla [dir=none]

xla -> CPU [shape=doubleoctagon]
xla -> GPU
xla -> TPU

jaxpr -> transform [dir=both, minlen=3]
{rank=same; jaxpr transform}

}
```

## How does it work? {.center}

```{dot}
//| echo: false
//| fig-height: 600px

strict digraph {

node [fontname="Inconsolata, sans-serif"]
bgcolor="transparent"

tracer  [label=Tracing, shape=rectangle, color=darkviolet, fontcolor=darkviolet]
jit [label=" Just-in-time \n(JIT)\ncompilation", shape=rectangle, color=chocolate, fontcolor=chocolate]
xla [label="Accelerated\n Linear Algebra \n(XLA)", shape=rectangle, color=deeppink3, fontcolor=deeppink3]
transform [label="Vectorization\nParallelization\n   Differentiation  ", shape=rectangle, color=chocolate, fontcolor=chocolate]

CPU [shape=octagon, color=darkslategray4, fontcolor=darkslategray4]
GPU [shape=octagon, color=darkslategray4, fontcolor=darkslategray4]
TPU [shape=octagon, color=darkslategray4, fontcolor=darkslategray4]


py [label="Pure Python\nfunctions", color=gray50, fontcolor=gray50]
jaxpr [label="Jaxprs\n(JAX expressions)\nintermediate\nrepresentation\n(IR)", color=gray30, fontcolor=gray30]
hlo [label="High-level\noptimized (HLO)\nprogram", color=gray10, fontcolor=gray10]

py -> tracer [dir=none]
tracer -> jaxpr
jaxpr -> jit [dir=none]
jit -> hlo
hlo -> xla [dir=none]

xla -> CPU [shape=doubleoctagon]
xla -> GPU
xla -> TPU

jaxpr -> transform [dir=both, minlen=3]
{rank=same; jaxpr transform}

}
```

## How does it work? {.center}

```{dot}
//| echo: false
//| fig-height: 600px

strict digraph {

node [fontname="Inconsolata, sans-serif"]
bgcolor="transparent"

tracer  [label=Tracing, shape=rectangle, color=darkviolet, fontcolor=darkviolet]
jit [label="jax.jit", shape=rectangle, color=chocolate, fontcolor=chocolate]
xla [label="Accelerated\n Linear Algebra \n(XLA)", shape=rectangle, color=deeppink3, fontcolor=deeppink3]
transform [label="jax.vmap\njax.pmap\njax.grad", shape=rectangle, color=chocolate, fontcolor=chocolate]

CPU [shape=octagon, color=darkslategray4, fontcolor=darkslategray4]
GPU [shape=octagon, color=darkslategray4, fontcolor=darkslategray4]
TPU [shape=octagon, color=darkslategray4, fontcolor=darkslategray4]


py [label="Pure Python\nfunctions", color=gray50, fontcolor=gray50]
jaxpr [label="Jaxprs\n(JAX expressions)\nintermediate\nrepresentation\n(IR)", color=gray30, fontcolor=gray30]
hlo [label="High-level\noptimized (HLO)\nprogram", color=gray10, fontcolor=gray10]

py -> tracer [dir=none]
tracer -> jaxpr
jaxpr -> jit [dir=none]
jit -> hlo
hlo -> xla [dir=none]

xla -> CPU [shape=doubleoctagon]
xla -> GPU
xla -> TPU

jaxpr -> transform [dir=both, minlen=3]
{rank=same; jaxpr transform}

}
```

## JAX itself is not a deep learning library {.center}

```{dot}
//| echo: false

strict digraph {

bgcolor="transparent"
node [fontname="Inconsolata, sans-serif"]

jx [label="JAX", fontcolor="#9D22B2", color="#9D22B2"]

dl [label="Deep learning", fontcolor="#5E98F6", shape=plaintext]
ll [label="LLMs", fontcolor="#5E98F6", shape=plaintext]
op [label="Optimizers", fontcolor="#5E98F6", shape=plaintext]
so [label="Solvers", fontcolor="#5E98F6", shape=plaintext]
pp [label="Probabilistic\nprogramming", fontcolor="#5E98F6", shape=plaintext]
pm [label="Probabilistic\nmodeling", fontcolor="#5E98F6", shape=plaintext]
ph [label="Physics\nsimulations", fontcolor="#5E98F6", shape=plaintext]

{ll so ph} -> jx [dir=back, color="#5E98F6"]
jx -> {pp dl pm op} [color="#5E98F6"]

}
```

## But a Python sublanguage ideal for deep learning {.center}

```{dot}
//| echo: false

strict digraph {

bgcolor="transparent"
node [fontname="Inconsolata, sans-serif"]

jx [label="JAX", fontcolor="#9D22B2", color="#9D22B2"]

dl [label="Deep learning", fontcolor="#5E98F6", shape=plaintext]
ll [label="LLMs", fontcolor=gray55, shape=plaintext]
op [label="Optimizers", fontcolor="#5E98F6", shape=plaintext]
so [label="Solvers", fontcolor=gray55, shape=plaintext]
pp [label="Probabilistic\nprogramming", fontcolor=gray55, shape=plaintext]
pm [label="Probabilistic\nmodeling", fontcolor=gray55, shape=plaintext]
ph [label="Physics\nsimulations", fontcolor=gray55, shape=plaintext]

{ll so ph} -> jx [dir=back, color=gray55]
jx -> {pp pm} [color=gray55]
jx -> {dl op} [color="#5E98F6"]

}
```

# The JAX AI stack

## Modular approach {.center}

![](img/jx_ecosystem.jpg){fig-alt="noshadow"}
