---
title: Compiling the metadata
author: Marie-Hélène Burle
---

:::{.def}

In this section, we process some of the metadata associated with the NABirds dataset by creating a [Polars](https://github.com/pola-rs/polars) DataFrame collecting all the information we will need while processing the images and training our model.

*[Polars](/python/hpc_polars) is a modern and ultra fast package that you should use instead of [pandas](https://github.com/pandas-dev/pandas) whenever you can if you care about performance.*

:::

## Metadata files

In addition to images, the dataset comes with a number of text files. To understand the dataset, of course, the place start is by reading ... the README.

:::{.callout-note collapse="true"}

## README

### The nabirds dataset

#### Versions

v0 - June 2015: initial release

For more information about the dataset, visit the project websites:

  http://www.vision.caltech.edu/visipedia
  http://vision.cornell.edu/se3/projects/visipedia/
  http://dl.allaboutbirds.org/nabirds

If you use the dataset in a publication, please cite the dataset in the style described on the dataset website (see url above).

Please see the nabirds.py file for example code on using the data. You can visualize images and annotations by running: python nabirds.py
Make sure you are in the nabirds/ directory.

#### Directory information

- images/

	The images organized in subdirectories based on species. See IMAGES AND CLASS LABELS section below for more info.

- parts/

	11 part locations per image. See PART LOCATIONS section below for more info.

### Images and class labels

Images are contained in the directory images/, with 555 subdirectories (one for each bird category).

#### List of image files (images.txt)

The list of image file names is contained in the file images.txt, with each line corresponding to one image:

<image_id> <image_name>

#### Train/test split (train_test_split.txt)

The suggested train/test split is contained in the file train_test_split.txt, with each line corresponding to one image:

<image_id> <is_training_image>

where <image_id> corresponds to the ID in images.txt, and a value of 1 or 0 for <is_training_image> denotes that the file is in the training or test set, respectively.

#### Image sizes (sizes.txt)

The size of each image in pixels:

<image_id> <width> <height>

where <image_id> corresponds to the ID in images.txt, and <width> and <height> correspond to the width and height of the image in pixels.

#### Image photographers (photographers.txt)

The photographer for each image:

<image_id> <photographer_name>

where <image_id> corresponds to the ID in images.txt, and <photographer_name> corresponds to the name of the photographer that took the photo. Please
be considerate and display the photographer's name when displaying their image.

#### List of class names (classes.txt)

The list of class names (bird species) is contained in the file classes.txt, with each line corresponding to one class:

<class_id> <class_name>

#### Image class labels (image_class_labels.txt)

The ground truth class labels (bird species labels) for each image are contained in the file image_class_labels.txt, with each line corresponding to one image:

<image_id> <class_id>

where <image_id> and <class_id> correspond to the IDs in images.txt and classes.txt, respectively.

#### Class hierarchy (hierarchy.txt)

The ground truth class labels (bird species labels) for each image are contained in the file image_class_labels.txt, with each line corresponding to one image:

<child_class_id> <parent_class_id>

where <child_class_id> and <parent_class_id> correspond to the IDs in classes.txt.

### Bounding boxes

Each image contains a single bounding box label.  Bounding box labels are contained in the file bounding_boxes.txt, with each line corresponding to one image:

<image_id> <x> <y> <width> <height>

where <image_id> corresponds to the ID in images.txt, and <x>, <y>, <width>, and <height> are all measured in pixels.

### Part locations

#### List of part names (parts/parts.txt)

The list of all part names is contained in the file parts/parts.txt, with each line corresponding to one part:

<part_id> <part_name>

#### Part locations (parts/part_locs.txt)

The set of all ground truth part locations is contained in the file parts/part_locs.txt, with each line corresponding to the annotation of a particular part in a particular image:

<image_id> <part_id> <x> <y> <visible>

where <image_id> and <part_id> correspond to the IDs in images.txt and parts/parts.txt, respectively.  <x> and <y> denote the pixel location of the center of the part.  <visible> is 0 if the part is not visible in the image and 1 otherwise.

:::

Each image is associated with a [UUID](https://en.wikipedia.org/wiki/Universally_unique_identifier).

We won't need all the information provided with this dataset for this course, but what we need is contained in the following files:

Name | Content
---| -------
bounding_boxes.txt | List of UUID and their corresponding bounding boxes (one bounding box per image around the bird)
classes.txt | List of class number and corresponding class name
image_class_labels.txt | List of UUID and their corresponding label
images.txt | List of UUID and their corresponding file name
photographers.txt | List of UUID and their corresponding photographer
train_test_split.txt | List of UUID and 1 or 0 if the image is for training or testing respectively (the dataset comes with a suggested split between training and validation)

## Fix problem files

The metadata comes in various space-separated CSV files. Two of them are problematic because they are jagged: the number of elements per line is inconsistent.

First of all, let's create new files without comma from these files. This should be easy to do in the Python function that we will use later, but for some reason that I could never figure out, I failed to implement it in Python. For this reason, I am cheating and doing it in Bash (or Zsh) using the utility [sed](https://en.wikipedia.org/wiki/Sed) that makes such transformations so easy.

In Bash or Zsh:

```{.bash}
sed 's/,//g' <path-of-the-nabirds-dir>/photographers.txt >
    <path-of-the-nabirds-dir>/photographers_nocommas.txt

sed 's/,//g' <path-of-the-nabirds-dir>/classes.txt >
    <path-of-the-nabirds-dir>/classes_nocommas.txt
```

:::{.note}

`<path-of-the-nabirds-dir>` is the path in which you downloaded the `nabirds` dataset.

:::

Alternatively:

```{.bash}
cd <path-of-the-nabirds-dir>

sed 's/,//g' photographers.txt > photographers_nocommas.txt
sed 's/,//g' classes.txt > classes_nocommas.txt
```

We could finish fixing these files in Bash (which would be a lot easier and much less wordy!) but because this is a Python course, let's do the rest in Python.

Let's create a function that will do the rest of the cleaning and write two new files that won't be problematic:

```{.python}
base_dir = "<path-of-the-nabirds-dir>"
```

:::{.note}

To be replaced by proper path.

:::

```{.python}
import os
import csv

def replace_spaces_except_first(input_filepath, output_filepath):
    """
    Replaces all spaces with underscores in a CSV file, except the first space
    on each line.

    Args:
        input_filepath (str): the path of the input file.
        output_filepath (str): the path of the output file.
    """
    with open(input_filepath, 'r') as infile, \
         open(output_filepath, 'w') as outfile:
        reader = csv.reader(infile)
        writer = csv.writer(outfile)

        for row in reader:
            processed_row = []
            for item in row:
                # Remove quotes
                item = item.replace('"', '')

                # Find the first space
                first_space_index = item.find(' ')

                # Keep the part up to the first space
                # and replace subsequent spaces with underscores
                part_with_first_space = item[:first_space_index + 1]
                part_after_first_space = item[first_space_index + 1:].replace(' ', '_')
                processed_item = part_with_first_space + part_after_first_space
                processed_row.append(processed_item)
            writer.writerow(processed_row)
```

Then we can apply the function on our files:

```{.python}
replace_spaces_except_first(
    os.path.join(base_dir, "photographers_nocommas.txt"),
    os.path.join(base_dir, "photographers_fixed.txt")
)

replace_spaces_except_first(
    os.path.join(base_dir, "classes_nocommas.txt"),
    os.path.join(base_dir, "classes_fixed.txt")
)
```

## Create variables

```{python}
#| echo: false

base_dir = "nabirds"
```

```{python}
import os

bb_file = os.path.join(base_dir, "bounding_boxes.txt")
classes_translation_file = os.path.join(base_dir, "classes_fixed.txt")
class_labels_file = os.path.join(base_dir, "image_class_labels.txt")
img_file = os.path.join(base_dir, "images.txt")
photographers_file = os.path.join(base_dir, "photographers_fixed.txt")
train_test_split_file = os.path.join(base_dir, "train_test_split.txt")
```

## Create a metadata Dataframe

First, we create a series of DataFrames from each CSV file:

```{python}
import polars as pl

bb = pl.read_csv(
    bb_file,
    separator=" ",
    has_header=False,
    new_columns=["UUID", "bb_x", "bb_y", "bb_width", "bb_height"]
)

classes = pl.read_csv(
    class_labels_file,
    separator=" ",
    has_header=False,
    new_columns=["UUID", "class"]
)

classes_translation = pl.read_csv(
    classes_translation_file,
    separator=" ",
    has_header=False,
    new_columns=["class", "id"]
)

img_paths = pl.read_csv(
    img_file,
    separator=" ",
    has_header=False,
    new_columns=["UUID", "path"]
)

photographers = pl.read_csv(
    photographers_file,
    separator=" ",
    has_header=False,
    new_columns=["UUID", "photographer"]
)

train_test_split = pl.read_csv(
    train_test_split_file,
    separator=" ",
    has_header=False,
    new_columns=["UUID", "is_training_img"]
)
```

Then we can combine the classes DataFrames so that the birds identifications becomes associated with the birds UUIDs:

```{python}
classes_metadata = (
    classes.join(classes_translation, on="class")
)
```

Finally, we combine all the DataFrames:

```{python}
metadata = (
    bb.join(classes_metadata, on="UUID")
    .join(img_paths, on="UUID")
    .join(photographers, on="UUID")
    .join(train_test_split, on="UUID")
)
```

## Sanity checks

Let's see what our DataFrame looks like:

```{python}
print(metadata)
```

And then let's explore a number of characteristics:

```{python}
print(metadata.columns)
print(metadata.row(0))
print(metadata.row(-1))
```

```{python}
print(metadata.head())
```

```{python}
print(metadata.tail())
```

```{python}
import random

random.seed(123)
print(metadata.sample())
```

```{python}
print(metadata.schema)
print(metadata.shape)
```

```{python}
print(metadata.glimpse())
```

```{python}
print(metadata.describe())
```

## Save DataFrame to Parquet

[Parquet](https://github.com/apache/parquet-format/)

```{python}
metadata.write_parquet("metadata.parquet")
```

Our metadata is ready. We can now start working with the pictures.
