---
title: Experiment tracking with
frontpic: img/logo_mlflow.svg
frontpicwidth: 45%
frontpicmargintop: 40px
frontpicmarginbottom: 40px
noshadow: noshadow
author: Marie-HÃ©lÃ¨ne Burle
date: 2025-11-25
date-format: long
execute:
  freeze: auto
  cache: true
  error: true
  echo: true
format:
  revealjs:
    <!-- embed-resources: true -->
    theme: [default, ../../revealjsmlflow.scss]
    logo: /img/favicon_sfudrac.png
    highlight-style: vim-dark
    code-line-numbers: false
    template-partials:
      - ../../title-slide.html
    pointer:
      color: "#b5111b"
      pointerSize: 32
    link-external-newwindow: true
    footer: <a href="wb_mlflow.qmd"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="rgb(1,148,226)" class="bi bi-arrow-90deg-up" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4.854 1.146a.5.5 0 0 0-.708 0l-4 4a.5.5 0 1 0 .708.708L4 2.707V12.5A2.5 2.5 0 0 0 6.5 15h8a.5.5 0 0 0 0-1h-8A1.5 1.5 0 0 1 5 12.5V2.707l3.146 3.147a.5.5 0 1 0 .708-.708l-4-4z"/></svg>&nbsp;Back to webinar page</a>
    auto-stretch: false
revealjs-plugins:
  - pointer
---

# Experiment tracking

## Lots of moving parts ... {.center}

Deep learning experiments come with a lot of components:

- Datasets
- Model architectures
- Hyperparameters

While developing an efficient model, various datasets will be trained on various architectures tuned with various hyperparameters

## ... making for challenging tracking {.center}

:::{.right}

[*hp = hyperparameter]{.small}

:::

```{dot}
//| echo: false
//| fig-height: 50%

digraph {
  
bgcolor="transparent"
node [fontname="Inconsolata, sans-serif", fontsize=9, penwidth=0.5]
edge [color=gray55, arrowhead="vee", arrowsize=0.5, penwidth=0.5]

data1 [label="data1", color=darkviolet]
data2 [label="data2", color=darkviolet]
data3 [label="data3", color=darkviolet]

hp1 [label="hp1", color=darkslategray4]
hp2 [label="hp2", color=darkslategray4]
hp3 [label="hp3", color=darkslategray4]

model1 [label="model1", color=deepskyblue3]
model2 [label="model2", color=deepskyblue3]
model3 [label="model3", color=deepskyblue3]

performance [label="performance1 ... performance27", color=darkorange4]

{data1 data2 data3} -> {model1 model2 model3} [color=darkviolet]
{hp1 hp2 hp3} -> {model1 model2 model3} [color=darkslategray4]

{model1 model2 model3} -> performance [color=deepskyblue3]
{model1 model2 model3} -> performance [color=deepskyblue3]
{model1 model2 model3} -> performance [color=deepskyblue3]
{model1 model2 model3} -> performance [color=deepskyblue3]
{model1 model2 model3} -> performance [color=deepskyblue3]
{model1 model2 model3} -> performance [color=deepskyblue3]
{model1 model2 model3} -> performance [color=deepskyblue3]
{model1 model2 model3} -> performance [color=deepskyblue3]
{model1 model2 model3} -> performance [color=deepskyblue3]

}
```

. . .

How did we get `performance19` again? ðŸ¤¯

## Experiment tracking tools  {.center}

The solution to this complexity is to use an experiment tracking tool such as [MLflow](https://mlflow.org/) and, optionally, a data versioning tool such as [DVC](https://dvc.org/)

:::{.note}

See [our webinar on DVC](wb_dvc)

:::

# MLflow

## Platform for AI life cycle {.center}

![from [MLflow website](https://mlflow.org/docs/latest/ml/)](img/mlflow_model_dev.png){fig-alt="noshadow" width="80%"}

## FOSS & compatible {.center}

- **Open-source**
- Works with **any ML or DL framework**
- **Vendor-neutral** if you run a server on a commercial platform
- **Can be combined with [dvc](https://dvc.org/) for [dataset versioning](https://dev.to/aws-builders/ml-done-right-versioning-datasets-and-models-with-dvc-mlflow-4p3f)**
- Works with any hyperparameter tuning framework &nbsp;âž” &nbsp;e.g. [integration](https://mlflow.org/docs/latest/ml/traditional-ml/tutorials/hyperparameter-tuning/notebooks/hyperparameter-tuning-with-child-runs/) with [Optuna](https://github.com/optuna/optuna) \
  &nbsp;&ensp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&ensp;&nbsp;[integration](https://docs.ray.io/en/latest/tune/examples/tune-mlflow.html) with [Ray Tune](https://github.com/ray-project/ray) \
  &nbsp;&ensp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&ensp;&nbsp;[integration](https://mlflow.org/docs/3.1.3/ml/getting-started/hyperparameter-tuning/) with [hyperopt](https://github.com/hyperopt/hyperopt)

## Used by many proprietary tools {.center}

The foundation of many proprietary no-code/low-code tuning platforms that just add a layer on top to interface with the user with text rather than code

:::{.note}

e.g. Microsoft Fabric, FLAML

:::

## Limitations {.center}

[MLflow projects](https://mlflow.org/docs/latest/ml/projects/#environment) do not (yet) support [uv](https://docs.astral.sh/uv/)

Some functionality missing for deployment and production for large companies (but irrelevant for research and no FOSS option exists)

## Installing MLflow {.center}

*With [uv](https://docs.astral.sh/uv/)*

Create a `uv` project:

```{.bash}
uv init --bare
```

Install MLflow:

```{.bash}
uv add mlflow
```

# Tracking models

## Overview {.center}

Track models at checkpoints

Compare with different datasets

Visualize with tracking UI

## Definitions {.center}

**Run**: \
Single execution of a model training event

**Model signature**: \
Description of a model's input and output data structure, data types, and features names

## MLflow tracking setups {.center}

![from [MLflow documentation](https://mlflow.org/docs/latest/ml/tracking/)](img/mlflow_tracking_setup.png){fig-alt="noshadow" width="85%"}

## MLflow tracking setups {.center}

::::{.columns}

:::{.column width="29%"}

![](img/mlflow_tracking_setup1.png){fig-alt="noshadow" width="65%"}

:::

:::{.column width="2%"}
:::

:::{.column width="69%"}

### Local

```{.python}
import mlflow

mlflow.set_experiment("<experiment-name>")
```

Logs get stored in an `mlruns` directory

:::{.note}

This method will be deprecated, so prefer the use of a database

:::

:::

::::

## MLflow tracking setups {.center}

::::{.columns}

:::{.column width="29%"}

![](img/mlflow_tracking_setup2.png){fig-alt="noshadow" width="80%"}

:::

:::{.column width="2%"}
:::

:::{.column width="69%"}

### Local with database

```{.python}
import mlflow

mlflow.set_tracking_uri("sqlite:///mlflow.db")
mlflow.set_experiment("<experiment-name>")
```

:::{.note}

Here we use SQLite which works well for a local database

:::

Logs get stored in an `mlflow.db` file

:::

::::

## MLflow tracking setups {.center}

::::{.columns}

:::{.column width="29%"}

![](img/mlflow_tracking_setup3.png){fig-alt="noshadow"}

:::

:::{.column width="2%"}
:::

:::{.column width="69%"}

### Remote tracking server

:::{.note}

For team development

:::

```{.python}
import mlflow

mlflow.set_tracking_uri("http://<host>:<port>")
mlflow.set_experiment("<experiment-name>")
```

:::

::::

# MLflow user interface

## MLflow UI {.center}

::::{.columns}

:::{.column width="29%"}

![](img/mlflow_tracking_setup1.png){fig-alt="noshadow" width="65%"}

:::

:::{.column width="2%"}
:::

:::{.column width="69%"}

### Local

```{.bash}
mlflow server
```

:::{.notenoit}

The server listens on <http://localhost:5000> by default, but you can choose any unused port:

```{.bash}
mlflow server --port 8080
```

:::

:::

::::

## MLflow UI {.center}

::::{.columns}

:::{.column width="29%"}

![](img/mlflow_tracking_setup2.png){fig-alt="noshadow" width="80%"}

:::

:::{.column width="2%"}
:::

:::{.column width="69%"}

### Local with database

```{.bash}
mlflow server \
	   --backend-store-uri sqlite:///mlflow.db
```

:::{.notenoit}

(Default port is `5000`)

With another port:

```{.bash}
mlflow server \
	   --backend-store-uri sqlite:///mlflow.db \
	   --port 8080
```

:::

:::

::::

## MLflow UI {.center}

::::{.columns}

:::{.column width="29%"}

![](img/mlflow_tracking_setup3.png){fig-alt="noshadow"}

:::

:::{.column width="2%"}
:::

:::{.column width="69%"}

### Remote tracking server

```{.bash}
mlflow server \
	   --host <host> \
	   --backend-store-uri postgresql+psycopg2://<username>:<password>@<host>:<port>/mlflowdb \
	   --port <port>
```

:::{.note}

Here we use PostgreSQL which works well to manage a database in a collaborative remote client-server system

(Requires installing the `psycopg2` package)

:::

:::

::::

## Access the UI {.center}

Open `http://<host>:<port>` in your browser

:::{.example}

Example:

For a local server on port `5000` (the default), open <http://localhost:5000/>

:::

## Log tracking data {.center}

The workflow looks like this:

```{.python}
import mlflow

with mlflow.start_run():
    mlflow.log_param("lr", 0.001)
    # Your ml code
    ...
    mlflow.log_metric("val_loss", val_loss)
```

## Organize runs {.center}

experiments
child runs
tags

# Tracking datasets

# Hyperparameter tuning

## Goal of tuning {.center}

Find the optimal set of hyperparameters that maximize a model's predictive accuracy and performance

âž” Find the right balance between high bias (underfitting) and high variance (overfitting) to improve the model's ability to generalize and perform well on new, unseen data

## Tuning frameworks {.center}

Hyperparameters optimization used to be done manually following a systematic grid pattern. This was extremely inefficient

Nowadays, there are many frameworks that do it automatically, faster, and better

:::{.example}

Example:

:::

- [Optuna](https://optuna.org/)
- [Hyperopt](http://hyperopt.github.io/hyperopt/)
- [Ray Tune](https://docs.ray.io/en/latest/tune/index.html)

## Workflow {.center}

- Define an objective function
- Define a search space
- Minimize the objective over the space

<!-- ## ML workflow {.center} -->

<!-- ```{mermaid} -->
<!-- %%| echo: false -->
<!-- flowchart LR -->
<!--    data[Data cleaning] -\-> tr1[Initial training] -->
<!--    tr1 -\-> tuning[Tuning] -->
<!--    tuning -\-> tr2[More training] -->
<!--    tr2 -\-> val[Validation] -->
<!--    val -\-> tr1 -->
<!--    val -\-> eval[Evaluation] -->
<!--    eval -\-> tr3[Final training] -->
<!--    tr3 -\-> deploy[Deployment] -->
<!-- ``` -->
