---
title: Experiment tracking with
frontpic: img/logo_mlflow.svg
frontpicwidth: 45%
frontpicmargintop: 40px
frontpicmarginbottom: 40px
noshadow: noshadow
author: Marie-HÃ©lÃ¨ne Burle
date: 2025-11-25
date-format: long
execute:
  freeze: auto
  cache: true
  error: true
  echo: true
format:
  revealjs:
    <!-- embed-resources: true -->
    theme: [default, ../../revealjsmlflow.scss]
    logo: /img/favicon_sfudrac.png
    highlight-style: vim-dark
    code-line-numbers: false
    template-partials:
      - ../../title-slide.html
    pointer:
      color: "#b5111b"
      pointerSize: 32
    link-external-newwindow: true
    footer: <a href="wb_mlflow.qmd"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="rgb(1,148,226)" class="bi bi-arrow-90deg-up" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4.854 1.146a.5.5 0 0 0-.708 0l-4 4a.5.5 0 1 0 .708.708L4 2.707V12.5A2.5 2.5 0 0 0 6.5 15h8a.5.5 0 0 0 0-1h-8A1.5 1.5 0 0 1 5 12.5V2.707l3.146 3.147a.5.5 0 1 0 .708-.708l-4-4z"/></svg>&nbsp;Back to webinar page</a>
    auto-stretch: false
revealjs-plugins:
  - pointer
---

# Experiment tracking

## Lots of moving parts ... {.center}

AI experiments come with a lot of components:

- Datasets
- Model architectures
- Hyperparameters

While developing an efficient model, various datasets will be trained on various architectures tuned with various hyperparameters

## ... making for challenging tracking {.center}

:::{.right}

[*hp = hyperparameter]{.small}

:::

```{dot}
//| echo: false
//| fig-height: 50%

digraph {
  
bgcolor="transparent"
node [fontname="Inconsolata, sans-serif", fontsize=9, penwidth=0.5]
edge [color=gray55, arrowhead="vee", arrowsize=0.5, penwidth=0.5]

data1 [label="data1", color=darkviolet]
data2 [label="data2", color=darkviolet]
data3 [label="data3", color=darkviolet]

hp1 [label="hp1", color=darkslategray4]
hp2 [label="hp2", color=darkslategray4]
hp3 [label="hp3", color=darkslategray4]

model1 [label="model1", color=deepskyblue3]
model2 [label="model2", color=deepskyblue3]
model3 [label="model3", color=deepskyblue3]

performance [label="performance1 ... performance27", color=darkorange4]

{data1 data2 data3} -> {model1 model2 model3} [color=darkviolet]
{hp1 hp2 hp3} -> {model1 model2 model3} [color=darkslategray4]

{model1 model2 model3} -> performance [color=deepskyblue3]
{model1 model2 model3} -> performance [color=deepskyblue3]
{model1 model2 model3} -> performance [color=deepskyblue3]
{model1 model2 model3} -> performance [color=deepskyblue3]
{model1 model2 model3} -> performance [color=deepskyblue3]
{model1 model2 model3} -> performance [color=deepskyblue3]
{model1 model2 model3} -> performance [color=deepskyblue3]
{model1 model2 model3} -> performance [color=deepskyblue3]
{model1 model2 model3} -> performance [color=deepskyblue3]

}
```

. . .

How did we get `performance19` again? ðŸ¤¯

## Experiment tracking tools  {.center}

<br>The solution to this complexity is to use an experiment tracking tool such as [MLflow](https://mlflow.org/) and, optionally, a data versioning tool such as [DVC](https://dvc.org/)<br><br>

:::{.note}

See [our webinar on DVC](wb_dvc)

:::

# MLflow overview

## Platform for AI life cycle {.center}

![from [MLflow website](https://mlflow.org/docs/latest/ml/)](img/mlflow_model_dev.png){fig-alt="noshadow" width="80%"}

## Use cases {.center}

- Compare algorithms
- Keep track of pipelines
- Generate SHAP plots (relative contributions of features to the prediction)
- Track models at checkpoints
- Compare models with different datasets
- Track hyperparameter tuning experiments
- Visualize plots of logged metrics in UI
- Keep models and model versions in a registry

## FOSS & compatible {.center}

- **Open-source**
- Works with **any ML or DL framework**
- **Vendor-neutral** if you run a server on a commercial platform
- **Can be combined with [dvc](https://dvc.org/) for [dataset versioning](https://dev.to/aws-builders/ml-done-right-versioning-datasets-and-models-with-dvc-mlflow-4p3f)**
- Works with any hyperparameter tuning framework &nbsp;âž” &nbsp;e.g. [integration](https://mlflow.org/docs/latest/ml/traditional-ml/tutorials/hyperparameter-tuning/notebooks/hyperparameter-tuning-with-child-runs/) with [Optuna](https://github.com/optuna/optuna) \
  &nbsp;&ensp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&ensp;&nbsp;[integration](https://docs.ray.io/en/latest/tune/examples/tune-mlflow.html) with [Ray Tune](https://github.com/ray-project/ray) \
  &nbsp;&ensp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&ensp;&nbsp;[integration](https://mlflow.org/docs/3.1.3/ml/getting-started/hyperparameter-tuning/) with [hyperopt](https://github.com/hyperopt/hyperopt)

## Used by many proprietary tools {.center}

<br>The foundation of many proprietary no-code/low-code tuning platforms that just add a layer on top to interface with the user with text rather than code <br><br>

:::{.note}

e.g. Microsoft Fabric, FLAML

:::

# Tracking with MLflow

## Install MLflow {.center}

*With [uv](https://docs.astral.sh/uv/)*

Create a `uv` project:

```{.bash}
uv init --bare
```

Install MLflow:

```{.bash}
uv add mlflow
```

:::{.emph}

<br>`uv` is an amazing Python projects/versions/virtual envs manager and I recommend using it on your machine. **However, it is currently not supported on the Alliance clusters where you need to keep using `pip` or risk issues with undetected modules**

:::

## Definitions in MLflow context {.center}

**Parent run**: \
One optimization task \
Contains multiple child runs

**Child run**: \
Individual execution of a model training event

**Model signature**: \
Description of a model's input and output data structure, data types, and features names

## MLflow tracking workflow {.center}

- Create an experiment
- Launch an MLflow server
- Open the user interface in a browser to visualize the logged data
- Log tracking data

## MLflow tracking setup {.center}

You can setup MLflow tracking locally or on a remote server, \
using databases or not^1^

In the next slides, I am breaking down the workflow for the various configurations

<br>

:::{.notenoit}

^1^Usage without databases will be deprecated at some point and is not recommended

:::

## {.center}

### Create an experiment

::::{.columns}

:::{.column width="29%"}

![](img/mlflow_tracking_setup1.png){fig-alt="noshadow" width="65%"}

:::

:::{.column width="2%"}
:::

:::{.column width="69%"}

#### <br>Local

```{.python}
import mlflow

mlflow.set_experiment("<experiment-name>")
```

<br>
Logs get stored in an `mlruns` directory

:::{.note}

This method will be deprecated, so prefer the use of a database

:::

:::

::::

## {.center}

### Create an experiment

::::{.columns}

:::{.column width="29%"}

![](img/mlflow_tracking_setup2.png){fig-alt="noshadow" width="80%"}

:::

:::{.column width="2%"}
:::

:::{.column width="69%"}

#### <br>Local with database

```{.python}
import mlflow

mlflow.set_tracking_uri("sqlite:///<database-name>.db")
mlflow.set_experiment("<experiment-name>")
```

<br>

:::{.note}

Here we use SQLite which works well for a local database

:::

Logs get stored in an `mlflow.db` file

:::

::::

## {.center}

### Create an experiment

::::{.columns}

:::{.column width="29%"}

![](img/mlflow_tracking_setup3.png){fig-alt="noshadow"}

:::

:::{.column width="2%"}
:::

:::{.column width="69%"}

#### <br>Remote tracking server

:::{.note}

For team development

:::

<br>
```{.python}
import mlflow

mlflow.set_tracking_uri("http://<host>:<port>")
mlflow.set_experiment("<experiment-name>")
```

:::

::::

## {.center}

### Launch MLflow server

::::{.columns}

:::{.column width="29%"}

![](img/mlflow_tracking_setup1.png){fig-alt="noshadow" width="65%"}

:::

:::{.column width="2%"}
:::

:::{.column width="69%"}

#### <br>Local

```{.bash}
mlflow server
```
<br>

:::{.notenoit}

The server listens on <http://localhost:5000> by default, but you can choose any unused port:

```{.bash}
mlflow server --port 8080
```

:::

:::

::::

## {.center}

### Launch MLflow server

::::{.columns}

:::{.column width="29%"}

![](img/mlflow_tracking_setup2.png){fig-alt="noshadow" width="80%"}

:::

:::{.column width="2%"}
:::

:::{.column width="69%"}

#### <br>Local with database

```{.bash}
mlflow server \
	   --backend-store-uri sqlite:///<database-name>.db
```
<br>

:::{.notenoit}

(Default port is `5000`)

With another port:

```{.bash}
mlflow server \
	   --backend-store-uri sqlite:///<database-name>.db \
	   --port 8080
```

:::

:::

::::

## {.center}

### Launch MLflow server

::::{.columns}

:::{.column width="29%"}

![](img/mlflow_tracking_setup3.png){fig-alt="noshadow"}

:::

:::{.column width="2%"}
:::

:::{.column width="69%"}

#### <br>Remote tracking server

```{.bash}
mlflow server \
	   --host <host> \
	   --backend-store-uri postgresql+psycopg2://<username>:<password>@<host>:<port>/mlflowdb \
	   --port <port>
```

<br>

:::{.note}

Here we use PostgreSQL which works well to manage a database in a collaborative remote client-server system

(Requires installing the `psycopg2` package)

:::

:::

::::

## {.center}

### Access the UI

Open `http://<host>:<port>` in a browser to view logs in the UI

:::{.example}

Example:

For a local server on port `5000` (the default), open <http://localhost:5000>

:::

## {.center}

### Log tracking data

Define and run the experiment you want to track

:::{.notenoit}

Examples:

Train a model, \
Tune hyperparameters, \
Run a model with different datasets, \
...

:::

# Use case 1: model tracking

## Benefits of using MLflow {.center}

- Monitor model metrics (e.g. loss, accuracy)
- Monitor system metrics (e.g. GPU, memory)
- Save checkpoints with metrics
- Record hyperparameters and optimizer settings
- Snapshot library versions for reproducibility

## Workflow {.center}

- Create an experiment
- Launch an MLflow server
- Open the user interface in a browser to visualize the logged data
- Log tracking data:
  - Prepare data
  - Define model
  - Define training parameters and optimizer
  - Train

## Demo {.center}

:::{.figure-caption}

modified from [MLflow website](https://mlflow.org/docs/latest/ml/getting-started/deep-learning/)

:::

We need several packages for this demo. I create a bare `uv` project:

```{.bash}
uv init --bare
```

and install the packages in its virtual environment:

```{.bash}
uv add mlflow psutil torch torchvision
```

I also install [ptpython](https://github.com/prompt-toolkit/ptpython) because I like to use it instead of the default Python shell, but this is not necessary to run the demo (which is why I am adding it in the dev group):

```{.bash}
uv add --dev ptpython
```

I can now launch `ptpython` (or `python`) to run the code:

```{.bash}
uv run ptpython
```

## {.center}

### Create an experiment

```{.python}
import mlflow

mlflow.set_tracking_uri("sqlite:///demo.db")
mlflow.set_experiment("model_tracking_demo")

mlflow.config.enable_system_metrics_logging()
mlflow.config.set_system_metrics_sampling_interval(1)
```

:::{.note}

You can see that the database file `model_tracking_demo.db` gets created

:::

## {.center}

### Launch MLflow server

*In Bash (not Python):*

```{.bash}
uv run mlflow server \
       --backend-store-uri sqlite:///model_tracking_demo.db
```

## {.center}

### Access the UI

Open <http://localhost:5000> in a browser to view logs in the UI

You can see our [model_tracking_demo]{.codelike} experiment

Click on it to see the interface where the runs will be logged as we train our model

## {.center}

### Log tracking data

In this demo, we log epoch metrics, checkpoints, final model from a standard model training with PyTorch

## {.center}

#### Prepare data

```{.python}
import torch
from torch.utils.data import DataLoader
from torchvision import datasets, transforms

# Define device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Load and prepare data
transform = transforms.Compose(
    [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]
)
train_dataset = datasets.FashionMNIST(
    "data", train=True, download=True, transform=transform
)
test_dataset = datasets.FashionMNIST("data", train=False, transform=transform)
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=1000)
```

## {.center}

#### Define model

```{.python}
import torch.nn as nn

class NeuralNetwork(nn.Module):
    def __init__(self):
        super().__init__()
        self.flatten = nn.Flatten()
        self.linear_relu_stack = nn.Sequential(
            nn.Linear(28 * 28, 512),
            nn.ReLU(),
            nn.Linear(512, 512),
            nn.ReLU(),
            nn.Linear(512, 10),
        )

    def forward(self, x):
        x = self.flatten(x)
        logits = self.linear_relu_stack(x)
        return logits

model = NeuralNetwork().to(device)
```

## {.center}

#### Set training conditions

```{.python}
import torch.optim as optim

# Training parameters
params = {
    "epochs": 4,
    "learning_rate": 1e-3,
    "batch_size": 64,
    "optimizer": "SGD",
    "model_type": "MLP",
    "hidden_units": [512, 512],
}

# Define optimizer and loss function
loss_fn = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=params["learning_rate"])
```

## {.center}

#### Train

```{.python}
with mlflow.start_run(run_name="run_1") as run:
    # Log training parameters
    mlflow.log_params(params)

    for epoch in range(params["epochs"]):
        model.train()
        train_loss, correct, total = 0, 0, 0

        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.to(device), target.to(device)

            # Forward pass
            optimizer.zero_grad()
            output = model(data)
            loss = loss_fn(output, target)

            # Backward pass
            loss.backward()
            optimizer.step()

            # Calculate metrics
            train_loss += loss.item()
            _, predicted = output.max(1)
            total += target.size(0)
            correct += predicted.eq(target).sum().item()

            # Log batch metrics (every 100 batches)
            if batch_idx % 100 == 0:
                batch_loss = train_loss / (batch_idx + 1)
                batch_acc = 100.0 * correct / total
                mlflow.log_metrics(
                    {"batch_loss": batch_loss, "batch_accuracy": batch_acc},
                    step=epoch * len(train_loader) + batch_idx,
                )

        # Calculate epoch metrics
        epoch_loss = train_loss / len(train_loader)
        epoch_acc = 100.0 * correct / total

        # Validation
        model.eval()
        val_loss, val_correct, val_total = 0, 0, 0
        with torch.no_grad():
            for data, target in test_loader:
                data, target = data.to(device), target.to(device)
                output = model(data)
                loss = loss_fn(output, target)

                val_loss += loss.item()
                _, predicted = output.max(1)
                val_total += target.size(0)
                val_correct += predicted.eq(target).sum().item()

        # Calculate and log epoch validation metrics
        val_loss = val_loss / len(test_loader)
        val_acc = 100.0 * val_correct / val_total

        # Log epoch metrics
        mlflow.log_metrics(
            {
                "train_loss": epoch_loss,
                "train_accuracy": epoch_acc,
                "val_loss": val_loss,
                "val_accuracy": val_acc,
            },
            step=epoch,
        )
        # Log checkpoint at the end of each epoch
        mlflow.pytorch.log_model(model, name=f"checkpoint_{epoch}")

        print(
            f"Epoch {epoch+1}/{params['epochs']}, "
            f"Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.2f}%, "
            f"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%"
        )

    # Log the final trained model
    model_info = mlflow.pytorch.log_model(model, name="final_model")
```

## {.center}

### Visualize metrics during training

Go to the UI in the browser

If you haven't done so yet, click on the experiment we called [model_tracking_demo]{.codelike}

Then click on the run we called [run_1]{.codelike} (if you don't name the run, it gets an automatically generated name, different for each run)

Finally go to the [Model metrics]{.codelike} tab to see the metrics logged in real time

The [System metrics]{.codelike} tab allows you to monitor your resource usage

<br>
Alternatively, you can click on the experiment and click on ![](img/mlflow_chartview.png){width="50px"} ([Chart view]{.codelike} button)

This is a great method if you want to compare multiple runs

## {.center}

### Create a new run

We can create new runs by training the model again:

```{.python}
with mlflow.start_run(run_name="run_2") as run:
    # Log training parameters
    mlflow.log_params(params)

    for epoch in range(params["epochs"]):
        model.train()
        train_loss, correct, total = 0, 0, 0

        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.to(device), target.to(device)

            # Forward pass
            optimizer.zero_grad()
            output = model(data)
            loss = loss_fn(output, target)

            # Backward pass
            loss.backward()
            optimizer.step()

            # Calculate metrics
            train_loss += loss.item()
            _, predicted = output.max(1)
            total += target.size(0)
            correct += predicted.eq(target).sum().item()

            # Log batch metrics (every 100 batches)
            if batch_idx % 100 == 0:
                batch_loss = train_loss / (batch_idx + 1)
                batch_acc = 100.0 * correct / total
                mlflow.log_metrics(
                    {"batch_loss": batch_loss, "batch_accuracy": batch_acc},
                    step=epoch * len(train_loader) + batch_idx,
                )

        # Calculate epoch metrics
        epoch_loss = train_loss / len(train_loader)
        epoch_acc = 100.0 * correct / total

        # Validation
        model.eval()
        val_loss, val_correct, val_total = 0, 0, 0
        with torch.no_grad():
            for data, target in test_loader:
                data, target = data.to(device), target.to(device)
                output = model(data)
                loss = loss_fn(output, target)

                val_loss += loss.item()
                _, predicted = output.max(1)
                val_total += target.size(0)
                val_correct += predicted.eq(target).sum().item()

        # Calculate and log epoch validation metrics
        val_loss = val_loss / len(test_loader)
        val_acc = 100.0 * val_correct / val_total

        # Log epoch metrics
        mlflow.log_metrics(
            {
                "train_loss": epoch_loss,
                "train_accuracy": epoch_acc,
                "val_loss": val_loss,
                "val_accuracy": val_acc,
            },
            step=epoch,
        )
        # Log checkpoint at the end of each epoch
        mlflow.pytorch.log_model(model, name=f"checkpoint_{epoch}")

        print(
            f"Epoch {epoch+1}/{params['epochs']}, "
            f"Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.2f}%, "
            f"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%"
        )

    # Log the final trained model
    model_info = mlflow.pytorch.log_model(model, name="final_model")
```

# Use case 2: datasets tracking

# Use case 3: tuning

## Goal of hyperparameter tuning {.center}

Find the optimal set of hyperparameters that maximize a model's predictive accuracy and performance

âž” find the right balance between high bias (underfitting) and high variance (overfitting) to improve the model's ability to generalize and perform well on new, unseen data

## Tuning frameworks {.center}

Hyperparameters optimization used to be done manually following a systematic grid pattern. This was extremely inefficient

Nowadays, there are many frameworks that do it automatically, faster, and better

:::{.example}

Example:

:::

- [Optuna](https://optuna.org/)
- [Hyperopt](http://hyperopt.github.io/hyperopt/)
- [Ray Tune](https://docs.ray.io/en/latest/tune/index.html)

## Workflow {.center}

- Create an experiment
- Launch an MLflow server
- Open the user interface in a browser to visualize the logged data
- Log tracking data:
  - Prepare data
  - Define an objective function
  - Run an optimization task

## Tuning demo {.center}

:::{.figure-caption}

modified from [MLflow website](https://mlflow.org/docs/latest/ml/getting-started/hyperparameter-tuning/)

:::

For this demo, we will use [optuna](https://github.com/optuna/optuna) as the hyperparameter optimization framework, so we need to install it (in Bash, not Python):

```{.bash}
uv add optuna
```

## {.center}

### Create an experiment

```{.python}
import mlflow

mlflow.set_tracking_uri("sqlite:///demo.db")
mlflow.set_experiment("tuning_demo")
```

## {.center}

### Launch MLflow server

*In Bash/zsh (not in Python!) at root of project*

```{.bash}
mlflow server \
       --backend-store-uri sqlite:///demo.db
```

## {.center}

### Access the UI

Open <http://localhost:5000> in a browser

Select the experiment "tuning_demo"

## {.center}

### Log tracking data

## {.center}

#### Prepare data

```{.python}
import sklearn

from sklearn.model_selection import train_test_split
from sklearn.datasets import fetch_california_housing

X, y = fetch_california_housing(return_X_y=True)
X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=0)
```

## {.center}

#### Define an objective function

```{.python}
def objective(trial):
    # Setting nested=True will create a child run under the parent run.
    with mlflow.start_run(nested=True, run_name=f"trial_{trial.number}") as child_run:
        rf_max_depth = trial.suggest_int("rf_max_depth", 2, 32)
        rf_n_estimators = trial.suggest_int("rf_n_estimators", 50, 300, step=10)
        rf_max_features = trial.suggest_float("rf_max_features", 0.2, 1.0)
        params = {
            "max_depth": rf_max_depth,
            "n_estimators": rf_n_estimators,
            "max_features": rf_max_features,
        }
        # Log current trial's parameters
        mlflow.log_params(params)

        regressor_obj = sklearn.ensemble.RandomForestRegressor(**params)
        regressor_obj.fit(X_train, y_train)

        y_pred = regressor_obj.predict(X_val)
        error = sklearn.metrics.mean_squared_error(y_val, y_pred)
        # Log current trial's error metric
        mlflow.log_metrics({"error": error})

        # Log the model file
        mlflow.sklearn.log_model(regressor_obj, name="model")
        # Make it easy to retrieve the best-performing child run later
        trial.set_user_attr("run_id", child_run.info.run_id)
        return error
```

## {.center}

#### Run an optimization task

```{.python}
import optuna

# Create a parent run that contains all child runs for different trials
with mlflow.start_run(run_name="optimization") as run:
    # Log the experiment settings
    n_trials = 30
    mlflow.log_param("n_trials", n_trials)

    study = optuna.create_study(direction="minimize")
    study.optimize(objective, n_trials=n_trials)

    # Log the best trial and its run ID
    mlflow.log_params(study.best_trial.params)
    mlflow.log_metrics({"best_error": study.best_value})
    if best_run_id := study.best_trial.user_attrs.get("run_id"):
        mlflow.log_param("best_child_run_id", best_run_id)
```

# Model registry


## Model registry demo {.center}
