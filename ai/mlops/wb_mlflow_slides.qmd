---
title: Experiment tracking with
frontpic: img/logo_mlflow.svg
frontpicwidth: 45%
frontpicmargintop: 40px
frontpicmarginbottom: 40px
noshadow: noshadow
author: Marie-HÃ©lÃ¨ne Burle
date: 2025-11-25
date-format: long
execute:
  freeze: auto
  cache: true
  error: true
  echo: true
format:
  revealjs:
    <!-- embed-resources: true -->
    theme: [default, ../../revealjsmlflow.scss]
    logo: /img/favicon_sfudrac.png
    highlight-style: vim-dark
    code-line-numbers: false
    template-partials:
      - ../../title-slide.html
    pointer:
      color: "#b5111b"
      pointerSize: 32
    link-external-newwindow: true
    footer: <a href="wb_mlflow.qmd"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="rgb(1,148,226)" class="bi bi-arrow-90deg-up" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4.854 1.146a.5.5 0 0 0-.708 0l-4 4a.5.5 0 1 0 .708.708L4 2.707V12.5A2.5 2.5 0 0 0 6.5 15h8a.5.5 0 0 0 0-1h-8A1.5 1.5 0 0 1 5 12.5V2.707l3.146 3.147a.5.5 0 1 0 .708-.708l-4-4z"/></svg>&nbsp;Back to webinar page</a>
    auto-stretch: false
revealjs-plugins:
  - pointer
---

# Experiment tracking

## Lots of moving parts ... {.center}

Deep learning experiments come with a lot of components:

- Datasets
- Model architectures
- Hyperparameters

While developing an efficient model, various datasets will be trained on various architectures tuned with various hyperparameters

## ... making for challenging tracking {.center}

:::{.right}

[*hp = hyperparameter]{.small}

:::

```{dot}
//| echo: false
//| fig-height: 50%

digraph {
  
bgcolor="transparent"
node [fontname="Inconsolata, sans-serif", fontsize=9, penwidth=0.5]
edge [color=gray55, arrowhead="vee", arrowsize=0.5, penwidth=0.5]

data1 [label="data1", color=darkviolet]
data2 [label="data2", color=darkviolet]
data3 [label="data3", color=darkviolet]

hp1 [label="hp1", color=darkslategray4]
hp2 [label="hp2", color=darkslategray4]
hp3 [label="hp3", color=darkslategray4]

model1 [label="model1", color=deepskyblue3]
model2 [label="model2", color=deepskyblue3]
model3 [label="model3", color=deepskyblue3]

performance [label="performance1 ... performance27", color=darkorange4]

{data1 data2 data3} -> {model1 model2 model3} [color=darkviolet]
{hp1 hp2 hp3} -> {model1 model2 model3} [color=darkslategray4]

{model1 model2 model3} -> performance [color=deepskyblue3]
{model1 model2 model3} -> performance [color=deepskyblue3]
{model1 model2 model3} -> performance [color=deepskyblue3]
{model1 model2 model3} -> performance [color=deepskyblue3]
{model1 model2 model3} -> performance [color=deepskyblue3]
{model1 model2 model3} -> performance [color=deepskyblue3]
{model1 model2 model3} -> performance [color=deepskyblue3]
{model1 model2 model3} -> performance [color=deepskyblue3]
{model1 model2 model3} -> performance [color=deepskyblue3]

}
```

. . .

How did we get `performance19` again? ðŸ¤¯

## Experiment tracking tools  {.center}

The solution to this complexity is to use an experiment tracking tool such as [MLflow](https://mlflow.org/) and, optionally, a data versioning tool such as [DVC](https://dvc.org/)

:::{.note}

See [our webinar on DVC](wb_dvc)

:::

# MLflow

## Platform for AI life cycle {.center}

![from [MLflow website](https://mlflow.org/docs/latest/ml/)](img/mlflow_model_dev.png){fig-alt="noshadow" width="80%"}

## FOSS & compatible {.center}

- **Open-source**
- Works with **any ML or DL framework**
- **Vendor-neutral** if you run a server on a commercial platform
- **Can be combined with [dvc](https://dvc.org/) for [dataset versioning](https://dev.to/aws-builders/ml-done-right-versioning-datasets-and-models-with-dvc-mlflow-4p3f)**
- Works with any hyperparameter tuning framework &nbsp;âž” &nbsp;e.g. [integration](https://mlflow.org/docs/latest/ml/traditional-ml/tutorials/hyperparameter-tuning/notebooks/hyperparameter-tuning-with-child-runs/) with [Optuna](https://github.com/optuna/optuna) \
  &nbsp;&ensp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&ensp;&nbsp;[integration](https://docs.ray.io/en/latest/tune/examples/tune-mlflow.html) with [Ray Tune](https://github.com/ray-project/ray) \
  &nbsp;&ensp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&ensp;&nbsp;[integration](https://mlflow.org/docs/3.1.3/ml/getting-started/hyperparameter-tuning/) with [hyperopt](https://github.com/hyperopt/hyperopt)

## Used by many proprietary tools {.center}

The foundation of many proprietary no-code/low-code tuning platforms that just add a layer on top to interface with the user with text rather than code

:::{.note}

e.g. Microsoft Fabric, FLAML

:::

## Limitations {.center}

[MLflow projects](https://mlflow.org/docs/latest/ml/projects/#environment) do not (yet) support [uv](https://docs.astral.sh/uv/)

Some functionality missing for deployment and production for large companies (but irrelevant for research and no FOSS option exists)

## Installing MLflow {.center}

*With [uv](https://docs.astral.sh/uv/)*

Create a `uv` project:

```{.bash}
uv init --bare
```

Install MLflow:

```{.bash}
uv add mlflow
```

# 

## definitions {.center}

<!-- **Run**: \ -->
<!-- Single execution of a model training event -->

**Parent run**: \
One optimization task \
Contains multiple child runs

**Child run**: \
Individual execution of a model training event

**Model signature**: \
Description of a model's input and output data structure, data types, and features names

## MLflow tracking setups {.center}

![from [MLflow documentation](https://mlflow.org/docs/latest/ml/tracking/)](img/mlflow_tracking_setup.png){fig-alt="noshadow" width="85%"}

## MLflow tracking setups {.center}

::::{.columns}

:::{.column width="29%"}

![](img/mlflow_tracking_setup1.png){fig-alt="noshadow" width="65%"}

:::

:::{.column width="2%"}
:::

:::{.column width="69%"}

### Local

```{.python}
import mlflow

mlflow.set_experiment("<experiment-name>")
```

Logs get stored in an `mlruns` directory

:::{.note}

This method will be deprecated, so prefer the use of a database

:::

:::

::::

## MLflow tracking setups {.center}

::::{.columns}

:::{.column width="29%"}

![](img/mlflow_tracking_setup2.png){fig-alt="noshadow" width="80%"}

:::

:::{.column width="2%"}
:::

:::{.column width="69%"}

### Local with database

```{.python}
import mlflow

mlflow.set_tracking_uri("sqlite:///mlflow.db")
mlflow.set_experiment("<experiment-name>")
```

:::{.note}

Here we use SQLite which works well for a local database

:::

Logs get stored in an `mlflow.db` file

:::

::::

## MLflow tracking setups {.center}

::::{.columns}

:::{.column width="29%"}

![](img/mlflow_tracking_setup3.png){fig-alt="noshadow"}

:::

:::{.column width="2%"}
:::

:::{.column width="69%"}

### Remote tracking server

:::{.note}

For team development

:::

```{.python}
import mlflow

mlflow.set_tracking_uri("http://<host>:<port>")
mlflow.set_experiment("<experiment-name>")
```

:::

::::

# MLflow user interface

## MLflow UI {.center}

::::{.columns}

:::{.column width="29%"}

![](img/mlflow_tracking_setup1.png){fig-alt="noshadow" width="65%"}

:::

:::{.column width="2%"}
:::

:::{.column width="69%"}

### Local

```{.bash}
mlflow server
```

:::{.notenoit}

The server listens on <http://localhost:5000> by default, but you can choose any unused port:

```{.bash}
mlflow server --port 8080
```

:::

:::

::::

## MLflow UI {.center}

::::{.columns}

:::{.column width="29%"}

![](img/mlflow_tracking_setup2.png){fig-alt="noshadow" width="80%"}

:::

:::{.column width="2%"}
:::

:::{.column width="69%"}

### Local with database

```{.bash}
mlflow server \
	   --backend-store-uri sqlite:///mlflow.db
```

:::{.notenoit}

(Default port is `5000`)

With another port:

```{.bash}
mlflow server \
	   --backend-store-uri sqlite:///mlflow.db \
	   --port 8080
```

:::

:::

::::

## MLflow UI {.center}

::::{.columns}

:::{.column width="29%"}

![](img/mlflow_tracking_setup3.png){fig-alt="noshadow"}

:::

:::{.column width="2%"}
:::

:::{.column width="69%"}

### Remote tracking server

```{.bash}
mlflow server \
	   --host <host> \
	   --backend-store-uri postgresql+psycopg2://<username>:<password>@<host>:<port>/mlflowdb \
	   --port <port>
```

:::{.note}

Here we use PostgreSQL which works well to manage a database in a collaborative remote client-server system

(Requires installing the `psycopg2` package)

:::

:::

::::

## Access the UI {.center}

Open `http://<host>:<port>` in your browser

:::{.example}

Example:

For a local server on port `5000` (the default), open <http://localhost:5000/>

:::

## Log tracking data {.center}

The workflow looks like this:

```{.python}
import mlflow

with mlflow.start_run():
    mlflow.log_param("lr", 0.001)
    # Your ml code
    ...
    mlflow.log_metric("val_loss", val_loss)
```

## Organize runs {.center}

experiments
child runs
tags

# Tracking models

## Overview {.center}

Track models at checkpoints

Compare with different datasets

Visualize with tracking UI

# Tracking datasets

# Hyperparameter tuning

## Goal of tuning {.center}

Find the optimal set of hyperparameters that maximize a model's predictive accuracy and performance

âž” Find the right balance between high bias (underfitting) and high variance (overfitting) to improve the model's ability to generalize and perform well on new, unseen data

## Tuning frameworks {.center}

Hyperparameters optimization used to be done manually following a systematic grid pattern. This was extremely inefficient

Nowadays, there are many frameworks that do it automatically, faster, and better

:::{.example}

Example:

:::

- [Optuna](https://optuna.org/)
- [Hyperopt](http://hyperopt.github.io/hyperopt/)
- [Ray Tune](https://docs.ray.io/en/latest/tune/index.html)

## Workflow {.center}

- Define an objective function
- Define a search space
- Minimize the objective over the space

<!-- ## ML workflow {.center} -->

<!-- ```{mermaid} -->
<!-- %%| echo: false -->
<!-- flowchart LR -->
<!--    data[Data cleaning] -\-> tr1[Initial training] -->
<!--    tr1 -\-> tuning[Tuning] -->
<!--    tuning -\-> tr2[More training] -->
<!--    tr2 -\-> val[Validation] -->
<!--    val -\-> tr1 -->
<!--    val -\-> eval[Evaluation] -->
<!--    eval -\-> tr3[Final training] -->
<!--    tr3 -\-> deploy[Deployment] -->
<!-- ``` -->

# Quick demos

[from [MLflow](https://mlflow.org/) website]{.figure-caption}

## Setup {.center}

```{.python}
import mlflow
import optuna
import sklearn

mlflow.set_tracking_uri("sqlite:///xxx.db")
mlflow.set_experiment("Cal housing")
```

## Prepare data {.center}

```{.python}
from sklearn.model_selection import train_test_split
from sklearn.datasets import fetch_california_housing

X, y = fetch_california_housing(return_X_y=True)
X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=0)
```

## Define objective function {.center}

```{.python}
def objective(trial):
    # Setting nested=True will create a child run under the parent run.
    with mlflow.start_run(nested=True, run_name=f"trial_{trial.number}") as child_run:
        rf_max_depth = trial.suggest_int("rf_max_depth", 2, 32)
        rf_n_estimators = trial.suggest_int("rf_n_estimators", 50, 300, step=10)
        rf_max_features = trial.suggest_float("rf_max_features", 0.2, 1.0)
        params = {
            "max_depth": rf_max_depth,
            "n_estimators": rf_n_estimators,
            "max_features": rf_max_features,
        }
        # Log current trial's parameters
        mlflow.log_params(params)

        regressor_obj = sklearn.ensemble.RandomForestRegressor(**params)
        regressor_obj.fit(X_train, y_train)

        y_pred = regressor_obj.predict(X_val)
        error = sklearn.metrics.mean_squared_error(y_val, y_pred)
        # Log current trial's error metric
        mlflow.log_metrics({"error": error})

        # Log the model file
        mlflow.sklearn.log_model(regressor_obj, name="model")
        # Make it easy to retrieve the best-performing child run later
        trial.set_user_attr("run_id", child_run.info.run_id)
        return error
```

## Define optimization task {.center}

```{.python}
# Create a parent run that contains all child runs for different trials
with mlflow.start_run(run_name="task") as run:
    # Log the experiment settings
    n_trials = 30
    mlflow.log_param("n_trials", n_trials)

    study = optuna.create_study(direction="minimize")
    study.optimize(objective, n_trials=n_trials)

    # Log the best trial and its run ID
    mlflow.log_params(study.best_trial.params)
    mlflow.log_metrics({"best_error": study.best_value})
    if best_run_id := study.best_trial.user_attrs.get("run_id"):
        mlflow.log_param("best_child_run_id", best_run_id)
```

## Launch the UI {.center}

*In a Bash/zsh (not in Python!) at root of project*

```{.bash}
mlflow server
```

Open <http://localhost:5000/> in your browser

```{.python}
import mlflow

mlflow.set_tracking_uri("sqlite:///xxx.db")
mlflow.set_experiment("Deep Learning Experiment")

mlflow.config.enable_system_metrics_logging()
mlflow.config.set_system_metrics_sampling_interval(1)
```

```{.python}
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms

# Define device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Load and prepare data
transform = transforms.Compose(
    [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]
)
train_dataset = datasets.FashionMNIST(
    "data", train=True, download=True, transform=transform
)
test_dataset = datasets.FashionMNIST("data", train=False, transform=transform)
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=1000)
```

```{.python}
class NeuralNetwork(nn.Module):
    def __init__(self):
        super().__init__()
        self.flatten = nn.Flatten()
        self.linear_relu_stack = nn.Sequential(
            nn.Linear(28 * 28, 512),
            nn.ReLU(),
            nn.Linear(512, 512),
            nn.ReLU(),
            nn.Linear(512, 10),
        )

    def forward(self, x):
        x = self.flatten(x)
        logits = self.linear_relu_stack(x)
        return logits

model = NeuralNetwork().to(device)
```

```{.python}
# Training parameters
params = {
    "epochs": 5,
    "learning_rate": 1e-3,
    "batch_size": 64,
    "optimizer": "SGD",
    "model_type": "MLP",
    "hidden_units": [512, 512],
}

# Define optimizer and loss function
loss_fn = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=params["learning_rate"])
```

```{.python}
with mlflow.start_run() as run:
    # Log training parameters
    mlflow.log_params(params)

    for epoch in range(params["epochs"]):
        model.train()
        train_loss = correct, total = 0, 0, 0

        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.to(device), target.to(device)

            # Forward pass
            optimizer.zero_grad()
            output = model(data)
            loss = loss_fn(output, target)

            # Backward pass
            loss.backward()
            optimizer.step()

            # Calculate metrics
            train_loss += loss.item()
            _, predicted = output.max(1)
            total += target.size(0)
            correct += predicted.eq(target).sum().item()

            # Log batch metrics (every 100 batches)
            if batch_idx % 100 == 0:
                batch_loss = train_loss / (batch_idx + 1)
                batch_acc = 100.0 * correct / total
                mlflow.log_metrics(
                    {"batch_loss": batch_loss, "batch_accuracy": batch_acc},
                    step=epoch * len(train_loader) + batch_idx,
                )

        # Calculate epoch metrics
        epoch_loss = train_loss / len(train_loader)
        epoch_acc = 100.0 * correct / total

        # Validation
        model.eval()
        val_loss, val_correct, val_total = 0, 0, 0
        with torch.no_grad():
            for data, target in test_loader:
                data, target = data.to(device), target.to(device)
                output = model(data)
                loss = loss_fn(output, target)

                val_loss += loss.item()
                _, predicted = output.max(1)
                val_total += target.size(0)
                val_correct += predicted.eq(target).sum().item()

        # Calculate and log epoch validation metrics
        val_loss = val_loss / len(test_loader)
        val_acc = 100.0 * val_correct / val_total

        # Log epoch metrics
        mlflow.log_metrics(
            {
                "train_loss": epoch_loss,
                "train_accuracy": epoch_acc,
                "val_loss": val_loss,
                "val_accuracy": val_acc,
            },
            step=epoch,
        )
        # Log checkpoint at the end of each epoch
        mlflow.pytorch.log_model(model, name=f"checkpoint_{epoch}")

        print(
            f"Epoch {epoch+1}/{params['epochs']}, "
            f"Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.2f}%, "
            f"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%"
        )

    # Log the final trained model
    model_info = mlflow.pytorch.log_model(model, name="final_model")
```

