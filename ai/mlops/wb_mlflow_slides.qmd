---
title: Experiment tracking with
frontpic: ../img/logo_mlflow.svg
frontpicwidth: 45%
frontpicmargintop: 40px
frontpicmarginbottom: 40px
noshadow: noshadow
author: Marie-HÃ©lÃ¨ne Burle
<!-- date: 2025-11-25 -->
<!-- date-format: long -->
execute:
  freeze: auto
  cache: true
  error: true
  echo: true
format:
  revealjs:
    <!-- embed-resources: true -->
    theme: [default, ../../revealjsmlflow.scss]
    logo: /img/favicon_sfudrac.png
    highlight-style: monokai
    code-line-numbers: false
    template-partials:
      - ../../title-slide.html
    pointer:
      color: "#b5111b"
      pointerSize: 32
    link-external-newwindow: true
    <!-- footer: <a href="wb_mlflow.qmd"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="rgb(1,148,226)" class="bi bi-arrow-90deg-up" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4.854 1.146a.5.5 0 0 0-.708 0l-4 4a.5.5 0 1 0 .708.708L4 2.707V12.5A2.5 2.5 0 0 0 6.5 15h8a.5.5 0 0 0 0-1h-8A1.5 1.5 0 0 1 5 12.5V2.707l3.146 3.147a.5.5 0 1 0 .708-.708l-4-4z"/></svg>&nbsp;Back to webinar page</a> -->
    auto-stretch: false
revealjs-plugins:
  - pointer
---

# Experiment tracking

## Lots of moving parts ... {.center}

Deep learning experiments come with a lot of components:

- Datasets
- Model architectures
- Hyperparameters

While developing an efficient model, various datasets will be trained on various architectures tuned with various hyperparameters

## ... making for challenging tracking {.center}

:::{.right}

[*hp = hyperparameter]{.small}

:::

```{dot}
//| echo: false
//| fig-height: 50%

digraph {
  
bgcolor="transparent"
node [fontname="Inconsolata, sans-serif", fontsize=9, penwidth=0.5]
edge [color=gray55, arrowhead="vee", arrowsize=0.5, penwidth=0.5]

data1 [label="data1", color=darkviolet]
data2 [label="data2", color=darkviolet]
data3 [label="data3", color=darkviolet]

hp1 [label="hp1", color=darkslategray4]
hp2 [label="hp2", color=darkslategray4]
hp3 [label="hp3", color=darkslategray4]

model1 [label="model1", color=deepskyblue3]
model2 [label="model2", color=deepskyblue3]
model3 [label="model3", color=deepskyblue3]

performance [label="performance1 ... performance27", color=darkorange4]

{data1 data2 data3} -> {model1 model2 model3} [color=darkviolet]
{hp1 hp2 hp3} -> {model1 model2 model3} [color=darkslategray4]

{model1 model2 model3} -> performance [color=deepskyblue3]
{model1 model2 model3} -> performance [color=deepskyblue3]
{model1 model2 model3} -> performance [color=deepskyblue3]
{model1 model2 model3} -> performance [color=deepskyblue3]
{model1 model2 model3} -> performance [color=deepskyblue3]
{model1 model2 model3} -> performance [color=deepskyblue3]
{model1 model2 model3} -> performance [color=deepskyblue3]
{model1 model2 model3} -> performance [color=deepskyblue3]
{model1 model2 model3} -> performance [color=deepskyblue3]

}
```

. . .

How did we get `performance19` again? ðŸ¤¯

## Experiment tracking tools  {.center}

The solution to this complexity is to use an experiment tracking tool such as [MLflow](https://mlflow.org/) and, optionally, a data versioning tool such as [DVC](https://dvc.org/)

<!-- :::{.note} -->

<!-- See [our webinar on DVC](wb_dvc) -->

<!-- ::: -->

# MLflow

## Platform for AI life cycle {.center}

![from [MLflow website](https://mlflow.org/docs/latest/ml/)](../img/mlflow_model_dev.png){fig-alt="noshadow" width="90%"}

<!-- this webinar focuses on experiment tracking -->

## FOSS & compatible {.center}

- **Open-source**
- Works with **any ML or DL framework**
- **Vendor-neutral** if you run a server on a commercial platform
- **Can be combined with [dvc](https://dvc.org/) for [dataset versioning](https://dev.to/aws-builders/ml-done-right-versioning-datasets-and-models-with-dvc-mlflow-4p3f)**
- Works with any hyperparameter tuning framework &nbsp;âž” &nbsp;e.g. [integration](https://mlflow.org/docs/latest/ml/traditional-ml/tutorials/hyperparameter-tuning/notebooks/hyperparameter-tuning-with-child-runs/) with [Optuna](https://github.com/optuna/optuna) \
  &nbsp;&ensp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&ensp;&nbsp;[integration](https://docs.ray.io/en/latest/tune/examples/tune-mlflow.html) with [Ray Tune](https://github.com/ray-project/ray) \
  &nbsp;&ensp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&ensp;&nbsp;[integration](https://mlflow.org/docs/3.1.3/ml/getting-started/hyperparameter-tuning/) with [hyperopt](https://github.com/hyperopt/hyperopt)

## Limitations {.center}

[MLflow projects](https://mlflow.org/docs/latest/ml/projects/#environment) do not (yet) support [uv](https://docs.astral.sh/uv/)

Some functionality missing for deployment and production for large companies (but irrelevant for research and no FOSS option exists)

<!-- ## Definitions {.center} -->

<!-- :::{.notenoline} -->

<!-- (In the context of MLflow) -->

<!-- ::: -->

<!-- **Run**: single execution of a model training event -->

## Installing MLflow {.center}

*With [uv](https://docs.astral.sh/uv/)*

Create a `uv` project:

```{.bash}
uv init --bare
```

Install MLflow:

```{.bash}
uv add mlflow
```

# Tracking models

<!-- ## Overview {.center} -->

<!-- Track models at checkpoints -->

<!-- Compare with different datasets -->

<!-- Visualize with tracking UI -->

## MLflow tracking setups {.center}

![from [MLflow documentation](https://mlflow.org/docs/latest/ml/tracking/)](../img/mlflow_tracking_setup.png){fig-alt="noshadow" width="90%"}

## MLflow tracking setups {.center}

::::{.columns}

:::{.column width="29%"}

![](../img/mlflow_tracking_setup1.png){fig-alt="noshadow" width="70%"}

:::

:::{.column width="2%"}
:::

:::{.column width="69%"}

### Local

```{.bash}
mlflow ui --port 5000
```

:::{.note}

You can choose any unused port

:::

:::{.note}

This is equivalent to:

```{.bash}
mlflow server --host 127.0.0.1 --port 5000
```

:::

Logs get stored in an `mlruns` directory

:::

::::

## MLflow tracking setups {.center}

::::{.columns}

:::{.column width="29%"}

![](../img/mlflow_tracking_setup2.png){fig-alt="noshadow" width="85%"}

:::

:::{.column width="2%"}
:::

:::{.column width="69%"}

### Local with data store

```{.bash}
mlflow server \
	   --backend-store-uri sqlite:///mlflow.db \
	   --port 5000
```

:::{.note}

Here we are using SQLite which works well for a local database

:::

Logs get stored in an `mlflow.db` file

:::

::::

## MLflow tracking setups {.center}

::::{.columns}

:::{.column width="29%"}

![](../img/mlflow_tracking_setup3.png){fig-alt="noshadow"}

:::

:::{.column width="2%"}
:::

:::{.column width="69%"}

### Remote tracking server

:::{.note}

For team development

:::

```{.bash}
mlflow server \
	   --host 0.0.0.0 \
	   --backend-store-uri postgresql+psycopg2://<username>:<password>@<host>:<port>/mlflowdb
	   --port 5000
```

:::{.note}

Here we use PostgreSQL which is works well to manage a database in a client-server system

(Requires installing the `psycopg2` package)

:::

:::

::::

## Log tracking data {.center}

```{.python}
import mlflow

with mlflow.start_run():
    mlflow.log_param("lr", 0.001)
    # Your ml code
    ...
    mlflow.log_metric("val_loss", val_loss)
```

<!-- ## Organize runs {.center} -->

<!-- experiments -->
<!-- child runs -->
<!-- tags -->

## Visualize logs {.center}

- Open `http://<host>:<port>` in your browser

:::{.example}

Example:

For a local server on port `5000`, open <http://127.0.0.1:5000>

:::

- Connect your running session to the server:

```{.python}
mlflow.set_tracking_uri(uri="http://<host>:<port>")
```

:::{.example}

Example for a local server on port `5000`:

:::

```{.python}
mlflow.set_tracking_uri("http://localhost:5000")
```

<!-- # Tracking datasets -->

# Hyperparameter tuning

## Goal of tuning {.center}

Find the optimal set of hyperparameters that maximize a model's predictive accuracy and performance

âž” find the right balance between high bias (underfitting) and high variance (overfitting) to improve the model's ability to generalize and perform well on new, unseen data

## Tuning frameworks {.center}

Hyperparameters optimization used to be done manually following a systematic grid pattern. This was extremely inefficient

Nowadays, there are many frameworks that do it automatically, faster, and better

:::{.example}

Example:

:::

- [Optuna](https://optuna.org/)
- [Hyperopt](http://hyperopt.github.io/hyperopt/)
- [Ray Tune](https://docs.ray.io/en/latest/tune/index.html)

## Workflow {.center}

- Define an objective function
- Define a search space
- Minimize the objective over the space

<!-- ## ML workflow {.center} -->

<!-- ```{mermaid} -->
<!-- %%| echo: false -->
<!-- flowchart LR -->
<!--    data[Data cleaning] -\-> tr1[Initial training] -->
<!--    tr1 -\-> tuning[Tuning] -->
<!--    tuning -\-> tr2[More training] -->
<!--    tr2 -\-> val[Validation] -->
<!--    val -\-> tr1 -->
<!--    val -\-> eval[Evaluation] -->
<!--    eval -\-> tr3[Final training] -->
<!--    tr3 -\-> deploy[Deployment] -->
<!-- ``` -->

# In practice

## Initial prompts {.center}

Try the following prompts to get you started with some example code:

:::{.def}

Create a simple example of model tuning using MLflow and Optuna.

:::

:::{.llm}

Create a simple example of model tuning using MLflow and hyperopt.

:::

## Understand your code {.center}

Try the following prompts:

:::{.llm}

Explain the objective function.

:::

:::{.info}

What is the search space in this code?

:::
