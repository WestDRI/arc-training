---
title: Deep learning with JAX
<!-- description: Building and training neural networks with [![](img/logo_flax.png){width="1.4em" fig-alt="noshadow"}](https://github.com/google/flax) -->
description: Building and training neural networks with [![](img/logo_flax.png){width="1.4em" fig-alt="noshadow"}](https://github.com/google/flax)
title-block-banner: true
---

:::{.topdef}

[JAX](https://github.com/google/jax) is a fast open source Python library for function transformations (including differentiation) and array computations on accelerators (GPUs/TPUs). It provides a structural framework on which domain specific libraries can be built.

There are various options to use JAX for deep learning. A solid approach, and the one we will use in this course, is to use [TorchData](https://github.com/pytorch/data), [TensorFlow Datasets](https://github.com/tensorflow/datasets), [Grain](https://github.com/google/grain), or [Hugging Face Datasets](https://github.com/huggingface/datasets) to load the data, [Flax](https://github.com/google/flax) to build neural networks, [Optax](https://github.com/google-deepmind/optax) for optimization, and [Orbax](https://github.com/google/orbax) for checkpointing. This modular approach makes full use of JAX's enhancements while keeping a syntax PyTorch users will find familiar.

:::

<br>
[[Start course âž¤](jxai/fl_jaxdl.qmd)]{.topinline}
