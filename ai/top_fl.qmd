---
title: Deep learning with JAX
<!-- description: Building and training neural networks with [![](img/logo_flax.png){width="1.4em" fig-alt="noshadow"}](https://github.com/google/flax) -->
description: Building and training neural networks with [![](img/logo_flax.png){width="1.4em" fig-alt="noshadow"}](https://github.com/google/flax)
title-block-banner: true
---

:::{.topdef}

[JAX](https://github.com/google/jax) is a fast open source Python library for function transformations (including differentiation) and array computations on accelerators (GPUs/TPUs). These attributes make it ideal for deep learning, but JAX is not, in itself, a deep learning library: it provides a structural framework on which libraries can be built without providing domain-specific tooling.

A solid approach to make full use of JAX's flexible autodiff and enhanced efficiency while keeping a syntax PyTorch users will find familiar is to use [TorchData](https://github.com/pytorch/data), [TensorFlow Datasets](https://github.com/tensorflow/datasets), [Grain](https://github.com/google/grain), or [Hugging Face Datasets](https://github.com/huggingface/datasets) to load the data, [Flax](https://github.com/google/flax) to build neural networks, [Optax](https://github.com/google-deepmind/optax) for optimization, and [Orbax](https://github.com/google/orbax) for checkpointing. This is the workflow we will follow in this course.

This introductory course does not require any prior knowledge.

:::

<br>
[[Start course âž¤](jxai/fl_jaxdl.qmd)]{.topinline}
