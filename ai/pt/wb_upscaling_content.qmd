---
title: Super-resolution with PyTorch
author: Marie-Hélène Burle
---

:::{.def}

*Content from [the webinar slides](wb_upscaling_slides.qmd) for easier browsing.*

:::

## Definitions

:::{layout="[5, 90]"}

:::{.col1}

[LR]{.emph} \
[HR]{.emph} \
[SR]{.emph} \
[SISR]{.emph}

:::

:::{.col2}

low resolution \
high resolution \
super-resolution = reconstruction of HR images from LR images \
single-image super-resolution = SR using a single input image

:::

:::

## History of super-resolution

### 2 main periods

- A rather slow history with various interpolation algorithms of increasing complexity before deep neural networks.
- An incredibly fast evolution since the advent of deep learning (DL).

### SR history Pre-DL

[Pixel-wise interpolation prior to DL.]{.emph}

Various methods ranging from simple (e.g. [nearest-neighbour](https://en.wikipedia.org/wiki/Nearest-neighbor_interpolation), [bicubic](https://en.wikipedia.org/wiki/Bicubic_interpolation))
to complex (e.g. [Gaussian process regression](https://en.wikipedia.org/wiki/Kriging), [iterative FIR Wiener filter](https://en.wikipedia.org/wiki/Wiener_filter)) algorithms.

#### [Nearest-neighbour interpolation](https://en.wikipedia.org/wiki/Nearest-neighbor_interpolation)

Simplest method of interpolation.

Simply uses the value of the nearest pixel.

#### [Bicubic interpolation](https://en.wikipedia.org/wiki/Bicubic_interpolation)

Consists of determining the 16 coefficients $a_{ij}$ in:

$$p(x, y) = \sum_{i=0}^3\sum_{i=0}^3 a\_{ij} x^i y^j$$

### SR history with DL

Deep learning has seen a fast evolution marked by the successive emergence of various frameworks and architectures over the past 10 years.

Some key network architectures and frameworks:

- CNN
- GAN
- Transformers

These have all been applied to SR.

SR using (amongst others):

- [Convolutional Neural Networks (SRCNN)](https://arxiv.org/abs/1501.00092) – 2014
- [Random Forests](https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Schulter_Fast_and_Accurate_2015_CVPR_paper.html) – 2015
- [Perceptual loss](https://link.springer.com/chapter/10.1007/978-3-319-46475-6_43) – 2016
- [Sub-pixel CNN](https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Shi_Real-Time_Single_Image_CVPR_2016_paper.html) – 2016
- [ResNet (SRResNet) & Generative Adversarial Network (SRGAN)](https://openaccess.thecvf.com/content_cvpr_2017/html/Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper.html) – 2017
- [Enhanced SRGAN (ESRGAN)](https://openaccess.thecvf.com/content_eccv_2018_workshops/w25/html/Wang_ESRGAN_Enhanced_Super-Resolution_Generative_Adversarial_Networks_ECCVW_2018_paper.html) – 2018
- [Predictive Filter Flow (PFF)](https://arxiv.org/abs/1811.11482) – 2018
- [Densely Residual Laplacian attention Network (DRLN)](https://ieeexplore.ieee.org/abstract/document/9185010) – 2019
- [Second-order Attention Network (SAN)](https://openaccess.thecvf.com/content_CVPR_2019/html/Dai_Second-Order_Attention_Network_for_Single_Image_Super-Resolution_CVPR_2019_paper.html) – 2019
- [Learned downscaling with Content Adaptive Resampler (CAR)](https://ieeexplore.ieee.org/abstract/document/8982168) – 2019
- [Holistic Attention Network (HAN)](https://link.springer.com/chapter/10.1007/978-3-030-58610-2_12) – 2020
- [Swin Transformer](https://openaccess.thecvf.com/content/ICCV2021W/AIM/html/Liang_SwinIR_Image_Restoration_Using_Swin_Transformer_ICCVW_2021_paper.html) – 2021

#### SRCNN

![Dong, C., Loy, C. C., He, K., & Tang, X. (2015). Image super-resolution using deep convolutional networks. IEEE transactions on pattern analysis and machine intelligence, 38(2), 295-307](img/srcnn1.png)

> Given a low-resolution image Y, the first convolutional layer of the SRCNN extracts a set of feature maps. The second layer maps these feature maps nonlinearly to high-resolution patch representations. The last layer combines the predictions within a spatial neighbourhood to produce the final high-resolution image F(Y)

Can use sparse-coding-based methods.

![Dong, C., Loy, C. C., He, K., & Tang, X. (2015). Image super-resolution using deep convolutional networks. IEEE transactions on pattern analysis and machine intelligence, 38(2), 295-307](img/srcnn2.png)

#### SRGAN

Do not provide the best PSNR, but can give more realistic results by providing more texture (less smoothing).

#### GAN

![[Stevens E., Antiga L., & Viehmann T. (2020). Deep Learning with PyTorch](https://www.manning.com/books/deep-learning-with-pytorch)](img/gan.png)

#### SRGAN

![Ledig, C., Theis, L., Huszár, F., Caballero, J., Cunningham, A., Acosta, A., ... & Shi, W. (2017). Photo-realistic single image super-resolution using a generative adversarial network. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 4681-4690)](img/srgan.jpg)

Followed by the ESRGAN and many other flavours of SRGANs.

## SwinIR

### Attention

:::{.note}

Mnih, V., Heess, N., & Graves, A. (2014). Recurrent models of visual attention. In Advances in neural information processing systems (pp. 2204-2212).

:::

(cited 2769 times)

:::{.note}

Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. In Advances in neural information processing systems (pp. 5998-6008)

:::

(cited 30999 times...)

### Transformers

![Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. In Advances in neural information processing systems (pp. 5998-6008)](img/transformer.png){width="50%" fig-alt="noshadow"}

Initially used for NLP to replace RNN as they allow parallelization. Now entering the domain of vision and others. Very performant with relatively few parameters.

### Swin Transformer

The [Swin Transformer](https://arxiv.org/abs/2103.14030) improved the use of transformers to the vision domain.

Swin = Shifted WINdows

Swin transformer (left) vs transformer as initially applied to vision (right):

![Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., ... & Guo, B. (2021). Swin transformer: Hierarchical vision transformer using shifted windows. arXiv preprint arXiv:2103.14030](img/swint.png)

### SwinIR

![Liang, J., Cao, J., Sun, G., Zhang, K., Van Gool, L., & Timofte, R. (2021). SwinIR: Image restoration using swin transformer. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 1833-1844)](img/SwinIR_archi.png)

### Training sets used

[DIV2K](https://data.vision.ee.ethz.ch/cvl/DIV2K/), Flickr2K, and other datasets.

### Models assessment

3 metrics commonly used:

[Peak sign-to-noise ratio (PSNR) measured in dB](https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio):

$\frac{\text{Maximum possible power of signal}}{\text{Power of noise (calculated as the mean squared error)}}$

[Calculated at the pixel level]{.notenoline}

$$PSNR = 10\,\cdot\,log_{10}\,\left(\frac{MAX_I^2}{MSE}\right)$$

[Structural similarity index measure (SSIM)](https://en.wikipedia.org/wiki/Structural_similarity):

Prediction of perceived image quality based on a "perfect" reference image.

$$SSIM(x,y) = \frac{(2\mu_x\mu_y + c_1) + (2 \sigma _{xy} + c_2)}
    {(\mu_x^2 + \mu_y^2+c_1) (\sigma_x^2 + \sigma_y^2+c_2)}$$

[Mean opinion score (MOS)](https://en.wikipedia.org/wiki/Mean_opinion_score):

Mean of subjective quality ratings.

$$MOS = \frac{\sum_{n=1}^N R\_n}{N}$$

### Metrics implementation

- Implement them yourself (using `torch.log10`, etc.).
- Use some library that implements them (e.g. [kornia](https://github.com/kornia/kornia/tree/master/kornia/losses)).
- Use code of open source project with good implementation (e.g. [SwinIR](https://github.com/JingyunLiang/SwinIR/blob/main/utils/util_calculate_psnr_ssim.py)).
- Use some higher level library that provides them (e.g. [ignite](https://pytorch.org/ignite/metrics.html)).
- Implement them yourself (using `torch.log10`, etc.).
- [Use some library that implements them (e.g. [kornia](https://github.com/kornia/kornia/tree/master/kornia/losses))]{.emph}.
- Use code of open source project with good implementation (e.g. [SwinIR](https://github.com/JingyunLiang/SwinIR/blob/main/utils/util_calculate_psnr_ssim.py)).
- Use some higher level library that provides them (e.g. [ignite](https://pytorch.org/ignite/metrics.html)).

```{.python}
import kornia

psnr_value = kornia.metrics.psnr(input, target, max_val)
ssim_value = kornia.metrics.ssim(img1, img2, window_size, max_val=1.0, eps=1e-12)
```

See the Kornia documentation for more info on [kornia.metrics.psnr](https://kornia.readthedocs.io/en/latest/metrics.html?highlight=psnr#kornia.metrics.psnr) and [kornia.metrics.ssim](https://kornia.readthedocs.io/en/latest/metrics.html?highlight=psnr#kornia.metrics.ssim).

### Benchmark datasets

[Set5](https://paperswithcode.com/dataset/set5):

![](img/set5.png)

[Set14](https://paperswithcode.com/dataset/set14):

![](img/set14.jpg)

[BSD100 (Berkeley Segmentation Dataset)](https://paperswithcode.com/dataset/bsd100):

![](img/bsd100.jpg)

### The Set5 dataset

A dataset consisting of 5 images which has been used [for at least 18 years](http://people.rennes.inria.fr/Aline.Roumy/results/SR_BMVC12.html) to assess SR methods.

### How to get the dataset?

From the [HuggingFace Datasets Hub](https://huggingface.co/datasets) with the HuggingFace [datasets](https://pypi.org/project/datasets/) package:

```{.python}
from datasets import load_dataset

set5 = load_dataset('eugenesiow/Set5', 'bicubic_x4', split='validation')
```
### Dataset exploration

```{.python}
print(set5)
len(set5)
set5[0]
set5.shape
set5.column_names
set5.features
set5.set_format('torch', columns=['hr', 'lr'])
set5.format
```

### Benchmarks

[A 2012 review of interpolation methods for SR](https://ieeexplore.ieee.org/abstract/document/6411957) gives the metrics for a series of interpolation methods (using other datasets).

:::{layout="[50, 50]"}

:::{.col1}

![](img/1_interpolation_psnr1.png)

![](img/3_interpolation_psnr2.png)

:::

:::{.col2}

![](img/2_interpolation_ssim1.png)

![](img/4_interpolation_ssim2.png)

:::

:::

### Interpolation methods

![](img/1_interpolation_psnr1_mean.png){width="80%"}

![](img/3_interpolation_psnr2_mean.png){width="80%"}

![](img/2_interpolation_ssim1_mean.png){width="80%"}

![](img/4_interpolation_ssim2_mean.png){width="80%"}

### DL methods

[The Papers with Code website](https://paperswithcode.com/) lists [available benchmarks on Set5](https://paperswithcode.com/sota/image-super-resolution-on-set5-4x-upscaling):

![[The Papers with Code website](https://paperswithcode.com/)](img/psnr_ssim_set5.png)

PSNR vs number of parameters for different methods on Set5x4:

![Liang, J., Cao, J., Sun, G., Zhang, K., Van Gool, L., & Timofte, R. (2021). SwinIR: Image restoration using swin transformer. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 1833-1844)](img/SwinIR_benchmark.png){width="80%" fig-alt="noshadow"}

Comparison between SwinIR & a representative CNN-based model (RCAN) on classical SR images x4:

![Liang, J., Cao, J., Sun, G., Zhang, K., Van Gool, L., & Timofte, R. (2021). SwinIR: Image restoration using swin transformer. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 1833-1844)](img/SwinIR_CNN_comparison1.png)

![Liang, J., Cao, J., Sun, G., Zhang, K., Van Gool, L., & Timofte, R. (2021). SwinIR: Image restoration using swin transformer. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 1833-1844)](img/SwinIR_CNN_comparison2.png){width="30%"}

![Liang, J., Cao, J., Sun, G., Zhang, K., Van Gool, L., & Timofte, R. (2021). SwinIR: Image restoration using swin transformer. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 1833-1844)](img/SwinIR_demo.jpg)

### Let's use SwinIR

```sh
# Get the model
git clone git@github.com:JingyunLiang/SwinIR.git
cd SwinIR

# Copy our test images in the repo
cp -r <some/path>/my_tests /testsets/my_tests

# Run the model on our images
python main_test_swinir.py --tile 400 --task real_sr --scale 4 --large_model --model_path model_zoo/swinir/003_realSR_BSRGAN_DFOWMFC_s64w8_SwinIR-L_x4_GAN.pth --folder_lq testsets/my_tests
```

Ran in 9 min on my machine with one GPU and 32GB of RAM.

### Results

:::{layout="[50, 50]"}

:::{.col1}

Initial images:

<br>
![](img/lr/berlin_1945_1.jpg)

<br>
![](img/lr_zoom/berlin_1945_1.jpg)

<br>
![](img/lr/berlin_1945_2.jpg)

<br>
![](img/lr_zoom/berlin_1945_2.jpg)

<br>
![](img/lr/bruegel.jpg)

<br>
![](img/lr_zoom/bruegel.jpg)

<br>
![](img/lr/vasarely.jpg)

<br>
![](img/lr_zoom/vasarely.jpg)

<br>
![](img/lr/bird.png)

<br>
![](img/lr_zoom/bird.png)

:::

:::{.col2}

Upscaled images:

<br>
![](img/hr/berlin_1945_1.jpg)

<br>
![](img/hr_zoom/berlin_1945_1.png)

<br>
![](img/hr/berlin_1945_2.jpg)

<br>
![](img/hr_zoom/berlin_1945_2.png)

<br>
![](img/hr/bruegel.jpg)

<br>
![](img/hr_zoom/bruegel.png)

<br>
![](img/hr/vasarely.jpg)

<br>
![](img/hr_zoom/vasarely.png)

<br>
![](img/hr/bird.jpg)

<br>
![](img/hr_zoom/bird.png)

:::

:::

### Metrics

We could use the [PSNR and SSIM implementations from SwinIR](https://github.com/JingyunLiang/SwinIR/blob/main/utils/util_calculate_psnr_ssim.py), but let's try the [Kornia](https://kornia.readthedocs.io/en/latest/index.html) functions we mentioned earlier:

- [kornia.metrics.psnr](https://kornia.readthedocs.io/en/latest/metrics.html?highlight=psnr#kornia.metrics.psnr)
- [kornia.metrics.ssim](https://kornia.readthedocs.io/en/latest/metrics.html?highlight=psnr#kornia.metrics.ssim)

Let's load the libraries we need:

```{.python}
import kornia
from PIL import Image
import torch
from torchvision import transforms
```

Then, we load one pair images (LR and HR):

```{.python}
berlin1_lr = Image.open("<some/path>/lr/berlin_1945_1.jpg")
berlin1_hr = Image.open("<some/path>/hr/berlin_1945_1.png")
```

We can display these images with:

```{.python}
berlin1_lr.show()
berlin1_hr.show()
```

Now, we need to resize them so that they have identical dimensions and turn them into tensors:

```{.python}
preprocess = transforms.Compose([
        transforms.Resize(256),
        transforms.ToTensor()
        ])

berlin1_lr_t = preprocess(berlin1_lr)
berlin1_hr_t = preprocess(berlin1_hr)
```

```{.python}
berlin1_lr_t.shape
berlin1_hr_t.shape
```

```
torch.Size([3, 267, 256])
torch.Size([3, 267, 256])
```

We now have tensors with 3 dimensions:

- the channels (RGB),
- the height of the image (in pixels),
- the width of the image (in pixels).

As data processing is done in batch in ML, we need to add a 4th dimension: the **batch size**.

(It will be equal to `1` since we have a batch size of a single image).

```{.python}
batch_berlin1_lr_t = torch.unsqueeze(berlin1_lr_t, 0)
batch_berlin1_hr_t = torch.unsqueeze(berlin1_hr_t, 0)
```

Our new tensors are now ready:

```{.python}
batch_berlin1_lr_t.shape
batch_berlin1_hr_t.shape
```

```
torch.Size([1, 3, 267, 256])
torch.Size([1, 3, 267, 256])
```

### PSNR

```{.python}
psnr_value = kornia.metrics.psnr(batch_berlin1_lr_t, batch_berlin1_hr_t, max_val=1.0)
psnr_value.item()
```

```
33.379642486572266
```

### SSIM

```{.python}
ssim_map = kornia.metrics.ssim(
    batch_berlin1_lr_t, batch_berlin1_hr_t, window_size=5, max_val=1.0, eps=1e-12)

ssim_map.mean().item()
```

```
0.9868119359016418
```
