---
title: NN vs biological neurons <br>Types of NN
aliases:
  - nn_slides.html
frontlogo: /img/logo_sfudrac.png
author: Marie-Hélène Burle
execute:
  freeze: auto
  cache: true
  error: true
  echo: true
format:
  revealjs:
    embed-resources: true
    theme: [default, ../../revealjs.scss]
    logo: /img/favicon_sfudrac.png
    highlight-style: monokai
    code-line-numbers: false
    template-partials:
      - ../../title-slide.html
    pointer:
      color: "#b5111b"
      pointerSize: 32
    link-external-newwindow: true
    footer: <a href="pt_nn.qmd"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="rgb(153, 153, 153)" class="bi bi-arrow-90deg-up" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4.854 1.146a.5.5 0 0 0-.708 0l-4 4a.5.5 0 1 0 .708.708L4 2.707V12.5A2.5 2.5 0 0 0 6.5 15h8a.5.5 0 0 0 0-1h-8A1.5 1.5 0 0 1 5 12.5V2.707l3.146 3.147a.5.5 0 1 0 .708-.708l-4-4z"/></svg>&nbsp;Back to the course</a>
revealjs-plugins:
  - pointer
---

::::{.columns}

:::{.column width="45%"}

In biological networks, the information consists of action potentials (neuron membrane rapid depolarizations) propagating through the network. In artificial ones, the information consists of tensors (multidimensional arrays) of weights and biases: each unit passes a weighted sum of an input tensor with an additional—possibly weighted—bias through an activation function before passing on the output tensor to the next layer of units.

:::

:::{.column width="10%"}

:::

:::{.column width="45%"}

Artificial neural networks are a series of layered units mimicking the concept of biological neurons: inputs are received by every unit of a layer, computed, then transmitted to units of the next layer. In the process of learning, experience strengthens some connections between units and weakens others.

:::

::::

---

::::{.columns}

:::{.column width="50%"}
<br>
<center>*Schematic of a biological neuron:*</center>

![From [Dhp1080, Wikipedia](https://commons.wikimedia.org/w/index.php?curid=1474927)](../img/neuron.png){fig-alt="noshadow" width="90%"}

:::

:::{.column width="50%"}
<br>
<center>*Schematic of an artificial neuron:*</center>

![Modified from [O.C. Akgun & J. Mei 2019](https://royalsocietypublishing.org/doi/10.1098/rsta.2019.0163)](../img/artificial_neuron.png){fig-alt="noshadow" width=90%}

:::

::::

---

While biological neurons are connected in extremely intricate patterns, artificial ones follow a layered structure. Another difference in complexity is in the number of units: the human brain has 65–90 billion neurons. ANN have much fewer units.

::::{.columns}

:::{.column width="50%"}
<br>
<center>*Neurons in mouse cortex:*</center>

![Neurons are in green, the dark branches are blood vessels. Image by [Na Ji, UC Berkeley](https://news.berkeley.edu/2020/03/19/high-speed-microscope-captures-fleeting-brain-signals/)](../img/brain_neurons.jpg){fig-alt="noshadow" width=77%}

:::

:::{.column width="50%"}
<br>
<center>*Neural network with 2 hidden layers:*</center>

![From [The Maverick Meerkat](https://themaverickmeerkat.com/2020-01-10-TicTacToe/)](../img/nn_multi_layer.png){fig-alt="noshadow" width=73%}

:::

::::

---

The information in biological neurons is an all-or-nothing electrochemical pulse or action potential. Greater stimuli don’t produce stronger signals but increase firing frequency. In contrast, artificial neurons pass the computation of their inputs through an activation function and the output can take any of the values possible with that function.

::::{.columns}

:::{.column width="50%"}

<center>*Threshold potential in biological neurons:*</center>

![Modified from [Blacktc, Wikimedia](https://commons.wikimedia.org/w/index.php?curid=78013076)](../img/all_none_law.png){fig-alt="noshadow" width=70%}

:::

:::{.column width="50%"}

<center>*Some common activation functions in ANNs:*</center>

![From [Diganta Misra 2019](https://arxiv.org/abs/1908.08681)](../img/act_func.png){fig-alt="noshadow" width=69%}

:::

::::

---

Central to both systems is the concept of learning.

::::{.columns}

:::{.column width="45%"}

The process of learning in biological NN happens through neuron death or growth and the creation or loss of synaptic connections between neurons.

:::

:::{.column width="10%"}

:::

:::{.column width="45%"}

In ANN, learning happens through optimization algorithms such as gradient descent which minimize cross entropy loss functions by adjusting the weights and biases connecting each layer of neurons over many iterations.

:::

::::

# Types of ANN

## Fully connected neural networks {.center}

::::{.columns}

:::{.column width="50%"}

![From [Glosser.ca, Wikipedia](https://commons.wikimedia.org/w/index.php?curid=24913461)](../img/nn.png){fig-alt="noshadow" width=60%}

:::

:::{.column width="50%"}

Each neuron receives inputs from every neuron of the previous layer and passes its output to every neuron of the next layer.

:::

::::

## Convolutional neural networks

![](../img/cnn.png){fig-alt="noshadow"}

:::{.caption}

From [Programming Journeys by Rensu Theart](https://codetolight.wordpress.com/2017/11/29/getting-started-with-pytorch-for-deep-learning-part-3-neural-network-basics/)

:::

Convolutional neural networks (CNN) are used for spatially structured data (e.g. images).

Images have huge input sizes and would require a very large number of neurons in a fully connected neural net. In convolutional layers, neurons receive input from a subarea (called *local receptive field*) of the previous layer. This greatly reduces the number of parameters. Optionally, pooling (combining the outputs of neurons in a subarea) reduces the data dimensions.

## Recurrent neural networks

![](../img/rnn.png){fig-alt="noshadow"}

:::{.caption}

From [fdeloche, Wikipedia](https://commons.wikimedia.org/wiki/File:Recurrent_neural_network_unfold.svg)

:::

Recurrent neural networks (RNN) such as Long Short-Term Memory (LSTM) are used for chain structured data (e.g. text).

They are not feedforward networks (i.e. networks for which the information moves only in the forward direction without any loop).

## Transformers

A combination of two RNNs (the *encoder* and the *decoder*) is used in sequence to sequence models for translation or picture captioning.

In 2014 the concept of *attention* (giving added weight to important words) was developed, greatly improving the ability of such models to process a lot of data.

The problem with recurrence is that it is not easily to parallelize (and thus to run fast on GPUs).

In 2017, a new model—the [transformer](https://arxiv.org/abs/1706.03762)—was proposed: by using only attention mechanisms and no recurrence, the transformer achieves better results in an easily parallelizable fashion.

With the addition of transfer learning, powerful transformers emerged in the field of [NLP](https://en.wikipedia.org/wiki/Natural_language_processing) (e.g. [Bidirectional Encoder Representations from Transformers (BERT)](https://arxiv.org/abs/1810.04805) from Google and [Generative Pre-trained Transformer-3 (GPT-3)](https://arxiv.org/pdf/2005.14165) from [OpenAI](https://openai.com/)).

---

::::{layout="[43, 57]" layout-valign="bottom"}

![](img/transformer.png){fig-alt="noshadow"}

:::{.caption}

[[Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30](https://proceedings.neurips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html)]{.caption}
<br><br><br>

:::

::::
