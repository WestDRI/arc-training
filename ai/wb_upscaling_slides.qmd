---
title: Super-resolution with PyTorch
aliases:
  - upscaling_slides.html
frontpic: "img/superresolution.jpg"
frontpicwidth: 40%
author: Marie-Hélène Burle
date: 2021-11-24
date-format: long
execute:
  error: true
  echo: true
format:
  revealjs:
    embed-resources: true
    theme: [default, ../revealjs.scss]
    logo: /img/logo_sfudrac.png
    highlight-style: ayu
    code-line-numbers: false
    template-partials:
      - ../title-slide.html
    pointer:
      color: "#b5111b"
      pointerSize: 32
    link-external-newwindow: true
    footer: <a href="wb_upscaling.html"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="rgb(153, 153, 153)" class="bi bi-arrow-90deg-up" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4.854 1.146a.5.5 0 0 0-.708 0l-4 4a.5.5 0 1 0 .708.708L4 2.707V12.5A2.5 2.5 0 0 0 6.5 15h8a.5.5 0 0 0 0-1h-8A1.5 1.5 0 0 1 5 12.5V2.707l3.146 3.147a.5.5 0 1 0 .708-.708l-4-4z"/></svg>&nbsp;Back to webinar page</a>
revealjs-plugins:
  - pointer
---

# Definitions

[LR:]{.emph} &ensp;&ensp;&nbsp;low resolution

[HR:]{.emph} &ensp;&ensp;&nbsp;high resolution

[SR:]{.emph} &ensp;&ensp;&nbsp;super-resolution = reconstruction of HR images from LR images

[SISR:]{.emph} &nbsp;&nbsp;single-image super-resolution = SR using a single input image

# History of super-resolution

## Can be broken down into 2 main periods: {.center}

- A rather slow history with various interpolation algorithms of increasing complexity before deep neural networks

- An incredibly fast evolution since the advent of deep learning (DL)

## SR history Pre-DL {.center}

[Pixel-wise interpolation prior to DL]{.emph}

Various methods ranging from simple (e.g. [nearest-neighbour](https://en.wikipedia.org/wiki/Nearest-neighbor_interpolation), [bicubic](https://en.wikipedia.org/wiki/Bicubic_interpolation))
to complex (e.g. [Gaussian process regression](https://en.wikipedia.org/wiki/Kriging), [iterative FIR Wiener filter](https://en.wikipedia.org/wiki/Wiener_filter)) algorithms

## SR history Pre-DL {.center}

### [Nearest-neighbour interpolation](https://en.wikipedia.org/wiki/Nearest-neighbor_interpolation)

Simplest method of interpolation

Simply uses the value of the nearest pixel

### [Bicubic interpolation](https://en.wikipedia.org/wiki/Bicubic_interpolation)

Consists of determining the 16 coefficients $a_{ij}$ in:

$$p(x, y) = \sum_{i=0}^3\sum_{i=0}^3 a\_{ij} x^i y^j$$

## SR history with DL {.center}

Deep learning has seen a fast evolution marked by the successive emergence of various frameworks and architectures over the past 10 years

Some key network architectures and frameworks:

- CNN
- GAN
- Transformers

These have all been applied to SR

---

### SR using (amongst others):

- [Convolutional Neural Networks (SRCNN)](https://arxiv.org/abs/1501.00092) – 2014
- [Random Forests](https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Schulter_Fast_and_Accurate_2015_CVPR_paper.html) – 2015
- [Perceptual loss](https://link.springer.com/chapter/10.1007/978-3-319-46475-6_43) – 2016
- [Sub-pixel CNN](https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Shi_Real-Time_Single_Image_CVPR_2016_paper.html) – 2016
- [ResNet (SRResNet) & Generative Adversarial Network (SRGAN)](https://openaccess.thecvf.com/content_cvpr_2017/html/Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper.html) – 2017
- [Enhanced SRGAN (ESRGAN)](https://openaccess.thecvf.com/content_eccv_2018_workshops/w25/html/Wang_ESRGAN_Enhanced_Super-Resolution_Generative_Adversarial_Networks_ECCVW_2018_paper.html) – 2018
- [Predictive Filter Flow (PFF)](https://arxiv.org/abs/1811.11482) – 2018
- [Densely Residual Laplacian attention Network (DRLN)](https://ieeexplore.ieee.org/abstract/document/9185010) – 2019
- [Second-order Attention Network (SAN)](https://openaccess.thecvf.com/content_CVPR_2019/html/Dai_Second-Order_Attention_Network_for_Single_Image_Super-Resolution_CVPR_2019_paper.html) – 2019
- [Learned downscaling with Content Adaptive Resampler (CAR)](https://ieeexplore.ieee.org/abstract/document/8982168) – 2019
- [Holistic Attention Network (HAN)](https://link.springer.com/chapter/10.1007/978-3-030-58610-2_12) – 2020
- [Swin Transformer](https://openaccess.thecvf.com/content/ICCV2021W/AIM/html/Liang_SwinIR_Image_Restoration_Using_Swin_Transformer_ICCVW_2021_paper.html) – 2021

## SRCNN {.center}

![](img/srcnn1.png)

:::{.caption}

Dong, C., Loy, C. C., He, K., & Tang, X. (2015). Image super-resolution using deep convolutional networks. IEEE transactions on pattern analysis and machine intelligence, 38(2), 295-307

:::

> Given a low-resolution image Y, the first convolutional layer of the SRCNN extracts a set of feature maps. The second layer maps these feature maps nonlinearly to high-resolution patch representations. The last layer combines the predictions within a spatial neighbourhood to produce the final high-resolution image F(Y)

## SRCNN {.center}

Can use sparse-coding-based methods

![](img/srcnn2.png)

:::{.caption}

Dong, C., Loy, C. C., He, K., & Tang, X. (2015). Image super-resolution using deep convolutional networks. IEEE transactions on pattern analysis and machine intelligence, 38(2), 295-307

:::

## SRGAN {.center}

Do not provide the best PSNR, but can give more realistic results by providing more texture (less smoothing)

## GAN {.center}

![](img/gan.png)

:::{.caption}

[Stevens E., Antiga L., & Viehmann T. (2020). Deep Learning with PyTorch](https://www.manning.com/books/deep-learning-with-pytorch)

:::

## SRGAN {.center}

![](img/srgan.jpg)

:::{.caption}

Ledig, C., Theis, L., Huszár, F., Caballero, J., Cunningham, A., Acosta, A., ... & Shi, W. (2017). Photo-realistic single image super-resolution using a generative adversarial network. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 4681-4690)

:::

## SRGAN {.center}

Followed by the ESRGAN and many other flavours of SRGANs

# SwinIR

## Attention {.center}

:::{.note}

Mnih, V., Heess, N., & Graves, A. (2014). Recurrent models of visual attention. In Advances in neural information processing systems (pp. 2204-2212)

:::

(cited 2769 times)

:::{.note}

Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. In Advances in neural information processing systems (pp. 5998-6008)

:::

(cited 30999 times...)

## Transformers {.center}

![](img/transformer.png)

:::{.caption}

Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. In Advances in neural information processing systems (pp. 5998-6008)

:::

## Transformers {.center}

Initially used for NLP to replace RNN as they allow parallelization
Now entering the domain of vision and others
Very performant with relatively few parameters

## Swin Transformer {.center}

The [Swin Transformer](https://arxiv.org/abs/2103.14030) improved the use of transformers to the vision domain

Swin = Shifted WINdows

## Swin Transformer {.center}

Swin transformer (left) vs transformer as initially applied to vision (right):

![](img/swint.png)

:::{.caption}

Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., ... & Guo, B. (2021). Swin transformer: Hierarchical vision transformer using shifted windows. arXiv preprint arXiv:2103.14030

:::

## SwinIR {.center}

![](img/SwinIR_archi.png)

:::{.caption}

Liang, J., Cao, J., Sun, G., Zhang, K., Van Gool, L., & Timofte, R. (2021). SwinIR: Image restoration using swin transformer. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 1833-1844)

:::

## Training sets used {.center}

[DIV2K](https://data.vision.ee.ethz.ch/cvl/DIV2K/), Flickr2K, and other datasets

## Models assessment {.center}

3 metrics commonly used:

#### [Peak sign-to-noise ratio (PSNR) measured in dB](https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio) {.center}

$\frac{\text{Maximum possible power of signal}}{\text{Power of noise (calculated as the mean squared error)}}$

[Calculated at the pixel level]{.note}

#### [Structural similarity index measure (SSIM)](https://en.wikipedia.org/wiki/Structural_similarity) {.center}

Prediction of perceived image quality based on a "perfect" reference image

#### [Mean opinion score (MOS)](https://en.wikipedia.org/wiki/Mean_opinion_score) {.center}

Mean of subjective quality ratings

## Models assessment {.center}

### [Peak sign-to-noise ratio (PSNR) measured in dB](https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio) {.center}

$$PSNR = 10\,\cdot\,log_{10}\,\left(\frac{MAX_I^2}{MSE}\right)$$

### [Structural similarity index measure (SSIM)](https://en.wikipedia.org/wiki/Structural_similarity) {.center}

$$SSIM(x,y) = \frac{(2\mu_x\mu_y + c_1) + (2 \sigma _{xy} + c_2)} 
    {(\mu_x^2 + \mu_y^2+c_1) (\sigma_x^2 + \sigma_y^2+c_2)}$$

### [Mean opinion score (MOS)](https://en.wikipedia.org/wiki/Mean_opinion_score) {.center}

$$MOS = \frac{\sum_{n=1}^N R\_n}{N}$$

## Metrics implementation {.center}

- Implement them yourself (using `torch.log10`, etc.)

- Use some library that implements them (e.g. [kornia](https://github.com/kornia/kornia/tree/master/kornia/losses))

- Use code of open source project with good implementation (e.g. [SwinIR](https://github.com/JingyunLiang/SwinIR/blob/main/utils/util_calculate_psnr_ssim.py))

- Use some higher level library that provides them (e.g. [ignite](https://pytorch.org/ignite/metrics.html))

## Metrics implementation {.center}

- Implement them yourself (using `torch.log10`, etc.)

- [Use some library that implements them (e.g. [kornia](https://github.com/kornia/kornia/tree/master/kornia/losses))]{.emph}

- Use code of open source project with good implementation (e.g. [SwinIR](https://github.com/JingyunLiang/SwinIR/blob/main/utils/util_calculate_psnr_ssim.py))

- Use some higher level library that provides them (e.g. [ignite](https://pytorch.org/ignite/metrics.html))

## Metrics implementation {.center}

```{.python}
import kornia

psnr_value = kornia.metrics.psnr(input, target, max_val)
ssim_value = kornia.metrics.ssim(img1, img2, window_size, max_val=1.0, eps=1e-12)
```

See the Kornia documentation for more info on [kornia.metrics.psnr](https://kornia.readthedocs.io/en/latest/metrics.html?highlight=psnr#kornia.metrics.psnr) & [kornia.metrics.ssim](https://kornia.readthedocs.io/en/latest/metrics.html?highlight=psnr#kornia.metrics.ssim)

## Benchmark datasets {.center}

### [Set5](https://paperswithcode.com/dataset/set5)

![](img/set5.png){width="30%"}

### [Set14](https://paperswithcode.com/dataset/set14)

![](img/set14.jpg){width="70%"}

### [BSD100 (Berkeley Segmentation Dataset)](https://paperswithcode.com/dataset/bsd100)

![](img/bsd100.jpg){width="70%"}

## Benchmark datasets {.center}

### [Set5]{.emph}

![](img/set5.png){width="30%"}

### [Set14](https://paperswithcode.com/dataset/set14)

![](img/set14.jpg){width="70%"}

### [BSD100 (Berkeley Segmentation Dataset)](https://paperswithcode.com/dataset/bsd100)

![](img/bsd100.jpg){width="70%"}

## The Set5 dataset {.center}

A dataset consisting of 5 images which has been used [for at least 18 years](http://people.rennes.inria.fr/Aline.Roumy/results/SR_BMVC12.html) to assess SR methods

## How to get the dataset? {.center}

From the [HuggingFace Datasets Hub](https://huggingface.co/datasets) with the HuggingFace [datasets](https://pypi.org/project/datasets/) package:

```{.python}
from datasets import load_dataset

set5 = load_dataset('eugenesiow/Set5', 'bicubic_x4', split='validation')
```
## Dataset exploration {.center}

```{.python}
print(set5)
len(set5)
set5[0]
set5.shape
set5.column_names
set5.features
set5.set_format('torch', columns=['hr', 'lr'])
set5.format
```

## Benchmarks {.center}

[A 2012 review of interpolation methods for SR](https://ieeexplore.ieee.org/abstract/document/6411957) gives the metrics for a series of interpolation methods (using other datasets)

---

::::{.columns}

:::{.column}

![](img/1_interpolation_psnr1.png){width="80%"}

![](img/3_interpolation_psnr2.png){width="80%"}

:::

:::{.column}

![](img/2_interpolation_ssim1.png){width="80%"}

![](img/4_interpolation_ssim2.png){width="80%"}

:::

::::

## Interpolation methods {.center}

![](img/1_interpolation_psnr1_mean.png){width="80%"}

![](img/3_interpolation_psnr2_mean.png){width="80%"}

![](img/2_interpolation_ssim1_mean.png){width="80%"}

![](img/4_interpolation_ssim2_mean.png){width="80%"}

## DL methods {.center}

[The Papers with Code website](https://paperswithcode.com/) lists [available benchmarks on Set5](https://paperswithcode.com/sota/image-super-resolution-on-set5-4x-upscaling)

---

![](img/psnr_ssim_set5.png){width="60%"}

---

#### PSNR vs number of parameters for different methods on Set5x4

![](img/SwinIR_benchmark.png){width="80%"}

:::{.caption}

Liang, J., Cao, J., Sun, G., Zhang, K., Van Gool, L., & Timofte, R. (2021). SwinIR: Image restoration using swin transformer. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 1833-1844)

:::

---

#### Comparison between SwinIR & a representative CNN-based model (RCAN) on classical SR images x4

![](img/SwinIR_CNN_comparison1.png)

:::{.caption}

Liang, J., Cao, J., Sun, G., Zhang, K., Van Gool, L., & Timofte, R. (2021). SwinIR: Image restoration using swin transformer. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 1833-1844)

:::

---

#### Comparison between SwinIR & a representative CNN-based model (RCAN) on classical SR images x4

![](img/SwinIR_CNN_comparison2.png)

:::{.caption}

Liang, J., Cao, J., Sun, G., Zhang, K., Van Gool, L., & Timofte, R. (2021). SwinIR: Image restoration using swin transformer. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 1833-1844)

:::

---

![](img/SwinIR_demo.jpg)

:::{.caption}

Liang, J., Cao, J., Sun, G., Zhang, K., Van Gool, L., & Timofte, R. (2021). SwinIR: Image restoration using swin transformer. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 1833-1844)

:::

## Let's use SwinIR {.center}

```sh
# Get the model
git clone git@github.com:JingyunLiang/SwinIR.git
cd SwinIR

# Copy our test images in the repo
cp -r <some/path>/my_tests /testsets/my_tests

# Run the model on our images
python main_test_swinir.py --tile 400 --task real_sr --scale 4 --large_model --model_path model_zoo/swinir/003_realSR_BSRGAN_DFOWMFC_s64w8_SwinIR-L_x4_GAN.pth --folder_lq testsets/my_tests
```

Ran in 9 min on my machine with one GPU and 32GB of RAM

## Results {.center}

::::{.columns}

:::{.column}

![](img/lr/berlin_1945_1.jpg)

:::

:::{.column}

![](img/hr/berlin_1945_1.jpg)

:::

::::

## Results {.center}

::::{.columns}

:::{.column}

![](img/lr_zoom/berlin_1945_1.jpg)

:::

:::{.column}

![](img/hr_zoom/berlin_1945_1.png)

:::

::::

## Results {.center}

::::{.columns}

:::{.column}

![](img/lr/berlin_1945_2.jpg)

:::

:::{.column}

![](img/hr/berlin_1945_2.jpg)

:::

::::

## Results {.center}

::::{.columns}

:::{.column}

![](img/lr_zoom/berlin_1945_2.jpg)

:::

:::{.column}

![](img/hr_zoom/berlin_1945_2.png)

:::

::::

## Results {.center}

::::{.columns}

:::{.column}

![](img/lr/bruegel.jpg)

:::

:::{.column}

![](img/hr/bruegel.jpg)

:::

::::

## Results {.center}

::::{.columns}

:::{.column}

![](img/lr_zoom/bruegel.jpg)

:::

:::{.column}

![](img/hr_zoom/bruegel.png)

:::

::::

## Results {.center}

::::{.columns}

:::{.column}

![](img/lr/vasarely.jpg)

:::

:::{.column}

![](img/hr/vasarely.jpg)

:::

::::

## Results {.center}

::::{.columns}

:::{.column}

![](img/lr_zoom/vasarely.jpg)

:::

:::{.column}

![](img/hr_zoom/vasarely.png)

:::

::::

## Results {.center}

::::{.columns}

:::{.column}

![](img/lr/bird.png)

:::

:::{.column}

![](img/hr/bird.jpg){width="93.5%"}

:::

::::

## Results {.center}

::::{.columns}

:::{.column}

![](img/lr_zoom/bird.png)

:::

:::{.column}

![](img/hr_zoom/bird.png)

:::

::::

## Metrics {.center}

We could use the [PSNR and SSIM implementations from SwinIR](https://github.com/JingyunLiang/SwinIR/blob/main/utils/util_calculate_psnr_ssim.py), but let's try the [Kornia](https://kornia.readthedocs.io/en/latest/index.html) functions we mentioned earlier:

- [kornia.metrics.psnr](https://kornia.readthedocs.io/en/latest/metrics.html?highlight=psnr#kornia.metrics.psnr)
- [kornia.metrics.ssim](https://kornia.readthedocs.io/en/latest/metrics.html?highlight=psnr#kornia.metrics.ssim)

## Metrics {.center}

Let's load the libraries we need:

```{.python}
import kornia
from PIL import Image
import torch
from torchvision import transforms
```

## Metrics {.center}

Then, we load one pair images (LR and HR):

```{.python}
berlin1_lr = Image.open("<some/path>/lr/berlin_1945_1.jpg")
berlin1_hr = Image.open("<some/path>/hr/berlin_1945_1.png")
```
<br>
We can display these images with:

```{.python}
berlin1_lr.show()
berlin1_hr.show()
```

## Metrics {.center}

Now, we need to resize them so that they have identical dimensions and turn them into tensors:

```{.python}
preprocess = transforms.Compose([
        transforms.Resize(256),
        transforms.ToTensor()
        ])

berlin1_lr_t = preprocess(berlin1_lr)
berlin1_hr_t = preprocess(berlin1_hr)
```

## Metrics {.center}

```{.python}
berlin1_lr_t.shape
berlin1_hr_t.shape
```

```
torch.Size([3, 267, 256])
torch.Size([3, 267, 256])
```

We now have tensors with 3 dimensions:

- the channels (RGB)
- the height of the image (in pixels)
- the width of the image (in pixels)

## Metrics {.center}

As data processing is done in batch in ML, we need to add a 4th dimension: the **batch size**

(It will be equal to `1` since we have a batch size of a single image)

```{.python}
batch_berlin1_lr_t = torch.unsqueeze(berlin1_lr_t, 0)
batch_berlin1_hr_t = torch.unsqueeze(berlin1_hr_t, 0)
```

## Metrics {.center}

Our new tensors are now ready:

```{.python}
batch_berlin1_lr_t.shape
batch_berlin1_hr_t.shape
```

```
torch.Size([1, 3, 267, 256])
torch.Size([1, 3, 267, 256])
```

## PSNR {.center}

```{.python}
psnr_value = kornia.metrics.psnr(batch_berlin1_lr_t, batch_berlin1_hr_t, max_val=1.0)
psnr_value.item()
```

```
33.379642486572266
```

## SSIM {.center}

```{.python}
ssim_map = kornia.metrics.ssim(
    batch_berlin1_lr_t, batch_berlin1_hr_t, window_size=5, max_val=1.0, eps=1e-12)

ssim_map.mean().item()
```

```
0.9868119359016418
```
