---
title: Deep learning with JAX
author: Marie-Hélène Burle
---

:::{.def}

JAX is perfect for developing deep learning models:

- it deals with multi-dimensional arrays,
- it is extremely fast,
- it is optimized for accelerators,
- and it is capable of flexible automatic differentiation.

JAX is however not a DL library. While it is possible to create neural networks directly in JAX, it makes more sense to use libraries built on JAX that provide the toolkit necessary to build and train neural networks.

:::

## Deep learning workflow 

Training a neural network from scratch requires a number of steps:

```{dot}
//| echo: false

digraph {
  
bgcolor="transparent"
node [fontname="Inconsolata, sans-serif", color=gray55]
edge [color=gray55]

"Load\ndataset" -> "Define\narchitecture" -> Train -> Test -> "Save\nmodel"

}
```
<br>
Pretrained models can also be used for [feature extraction](https://en.wikipedia.org/wiki/Feature_engineering) or [transfer learning](https://en.wikipedia.org/wiki/Transfer_learning).

## Deep learning ecosystem for JAX

Here is a classic ecosystem of libraries for deep learning with JAX:

- **Load datasets**

:::{.ind}

There are already good tools to load datasets (e.g. [PyTorch](https://pytorch.org/), [TensorFlow](https://www.tensorflow.org/), [Hugging Face](https://huggingface.co/)), so JAX did not worry about creating its own implementation.

:::

- **Define network architecture**

:::{.ind}

Neural networks can be build in JAX from scratch, but a number of packages built on JAX provide the necessary toolkit. [Flax](https://flax.readthedocs.io/en/latest/index.html) is a popular option and the one we will use in this course.

:::

- **Train**

:::{.ind}

Training a model requires optimization functions. These can be implemented in JAX from scratch but the library [Optax](https://optax.readthedocs.io/en/latest/) provides the core components.

:::

- **Test**

:::{.ind}

Testing a model is easy to do directly in JAX.

:::

- **Save model**

:::{.ind}

Flax provides methods to save a model.

:::

<br>
To sum up, here is a common ecosystem of libraries to use JAX for neural networks:<br><br>

```{dot}
//| echo: false

digraph {
  
bgcolor="transparent"
node [fontname="Inconsolata, sans-serif", color=gray55]
edge [color=gray55]

"Load\ndataset" [shape=plaintext, group=g1]
"Define\narchitecture" [shape=plaintext, group=g1]
Train [shape=plaintext, group=g1]
Test [shape=plaintext, group=g1]
"Save\nmodel" [shape=plaintext, group=g1]

PyTorch [fontcolor=burlywood3]
TensorFlow [group=g2, fontcolor=burlywood3]
"Hugging Face" [fontcolor=burlywood3]
flax1 [label=Flax, group=g2, fontcolor=darkorange4]
Flax [fontcolor=darkorange4]

{rank=same; "Load\ndataset" PyTorch TensorFlow "Hugging Face"}
"Load\ndataset" -> PyTorch -> TensorFlow -> "Hugging Face" [style=invis]

{rank=same; "Define\narchitecture" flax1}
"Define\narchitecture" -> flax1 [style=invis]

jax1 [label=<<FONT COLOR="deepskyblue3">JAX</FONT><FONT COLOR="gray55">+ </FONT><FONT COLOR="chocolate">Optax</FONT>>]

JAX [fontcolor=deepskyblue3]

{PyTorch TensorFlow "Hugging Face"} -> flax1 -> jax1 -> JAX -> Flax

"Load\ndataset" -> "Define\narchitecture" -> Train -> Test -> "Save\nmodel" [style=invis]

}
```

<br>
When working from pretrained models, Hugging Face also provides a great API to download from thousands of pretrained models.
