{
  "hash": "884af8bfcaec7ad81261c9e558bea540",
  "result": {
    "markdown": "---\ntitle: Creating an audio DataLoader\nauthor: Marie-Hélène Burle\n---\n\n## Load packages\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nimport torch\nimport torchaudio\nimport matplotlib.pyplot as plt\n```\n:::\n\n\n## Download and unzip data\n\nPyTorch comes with many classic datasets.\n\n:::{.example}\n\nExamples:\n\n- [list of available datasets for vision](https://pytorch.org/vision/stable/datasets.html)\n- [list of audio datasets](https://pytorch.org/audio/stable/datasets.html?highlight=dataset#module-torchaudio.datasets)\n- [list of texts datasets](https://pytorch.org/text/stable/datasets.html?highlight=dataset)\n\n:::\n\nThis is convenient to develop and test your model, or to compare its performance with existing models using these datasets.\n\nHere, we will use [the YESNO dataset](https://www.openslr.org/1/) which can be accessed through [the torchaudio.datasets.YESNO class](https://pytorch.org/audio/stable/datasets.html#torchaudio.datasets.YESNO):\n\n```{.python}\nhelp(torchaudio.datasets.YESNO)\n```\n```\nHelp on class YESNO in module torchaudio.datasets.yesno:\n\nclass YESNO(torch.utils.data.dataset.Dataset)\n\n |  YESNO(root: Union[str, pathlib.Path], url: str =\n |    'http://www.openslr.org/resources/1/waves_yesno.tar.gz', \n |    folder_in_archive: str = 'waves_yesno', \n |    download: bool = False) -> None\n |  \n |  Args:\n |    root (str or Path): Path to the directory where the dataset is found \n |      or downloaded.\n |    url (str, optional): The URL to download the dataset from.\n |      (default: \"http://www.openslr.org/resources/1/waves_yesno.tar.gz\")\n |    folder_in_archive (str, optional):\n |      The top-level directory of the dataset. (default: \"waves_yesno\")\n |    download (bool, optional):\n |      Whether to download the dataset if it is not found at root path. \n |      (default: False).\n```\n\nThe `root` argument sets the location of the downloaded data.\n\n### Where to store this data in the cluster\n\nWe will all use the same data. It would make little sense to all download it in our home directory.\n\n:::{.note}\n\nIn the Alliance clusters, a good place to store data shared amongst members of a project is in the `/project` file system.\n\nYou usually belong to `/project/def-<group>`, where `<group>` is the name of your PI. You can access it from your home directory through the symbolic link `~/projects/def-<group>`.\n\n:::\n\nIn our training cluster, we are all part of the group `def-sponsor00`, accessible through `/project/def-sponsor00` (or the hyperlink `~/projects/def-sponsor00`).\n\nWe will thus use `~/projects/def-sponsor00/data` as the `root` argument for `torchaudio.datasets.yesno`):\n\n```{.python}\nyesno_data = torchaudio.datasets.YESNO(\n    '~/projects/def-sponsor00/data/',\n    download=True)\n```\n\n\n\n## Explore the data\n\nA data point in YESNO is a tuple of `waveform`, `sample_rate`, and `labels` (the labels are `1` for \"yes\" and `0` for \"no\").\n\nLet's have a look at the first data point:\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nyesno_data[0]\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\n(tensor([[ 3.0518e-05,  6.1035e-05,  3.0518e-05,  ..., -1.8616e-03,\n          -2.2583e-03, -1.3733e-03]]),\n 8000,\n [0, 0, 0, 0, 1, 1, 1, 1])\n```\n:::\n:::\n\n\nOr, more nicely:\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nwaveform, sample_rate, labels = yesno_data[0]\nprint(\"Waveform: {}\\nSample rate: {}\\nLabels: {}\".format(waveform, sample_rate, labels))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nWaveform: tensor([[ 3.0518e-05,  6.1035e-05,  3.0518e-05,  ..., -1.8616e-03,\n         -2.2583e-03, -1.3733e-03]])\nSample rate: 8000\nLabels: [0, 0, 0, 0, 1, 1, 1, 1]\n```\n:::\n:::\n\n\nYou can also plot the data. For this, we will use `pyplot` from `matplotlib`.\n\nLet's look at the waveform:\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nplt.figure()\nplt.plot(waveform.t().numpy())\n```\n\n::: {.cell-output .cell-output-display}\n![](audio_dataloader_files/figure-html/cell-6-output-1.png){width=582 height=411}\n:::\n:::\n\n\n## Split the data into a training set and a testing set\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\ntrain_size = int(0.8 * len(yesno_data))\ntest_size = len(yesno_data) - train_size\ntrain_dataset, test_dataset = torch.utils.data.random_split(yesno_data, [train_size, test_size])\n```\n:::\n\n\n## Create training and testing DataLoaders\n\nDataLoaders are Python iterables created by the [torch.utils.data.DataLoader class](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) from a dataset and a sampler.\n\nWe already have a dataset (`yesno_data`). Now we need a sampler (or sampling strategy) to draw samples from it. The sampling strategy contains the batch size, whether the data get shuffled prior to sampling, the number of workers used if the data is loaded in parallel, etc.\n\nTo create a training DataLoader with shuffled data and batch size of 1 (the default), we run:\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\ntrain_loader = torch.utils.data.DataLoader(train_dataset, shuffle=True)\n```\n:::\n\n\n`data_loader` is an iterable of 0.8*60=48 elements (80% of the 60 samples in the YESNO dataset):\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nlen(train_loader)\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```\n48\n```\n:::\n:::\n\n\nWe do the same to create the testing DataLoader:\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\ntest_loader = torch.utils.data.DataLoader(test_dataset, shuffle=True)\n```\n:::\n\n\n## Why do we need to create a DataLoader?\n\nA DataLoader is the iterable that \"presents\" data to a model. When we train a model, we run it for each element of the DataLoader in a for loop:\n\n```{.python}\nfor i in data_loader:\n    <run some model>\n```\n\n",
    "supporting": [
      "audio_dataloader_files"
    ],
    "filters": [],
    "includes": {}
  }
}