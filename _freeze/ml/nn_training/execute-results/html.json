{
  "hash": "7449a4071269befc339390cded70da56",
  "result": {
    "markdown": "---\ntitle: Training a model\nauthor: Marie-Hélène Burle\n---\n\n## Prerequisites\n\nBefore we can train a model, we need to:\n\n- load the needed packages,\n- get the data,\n- create data loaders for training and testing,\n- define a model.\n\nLet's do this for the [FashionMNIST dataset](https://github.com/zalandoresearch/fashion-mnist):\n\n```{.python}\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets\nfrom torchvision.transforms import ToTensor, Lambda\n\ntraining_data = datasets.FashionMNIST(\n    root=\"~/projects/def-sponsor00/data/\",\n    train=True,\n    download=True,\n    transform=ToTensor(),\n    target_transform=Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n)\n\ntest_data = datasets.FashionMNIST(\n    root=\"~/projects/def-sponsor00/data/\",\n    train=False,\n    download=True,\n    transform=ToTensor(),\n    target_transform=Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n)\n\ntrain_dataloader = DataLoader(training_data, batch_size=10)\ntest_dataloader = DataLoader(test_data, batch_size=10)\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.flatten = nn.Flatten()\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(28*28, 512),\n            nn.ReLU(),\n            nn.Linear(512, 512),\n            nn.ReLU(),\n            nn.Linear(512, 10),\n        )\n\n    def forward(self, x):\n        x = self.flatten(x)\n        logits = self.linear_relu_stack(x)\n        return logits\n\nmodel = Net()\n```\n\n## Hyperparameters\n\nWhile the learning parameters of a model (weights and biases) are the values that get adjusted through training (and they will become part of the final program, along with the model architecture, once training is over), hyperparameters control the training process.\n\nThey include:\n\n- batch size: number of samples passed through the model before the parameters are updated,\n- number of epochs: number iterations,\n- learning rate: size of the incremental changes to model parameters at each iteration. Smaller values yield slow learning speed, while large values may miss minima.\n\nLet's define them here:\n\n```{.python}\nlearning_rate = 1e-3\nbatch_size = 64\nepochs = 5\n```\n\n## Define the loss function\n\nTo assess the predicted outputs of our model against the true values from the labels, we also need a loss function (e.g. mean square error for regressions: `nn.MSELoss` or negative log likelihood for classification: `nn.NLLLoss`)\n\nThe machine learning literature is rich in information about various loss functions.\n\nHere is an example with `nn.CrossEntropyLoss` which combines `nn.LogSoftmax` and `nn.NLLLoss`:\n\n```{.python}\nloss_fn = nn.CrossEntropyLoss()\n```\n\n## Initialize the optimizer\n\nThe optimization algorithm determines how the model parameters get adjusted at each iteration.\n\nThere are many optimizers and you need to search in the literature which one performs best for your time of model and data.\n\nBelow is an example with stochastic gradient descent:\n\n```{.python}\noptimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n```\n\n:::{.note}\n\n- lr is the learning rate\n- momentum is a method increasing convergence rate and reducing oscillation for SDG \n\n:::\n\n## Define the train and test loops\n\nFinally, we need to define the train and test loops.\n\nThe train loop:\n\n- gets a batch of training data from the DataLoader,\n- resets the gradients of model parameters with `optimizer.zero_grad()`,\n- calculates predictions from the model for an input batch,\n- calculates the loss for that set of predictions vs. the labels on the dataset,\n- calculates the backward gradients over the learning parameters (that's the backpropagation) with `loss.backward()`,\n- adjusts the parameters by the gradients collected in the backward pass  with `optimizer.step()`.\n\nThe test loop evaluates the model’s performance against the test data.\n\n```{.python}\ndef train(dataloader, model, loss_fn, optimizer):\n    size = len(dataloader.dataset)\n    for batch, (X, y) in enumerate(dataloader):\n        # Compute prediction and loss\n        pred = model(X)\n        loss = loss_fn(pred, y)\n\n        # Backpropagation\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if batch % 100 == 0:\n            loss, current = loss.item(), batch * len(X)\n            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n\n\ndef test(dataloader, model, loss_fn):\n    size = len(dataloader.dataset)\n    num_batches = len(dataloader)\n    test_loss, correct = 0, 0\n\n    with torch.no_grad():\n        for X, y in dataloader:\n            pred = model(X)\n            test_loss += loss_fn(pred, y).item()\n            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n\n    test_loss /= num_batches\n    correct /= size\n    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n```\n\n## Train\n\nTo train our model, we just run the loop over the epochs:\n\n```{.python}\nfor t in range(epochs):\n    print(f\"Epoch {t+1}\\n-------------------------------\")\n    train(train_dataloader, model, loss_fn, optimizer)\n    test(test_dataloader, model, loss_fn)\nprint(\"Training completed\")\n```\n\n",
    "supporting": [
      "nn_training_files"
    ],
    "filters": [],
    "includes": {}
  }
}