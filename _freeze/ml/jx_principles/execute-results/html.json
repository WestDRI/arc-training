{
  "hash": "e250fe1ecf1ea1ec635e6afda37f193d",
  "result": {
    "markdown": "---\ntitle: JAX principles\nauthor: Marie-Hélène Burle\n---\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport jax.numpy as jnp\nfrom jax import grad, vmap, pmap, jit\n```\n\n::: {.cell-output .cell-output-error}\n```\nModuleNotFoundError: No module named 'jax.numpy'; 'jax' is not a package\n```\n:::\n:::\n\n\nComposable transformations:\n\n- `grad` transform: turn a loss function (e.g. `mse_loss`) and turn it into a function that computes the gradients\n- `jit` transform: just-in-time compilation\n- `vmap` transform: work in a **v**ectorized fashion across elements of a batch\n- `pmap` transform: work in **p**arallel across processing units\n\nPython → tracer values → Intermediate Representation (IR) → autodiff\n\n\n\n```{dot}\n//| echo: false\n//| fig-height: 800px\n\nstrict digraph {\n\nbgcolor=\"transparent\"\nnode [fontname=\"Inconsolata, sans-serif\"]\n\n\"tracer\\nvalues\" [shape=rectangle]\n\"jit\\ncompilation\" [shape=rectangle]\n\"Accelerated\\nLinear Algebra\\n(XLA)\" [shape=rectangle]\n\"transforms\" [shape=rectangle]\n\nCPU [shape=octagon]\nGPU [shape=octagon]\nTPU [shape=octagon]\n\n\"Python code\\nwith only pure\\nfunctions\"-> \"tracer\\nvalues\" [dir=none]\n\"tracer\\nvalues\" -> \"Intermediate\\nRepresentation\\n(IR)\"\n\"Intermediate\\nRepresentation\\n(IR)\" -> \"jit\\ncompilation\" [dir=none]\n\"jit\\ncompilation\" -> \"High Level\\nOptimized code\\n(HLO)\"\n\"High Level\\nOptimized code\\n(HLO)\" -> \"Accelerated\\nLinear Algebra\\n(XLA)\" [dir=none]\n\n\"Accelerated\\nLinear Algebra\\n(XLA)\" -> CPU [shape=doubleoctagon]\n\"Accelerated\\nLinear Algebra\\n(XLA)\" -> GPU\n\"Accelerated\\nLinear Algebra\\n(XLA)\" -> TPU\n\n\"Intermediate\\nRepresentation\\n(IR)\" -> \"transforms\" [dir=both, minlen=3]\n\n{rank=same; \"Intermediate\\nRepresentation\\n(IR)\" \"transforms\"}\n\n}\n```\n\n",
    "supporting": [
      "jx_principles_files"
    ],
    "filters": [],
    "includes": {}
  }
}