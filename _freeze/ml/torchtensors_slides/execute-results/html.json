{
  "hash": "a3eeb9098191842621adeb9478c95a4a",
  "result": {
    "markdown": "---\ntitle: Everything you wanted to know (and more) about PyTorch tensors\nfrontlogo: /img/sfudrac.png\nauthor: Marie-Hélène Burle\ndate: '2022-01-27'\ndate-format: long\nexecute:\n  error: true\n  echo: true\nformat:\n  revealjs:\n    embed-resources: true\n    theme:\n      - default\n      - ../revealjs.scss\n    logo: /img/sfudrac_logo.png\n    highlight-style: monokai\n    code-line-numbers: false\n    code-overflow: wrap\n    template-partials:\n      - ../title-slide.html\n    pointer:\n      color: '#b5111b'\n      pointerSize: 32\n    link-external-newwindow: true\n    footer: '<a href=\"torchtensors.html\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"16\" height=\"16\" fill=\"rgb(153, 153, 153)\" class=\"bi bi-arrow-90deg-up\" viewBox=\"0 0 16 16\"><path fill-rule=\"evenodd\" d=\"M4.854 1.146a.5.5 0 0 0-.708 0l-4 4a.5.5 0 1 0 .708.708L4 2.707V12.5A2.5 2.5 0 0 0 6.5 15h8a.5.5 0 0 0 0-1h-8A1.5 1.5 0 0 1 5 12.5V2.707l3.146 3.147a.5.5 0 1 0 .708-.708l-4-4z\"/></svg>&nbsp;Back to workshop page</a>'\nrevealjs-plugins:\n  - pointer\n---\n\n## Acknowledgements\n\nMany drawings in this webinar come from the book:\n\n![](img/book_cover.jpg){width=\"30%\"}\n\nThe section on storage is also highly inspired by it\n\n## Using tensors locally\n<br>\n\nYou need to have [Python](https://www.python.org/downloads/) & [PyTorch](https://pytorch.org/get-started/locally/) installed\n\nAdditionally, you might want to use an IDE such as [elpy](https://github.com/jorgenschaefer/elpy) if you are an Emacs user, [JupyterLab](https://jupyter.org/), etc.\n\n:::{.note}\n\nNote that PyTorch does not yet support Python 3.10 except in some Linux distributions or on systems where a wheel has been built\nFor the time being, you might have to use it with Python 3.9\n\n:::\n\n## Using tensors on CC clusters\n\nIn the cluster terminal:\n\n```sh\navail_wheels \"torch*\" # List available wheels & compatible Python versions\nmodule avail python\t  # List available Python versions\nmodule load python/3.9.6             # Load a sensible Python version\nvirtualenv --no-download env         # Create a virtual env\nsource env/bin/activate\t\t         # Activate the virtual env\npip install --no-index --upgrade pip # Update pip\npip install --no-index torch\t\t # Install PyTorch\n```\nYou can then launch jobs with `sbatch` or `salloc`\n\nLeave the virtual env with the command: `deactivate`\n\n## Outline\n\n- ### What is a PyTorch tensor?\n- ### Memory storage\n- ### Data type (dtype)\n- ### Basic operations\n- ### Working with NumPy\n- ### Linear algebra\n- ### Harvesting the power of GPUs\n- ### Distributed operations\n\n## Outline\n\n- ### [What is a PyTorch tensor?]{.emph}\n- ### Memory storage\n- ### Data type (dtype)\n- ### Basic operations\n- ### Working with NumPy\n- ### Linear algebra\n- ### Harvesting the power of GPUs\n- ### Distributed operations\n\n## ANN do not process information directly\n\n![](img/ml1.png)\n[Modified from Stevens, E., Antiga, L., & Viehmann, T. (2020). Deep learning with PyTorch. Manning Publications]{.caption}\n\n## It needs to be converted to numbers\n\n![](img/ml2.png)\n[Modified from Stevens, E., Antiga, L., & Viehmann, T. (2020). Deep learning with PyTorch. Manning Publications]{.caption}\n\n## These numbers must be stored in a data structure\n\n. . .\n\n[PyTorch tensors are Python objects holding multidimensional arrays]{.emph}\n\n![](img/tensor.png)\n[Stevens, E., Antiga, L., & Viehmann, T. (2020). Deep learning with PyTorch. Manning Publications]{.caption}\n\n## Why a new object when NumPy already exists?\n\n. . .\n\n- Can run on accelerators (GPUs, TPUs...)\n\n\n- Keep track of computation graphs, allowing automatic differentiation\n\n\n- Future plan for sharded tensors to run distributed computations\n\n## What is a PyTorch tensor?\n\nPyTorch is foremost a [deep learning library]{.emph}\n\nIn deep learning, the information contained in objects of interest (e.g. images, texts, sounds) is converted to [floating-point numbers]{.emph} (e.g. pixel values, token values, frequencies) \n\nAs this information is complex, [multiple dimensions are required]{.emph} (e.g. two dimensions for the width & height of an image, plus one dimension for the RGB colour channels) \n\nAdditionally, items are grouped into batches to be processed together, adding yet another dimension\n\n[Multidimensional arrays are thus particularly well suited for deep learning]{.emph}\n\n## What is a PyTorch tensor?\n\nArtificial neurons perform basic computations on these tensors\n\nTheir number however is huge & computing efficiency is paramount\n\nGPUs/TPUs are particularly well suited to perform many simple operations in parallel\n\nThe very popular [NumPy library](https://numpy.org/) has, at its core, a mature multidimensional array object well integrated into the scientific Python ecosystem\n\nBut the PyTorch tensor has additional efficiency characteristics ideal for machine learning & it can be converted to/from NumPy's ndarray if needed\n\n## Outline\n\n- ### What is a PyTorch tensor?\n- ### [Memory storage]{.emph}\n- ### Data type (dtype)\n- ### Basic operations\n- ### Working with NumPy\n- ### Linear algebra\n- ### Harvesting the power of GPUs\n- ### Distributed operations\n\n## Efficient memory storage\n\nIn Python, collections (lists, tuples) are groupings of boxed Python objects\n\nPyTorch tensors & NumPy ndarrays are made of unboxed C numeric types\n\n![](img/memory_storage.png){width=\"70%\"}\n\n:::{.caption}\n\nStevens, E., Antiga, L., & Viehmann, T. (2020). Deep learning with PyTorch. Manning Publications\n\n:::\n\n## Efficient memory storage\n\nThey are usually contiguous memory blocks, but the main difference is that they are unboxed: floats will thus take 4 (32-bit) or 8 (64-bit) bytes each\n\nBoxed values take up more memory (memory for the pointer + memory for the primitive)\n\n![](img/memory_storage.png){width=\"70%\"}\n\n:::{.caption}\n\nStevens, E., Antiga, L., & Viehmann, T. (2020). Deep learning with PyTorch. Manning Publications\n\n:::\n\n## Implementation\n\nUnder the hood, the values of a PyTorch tensor are stored as a `torch.Storage` instance which is a [one-dimensional array]{.emph}\n\n. . .\n\n```{.python}\nimport torch\nt = torch.arange(10.).view(2, 5); print(t) # Functions explained later\n```\n\n<div style=\"font-size: 20px; margin: 25px 0 12px 1px; text-align: left; color: #802b00\">[Out]</div>\n\n```{.python}\ntensor([[ 0.,  1.,  2., 3.,  4.],\n        [ 5.,  6.,  7.,  8.,  9.]])\n```\n\n## Implementation\n<br>\n\n```{.python}\nstorage = t.storage(); print(storage)\n```\n\n<div style=\"font-size: 20px; margin: 25px 0 12px 1px; text-align: left; color: #802b00\">[Out]</div>\n\n```{.python}\n 0.0\n 1.0\n 2.0\n 3.0\n 4.0\n 5.0\n 6.0\n 7.0\n 8.0\n 9.0\n[torch.FloatStorage of size 10]\n```\n\n## Implementation\n\nThe storage can be indexed\n\n```{.python}\nstorage[3]\n```\n\n<div style=\"font-size: 20px; margin: 25px 0 12px 1px; text-align: left; color: #802b00\">[Out]</div>\n\n```{.python}\n3.0\n```\n\n## Implementation\n<br>\n\n```{.python}\nstorage[3] = 10.0; print(storage)\n```\n\n<div style=\"font-size: 20px; margin: 25px 0 12px 1px; text-align: left; color: #802b00\">[Out]</div>\n\n```{.python}\n 0.0\n 1.0\n 2.0\n 10.0\n 4.0\n 5.0\n 6.0\n 7.0\n 8.0\n 9.0\n[torch.FloatStorage of size 10]\n```\n\n## Implementation\n\nTo view a multidimensional array from storage, we need [metadata]{.emph}:\n\n\n- the [size]{.emph} (*shape* in NumPy) sets the number of elements in each dimension\n\n- the [offset]{.emph} indicates where the first element of the tensor is in the storage\n\n- the [stride]{.emph} establishes the increment between each element\n\n## Storage metadata\n\n![](img/tensor_metadata.png){width=\"70%\"}\n\n:::{.caption}\n\nStevens, E., Antiga, L., & Viehmann, T. (2020). Deep learning with PyTorch. Manning Publications\n\n:::\n\n## Storage metadata\n<br>\n\n```{.python}\nt.size()\nt.storage_offset()\nt.stride()\n```\n\n<div style=\"font-size: 20px; margin: 25px 0 12px 1px; text-align: left; color: #802b00\">[Out]</div>\n\n```{.python}\ntorch.Size([2, 5])\n0\n(5, 1)\n```\n\n## Storage metadata\n\n![](img/my_tensor_metadata.jpg)\n\n## Sharing storage\n\nMultiple tensors can use the same storage, saving a lot of memory since the metadata is a lot lighter than a whole new array\n\n![](img/sharing_storage.png){width=\"70%\"}\n\n:::{.caption}\n\nStevens, E., Antiga, L., & Viehmann, T. (2020). Deep learning with PyTorch. Manning Publications\n\n:::\n\n## Transposing in 2 dimensions\n<br>\n\n```{.python}\nt = torch.tensor([[3, 1, 2], [4, 1, 7]]); print(t)\nt.size()\nt.t()\nt.t().size()\n```\n\n<div style=\"font-size: 20px; margin: 25px 0 12px 1px; text-align: left; color: #802b00\">[Out]</div>\n\n```{.python}\ntensor([[3, 1, 2],\n        [4, 1, 7]])\ntorch.Size([2, 3])\ntensor([[3, 4],\n        [1, 1],\n        [2, 7]])\ntorch.Size([3, 2])\n```\n\n## Transposing in 2 dimensions\n\n= flipping the stride elements around\n\n![](img/transpose.png){width=\"70%\"}\n\n:::{.caption}\n\nStevens, E., Antiga, L., & Viehmann, T. (2020). Deep learning with PyTorch. Manning Publications\n\n:::\n\n## Transposing in higher dimensions\n<br>\n\n`torch.t()` is a shorthand for `torch.transpose(0, 1)`:\n\n```{.python}\ntorch.equal(t.t(), t.transpose(0, 1))\n```\n\n<div style=\"font-size: 20px; margin: 25px 0 12px 1px; text-align: left; color: #802b00\">[Out]</div>\n\n```{.python}\nTrue\n```\n\nWhile `torch.t()` only works for 2D tensors, `torch.transpose()` can be used to transpose 2 dimensions in tensors of any number of dimensions\n\n## Transposing in higher dimensions\n<br>\n\n```{.python}\nt = torch.zeros(1, 2, 3); print(t)\n\nt.size()\nt.stride()\n```\n\n<div style=\"font-size: 20px; margin: 25px 0 12px 1px; text-align: left; color: #802b00\">[Out]</div>\n\n```{.python}\ntensor([[[0., 0., 0.],\n         [0., 0., 0.]]])\n\ntorch.Size([1, 2, 3])\n(6, 3, 1)\n```\n\n## Transposing in higher dimensions\n<br>\n\n```{.python}\nt.transpose(0, 1)\n\nt.transpose(0, 1).size()\nt.transpose(0, 1).stride()\n```\n\n<div style=\"font-size: 20px; margin: 25px 0 12px 1px; text-align: left; color: #802b00\">[Out]</div>\n\n```{.python}\ntensor([[[0., 0., 0.]],\n        [[0., 0., 0.]]])\n\ntorch.Size([2, 1, 3])\n(3, 6, 1)  # Notice how transposing flipped 2 elements of the stride\n```\n\n## Transposing in higher dimensions\n<br>\n\n```{.python}\nt.transpose(0, 2)\n\nt.transpose(0, 2).size()\nt.transpose(0, 2).stride()\n```\n\n<div style=\"font-size: 20px; margin: 25px 0 12px 1px; text-align: left; color: #802b00\">[Out]</div>\n\n```{.python}\ntensor([[[0.],\n         [0.]],\n        [[0.],\n         [0.]],\n        [[0.],\n         [0.]]])\n\ntorch.Size([3, 2, 1])\n(1, 3, 6)\n```\n\n## Transposing in higher dimensions\n<br>\n\n```{.python}\nt.transpose(1, 2)\n\nt.transpose(1, 2).size()\nt.transpose(1, 2).stride()\n```\n\n<div style=\"font-size: 20px; margin: 25px 0 12px 1px; text-align: left; color: #802b00\">[Out]</div>\n\n```{.python}\ntensor([[[0., 0.],\n         [0., 0.],\n         [0., 0.]]])\n\ntorch.Size([1, 3, 2])\n(6, 1, 3)\n```\n\n## Outline\n\n- ### What is a PyTorch tensor?\n- ### Memory storage\n- ### [Data type (dtype)]{.emph}\n- ### Basic operations\n- ### Working with NumPy\n- ### Linear algebra\n- ### Harvesting the power of GPUs\n- ### Distributed operations\n\n## Default dtype\n<br>\n\nSince PyTorch tensors were built with utmost efficiency in mind for neural networks, the default data type is [32-bit floating points]{.emph}\n\nThis is sufficient for accuracy & much faster than 64-bit floating points\n\n:::{.note}\n\nNote that, by contrast, NumPy ndarrays use 64-bit as their default\n\n:::\n\n## List of PyTorch tensor dtypes\n\n<table>\n<tr><td>torch.float16 / torch.half</td><td>&emsp;&emsp;</td><td>16-bit / half-precision floating-point</td></tr>\n<tr><td>torch.float32 / torch.float</td><td></td><td>32-bit / single-precision floating-point</td></tr>\n<tr style=\"border-bottom: 1px solid white;\"><td>torch.float64 / torch.double</td><td></td><td>64-bit / double-precision floating-point</td></tr>\n<tr><td></td><td></td></tr>\n<tr><td>torch.uint8</td><td></td><td>unsigned 8-bit integers</td></tr>\n<tr><td>torch.int8</td><td></td><td>signed 8-bit integers</td></tr>\n<tr><td>torch.int16 / torch.short</td><td></td><td>signed 16-bit integers</td></tr>\n<tr><td>torch.int32 / torch.int</td><td></td><td>signed 32-bit integers</td></tr>\n<tr style=\"border-bottom: 1px solid white;\"><td>torch.int64 / torch.long</td><td></td><td>signed 64-bit integers</td></tr>\n<tr><td></td><td></td><td></td></tr>\n<tr><td>torch.bool</td><td></td><td>boolean</td></tr>\n</table>\n\n## Checking & changing dtype\n<br>\n\n```{.python}\nt = torch.rand(2, 3); print(t)\nt.dtype   # Remember that the default dtype for PyTorch tensors is float32\nt2 = t.type(torch.float64); print(t2) # If dtype ≠ default, it is printed\nt2.dtype\n```\n\n<div style=\"font-size: 20px; margin: 25px 0 12px 1px; text-align: left; color: #802b00\">[Out]</div>\n\n```{.python}\ntensor([[0.8130, 0.3757, 0.7682],\n        [0.3482, 0.0516, 0.3772]])\ntorch.float32\ntensor([[0.8130, 0.3757, 0.7682],\n        [0.3482, 0.0516, 0.3772]], dtype=torch.float64)\ntorch.float64\n```\n\n## Outline\n\n- ### What is a PyTorch tensor?\n- ### Memory storage\n- ### Data type (dtype)\n- ### [Basic operations]{.emph}\n- ### Working with NumPy\n- ### Linear algebra\n- ### Harvesting the power of GPUs\n- ### Distributed operations\n\n## Creating tensors\n\n- `torch.tensor`: &emsp;&emsp;Input individual values\n- `torch.arange`: &emsp;&emsp;Similar to `range` but creates a 1D tensor\n- `torch.linspace`: &emsp;1D linear scale tensor\n- `torch.logspace`: &emsp;1D log scale tensor\n- `torch.rand`: &emsp;&emsp;&emsp;&ensp;Random numbers from a uniform distribution on `[0, 1)`\n- `torch.randn`: &emsp;&emsp;&emsp;Numbers from the standard normal distribution\n- `torch.randperm`: &emsp;&nbsp;Random permutation of integers\n- `torch.empty`: &emsp;&emsp;&emsp;Uninitialized tensor\n- `torch.zeros`: &emsp;&emsp;&emsp;Tensor filled with `0`\n- `torch.ones`: &emsp;&emsp;&emsp;&ensp;&nbsp;Tensor filled with `1`\n- `torch.eye`: &emsp;&emsp;&emsp;&emsp;&ensp;Identity matrix\n\n## Creating tensors\n<br>\n\n```{.python}\ntorch.manual_seed(0)  # If you want to reproduce the result\ntorch.rand(1)\n\ntorch.manual_seed(0)  # Run before each operation to get the same result\ntorch.rand(1).item()  # Extract the value from a tensor\n```\n\n<div style=\"font-size: 20px; margin: 25px 0 12px 1px; text-align: left; color: #802b00\">[Out]</div>\n\n```{.python}\ntensor([0.4963])\n\n0.49625658988952637\n```\n\n## Creating tensors\n<br>\n\n```{.python}\ntorch.rand(1)\ntorch.rand(1, 1)\ntorch.rand(1, 1, 1)\ntorch.rand(1, 1, 1, 1)\n```\n\n<div style=\"font-size: 20px; margin: 25px 0 12px 1px; text-align: left; color: #802b00\">[Out]</div>\n\n```{.python}\ntensor([0.6984])\ntensor([[0.5675]])\ntensor([[[0.8352]]])\ntensor([[[[0.2056]]]])\n```\n\n## Creating tensors\n<br>\n\n```{.python}\ntorch.rand(2)\ntorch.rand(2, 2, 2, 2)\n```\n\n<div style=\"font-size: 20px; margin: 25px 0 12px 1px; text-align: left; color: #802b00\">[Out]</div>\n\n```{.python}\ntensor([0.5932, 0.1123])\ntensor([[[[0.1147, 0.3168],\n          [0.6965, 0.9143]],\n         [[0.9351, 0.9412],\n          [0.5995, 0.0652]]],\n        [[[0.5460, 0.1872],\n          [0.0340, 0.9442]],\n         [[0.8802, 0.0012],\n          [0.5936, 0.4158]]]])\n```\n\n## Creating tensors\n<br>\n\n```{.python}\ntorch.rand(2)\ntorch.rand(3)\ntorch.rand(1, 1)\ntorch.rand(1, 1, 1)\ntorch.rand(2, 6)\n```\n\n<div style=\"font-size: 20px; margin: 25px 0 12px 1px; text-align: left; color: #802b00\">[Out]</div>\n\n```{.python}\ntensor([0.7682, 0.0885])\ntensor([0.1320, 0.3074, 0.6341])\ntensor([[0.4901]])\ntensor([[[0.8964]]])\ntensor([[0.4556, 0.6323, 0.3489, 0.4017, 0.0223, 0.1689],\n        [0.2939, 0.5185, 0.6977, 0.8000, 0.1610, 0.2823]])\n```\n\n## Creating tensors\n<br>\n\n```{.python}\ntorch.rand(2, 4, dtype=torch.float64)  # You can set dtype\ntorch.ones(2, 1, 4, 5)\n```\n\n<div style=\"font-size: 20px; margin: 25px 0 12px 1px; text-align: left; color: #802b00\">[Out]</div>\n\n```{.python}\ntensor([[0.6650, 0.7849, 0.2104, 0.6767],\n        [0.1097, 0.5238, 0.2260, 0.5582]], dtype=torch.float64)\ntensor([[[[1., 1., 1., 1., 1.],\n          [1., 1., 1., 1., 1.],\n          [1., 1., 1., 1., 1.],\n          [1., 1., 1., 1., 1.]]],\n        [[[1., 1., 1., 1., 1.],\n          [1., 1., 1., 1., 1.],\n          [1., 1., 1., 1., 1.],\n          [1., 1., 1., 1., 1.]]]])\n```\n\n## Creating tensors\n<br>\n\n```{.python}\nt = torch.rand(2, 3); print(t)\ntorch.zeros_like(t)             # Matches the size of t\ntorch.ones_like(t)\ntorch.randn_like(t)\n```\n\n<div style=\"font-size: 20px; margin: 25px 0 12px 1px; text-align: left; color: #802b00\">[Out]</div>\n\n```{.python}\ntensor([[0.4051, 0.6394, 0.0871],\n        [0.4509, 0.5255, 0.5057]])\ntensor([[0., 0., 0.],\n        [0., 0., 0.]])\ntensor([[1., 1., 1.],\n        [1., 1., 1.]])\ntensor([[-0.3088, -0.0104,  1.0461],\n        [ 0.9233,  0.0236, -2.1217]])\n```\n\n## Creating tensors\n<br>\n\n```{.python}\ntorch.arange(2, 10, 4)    # From 2 to 10 in increments of 4\ntorch.linspace(2, 10, 4)  # 4 elements from 2 to 10 on the linear scale\ntorch.logspace(2, 10, 4)  # Same on the log scale\ntorch.randperm(4)\ntorch.eye(3)\n```\n\n<div style=\"font-size: 20px; margin: 25px 0 12px 1px; text-align: left; color: #802b00\">[Out]</div>\n\n```{.python}\ntensor([2, 6])\ntensor([2.0000,  4.6667,  7.3333, 10.0000])\ntensor([1.0000e+02, 4.6416e+04, 2.1544e+07, 1.0000e+10])\ntensor([1, 3, 2, 0])\ntensor([[1., 0., 0.],\n        [0., 1., 0.],\n        [0., 0., 1.]])\n```\n\n## Tensor information\n<br>\n\n```{.python}\nt = torch.rand(2, 3); print(t)\nt.size()\nt.dim()\nt.numel()\n```\n\n<div style=\"font-size: 20px; margin: 25px 0 12px 1px; text-align: left; color: #802b00\">[Out]</div>\n\n```{.python}\ntensor([[0.5885, 0.7005, 0.1048],\n        [0.1115, 0.7526, 0.0658]])\ntorch.Size([2, 3])\n2\n6\n```\n\n## Tensor indexing\n<br>\n\n```{.python}\nx = torch.rand(3, 4)\nx[:]                 # With a range, the comma is implicit: same as x[:, ]\nx[:, 2]\nx[1, :]\nx[2, 3]\n```\n\n<div style=\"font-size: 20px; margin: 25px 0 12px 1px; text-align: left; color: #802b00\">[Out]</div>\n\n```{.python}\ntensor([[0.6575, 0.4017, 0.7391, 0.6268],\n        [0.2835, 0.0993, 0.7707, 0.1996],\n        [0.4447, 0.5684, 0.2090, 0.7724]])\ntensor([0.7391, 0.7707, 0.2090])\ntensor([0.2835, 0.0993, 0.7707, 0.1996])\ntensor(0.7724)\n```\n\n## Tensor indexing\n<br>\n\n```{.python}\nx[-1:]        # Last element (implicit comma, so all columns)\nx[-1]         # No range, no implicit comma: we are indexing \n# from a list of tensors, so the result is a one dimensional tensor\n# (Each dimension is a list of tensors of the previous dimension)\nx[-1].size()  # Same number of dimensions than x (2 dimensions)\nx[-1:].size() # We dropped one dimension\n```\n\n<div style=\"font-size: 20px; margin: 25px 0 12px 1px; text-align: left; color: #802b00\">[Out]</div>\n\n```{.python}\ntensor([[0.8168, 0.0879, 0.2642, 0.3777]])\ntensor([0.8168, 0.0879, 0.2642, 0.3777])\n\ntorch.Size([4])\ntorch.Size([1, 4])\n```\n\n## Tensor indexing\n<br>\n\n```{.python}\nx[0:1]     # Python ranges are inclusive to the left, not the right\nx[:-1]     # From start to one before last (& implicit comma)\nx[0:3:2]   # From 0th (included) to 3rd (excluded) in increment of 2\n```\n\n<div style=\"font-size: 20px; margin: 25px 0 12px 1px; text-align: left; color: #802b00\">[Out]</div>\n\n```{.python}\ntensor([[0.5873, 0.0225, 0.7234, 0.4538]])\ntensor([[0.5873, 0.0225, 0.7234, 0.4538],\n        [0.9525, 0.0111, 0.6421, 0.4647]])\ntensor([[0.5873, 0.0225, 0.7234, 0.4538],\n        [0.8168, 0.0879, 0.2642, 0.3777]])\n```\n\n## Tensor indexing\n<br>\n\n```{.python}\nx[None]          # Adds a dimension of size one as the 1st dimension\nx.size()\nx[None].size()\n```\n\n<div style=\"font-size: 20px; margin: 25px 0 12px 1px; text-align: left; color: #802b00\">[Out]</div>\n\n```{.python}\ntensor([[[0.5873, 0.0225, 0.7234, 0.4538],\n         [0.9525, 0.0111, 0.6421, 0.4647],\n         [0.8168, 0.0879, 0.2642, 0.3777]]])\ntorch.Size([3, 4])\ntorch.Size([1, 3, 4])\n```\n\n## *A word of caution about indexing*\n<br>\n\nWhile indexing elements of a tensor to extract some of the data as a final step of some computation is fine, [you should not use indexing to run operations on tensor elements in a loop]{.emph} as this would be extremely inefficient\n\nInstead, you want to use [vectorized operations]{.emph}\n\n## Vectorized operations\n<br>\n\nSince PyTorch tensors are homogeneous (i.e. made of a single data type), [as with NumPy's ndarrays](https://www.pythonlikeyoumeanit.com/Module3_IntroducingNumpy/VectorizedOperations.html#Vectorized-Operations), operations are vectorized & thus staggeringly fast\n\nNumPy is mostly written in C & PyTorch in C++. With either library, when you run vectorized operations on arrays/tensors, you don't use raw Python (slow) but compiled C/C++ code (much faster)\n\n[Here](https://pythonspeed.com/articles/vectorization-python/) is an excellent post explaining Python vectorization & why it makes such a big difference\n\n## Vectorized operations: comparison\n\nRaw Python method\n\n```{.python}\n# Create tensor. We use float64 here to avoid truncation errors\nt = torch.rand(10**6, dtype=torch.float64)\n# Initialize the sum\nsum = 0\n# Run loop\nfor i in range(len(t)): sum += t[i]\n# Print result\nprint(sum)\n```\nVectorized function\n\n```{.python}\nt.sum()\n```\n\n## Vectorized operations: comparison\n\nBoth methods give the same result\n\n:::{.note}\n\nThis is why we used float64:<br>\nWhile the accuracy remains excellent with float32 if we use the PyTorch function torch.sum(),\nthe raw Python loop gives a fairly inaccurate result\n\n:::\n\n<div style=\"font-size: 20px; margin: 25px 0 12px 1px; text-align: left; color: #802b00\">[Out]</div>\n\n```{.python}\ntensor(500023.0789, dtype=torch.float64)\n\ntensor(500023.0789, dtype=torch.float64)\n```\n\n## Vectorized operations: timing\n\nLet's compare the timing with PyTorch built-in benchmark utility\n\n```{.python}\n# Load utility\nimport torch.utils.benchmark as benchmark\n\n# Create a function for our loop\ndef sum_loop(t, sum):\n    for i in range(len(t)): sum += t[i]\n```\n\n## Vectorized operations: timing\n\nNow we can create the timers\n\n```{.python}\nt0 = benchmark.Timer(\n    stmt='sum_loop(t, sum)',\n    setup='from __main__ import sum_loop',\n    globals={'t': t, 'sum': sum})\n\nt1 = benchmark.Timer(\n    stmt='t.sum()',\n    globals={'t': t})\n```\n\n## Vectorized operations: timing\n\nLet's time 100 runs to have a reliable benchmark\n\n```{.python}\nprint(t0.timeit(100))\nprint(t1.timeit(100))\n```\n\n:::{.note}\n\nI ran the code on my laptop with a dedicated GPU & 32GB RAM\n\n:::\n\n## Vectorized operations: timing\n\nTiming of raw Python loop\n\n```{.python}\nsum_loop(t, sum)\nsetup: from __main__ import sum_loop\n  1.37 s\n  1 measurement, 100 runs , 1 thread\n```\n\nTiming of vectorized function\n\n```{.python}\nt.sum()\n  191.26 us\n  1 measurement, 100 runs , 1 thread\n```\n\n## Vectorized operations: timing\n\nSpeedup:\n\n```{.python}\n1.37/(191.26 * 10**-6) = 7163\n```\n\n. . .\n\n[The vectorized function runs more than 7,000 times faster!!!]{.emph}\n\n## Even more important on GPUs\n\n*We will talk about GPUs in detail later*\n\nTiming of raw Python loop on GPU [(actually slower on GPU!)]{.emph}\n\n```{.python}\nsum_loop(t, sum)\nsetup: from __main__ import sum_loop\n  4.54 s\n  1 measurement, 100 runs , 1 thread\n```\n\nTiming of vectorized function on GPU (here we do get a speedup)\n\n```{.python}\nt.sum()\n  50.62 us\n  1 measurement, 100 runs , 1 thread\n```\n## Even more important on GPUs\n\nSpeedup:\n\n```{.python}\n4.54/(50.62 * 10**-6) = 89688\n```\n\n. . .\n\n**On GPUs, it is even more important not to index repeatedly from a tensor**\n\n. . .\n\n[On GPUs, the vectorized function runs almost 90,000 times faster!!!]{.emph}\n\n## Simple mathematical operations\n<br>\n\n```{.python}\nt1 = torch.arange(1, 5).view(2, 2); print(t1)\nt2 = torch.tensor([[1, 1], [0, 0]]); print(t2)\nt1 + t2 # Operation performed between elements at corresponding locations\nt1 + 1  # Operation applied to each element of the tensor\n```\n\n<div style=\"font-size: 20px; margin: 25px 0 12px 1px; text-align: left; color: #802b00\">[Out]</div>\n\n```{.python}\ntensor([[1, 2],\n        [3, 4]])\ntensor([[1, 1],\n        [0, 0]])\ntensor([[2, 3],\n        [3, 4]])\ntensor([[2, 3],\n        [4, 5]])\n```\n\n## Reduction\n<br>\n\n```{.python}\nt = torch.ones(2, 3, 4); print(t)\nt.sum()   # Reduction over all entries\n```\n\n<div style=\"font-size: 20px; margin: 25px 0 12px 1px; text-align: left; color: #802b00\">[Out]</div>\n\n```{.python}\ntensor([[[1., 1., 1., 1.],\n         [1., 1., 1., 1.],\n         [1., 1., 1., 1.]],\n        [[1., 1., 1., 1.],\n         [1., 1., 1., 1.],\n         [1., 1., 1., 1.]]])\ntensor(24.)\n```\n\n:::{.note}\n\nOther reduction functions (e.g. mean) behave the same way\n\n:::\n\n## Reduction\n<br>\n\n```{.python}\n# Reduction over a specific dimension\nt.sum(0)  \nt.sum(1)\nt.sum(2)\n```\n\n<div style=\"font-size: 20px; margin: 25px 0 12px 1px; text-align: left; color: #802b00\">[Out]</div>\n\n```{.python}\ntensor([[2., 2., 2., 2.],\n        [2., 2., 2., 2.],\n        [2., 2., 2., 2.]])\ntensor([[3., 3., 3., 3.],\n        [3., 3., 3., 3.]])\ntensor([[4., 4., 4.],\n        [4., 4., 4.]])\n```\n\n## Reduction\n<br>\n\n```{.python}\n# Reduction over multiple dimensions\nt.sum((0, 1))\nt.sum((0, 2))\nt.sum((1, 2))\n```\n\n<div style=\"font-size: 20px; margin: 25px 0 12px 1px; text-align: left; color: #802b00\">[Out]</div>\n\n```{.python}\ntensor([6., 6., 6., 6.])\ntensor([8., 8., 8.])\ntensor([12., 12.])\n```\n\n## In-place operations\n\nWith operators post-fixed with `_`:\n\n```{.python}\nt1 = torch.tensor([1, 2]); print(t1)\nt2 = torch.tensor([1, 1]); print(t2)\nt1.add_(t2); print(t1)\nt1.zero_(); print(t1)\n```\n\n<div style=\"font-size: 20px; margin: 25px 0 12px 1px; text-align: left; color: #802b00\">[Out]</div>\n\n```{.python}\ntensor([1, 2])\ntensor([1, 1])\ntensor([2, 3])\ntensor([0, 0])\n```\n\n## In-place operations vs reassignments\n<br>\n\n```{.python}\nt1 = torch.ones(1); t1, hex(id(t1))\nt1.add_(1); t1, hex(id(t1))        # In-place operation: same address\nt1 = t1.add(1); t1, hex(id(t1))    # Reassignment: new address in memory\nt1 = t1 + 1; t1, hex(id(t1))       # Reassignment: new address in memory\n```\n\n<div style=\"font-size: 20px; margin: 25px 0 12px 1px; text-align: left; color: #802b00\">[Out]</div>\n\n```{.python}\n(tensor([1.]), '0x7fc61accc3b0')\n(tensor([2.]), '0x7fc61accc3b0')\n(tensor([3.]), '0x7fc61accc5e0')\n(tensor([4.]), '0x7fc61accc6d0')\n```\n\n## Tensor views\n<br>\n\n```{.python}\nt = torch.tensor([[1, 2, 3], [4, 5, 6]]); print(t)\nt.size()\nt.view(6)\nt.view(3, 2)\nt.view(3, -1) # Same: with -1, the size is inferred from other dimensions\n```\n\n<div style=\"font-size: 20px; margin: 25px 0 12px 1px; text-align: left; color: #802b00\">[Out]</div>\n\n```{.python}\ntensor([[1, 2, 3],\n        [4, 5, 6]])\ntorch.Size([2, 3])\ntensor([1, 2, 3, 4, 5, 6])\ntensor([[1, 2],\n        [3, 4],\n        [5, 6]])\n```\n\n## Note the difference\n<br>\n\n```{.python}\nt1 = torch.tensor([[1, 2, 3], [4, 5, 6]]); print(t1)\nt2 = t1.t(); print(t2)\nt3 = t1.view(3, 2); print(t3)\n```\n\n<div style=\"font-size: 20px; margin: 25px 0 12px 1px; text-align: left; color: #802b00\">[Out]</div>\n\n```{.python}\ntensor([[1, 2, 3],\n        [4, 5, 6]])\ntensor([[1, 4],\n        [2, 5],\n        [3, 6]])\ntensor([[1, 2],\n        [3, 4],\n        [5, 6]])\n```\n\n## Logical operations\n<br>\n\n```{.python}\nt1 = torch.randperm(5); print(t1)\nt2 = torch.randperm(5); print(t2)\nt1 > 3                            # Test each element\nt1 < t2                           # Test corresponding pairs of elements\n```\n\n<div style=\"font-size: 20px; margin: 25px 0 12px 1px; text-align: left; color: #802b00\">[Out]</div>\n\n```{.python}\ntensor([4, 1, 0, 2, 3])\ntensor([0, 4, 2, 1, 3])\ntensor([ True, False, False, False, False])\ntensor([False,  True,  True, False, False])\n```\n\n## Outline\n\n- ### What is a PyTorch tensor?\n- ### Memory storage\n- ### Data type (dtype)\n- ### Basic operations\n- ### [Working with NumPy]{.emph}\n- ### Linear algebra\n- ### Harvesting the power of GPUs\n- ### Distributed operations\n\n## Conversion without copy\n\nPyTorch tensors can be converted to NumPy ndarrays & vice-versa in a very efficient manner as both objects share the same memory\n\n```{.python}\nt = torch.rand(2, 3); print(t)\nt_np = t.numpy(); print(t_np)      # From PyTorch tensor to NumPy ndarray\n```\n<div style=\"font-size: 20px; margin: 25px 0 12px 1px; text-align: left; color: #802b00\">[Out]</div>\n```{.python}\ntensor([[0.8434, 0.0876, 0.7507],\n        [0.1457, 0.3638, 0.0563]])   # PyTorch Tensor\n\n[[0.84344184 0.08764815 0.7506627 ]\n [0.14567494 0.36384273 0.05629885]] # NumPy ndarray\n```\n\n## Mind the different defaults\n<br>\n\n```{.python}\nt_np.dtype\n```\n\n<div style=\"font-size: 20px; margin: 25px 0 12px 1px; text-align: left; color: #802b00\">[Out]</div>\n\n```{.python}\ndtype('float32')\n```\n\n:::{.note}\n\nRemember that PyTorch tensors use 32-bit floating points by default <br>\n(because this is what you want in neural networks) <br><br>\n\n:::\n\n:::{.note}\n\nBut NumPy defaults to 64-bit <br>\nDepending on your workflow, you might have to change dtype\n\n:::\n\n## From NumPy to PyTorch\n<br>\n\n```{.python}\nimport numpy as np\na = np.random.rand(2, 3); print(a)\na_pt = torch.from_numpy(a); print(a_pt)    # From ndarray to tensor\n```\n\n<div style=\"font-size: 20px; margin: 25px 0 12px 1px; text-align: left; color: #802b00\">[Out]</div>\n\n```{.python}\n[[0.55892276 0.06026952 0.72496545]\n [0.65659463 0.27697739 0.29141587]]\n\ntensor([[0.5589, 0.0603, 0.7250],\n        [0.6566, 0.2770, 0.2914]], dtype=torch.float64)\n```\n\n:::{.note}\n\n Here again, you might have to change dtype\n\n:::\n\n## Notes about conversion without copy\n\n`t` & `t_np` are objects of different Python types, so, as far as Python is concerned, they have different addresses\n\n```{.python}\nid(t) == id(t_np)\n```\n<div style=\"font-size: 20px; margin: 25px 0 12px 1px; text-align: left; color: #802b00\">[Out]</div>\n```{.python}\nFalse\n```\n\n## Notes about conversion without copy\n\nHowever—[that's quite confusing](https://stackoverflow.com/q/61526297/9210961)—they share an underlying C array in memory & modifying one in-place also modifies the other\n\n```{.python}\nt.zero_()\nprint(t_np)\n```\n\n<div style=\"font-size: 20px; margin: 25px 0 12px 1px; text-align: left; color: #802b00\">[Out]</div>\n\n```{.python}\ntensor([[0., 0., 0.],\n        [0., 0., 0.]])\n\n[[0. 0. 0.]\n [0. 0. 0.]]\n```\n\n## Notes about conversion without copy\n\nLastly, as NumPy only works on CPU, to convert a PyTorch tensor allocated to the GPU, the content will have to be copied to the CPU first\n\n## Outline\n\n- ### What is a PyTorch tensor?\n- ### Memory storage\n- ### Data type (dtype)\n- ### Basic operations\n- ### Working with NumPy\n- ### [Linear algebra]{.emph}\n- ### Harvesting the power of GPUs\n- ### Distributed operations\n\n## [torch.linalg](https://pytorch.org/docs/master/linalg.html?highlight=linalg#module-torch.linalg) module\n\n- All functions from [numpy.linalg](https://numpy.org/doc/stable/reference/routines.linalg.html) implemented (with accelerator & automatic differentiation support)\n\n- Some additional functions\n\n:::{.note}\n\nRequires torch >= 1.9 <br>\nLinear algebra support was less developed before the introduction of this module\n\n:::\n\n## System of linear equations solver\n\nLet's have a look at an extremely basic example:\n\n```\n2x + 3y - z = 5\nx - 2y + 8z = 21\n6x + y - 3z = -1\n```\n\nWe are looking for the values of `x`, `y`, & `z` that would satisfy this system\n\n## System of linear equations solver\n\nWe create a 2D tensor `A` of size `(3, 3)` with the coefficients of the equations <br>\n& a 1D tensor `b` of size `3` with the right hand sides values of the equations\n\n```{.python}\nA = torch.tensor([[2., 3., -1.], [1., -2., 8.], [6., 1., -3.]]); print(A)\nb = torch.tensor([5., 21., -1.]); print(b)\n```\n\n<div style=\"font-size: 20px; margin: 25px 0 12px 1px; text-align: left; color: #802b00\">[Out]</div>\n\n```{.python}\ntensor([[ 2.,  3., -1.],\n        [ 1., -2.,  8.],\n        [ 6.,  1., -3.]])\ntensor([ 5., 21., -1.])\n```\n\n## System of linear equations solver\n\nSolving this system is as simple as running the `torch.linalg.solve` function:\n\n```{.python}\nx = torch.linalg.solve(A, b); print(x)\n```\n\n<div style=\"font-size: 20px; margin: 25px 0 12px 1px; text-align: left; color: #802b00\">[Out]</div>\n\n```{.python}\ntensor([1., 2., 3.])\n```\n\nOur solution is:\n\n```\nx = 1\ny = 2\nz = 3\n```\n\n## Verify our result\n<br>\n\n```{.python}\ntorch.allclose(A @ x, b)\n```\n\n<div style=\"font-size: 20px; margin: 25px 0 12px 1px; text-align: left; color: #802b00\">[Out]</div>\n\n```{.python}\nTrue\n```\n\n## System of linear equations solver\n\nHere is another simple example:\n\n```{.python}\n# Create a square normal random matrix\nA = torch.randn(4, 4); print(A)\n# Create a tensor of right hand side values\nb = torch.randn(4); print(b)\n\n# Solve the system\nx = torch.linalg.solve(A, b); print(x)\n\n# Verify\ntorch.allclose(A @ x, b)\n```\n\n## System of linear equations solver\n\n<div style=\"font-size: 20px; margin: 25px 0 12px 1px; text-align: left; color: #802b00\">[Out]</div>\n\n```{.python}\ntensor([[ 1.5091,  2.0820,  1.7067,  2.3804], # A (coefficients)\n        [-1.1256, -0.3170, -1.0925, -0.0852],\n        [ 0.3276, -0.7607, -1.5991,  0.0185],\n        [-0.7504,  0.1854,  0.6211,  0.6382]])\n\ntensor([-1.0886, -0.2666,  0.1894, -0.2190])  # b (right hand side values)\n\ntensor([ 0.1992, -0.7011,  0.2541, -0.1526])  # x (our solution)\n\nTrue\t\t\t\t\t\t\t\t\t      # Verification\n```\n\n## With 2 multidimensional tensors\n<br>\n\n```{.python}\nA = torch.randn(2, 3, 3)              # Must be batches of square matrices\nB = torch.randn(2, 3, 5)              # Dimensions must be compatible\nX = torch.linalg.solve(A, B); print(X)\ntorch.allclose(A @ X, B)\n```\n\n<div style=\"font-size: 20px; margin: 25px 0 12px 1px; text-align: left; color: #802b00\">[Out]</div>\n\n```{.python}\ntensor([[[-0.0545, -0.1012,  0.7863, -0.0806, -0.0191],\n         [-0.9846, -0.0137, -1.7521, -0.4579, -0.8178],\n         [-1.9142, -0.6225, -1.9239, -0.6972,  0.7011]],\n        [[ 3.2094,  0.3432, -1.6604, -0.7885,  0.0088],\n         [ 7.9852,  1.4605, -1.7037, -0.7713,  2.7319],\n         [-4.1979,  0.0849,  1.0864,  0.3098, -1.0347]]])\nTrue\n```\n\n## Matrix inversions\n\n. . .\n\n:::{.note}\n\n It is faster & more numerically stable to solve a system of linear equations directly than to compute the inverse matrix first\n\n:::\n\n. . .\n\n[Limit matrix inversions to situations where it is truly necessary]{.emph}\n\n## Matrix inversions\n<br>\n\n```{.python}\nA = torch.rand(2, 3, 3)      # Batch of square matrices\nA_inv = torch.linalg.inv(A)  # Batch of inverse matrices\nA @ A_inv                    # Batch of identity matrices\n```\n\n<div style=\"font-size: 20px; margin: 25px 0 12px 1px; text-align: left; color: #802b00\">[Out]</div>\n\n```{.python}\ntensor([[[ 1.0000e+00, -6.0486e-07,  1.3859e-06],\n         [ 5.5627e-08,  1.0000e+00,  1.0795e-06],\n         [-1.4133e-07,  7.9992e-08,  1.0000e+00]],\n        [[ 1.0000e+00,  4.3329e-08, -3.6741e-09],\n         [-7.4627e-08,  1.0000e+00,  1.4579e-07],\n         [-6.3580e-08,  8.2354e-08,  1.0000e+00]]])\n```\n\n## Other linear algebra functions\n\n[torch.linalg](https://pytorch.org/docs/master/linalg.html?highlight=linalg#module-torch.linalg) contains many more functions:\n\n- [torch.tensordot](https://pytorch.org/docs/master/generated/torch.tensordot.html#torch.tensordot) which generalizes matrix products\n\n- [torch.linalg.tensorsolve](https://pytorch.org/docs/master/generated/torch.linalg.tensorsolve.html#torch.linalg.tensorsolve) which computes the solution `X` to the system `torch.tensordot(A, X) = B`\n\n- [torch.linalg.eigvals](https://pytorch.org/docs/master/generated/torch.linalg.eigvals.html#torch.linalg.eigvals) which computes the eigenvalues of a square matrix\n\n- and more\n\n## Outline\n\n- ### What is a PyTorch tensor?\n- ### Memory storage\n- ### Data type (dtype)\n- ### Basic operations\n- ### Working with NumPy\n- ### Linear algebra\n- ### [Harvesting the power of GPUs]{.emph}\n- ### Distributed operations\n\n## Device attribute\n\nTensor data can be placed in the memory of various processor types:\n\n- the RAM of CPU\n\n- the RAM of a GPU with CUDA support\n\n- the RAM of a GPU with [AMD's ROCm support](https://pytorch.org/blog/pytorch-for-amd-rocm-platform-now-available-as-python-package/)\n\n- the RAM of an [XLA device](https://www.tensorflow.org/xla) (e.g. [Cloud TPU](https://cloud.google.com/tpu)) with the [torch_xla package](https://github.com/pytorch/xla/)\n\n## Device attribute\n\nThe values for the device attributes are:\n\n- CPU: &nbsp;`'cpu'`\n\n- GPU (CUDA & AMD's ROCm): &nbsp;`'cuda'`\n\n- XLA: &nbsp;`xm.xla_device()`\n\nThis last option requires to load the [torch_xla package](https://github.com/pytorch/xla/) first:\n\n```{.python}\nimport torch_xla\nimport torch_xla.core.xla_model as xm\n```\n\n## Creating a tensor on a specific device\n\nBy default, tensors are created on the CPU\n\n```{.python}\nt1 = torch.rand(2); print(t1)\n```\n\n<div style=\"font-size: 20px; margin: 25px 0 12px 1px; text-align: left; color: #802b00\">[Out]</div>\n\n```{.python}\ntensor([0.1606, 0.9771])  # Implicit: device='cpu'\n```\n\n:::{.note}\n\nPrinted tensors only display attributes with values ≠ default values\n\n:::\n\n## Creating a tensor on a specific device\n\nYou can create a tensor on an accelerator by specifying the device attribute\n\n```{.python}\nt2_gpu = torch.rand(2, device='cuda'); print(t2_gpu)\n```\n\n<div style=\"font-size: 20px; margin: 25px 0 12px 1px; text-align: left; color: #802b00\">[Out]</div>\n\n```{.python}\ntensor([0.0664, 0.7829], device='cuda:0')  # :0 means the 1st GPU\n```\n\n## Copying a tensor to a specific device\n\nYou can also make copies of a tensor on other devices\n\n```{.python}\n# Make a copy of t1 on the GPU\nt1_gpu = t1.to(device='cuda'); print(t1_gpu)\nt1_gpu = t1.cuda()  # Same as above written differently\n\n# Make a copy of t2_gpu on the CPU\nt2 = t2_gpu.to(device='cpu'); print(t2)\nt2 = t2_gpu.cpu()   # For the altenative form\n```\n\n<div style=\"font-size: 20px; margin: 25px 0 12px 1px; text-align: left; color: #802b00\">[Out]</div>\n\n```{.python}\ntensor([0.1606, 0.9771], device='cuda:0')\ntensor([0.0664, 0.7829]) # Implicit: device='cpu'\n```\n\n## Multiple GPUs\n\nIf you have multiple GPUs, you can optionally specify which one a tensor should be created on or copied to\n\n```{.python}\nt3_gpu = torch.rand(2, device='cuda:0')  # Create a tensor on 1st GPU\nt4_gpu = t1.to(device='cuda:0')          # Make a copy of t1 on 1st GPU\nt5_gpu = t1.to(device='cuda:1')          # Make a copy of t1 on 2nd GPU\n```\n\n. . .\n\nOr the equivalent short forms for the last two:\n\n```{.python}\nt4_gpu = t1.cuda(0)\nt5_gpu = t1.cuda(1)\n```\n\n## Timing\n\nLet's compare the timing of some matrix multiplications on CPU & GPU with PyTorch built-in benchmark utility\n\n```{.python}\n# Load utility\nimport torch.utils.benchmark as benchmark\n# Define tensors on the CPU\nA = torch.randn(500, 500)\nB = torch.randn(500, 500)\n# Define tensors on the GPU\nA_gpu = torch.randn(500, 500, device='cuda')\nB_gpu = torch.randn(500, 500, device='cuda')\n```\n\n:::{.note}\n\nI ran the code on my laptop with a dedicated GPU & 32GB RAM\n\n:::\n\n## Timing\n\nLet's time 100 runs to have a reliable benchmark\n\n```{.python}\nt0 = benchmark.Timer(\n    stmt='A @ B',\n    globals={'A': A, 'B': B})\n\nt1 = benchmark.Timer(\n    stmt='A_gpu @ B_gpu',\n    globals={'A_gpu': A_gpu, 'B_gpu': B_gpu})\n\nprint(t0.timeit(100))\nprint(t1.timeit(100))\n```\n\n## Timing\n\n<div style=\"font-size: 20px; margin: 25px 0 12px 1px; text-align: left; color: #802b00\">[Out]</div>\n\n```{.python}\nA @ B\n  2.29 ms\n  1 measurement, 100 runs , 1 thread\n\nA_gpu @ B_gpu\n  108.02 us\n  1 measurement, 100 runs , 1 thread\n```\n\nSpeedup:\n\n```{.python}\n(2.29 * 10**-3)/(108.02 * 10**-6) = 21\n```\nThis computation was 21 times faster on my GPU than on CPU\n\n## Timing\n\nBy replacing `500` with `5000`, we get:\n\n```{.python}\nA @ B\n  2.21 s\n  1 measurement, 100 runs , 1 thread\n\nA_gpu @ B_gpu\n  57.88 ms\n  1 measurement, 100 runs , 1 thread\n```\n\nSpeedup:\n\n```{.python}\n2.21/(57.88 * 10**-3) = 38\n```\nThe larger the computation, the greater the benefit: now 38 times faster\n\n## Outline\n\n- ### What is a PyTorch tensor?\n- ### Memory storage\n- ### Data type (dtype)\n- ### Basic operations\n- ### Working with NumPy\n- ### Linear algebra\n- ### Harvesting the power of GPUs\n- ### [Distributed operations]{.emph}\n\n## Parallel tensor operations\n\nPyTorch already allows for [distributed training of ML models](https://pytorch.org/tutorials/beginner/dist_overview.html)\n\nThe implementation of distributed tensor operations—for instance for linear algebra—is [in the work through the use of a ShardedTensor primitive](https://github.com/pytorch/pytorch/issues/69971) that can be sharded across nodes\n\nSee also [this issue](https://github.com/pytorch/pytorch/issues/55207) for more comments about upcoming developments on (among other things) tensor sharding\n\n# Questions?\n\n",
    "supporting": [
      "torchtensors_slides_files"
    ],
    "filters": [],
    "includes": {}
  }
}