{
  "hash": "276412d64b2d771cb44129e1894ed4a5",
  "result": {
    "markdown": "---\ntitle: PyTorch tensors\nauthor: Marie-Hélène Burle\n---\n\n:::{.def}\n\nBefore information can be processed by algorithms, it needs to be converted to floating point numbers. Indeed, you don't pass a sentence or an image through a model; instead you input numbers representing a sequence of words or pixel values.\n\nAll these floating point numbers need to be stored in a data structure. The most suited structure is multidimensional (to hold several layers of information) and homogeneous—all data of the same type—for efficiency.\n\nPython already has several multidimensional array structures (e.g. [NumPy](https://numpy.org/)'s ndarray) but the particularities of deep learning call for special characteristics such as the ability to run operations on GPUs and/or in a distributed fashion, the ability to keep track of computation graphs for [automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation), and different defaults (lower precision for improved training performance).\n\nThe PyTorch tensor is a Python data structure with these characteristics that can easily be converted to/from NumPy's ndarray and integrates well with other Python libraries such as [Pandas](https://pandas.pydata.org/).\n\nIn this section, we will explore the basics of PyTorch tensors.\n\n:::\n\n## Data types\n\nSince PyTorch tensors were built with efficiency in mind for neural networks, the default data type is **32-bit floating points**.\n\nThis is sufficient for accuracy and much faster than 64-bit floating points.\n\n:::{.note}\n\nBy contrast, NumPy ndarrays use 64-bit as their default.\n\n:::\n\n### List of data types\n\n| dtype | Description |\n| ----- | ----- |\n| torch.float16 / torch.half | 16-bit / half-precision floating-point |\n| torch.float32 / torch.float | 32-bit / single-precision floating-point |\n| torch.float64 / torch.double | 64-bit / double-precision floating-point |\n| torch.uint8 | unsigned 8-bit integers |\n| torch.int8 | signed 8-bit integers |\n| torch.int16 / torch.short | signed 16-bit integers |\n| torch.int32 / torch.int | signed 32-bit integers |\n| torch.int64 / torch.long | signed 64-bit integers |\n| torch.bool | boolean |\n\n## Creating tensors\n\n- `torch.tensor`: &emsp;&emsp;Input individual values\n- `torch.arange`: &emsp;&emsp;Similar to `range` but creates a 1D tensor\n- `torch.linspace`: &emsp;1D linear scale tensor\n- `torch.logspace`: &emsp;1D log scale tensor\n- `torch.rand`: &emsp;&emsp;&emsp;&nbsp;Random numbers from a uniform distribution on `[0, 1)`\n- `torch.randn`: &emsp;&emsp;&ensp;&nbsp;Numbers from the standard normal distribution\n- `torch.randperm`: &emsp;&nbsp;Random permutation of integers\n- `torch.empty`: &emsp;&emsp;&ensp;&nbsp;Uninitialized tensor\n- `torch.zeros`: &emsp;&emsp;&ensp;&nbsp;Tensor filled with `0`\n- `torch.ones`: &emsp;&emsp;&emsp;&nbsp;Tensor filled with `1`\n- `torch.eye`: &emsp;&emsp;&emsp;&ensp;&nbsp;&nbsp;Identity matrix\n\nFirst, let's import PyTorch:\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nimport torch\n```\n:::\n\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\ntorch.manual_seed(0)  # If you want to reproduce the result\ntorch.rand(1)\n```\n\n::: {.cell-output .cell-output-display execution_count=158}\n```\ntensor([0.4963])\n```\n:::\n:::\n\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\ntorch.manual_seed(0)  # Run before each operation to get the same result\ntorch.rand(1).item()  # Extract the value from a tensor\n```\n\n::: {.cell-output .cell-output-display execution_count=159}\n```\n0.49625658988952637\n```\n:::\n:::\n\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\ntorch.rand(1)\ntorch.rand(1, 1)\ntorch.rand(1, 1, 1)\ntorch.rand(1, 1, 1, 1)\n```\n\n::: {.cell-output .cell-output-display execution_count=160}\n```\ntensor([[[[0.3074]]]])\n```\n:::\n:::\n\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\ntorch.rand(2)\ntorch.rand(2, 2, 2, 2)\n```\n\n::: {.cell-output .cell-output-display execution_count=161}\n```\ntensor([[[[0.8964, 0.4556],\n          [0.6323, 0.3489]],\n\n         [[0.4017, 0.0223],\n          [0.1689, 0.2939]]],\n\n\n        [[[0.5185, 0.6977],\n          [0.8000, 0.1610]],\n\n         [[0.2823, 0.6816],\n          [0.9152, 0.3971]]]])\n```\n:::\n:::\n\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\ntorch.rand(2)\ntorch.rand(3)\ntorch.rand(1, 1)\ntorch.rand(1, 1, 1)\ntorch.rand(2, 6)\n```\n\n::: {.cell-output .cell-output-display execution_count=162}\n```\ntensor([[0.3051, 0.9320, 0.1759, 0.2698, 0.1507, 0.0317],\n        [0.2081, 0.9298, 0.7231, 0.7423, 0.5263, 0.2437]])\n```\n:::\n:::\n\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\ntorch.rand(2, 4, dtype=torch.float64)  # You can set dtype\ntorch.ones(2, 1, 4, 5)\n```\n\n::: {.cell-output .cell-output-display execution_count=163}\n```\ntensor([[[[1., 1., 1., 1., 1.],\n          [1., 1., 1., 1., 1.],\n          [1., 1., 1., 1., 1.],\n          [1., 1., 1., 1., 1.]]],\n\n\n        [[[1., 1., 1., 1., 1.],\n          [1., 1., 1., 1., 1.],\n          [1., 1., 1., 1., 1.],\n          [1., 1., 1., 1., 1.]]]])\n```\n:::\n:::\n\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nt = torch.rand(2, 3); print(t)\ntorch.zeros_like(t)             # Matches the size of t\ntorch.ones_like(t)\ntorch.randn_like(t)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntensor([[0.1535, 0.2417, 0.7262],\n        [0.7011, 0.2038, 0.6511]])\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=164}\n```\ntensor([[ 0.1920,  0.5428, -2.2188],\n        [ 0.2590, -1.0297, -0.5008]])\n```\n:::\n:::\n\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\ntorch.arange(2, 10, 4)    # From 2 to 10 in increments of 4\n```\n\n::: {.cell-output .cell-output-display execution_count=165}\n```\ntensor([2, 6])\n```\n:::\n:::\n\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\ntorch.linspace(2, 10, 4)  # 4 elements from 2 to 10 on the linear scale\n```\n\n::: {.cell-output .cell-output-display execution_count=166}\n```\ntensor([ 2.0000,  4.6667,  7.3333, 10.0000])\n```\n:::\n:::\n\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\ntorch.logspace(2, 10, 4)  # Same on the log scale\n```\n\n::: {.cell-output .cell-output-display execution_count=167}\n```\ntensor([1.0000e+02, 4.6416e+04, 2.1544e+07, 1.0000e+10])\n```\n:::\n:::\n\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\ntorch.randperm(4)\n```\n\n::: {.cell-output .cell-output-display execution_count=168}\n```\ntensor([3, 2, 0, 1])\n```\n:::\n:::\n\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\ntorch.eye(3)\n```\n\n::: {.cell-output .cell-output-display execution_count=169}\n```\ntensor([[1., 0., 0.],\n        [0., 1., 0.],\n        [0., 0., 1.]])\n```\n:::\n:::\n\n\n## Tensor information\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\nt = torch.rand(2, 3)\nt\n```\n\n::: {.cell-output .cell-output-display execution_count=170}\n```\ntensor([[0.1872, 0.0340, 0.9442],\n        [0.8802, 0.0012, 0.5936]])\n```\n:::\n:::\n\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\nt.size()\n```\n\n::: {.cell-output .cell-output-display execution_count=171}\n```\ntorch.Size([2, 3])\n```\n:::\n:::\n\n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\nt.dim()\n```\n\n::: {.cell-output .cell-output-display execution_count=172}\n```\n2\n```\n:::\n:::\n\n\n::: {.cell execution_count=18}\n``` {.python .cell-code}\nt.numel()\n```\n\n::: {.cell-output .cell-output-display execution_count=173}\n```\n6\n```\n:::\n:::\n\n\n## Indexing tensors\n\n::: {.cell execution_count=19}\n``` {.python .cell-code}\nt = torch.rand(3, 4)\nt\n```\n\n::: {.cell-output .cell-output-display execution_count=174}\n```\ntensor([[0.4158, 0.4177, 0.2711, 0.6923],\n        [0.2038, 0.6833, 0.7529, 0.8579],\n        [0.6870, 0.0051, 0.1757, 0.7497]])\n```\n:::\n:::\n\n\n::: {.cell execution_count=20}\n``` {.python .cell-code}\nt[:]                 # With a range, the comma is implicit: same as t[:, ]\n```\n\n::: {.cell-output .cell-output-display execution_count=175}\n```\ntensor([[0.4158, 0.4177, 0.2711, 0.6923],\n        [0.2038, 0.6833, 0.7529, 0.8579],\n        [0.6870, 0.0051, 0.1757, 0.7497]])\n```\n:::\n:::\n\n\n::: {.cell execution_count=21}\n``` {.python .cell-code}\nt[:, 2]\n```\n\n::: {.cell-output .cell-output-display execution_count=176}\n```\ntensor([0.2711, 0.7529, 0.1757])\n```\n:::\n:::\n\n\n::: {.cell execution_count=22}\n``` {.python .cell-code}\nt[1, :]\n```\n\n::: {.cell-output .cell-output-display execution_count=177}\n```\ntensor([0.2038, 0.6833, 0.7529, 0.8579])\n```\n:::\n:::\n\n\n::: {.cell execution_count=23}\n``` {.python .cell-code}\nt[2, 3]\n```\n\n::: {.cell-output .cell-output-display execution_count=178}\n```\ntensor(0.7497)\n```\n:::\n:::\n\n\n:::{.info}\n\n**A word of caution about indexing**\n\nWhile indexing elements of a tensor to extract some of the data as a final step of some computation is fine, [you should not use indexing to run operations on tensor elements in a loop]{.emph} as this would be extremely inefficient\n\nInstead, you want to use [vectorized operations]{.emph}\n\n:::\n\n## Vectorized operations\n\nSince PyTorch tensors are homogeneous (i.e. made of a single data type), [as with NumPy's ndarrays](https://www.pythonlikeyoumeanit.com/Module3_IntroducingNumpy/VectorizedOperations.html#Vectorized-Operations), operations are vectorized & thus staggeringly fast\n\nNumPy is mostly written in C & PyTorch in C++. With either library, when you run vectorized operations on arrays/tensors, you don't use raw Python (slow) but compiled C/C++ code (much faster)\n\n[Here](https://pythonspeed.com/articles/vectorization-python/) is an excellent post explaining Python vectorization & why it makes such a big difference\n\n",
    "supporting": [
      "tensors_files"
    ],
    "filters": [],
    "includes": {}
  }
}