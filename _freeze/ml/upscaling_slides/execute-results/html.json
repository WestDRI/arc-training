{
  "hash": "117f150fe9bf65101cf76dc03fdae2bb",
  "result": {
    "markdown": "---\ntitle: Super-resolution with PyTorch\nfrontpic: img/superresolution.png\nauthor: Marie-Hélène Burle\ndate: '2021-11-24'\ndate-format: long\nexecute:\n  error: true\n  echo: true\nformat:\n  revealjs:\n    embed-resources: true\n    theme:\n      - default\n      - ../revealjs.scss\n    logo: /img/sfudrac_logo.png\n    highlight-style: monokai\n    code-line-numbers: false\n    code-overflow: wrap\n    template-partials:\n      - ../title-slide.html\n    pointer:\n      color: '#b5111b'\n      pointerSize: 32\n    link-external-newwindow: true\n    footer: '<a href=\"upscaling.html\"><svg xmlns=\"http://www.w3.org/2000/svg\" width=\"16\" height=\"16\" fill=\"rgb(153, 153, 153)\" class=\"bi bi-arrow-90deg-up\" viewBox=\"0 0 16 16\"><path fill-rule=\"evenodd\" d=\"M4.854 1.146a.5.5 0 0 0-.708 0l-4 4a.5.5 0 1 0 .708.708L4 2.707V12.5A2.5 2.5 0 0 0 6.5 15h8a.5.5 0 0 0 0-1h-8A1.5 1.5 0 0 1 5 12.5V2.707l3.146 3.147a.5.5 0 1 0 .708-.708l-4-4z\"/></svg>&nbsp;Back to workshop page</a>'\nrevealjs-plugins:\n  - pointer\n---\n\n# Definitions\n\n[LR:]{.emph} &ensp;&ensp;&nbsp;low resolution\n\n[HR:]{.emph} &ensp;&ensp;&nbsp;high resolution\n\n[SR:]{.emph} &ensp;&ensp;&nbsp;super-resolution = reconstruction of HR images from LR images\n\n[SISR:]{.emph} &nbsp;&nbsp;single-image super-resolution = SR using a single input image\n\n# History of super-resolution\n\n## Can be broken down into 2 main periods:\n<br>\n\n- A rather slow history with various interpolation algorithms of increasing complexity before deep neural networks\n\n- An incredibly fast evolution since the advent of deep learning (DL)\n\n## SR history Pre-DL\n<br>\n\n[Pixel-wise interpolation prior to DL]{.emph}\n\nVarious methods ranging from simple (e.g. [nearest-neighbour](https://en.wikipedia.org/wiki/Nearest-neighbor_interpolation), [bicubic](https://en.wikipedia.org/wiki/Bicubic_interpolation))\nto complex (e.g. [Gaussian process regression](https://en.wikipedia.org/wiki/Kriging), [iterative FIR Wiener filter](https://en.wikipedia.org/wiki/Wiener_filter)) algorithms\n\n## SR history Pre-DL\n<br>\n\n### [Nearest-neighbour interpolation](https://en.wikipedia.org/wiki/Nearest-neighbor_interpolation)\n\nSimplest method of interpolation\n\nSimply uses the value of the nearest pixel\n\n### [Bicubic interpolation](https://en.wikipedia.org/wiki/Bicubic_interpolation)\n\nConsists of determining the 16 coefficients $a_{ij}$ in:\n\n$$p(x, y) = \\sum_{i=0}^3\\sum_{i=0}^3 a\\_{ij} x^i y^j$$\n\n## SR history with DL\n<br>\n\nDeep learning has seen a fast evolution marked by the successive emergence of various frameworks and architectures over the past 10 years\n\nSome key network architectures and frameworks:\n\n- CNN\n- GAN\n- Transformers\n\nThese have all been applied to SR\n\n---\n\n### SR using (amongst others):\n\n- [Convolutional Neural Networks (SRCNN)](https://arxiv.org/abs/1501.00092) — 2014\n- [Random Forests](https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Schulter_Fast_and_Accurate_2015_CVPR_paper.html) — 2015\n- [Perceptual loss](https://link.springer.com/chapter/10.1007/978-3-319-46475-6_43) — 2016\n- [Sub-pixel CNN](https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Shi_Real-Time_Single_Image_CVPR_2016_paper.html) — 2016\n- [ResNet (SRResNet) & Generative Adversarial Network (SRGAN)](https://openaccess.thecvf.com/content_cvpr_2017/html/Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper.html) — 2017\n- [Enhanced SRGAN (ESRGAN)](https://openaccess.thecvf.com/content_eccv_2018_workshops/w25/html/Wang_ESRGAN_Enhanced_Super-Resolution_Generative_Adversarial_Networks_ECCVW_2018_paper.html) — 2018\n- [Predictive Filter Flow (PFF)](https://arxiv.org/abs/1811.11482) — 2018\n- [Densely Residual Laplacian attention Network (DRLN)](https://ieeexplore.ieee.org/abstract/document/9185010) — 2019\n- [Second-order Attention Network (SAN)](https://openaccess.thecvf.com/content_CVPR_2019/html/Dai_Second-Order_Attention_Network_for_Single_Image_Super-Resolution_CVPR_2019_paper.html) — 2019\n- [Learned downscaling with Content Adaptive Resampler (CAR)](https://ieeexplore.ieee.org/abstract/document/8982168) — 2019\n- [Holistic Attention Network (HAN)](https://link.springer.com/chapter/10.1007/978-3-030-58610-2_12) — 2020\n- [Swin Transformer](https://openaccess.thecvf.com/content/ICCV2021W/AIM/html/Liang_SwinIR_Image_Restoration_Using_Swin_Transformer_ICCVW_2021_paper.html) — 2021\n\n## SRCNN\n\n![](img/srcnn1.png)\n\n:::{.caption}\n\nDong, C., Loy, C. C., He, K., & Tang, X. (2015). Image super-resolution using deep convolutional networks. IEEE transactions on pattern analysis and machine intelligence, 38(2), 295-307\n\n:::\n\n> Given a low-resolution image Y, the first convolutional layer of the SRCNN extracts a set of feature maps. The second layer maps these feature maps nonlinearly to high-resolution patch representations. The last layer combines the predictions within a spatial neighbourhood to produce the final high-resolution image F(Y)\n\n## SRCNN\n\nCan use sparse-coding-based methods\n\n![](img/srcnn2.png)\n\n:::{.caption}\n\nDong, C., Loy, C. C., He, K., & Tang, X. (2015). Image super-resolution using deep convolutional networks. IEEE transactions on pattern analysis and machine intelligence, 38(2), 295-307\n\n:::\n\n## SRGAN\n<br>\n\nDo not provide the best PSNR, but can give more realistic results by providing more texture (less smoothing)\n\n## GAN\n\n![](img/gan.png)\n\n:::{.caption}\n\n[Stevens E., Antiga L., & Viehmann T. (2020). Deep Learning with PyTorch](https://www.manning.com/books/deep-learning-with-pytorch)\n\n:::\n\n## SRGAN\n\n![](img/srgan.png)\n\n:::{.caption}\n\nLedig, C., Theis, L., Huszár, F., Caballero, J., Cunningham, A., Acosta, A., ... & Shi, W. (2017). Photo-realistic single image super-resolution using a generative adversarial network. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 4681-4690)\n\n:::\n\n## SRGAN\n<br>\n\nFollowed by the ESRGAN and many other flavours of SRGANs\n\n# SwinIR\n\n## Attention\n<br>\n\n:::{.note}\n\nMnih, V., Heess, N., & Graves, A. (2014). Recurrent models of visual attention. In Advances in neural information processing systems (pp. 2204-2212)\n\n:::\n\n(cited 2769 times)\n\n:::{.note}\n\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. In Advances in neural information processing systems (pp. 5998-6008)\n\n:::\n\n(cited 30999 times...)\n\n## Transformers\n\n![](img/transformer.png)\n\n:::{.caption}\n\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. In Advances in neural information processing systems (pp. 5998-6008)\n\n:::\n\n## Transformers\n<br>\n\nInitially used for NLP to replace RNN as they allow parallelization\nNow entering the domain of vision and others\nVery performant with relatively few parameters\n\n## Swin Transformer\n<br>\n\nThe [Swin Transformer](https://arxiv.org/abs/2103.14030) improved the use of transformers to the vision domain\n\nSwin = Shifted WINdows\n\n## Swin Transformer\n\nSwin transformer (left) vs transformer as initially applied to vision (right):\n\n![](img/swint.png)\n\n:::{.caption}\n\nLiu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., ... & Guo, B. (2021). Swin transformer: Hierarchical vision transformer using shifted windows. arXiv preprint arXiv:2103.14030\n\n:::\n\n## SwinIR\n<br>\n\n![](img/SwinIR_archi.png)\n\n:::{.caption}\n\nLiang, J., Cao, J., Sun, G., Zhang, K., Van Gool, L., & Timofte, R. (2021). SwinIR: Image restoration using swin transformer. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 1833-1844)\n\n:::\n\n## Training sets used\n<br>\n\n[DIV2K](https://data.vision.ee.ethz.ch/cvl/DIV2K/), Flickr2K, and other datasets\n\n## Models assessment\n<br>\n\n3 metrics commonly used:\n\n#### [Peak sign-to-noise ratio (PSNR) measured in dB](https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio)\n\n$\\frac{\\text{Maximum possible power of signal}}{\\text{Power of noise (calculated as the mean squared error)}}$\n\n[Calculated at the pixel level]{.note}\n\n#### [Structural similarity index measure (SSIM)](https://en.wikipedia.org/wiki/Structural_similarity)\n\nPrediction of perceived image quality based on a \"perfect\" reference image\n\n#### [Mean opinion score (MOS)](https://en.wikipedia.org/wiki/Mean_opinion_score)\n\nMean of subjective quality ratings\n\n## Models assessment\n<br>\n\n#### [Peak sign-to-noise ratio (PSNR) measured in dB](https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio)\n\n$$PSNR = 10\\,\\cdot\\,log_{10}\\,\\left(\\frac{MAX_I^2}{MSE}\\right)$$\n\n#### [Structural similarity index measure (SSIM)](https://en.wikipedia.org/wiki/Structural_similarity)\n\n$$SSIM(x,y) = \\frac{(2\\mu_x\\mu_y + c_1) + (2 \\sigma _{xy} + c_2)} \n    {(\\mu_x^2 + \\mu_y^2+c_1) (\\sigma_x^2 + \\sigma_y^2+c_2)}$$\n\n#### [Mean opinion score (MOS)](https://en.wikipedia.org/wiki/Mean_opinion_score)\n\n$$MOS = \\frac{\\sum_{n=1}^N R\\_n}{N}$$\n\n## Metrics implementation\n<br>\n\n- Implement them yourself (using `torch.log10`, etc.)\n\n- Use some library that implements them (e.g. [kornia](https://github.com/kornia/kornia/tree/master/kornia/losses))\n\n- Use code of open source project with good implementation (e.g. [SwinIR](https://github.com/JingyunLiang/SwinIR/blob/main/utils/util_calculate_psnr_ssim.py))\n\n- Use some higher level library that provides them (e.g. [ignite](https://pytorch.org/ignite/metrics.html))\n\n## Metrics implementation\n<br>\n\n- Implement them yourself (using `torch.log10`, etc.)\n\n- [Use some library that implements them (e.g. [kornia](https://github.com/kornia/kornia/tree/master/kornia/losses))]{.emph}\n\n- Use code of open source project with good implementation (e.g. [SwinIR](https://github.com/JingyunLiang/SwinIR/blob/main/utils/util_calculate_psnr_ssim.py))\n\n- Use some higher level library that provides them (e.g. [ignite](https://pytorch.org/ignite/metrics.html))\n\n## Metrics implementation\n<br>\n\n```{.python}\nimport kornia\n\npsnr_value = kornia.metrics.psnr(input, target, max_val)\nssim_value = kornia.metrics.ssim(img1, img2, window_size, max_val=1.0, eps=1e-12)\n```\n\nSee the Kornia documentation for more info on [kornia.metrics.psnr](https://kornia.readthedocs.io/en/latest/metrics.html?highlight=psnr#kornia.metrics.psnr) & [kornia.metrics.ssim](https://kornia.readthedocs.io/en/latest/metrics.html?highlight=psnr#kornia.metrics.ssim)\n\n## Benchmark datasets\n\n#### [Set5](https://paperswithcode.com/dataset/set5)\n\n![](img/set5.png){width=\"30%\"}\n\n#### [Set14](https://paperswithcode.com/dataset/set14)\n\n![](img/set14.png){width=\"70%\"}\n\n#### [BSD100 (Berkeley Segmentation Dataset)](https://paperswithcode.com/dataset/bsd100)\n\n![](img/bsd100.png){width=\"70%\"}\n\n## Benchmark datasets\n\n#### [Set5]{.emph}\n\n![](img/set5.png){width=\"30%\"}\n\n#### [Set14](https://paperswithcode.com/dataset/set14)\n\n![](img/set14.png){width=\"70%\"}\n\n#### [BSD100 (Berkeley Segmentation Dataset)](https://paperswithcode.com/dataset/bsd100)\n\n![](img/bsd100.png){width=\"70%\"}\n\n## The Set5 dataset\n<br>\n\nA dataset consisting of 5 images which has been used [for at least 18 years](http://people.rennes.inria.fr/Aline.Roumy/results/SR_BMVC12.html) to assess SR methods\n\n## How to get the dataset?\n<br>\n\nFrom the [HuggingFace Datasets Hub](https://huggingface.co/datasets) with the HuggingFace [datasets](https://pypi.org/project/datasets/) package:\n\n```{.python}\nfrom datasets import load_dataset\n\nset5 = load_dataset('eugenesiow/Set5', 'bicubic_x4', split='validation')\n```\n## Dataset exploration\n<br>\n\n```{.python}\nprint(set5)\nlen(set5)\nset5[0]\nset5.shape\nset5.column_names\nset5.features\nset5.set_format('torch', columns=['hr', 'lr'])\nset5.format\n```\n\n## Benchmarks\n<br>\n\n[A 2012 review of interpolation methods for SR](https://ieeexplore.ieee.org/abstract/document/6411957) gives the metrics for a series of interpolation methods (using other datasets)\n\n---\n\n::::{.columns}\n\n:::{.column}\n\n![](img/1_interpolation_psnr1.png){width=\"80%\"}\n\n![](img/3_interpolation_psnr2.png){width=\"80%\"}\n\n:::\n\n:::{.column}\n\n![](img/2_interpolation_ssim1.png){width=\"80%\"}\n\n![](img/4_interpolation_ssim2.png){width=\"80%\"}\n\n:::\n\n::::\n\n## Interpolation methods\n<br>\n\n![](img/1_interpolation_psnr1_mean.png){width=\"80%\"}\n\n![](img/3_interpolation_psnr2_mean.png){width=\"80%\"}\n\n![](img/2_interpolation_ssim1_mean.png){width=\"80%\"}\n\n![](img/4_interpolation_ssim2_mean.png){width=\"80%\"}\n\n## DL methods\n<br>\n\n[The Papers with Code website](https://paperswithcode.com/) lists [available benchmarks on Set5](https://paperswithcode.com/sota/image-super-resolution-on-set5-4x-upscaling)\n\n---\n\n![](img/psnr_ssim_set5.png){width=\"60%\"}\n\n---\n\n#### PSNR vs number of parameters for different methods on Set5x4\n\n![](img/SwinIR_benchmark.png){width=\"80%\"}\n\n:::{.caption}\n\nLiang, J., Cao, J., Sun, G., Zhang, K., Van Gool, L., & Timofte, R. (2021). SwinIR: Image restoration using swin transformer. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 1833-1844)\n\n:::\n\n---\n\n#### Comparison between SwinIR & a representative CNN-based model (RCAN) on classical SR images x4\n\n![](img/SwinIR_CNN_comparison1.png)\n\n:::{.caption}\n\nLiang, J., Cao, J., Sun, G., Zhang, K., Van Gool, L., & Timofte, R. (2021). SwinIR: Image restoration using swin transformer. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 1833-1844)\n\n:::\n\n---\n\n#### Comparison between SwinIR & a representative CNN-based model (RCAN) on classical SR images x4\n\n![](img/SwinIR_CNN_comparison2.png)\n\n:::{.caption}\n\nLiang, J., Cao, J., Sun, G., Zhang, K., Van Gool, L., & Timofte, R. (2021). SwinIR: Image restoration using swin transformer. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 1833-1844)\n\n:::\n\n---\n\n![](img/SwinIR_demo.png)\n\n:::{.caption}\n\nLiang, J., Cao, J., Sun, G., Zhang, K., Van Gool, L., & Timofte, R. (2021). SwinIR: Image restoration using swin transformer. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 1833-1844)\n\n:::\n\n## Let's use SwinIR\n<br>\n\n```sh\n# Get the model\ngit clone git@github.com:JingyunLiang/SwinIR.git\ncd SwinIR\n\n# Copy our test images in the repo\ncp -r <some/path>/my_tests /testsets/my_tests\n\n# Run the model on our images\npython main_test_swinir.py --tile 400 --task real_sr --scale 4 --large_model --model_path model_zoo/swinir/003_realSR_BSRGAN_DFOWMFC_s64w8_SwinIR-L_x4_GAN.pth --folder_lq testsets/my_tests\n```\n\nRan in 9 min on my machine with one GPU and 32GB of RAM\n\n## Results\n\n::::{.columns}\n\n:::{.column}\n\n![](img/lr/berlin_1945_1.jpg)\n\n:::\n\n:::{.column}\n\n![](img/hr/berlin_1945_1.jpg)\n\n:::\n\n::::\n\n## Results\n\n::::{.columns}\n\n:::{.column}\n\n![](img/lr_zoom/berlin_1945_1.jpg)\n\n:::\n\n:::{.column}\n\n![](img/hr_zoom/berlin_1945_1.png)\n\n:::\n\n::::\n\n## Results\n\n::::{.columns}\n\n:::{.column}\n\n![](img/lr/berlin_1945_2.jpg)\n\n:::\n\n:::{.column}\n\n![](img/hr/berlin_1945_2.jpg)\n\n:::\n\n::::\n\n## Results\n\n::::{.columns}\n\n:::{.column}\n\n![](img/lr_zoom/berlin_1945_2.jpg)\n\n:::\n\n:::{.column}\n\n![](img/hr_zoom/berlin_1945_2.png)\n\n:::\n\n::::\n\n## Results\n\n::::{.columns}\n\n:::{.column}\n\n![](img/lr/bruegel.jpg)\n\n:::\n\n:::{.column}\n\n![](img/hr/bruegel.jpg)\n\n:::\n\n::::\n\n## Results\n\n::::{.columns}\n\n:::{.column}\n\n![](img/lr_zoom/bruegel.jpg)\n\n:::\n\n:::{.column}\n\n![](img/hr_zoom/bruegel.png)\n\n:::\n\n::::\n\n## Results\n\n::::{.columns}\n\n:::{.column}\n\n![](img/lr/vasarely.jpg)\n\n:::\n\n:::{.column}\n\n![](img/hr/vasarely.jpg)\n\n:::\n\n::::\n\n## Results\n\n::::{.columns}\n\n:::{.column}\n\n![](img/lr_zoom/vasarely.jpg)\n\n:::\n\n:::{.column}\n\n![](img/hr_zoom/vasarely.png)\n\n:::\n\n::::\n\n## Results\n\n::::{.columns}\n\n:::{.column}\n\n![](img/lr/bird.png)\n\n:::\n\n:::{.column}\n\n![](img/hr/bird.jpg){width=\"93.5%\"}\n\n:::\n\n::::\n\n## Results\n\n::::{.columns}\n\n:::{.column}\n\n![](img/lr_zoom/bird.png)\n\n:::\n\n:::{.column}\n\n![](img/hr_zoom/bird.png)\n\n:::\n\n::::\n\n## Metrics\n<br>\n\nWe could use the [PSNR and SSIM implementations from SwinIR](https://github.com/JingyunLiang/SwinIR/blob/main/utils/util_calculate_psnr_ssim.py), but let's try the [Kornia](https://kornia.readthedocs.io/en/latest/index.html) functions we mentioned earlier:\n\n- [kornia.metrics.psnr](https://kornia.readthedocs.io/en/latest/metrics.html?highlight=psnr#kornia.metrics.psnr)\n\n- [kornia.metrics.ssim](https://kornia.readthedocs.io/en/latest/metrics.html?highlight=psnr#kornia.metrics.ssim)\n\n## Metrics\n<br>\n\nLet's load the libraries we need:\n\n```{.python}\nimport kornia\nfrom PIL import Image\nimport torch\nfrom torchvision import transforms\n```\n\n## Metrics\n<br>\n\nThen, we load one pair images (LR and HR):\n\n```{.python}\nberlin1_lr = Image.open(\"<some/path>/lr/berlin_1945_1.jpg\")\nberlin1_hr = Image.open(\"<some/path>/hr/berlin_1945_1.png\")\n```\n<br>\n\nWe can display these images with:\n\n```{.python}\nberlin1_lr.show()\nberlin1_hr.show()\n```\n\n## Metrics\n<br>\n\nNow, we need to resize them so that they have identical dimensions and turn them into tensors:\n\n```{.python}\npreprocess = transforms.Compose([\n        transforms.Resize(256),\n        transforms.ToTensor()\n        ])\n\nberlin1_lr_t = preprocess(berlin1_lr)\nberlin1_hr_t = preprocess(berlin1_hr)\n```\n\n## Metrics\n<br>\n\n```{.python}\nberlin1_lr_t.shape\nberlin1_hr_t.shape\n```\n\n<div style=\"font-size: 20px; margin: 25px 0 12px 1px; text-align: left; color: #802b00\">[Out]</div>\n\n```{.python}\ntorch.Size([3, 267, 256])\ntorch.Size([3, 267, 256])\n```\n\nWe now have tensors with 3 dimensions:\n\n- the channels (RGB)\n- the height of the image (in pixels)\n- the width of the image (in pixels)\n\n## Metrics\n<br>\n\nAs data processing is done in batch in ML, we need to add a 4th dimension: the **batch size**\n\n(It will be equal to `1` since we have a batch size of a single image)\n\n```{.python}\nbatch_berlin1_lr_t = torch.unsqueeze(berlin1_lr_t, 0)\nbatch_berlin1_hr_t = torch.unsqueeze(berlin1_hr_t, 0)\n```\n\n## Metrics\n<br>\n\nOur new tensors are now ready:\n\n```{.python}\nbatch_berlin1_lr_t.shape\nbatch_berlin1_hr_t.shape\n```\n\n<div style=\"font-size: 20px; margin: 25px 0 12px 1px; text-align: left; color: #802b00\">[Out]</div>\n\n```{.python}\ntorch.Size([1, 3, 267, 256])\ntorch.Size([1, 3, 267, 256])\n```\n\n## PSNR\n<br>\n\n```{.python}\npsnr_value = kornia.metrics.psnr(batch_berlin1_lr_t, batch_berlin1_hr_t, max_val=1.0)\npsnr_value.item()\n```\n\n<div style=\"font-size: 20px; margin: 25px 0 12px 1px; text-align: left; color: #802b00\">[Out]</div>\n\n```{.python}\n33.379642486572266\n```\n\n## SSIM\n<br>\n\n```{.python}\nssim_map = kornia.metrics.ssim(batch_berlin1_lr_t, batch_berlin1_hr_t, window_size=5, max_val=1.0, eps=1e-12)\nssim_map.mean().item()\n```\n\n<div style=\"font-size: 20px; margin: 25px 0 12px 1px; text-align: left; color: #802b00\">[Out]</div>\n\n```{.python}\n0.9868119359016418\n```\n\n# Questions?\n\n",
    "supporting": [
      "upscaling_slides_files"
    ],
    "filters": [],
    "includes": {}
  }
}