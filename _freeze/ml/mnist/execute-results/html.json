{
  "hash": "c8967a915736f702441997271e1142b7",
  "result": {
    "markdown": "---\ntitle: Classifying the MNIST dataset\n---\n\n:::{.def}\n\nIn this workshop, we will classify the MNIST dataset—a classic of machine learning—with PyTorch.\n\n:::\n\n# The MNIST dataset\n\nThe [MNIST](http://yann.lecun.com/exdb/mnist/) is a classic dataset commonly used for testing machine learning systems. It consists of pairs of images of handwritten digits and their corresponding labels.\n\nThe images are composed of 28x28 pixels of greyscale RGB codes ranging from 0 to 255 and the labels are the digits from 0 to 9 that each image represents.\n\nThere are 60,000 training pairs and 10,000 testing pairs.\n\nThe goal is to build a neural network which can learn from the training set to properly identify the handwritten digits and which will perform well when presented with the testing set that it has never seen. This is a typical case of [supervised learning](https://westgrid-ml.netlify.app/autumnschool2020/02_pt_ml#headline-3).\n\n![](img/mnist_nn.png){fig-alt=\"noshadow\"}\n\nNow, let's explore the MNIST with PyTorch.\n\n# Download, unzip, and transform the data\n\n## Where to store the data in the cluster\n\nWe will all use the same data. It would make little sense to all download it in our home directory.\n\nOn the Alliance clusters, a good place to store data shared amongst members of a project is in the `/project` file system.\n\nYou usually belong to `/project/def-<group>`, where `<group>` is the name of your PI. You can access it from your home directory through the symbolic link `~/projects/def-<group>`.\n\nIn our training cluster, we are all part of the group `def-sponsor00`, accessible through `/project/def-sponsor00` (or the hyperlink `~/projects/def-sponsor00`).\n\n**We will thus all access the MNIST data in `~/projects/def-sponsor00/data`.**\n\n## How to obtain the data?\n\nThe dataset can be downloaded directly from [the MNIST website](http://yann.lecun.com/exdb/mnist/), but the PyTorch package TorchVision has tools to download and transform several classic vision datasets, including the MNIST.\n\n```{.python}\nhelp(torchvision.datasets.MNIST)\n```\n```\nHelp on class MNIST in module torchvision.datasets.mnist:\n\nclass MNIST(torchvision.datasets.vision.VisionDataset)\n\n |  MNIST(root: str, train: bool = True, \n |    transform: Optional[Callable] = None,\n |    target_transform: Optional[Callable] = None, \n |    download: bool = False) -> None\n |   \n |  Args:\n |    root (string): Root directory of dataset where \n |      MNIST/raw/train-images-idx3-ubyte and \n |      MNIST/raw/t10k-images-idx3-ubyte exists.\n |    train (bool, optional): If True, creates dataset from \n |      train-images-idx3-ubyte, otherwise from t10k-images-idx3-ubyte.\n |    download (bool, optional): If True, downloads the dataset from the \n |      internet and puts it in root directory. If dataset is already \n |      downloaded, it is not downloaded again.\n |    transform (callable, optional): A function/transform that takes in \n |      an PIL image and returns a transformed version.\n |      E.g, transforms.RandomCrop\n |    target_transform (callable, optional): A function/transform that \n |      takes in the target and transforms it.\n```\n\nNote that here too, the `root` argument sets the location of the downloaded data and we will use `/project/def-sponsor00/data/`.\n\n## Prepare the data\n\nFirst, let's load the needed libraries:\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nimport torch\nfrom torchvision import datasets, transforms\nfrom matplotlib import pyplot as plt\n```\n:::\n\n\nThe MNIST dataset already consists of a training and a testing sets, so we don't have to split the data manually. Instead, we can directly create 2 different objects with the same function (`train=True` selects the train set and `train=False` selects the test set).\n\nWe will transform the raw data to tensors and normalize them using the mean and standard deviation of the MNIST training set: `0.1307` and `0.3081` respectively (even though the mean and standard deviation of the test data are slightly different, it is important to normalize the test data with the values of the training data to apply the same treatment to both sets).\n\nSo we first need to define a transformation:\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.1307,), (0.3081,))\n])\n```\n:::\n\n\n## We can now create our data objects\n\n### Training data\n\n:::{.note}\n\nRemember that `train=True` selects the training set of the MNIST.\n\n:::\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\ntrain_data = datasets.MNIST(\n    '~/projects/def-sponsor00/data',\n    train=True, download=True, transform=transform)\n```\n:::\n\n\n### Test data\n\n:::{.note}\n\n`train=False` selects the test set.\n\n:::\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\ntest_data = datasets.MNIST(\n    '~/projects/def-sponsor00/data',\n    train=False, transform=transform)\n```\n:::\n\n\n# Exploring the data\n\n## Data inspection\n\nFirst, let's check the size of `train_data`:\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nprint(len(train_data))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n60000\n```\n:::\n:::\n\n\nThat makes sense since the MNIST's training set has 60,000 pairs. `train_data` has 60,000 elements and we should expect each element to be of size 2 since it is a pair. Let's double-check with the first element:\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nprint(len(train_data[0]))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n2\n```\n:::\n:::\n\n\nSo far, so good. We can print that first pair:\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nprint(train_data[0])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(tensor([[[-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.3860, -0.1951,\n          -0.1951, -0.1951,  1.1795,  1.3068,  1.8032, -0.0933,  1.6887,\n           2.8215,  2.7197,  1.1923, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.0424,  0.0340,  0.7722,  1.5359,  1.7396,  2.7960,\n           2.7960,  2.7960,  2.7960,  2.7960,  2.4396,  1.7650,  2.7960,\n           2.6560,  2.0578,  0.3904, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n           0.1995,  2.6051,  2.7960,  2.7960,  2.7960,  2.7960,  2.7960,\n           2.7960,  2.7960,  2.7960,  2.7706,  0.7595,  0.6195,  0.6195,\n           0.2886,  0.0722, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.1951,  2.3633,  2.7960,  2.7960,  2.7960,  2.7960,  2.7960,\n           2.0960,  1.8923,  2.7197,  2.6433, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242,  0.5940,  1.5614,  0.9377,  2.7960,  2.7960,  2.1851,\n          -0.2842, -0.4242,  0.1231,  1.5359, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.2460, -0.4115,  1.5359,  2.7960,  0.7213,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242,  1.3450,  2.7960,  1.9942,\n          -0.3988, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.2842,  1.9942,  2.7960,\n           0.4668, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.0213,  2.6433,\n           2.4396,  1.6123,  0.9504, -0.4115, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.6068,\n           2.6306,  2.7960,  2.7960,  1.0904, -0.1060, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n           0.1486,  1.9432,  2.7960,  2.7960,  1.4850, -0.0806, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.2206,  0.7595,  2.7833,  2.7960,  1.9560, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242,  2.7451,  2.7960,  2.7451,  0.3904,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n           0.1613,  1.2305,  1.9051,  2.7960,  2.7960,  2.2105, -0.3988,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.0722,  1.4596,\n           2.4906,  2.7960,  2.7960,  2.7960,  2.7578,  1.8923, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.1187,  1.0268,  2.3887,  2.7960,\n           2.7960,  2.7960,  2.7960,  2.1342,  0.5686, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.1315,  0.4159,  2.2869,  2.7960,  2.7960,  2.7960,\n           2.7960,  2.0960,  0.6068, -0.3988, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.1951,\n           1.7523,  2.3633,  2.7960,  2.7960,  2.7960,  2.7960,  2.0578,\n           0.5940, -0.3097, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242,  0.2758,  1.7650,  2.4524,\n           2.7960,  2.7960,  2.7960,  2.7960,  2.6815,  1.2686, -0.2842,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242,  1.3068,  2.7960,  2.7960,\n           2.7960,  2.2742,  1.2941,  1.2559, -0.2206, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242]]]), 5)\n```\n:::\n:::\n\n\nAnd you can see that it is a tuple with:\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nprint(type(train_data[0]))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'tuple'>\n```\n:::\n:::\n\n\nWhat is that tuple made of?\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nprint(type(train_data[0][0]))\nprint(type(train_data[0][1]))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'torch.Tensor'>\n<class 'int'>\n```\n:::\n:::\n\n\nIt is made of the tensor for the first image (remember that we transformed the images into tensors when we created the objects `train_data` and `test_data`) and the integer of the first label (which you can see is 5 when you print `train_data[0][1]`).\n\nSo since `train_data[0][0]` is the tensor representing the image of the first pair, let's check its size:\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\nprint(train_data[0][0].size())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntorch.Size([1, 28, 28])\n```\n:::\n:::\n\n\nThat makes sense: a color image would have 3 layers of RGB values (so the size in the first dimension would be 3), but because the MNIST has black and white images, there is a single layer of values—the values of each pixel on a gray scale—so the first dimension has a size of 1. The 2nd and 3rd dimensions correspond to the width and length of the image in pixels, hence 28 and 28.\n\n:::{.exo}\n\n:::{.yourturn}\n\nYour turn:\n\n:::\n\nRun the following:\n\n```{.python}\nprint(train_data[0][0][0])\nprint(train_data[0][0][0][0])\nprint(train_data[0][0][0][0][0])\n```\n\nAnd think about what each of them represents. \\\nThen explore the `test_data` object.\n\n:::\n\n## Plotting an image from the data\n\nFor this, we will use `pyplot` from `matplotlib`.\n\nFirst, we select the image of the first pair and we resize it from 3 to 2 dimensions by removing its dimension of size 1 with `torch.squeeze`:\n\n```{.python}\nimg = torch.squeeze(train_data[0][0])\n```\n\nThen, we plot it with `pyplot`, but since we are in a cluster, instead of showing it to screen with `plt.show()`, we save it to file:\n\n```{.python}\nplt.imshow(img, cmap='gray')\n```\n\nThis is what that first image looks like:\n\n![](img/mnist_digit.png){fig-alt=\"noshadow\"}\n\nAnd indeed, it matches the first label we explored earlier (`train_data[0][1]`).\n\n## Plotting an image with its pixel values\n\nWe can plot it with more details by showing the value of each pixel in the image. One little twist is that we need to pick a threshold value below which we print the pixel values in white otherwise they would not be visible (black on near black background). We also round the pixel values to one decimal digit so as not to clutter the result.\n\n```{.python}\nimgplot = plt.figure(figsize = (12, 12))\nsub = imgplot.add_subplot(111)\nsub.imshow(img, cmap='gray')\nwidth, height = img.shape\nthresh = img.max() / 2.5\nfor x in range(width):\n    for y in range(height):\n        val = round(img[x][y].item(), 1)\n        sub.annotate(str(val), xy=(y, x),\n                     horizontalalignment='center',\n                     verticalalignment='center',\n                     color='white' if img[x][y].item() < thresh else 'black')\n```\n\n![](img/mnist_digit_pixels.png){fig-alt=\"noshadow\"}\n\n# Batch processing\n\nPyTorch provides the [torch.utils.data.DataLoader](https://pytorch.org/docs/stable/data.html?highlight=dataloader#module-torch.utils.data) class which combines a dataset and an optional sampler and provides an iterable (while training or testing our neural network, we will iterate over that object). It allows, [among many other things](https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader), to set the batch size and shuffle the data.\n\nSo our last step in preparing the data is to pass it through `DataLoader`.\n\n## Create DataLoaders\n\n### Training data\n\n```{.python}\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=20, shuffle=True)\n```\n\n### Test data\n\n```{.python}\ntest_loader = torch.utils.data.DataLoader(\n    test_data, batch_size=20, shuffle=False)\n```\n\n## Plot a full batch of images with their labels\n\nNow that we have passed our data through `DataLoader`, it is easy to select one batch from it. Let's plot an entire batch of images with their labels.\n\nFirst, we need to get one batch of training images and their labels:\n\n```{.python}\ndataiter = iter(train_loader)\nbatchimg, batchlabel = dataiter.next()\n```\n\nThen, we can plot them:\n\n```{.python}\nbatchplot = plt.figure(figsize=(20, 5))\nfor i in torch.arange(20):\n    sub = batchplot.add_subplot(2, 10, i+1, xticks=[], yticks=[])\n    sub.imshow(torch.squeeze(batchimg[i]), cmap='gray')\n    sub.set_title(str(batchlabel[i].item()), fontsize=25)\n```\n\n![](img/mnist_digit_batch.png){fig-alt=\"noshadow\"}\n\n# Time to build a NN to classify the MNIST\n\nLet's build a [multi-layer perceptron (MLP)](https://en.wikipedia.org/wiki/Multilayer_perceptron): the simplest neural network. It is a *feed-forward* (i.e. no loop), *fully-connected* (i.e. each neuron of one layer is connected to all the neurons of the adjacent layers) *neural network with a single hidden layer*.\n\n![](img/mlp.png){fig-alt=\"noshadow\" width=70%}\n\n## Load packages\n\n```{.python}\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torchvision import datasets, transforms\nfrom torch.optim.lr_scheduler import StepLR\n```\n\nThe `torch.nn.functional` module contains all the functions of the `torch.nn` package. \\\nThese functions include loss functions, activation functions, pooling functions...\n\n## Create a `SummaryWriter` instance for TensorBoard\n\n```{.python}\nwriter = SummaryWriter()\n```\n\n## Define the architecture of the network\n\n```{.python}\n# To build a model, create a subclass of torch.nn.Module:\nclass Net(nn.Module):\n\tdef __init__(self):\n\t\tsuper(Net, self).__init__()\n\t\tself.fc1 = nn.Linear(784, 128)\n\t\tself.fc2 = nn.Linear(128, 10)\n\n    # Method for the forward pass:\n\tdef forward(self, x):\n\t\tx = torch.flatten(x, 1)\n\t\tx = self.fc1(x)\n\t\tx = F.relu(x)\n\t\tx = self.fc2(x)\n\t\toutput = F.log_softmax(x, dim=1)\n\t\treturn output\n```\n\nPython’s class inheritance gives our subclass all the functionality of `torch.nn.Module` while allowing us to customize it.\n\n## Define a training function\n\n```{.python}\ndef train(model, device, train_loader, optimizer, epoch):\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n        optimizer.zero_grad()  # reset the gradients to 0\n        output = model(data)\n        loss = F.nll_loss(output, target)  # negative log likelihood\n        writer.add_scalar(\"Loss/train\", loss, epoch)\n        loss.backward()\n        optimizer.step()\n        if batch_idx % 10 == 0:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(data), len(train_loader.dataset),\n                100. * batch_idx / len(train_loader), loss.item()))\n```\n\n## Define a testing function\n\n```{.python}\ndef test(model, device, test_loader):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            # Sum up batch loss:\n            test_loss += F.nll_loss(output, target, reduction='sum').item()\n            # Get the index of the max log-probability:\n            pred = output.argmax(dim=1, keepdim=True)\n            correct += pred.eq(target.view_as(pred)).sum().item()\n\n    test_loss /= len(test_loader.dataset)\n\n    # Print a summary\n    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n        test_loss, correct, len(test_loader.dataset),\n        100. * correct / len(test_loader.dataset)))\n```\n\n## Define a function main() which runs our network\n\n```{.python}\ndef main():\n    epochs = 1\n    torch.manual_seed(1)\n    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n\n    transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,))\n    ])\n\n    train_data = datasets.MNIST(\n        '~/projects/def-sponsor00/data',\n        train=True, download=True, transform=transform)\n\n    test_data = datasets.MNIST(\n        '~/projects/def-sponsor00/data',\n        train=False, transform=transform)\n\n    train_loader = torch.utils.data.DataLoader(train_data, batch_size=64)\n    test_loader = torch.utils.data.DataLoader(test_data, batch_size=1000)\n    model = Net().to(device)  # create instance of our network and send it to device\n    optimizer = optim.Adadelta(model.parameters(), lr=1.0)\n    scheduler = StepLR(optimizer, step_size=1, gamma=0.7)\n\n    for epoch in range(1, epochs + 1):\n        train(model, device, train_loader, optimizer, epoch)\n        test(model, device, test_loader)\n        scheduler.step()\n```\n\n## Run the network\n\n```{.python}\nmain()\n```\n\n## Write pending events to disk and close the TensorBoard\n\n```{.python}\nwriter.flush()\nwriter.close()\n```\n\nThe code is working. Time to actually train our model!\n\nJupyter is a fantastic tool. It has a major downside however: when you launch a Jupyter server, you are running a job on a compute node. If you want to play for 8 hours in Jupyter, you are requesting an 8 hour job. Now, most of the time you spend on Jupyter is spent typing, running bits and pieces of code, or doing nothing at all. If you ask for GPUs, many CPUs, and lots of RAM, all of it will remain idle almost all of the time. It is a really suboptimal use of the Alliance resources.\n\nIn addition, if you ask for lots of resources for a long time, you will have to wait a long time in the queue before they get allocated to you.\n\nLastly, you will go through your allocation quickly.\n\nA much better strategy is to develop and test your code (with very little data, few epochs, etc.) in an interactive job (with `salloc`) or in Jupyter, *then*, launch an `sbatch` job to actually train your model. This ensures that heavy duty resources such as GPU(s) are only allocated to you when you are actually needing and using them.\n\n:::{.note}\n\nConcrete example with our training cluster: this cluster only has 1 GPU. If you want to use it in Jupyter, you have to request it for your Jupyter session. This means that the entire time your Jupyter session is active, nobody else can use that GPU. While you let your session idle or do tasks that do not require a GPU, this is not a good use of resources.\n\n:::\n\n# Let's train and test our model\n\n## Log in the training cluster\n\nOpen a terminal and SSH to our training cluster [as we saw in the first lesson](https://westgrid-ml.netlify.app/autumnschool2020/01_pt_intro/#headline-3).\n\n## Load necessary modules\n\nFirst, we need to load the Python and CUDA modules. This is done with the [Lmod](https://github.com/TACC/Lmod) tool through the [module](https://docs.computecanada.ca/wiki/Utiliser_des_modules/en) command. Here are some key [Lmod commands](https://lmod.readthedocs.io/en/latest/010_user.html):\n\n```{.bash}\n# Get help on the module command\n$ module help\n\n# List modules that are already loaded\n$ module list\n\n# See which modules are available for a tool\n$ module avail <tool>\n\n# Load a module\n$ module load <module>[/<version>]\n```\n\nHere are the modules we need:\n\n```{.bash}\n$ module load nixpkgs/16.09 gcc/7.3.0 cuda/10.0.130 cudnn/7.6 python/3.8.2\n```\n\n## Install Python packages\n\nYou also need the Python packages `matplotlib`, `torch`, `torchvision`, and `tensorboard`.\n\nOn the Alliance clusters, you need to create a virtual environment in which you install packages with `pip`.\n\n:::{.box}\n\n**Do not use Anaconda.**\n\nWhile Anaconda is a great tool on personal computers, it is not an appropriate tool when working on the Alliance clusters: binaries are unoptimized for those clusters and library paths are inconsistent with their architecture. Anaconda installs packages in `$HOME` where it creates a very large number of small files. It can also create conflicts by modifying `.bashrc`.\n\n:::\n\nFor this workshop, since we all need the same packages, I already created a virtual environment that we will all use. *All you have to do is to activate it with*:\n\n```{.bash}\n$ source ~/projects/def-sponsor00/env/bin/activate\n```\n\nIf you want to exit the virtual environment, you can press Ctrl-D or run:\n\n```{.bash}\n(env) $ deactivate\n```\n\n:::{.note}\n\nFor future reference, below is how you would install packages on a real Alliance cluster (but please don't do it in the training cluster as it is unnecessary and would only slow it down).\n\nCreate a virtual environment:\n\n```{.bash}\n$ virtualenv --no-download ~/env\n```\n\nActivate the virtual environment:\n\n```{.bash}\n$ source ~/env/bin/activate\n```\n\nUpdate pip:\n\n```{.bash}\n(env) $ pip install --no-index --upgrade pip\n```\n\nInstall the packages you need in the virtual environment:\n\n```{.bash}\n(env) $ pip install --no-cache-dir --no-index matplotlib torch torchvision tensorboard\n```\n\n:::\n\n## Write a Python script\n\nCreate a directory for this project and `cd` into it:\n\n```{.bash}\nmkdir mnist\ncd mnist\n```\n\nStart a Python script with the text editor of your choice:\n\n```{.bash}\nnano nn.py\n```\n\nIn it, copy-paste the code we played with in Jupyter, but this time have it run for 10 epochs:\n\n```{.python}\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torchvision import datasets, transforms\nfrom torch.optim.lr_scheduler import StepLR\n\nwriter = SummaryWriter()\n\nclass Net(nn.Module):\n\tdef __init__(self):\n\t\tsuper(Net, self).__init__()\n\t\tself.fc1 = nn.Linear(784, 128)\n\t\tself.fc2 = nn.Linear(128, 10)\n\n\tdef forward(self, x):\n\t\tx = torch.flatten(x, 1)\n\t\tx = self.fc1(x)\n\t\tx = F.relu(x)\n\t\tx = self.fc2(x)\n\t\toutput = F.log_softmax(x, dim=1)\n\t\treturn output\n\ndef train(model, device, train_loader, optimizer, epoch):\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = F.nll_loss(output, target)\n        writer.add_scalar(\"Loss/train\", loss, epoch)\n        loss.backward()\n        optimizer.step()\n        if batch_idx % 10 == 0:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(data), len(train_loader.dataset),\n                100. * batch_idx / len(train_loader), loss.item()))\n\ndef test(model, device, test_loader):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            test_loss += F.nll_loss(output, target, reduction='sum').item()\n            pred = output.argmax(dim=1, keepdim=True)\n            correct += pred.eq(target.view_as(pred)).sum().item()\n\n    test_loss /= len(test_loader.dataset)\n\n    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n        test_loss, correct, len(test_loader.dataset),\n        100. * correct / len(test_loader.dataset)))\n\ndef main():\n    epochs = 10  # don't forget to change the number of epochs\n    torch.manual_seed(1)\n    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n\n    transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,))\n    ])\n\n    train_data = datasets.MNIST(\n        '~/projects/def-sponsor00/data',\n        train=True, download=True, transform=transform)\n\n    test_data = datasets.MNIST(\n        '~/projects/def-sponsor00/data',\n        train=False, transform=transform)\n\n    train_loader = torch.utils.data.DataLoader(train_data, batch_size=64)\n    test_loader = torch.utils.data.DataLoader(test_data, batch_size=1000)\n    model = Net().to(device)\n    optimizer = optim.Adadelta(model.parameters(), lr=1.0)\n    scheduler = StepLR(optimizer, step_size=1, gamma=0.7)\n\n    for epoch in range(1, epochs + 1):\n        train(model, device, train_loader, optimizer, epoch)\n        test(model, device, test_loader)\n        scheduler.step()\n\nmain()\n\nwriter.flush()\nwriter.close()\n```\n\n## Write a Slurm script\n\nWrite a shell script with the text editor of your choice:\n\n```{.bash}\nnano nn.sh\n```\n\nThis is what you want in that script:\n\n```{.bash}\n#!/bin/bash\n#SBATCH --time=5:0\n#SBATCH --cpus-per-task=1\n#SBATCH --gres=gpu:1\n#SBATCH --mem=4G\n#SBATCH --output=%x_%j.out\n#SBATCH --error=%x_%j.err\n\npython ~/mnist/nn.py\n```\n\n:::{.note}\n\n`--time` accepts these formats: \"min\", \"min:s\", \"h:min:s\", \"d-h\", \"d-h:min\" & \"d-h:min:s\" \\\n`%x` will get replaced by the script name & `%j` by the job number\n\n:::\n\n## Submit a job\n\nFinally, you need to submit your job to Slurm:\n\n```{.bash}\n$ sbatch ~/mnist/nn.sh\n```\n\nYou can check the status of your job with:\n\n```{.bash}\n$ sq\n```\n\n:::{.note}\n\n`PD` = pending\\\n`R` = running\\\n`CG` = completing (Slurm is doing the closing processes) \\\nNo information = your job has finished running\n\n:::\n\nYou can cancel it with:\n\n```{.bash}\n$ scancel <jobid>\n```\n\nOnce your job has finished running, you can display efficiency measures with:\n\n```{.bash}\n$ seff <jobid>\n```\n\n# Let's explore our model's metrics with TensorBoard\n\n[TensorBoard](https://github.com/tensorflow/tensorboard) is a web visualization toolkit developed by TensorFlow which can be used with PyTorch.\n\nBecause we have sent our model's metrics logs to TensorBoard as part of our code, a directory called `runs` with those logs was created in our `~/mnist` directory.\n\n## Launch TensorBoard\n\nTensorBoard requires too much processing power to be run on the login node. When you run long jobs, the best strategy is to launch it in the background as part of the job. This allows you to monitor your model as it is running (and cancel it if things don't look right).\n\n:::{.example}\n\nExample:\n\n:::\n\n```{.bash}\n#!/bin/bash\n#SBATCH ...\n#SBATCH ...\n\ntensorboard --logdir=runs --host 0.0.0.0 &\npython ~/mnist/nn.py\n```\n\nBecause we only have 1 GPU and are taking turns running our jobs, we need to keep our jobs very short here. So we will launch a separate job for TensorBoard. This time, we will launch an interactive job:\n\n```{.bash}\nsalloc --time=1:0:0 --mem=2000M\n```\n\nTo launch TensorBoard, we need to activate our Python virtual environment (TensorBoard was installed by `pip`):\n\n```{.bash}\nsource ~/projects/def-sponsor00/env/bin/activate\n```\n\nThen we can launch TensorBoard in the background:\n\n```{.bash}\ntensorboard --logdir=~/mnist/runs --host 0.0.0.0 &\n```\n\nNow, we need to create a connection with SSH tunnelling between your computer and the compute note running your TensorBoard job.\n\n## Connect to TensorBoard from your computer\n\n**From a new terminal on your computer**, run:\n\n```{.bash}\nssh -NfL localhost:6006:<hostname>:6006 userxxx@uu.c3.ca\n```\n\n:::{.note}\n\nReplace `<hostname>` by the name of the compute node running your `salloc` job. You can find it by looking at your prompt (your prompt shows `<username>@<hostname>`).\n\nReplace `<userxxx>` by your user name.\n\n:::\n\nNow, you can open a browser on your computer and access TensorBoard at [http://localhost:6006](http://localhost:6006).\n\n",
    "supporting": [
      "mnist_files"
    ],
    "filters": [],
    "includes": {}
  }
}