{
  "hash": "c0da59744465a5e36196398dc86a9a59",
  "result": {
    "markdown": "---\ntitle: foreach and doFuture\naliases:\n  - parallel_loops.html\nauthor: Marie-Hélène Burle\n---\n\n\n:::{.def}\n\nOne of the options to parallelize code with the [future](https://cran.r-project.org/web/packages/future/index.html) package is to use [foreach](https://cran.r-project.org/web/packages/foreach/index.html) with [doFuture](https://cran.r-project.org/web/packages/doFuture/index.html). In this section, we will go over an example with the random forest algorithm.\n\n:::\n\n## Our example code: random forest\n\n### On the `iris` dataset\n\nRandom forest is a commonly used ensemble learning technique for classification and regression. The idea is to combine the results from many decision trees on bootstrap samples of the dataset to improve the predictive accuracy and control over-fitting. The algorithm used was developed by Tin Kam Ho, then improved by Leo Breiman and Adele Cutler. An implementation in R is provided by the `randomForest()` function from the [randomForest](https://cran.r-project.org/web/packages/randomForest/index.html) package. Let's use it to classify the [iris dataset](https://en.wikipedia.org/wiki/Iris_flower_data_set) that comes [packaged with R](https://search.r-project.org/CRAN/refmans/MVTests/html/iris.html).\n\nFirst, let's have a look at the dataset:\n\n\n::: {.cell hash='hpc_foreach_cache/html/unnamed-chunk-1_b18d58fde4cbac727b57d7c1ea4e7bda'}\n\n```{.r .cell-code}\n# Structure of the dataset\nstr(iris)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'data.frame':\t150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n```\n:::\n\n```{.r .cell-code}\n# Dimensions of the dataset\ndim(iris)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 150   5\n```\n:::\n\n```{.r .cell-code}\n# First 6 data points\nhead(iris)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n```\n:::\n\n```{.r .cell-code}\n# The 3 species (3 levels of the factor)\nlevels(iris$Species)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"setosa\"     \"versicolor\" \"virginica\" \n```\n:::\n:::\n\n\nThe goal is to create a random forest model (let's call it `rf`) that can classify an iris flower in one of the 3 species based on the 4 measurements of its sepals and petals.\n\n\n::: {.cell hash='hpc_foreach_cache/html/unnamed-chunk-2_3348449c834ce3f068cfb53753268b39'}\n\n```{.r .cell-code}\nlibrary(randomForest)\n\nset.seed(123)\nrf <- randomForest(Species ~ ., data=iris)\n```\n:::\n\n\n:::{.note}\n\nOur response variable (`Species`) is a factor, so classification is assumed.\n\nThe `.` on the right side of the formula represents all other variables (so we are using all variables, except for the response variable `Species` of course, as feature variables).\n\n:::\n\n\n::: {.cell hash='hpc_foreach_cache/html/unnamed-chunk-3_ce0d110248137685209fe19dcae3dce1'}\n\n```{.r .cell-code}\nrf\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\n randomForest(formula = Species ~ ., data = iris) \n               Type of random forest: classification\n                     Number of trees: 500\nNo. of variables tried at each split: 2\n\n        OOB estimate of  error rate: 4.67%\nConfusion matrix:\n           setosa versicolor virginica class.error\nsetosa         50          0         0        0.00\nversicolor      0         47         3        0.06\nvirginica       0          4        46        0.08\n```\n:::\n:::\n\n\nAs can be seen by the confusion matrix, our model performs well.\n\nWe can use it on new data to make predictions. Let's try with some made-up data:\n\n\n::: {.cell hash='hpc_foreach_cache/html/unnamed-chunk-4_0dfca8182dac1ab3a593fa8412b264a2'}\n\n```{.r .cell-code}\nnew_data <- data.frame(\n  Sepal.Length = c(5.3, 4.6, 6.5),\n  Sepal.Width = c(3.1, 3.9, 2.5),\n  Petal.Length = c(1.5, 1.5, 5.0),\n  Petal.Width = c(0.2, 0.1, 2.1)\n)\n\nnew_data\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Sepal.Length Sepal.Width Petal.Length Petal.Width\n1          5.3         3.1          1.5         0.2\n2          4.6         3.9          1.5         0.1\n3          6.5         2.5          5.0         2.1\n```\n:::\n\n```{.r .cell-code}\npredict(rf, new_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        1         2         3 \n   setosa    setosa virginica \nLevels: setosa versicolor virginica\n```\n:::\n:::\n\n\n### Let's make it big\n\nNow, the iris dataset only has 150 observations and we used the default number of trees (500) of the `randomForest()` function, so things ran fast. Often, random forests are run on large datasets. Let's artificially increase the iris dataset and use more trees to create a situation in which parallelization would make sense.\n\nOne easy way is to replicate each row 100 times (and we can then delete the row names that get created by this operation):\n\n\n::: {.cell hash='hpc_foreach_cache/html/unnamed-chunk-5_69149c52e50fe8b702c5a5ea86b51c5c'}\n\n```{.r .cell-code}\nbig_iris <- iris[rep(seq_len(nrow(iris)), each = 1e2), ]\nrownames(big_iris) <- NULL\n```\n:::\n\n::: {.cell hash='hpc_foreach_cache/html/unnamed-chunk-6_aefb44a38e057feb17fdfcd0391cefd3'}\n\n```{.r .cell-code}\ndim(big_iris)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 15000     5\n```\n:::\n:::\n\n\nAnd then we can run `randomForest()` on this dataset and 2000 trees.\n\n## Hidden parallelism check\n\nBefore parallelizing your code, remember to check whether the package you are using is already doing any parallelization under the hood (after all, maybe the `randomForest` package runs things in parallel. We don't know).\n\nOne way to do this is to test the package on your local machine and, while some sample code is running, to open [htop](https://htop.dev/) and see how many cores are used.\n\nWhy do this on your local machine? because on the cluster, if you launch `htop` while your batch job is running, you will be looking at processes running on the login node while your code is running on compute node(s). So this will not help you. You could salloc on the/one of the compute node(s) running your job and run `htop` there, but in production clusters, compute nodes are large and you will see all the processes from all the other users using that compute node. So this test is just easier done locally.\n\nOn my machine I ran:\n\n```{.r}\nlibrary(randomForest)\n\nbig_iris <- iris[rep(seq_len(nrow(iris)), each = 1e2), ]\nrownames(big_iris) <- NULL\n\nset.seed(123)\nrf <- randomForest(Species ~ ., data=big_iris, ntree=2000)\n```\n\nAnd I could confirm that the function does not run in parallel.\n\nSo let's parallelize this code.\n\n## The `foreach` package\n\nThe [foreach](https://cran.r-project.org/web/packages/foreach/index.html) package provides a construct for repeated executions, i.e. it can replace for loops, while loops, repeat loops, and functional programming code written with the *apply functions or the [purrr](https://cran.r-project.org/web/packages/purrr/index.html) package. The [foreach vignette](https://cran.r-project.org/web/packages/foreach/vignettes/foreach.html) gives many examples.\n\n## The `doFuture` package\n\nThe most useful part of `foreach` is that it allows for easily parallelization with countless backends: `doFuture`, `doMC`, `doMPI`, `doParallel`, `doRedis`, `doRNG`, `doSNOW`, and `doAzureParallel`.\n\nThe [doFuture](https://cran.r-project.org/web/packages/doFuture/index.html) package is the most modern of these backends. It allows to evaluate `foreach` expressions across the evaluation strategies of the [future](https://cran.r-project.org/web/packages/future/index.html) package very easily. All you have to do is to register it as a backend, declare the evaluation strategy of futures of your choice, make sure to generate parallel-safe random numbers for reproducibility (if your code uses randomness), and replace `%do%` with `%dofuture%`.\n\n## Benchmarks\n\nWe will run and benchmark all versions of our code by submitting batch jobs to Slurm.\n\n### Initial code\n\nFirst, let's benchmark the initial (non parallel, not using `foreach`) code. We need to create an R script. Let's call it `reference.R` (I will use [Emacs](https://www.gnu.org/software/emacs/), but you can use the [nano text editor](https://www.nano-editor.org/) with the command `nano` to write the script if you want):\n\n```{.r filename=\"reference.R\"}\nlibrary(randomForest)\nlibrary(bench)\n\nbig_iris <- iris[rep(seq_len(nrow(iris)), each = 1e2), ]\nrownames(big_iris) <- NULL\n\ncat(\"\\nBenchmarking results:\\n\\n\")\n\nset.seed(123)\nmark(rf <- randomForest(Species ~ ., data=big_iris, ntree=2000))\n```\n\nThen we need to create a Bash script for Slurm. Let's call it `reference.sh`:\n\n```{.bash filename=\"reference.sh\"}\n#!/bin/bash\n#SBATCH --time=5\n#SBATCH --mem-per-cpu=7500M\n\nRscript reference.R\n```\n\n:::{.note}\n\nYou can see the full list of `sbatch` options [here](https://slurm.schedmd.com/sbatch.html).\n\n:::\n\nAnd now we submit the job with:\n\n```{.bash}\nsbatch reference.sh\n```\n\nYou can monitor your job with `sq`. The result will be written to a file called `slurm-xx.out` with `xx` being the number of the job that just ran. To see the result, we can simply print the content of that file to screen with `cat` (you can run `ls` to see the list of files in the current directory). **Make sure that your job has finished running before printing the result** (otherwise you might get a partial output which can be confusing).\n\n```{.bash}\ncat slurm-xx.out    # Replace xx by the job number\n```\n\n```\nrandomForest 4.7-1.1\nType rfNews() to see new features/changes/bug fixes.\n\nBenchmarking results:\n\n# A tibble: 1 × 13\n  expression      min median `itr/sec` mem_alloc `gc/sec` n_itr  n_gc total_time\n  <bch:expr>    <bch> <bch:>     <dbl> <bch:byt>    <dbl> <int> <dbl>   <bch:tm>\n1 rf <- random… 6.33s  6.33s     0.158        NA    0.474     1     3      6.33s\n# ℹ 4 more variables: result <list>, memory <list>, time <list>, gc <list>\nWarning message:\nSome expressions had a GC in every iteration; so filtering is disabled.\n```\n\n### `foreach` expression\n\nNow, let's try the `foreach` version:\n\n```{.r filename=\"foreach.R\"}\nlibrary(foreach)\nlibrary(randomForest)\nlibrary(bench)\n\nbig_iris <- iris[rep(seq_len(nrow(iris)), each = 1e2), ]\nrownames(big_iris) <- NULL\n\ncat(\"\\nBenchmarking results:\\n\\n\")\n\nset.seed(123)\nmark(\n  rf <- foreach(ntree = rep(250, 8), .combine = combine) %do%\n    randomForest(Species ~ ., data=big_iris, ntree=ntree)\n)\n```\n\n```{.bash filename=\"foreach.sh\"}\n#!/bin/bash\n#SBATCH --time=5\n#SBATCH --mem-per-cpu=7500M\n\nRscript foreach.R\n```\n\n```{.bash}\nsbatch foreach.sh\n```\n\n```\nrandomForest 4.7-1.1\nType rfNews() to see new features/changes/bug fixes.\n\nBenchmarking results:\n\n# A tibble: 1 × 13\n  expression      min median `itr/sec` mem_alloc `gc/sec` n_itr  n_gc total_time\n  <bch:expr>    <bch> <bch:>     <dbl> <bch:byt>    <dbl> <int> <dbl>   <bch:tm>\n1 rf <- foreac… 7.04s  7.04s     0.142        NA     4.55     1    32      7.04s\n# ℹ 4 more variables: result <list>, memory <list>, time <list>, gc <list>\nWarning message:\nSome expressions had a GC in every iteration; so filtering is disabled.\n```\n\n:::{.note}\n\nThe `foreach` expression is slower than the standard expression (it is always the case: `foreach` slows things down before this overhead gets offset by parallelization).\n\n:::\n\n### Plan sequential\n\nYou might wonder why the sequential evaluation strategy exists (i.e. why go through all the trouble of writing your code with `foreach` and `doFuture` to then run it without parallelism?).\n\nThere are many reasons:\n\n- It can be very useful for debugging.\n- It makes it easy to switch the futures execution strategy back and forth for different sections of the code (maybe you don't want to run everything in parallel).\n- It allows other people to run the same code on their different hardware without changing it (if they don't have the resources to run things in parallel, they only have to change the execution strategy).\n\nTo turn the code into a parallelizable version with [doFuture](https://cran.r-project.org/web/packages/doFuture/index.html), we replace `%do%` with `%dofuture%`.\n\nHere, we also need to use the option `.options.future = list(seed = TRUE)`: whenever your parallel code rely on a random process, it isn't enough to use `set.seed()` to ensure reproducibility, you also need to generate parallel-safe random numbers. In random forest, each tree is trained on a random subset of the data and random variables are selected for splitting at each node. The option `.options.future = list(seed = TRUE)` pregenerates the random seeds using L’Ecuyer-CMRG RNG streams[^1].\n\n[^1]: [L’Ecuyer, P. (1999). Good parameters and implementations for combined multiple recursive random number generators. Operations Research, 47, 159–164.](https://pubsonline.informs.org/doi/10.1287/opre.47.1.159)\n\nThis is the parallelizable `foreach` code, but run sequentially:\n\n```{.r filename=\"sequential.R\"}\nlibrary(doFuture)    # Also loads foreach and future\nlibrary(randomForest)\nlibrary(bench)\n\nregisterDoFuture()   # Set the parallel backend\nplan(sequential)     # Set the evaluation strategy\n\nbig_iris <- iris[rep(seq_len(nrow(iris)), each = 1e2), ]\nrownames(big_iris) <- NULL\n\ncat(\"\\nBenchmarking results:\\n\\n\")\n\nset.seed(123)\nmark(\n  rf <- foreach(\n    ntree = rep(250, 8),\n    .options.future = list(seed = TRUE),\n    .combine = combine\n  ) %dofuture%\n    randomForest(Species ~ ., data=big_iris, ntree=ntree)\n)\n```\n\n```{.bash filename=\"sequential.sh\"}\n#!/bin/bash\n#SBATCH --time=5\n#SBATCH --mem-per-cpu=7500M\n\nRscript sequential.R\n```\n\n```{.bash}\nsbatch sequential.sh\n```\n\n```\nLoading required package: foreach\nLoading required package: future\nrandomForest 4.7-1.1\nType rfNews() to see new features/changes/bug fixes.\n\nBenchmarking results:\n\n# A tibble: 1 × 13\n  expression      min median `itr/sec` mem_alloc `gc/sec` n_itr  n_gc total_time\n  <bch:expr>    <bch> <bch:>     <dbl> <bch:byt>    <dbl> <int> <dbl>   <bch:tm>\n1 rf <- foreac… 8.39s  8.39s     0.119        NA     3.81     1    32      8.39s\n# ℹ 4 more variables: result <list>, memory <list>, time <list>, gc <list>\nWarning message:\nSome expressions had a GC in every iteration; so filtering is disabled.\n```\n\n:::{.note}\n\nEach time we add unnecessary complexity in the code, things run a little slower.\n\n:::\n\n### Multi-processing in shared memory\n\nNow, it is time to parallelize. First, we will use multiple cores on a single node (shared-memory parallelism).\n\n#### Number of cores\n\nThe `future` package provides the `availableCores()` function to detect the number of available cores. We will run it as part of our script as a check on our available hardware.\n\nThe cluster for this course is made of 20 nodes with 4 CPUs each. We want to test shared memory parallelism, so our job needs to stay within one node. We can thus ask for a maximum of 4 CPUs and we want to ensure that we aren't getting them on different nodes. Let's go with that maximum of 4 cores.\n\n#### Multisession\n\nShared memory multi-processing can be run with `plan(multisession)` that will spawn new R sessions in the background to evaluate futures.\n\n```{.r filename=\"multisession.R\"}\nlibrary(doFuture)\nlibrary(randomForest)\nlibrary(bench)\n\n# Check number of cores:\ncat(\"\\nWe have\", availableCores(), \"cores.\\n\\n\")\n\nregisterDoFuture()   # Set the parallel backend\nplan(multisession)   # Set the evaluation strategy\n\nbig_iris <- iris[rep(seq_len(nrow(iris)), each = 1e2), ]\nrownames(big_iris) <- NULL\n\ncat(\"\\nBenchmarking results:\\n\\n\")\n\nset.seed(123)\nmark(\n  rf <- foreach(\n    ntree = rep(250, 8),\n    .options.future = list(seed = TRUE),\n    .combine = combine\n  ) %dofuture%\n    randomForest(Species ~ ., data=big_iris, ntree=ntree)\n)\n```\n\n```{.bash filename=\"multisession.sh\"}\n#!/bin/bash\n#SBATCH --time=5\n#SBATCH --mem-per-cpu=7500M\n#SBATCH --cpus-per-task=4\n\nRscript multisession.R\n```\n\n```{.bash}\nsbatch multisession.sh\n```\n\n```\nLoading required package: foreach\nLoading required package: future\nrandomForest 4.7-1.1\nType rfNews() to see new features/changes/bug fixes.\n\nWe have 4 cores.\n\nBenchmarking results:\n\n# A tibble: 1 × 13\n  expression      min median `itr/sec` mem_alloc `gc/sec` n_itr  n_gc total_time\n  <bch:expr>    <bch> <bch:>     <dbl> <bch:byt>    <dbl> <int> <dbl>   <bch:tm>\n1 rf <- foreac… 2.72s  2.72s     0.368        NA     2.21     1     6      2.72s\n# ℹ 4 more variables: result <list>, memory <list>, time <list>, gc <list>\nWarning message:\nSome expressions had a GC in every iteration; so filtering is disabled.\n```\n\nSpeedup: 3.1.\n\n:::{.note}\n\nNot too bad, considering that the ideal speedup, without any overhead, would be 4.\n\n:::\n\n:::{.note}\n\n```{.bash}\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=4\n```\n\ncould be used instead of:\n\n```{.bash}\n#SBATCH --cpus-per-task=4\n```\n\nWhat matters is to have 4 cores running on the same node to be in a shared memory parallelism scenario.\n\n:::\n\n#### Multicore\n\nShared memory multi-processing can also be run with `plan(multicore)` (except on Windows) that will fork the current R process to evaluate futures.\n\n```{.r filename=\"multicore.R\"}\nlibrary(doFuture)\nlibrary(randomForest)\nlibrary(bench)\n\n# Check number of cores:\ncat(\"\\nWe have\", availableCores(), \"cores.\\n\\n\")\n\nregisterDoFuture()   # Set the parallel backend\nplan(multicore)      # Set the evaluation strategy\n\nbig_iris <- iris[rep(seq_len(nrow(iris)), each = 1e2), ]\nrownames(big_iris) <- NULL\n\ncat(\"\\nBenchmarking results:\\n\\n\")\n\nset.seed(123)\nmark(\n  rf <- foreach(\n    ntree = rep(250, 8),\n    .options.future = list(seed = TRUE),\n    .combine = combine\n  ) %dofuture%\n    randomForest(Species ~ ., data=big_iris, ntree=ntree)\n)\n```\n\n```{.bash filename=\"multicore.sh\"}\n#!/bin/bash\n#SBATCH --time=5\n#SBATCH --mem-per-cpu=7500M\n#SBATCH --cpus-per-task=4\n\nRscript multicore.R\n```\n\n```{.bash}\nsbatch multicore.sh\n```\n\n```\nLoading required package: foreach\nLoading required package: future\nrandomForest 4.7-1.1\nType rfNews() to see new features/changes/bug fixes.\n\nWe have 4 cores.\n\nBenchmarking results:\n\n# A tibble: 1 × 13\n  expression      min median `itr/sec` mem_alloc `gc/sec` n_itr  n_gc total_time\n  <bch:expr>    <bch> <bch:>     <dbl> <bch:byt>    <dbl> <int> <dbl>   <bch:tm>\n1 rf <- foreac… 3.15s  3.15s     0.318        NA     13.7     1    43      3.15s\n# ℹ 4 more variables: result <list>, memory <list>, time <list>, gc <list>\nWarning message:\nSome expressions had a GC in every iteration; so filtering is disabled.\n```\n\nSpeedup: 2.7.\n\n:::{.note}\n\nWhile in theory we should get a similar speedup, we are getting a lower one here.\n\n:::\n\n### Multi-processing in distributed memory\n\nLet's run our distributed parallel code using 8 cores across 2 nodes.\n\nWe need to create a cluster of workers. We do this by creating a character vector with the names of the nodes our tasks are running on and passing it to the `makeCluster()` function from the `parallel` package (included in R):\n\n```{.r}\n# Create a character vector with the nodes names\nhosts <- system(\"srun hostname -s\", intern = T)\n\n# Create the cluster of workers\ncl <- parallel::makeCluster(hosts)\n```\n\nWe can verify that we did get 8 tasks by accessing the `SLURM_NTASKS` environment variable from within R:\n\n```{.r}\nas.numeric(Sys.getenv(\"SLURM_NTASKS\"))\n```\n\nHere is the R script:\n\n```{.r filename=\"distributed.R\"}\nlibrary(doFuture)\nlibrary(randomForest)\nlibrary(bench)\n\n# Check number of tasks:\ncat(\"\\nWe have\", as.numeric(Sys.getenv(\"SLURM_NTASKS\")), \"tasks.\\n\\n\")\n\n# Create a character vector with the nodes names\nhosts <- system(\"srun hostname -s\", intern = T)\n\n# Look at the location of our tasks:\ncat(\"\\nOur tasks are running on the following nodes: \", hosts)\n\n# Create the cluster of workers\ncl <- parallel::makeCluster(hosts)\n\nregisterDoFuture()           # Set the parallel backend\nplan(cluster, workers = cl)  # Set the evaluation strategy\n\nbig_iris <- iris[rep(seq_len(nrow(iris)), each = 1e2), ]\nrownames(big_iris) <- NULL\n\ncat(\"\\nBenchmarking results:\\n\\n\")\n\nset.seed(123)\nmark(\n  rf <- foreach(\n    ntree = rep(250, 8),\n    .options.future = list(seed = TRUE),\n    .combine = combine\n  ) %dofuture%\n    randomForest(Species ~ ., data=big_iris, ntree=ntree)\n)\n```\n\n:::{.note}\n\nThe cluster of workers can be stopped with:\n\n```{.r}\nparallel::stopCluster(cl)\n```\n\nHere, this is not necessary since our job stops running as soon as the execution is complete, but in other systems, this will prevent you from monopolizing hardware or paying unnecessarily.\n\n:::\n\nAnd now we need to ask [Slurm](https://en.wikipedia.org/wiki/Slurm_Workload_Manager) for 8 tasks on 2 nodes:\n\n```{.bash filename=\"distributed.sh\"}\n#!/bin/bash\n#SBATCH --time=5\n#SBATCH --mem-per-cpu=7500M\n#SBATCH --ntasks-per-node=4\n#SBATCH --nodes=2\n\nRscript distributed.R\n```\n\n```{.bash}\nsbatch distributed.sh\n```\n\n```\nLoading required package: foreach\nLoading required package: future\nrandomForest 4.7-1.1\nType rfNews() to see new features/changes/bug fixes.\n\nWe have 8 tasks.\n\nOur tasks are running on the following nodes: \"node1\" \"node1\" \"node1\" \"node1\" \"node2\" \"node2\" \"node2\" \"node2\"\n\nBenchmarking results:\n\n# A tibble: 1 × 13\n  expression      min median `itr/sec` mem_alloc `gc/sec` n_itr  n_gc total_time\n  <bch:expr>    <bch> <bch:>     <dbl> <bch:byt>    <dbl> <int> <dbl>   <bch:tm>\n1 rf <- foreac…  1.6s   1.6s     0.624        NA     3.12     1     5       1.6s\n# ℹ 4 more variables: result <list>, memory <list>, time <list>, gc <list>\nWarning message:\nSome expressions had a GC in every iteration; so filtering is disabled.\n```\n\nSpeedup: 5.2.\n\n:::{.note}\n\nThe overhead is larger in distributed parallelism due to message passing between nodes. We are further from the ideal speedup of 8, but we still got a speedup larger than what we could have obtained with shared-memory parallelism.\n\n:::\n\n:::{.note}\n\n```{.bash}\n#SBATCH --ntasks=8\n```\n\ncould be used instead of:\n\n```{.bash}\n#SBATCH --ntasks-per-node=4\n#SBATCH --nodes=2\n```\n\nHowever the latter is slightly better because it allows us to use 2 full nodes instead of having tasks running on any number of nodes. However, it also means that we might have to wait longer for our job to run as it is more restrictive.\n\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}