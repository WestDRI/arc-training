{
  "hash": "ae6728f3e360c6e9613215e319077561",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Web scraping with R\naliases:\n  - webscraping.html\nauthor: Marie-Hélène Burle\nexecute:\n  cache: false\n---\n\n\n\n\n:::{.def}\n\nThe internet is a trove of information. A lot of it is publicly available and thus suitable for use in research. Extracting that information and putting it in an organized format for analysis can, however, be extremely tedious. Web scraping tools allow to automate parts of that process and R is a popular language for the task.\n\nIn this workshop, we will guide you through a simple example using the package [rvest](https://rvest.tidyverse.org/).\n\n:::\n\n:::{.callout-note collapse=\"true\"}\n\n## Running R\n\nFor this workshop, we will use a temporary RStudio server.\n\nTo access it, go to the website given during the workshop and sign in using the username and password you will be given (you can ignore the OTP entry).\n\nThis will take you to our JupyterHub. There, click on the \"RStudio\" button and our RStudio server will open in a new tab.\n\n:::{.note}\n\nOur RStudio server already has the two packages that we will be using installed ([rvest](https://cran.r-project.org/web/packages/rvest/index.html) and [tibble](https://cran.r-project.org/web/packages/tibble/index.html)). If you want to run the code on your machine, you need to install them with `install.packages()` first.\n\n:::\n\n:::\n\n## HTML and CSS\n\n[HyperText Markup Language](https://en.wikipedia.org/wiki/HTML) (HTML) is the standard markup language for websites: it encodes the information related to the formatting and structure of webpages. Additionally, some of the customization can be stored in [Cascading Style Sheets](https://en.wikipedia.org/wiki/CSS) (CSS) files.\n\nHTML uses tags of the form:\n\n```{.html}\n<some_tag>Your content</some_tag>\n```\n\nSome tags have attributes:\n\n```{.html}\n<some_tag attribute_name=\"attribute value\">Your content</some_tag>\n```\n\n:::{.example}\n\nExamples:\n\n:::\n\nSite structure:\n\n- `<h2>This is a heading of level 2</h2>`\n- `<p>This is a paragraph</p>`\n\nFormatting:\n\n- `<b>This is bold</b>`\n- `<a href=\"https://some.url\">This is the text for a link</a>`\n\n## Web scrapping\n\nWeb scraping is a general term for a set of tools which allow for the extraction of data from the web automatically.\n\nWhile most of the data on the internet is publicly available, it is illegal to scrape some sites and you should always look into the policy of a site before attempting to scrape it. Some sites will also block you if you submit too many requests in a short amount of time, so if you plan on scraping sites at a fairly large scale, you should look into the [polite](https://dmi3kno.github.io/polite/) package which will help you scrape responsibly.\n\n## Example for this workshop\n\nWe will use [a website](https://trace.tennessee.edu/utk_graddiss/index.html) from the [University of Tennessee](https://www.utk.edu/) containing a database of PhD theses from that university.\n\nOur goal is to scrape data from this site to produce a dataframe with the date, major, and advisor for each dissertation.\n\n:::{.note}\n\nWe will only do this for the first page which contains the links to the 100 most recent theses. If you really wanted to gather all the data, you would have to do this for all pages.\n\n:::\n\n## Let's look at the sites\n\nFirst of all, let's have a close look at the websites we want to scrape to think carefully about what we want to do. Before starting to write code, it is always a good idea to think about what you are trying to achieve with your code.\n\nTo create a dataframe with the data for all the dissertations on that first page, we need to do two things:\n\n- Step 1: from the [dissertations database first page](https://trace.tennessee.edu/utk_graddiss/index.html), we want to scrape the list of URLs for the dissertation pages.\n\n- Step 2: once we have the URLs, we want to scrape those pages too to get the date, major, and advisor for each dissertation.\n\n## Package\n\nTo do all this, we will use the package [rvest](https://cran.r-project.org/web/packages/rvest/index.html), part of the [tidyverse](https://www.tidyverse.org/) (a modern set of R packages). It is a package influenced by the popular Python package [Beautiful Soup](https://en.wikipedia.org/wiki/Beautiful_Soup_(HTML_parser)) and it makes scraping websites with R really easy.\n\nLet's load it:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(rvest)\n```\n:::\n\n\n\n\n## Read in HTML from main site\n\nAs mentioned above, our site is the [database of PhD dissertations from the University of Tennessee](https://trace.tennessee.edu/utk_graddiss/index.html).\n\nLet's create a character vector with the URL:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nurl <- \"https://trace.tennessee.edu/utk_graddiss/index.html\"\n```\n:::\n\n\n\n\nFirst, we read in the html data from that page:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhtml <- read_html(url)\n```\n:::\n\n\n\n\nLet's have a look at the raw data:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhtml\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n{html_document}\n<html lang=\"en\">\n[1] <head>\\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8 ...\n[2] <body>\\n<!-- FILE /srv/sequoia/main/data/trace.tennessee.edu/assets/heade ...\n```\n\n\n:::\n:::\n\n\n\n\n## Test run\n\n### Identify the relevant HTML markers\n\nThe html code for this webpage contains the data we are interested in, but it is mixed in with a lot of HTML formatting and data we don't care about. We need to extract the data relevant to us and turn it into a workable format.\n\nThe first step is to find the HTML markers that contain our data. One option is to use a web inspector or—even easier—the [SelectorGadget](https://selectorgadget.com/), a JavaScript bookmarklet built by [Andrew Cantino](https://andrewcantino.com/).\n\nTo use this tool, go to the [SelectorGadget](https://selectorgadget.com/) website and drag the link of the bookmarklet to your bookmarks bar.\n\nNow, go to the [dissertations database first page](https://trace.tennessee.edu/utk_graddiss/index.html) and click on the bookmarklet in your bookmarks bar. You will see a floating box at the bottom of your screen. As you move your mouse across the screen, an orange rectangle appears around each element over which you pass.\n\nClick on one of the dissertation links: now, there is an `a` appearing in the box at the bottom as well as the number of elements selected. The selected elements are highlighted in yellow. Those elements are links (in HTML, `a` tags define hyperlinks).\n\nAs you can see, all the links we want are selected. However, there are many other links we don't want that are also highlighted. In fact, *all* links in the document are selected. We need to remove the categories of links that we don't want. To do this, hover above any of the links we don't want. You will see a red rectangle around it. Click on it: now all similar links are gone. You might have to do this a few times until only the relevant links (i.e. those that lead to the dissertation information pages) remain highlighted.\n\nAs there are 100 such links per page, the count of selected elements in the bottom floating box should be down to 100.\n\nIn the main section of the floating box, you can now see: `.article-listing a`. This means that the data we want are under the HTML elements `.article-listing a` (the class `.article-listing` and the tag `a`).\n\n### Extract test URL\n\nIt is a good idea to test things out on a single element before doing a massive batch scraping of a site, so let's test our method for the first dissertation.\n\nTo start, we need to extract the first URL. The function `html_element()` from the package `rvest` extracts the first element matching some character. Let's pass to this function our `html` object and the character `\".article-listing a\"` and assign the result to an object that we will call `test`:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest <- html %>% html_element(\".article-listing a\")\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError in dyn.load(file, DLLpath = DLLpath, ...): unable to load shared object '/home/marie/R/lib/stringi/libs/stringi.so':\n  libicui18n.so.74: cannot open shared object file: No such file or directory\n```\n\n\n:::\n:::\n\n\n\n\n:::{.note}\n\n`%>%` is a pipe from the [magrittr](https://magrittr.tidyverse.org/) tidyverse package. It passes the output from the left-hand side expression as the first argument of the right-hand side expression. We could have written this as:\n\n```{.r}\ntest <- html_element(html, \".article-listing a\")\n```\n\n:::\n\nOur new object is a list:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntypeof(test)\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError in eval(expr, envir, enclos): object 'test' not found\n```\n\n\n:::\n:::\n\n\n\n\nLet's print it:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError in eval(expr, envir, enclos): object 'test' not found\n```\n\n\n:::\n:::\n\n\n\n\nThe URL is in there, so we successfully extracted the correct element, but we need to do more cleaning.\n\n`a` is one of the HTML tags that have an attribute (`href`) as you can see when you print `test`. It is actually the value of that attribute that we want. To extract an attribute value, we use the function `html_attr()`:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nurl_test <- test %>% html_attr(\"href\")\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError in eval(expr, envir, enclos): object 'test' not found\n```\n\n\n:::\n\n```{.r .cell-code}\nurl_test\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError in eval(expr, envir, enclos): object 'url_test' not found\n```\n\n\n:::\n:::\n\n\n\n\nThis is our URL.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstr(url_test)\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError in eval(expr, envir, enclos): object 'url_test' not found\n```\n\n\n:::\n:::\n\n\n\n\nIt is saved in a character vector, which is perfect.\n\n:::{.note}\n\nInstead of creating the intermediate objects `html` and `test`, we could have chained the functions:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nurl_test <- read_html(url) %>%\n  html_element(\".article-listing a\") %>%\n  html_attr(\"href\")\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError in dyn.load(file, DLLpath = DLLpath, ...): unable to load shared object '/home/marie/R/lib/stringi/libs/stringi.so':\n  libicui18n.so.74: cannot open shared object file: No such file or directory\n```\n\n\n:::\n:::\n\n\n\n\n:::\n\n### Read in HTML data for test URL\n\nNow that we have the URL for the first dissertation information page, we want to extract the date, major, and advisor for that dissertation.\n\nWe just saw that `url_test` is a character vector representing a URL. We know how to deal with this.\n\nThe first thing to do—as we did earlier with the database site—is to read in the html data. Let's assign it to a new object that we will call `html_test`:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhtml_test <- read_html(url_test)\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError in eval(expr, envir, enclos): object 'url_test' not found\n```\n\n\n:::\n\n```{.r .cell-code}\nhtml_test\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError in eval(expr, envir, enclos): object 'html_test' not found\n```\n\n\n:::\n:::\n\n\n\n\n### Get data for test URL\n\nNow, we want to extract the publication date. Thanks to the [SelectorGadget](https://selectorgadget.com/), following [the method we saw earlier](#identify-the-relevant-html-markers), we can see that we now need the element marked by `#publication_date p`.\n\nWe start by extracting the data as we did earlier by passing our object `html_test` and the character `\"#publication_date p\"` to `html_element()`.\n\nWhile earlier we wanted the value of a tag attribute (i.e. part of the metadata), here we want the actual text (i.e. part of the actual content). To extract text from a snippet of HTML, we pass it to `html_text2()`.\n\nLet's run both operations at once to save the creation of an intermediate object:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndate_test <- html_test %>%\n  html_element(\"#publication_date p\") %>%\n  html_text2()\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError in eval(expr, envir, enclos): object 'html_test' not found\n```\n\n\n:::\n:::\n\n\n\n\n:::{.note}\n\nNote the difference with what we did earlier to extract the URL: if we had used `html_text2()` then we would have gotten the text part of the link (`\"The Novel Chlorination of Zirconium Metal and Its Application to a Recycling Protocol for Zircaloy Cladding from Spent Nuclear Fuel Rods\"`) rather than the URL (`\"https://trace.tennessee.edu/utk_graddiss/7600\"`).\n\n:::\n\nLet's verify that our `date` object indeed contains the date:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndate_test\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError in eval(expr, envir, enclos): object 'date_test' not found\n```\n\n\n:::\n:::\n\n\n\n\nWe also want the major for this thesis. The [SelectorGadget](https://selectorgadget.com/) allows us to find that this time, it is the `#department p` element that we need. Let's extract it in the same fashion:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmajor_test <- html_test %>%\n  html_element(\"#department p\") %>%\n  html_text2()\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError in eval(expr, envir, enclos): object 'html_test' not found\n```\n\n\n:::\n\n```{.r .cell-code}\nmajor_test\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError in eval(expr, envir, enclos): object 'major_test' not found\n```\n\n\n:::\n:::\n\n\n\n\nAnd for the advisor, we need the `#advisor1 p` element:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nadvisor_test <- html_test %>%\n  html_element(\"#advisor1 p\") %>%\n  html_text2()\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError in eval(expr, envir, enclos): object 'html_test' not found\n```\n\n\n:::\n\n```{.r .cell-code}\nadvisor_test\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError in eval(expr, envir, enclos): object 'advisor_test' not found\n```\n\n\n:::\n:::\n\n\n\n\n:::{.exo}\n\n:::{.yourturn}\n\nYour turn:\n\n:::\n\nTry using the [SelectorGadget](https://selectorgadget.com/) to identify the element necessary to extract the abstract of this dissertation.\n\nNow, write the code to extract it and make sure you actually get what you want.\n\n:::\n\nWe now have the date, major, and advisor for the first dissertation. We can create a matrix by passing them as arguments to `cbind()`:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresult_test <- cbind(date_test, major_test, advisor_test)\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError in eval(expr, envir, enclos): object 'date_test' not found\n```\n\n\n:::\n\n```{.r .cell-code}\nresult_test\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError in eval(expr, envir, enclos): object 'result_test' not found\n```\n\n\n:::\n:::\n\n\n\n\n## Full run\n\n### Extract all URLs\n\nNow that we have tested our code on the first dissertation, we can apply it on all 100 dissertations of the first page of the database.\n\nInstead of using `html_element()`, this time we will use `html_elements()` which extracts *all* matching elements (instead of just the first one):\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat <- html %>% html_elements(\".article-listing a\")\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError in dyn.load(file, DLLpath = DLLpath, ...): unable to load shared object '/home/marie/R/lib/stringi/libs/stringi.so':\n  libicui18n.so.74: cannot open shared object file: No such file or directory\n```\n\n\n:::\n\n```{.r .cell-code}\ndat\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError in eval(expr, envir, enclos): object 'dat' not found\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntypeof(dat)\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError in eval(expr, envir, enclos): object 'dat' not found\n```\n\n\n:::\n\n```{.r .cell-code}\nlength(dat)\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError in eval(expr, envir, enclos): object 'dat' not found\n```\n\n\n:::\n\n```{.r .cell-code}\ntypeof(dat[[1]])\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError in eval(expr, envir, enclos): object 'dat' not found\n```\n\n\n:::\n:::\n\n\n\n\nWe now have a list of lists.\n\nAs we did for a single URL in the test run, we now want to extract all the URLs. We will do this using a loop.\n\nBefore running for loops, it is important to initialize empty loops. It is much more efficient than growing the result at each iteration.\n\nSo let's initialize an empty list that we call `list_urls` of the appropriate size:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlist_urls <- vector(\"list\", length(dat))\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError in eval(expr, envir, enclos): object 'dat' not found\n```\n\n\n:::\n:::\n\n\n\n\nNow we can run a loop to fill in our list:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfor (i in seq_along(dat)) {\n  list_urls[[i]] <- dat[[i]] %>% html_attr(\"href\")\n}\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError in eval(expr, envir, enclos): object 'dat' not found\n```\n\n\n:::\n:::\n\n\n\n\nLet's print again the first element of `list_urls` to make sure all looks good:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlist_urls[[1]]\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError in eval(expr, envir, enclos): object 'list_urls' not found\n```\n\n\n:::\n:::\n\n\n\n\nWe now have a list of URLs (in the form of character vectors) as we wanted.\n\n### Extract data from each page\n\nWe will now extract the data (date, major, and advisor) for all URLs in our list.\n\nAgain, before running a for loop, we need to allocate memory first by creating an empty container (here a list):\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlist_data <- vector(\"list\", length(list_urls))\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError in eval(expr, envir, enclos): object 'list_urls' not found\n```\n\n\n:::\n:::\n\n\n\n\nWe move the code we tested for a single URL inside a loop and we add one result to the `list_data` list at each iteration until we have all 100 dissertation sites scraped. Because there are quite a few of us running the code at the same time, we don't want the site to block our request. To play safe, we will add a little delay (0.1 second) at each iteration (many sites will block requests if they are too frequent):\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfor (i in seq_along(list_urls)) {\n  html <- read_html(list_urls[[i]])\n  date <- html %>%\n    html_element(\"#publication_date p\") %>%\n    html_text2()\n  major <- html %>%\n    html_element(\"#department p\") %>%\n    html_text2()\n  advisor <- html %>%\n    html_element(\"#advisor1 p\") %>%\n    html_text2()\n  Sys.sleep(0.1)  # Add a little delay\n  list_data[[i]] <- cbind(date, major, advisor)\n}\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError in eval(expr, envir, enclos): object 'list_urls' not found\n```\n\n\n:::\n:::\n\n\n\n\nLet's make sure all looks good by printing the first element of `list_data`:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlist_data[[1]]\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError in eval(expr, envir, enclos): object 'list_data' not found\n```\n\n\n:::\n:::\n\n\n\n\n## Store results in DataFrame\n\nWe can turn this big list into a dataframe:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresult <- do.call(rbind.data.frame, list_data)\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError in eval(expr, envir, enclos): object 'list_data' not found\n```\n\n\n:::\n:::\n\n\n\n\n`result` is a long dataframe, so we will only print the first few elements:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(result)\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError in eval(expr, envir, enclos): object 'result' not found\n```\n\n\n:::\n:::\n\n\n\n\nIf you like the tidyverse, you can turn it into a tibble:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresult <- result %>% tibble::as_tibble()\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError in eval(expr, envir, enclos): object 'result' not found\n```\n\n\n:::\n:::\n\n\n\n\n:::{.note}\n\nThe notation `tibble::as_tibble()` means that we are using the function `as_tibble()` from the package [tibble](https://tibble.tidyverse.org/). A tibble is the [tidyverse](https://www.tidyverse.org/) version of a dataframe. One advantage is that it will only print the first 10 rows by default instead of printing the whole dataframe, so you don't have to use `head()` when printing long dataframes:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresult\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError in eval(expr, envir, enclos): object 'result' not found\n```\n\n\n:::\n:::\n\n\n\n\n:::\n\nWe can capitalize the headers:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnames(result) <- c(\"Date\", \"Major\", \"Advisor\")\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError: object 'result' not found\n```\n\n\n:::\n:::\n\n\n\n\nThis is what our final result looks like:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresult\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError in eval(expr, envir, enclos): object 'result' not found\n```\n\n\n:::\n:::\n\n\n\n\n## Save results to file\n\nAs a final step, we will save our data to a CSV file:\n\n```{.r}\nwrite.csv(result, \"dissertations_data.csv\", row.names = FALSE)\n```\n\n## Functions recap\n\nBelow is a recapitulation of the `rvest` functions we have used today:\n\n| Functions | Usage |\n|-----------|-------|\n| `read_html()` | Read in HTML from URL |\n| `html_element()` | Extract first matching element |\n| `html_elements()` | Extract all matching elements |\n| `html_attr()` | Extract the value of an attribute |\n| `html_text2()` | Extract text |\n\n## Recording\n\n:::{.example}\n\nVideo of this workshop for [the Digital Research Alliance of Canada HSS Winter Series 2023](https://hss23.netlify.app/):\n\n:::\n\n\n\n\n{{< video https://www.youtube.com/embed/CX67yt_VbUI >}}\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}