{
  "hash": "bc55cd8ebed88a7a3b256b46cfa0325c",
  "result": {
    "markdown": "---\ntitle: Parallel loops with foreach & doFuture\nauthor: Marie-Hélène Burle\n---\n\n\n## Parallel programming\n\n### Multi-threading\n\nWe talk about **multi-threading** when a single process (with its own memory) runs multiple threads.\n\nThe execution can happen in parallel—if each thread has access to a CPU core—or by alternating some of the threads on some CPU cores.\n\nBecause all threads in a process write to the same memory addresses, multi-threading can lead to [race conditions](https://en.wikipedia.org/wiki/Race_condition).\n\nMulti-threading does not seem to be a common approach to parallelizing R code.\n\n### Multi-processing in shared memory\n\n**Multi-processing in shared memory** happens when multiple processes execute code on multiple CPU cores of a single node (or a single machine).\n\nThe different processes need to communicate with each other, but because they are all running on the CPU cores of a single node, messages can pass via shared memory.\n\n### Multi-processing in distributed memory\n\nWhen processes involved in the execution of some code run on multiple nodes of a cluster, messages between them need to travel over the cluster interconnect. In that case, we talk about **distributed memory**.\n\n## Running R code in parallel\n\n### Package parallel (base R)\n\nThe `parallel` package has been part of the \"base\" package group since version 2.14.0. \\\nThis means that it is comes with R.\n\nMost parallel approaches in R build on this package.\n\nWe will make use of it to create and close an ad-hoc cluster.\n\n:::{.note}\n\nThe [parallelly](https://parallelly.futureverse.org/) package adds functionality to the `parallel` package.\n\n:::\n\n### Package foreach\n\nThe [foreach](https://cran.r-project.org/web/packages/foreach/index.html) package implements a looping construct without an explicit counter. It doesn't require the preallocation of an output container, it brings to R an equivalent of the Python or Julia list comprehensions, and mostly, it allows for an easy execution of loops in parallel. Unlike loops, it creates variables (loops are used for their side-effect).\n\nLet's look at an example to calculate the sum of 1e4 random vectors of length 3.\n\nWe will use `foreach` and `iterators` (which creates convenient iterators for `foreach`):\n\n\n::: {.cell hash='parallel_loops_cache/html/unnamed-chunk-1_87472469db7dff122cd21fca90187633'}\n\n```{.r .cell-code}\nlibrary(foreach)\nlibrary(iterators)\n```\n:::\n\n\n- Classic while loop:\n\n\n::: {.cell hash='parallel_loops_cache/html/unnamed-chunk-2_14b53e147009c4417020c8082dd7f6d7'}\n\n```{.r .cell-code}\nset.seed(2)\n\nresult1 <- numeric(3)  # First we need to preallocate an output container\ni <- 0                 # Then we need to initialise a counter variable\n\nwhile(i < 1e4) {                 # Finally we run the loop\n  result1 <- result1 + runif(3)  # calculate the sum\n  i <- i + 1                     # update the counter\n}\n```\n:::\n\n\n- With foreach:\n\n\n::: {.cell hash='parallel_loops_cache/html/unnamed-chunk-3_dbe7f4c62975af9ce904f2a9d0204430'}\n\n```{.r .cell-code}\nset.seed(2)\n\nresult2 <- foreach(icount(1e4), .combine = '+') %do% runif(3)\n```\n:::\n\n\nWe can verify that both expressions returned the same result:\n\n\n::: {.cell hash='parallel_loops_cache/html/unnamed-chunk-4_3ad0e9ee2006f5f5fa000bd779e23969'}\n\n```{.r .cell-code}\nall.equal(result1, result2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] TRUE\n```\n:::\n:::\n\n\nThe best part of `foreach` is that it makes for easy parallelization of loops if you replace `%do%` with `%dopar%`.\n\nFor this reason, many parallelization backends use `foreach`: `doFuture`, `doMC`, `doMPI`, `doFuture`, `doParallel`, `doRedis`, `doRNG`, `doSNOW`, and `doAzureParallel`.\n\nIn this webinar, I will use [doFuture](https://cran.r-project.org/web/packages/doFuture/index.html) which makes `foreach::%dopar%` work on any type of future thanks to the [future](https://cran.r-project.org/web/packages/future/index.html) package.\n\nSo first, what is the `future` package?\n\n### Package future\n\nA [future](https://en.wikipedia.org/wiki/Futures_and_promises) is an object that acts as an abstract representation for a value in the future. A future can be *resolved* (if the value has been computed) or *unresolved*. If the value is queried while the future is unresolved, the process is blocked until the future is resolved.\n\nFutures allow for asynchronous and parallel evaluations. The `future` package provides a simple and unified API to evaluate futures.\n\n### Plans\n\nThe `future` package does this thanks to the `plan` function:\n\n- `plan(sequential)`: futures are evaluated sequentially in the current R session\n- `plan(multisession)`: futures are evaluated by new R sessions spawned in the background (*multi-processing in shared memory*)\n- `plan(multicore)`: futures are evaluated in processes forked from the existing process (*multi-processing in shared memory*)\n- `plan(cluster)`: futures are evaluated on an ad-hoc cluster (allows for *distributed parallelism* across multiple nodes)\n\n### Consistency\n\nTo ensure a consistent behaviour across plans, all evaluations are done in a local environment:\n\n\n::: {.cell hash='parallel_loops_cache/html/unnamed-chunk-5_f81f0a2a227703a875980bb3e3476d14'}\n\n```{.r .cell-code}\nlibrary(future)\n\na <- 1\n\nb %<-% {\n  a <- 2\n}\n\na\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1\n```\n:::\n:::\n\n\n### Let's return to our example\n\nWe had:\n\n```{.r}\nset.seed(2)\nresult2 <- foreach(icount(1e4), .combine = '+') %do% runif(3)\n```\n\nWe can replace `%do%` with `%dopar%`:\n\n```{.r}\nset.seed(2)\nresult3 <- foreach(icount(1e4), .combine = '+') %dopar% runif(3)\n```\n\nBecause we haven't set any parallel backend to `foreach`, this won't make any difference. To run this in parallel using `future` and `doFuture`, we first need to load `doFuture` and set a parallel plan (e.g. `plan(multicore)`).\n\nBecause of the overhead of parallelization, it wouldn't make sense to to parallelize such a short code. Let's go over a better toy example and time it.\n\n## Toy example\n\n### Load packages\n\nI will use the [randomForest](https://cran.r-project.org/web/packages/randomForest/index.html) package to create a classification algorithm based on a forest of decision trees.\n\nBecause the code includes randomly generated numbers, I will also use the [doRNG](https://cran.r-project.org/web/packages/doRNG/index.html) package which replaces `%dopar%` with `%dorng%` to follow the recommendations of [L'Ecuyer, P. (1999)](https://pubsonline.informs.org/doi/10.1287/opre.47.1.159)[^1] and ensure reproducibility.\n\n[^1]: L'Ecuyer, P. (1999). Good parameters and implementations for combined multiple recursive random number generators. Operations Research, 47, 159–164.\n\n```{.r}\nlibrary(doFuture)\nlibrary(doRNG)\nlibrary(randomForest)\nlibrary(bench)\n```\n\n```\nLoading required package: foreach\nLoading required package: future\nLoading required package: rngtools\n```\n\n### The code to parallelize\n\nThe goal is to create a classifier based on some data (here a matrix of random numbers for simplicity) and a response variable (as factor). This model could then be passed in the `predict()` function with novel data to generate predictions of classification. But here we are only interested in the creation of the model as this is the part that is computationally intensive. We aren't interested in actually using it.\n\n```{.r}\nset.seed(11)\n\ntraindata <- matrix(runif(1e5), 100)\nfac <- gl(2, 50)\n\nrf <- foreach(ntree = rep(250, 8), .combine = combine) %do%\n  randomForest(x = traindata, y = fac, ntree = ntree)\n\nrf\n```\n\n```\nCall:\n randomForest(x = traindata, y = fac, ntree = ntree)\n               Type of random forest: classification\n                     Number of trees: 2000\nNo. of variables tried at each split: 31\n```\n\n### Reference timing\n\nThis is the non parallelizable code with `%do%`:\n\n```{.r}\ntref <- mark(\n  rf1 <- foreach(ntree = rep(250, 8), .combine = combine) %do%\n    randomForest(x = traindata, y = fac, ntree = ntree),\n  memory=F\n)\n\ntref$median\n```\n\n```\n[1] 5.66s\n```\n\n### Plan sequential\n\nThis is the parallelizable `foreach` code, but run sequentially:\n\n```{.r}\nregisterDoFuture()\n\nplan(sequential)\n\n# Using bench::mark()\ntseq <- mark(\n  rf2 <- foreach(ntree = rep(250, 8), .combine = combine) %dorng%\n    randomForest(x = traindata, y = fac, ntree = ntree),\n  memory=F\n)\n\ntseq$median\n```\n\n```\n[1] 5.78s\n```\n\n:::{.note}\n\nNo surprise: those are similar.\n\n:::\n\n### Multi-processing in shared memory\n\n`future` provides `availableCores()` to detect the number of available cores:\n\n```{.r}\navailableCores()\n```\n\n```\nsystem\n     4\n```\n\n:::{.note}\n\nThis is the same as `parallel::detectCores()`.\n\n:::\n\nThis detects the number of CPU cores available to me on the current compute node, that is, what I can use for shared memory multi-processing.\n\n### Plan multisession\n\nShared memory multi-processing can be run with `plan(multisession)` that will spawn new R sessions in the background to evaluate futures:\n\n```{.r}\nplan(multisession)\n\ntms <- mark(\n  rf2 <- foreach(ntree = rep(250, 8), .combine = combine) %dorng%\n    randomForest(x = traindata, y = fac, ntree = ntree),\n  memory=F\n)\n\ntms$median\n```\n\n```\n[1] 2s\n```\n\n:::{.note}\n\nWe got a speedup of `5.78 / 2 = 2.9`. Not bad considering that we have 4 CPU cores (the ideal speedup would be 4, but there is always some overhead to parallelization).\n\n:::\n\n### Plan multicore\n\nShared memory multi-processing can also be run with `plan(multicore)` (except on Windows) that will fork the current R process to evaluate futures:\n\n```{.r}\nplan(multicore)\n\ntmc <- mark(\n  rf2 <- foreach(ntree = rep(250, 8), .combine = combine) %dorng%\n    randomForest(x = traindata, y = fac, ntree = ntree),\n  memory=F\n)\n\ntmc$median\n```\n\n```\n[1] 1.9s\n```\n\n:::{.note}\n\nWe got a very similar speedup of `5.78 / 1.9 = 3.0`.\n\n:::\n\n### Multi-processing in distributed memory\n\nI did request 8 tasks however. But because the training cluster I built for this webinar only has nodes of the `c4-30gb` flavour, those tasks use 2 nodes.\n\nLet's verify that I did get 8 tasks:\n\n```{.r}\nas.numeric(Sys.getenv(\"SLURM_NTASKS\"))\n```\n\n```\n[1] 8\n```\n\nI can create a character vector with the name of the node each worker is on:\n\n```{.r}\n(hosts <- system(\"srun hostname | cut -f 1 -d '.'\", intern = T))\n```\n\n```\nchr [1:8] \"node1\" \"node1\" \"node1\" \"node1\" \"node2\" \"node2\" \"node2\" \"node2\"\n```\n\nThis allows me to create the cluster of workers:\n\n```{.r}\n(cl <- parallel::makeCluster(hosts))      # defaults to type=\"PSOCK\" which is good\n```\n\n```\nsocket cluster with 8 nodes on hosts ‘node1’, ‘node2’\n```\n\n### Plan cluster\n\nI can now try the code with distributed parallelism using all 8 CPU cores across both nodes:\n\n```{.r}\nplan(cluster, workers = cl)\n\ntdis <- mark(\n  rf2 <- foreach(ntree = rep(250, 8), .combine = combine) %dorng%\n    randomForest(x = traindata, y = fac, ntree = ntree),\n  memory=F\n)\n\ntdis$median\n```\n\n```\n[1] 1.14s\n```\n\n:::{.note}\n\nSpeedup: `5.78 / 1.14 = 5.1`. Here again, this is not bad with 8 CPU cores, considering the added overhead of message passing between both nodes.\n\n:::\n\nThe cluster of workers can be stopped with:\n\n```{.r}\nparallel::stopCluster(cl)\n```\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}