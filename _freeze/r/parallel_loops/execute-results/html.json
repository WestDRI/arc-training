{
  "hash": "54b13c41c1b754370fed63cc2d4827ff",
  "result": {
    "markdown": "---\ntitle: Parallel loops with foreach & doFuture\nauthor: Marie-Hélène Burle\n---\n\n\n## `foreach` package\n\nThe [`foreach`](https://cran.r-project.org/web/packages/foreach/index.html) package implements a looping construct without an explicit counter. It doesn't require the preallocation of an output container, it brings to R an equivalent of the Python or Julia list comprehensions, and mostly, it allows for an easy execution of loops in parallel.\n\nUnlike loops, it creates variables (loops are used for their side-effect).\n\nLet's look at an example to calculate the sum of 1e4 random vectors of length 3.\n\nBut first, what type of job shall we run here? This example is tiny and sequential. As the cluster for this course contains 10 nodes with 4 CPUs each, each of us can monopolize a CPU. We can thus use an interactive job:\n\n```{.bash}\nsalloc --time=30 --mem-per-cpu=7000M --ntasks=1\n```\n\n:::{.note}\n\nMake sure to type this in the Bash terminal. We haven't launched R yet!\n\n:::\n\n:::{.note}\n\nYou can see the full list of `salloc` options [here](https://slurm.schedmd.com/salloc.html).\n\n:::\n\nThen we can launch R interactively:\n\n```{.bash}\nR\n```\n\n:::{.note}\n\nWe are now in the R terminal and can start typing R commands.\n\n:::\n\nWe will use `foreach` and `iterators` (which creates convenient iterators for `foreach`):\n\n\n::: {.cell hash='parallel_loops_cache/html/unnamed-chunk-1_87472469db7dff122cd21fca90187633'}\n\n```{.r .cell-code}\nlibrary(foreach)\nlibrary(iterators)\n```\n:::\n\n\n- Classic while loop:\n\n\n::: {.cell hash='parallel_loops_cache/html/unnamed-chunk-2_450c3d0d8486ccec2893acf0ed7987ed'}\n\n```{.r .cell-code}\nset.seed(2)\nresult1 <- numeric(3)            # Preallocate output container\ni <- 0                           # Initialise counter variable\n\nwhile(i < 1e4) {\n  result1 <- result1 + runif(3)  # Calculate the sum\n  i <- i + 1                     # Update the counter\n}\n```\n:::\n\n\n- With foreach:\n\n\n::: {.cell hash='parallel_loops_cache/html/unnamed-chunk-3_e6fd3f4c286dca2bbfe32a09836316c5'}\n\n```{.r .cell-code}\nset.seed(2)\nresult2 <- foreach(icount(1e4), .combine = '+') %do% runif(3)\n```\n:::\n\n\nWe can verify that both expressions returned the same result:\n\n\n::: {.cell hash='parallel_loops_cache/html/unnamed-chunk-4_3ad0e9ee2006f5f5fa000bd779e23969'}\n\n```{.r .cell-code}\nall.equal(result1, result2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] TRUE\n```\n:::\n:::\n\n\nThe best part of `foreach` is that you can turn sequential loops into parallel ones by registering a parallel backend and replacing `%do%` with `%dopar%`.\n\nThere are many parallelization backends available: `doFuture`, `doMC`, `doMPI`, `doParallel`, `doRedis`, `doRNG`, `doSNOW`, and `doAzureParallel`.\n\nIn this lesson, we will use [`doFuture`](https://cran.r-project.org/web/packages/doFuture/index.html) which allows to evaluate `foreach` expressions following any of the strategies of the [`future`](https://cran.r-project.org/web/packages/future/index.html) package.\n\nSo first, what is the [`future`](https://cran.r-project.org/web/packages/future/index.html) package?\n\n## `future` package\n\nA [future](https://en.wikipedia.org/wiki/Futures_and_promises) is an object that acts as an abstract representation for a value in the future. A future can be *resolved* (if the value has been computed) or *unresolved*. If the value is queried while the future is unresolved, the process is blocked until the future is resolved. Futures thus allow for asynchronous and parallel evaluations.\n\nThe [`future`](https://cran.r-project.org/web/packages/future/index.html) package allows to evaluate futures sequentially or in various forms of parallelism while keeping code simple and consistent. The evaluation strategy is set thanks to the `plan` function:\n\n- `plan(sequential)`: \\\nFutures are evaluated sequentially in the current R session.\n\n- `plan(multisession)`: \\\nFutures are evaluated by new R sessions spawned in the background (*multi-processing in shared memory*).\n\n- `plan(multicore)`: \\\nFutures are evaluated in processes forked from the existing process (*multi-processing in shared memory*).\n\n- `plan(cluster)`: \\\nFutures are evaluated on an ad-hoc cluster (*distributed parallelism* across multiple nodes).\n\n:::{.note}\n\n###### Consistency\n\nTo ensure a consistent behaviour across plans, all evaluations are done in a local environment:\n\n\n::: {.cell hash='parallel_loops_cache/html/unnamed-chunk-5_fd799efc10a17a566e1580f48c493402'}\n\n```{.r .cell-code}\nlibrary(future)\n\na <- 1\n\nb %<-% {      # %<-% is used instead of <- to use futures\n  a <- 2\n}\n\na\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1\n```\n:::\n:::\n\n\n:::\n\n## `doFuture` package\n\nThe [`doFuture`](https://cran.r-project.org/web/packages/doFuture/index.html) package allows to evaluate `foreach` expressions across the evaluation strategies of the [`future`](https://cran.r-project.org/web/packages/future/index.html) package.\n\nLet's go back to our `foreach` example. We had:\n\n```{.r}\nlibrary(foreach)\n\nset.seed(2)\nresult2 <- foreach(icount(1e4), .combine = '+') %do% runif(3)\n```\n\nWe could replace `%do%` with `%dopar%`:\n\n```{.r}\nlibrary(foreach)\n\nset.seed(2)\nresult3 <- foreach(icount(1e4), .combine = '+') %dopar% runif(3)\n```\n\nSince we haven't registered any parallel backend, the expression would still be evaluated sequentially. To run this in parallel, we would need to load `doFuture`, register it as a backend (with `registerDoFuture()`), and choose a parallel strategy (e.g. `plan(multicore)`):\n\n```{.r}\nlibrary(foreach)\nlibrary(doFuture)\n\nregisterDoFuture()\nplan(multicore)\n\nset.seed(2)\nresult3 <- foreach(icount(1e4), .combine = '+') %dopar% runif(3)\n```\n\nOf course, we would also need to run this in a Slurm job with multiple CPUs.\n\nWith the overhead of parallelization however, it doesn't make sense to parallelize such a short code, so let's go over a proper toy example and do some benchmarking.\n\n## Toy example\n\n:::{.note}\n\nBefore getting started with this toy example, remember that you need to run the following:\n\n```{.bash}\n# Load necessary modules\nmodule load StdEnv/2020 gcc/11.3.0 r/4.2.1\n\n# Start interactive job with 1CPU\nsalloc --time=30 --mem-per-cpu=7000M --ntasks=1\n```\n\n:::\n\n### Load packages\n\nFor this toy example, we will use a modified version of one of the examples in the [foreach vignette](https://cran.r-project.org/web/packages/foreach/vignettes/foreach.html): we will build a classification model made of a forest of decision trees thanks to the [`randomForest`](https://cran.r-project.org/web/packages/randomForest/index.html) package.\n\nBecause the code includes randomly generated numbers, we will use the [`doRNG`](https://cran.r-project.org/web/packages/doRNG/index.html) package which replaces `foreach::%dopar%` with `doRNG::%dorng%`. This follows the recommendations of Pierre L'Ecuyer (1999)[^1] and ensures reproducibility.\n\n[^1]: [L'Ecuyer, P. (1999). Good parameters and implementations for combined multiple recursive random number generators. Operations Research, 47, 159–164.](https://pubsonline.informs.org/doi/10.1287/opre.47.1.159)\n\n```{.r}\nlibrary(doFuture)       # This will also load the `future` package\nlibrary(doRNG)          # This will also load the `foreach` package\nlibrary(randomForest)\nlibrary(bench)          # To do some benchmarking\n```\n\n```\nLoading required package: foreach\nLoading required package: future\nLoading required package: rngtools\n```\n\n### The code to parallelize\n\nThe goal is to create a classifier based on some data (here a matrix of random numbers for simplicity) and a response variable (as factor). This model could then be passed in the `predict()` function with novel data to generate predictions of classification. But here we are only interested in the creation of the model as this is the part that is computationally intensive. We aren't interested in actually using it.\n\n```{.r}\nset.seed(11)\ntraindata <- matrix(runif(1e5), 100)\nfac <- gl(2, 50)\n\nrf <- foreach(ntree = rep(250, 8), .combine = combine) %do%\n  randomForest(x = traindata, y = fac, ntree = ntree)\n\nrf\n```\n\n```\nCall:\n randomForest(x = traindata, y = fac, ntree = ntree)\n               Type of random forest: classification\n                     Number of trees: 2000\nNo. of variables tried at each split: 31\n```\n\n### Reference timing\n\nThis is the non parallelizable code with `%do%`:\n\n```{.r}\nbm <- mark(\n  rf <- foreach(ntree = rep(250, 8), .combine = combine) %do%\n    randomForest(x = traindata, y = fac, ntree = ntree),\n  memory = FALSE\n)\n\nbm$median\n```\n\n```\n[1] 5.66s\n```\n\n:::{.note}\n\n`bench::mark()` is currently unable to provide memory information for parallel code. While it could output memory usage for this sequential run, we won't be able to compare it with parallel runs, so we might as well not ask for that information.\n\n:::\n\n### Plan sequential\n\nThis is the parallelizable `foreach` code, but run sequentially:\n\n```{.r}\nregisterDoFuture()   # Set the parallel backend\nplan(sequential)     # Set the evaluation strategy\n\n# Using bench::mark()\nbm <- mark(\n  rf <- foreach(ntree = rep(250, 8), .combine = combine) %dorng%\n    randomForest(x = traindata, y = fac, ntree = ntree),\n  memory = FALSE\n)\n\nbm$median\n```\n\n```\n[1] 5.78s\n```\n\n:::{.note}\n\nNo surprise: those are similar.\n\n:::\n\n### Multi-processing in shared memory\n\n#### Number of cores\n\n`future` provides `availableCores()` to detect the number of available cores:\n\n```{.r}\navailableCores()\n```\n\n```\ncgroups.cpuset\n             1\n```\n\n:::{.note}\n\nSimilar to `parallel::detectCores()`.\n\n:::\n\nThis detects the number of CPU cores available to us on the current compute node, that is, what we can use for shared memory multi-processing. Since we asked for a single task (`--ntasks=1`) and since by default Slurm grants one CPU per task, we have a single CPU available.\n\nTo be able to run our code in parallel, we need to have access to at least 2 CPUs each. So let's quit the R session (with Ctrl+D or `quit()`—when asked whether to save a workspace image, answer `n`), terminate our interactive job (also with Ctrl+D) and ask for a different job.\n\n:::{.emph}\n\nDon't forget to relinquish your interactive job with Ctrl+D otherwise it will be running for the full 30 min, making the hardware it uses unavailable to all of us until the job expires.\n\n:::\n\nNow what job should we run?\n\nRemember that the cluster for this course is made of 10 nodes of 4 CPUs each. We want to test shared memory parallelism, so our job needs to stay within one node. We can thus ask for a maximum of 4 CPUs and we want to ensure that we aren't getting them on different nodes. If we all ask for 4 CPUs in an interactive session, the first 10 of us will get them and the job requests for the rest of us will remain pending, waiting for resources to become available, for as long as the lucky 10 are running their session. That's the big downside of interactive sessions.\n\nA much better approach here is to write the code in a script and run it with `sbatch`. That way, everybody will get to run their code with minimal delay.\n\nOpen a text file (let's call it `rf.R` since it creates a random forest object) with the text editor of your choice, for instance `nano`:\n\n```{.bash}\nnano rf.R\n```\n\nWe will first play with it to see how many cores are available to us, so write in your script:\n\n```{.r filename=\"rf.R\"}\nlibrary(future)\t  # Don't forget to load the packages in your script\navailableCores()\n```\n\nSave and close the text editor.\n\nNow, we want to create a shell script for Slurm. Let's call it `rf.sh`:\n\n```{.bash}\nnano rf.sh\n```\n\nIn it lives the hardware request and the code that needs to run:\n\n```{.bash filename=\"rf.sh\"}\n#!/bin/bash\n#SBATCH --time=10             # 10 min\n#SBATCH --mem-per-cpu=7000M\n#SBATCH --nodes=1\n#SBATCH --cpus-per-task=4\n\nRscript rf.R                  # This is the code that we are running\n```\n\n:::{.note}\n\nYou can see the full list of `sbatch` options [here](https://slurm.schedmd.com/sbatch.html).\n\n:::\n\nSave and close the text editor.\n\nWe can now run the batch script:\n\n```{.bash}\nsbatch rf.sh\n```\n\nYou can monitor it with `sq`, but this should be quasi instant. The result will be written to a file called `slurm-xx.out` with `xx` being the number of the job that just ran.\n\n:::{.note}\n\nYou can specify the output file name in the options of your sbatch script.\n\n:::\n\nTo see the result, we can simply print the content of that file to screen (you can run `ls` to see the list of files in the current directory):\n\n```{.bash}\ncat slurm-xx.out    # Replace xx by the job number\n```\n\n```\nsystem\n     4\n```\n\nWe now have 4 CPUs available on one node, so we can test shared memory parallelism.\n\n#### Plan multisession\n\nShared memory multi-processing can be run with `plan(multisession)` that will spawn new R sessions in the background to evaluate futures.\n\nEdit the R script (with `nano rf.R`):\n\n```{.r filename=\"rf.R\"}\nlibrary(doFuture)\nlibrary(doRNG)\nlibrary(randomForest)\nlibrary(bench)\n\nregisterDoFuture()   # Set the parallel backend\nplan(multisession)\n\nset.seed(11)\ntraindata <- matrix(runif(1e5), 100)\nfac <- gl(2, 50)\n\nbm <- mark(\n  rf <- foreach(ntree = rep(250, 8), .combine = combine) %dorng%\n    randomForest(x = traindata, y = fac, ntree = ntree),\n  memory = FALSE\n)\n\nbm$median\n```\n\nRun the job with the new R script:\n\n```{.bash}\nsbatch rf.sh\n```\n\nWe now get in the output file:\n\n```\nLoading required package: foreach\nLoading required package: future\nLoading required package: rngtools\nrandomForest 4.7-1.1\nType rfNews() to see new features/changes/bug fixes.\nWarning message:\nSome expressions had a GC in every iteration; so filtering is disabled.\n[1] 2.08s\n```\n\n:::{.note}\n\nWe got a speedup of `5.78 / 2.08 = 2.8`. Not bad considering that we have 4 CPU cores (the ideal speedup would be 4, but there is always some overhead to parallelization).\n\n:::\n\n#### Plan multicore\n\nShared memory multi-processing can also be run with `plan(multicore)` (except on Windows) that will fork the current R process to evaluate futures.\n\nLet's modify our R script again:\n\n```{.r filename=\"rf.R\"}\nlibrary(doFuture)\nlibrary(doRNG)\nlibrary(randomForest)\nlibrary(bench)\n\nregisterDoFuture()   # Set the parallel backend\nplan(multicore)\n\nset.seed(11)\ntraindata <- matrix(runif(1e5), 100)\nfac <- gl(2, 50)\n\nbm <- mark(\n  rf <- foreach(ntree = rep(250, 8), .combine = combine) %dorng%\n    randomForest(x = traindata, y = fac, ntree = ntree),\n  memory = FALSE\n)\n\nbm$median\n```\n\nRun the job:\n\n```{.bash}\nsbatch rf.sh\n```\n\nWe get:\n\n```\nLoading required package: foreach\nLoading required package: future\nLoading required package: rngtools\nrandomForest 4.7-1.1\nType rfNews() to see new features/changes/bug fixes.\nWarning message:\nSome expressions had a GC in every iteration; so filtering is disabled.\n[1] 1.89s\n```\n\n:::{.note}\n\nWe got a similar speedup of `5.78 / 1.89 = 3.1`.\n\n:::\n\n### Multi-processing in distributed memory\n\n#### Create a cluster of workers\n\nTo test parallel execution in distributed memory, let's ask [Slurm](https://en.wikipedia.org/wiki/Slurm_Workload_Manager) for 8 tasks by editing our `rf.sh` script:\n\n```{.bash filename=\"rf.sh\"}\n#!/bin/bash\n#SBATCH --time=10\n#SBATCH --mem-per-cpu=7000M\n#SBATCH --ntasks=8\n\nRscript rf.R      # This is the code that we are running\n```\n\nLet's verify that we do get 8 tasks by accessing the `SLURM_NTASKS` environment variable from within R.\n\nEdit `rf.R` to contain the following:\n\n```{.r filename=\"rf.R\"}\nas.numeric(Sys.getenv(\"SLURM_NTASKS\"))\n```\n\nRun the job:\n\n```{.bash}\nsbatch rf.sh\n```\n\nWe get:\n\n```\n[1] 8\n```\n\nLet's see which nodes we are using:\n\n```{.r filename=\"rf.R\"}\nsystem(\"srun hostname | cut -f 1 -d '.'\", intern = T)\n```\n\nWe get:\n\n```\n[1] \"node1\" \"node1\" \"node1\" \"node1\" \"node2\" \"node2\" \"node2\" \"node2\"\n```\n\nTo run the RandomForest code with distributed parallelism using 8 CPU cores across both nodes, we will need to create a cluster of workers. We do this with the `makeCluster()` function from the base R `parallel` package: we create a character vector with the names of the nodes our tasks are running on and pass this vector to the `makeCluster()` function:\n\n```{.r}\n## Create a character vector with the nodes names\nhosts <- system(\"srun hostname | cut -f 1 -d '.'\", intern = T)\n\n## Create the cluster of workers\ncl <- parallel::makeCluster(hosts)\n```\n\nLet's test it:\n\n```{.r filename=\"rf.R\"}\nlibrary(doFuture)\n\nhosts <- system(\"srun hostname | cut -f 1 -d '.'\", intern = T)\ncl <- parallel::makeCluster(hosts)\n\ncl\n```\n\nIf we run this code, we get:\n\n```\nLoading required package: foreach\nLoading required package: future\nsocket cluster with 8 nodes on hosts ‘node1’, ‘node2’\n```\n\n:::{.note}\n\nMake sure that your code has finished running before printing the output file. Remember that you can monitor the job with `sq`.\n\n:::\n\n#### Plan cluster\n\nWe can now run the code in distributed memory parallelism:\n\n```{.r filename=\"rf.R\"}\nlibrary(doFuture)\nlibrary(doRNG)\nlibrary(randomForest)\nlibrary(bench)\n\nregisterDoFuture()   # Set the parallel backend\n\nhosts <- system(\"srun hostname | cut -f 1 -d '.'\", intern = T)\ncl <- parallel::makeCluster(hosts)\nplan(cluster, workers = cl)\n\nset.seed(11)\ntraindata <- matrix(runif(1e5), 100)\nfac <- gl(2, 50)\n\nbm <- mark(\n  rf <- foreach(ntree = rep(250, 8), .combine = combine) %dorng%\n    randomForest(x = traindata, y = fac, ntree = ntree),\n  memory = FALSE\n)\n\nbm$median\n```\n\nWe get:\n\n```\nLoading required package: foreach\nLoading required package: future\nLoading required package: rngtools\nrandomForest 4.7-1.1\nType rfNews() to see new features/changes/bug fixes.\nWarning message:\nSome expressions had a GC in every iteration; so filtering is disabled.\n[1] 1.16s\n```\n\n:::{.note}\n\nSpeedup: `5.78 / 1.16 = 5.0`. Here again, this is not bad with 8 CPU cores, considering the added overhead of message passing between both nodes.\n\n:::\n\nThe cluster of workers can be stopped with:\n\n```{.r}\nparallel::stopCluster(cl)\n```\n\nHere, this is not necessary since our job stops running as soon as the execution is complete.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}