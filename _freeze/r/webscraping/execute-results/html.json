{
  "hash": "6fc20bd8bc0d81fbc328bfd2cdedf4c5",
  "result": {
    "markdown": "---\ntitle: Web scraping with R\nauthor: Marie-Hélène Burle\nexecute:\n  cache: false\n---\n\n\n:::{.def}\n\nThe internet is a trove of information. A lot of it is publicly available and thus suitable for use in research. Extracting that information and putting it in an organized format for analysis can, however, be extremely tedious. Web scraping tools allow to automate parts of that process and R is a popular language for the task.\n\nIn this workshop, we will guide you through a simple example using the package [rvest](https://rvest.tidyverse.org/).\n\n:::\n\n:::{.callout-accordion collapse=\"true\"}\n\n## ***Running R***\n\nFor this workshop, we will use a temporary RStudio server. To access it, go to the website given during the workshop and sign in using the username and password you will be given (you can ignore the OTP entry).\n\n:::{.note}\n\nOur RStudio server already has the two packages that we will be using installed ([rvest](https://cran.r-project.org/web/packages/rvest/index.html) and [tibble](https://cran.r-project.org/web/packages/tibble/index.html)). If you want to run the code on your machine, you need to install them with `install.packages()` first.\n\n:::\n\n:::\n\n## HTML and CSS\n\n[HyperText Markup Language](https://en.wikipedia.org/wiki/HTML) (HTML) is the standard markup language for websites: it encodes the information related to the formatting and structure of webpages. Additionally, some of the customization can be stored in [Cascading Style Sheets](https://en.wikipedia.org/wiki/CSS) (CSS) files.\n\nHTML uses tags of the form:\n\n```{.html}\n<some_tag>Your content</some_tag>\n```\n\nSome tags have attributes:\n\n```{.html}\n<some_tag attribute_name=\"attribute value\">Your content</some_tag>\n```\n\n:::{.example}\n\nExamples:\n\n:::\n\nSite structure:\n\n- `<h2>This is a heading of level 2</h2>`\n- `<p>This is a paragraph</p>`\n\nFormatting:\n\n- `<b>This is bold</b>`\n- `<a href=\"https://some.url\">This is the text for a link</a>`\n\n## Web scrapping\n\nWeb scraping is a general term for a set of tools which allow for the extraction of data from the web automatically.\n\nWhile most of the data on the internet is publicly available, it is illegal to scrape some sites and you should always look into the policy of a site before attempting to scrape it. Some sites will also block you if you submit too many requests in a short amount of time, so if you plan on scraping sites at a fairly large scale, you should look into the [polite](https://dmi3kno.github.io/polite/) package which will help you scrape responsibly.\n\n## Example\n\n### Goal\n\nWe will use [a website](https://trace.tennessee.edu/utk_graddiss/index.html) from the [University of Tennessee](https://www.utk.edu/) containing a database of PhD theses from that university.\n\nOur goal is to scrape data from this site to produce a dataframe with the date, major, and principal investigator (PI) for each dissertation.\n\n:::{.note}\n\nWe will only do this for the first page which contains the links for the 100 most recent theses. If you really wanted to gather all the data, you would have to do this for all pages.\n\n:::\n\n### Let's look at the sites\n\nFirst of all, let's have a close look at the websites we want to scrape to think carefully about what we want to do. Before starting to write code, it is always a good idea to think about what you are trying to achieve with your code.\n\nTo create a dataframe with the data for all the dissertations on that first page, we need to do two things:\n\n- Step 1: from the [dissertations database first page](https://trace.tennessee.edu/utk_graddiss/index.html), we want to scrape the list of URLs for the dissertation pages.\n\n- Step 2: once we have the links, we want to scrape those pages too to get the date, major, and principal investigator (PI) for each dissertation.\n\n### Package\n\nTo do all this, we will use the package [rvest](https://cran.r-project.org/web/packages/rvest/index.html), part of the [tidyverse](https://www.tidyverse.org/) (a modern set of R packages). It is a package influenced by the popular Python package [Beautiful Soup](https://en.wikipedia.org/wiki/Beautiful_Soup_(HTML_parser)) and it makes scraping websites with R really easy.\n\nLet's load it:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(rvest)\n```\n:::\n\n\n### Read in HTML data\n\nAs mentioned above, our site is the [database of PhD dissertations from the University of Tennessee](https://trace.tennessee.edu/utk_graddiss/index.html).\n\nLet's create a character vector with the url:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nurl <- \"https://trace.tennessee.edu/utk_graddiss/index.html\"\n```\n:::\n\n\nFirst, we read in the html data from that page:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhtml <- read_html(url)\n```\n:::\n\n\nLet's have a look at the raw data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhtml\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n{html_document}\n<html lang=\"en\">\n[1] <head>\\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8 ...\n[2] <body>\\n<!-- FILE /srv/sequoia/main/data/trace.tennessee.edu/assets/heade ...\n```\n:::\n:::\n\n\n### Extracting relevant data\n\n#### Method\n\nThe html code for this webpage contains the data we are interested in, but it is mixed in with a lot of HTML formatting information and data we aren't interested in. We need to extract it and turn it into a workable format.\n\nThe first step is to find the CSS elements that contain the data we want. For this, you can use a web inspector or—even easier—the [SelectorGadget](https://selectorgadget.com/), a JavaScript bookmarklet built by [Andrew Cantino](https://andrewcantino.com/).\n\nThis bookmarklet allows us to see that the elements we want (the links to the dissertation information pages) are under the CSS class `.article-listing`.\n\n#### Extracting a single link\n\nIt is a good idea to test things out on a single element before doing a massive batch scraping of a site, so let's test our method on the first dissertation.\n\nFirst, we need to extract the first URL. The function `html_element()` from the package `rvest` does exactly this. Let's assign the result to an object that we will call `test`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest <- html %>% html_element(\".article-listing\")\n```\n:::\n\n\n:::{.note}\n\n`%>%` is a pipe from the [magrittr](https://magrittr.tidyverse.org/) tidyverse package. It passes the output from the left-hand side expression as the first argument of the right-hand side expression. We could have written this as:\n\n```{.r}\ntest <- html_element(html, \".article-listing\")\n```\n\n:::\n\nOur new object is a list:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntypeof(test)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"list\"\n```\n:::\n:::\n\n\nLet's print it:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n{html_node}\n<p class=\"article-listing\">\n[1] <a href=\"https://trace.tennessee.edu/utk_graddiss/7600\">The Novel Chlorin ...\n```\n:::\n:::\n\n\nThe link is in there, so we successfully extracted the correct element, but we need to do more cleaning.\n\nAs you can see from printing `test`, the link is in an `a` anchor element. Let's extract it by running the function `html_element()` again:\n\n\n::: {.cell}\n\n```{.r .cell-code}\na_test <- test %>% html_element(\"a\")\na_test\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n{html_node}\n<a href=\"https://trace.tennessee.edu/utk_graddiss/7600\">\n```\n:::\n:::\n\n\n:::{.note}\n\nAn alternative method is to use the `html_children()` function which gets the children element (the element inside an element):\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest %>% html_children()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n{xml_nodeset (1)}\n[1] <a href=\"https://trace.tennessee.edu/utk_graddiss/7600\">The Novel Chlorin ...\n```\n:::\n:::\n\n\n:::\n\nThis is much better, but we still need to extract the `href` attribute:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlink_test <- a_test %>% html_attr(\"href\")\nlink_test\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"https://trace.tennessee.edu/utk_graddiss/7600\"\n```\n:::\n:::\n\n\nThis is our link.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstr(link_test)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n chr \"https://trace.tennessee.edu/utk_graddiss/7600\"\n```\n:::\n:::\n\n\nIt is saved in a character vector, which is perfect.\n\n#### Getting data from our test link\n\nNow that we have the URL for the first dissertation information page, we want to extract the date, major, and principal investigator (PI) for that dissertation.\n\nWe just saw that `link_test` is a character vector representing a URL. We know how to deal with this.\n\nThe first thing to do—as we did earlier with the database site—is to read in the html data. Let's assign it to a new object that we will call `html_test`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhtml_test <- read_html(link_test)\nhtml_test\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n{html_document}\n<html lang=\"en\">\n[1] <head>\\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8 ...\n[2] <body>\\n<!-- FILE /srv/sequoia/main/data/trace.tennessee.edu/assets/heade ...\n```\n:::\n:::\n\n\nNow, we want to extract the publication date. Thanks to the [SelectorGadget](https://selectorgadget.com/), we can see that it is in the element `#publication_date p`. While earlier we wanted a link (i.e. part of the CSS formatting data), here we just want the text. In this case, we extract the element with `html_element()` (as before) and pass the result to `html_text2()`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndate_test <- html_test %>%\n  html_element(\"#publication_date p\") %>%\n  html_text2()\n```\n:::\n\n\n:::{.note}\n\n`html_text2()` extracts the text part of an element and formats it nicely.\n\nNote the difference with what we did earlier to extract the link: if we had used `html_text2()` then we would have gotten the text part of the link—that is \"The Novel Chlorination of Zirconium Metal and Its Application to a Recycling Protocol for Zircaloy Cladding from Spent Nuclear Fuel Rods\"—rather than the link itself.\n\n:::\n\nLet's verify that our `date` object indeed contains the date:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndate_test\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"5-2023\"\n```\n:::\n:::\n\n\nWe also want the major for this thesis. The [SelectorGadget](https://selectorgadget.com/) allows us to find that this time, it is the `#department p` element that we need. Let's extract it in the same fashion:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmajor_test <- html_test %>%\n  html_element(\"#department p\") %>%\n  html_text2()\nmajor_test\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Chemistry\"\n```\n:::\n:::\n\n\nAnd for the PI, we need the `#advisor1 p` element:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npi_test <- html_test %>%\n  html_element(\"#advisor1 p\") %>%\n  html_text2()\npi_test\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Craig E. Barnes\"\n```\n:::\n:::\n\n\n:::{.exo}\n\n:::{.yourturn}\n\nYour turn:\n\n:::\n\nTry using the [SelectorGadget](https://selectorgadget.com/) to identify the element necessary to extract the abstract of this dissertation.\n\nNow, write the code to extract it.\n\n:::\n\nWe now have the date, major, and PI for the first dissertation. We could create a matrix with them by passing them as arguments to `cbind()`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresult_test <- cbind(date_test, major_test, pi_test)\nresult_test\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     date_test major_test  pi_test          \n[1,] \"5-2023\"  \"Chemistry\" \"Craig E. Barnes\"\n```\n:::\n:::\n\n\n#### Extracting all links\n\nNow that we have tested our code on the first dissertation, we can apply it on all 100 dissertations of the first page of the database.\n\nInstead of using `html_element()`, this time we will use `html_elements()` which extracts *all* matching elements (instead of just the first one):\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat <- html %>% html_elements(\".article-listing\")\ndat\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n{xml_nodeset (100)}\n [1] <p class=\"article-listing\"><a href=\"https://trace.tennessee.edu/utk_grad ...\n [2] <p class=\"article-listing\"><a href=\"https://trace.tennessee.edu/utk_grad ...\n [3] <p class=\"article-listing\"><a href=\"https://trace.tennessee.edu/utk_grad ...\n [4] <p class=\"article-listing\"><a href=\"https://trace.tennessee.edu/utk_grad ...\n [5] <p class=\"article-listing\"><a href=\"https://trace.tennessee.edu/utk_grad ...\n [6] <p class=\"article-listing\"><a href=\"https://trace.tennessee.edu/utk_grad ...\n [7] <p class=\"article-listing\"><a href=\"https://trace.tennessee.edu/utk_grad ...\n [8] <p class=\"article-listing\"><a href=\"https://trace.tennessee.edu/utk_grad ...\n [9] <p class=\"article-listing\"><a href=\"https://trace.tennessee.edu/utk_grad ...\n[10] <p class=\"article-listing\"><a href=\"https://trace.tennessee.edu/utk_grad ...\n[11] <p class=\"article-listing\"><a href=\"https://trace.tennessee.edu/utk_grad ...\n[12] <p class=\"article-listing\"><a href=\"https://trace.tennessee.edu/utk_grad ...\n[13] <p class=\"article-listing\"><a href=\"https://trace.tennessee.edu/utk_grad ...\n[14] <p class=\"article-listing\"><a href=\"https://trace.tennessee.edu/utk_grad ...\n[15] <p class=\"article-listing\"><a href=\"https://trace.tennessee.edu/utk_grad ...\n[16] <p class=\"article-listing\"><a href=\"https://trace.tennessee.edu/utk_grad ...\n[17] <p class=\"article-listing\"><a href=\"https://trace.tennessee.edu/utk_grad ...\n[18] <p class=\"article-listing\"><a href=\"https://trace.tennessee.edu/utk_grad ...\n[19] <p class=\"article-listing\"><a href=\"https://trace.tennessee.edu/utk_grad ...\n[20] <p class=\"article-listing\"><a href=\"https://trace.tennessee.edu/utk_grad ...\n...\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntypeof(dat)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"list\"\n```\n:::\n\n```{.r .cell-code}\nlength(dat)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 100\n```\n:::\n\n```{.r .cell-code}\ntypeof(dat[[1]])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"list\"\n```\n:::\n:::\n\n\nWe now have a list of lists.\n\nAs we did for a single link, we want to extract all the links to have a list of links. We will do this using a loop.\n\nBefore running for loops, it is important to initialize empty loops. It is much more efficient than growing the result at each iteration.\n\nSo let's initialize an empty list:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlist_links <- vector(\"list\", length(dat))\n```\n:::\n\n\nLet's have a look at one element of our list (the second one for instance):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlist_links[[2]]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNULL\n```\n:::\n:::\n\n\nWe now have an empty list of the appropriate size. We can run our loop:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfor (i in seq_along(dat)) {\n  list_links[[i]] <- dat[[i]] %>%\n    html_element(\"a\") %>%\n    html_attr(\"href\")\n}\n```\n:::\n\n\nLet's print again the second element of our list to make sure all looks good:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlist_links[[2]]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"https://trace.tennessee.edu/utk_graddiss/7714\"\n```\n:::\n:::\n\n\nWe have a character vector with one link. That's great! `list_links` is a list of links (in the form of character vectors) as we wanted.\n\n#### Getting the data from the list of links\n\nWe will now extract the data (date, major, and PI) for all links in our list.\n\nAgain, before running a for loop, we need to allocate memory first by creating an empty container:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlist_data <- vector(\"list\", length(list_links))\n```\n:::\n\n\nWe move the code we tested for a single dissertation inside a loop and we add one result to the `list_data` list at each iteration until we have all 100 dissertation sites scraped. Because there are quite a few of us running the code at the same time, we don't want the site to block our request. To play safe, we will add a little delay (0.1 second) at each iteration:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfor (i in seq_along(list_links)) {\n  html <- read_html(list_links[[i]])\n  date <- html %>%\n    html_element(\"#publication_date p\") %>%\n    html_text2()\n  major <- html %>%\n    html_element(\"#department p\") %>%\n    html_text2()\n  pi <- html %>%\n    html_element(\"#advisor1 p\") %>%\n    html_text2()\n  Sys.sleep(0.1)  # Add a little delay\n  list_data[[i]] <- cbind(date, major, pi)\n}\n```\n:::\n\n\nLet's make sure all looks good by printing the second element of `list_data`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlist_data[[2]]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     date      major       pi                   \n[1,] \"12-2022\" \"Chemistry\" \"Dr. Ampofo K. Darko\"\n```\n:::\n:::\n\n\nAll looking good, so let's turn this big list into a dataframe:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresult <- do.call(rbind.data.frame, list_data)\n```\n:::\n\n\n`result` is a long dataframe, so we will only print the first few elements:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(result)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     date                                      major                  pi\n1  5-2023                                  Chemistry     Craig E. Barnes\n2 12-2022                                  Chemistry Dr. Ampofo K. Darko\n3 12-2022                     Industrial Engineering     James Ostrowski\n4  5-2022 Entomology, Plant Pathology and Nematology       Heather Kelly\n5  5-2022                     Mechanical Engineering     Caleb D. Rucker\n6 12-2022                     Electrical Engineering            Yilu Liu\n```\n:::\n:::\n\n\nIf you like the tidyverse, you can turn it into a tibble:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresult <- result %>% tibble::as_tibble()\n```\n:::\n\n\n:::{.note}\n\nThe notation `tibble::as_tibble()` means that we are using the function `as_tibble()` from the package [tibble](https://tibble.tidyverse.org/). A tibble is the [tidyverse](https://www.tidyverse.org/) version of a dataframe. One advantage is that it will only print the first 10 rows by default instead of printing the whole dataframe, so you don't have to use `head()` when printing long dataframes:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresult\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 100 × 3\n   date    major                                      pi                   \n   <chr>   <chr>                                      <chr>                \n 1 5-2023  Chemistry                                  Craig E. Barnes      \n 2 12-2022 Chemistry                                  Dr. Ampofo K. Darko  \n 3 12-2022 Industrial Engineering                     James Ostrowski      \n 4 5-2022  Entomology, Plant Pathology and Nematology Heather Kelly        \n 5 5-2022  Mechanical Engineering                     Caleb D. Rucker      \n 6 12-2022 Electrical Engineering                     Yilu Liu             \n 7 5-2022  Comparative and Experimental Medicine      Brian K. Whitlock    \n 8 5-2022  History                                    Jay Rubenstein       \n 9 12-2022 Anthropology                               Dawnie W. Steadman   \n10 12-2022 Mechanical Engineering                     Stephanie C. TerMaath\n# … with 90 more rows\n```\n:::\n:::\n\n\n:::\n\nWe can rename the headers:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnames(result) <- c(\"Date\", \"Major\", \"PI\")\n```\n:::\n\n\nThis is what our final result looks like:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresult\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 100 × 3\n   Date    Major                                      PI                   \n   <chr>   <chr>                                      <chr>                \n 1 5-2023  Chemistry                                  Craig E. Barnes      \n 2 12-2022 Chemistry                                  Dr. Ampofo K. Darko  \n 3 12-2022 Industrial Engineering                     James Ostrowski      \n 4 5-2022  Entomology, Plant Pathology and Nematology Heather Kelly        \n 5 5-2022  Mechanical Engineering                     Caleb D. Rucker      \n 6 12-2022 Electrical Engineering                     Yilu Liu             \n 7 5-2022  Comparative and Experimental Medicine      Brian K. Whitlock    \n 8 5-2022  History                                    Jay Rubenstein       \n 9 12-2022 Anthropology                               Dawnie W. Steadman   \n10 12-2022 Mechanical Engineering                     Stephanie C. TerMaath\n# … with 90 more rows\n```\n:::\n:::\n\n\n## Functions recap\n\nBelow is a recapitulation of the `rvest` functions we have used today:\n\n| Functions | Usage |\n|-----------|-------|\n| `read_html()` | Read in HTML data |\n| `html_element()` | Extract first matching element |\n| `html_elements()` | Extract all matching elements |\n| `html_children()` | Get children element |\n| `html_attr()` | Get an attribute |\n| `html_text2()` | Retrieve text |\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}