{
  "hash": "0556267bd7c5311ac2cf4c30dd5d7ab0",
  "result": {
    "markdown": "---\ntitle: Web scraping with R\nauthor: Marie-Hélène Burle\n---\n\n\n:::{.def}\n\nThe internet is a trove of information. A lot of it is publicly available and thus suitable for use in research. Extracting that information and putting it in an organized format for analysis can, however, be extremely tedious. Web scraping tools allow to automate parts of that process and R is a popular language for the task.\n\nIn this workshop, we will guide you through a simple example using the package [rvest](https://rvest.tidyverse.org/).\n\n:::\n\n<!-- https://trace.tennessee.edu/utk_graddiss/index.html -->\n<!-- 1. Scrape all the links of that page into a list -->\n<!-- 2. Use each link to extract the information \"Date of Award\", \"Degree Type\", etc. through a big loop -->\n\nFor this workshop, we will use the package [rvest](https://cran.r-project.org/web/packages/rvest/index.html), part of the [tidyverse](https://www.tidyverse.org/)—a modern set of R packages.\n\nLoad the required packages:\n\n\n::: {.cell hash='webscraping_cache/html/unnamed-chunk-1_8cb77e214d390a20bded2a189c9eecfc'}\n\n```{.r .cell-code}\nlibrary(rvest)\n```\n:::\n\n\nOur site:\n\n\n::: {.cell hash='webscraping_cache/html/unnamed-chunk-2_cb383dcb7525a96c98958aca7d50ed1e'}\n\n```{.r .cell-code}\nurl <- \"https://trace.tennessee.edu/utk_graddiss/index.html\"\n```\n:::\n\n\nRead in the data from our URL:\n\n\n::: {.cell hash='webscraping_cache/html/unnamed-chunk-3_b2a10c7ba3c1c81f77ea0bf50f93eb67'}\n\n```{.r .cell-code}\ndiss_html <- read_html(url)\n```\n:::\n\n\nExplore the raw data:\n\n\n::: {.cell hash='webscraping_cache/html/unnamed-chunk-4_bc37e63e65c70ce852f4ca39a7b28b93'}\n\n```{.r .cell-code}\ndiss_html\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n{html_document}\n<html lang=\"en\">\n[1] <head>\\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8 ...\n[2] <body>\\n<!-- FILE /srv/sequoia/main/data/trace.tennessee.edu/assets/heade ...\n```\n:::\n:::\n\n::: {.cell hash='webscraping_cache/html/unnamed-chunk-5_23cecdb92d4027876ec5b3457c8de837'}\n\n```{.r .cell-code}\ndiss <- diss_html %>% html_elements(\"p.article-listing\")\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in xml_ns.xml_document(x): external pointer is not valid\n```\n:::\n\n```{.r .cell-code}\ndiss\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): object 'diss' not found\n```\n:::\n:::\n\n```{.r}\n# Then find elements that match a css selector or XPath expression\n# using html_elements(). In this example, each <section> corresponds\n# to a different film\n\n\n# Then use html_element() to extract one element per film. Here\n# we the title is given by the text inside <h2>\ntitle <- films %>% \n  html_element(\"h2\") %>% \n  html_text2()\ntitle\n\n# Or use html_attr() to get data out of attributes. html_attr() always\n# returns a string so we convert it to an integer using a readr function\nepisode <- films %>% \n  html_element(\"h2\") %>% \n  html_attr(\"data-id\") %>% \n  readr::parse_integer()\nepisode\n```\n\n```{.r}\nhtml <- read_html(\"https://en.wikipedia.org/w/index.php?title=The_Lego_Movie&oldid=998422565\")\n\nhtml %>% \n  html_element(\".tracklist\") %>% \n  html_table()\n```\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}