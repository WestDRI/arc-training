{
  "hash": "bd3218c8b100cd6b0719b5a98f5d5425",
  "result": {
    "markdown": "---\ntitle: \"Measuring performance:\"\nsubtitle: Profiling & benchmarking\nauthor: Marie-Hélène Burle\n---\n\n\n:::{.def}\n\nBefore we talk about ways to improve performance, let's see how to measure it.\n\n:::\n\n## When should you care?\n\n> \"There is no doubt that the grail of efficiency leads to abuse. Programmers waste enormous amounts of time thinking about, or worrying about, the speed of noncritical parts of their programs, and these attempts at efficiency actually have a strong negative impact when debugging and maintenance are considered. We should forget about small efficiencies, say about 97% of the time: premature optimization is the root of all evil. Yet we should not pass up our opportunities in that critical 3%.\"\n>\n> [— Donald Knuth](https://en.wikipedia.org/wiki/Donald_Knuth)\n\nOptimizing code takes time, can lead to mistakes, and may make code harder to read. Consequently, not all code is worth optimizing and before jumping into optimizations, you need a strategy.\n\nYou should consider optimizations when:\n\n- you have debugged your code (optimization comes last, don't optimize a code that doesn't run),\n- you will run a section of code (e.g. a function) many times (your optimization efforts will really pay off),\n- a section of code is particularly slow.\n\nHow do you know which sections of your code are slow? Don't rely on intuition. You need to profile your code to identify bottlenecks.\n\n## Profiling\n\n> \"It is often a mistake to make a priori judgments about what parts of a program are really critical, since the universal experience of programmers who have been using measurement tools has been that their intuitive guesses fail.\"\n>\n> [— Donald Knuth](https://en.wikipedia.org/wiki/Donald_Knuth)\n\nR comes with a profiler: `Rprof()`.\n\n[profvis](https://cran.r-project.org/web/packages/profvis/index.html) is a newer tool, built by [posit](https://posit.co/) (formerly RStudio Inc). Under the hood, it runs `Rprof()` to collect data, then produces an interactive html widget with a flame graph that allows for an easy visual identification of slow sections of code.\n\nWhile this tool integrates well within the RStudio IDE, it is not very well suited for remote work on a cluster. One option is to profile your code with small data on your own machine. Another option is to use the base profiler `Rprof()` directly as in [this example](https://rstudio.github.io/r-manuals/r-exts/Tidying-and-profiling-R-code.html#profiling-r-code-for-speed).\n\n## Benchmarking\n\nOnce you have identified expressions that are particularly slow, you can use benchmarking tools to compare variations of the code.\n\nIn the most basic fashion, you can use `system.time()`, but this is limited and imprecise.\n\nThe [microbenchmark](https://cran.r-project.org/web/packages/microbenchmark/index.html) package is a much better option. It gives the minimum time, lower quartile, mean, median, upper quartile, and maximum time of R expressions.\n\nThe newer [bench](https://cran.r-project.org/web/packages/bench/index.html) package is very similar, but it has less overhead, is more accurate, and—for sequential code—gives information on memory usage and garbage collections. This is the package that we will use for this course.\n\n### `bench::mark()` example\n\n:::{.info}\n\nTo have access to the packages that have been pre-installed on the training cluster, you need to change the location of the R library by running in the R console:\n\n```{.r}\n.libPaths(\"/project/def-sponsor00/R/lib\")\n```\n\nYou only need to run this once per session.\n\n:::\n\nOnce you have run the line above, you can load the `bench` package:\n\n\n::: {.cell hash='performance_cache/html/unnamed-chunk-1_adfe59d64f0de6431a4081148ce3e9d4'}\n\n```{.r .cell-code}\nlibrary(bench)\n```\n:::\n\n\nThe main function from this package is `mark()`. You can pass as argument(s) one or multiple expressions that you want to benchmark. By default, it ensures that all expressions output the same result. If you want to remove this test, add the argument `check = FALSE`.\n\nWhile `mark()` gives memory usage and garbage collection information for sequential code, this functionality is not yet implemented for parallel code. When benchmarking parallel expressions, we will have to use the argument `memory = FALSE`.\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}