{
  "hash": "3f2faf6ee358f982e1984fdfdf8ab0ec",
  "result": {
    "markdown": "---\ntitle: Partitioning data with multidplyr\naliases:\n  - partition.html\nauthor: Marie-Hélène Burle\n---\n\n\n:::{.def}\n\nThe package [multidplyr](https://cran.r-project.org/web/packages/multidplyr/index.html) provides simple techniques to partition data across a set of workers on the same node.\n\n:::\n\n## Data partitioning for memory\n\n### Case example\n\nWhat if we have an even bigger dataset?\n\nThe `randomForest()` function has limitations:\n\n- It is a memory hog.\n- It doesn't run if your data frame has too many rows.\n\nIf you try to run:\n\n```{.r filename=\"bigger.R\"}\nlibrary(randomForest)\n\nbigger_iris <- iris[rep(seq_len(nrow(iris)), each = 1e3), ]\nrownames(bigger_iris) <- NULL\n\nset.seed(123)\nrf <- randomForest(Species ~ ., data = bigger_iris)\n\nrf\n```\n\non a single core, you will get:\n\n```\nrandomForest 4.7-1.1\nType rfNews() to see new features/changes/bug fixes.\n/var/spool/slurmd/job00016/slurm_script: line 5: 74451 Killed                  Rscript data_partition.R\nslurmstepd: error: Detected 1 oom-kill event(s) in StepId=16.batch. Some of your processes may have been killed by the cgroup out-of-memory handler.\n```\n\nYou have ran out of memory.\n\nReducing the number of trees won't help as the problem comes from the size of the data frame.\n\nSimilarly, using `foreach` and `doFuture` as we did previously won't help either because that spreads the number of trees on various cores, but again, the problem doesn't come from the number of trees, but for the size of the dataset.\n\n:::{.note}\n\nWith `plan(multisession)`, you would get:\n\n```\nCluster with multisession\nError in unserialize(node$con) :\n  MultisessionFuture (doFuture2-3) failed to receive message results from cluster RichSOCKnode #3 (PID 445273 on localhost ‘localhost’). The reason reported was ‘error reading from connection’. Post-mortem diagnostic: No process exists with this PID, i.e. the localhost worker is no longer alive. The total size of the 3 globals exported is 5.15 MiB. There are three globals: ‘big_iris’ (5.15 MiB of class ‘list’), ‘...future.seeds_ii’ (160 bytes of class ‘list’) and ‘...future.x_ii’ (112 bytes of class ‘list’)\n```\n\nAnd with `plan(multicore)`:\n\n```\nCluster with multicore\nError: Failed to retrieve the result of MulticoreFuture (doFuture2-2) from the forked worker (on localhost; PID 444769). Post-mortem diagnostic: No process exists with this PID, i.e. the forked localhost worker is no longer alive. The total size of the 3 globals exported is 5.15 MiB. There are three globals: ‘big_iris’ (5.15 MiB of class ‘list’), ‘...future.seeds_ii’ (160 bytes of class ‘list’) and ‘...future.x_ii’ (112 bytes of class ‘list’)\nIn addition: Warning message:\nIn mccollect(jobs = jobs, wait = TRUE) :\n  1 parallel job did not deliver a result\n```\n\n:::\n\nYou can even try spreading the trees on multiple nodes, but things will fail as well, without any error message.\n\nOf course, you could always try on a different machine—one with more memory. I used my machine which has more memory than this training cluster and it worked.\n\nBut then, what if `big_iris` is even bigger? Say, if we have this for instance:\n\n```{.r}\nbigger_iris <- iris[rep(seq_len(nrow(iris)), each = 1e4), ]\n```\n\nThen no amount of memory will save you and you will get errors similar to this:\n\n```\nError in randomForest.default(m, y, ...) : \n  long vectors (argument 28) are not supported in .C\n```\n\nThat's because `randomForest()` does not accept datasets with too many rows.\n\n:::{.info}\n\nThe bottom line is that there are situation in which the data is just too big. In such cases, you want to look at **data parallelism**: instead of splitting your code into tasks that can run in parallel as we did previously, you split the data into chunks and run the code in parallel on those chunks.\n\n:::\n\n:::{.note}\n\nOf course, you could also simply run the code on a subset of your data. In many situation, reducing your data by sampling it properly will be good enough. But there are situations in which you want to use a huge dataset.\n\n:::\n\nYou could split the data manually and run the code on each chunk, but it would be tedious and very lengthy. And to run the code on all the chunks in parallel, you could implement that yourself. There is a much simpler option provided by the [multidplyr](https://cran.r-project.org/web/packages/multidplyr/index.html) package.\n\n### Using multidplyr\n\nLet's break down the code with `multidplyr`.\n\nFirst, we load the packages that are running in the main session:\n\n```{.r}\nlibrary(multidplyr)\nlibrary(dplyr, warn.conflicts = FALSE)\n```\n\n:::{.note}\n\nWe load `dplyr` for the `do()` function.\n\nNotice that we aren't loading the `randomForest` package yet: that's because we will use it on workers, not in the main session.\n\n:::\n\nThen we need to create a cluster of workers. Let's use 4 workers that we will run on a full node:\n\n```{.r}\ncl <- new_cluster(4)\n```\n\nNow we can load the `randomForest` package on each worker:\n\n```{.r}\ncluster_library(cl, \"randomForest\")\n```\n\nOf course, we need to generate our big dataset:\n\n\n::: {.cell hash='hpc_partition_cache/html/unnamed-chunk-1_9e5694b846a345491e1cd038efb16202'}\n\n```{.r .cell-code}\nbigger_iris <- iris[rep(seq_len(nrow(iris)), each = 1e3), ]\nrownames(bigger_iris) <- NULL\n```\n:::\n\n\nThen we create a partitioned data frame on the workers with the `partition()` function. The function will try to split the data as heavenly as possible among workers.\n\nIf you group observations by some variable (with `dplyr::group_by()`) beforehand, `multidplyr` will ensure that all data points in a group end up on the same worker. This is very convenient in a lot of cases, but is not relevant here. Without grouping observations first, it is unclear how `partition()` chooses which observation goes to which worker. In our data, we have all the `setosa` observations first, then all the `versicolor`, and finally all the `virginica`. We want to make sure that the `randomForest()` function runs on a sample of all 3 species. We will thus randomly shuffle the data before partitioning it (when we were parallelizing by splitting the trees, we didn't have to worry about that since each subset of trees was running on the entire dataset):\n\n\n::: {.cell hash='hpc_partition_cache/html/unnamed-chunk-2_9a74de25f40ce618feb572ad20441bb7'}\n\n```{.r .cell-code}\nset.seed(11)\nbigger_iris_shuffled <- bigger_iris[sample(nrow(bigger_iris)), ]\n\n# The rows are now randomly shuffled:\nhead(bigger_iris_shuffled)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n65570           6.7         3.1          4.4         1.4 versicolor\n19004           5.1         3.8          1.5         0.3     setosa\n73612           6.1         2.8          4.7         1.2 versicolor\n28886           5.2         3.4          1.4         0.2     setosa\n121310          5.6         2.8          4.9         2.0  virginica\n21667           5.1         3.7          1.5         0.4     setosa\n```\n:::\n:::\n\n\n```{.r}\nsplit_iris <- partition(bigger_iris_shuffled, cl)\n```\n\nIf we want the code to be reproducible, we should set the seed on each worker:\n\n```{.r}\ncluster_send(cl, set.seed(123))\n```\n\n:::{.note}\n\nRun `cluster_send()` to send code to each worker when you aren't interested in any result (as is the case here) and `cluster_call()` if you want a computation to be executed on each worker and a result to be returned.\n\n:::\n\nNow we can run the `randomForest()` function on each worker:\n\n```{.r}\nsplit_rfs <- split_iris %>%\n  do(rf = randomForest(Species ~ ., data = .))\n```\n\n:::{.note}\n\n`split_rfs` is a partitioned data frame. You can check with `str(split_rfs)`.\n\n:::\n\nNow we need to bring the partitioned result in the main process:\n\n```{.r}\nrfs <- split_rfs %>% collect()\n```\n\n:::{.note}\n\n`rfs` is a data frame with a single column called `rf`. Which means that `rfs$rf` is a list. And each element of that list is a randomForest object (the 4 intermediate models created by the 4 workers). You can check by running `str(rfs)` and `str(rfs$df)`.\n\n:::\n\nNow, if we don't need to explore those intermediate objects, we can combined the previous 2 commands as:\n\n```{.r}\nrfs <- split_iris %>%\n  do(rf = randomForest(Species ~ ., data = .)) %>%\n  collect()\n```\n\nTo have a look at the intermediate models, simply run:\n\n```{.r}\nrfs$rf\n```\n\nFinally, we need to combine the 4 randomForest models into a single one. This can be done with the `combine()` function from the `randomForest` package (the same function we already used in our foreach expressions):\n\n```{.r}\nrf_all <- do.call(combine, rfs$rf)\n```\n\n:::{.note}\n\nBe careful that `randomForest` and `dplyr` both have a `combine()` function. The one we want here is the one from the `randomForest` package. To avoid all conflict and confusion, you can use `randomForest::combine()`. `combine()` is ok if you make sure to load `dplyr` **before** `randomForest` since latest loaded functions overwrite earlier loaded ones.\n\n:::\n\nWhy are we using `do.call()`? If we use:\n\n```{.r}\ncombine(rfs$rf)\n```\n\nWe get the silly message:\n\n```\nError in combine(rfs$rf) :\n  Argument must be a list of randomForest objects\n```\n\nThat is because `randomForest::combine()` expects a list of randomForest objects, but cannot accept an object of type list.\n\nAnd we can print our final randomForest object:\n\n```{.r}\nrf_all\n```\n\nLet's combine it all in a script (let's call it `partition.R`):\n\n```{.r filename=\"partition.R\"}\nlibrary(multidplyr)\nlibrary(dplyr, warn.conflicts = FALSE)\n\n# Create cluster of workers\ncl <- new_cluster(4)\n\n# Load randomForest on each worker\ncluster_library(cl, \"randomForest\")\n\n# Create our big data frame\nbigger_iris <- iris[rep(seq_len(nrow(iris)), each = 1e3), ]\nrownames(bigger_iris) <- NULL\n\n# Create a partitioned data frame on the workers\nsplit_iris <- partition(bigger_iris, cl)\n\n# Set the seed on each worker\ncluster_send(cl, set.seed(123))\n\n# Run the randomForest() function on each worker\nsplit_rfs <- split_iris %>%\n  do(rf = randomForest(Species ~ ., data = .))\n# split_rfs is a partitioned data frame\n\n# Bring partitioned result in the main process\nrfs <- split_rfs %>%\n  collect()\n# rfs is a normal data frame\n\n# Combine the randomForest models\nrf_all <- do.call(combine, rfs$rf)\n\n# Let's have a look at the intermediate models\ncat(\"Here are the split randomForest models:\\n\\n\")\nrfs$rf\n\n# Let's look at our final randomForest object\ncat(\"\\nAnd here is our final randomForest model:\\n\")\nrf_all\n\n# Let's test our model\ncat(\"\\nPredictions on new data:\\n\\n\")\nnew_data <- data.frame(\n  Sepal.Length = c(5.3, 4.6, 6.5),\n  Sepal.Width = c(3.1, 3.9, 2.5),\n  Petal.Length = c(1.5, 1.5, 5.0),\n  Petal.Width = c(0.2, 0.1, 2.1)\n)\n\npredict(rf_all, new_data)\n```\n\nLet's run it with the `partition.sh` script:\n\n```{.bash filename=\"reference.sh\"}\n#!/bin/bash\n#SBATCH --time=10\n#SBATCH --mem-per-cpu=7500M\n#SBATCH --cpus-per-task=4\n\nRscript partition.R\n```\n\nAfter running `sbatch partition.sh`, we get:\n\n```\nrandomForest 4.7-1.1\nType rfNews() to see new features/changes/bug fixes.\n\nAttaching package: ‘randomForest’\n\nThe following object is masked from ‘package:dplyr’:\n\n    combine\n\nHere are the split randomForest models:\n\n[[1]]\n\nCall:\n randomForest(formula = Species ~ ., data = .)\n               Type of random forest: classification\n                     Number of trees: 500\nNo. of variables tried at each split: 2\n\n        OOB estimate of  error rate: 0%\nConfusion matrix:\n           setosa versicolor virginica class.error\nsetosa      12500          0         0           0\nversicolor      0      12500         0           0\nvirginica       0          0     12500           0\n\n[[2]]\n\nCall:\n randomForest(formula = Species ~ ., data = .)\n               Type of random forest: classification\n                     Number of trees: 500\nNo. of variables tried at each split: 2\n\n        OOB estimate of  error rate: 0%\nConfusion matrix:\n           setosa versicolor virginica class.error\nsetosa      12500          0         0           0\nversicolor      0      12500         0           0\nvirginica       0          0     12500           0\n\n[[3]]\n\nCall:\n randomForest(formula = Species ~ ., data = .)\n               Type of random forest: classification\n                     Number of trees: 500\nNo. of variables tried at each split: 2\n\n        OOB estimate of  error rate: 0%\nConfusion matrix:\n           setosa versicolor virginica class.error\nsetosa      12500          0         0           0\nversicolor      0      12500         0           0\nvirginica       0          0     12500           0\n\n[[4]]\n\nCall:\n randomForest(formula = Species ~ ., data = .)\n               Type of random forest: classification\n                     Number of trees: 500\nNo. of variables tried at each split: 2\n\n        OOB estimate of  error rate: 0%\nConfusion matrix:\n           setosa versicolor virginica class.error\nsetosa      12500          0         0           0\nversicolor      0      12500         0           0\nvirginica       0          0     12500           0\n\n\nAnd here is our final randomForest model:\n\nCall:\n randomForest(formula = Species ~ ., data = .)\n               Type of random forest: classification\n                     Number of trees: 2000\nNo. of variables tried at each split: 2\n\n\nPredictions on new data:\n\n        1         2         3\n   setosa    setosa virginica\nLevels: setosa versicolor virginica\n```\n\nOf course, we don't have to print so much. The minimal script you need looks like this:\n\n```{.r filename=\"partition.R\"}\nlibrary(multidplyr)\nlibrary(dplyr, warn.conflicts = FALSE)\n\n# Create cluster of workers\ncl <- new_cluster(4)\n\n# Load randomForest on each worker\ncluster_library(cl, \"randomForest\")\n\nbigger_iris <- iris[rep(seq_len(nrow(iris)), each = 1e3), ]\nrownames(bigger_iris) <- NULL\n\n# Create a partitioned data frame on the workers\nsplit_iris <- partition(bigger_iris, cl)\n\n# Set the seed on each worker\ncluster_send(cl, set.seed(123))\n\n# Run the randomForest() function on each worker\n# and collect the results\nrfs <- split_iris %>%\n  do(rf = randomForest(Species ~ ., data = .)) %>%\n  collect()\n\n# Combine the randomForest models\nrf_all <- do.call(combine, rfs$rf)\n```\n\n### Conclusion\n\n`multidplyr` allowed us to split our data frame across multiple workers on one node and this solved the memory issues we had with our large dataset.\n\n## Data partitioning for speed\n\nBeside the memory advantage, are we getting any speedup from data parallelization? i.e. how does this code compare with the parallelization we did as regard the number of trees with `foreach` and `doFuture`?\n\nWe want to make sure to compare the same things. So we go back to our smaller `big_iris` and we up the number of trees back to 2000.\n\nWe will compare it with the plans `multisession` and `multicore` that [we performed earlier](https://mint.westdri.ca/r/hpc_foreach#multisession). The minimum and median times for these two options for shared memory parallelism were of 2.72s and 3.15s respectively.\n\n```{.r filename=\"partition_bench.R\"}\nlibrary(multidplyr)\nlibrary(dplyr, warn.conflicts = FALSE)\nlibrary(bench)\n\ncl <- new_cluster(4)\ncluster_library(cl, \"randomForest\")\n\nbig_iris <- iris[rep(seq_len(nrow(iris)), each = 1e2), ]\nrownames(big_iris) <- NULL\n\ncluster_send(cl, set.seed(123))\n\npart_rf <- function(data, cluster) {\n  split_data <- partition(data, cluster)\n  rfs <- split_data %>%\n    do(rf = randomForest(Species ~ ., data = ., ntree = 2000)) %>%\n    collect()\n  do.call(combine, rfs$rf)\n}\n\nmark(rf_all <- part_rf(big_iris, cl))\n```\n\n```{.bash filename=\"partition_bench.sh\"}\n#!/bin/bash\n#SBATCH --time=10\n#SBATCH --mem-per-cpu=7500M\n#SBATCH --cpus-per-task=4\n\nRscript partition_bench.R\n```\n\n```{.bash}\nsbatch partition_bench.sh\n```\n\n```\nrandomForest 4.7-1.1\nType rfNews() to see new features/changes/bug fixes.\n\nAttaching package: ‘randomForest’\n\nThe following object is masked from ‘package:dplyr’:\n\n    combine\n\n# A tibble: 1 × 13\n  expression      min median `itr/sec` mem_alloc `gc/sec` n_itr  n_gc total_time\n  <bch:expr>    <bch> <bch:>     <dbl> <bch:byt>    <dbl> <int> <dbl>   <bch:tm>\n1 rf_all <- pa… 2.48s  2.48s     0.403        NA     2.02     1     5      2.48s\n# ℹ 4 more variables: result <list>, memory <list>, time <list>, gc <list>\nWarning message:\nSome expressions had a GC in every iteration; so filtering is disabled.\n```\n\n:::{.note}\n\n**What about distributed memory?**\n\nCan `multidplyr` run in distributed memory? There is nothing on this in the documentation, so I tried it.\n\nI upped the number of workers to 8 and ran the code on 2 nodes with 4 cores per node and got no speedup. I also created a dataset 10 times bigger (with `each = 1e4`), which creates an OOM on 4 cores one a single node and tried it on 11 nodes with 4 cores (10 to match the 10 times size increase, plus one to play safe). This didn't solve the OOM issue. I tried various other tests, all with no success.\n\nIn conclusion, it seems that `multidply`'s way of creating a cluster of workers doesn't have a mechanism to spread them across nodes and that the package thus does not allow to split data across nodes.\n\nIn cases where your data is so big that it doesn't fit in the memory of a single node, it doesn't seem that any R package currently allow to split the data automatically for you.\n\n:::\n\n### Conclusion\n\nAs we could see, we got similar results: in this case, it is the same to spread the number of trees running on the full data on 4 cores (as we did with `foreach` and `doFuture` or to run all the trees on the data spread on 4 cores.\n\nThe difference being that `foreach` and `doFuture` allowed us to spread the trees across nodes while `multidplyr` does not allow this for the data.\n\n## Direct data loading\n\nThe method we used is very convenient, but it involves copying the data to the workers. If you want to save some memory, you can load the split data directly to the workers.\n\nFor this, first, split your data into several files and have all those files (and only those files) in a directory.\n\nThen, you can run:\n\n```{.r}\nlibrary(multidplyr)\nlibrary(dplyr)\nlibrary(vroom)\n\n# Create the cluster of workers\ncl <- new_cluster(4)\n\n# Create a character vector with the list of data files\nfiles <- dir(\"/path/to/data/directory\", full.names = TRUE)\n\n# Split up the vector amongst the workers\ncluster_assign_partition(cl, files = files)\n\n# Create a data frame called split_iris on each worker\ncluster_send(cl, split_iris <- vroom(files))\n\n# Create the partitioned data frame from the workers' data frames\nsplit_iris <- party_df(cl, \"split_iris\")\n```\n\nFrom here on, you can work as we did earlier.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}