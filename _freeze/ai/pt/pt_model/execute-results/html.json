{
  "hash": "6ae3fdbcc731771ab856268126bc6774",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Building a model\nauthor: Marie-Hélène Burle\n---\n\n:::{.def}\n\nKey to creating neural networks in PyTorch is the `torch.nn` package which contains the `nn.Module` and a `forward` method which returns an output from some input.\n\nLet's build a neural network to classify the MNIST.\n\n:::\n\nFirst, we need to define the architecture of the network. There are many types of architectures. For images, CNN are well suited.\n\nIn Python, you can define a subclass of an existing class with:\n\n```{.python}\nclass YourSubclass(BaseClass):\n    <definition of your subclass>        \n```\n\nThe subclass is derived from the base class and inherits its properties. PyTorch contains the class `torch.nn.Module` which is used as the base class when defining a neural network.\n\n::: {#932f721b .cell execution_count=1}\n``` {.python .cell-code}\n# Load packages\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Net(nn.Module):\n    # Define the architecture of the network\n    def __init__(self):\n        super(Net, self).__init__()\n        # 1 input image channel, 6 output channels,\n        # 5x5 square convolution kernel\n        self.conv1 = nn.Conv2d(1, 6, 5)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        # an affine operation: y = Wx + b\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 5*5 from image dimension\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    # Set the flow of data through the network for the forward pass\n    # x represents the data\n    def forward(self, x):\n        # Max pooling over a (2, 2) window\n        # F.relu is the rectified-linear activation function\n        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n        # If the size is a square, you can specify with a single number\n        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n        # flatten all dimensions except the batch dimension\n        x = torch.flatten(x, 1)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n```\n:::\n\n\nLet's create an instance of `Net` and print its structure:\n\n::: {#2aaa2b5c .cell execution_count=2}\n``` {.python .cell-code}\nnet = Net()\nprint(net)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNet(\n  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n  (fc1): Linear(in_features=400, out_features=120, bias=True)\n  (fc2): Linear(in_features=120, out_features=84, bias=True)\n  (fc3): Linear(in_features=84, out_features=10, bias=True)\n)\n```\n:::\n:::\n\n\n::: {#556d9ef4 .cell execution_count=3}\n``` {.python .cell-code}\nparams = list(net.parameters())\nprint(len(params))\nprint(params[0].size())  # conv1's .weight\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n10\ntorch.Size([6, 1, 5, 5])\n```\n:::\n:::\n\n\n",
    "supporting": [
      "pt_model_files"
    ],
    "filters": [],
    "includes": {}
  }
}