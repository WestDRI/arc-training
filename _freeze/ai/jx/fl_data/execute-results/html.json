{
  "hash": "2cfa828df6e7a9ceaf2e9c54b5401481",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Loading data\naliases:\n  - fl_dataset.html\nauthor: Marie-HÃ©lÃ¨ne Burle\nbibliography: fl.bib\n---\n\n:::{.def}\n\nFlax does not implement methods to load datasets since [Hugging Face](https://github.com/huggingface/datasets), [PyTorch](https://github.com/pytorch/pytorch), and [TensorFlow](https://github.com/tensorflow/datasets) already provide great APIs for this.\n\n:::\n\n## Hugging Face datasets\n\nThe [Datasets](https://github.com/huggingface/datasets) library from ðŸ¤— is a lightweight, framework-agnostic, and easy to use API to download datasets from the [Hugging Face Hub](https://huggingface.co/datasets). It uses [Apache Arrow](https://arrow.apache.org/)'s efficient caching system, allowing large datasets to be used on machines with small memory [@lhoest-etal-2021-datasets].\n\n### Search dataset\n\nGo to the [Hugging Face Hub](https://huggingface.co/datasets) and search through thousands of open source datasets provided by the community.\n\n### Inspect dataset\n\nYou can get information on a dataset before downloading it.\n\nLoad the dataset builder for the dataset you are interested in:\n\n::: {#ea59a2c7 .cell execution_count=2}\n``` {.python .cell-code}\nfrom datasets import load_dataset_builder\nds_builder = load_dataset_builder(\"mnist\")\n```\n\n::: {.cell-output .cell-output-error}\n```\nModuleNotFoundError: No module named 'datasets'\n```\n:::\n:::\n\n\nGet a description of the dataset:\n\n::: {#c5188978 .cell execution_count=3}\n``` {.python .cell-code}\nds_builder.info.description\n```\n\n::: {.cell-output .cell-output-error}\n```\nNameError: name 'ds_builder' is not defined\n```\n:::\n:::\n\n\nGet information on the features:\n\n::: {#54118a95 .cell execution_count=4}\n``` {.python .cell-code}\nds_builder.info.features\n```\n\n::: {.cell-output .cell-output-error}\n```\nNameError: name 'ds_builder' is not defined\n```\n:::\n:::\n\n\n### Download dataset and load in session\n\n```{.python}\nfrom datasets import load_dataset\n\ndef get_dataset_hf():\n    mnist = load_dataset(\"mnist\")\n\n    ds = {}\n\n    for split in ['train', 'test']:\n        ds[split] = {\n            'image': np.array([np.array(im) for im in mnist[split]['image']]),\n            'label': np.array(mnist[split]['label'])\n        }\n\n        ds[split]['image'] = jnp.float32(ds[split]['image']) / 255\n        ds[split]['label'] = jnp.int16(ds[split]['label'])\n\n        ds[split]['image'] = jnp.expand_dims(ds[split]['image'], 3)\n\n    return ds['train'], ds['test']\n```\n\n## PyTorch Torchvision datasets\n\n```{.python}\nfrom torchvision import datasets\n\ndef get_dataset_torch():\n    mnist = {\n        'train': datasets.MNIST('./data', train=True, download=True),\n        'test': datasets.MNIST('./data', train=False, download=True)\n    }\n\n    ds = {}\n\n    for split in ['train', 'test']:\n        ds[split] = {\n            'image': mnist[split].data.numpy(),\n            'label': mnist[split].targets.numpy()\n        }\n\n        ds[split]['image'] = jnp.float32(ds[split]['image']) / 255\n        ds[split]['label'] = jnp.int16(ds[split]['label'])\n\n        ds[split]['image'] = jnp.expand_dims(ds[split]['image'], 3)\n\n    return ds['train'], ds['test']\n```\n\n## TensorFlow datasets\n\n[](https://github.com/tensorflow/datasets)\n\n[](https://blog.tensorflow.org/2019/02/introducing-tensorflow-datasets.html)\n\n```{.python}\nimport tensorflow_datasets as tfds\n\ndef get_dataset_tf(epochs, batch_size):\n    mnist = tfds.builder('mnist')\n    mnist.download_and_prepare()\n\n    ds = {}\n\n    for set in ['train', 'test']:\n        ds[set] = tfds.as_numpy(mnist.as_dataset(split=set, batch_size=-1))\n\n        # cast to jnp and rescale pixel values\n        ds[set]['image'] = jnp.float32(ds[set]['image']) / 255\n        ds[set]['label'] = jnp.int16(ds[set]['label'])\n\n    return ds['train'], ds['test']\n```\n\n",
    "supporting": [
      "fl_data_files"
    ],
    "filters": [],
    "includes": {}
  }
}