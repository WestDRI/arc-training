{
  "hash": "96cc055b8297898270202ce53501d702",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Accelerated array computing and flexible differentiation with JAX\nauthor: Marie-Hélène Burle\n---\n\n:::{.def}\n\n*Content from [the webinar slides](wb_jax_slides.qmd) for easier browsing.*\n\n:::\n\n## Context\n\n### What is JAX?\n\n- Library for Python developed by Google.\n- Key data structure: Array.\n- Composition, transformation, and differentiation of numerical programs.\n- Compilation for CPUs, GPUs, and TPUs.\n- NumPy-like and lower-level APIs.\n- Requires strict functional programming.\n\n### Why JAX?\n\n```{dot}\n//| echo: false\n//| fig-height: 450px\n\nstrict digraph {\n  \nbgcolor=\"transparent\"\ngraph [fontname=\"Inconsolata, sans-serif\"]\nnode [fontname=\"Inconsolata, sans-serif\", fontsize=15]\n\n01 [label=\"Autodiff method\", shape=underline, group=g1, group=g1, fontcolor=gray55, color=gray55]\n1 [label=\"Static graph\\nand XLA\", shape=plaintext, group=g1, group=g1]\n2 [label=\"Dynamic graph\", shape=plaintext, group=g1]\n4 [label=\"Dynamic graph\\nand XLA\", shape=plaintext, group=g1]\n5 [label=\"Pseudo-dynamic\\nand XLA\", shape=plaintext, group=g1]\n\n02 [label=\"Framework\", shape=underline, group=g2, fontcolor=gray55, color=gray55]\na [label=\"TensorFlow\", shape=oval, group=g2, color=darkorange4, fontcolor=darkorange4]\nb [label=\"PyTorch\", shape=oval, group=g2, color=chocolate, fontcolor=chocolate]\nd [label=\"TensorFlow2\", shape=oval, group=g2, color=darkorange4, fontcolor=darkorange4]\ne [label=\"JAX\", shape=oval, group=g2, color=deepskyblue3, fontcolor=deepskyblue3]\n\n03 [label=Advantage, shape=underline, group=g3, fontcolor=gray55, color=gray55]\n7 [label=\"Mostly\\noptimized AD\", shape=plaintext, fontcolor=darkolivegreen, group=g3]\n8 [label=\"Convenient\", shape=plaintext, fontcolor=darkolivegreen, group=g3]\n9 [label=\"Convenient\", shape=plaintext, fontcolor=darkolivegreen, group=g3]\n10 [label=\"Convenient and\\nmostly optimized AD\", shape=plaintext, fontcolor=darkolivegreen, group=g3]\n\n04 [label=Disadvantage, shape=underline, group=g4, fontcolor=gray55, color=gray55]\nA [label=\"Manual writing of IR\", shape=plaintext, fontcolor=darkorchid2, group=g4]\nB [label=\"Limited AD optimization\", shape=plaintext, fontcolor=darkorchid2, group=g4]\nD [label=\"Disappointing speed\", shape=plaintext, fontcolor=darkorchid2, group=g4]\nE [label=\"Pure functions\", shape=plaintext, fontcolor=darkorchid2, group=g4]\n\n{rank=same; 01 02 03 04}\n{rank=same; 1 a 7 A}\n{rank=same; 2 b 8 B}\n{rank=same; 4 d 9 D}\n{rank=same; 5 e 10 E}\n\n01 -> 02 -> 03 -> 04 [style=invis]\n1 -> a -> 7 -> A [style=invis]\n2 -> b -> 8 -> B [style=invis]\n4 -> d -> 9 -> D [style=invis]\n5 -> e -> 10 -> E [style=invis]\n\n01 -> 1 [style=invis]\n1 -> 2 -> 4 -> 5 [color=gray55]\n02 -> a -> b -> d -> e [style=invis]\n03 -> 7 -> 8 -> 9 -> 10 [style=invis]\n04 -> A -> B -> D -> E [style=invis]\n\n}\n```\n\nSummarized from [a blog post](https://www.stochasticlifestyle.com/engineering-trade-offs-in-automatic-differentiation-from-tensorflow-and-pytorch-to-jax-and-julia/) by [Chris Rackauckas](https://chrisrackauckas.com/).\n\n## Getting started\n\n### Installation\n\n<br>\nInstall from pip wheels:\n\n- Personal computer: use wheels installation commands [from official site](https://jax.readthedocs.io/en/latest/installation.html).\n- Alliance clusters: `python -m pip install jax --no-index`.\n\n:::{.note}\n\nWindows: GPU support only via WSL.\n\n:::\n\n### The NumPy API\n\n:::{.panel-tabset}\n\n#### NumPy\n\n::: {#84f47a46 .cell execution_count=2}\n``` {.python .cell-code}\nimport numpy as np\n\nprint(np.array([(1, 2, 3), (4, 5, 6)]))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[1 2 3]\n [4 5 6]]\n```\n:::\n:::\n\n\n::: {#e5e5a69c .cell execution_count=3}\n``` {.python .cell-code}\nprint(np.arange(5))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[0 1 2 3 4]\n```\n:::\n:::\n\n\n::: {#7878df53 .cell execution_count=4}\n``` {.python .cell-code}\nprint(np.zeros(2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[0. 0.]\n```\n:::\n:::\n\n\n::: {#0560ef12 .cell execution_count=5}\n``` {.python .cell-code}\nprint(np.linspace(0, 2, 9))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[0.   0.25 0.5  0.75 1.   1.25 1.5  1.75 2.  ]\n```\n:::\n:::\n\n\n#### JAX NumPy\n\n```{.python}\nimport jax.numpy as jnp\n\nprint(jnp.array([(1, 2, 3), (4, 5, 6)]))\n```\n\n```\n[[1 2 3]\n [4 5 6]]\n```\n\n```{.python}\nprint(jnp.arange(5))\n```\n\n```\n[0 1 2 3 4]\n```\n\n```{.python}\nprint(jnp.zeros(2))\n```\n\n```\n[0. 0.]\n```\n\n```{.python}\nprint(jnp.linspace(0, 2, 9))\n```\n\n```\n[0.   0.25 0.5  0.75 1.   1.25 1.5  1.75 2.  ]\n```\n\n:::\n\n## But JAX NumPy is not NumPy...\n\n### Different types\n\n:::{.panel-tabset}\n\n#### Numpy\n\n::: {#16d1f230 .cell execution_count=6}\n``` {.python .cell-code}\ntype(np.zeros((2, 3)))\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\nnumpy.ndarray\n```\n:::\n:::\n\n\n#### JAX NumPy\n\n```{.python}\ntype(jnp.zeros((2, 3)))\n```\n\n```\njaxlib.xla_extension.ArrayImpl\n```\n\n:::\n\n### Different default data types\n\n:::{.panel-tabset}\n\n#### Numpy\n\n::: {#9094cdf3 .cell execution_count=7}\n``` {.python .cell-code}\nnp.zeros((2, 3)).dtype\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```\ndtype('float64')\n```\n:::\n:::\n\n\n#### JAX NumPy\n\n```{.python}\njnp.zeros((2, 3)).dtype\n```\n\n```\ndtype('float32')\n```\n\n:::{.note}\n\nStandard for DL and libraries built for accelerators. \\\nFloat64 are very slow on GPUs and not supported on TPUs.\n\n:::\n\n:::\n\n### Immutable arrays\n\n:::{.panel-tabset}\n\n#### Numpy\n\n::: {#c79707b0 .cell execution_count=8}\n``` {.python .cell-code}\na = np.arange(5)\na[0] = 9\nprint(a)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[9 1 2 3 4]\n```\n:::\n:::\n\n\n#### JAX NumPy\n\n```{.python}\na = jnp.arange(5)\na[0] = 9\n```\n\n```\nTypeError: '<class 'jaxlib.xla_extension.ArrayImpl'>' object does not support item assignment. JAX arrays are immutable.\n```\n\n```{.python}\nb = a.at[0].set(9)\nprint(b)\n```\n\n```\n[9 1 2 3 4]\n```\n\n:::\n\n### Strict input control\n\n:::{.panel-tabset}\n\n#### Numpy\n\nNumPy is easy-going:\n\n::: {#0482ebab .cell execution_count=9}\n``` {.python .cell-code}\nnp.sum([1.0, 2.0])  # argument is a list\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```\nnp.float64(3.0)\n```\n:::\n:::\n\n\n::: {#af27d8dd .cell execution_count=10}\n``` {.python .cell-code}\nnp.sum((1.0, 2.0))  # argument is a tuple\n```\n\n::: {.cell-output .cell-output-display execution_count=9}\n```\nnp.float64(3.0)\n```\n:::\n:::\n\n\n#### JAX NumPy\n\nTo avoid inefficiencies, JAX will only accept arrays:\n\n```{.python}\njnp.sum([1.0, 2.0])\n```\n\n```\nTypeError: sum requires ndarray or scalar arguments, got <class 'list'>\n```\n\n```{.python}\njnp.sum((1.0, 2.0))\n```\n\n```\nTypeError: sum requires ndarray or scalar arguments, got <class 'tuple'>\n```\n\n:::\n\n### Out of bounds indexing\n\n:::{.panel-tabset}\n\n#### Numpy\n\nNumPy will error if you index out of bounds:\n\n::: {#cbb5d83f .cell execution_count=11}\n``` {.python .cell-code}\nprint(np.arange(5)[10])\n```\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-bright-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-bright-red-fg\">IndexError</span>                                Traceback (most recent call last)\n<span class=\"ansi-bright-cyan-fg\">Cell</span><span class=\"ansi-bright-cyan-fg\"> </span><span class=\"ansi-green-fg\">In[10]</span><span class=\"ansi-green-fg\">, line 1</span>\n<span class=\"ansi-bright-green-fg\">----&gt; </span><span class=\"ansi-bright-green-fg\">1</span> <span class=\"ansi-bright-white-fg\">print</span><span class=\"ansi-bright-white-fg\">(</span><span class=\"ansi-bright-white-fg ansi-yellow-bg\">np</span><span style=\"color:rgb(255,95,135)\" class=\"ansi-yellow-bg\">.</span><span class=\"ansi-bright-white-fg ansi-yellow-bg\">arange</span><span class=\"ansi-bright-white-fg ansi-yellow-bg\">(</span><span style=\"color:rgb(175,135,255)\" class=\"ansi-yellow-bg\">5</span><span class=\"ansi-bright-white-fg ansi-yellow-bg\">)</span><span class=\"ansi-bright-white-fg ansi-yellow-bg\">[</span><span style=\"color:rgb(175,135,255)\" class=\"ansi-yellow-bg\">10</span><span class=\"ansi-bright-white-fg ansi-yellow-bg\">]</span><span class=\"ansi-bright-white-fg\">)</span>\n\n<span class=\"ansi-bright-red-fg\">IndexError</span>: index 10 is out of bounds for axis 0 with size 5</pre>\n```\n:::\n\n:::\n:::\n\n\n#### JAX NumPy\n\nJAX will silently return the closest boundary:\n\n```{.python}\nprint(jnp.arange(5)[10])\n```\n\n```\n4\n```\n\n:::\n\n### PRNG\n\nTraditional [pseudorandom number generators](https://en.wikipedia.org/wiki/Pseudorandom_number_generator) are based on nondeterministic state of OS.\n\nSlow and problematic for parallel executions.\n\nJAX relies on explicitly-set random state called a *key*:\n\n```{.python}\nfrom jax import random\n\ninitial_key = random.PRNGKey(18)\nprint(initial_key)\n```\n\n```\n[ 0 18]\n```\n\nEach key can only be used for one random function, but it can be split into new keys:\n\n```{.python}\nnew_key1, new_key2 = random.split(initial_key)\n```\n\n:::{.note}\n\n`initial_key` can't be used anymore now.\n\n:::\n\n```{.python}\nprint(new_key1)\n```\n\n```\n[4197003906 1654466292]\n```\n\n```{.python}\nprint(new_key2)\n```\n\n```\n[1685972163 1654824463]\n```\n\nWe need to keep one key to split whenever we need and we can use the other one.\n\nTo make sure we don't reuse a key by accident, it is best to overwrite the initial key with one of the new ones.\n\nHere are easier names:\n\n```{.python}\nkey = random.PRNGKey(18)\nkey, subkey = random.split(key)\n```\n\nWe can now use `subkey` to generate a random array:\n\n```{.python}\nx = random.normal(subkey, (3, 2))\n```\n\n### Benchmarking\n\nJAX uses asynchronous dispatch.\n\nInstead of waiting for a computation to complete before control returns to Python, the computation is dispatched to an accelerator and a [future](https://en.wikipedia.org/wiki/Futures_and_promises) is created.\n\nTo get proper timings, we need to make sure the future is resolved by using the `block_until_ready()` method.\n\n## JAX functioning\n\n```{dot}\n//| echo: false\n//| fig-height: 600px\n\nstrict digraph {\n\nnode [fontname=\"Inconsolata, sans-serif\"]\nedge [color=gray55]\nbgcolor=\"transparent\"\n\ntracer  [label=Tracing, shape=rectangle, color=darkviolet, fontcolor=darkviolet]\njit [label=\" Just-in-time \\n(JIT)\\ncompilation\", shape=rectangle, color=chocolate, fontcolor=chocolate]\nxla [label=\"Accelerated\\n Linear Algebra \\n(XLA)\", shape=rectangle, color=deeppink3, fontcolor=deeppink3]\ntransform [label=\" Transformations \", shape=rectangle, color=chocolate, fontcolor=chocolate]\n\nCPU [shape=octagon, color=darkslategray4, fontcolor=darkslategray4]\nGPU [shape=octagon, color=darkslategray4, fontcolor=darkslategray4]\nTPU [shape=octagon, color=darkslategray4, fontcolor=darkslategray4]\n\n\npy [label=\"Pure Python\\nfunctions\", color=gray50, fontcolor=gray50]\njaxpr [label=\"Jaxprs\\n(JAX expressions)\\nintermediate\\nrepresentation\\n(IR)\", color=gray30, fontcolor=gray30]\nhlo [label=\"High-level\\noptimized (HLO)\\nprogram\", color=gray10, fontcolor=gray10]\n\npy -> tracer [dir=none]\ntracer -> jaxpr\njaxpr -> jit [dir=none]\njit -> hlo\nhlo -> xla [dir=none]\n\nxla -> CPU [shape=doubleoctagon]\nxla -> GPU\nxla -> TPU\n\njaxpr -> transform [dir=both, minlen=3]\n{rank=same; jaxpr transform}\n\n}\n```\n\n```{dot}\n//| echo: false\n//| fig-height: 600px\n\nstrict digraph {\n\nnode [fontname=\"Inconsolata, sans-serif\"]\nedge [color=gray55]\nbgcolor=\"transparent\"\n\ntracer  [label=Tracing, shape=rectangle, color=darkviolet, fontcolor=darkviolet]\njit [label=\" Just-in-time \\n(JIT)\\ncompilation\", shape=rectangle, color=chocolate, fontcolor=chocolate]\nxla [label=\"Accelerated\\n Linear Algebra \\n(XLA)\", shape=rectangle, color=deeppink3, fontcolor=deeppink3]\ntransform [label=\"Vectorization\\nParallelization\\n   Differentiation  \", shape=rectangle, color=chocolate, fontcolor=chocolate]\n\nCPU [shape=octagon, color=darkslategray4, fontcolor=darkslategray4]\nGPU [shape=octagon, color=darkslategray4, fontcolor=darkslategray4]\nTPU [shape=octagon, color=darkslategray4, fontcolor=darkslategray4]\n\n\npy [label=\"Pure Python\\nfunctions\", color=gray50, fontcolor=gray50]\njaxpr [label=\"Jaxprs\\n(JAX expressions)\\nintermediate\\nrepresentation\\n(IR)\", color=gray30, fontcolor=gray30]\nhlo [label=\"High-level\\noptimized (HLO)\\nprogram\", color=gray10, fontcolor=gray10]\n\npy -> tracer [dir=none]\ntracer -> jaxpr\njaxpr -> jit [dir=none]\njit -> hlo\nhlo -> xla [dir=none]\n\nxla -> CPU [shape=doubleoctagon]\nxla -> GPU\nxla -> TPU\n\njaxpr -> transform [dir=both, minlen=3]\n{rank=same; jaxpr transform}\n\n}\n```\n\n## JIT compilation\n\n### JIT syntax\n\n```{.python}\nfrom jax import jit\n\nkey = random.PRNGKey(8)\nkey, subkey1, subkey2 = random.split(key, 3)\n\na = random.normal(subkey1, (500, 500))\nb = random.normal(subkey2, (500, 500))\n\ndef sum_squared_error(a, b):\n    return jnp.sum((a-b)**2)\n```\n\nOur function could simply be used as:\n\n```{.python}\nsse = sum_squared_error(a, b)\n```\n\nOur code will run faster if we create a JIT compiled version and use that instead:\n\n```{.python}\nsum_squared_error_jit = jit(sum_squared_error)\n\nsse = sum_squared_error_jit(a, b)\n```\n\nAlternatively, this can be written as:\n\n```{.python}\nsse = jit(sum_squared_error)(a, b)\n```\n\nOr with the `@jit` decorator:\n\n```{.python}\n@jit\ndef sum_squared_error(a, b):\n    return jnp.sum((a - b) ** 2)\n\nsse = sum_squared_error(a, b)\n```\n\n### Static vs traced variables\n\n```{.python}\n@jit\ndef cond_func(x):\n    if x < 0.0:\n        return x ** 2.0\n    else:\n        return x ** 3.0\n\nprint(cond_func(1.0))\n```\n\n```\njax.errors.TracerBoolConversionError: Attempted boolean conversion of traced array with shape bool[]\n```\n\nJIT compilation uses tracing of the code based on shape and dtype so that the same compiled code can be reused for new values with the same characteristics.\n\nTracer objects are not real values but abstract representation that are more general.\n\nHere, an abstract general value does not work as it wouldn't know which branch to take.\n\nOne solution is to tell `jit()` to exclude the problematic arguments from tracing ...\n\n... with arguments positions:\n\n```{.python}\ndef cond_func(x):\n    if x < 0.0:\n        return x ** 2.0\n    else:\n        return x ** 3.0\n\ncond_func_jit = jit(cond_func, static_argnums=(0,))\n\nprint(cond_func_jit(2.0))\nprint(cond_func_jit(-2.0))\n```\n\n```\n8.0\n4.0\n```\n\n... with arguments names:\n\n```{.python}\ndef cond_func(x):\n    if x < 0.0:\n        return x ** 2.0\n    else:\n        return x ** 3.0\n\ncond_func_jit_alt = jit(cond_func, static_argnames=\"x\")\n\nprint(cond_func_jit_alt(2.0))\nprint(cond_func_jit_alt(-2.0))\n```\n\n```\n8.0\n4.0\n```\n\n### Control flow primitives\n\nAnother solution, is to use one of the structured control flow primitives:\n\n```{.python}\nfrom jax import lax\n\nlax.cond(False, lambda x: x ** 2.0, lambda x: x ** 3.0, jnp.array([2.]))\n```\n\n```\nArray([8.], dtype=float32)\n```\n\n```{.python}\nlax.cond(True, lambda x: x ** 2.0, lambda x: x ** 3.0, jnp.array([-2.]))\n```\n\n```\nArray([4.], dtype=float32)\n```\n\nOther control flow primitives:\n\n- `lax.while_loop`\n- `lax.fori_loop`\n- `lax.scan`\n\nOther pseudo dynamic control flow functions:\n\n- `lax.select` (NumPy API `jnp.where` and `jnp.select`)\n- `lax.switch` (NumPy API `jnp.piecewise`)\n\n### Static vs traced operations\n\nSimilarly, you can mark problematic operations as static so that they don't get traced during JIT compilation:\n\n```{.python}\n@jit\ndef f(x):\n    return x.reshape(jnp.array(x.shape).prod())\n\nx = jnp.ones((2, 3))\nprint(f(x))\n```\n\n```\nTypeError: Shapes must be 1D sequences of concrete values of integer type, got [Traced<ShapedArray(int32[])>with<DynamicJaxprTrace(level=1/0)>]\n```\n\nThe problem here is that the shape of the argument to `prod()` depends on the value of `x` which is unknown at compilation time.\n\nOne solution is to use the NumPy version of `prod()`:\n\n```{.python}\nimport numpy as np\n\n@jit\ndef f(x):\n    return x.reshape((np.prod(x.shape)))\n\nprint(f(x))\n```\n\n```\n[1. 1. 1. 1. 1. 1.]\n```\n\n## Functionally pure functions\n\n### Jaxprs\n\n```{.python}\nimport jax\n\nx = jnp.array([1., 4., 3.])\ny = jnp.array([8., 1., 2.])\n\ndef f(x, y):\n    return 2 * x**jax.make_jaxpr(f)(x, y) \n```\n\n```\n{ lambda ; a:f32[3] b:f32[3]. let\n    c:f32[3] = integer_pow[y=2] a\n    d:f32[3] = mul 2.0 c\n    e:f32[3] = add d b\n  in (e,) }\n```\n\n### Outputs only based on inputs\n\n```{.python}\ndef f(x):\n    return a + x\n```\n\n`f` uses the variable `a` from the global environment.\n\nThe output does not solely depend on the inputs: *not a pure function*.\n\n```{.python}\na = jnp.ones(3)\nprint(a)\n```\n\n```\n[1. 1. 1.]\n```\n\n```{.python}\ndef f(x):\n    return print(jit(f)(jnp.ones(3)))\n```\n\n```\n[2. 2. 2.]\n```\n\n:::{.note}\n\nThings seem ok here because this is the first run (tracing).\n\n:::\n\nNow, let's change the value of `a` to an array of zeros:\n\n```{.python}\na = jnp.zeros(3)\nprint(a)\n```\n\n```\n[0. 0. 0.]\n```\n\nAnd rerun the same code:\n\n```{.python}\nprint(jit(f)(jnp.ones(3)))\n```\n\n```\n[2. 2. 2.]\n```\n\n:::{.note}\n\nOur cached compiled program is run and we get a wrong result.\n\n:::\n\nThe new value for `a` will only take effect if we re-trigger tracing by changing the shape and/or dtype of `x`:\n\n```{.python}\na = jnp.zeros(4)\nprint(a)\n```\n\n```\n[0. 0. 0. 0.]\n```\n\n```{.python}\nprint(jit(f)(jnp.ones(4)))\n```\n\n```\n[1. 1. 1. 1.]\n```\n\nPassing to `f()` an argument of a different shape forced retracing.\n\n### No side effects\n\nSide effects: anything beside returned output.\n\nExamples:\n\n- Printing to standard output\n- Reading from file/writing to file\n- Modifying a global variable\n\nThe side effects will happen during tracing, but not on subsequent runs. You cannot rely on side effects in your code.\n\n```{.python}\ndef f(a, b):\n    print(\"Calculating sum\")\n    return print(jit(f)(jnp.arange(3), jnp.arange(3)))\n```\n\n```\nCalculating sum\n[0 2 4]\n```\n\n:::{.note}\n\nPrinting happened here because this is the first run.\n\n:::\n\nLet's rerun the function:\n\n```{.python}\nprint(jit(f)(jnp.arange(3), jnp.arange(3)))\n```\n\n```\n[0 2 4]\n```\n\nThis time, no printing.\n\n## Other transformations\n\n### Automatic differentiation\n\nConsidering the function `f`:\n\n```{.python}\nf = lambda x: x**3 + 2*x**2 - 3*x + 8\n```\n\nWe can create a new function `dfdx` that computes the gradient of `f` w.r.t. `x`:\n\n```{.python}\nfrom jax import grad\n\ndfdx = grad(f)\n```\n\n`dfdx` returns the derivatives.\n\n```{.python}\nprint(dfdx(1.))\n```\n\n```\n4.0\n```\n\n### Composing transformations\n\nTransformations can be composed:\n\n```{.python}\nprint(jit(grad(f))(1.))\n```\n\n```\n4.0\n```\n\n```{.python}\nprint(grad(jit(f))(1.))\n```\n\n```\n4.0\n```\n\n### Forward and reverse modes\n\nOther autodiff methods:\n\n- Reverse-mode vector-Jacobian products: `jax.vjp`\n- Forward-mode Jacobian-vector products: `jax.jvp`\n\n### Higher-order differentiation\n\nWith a single variable, the `grad` function calls can be nested:\n\n```{.python}\nd2fdx = grad(dfdx)   # function to compute 2nd order derivatives\nd3fdx = grad(d2fdx)  # function to compute 3rd order derivatives\n...\n```\n\nWith several variables:\n\n- `jax.jacfwd` for forward-mode\n- `jax.jacrev` for reverse-mode\n\n### Pytrees\n\nJAX has a nested container structure: *pytree* extremely useful for DNN.\n\n### Vectorization and parallelization\n\nOther transformations for parallel run of computations across batches of arrays:\n\n- Automatic vectorization with `jax.vmap`\n- Parallelization across devices with `jax.pmap`\n\n## Pushing optimizations further\n\n### [Lax](https://jax.readthedocs.io/en/latest/jax.lax.html) API\n\n`jax.numpy` is a high-level NumPy-like API wrapped around `jax.lax`.\n\n`jax.lax` is a more efficient lower-level API itself wrapped around XLA.\n\n### [Pallas](https://jax.readthedocs.io/en/latest/pallas/index.html): extension to write GPU and TPU kernels\n\n```{dot}\n//| echo: false\n//| fig-height: 580px\n\nstrict digraph {\n\nnode [fontname=\"Inconsolata, sans-serif\"]\nedge [color=gray55]\nbgcolor=\"transparent\"\n\ntracer  [label=Tracing, shape=rectangle, color=darkviolet, fontcolor=darkviolet]\njit [label=\" Just-in-time \\n(JIT)\\ncompilation\", shape=rectangle, color=chocolate, fontcolor=chocolate]\ntriton [label=\"Triton\", shape=rectangle, color=deeppink3, fontcolor=deeppink3]\nmosaic [label=\"Mosaic\", shape=rectangle, color=deeppink3, fontcolor=deeppink3]\ntransform [label=\"Vectorization\\nParallelization\\n   Differentiation  \", shape=rectangle, color=chocolate, fontcolor=chocolate]\n\nGPU [shape=octagon, color=darkslategray4, fontcolor=darkslategray4]\nTPU [shape=octagon, color=darkslategray4, fontcolor=darkslategray4]\n\n\npy [label=\"Pure Python\\nfunctions\", color=gray50, fontcolor=gray50]\njaxpr [label=\"Jaxprs\\n(JAX expressions)\\nintermediate\\nrepresentation\\n(IR)\", color=gray30, fontcolor=gray30]\nhlo [label=\"High-level\\noptimized (HLO)\\nprogram\", color=gray10, fontcolor=gray10]\n\npy -> tracer [dir=none]\ntracer -> jaxpr\njaxpr -> jit [dir=none]\njit -> hlo\nhlo -> triton [dir=none]\nhlo -> mosaic [dir=none]\n\ntriton -> GPU [shape=doubleoctagon]\nmosaic -> TPU\n\njaxpr -> transform [dir=both, minlen=3]\n{rank=same; jaxpr transform}\n\n}\n```\n\n",
    "supporting": [
      "wb_jax_content_files"
    ],
    "filters": [],
    "includes": {}
  }
}