{
  "hash": "a839b11afbd85767bd58e7b6a30e0621",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Data preprocessing\nauthor: Marie-Hélène Burle\n---\n\n:::{.def}\n\n\n\n:::\n\n:::{.callout-note collapse=\"true\"}\n\n## Minimal necessary code from previous sections\n\n```{.python}\nbase_dir = \"<path-of-the-nabirds-dir>\"\n```\n\n:::{.note}\n\nTo be replaced by actual path.\n\n:::\n\n\n\n::: {#30941539 .cell execution_count=2}\n``` {.python .cell-code}\nimport os\nimport polars as pl\nimport imageio.v3 as iio\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\nimg_dir = os.path.join(base_dir, \"images\")\n\nbb_file = os.path.join(base_dir, \"bounding_boxes.txt\")\nclasses_translation_file = os.path.join(base_dir, \"classes_fixed.txt\")\nclass_labels_file = os.path.join(base_dir, \"image_class_labels.txt\")\nimg_file = os.path.join(base_dir, \"images.txt\")\nphotographers_file = os.path.join(base_dir, \"photographers_fixed.txt\")\nsizes_file = os.path.join(base_dir, \"sizes.txt\")\ntrain_test_split_file = os.path.join(base_dir, \"train_test_split.txt\")\n\nbb = pl.read_csv(\n    bb_file,\n    separator=\" \",\n    has_header=False,\n    new_columns=[\"UUID\", \"bb_x\", \"bb_y\", \"bb_width\", \"bb_height\"]\n)\n\nclasses = pl.read_csv(\n    class_labels_file,\n    separator=\" \",\n    has_header=False,\n    new_columns=[\"UUID\", \"class\"]\n)\n\nclasses_translation = pl.read_csv(\n    classes_translation_file,\n    separator=\" \",\n    has_header=False,\n    new_columns=[\"class\", \"id\"]\n)\n\nimg_paths = pl.read_csv(\n    img_file,\n    separator=\" \",\n    has_header=False,\n    new_columns=[\"UUID\", \"path\"]\n)\n\nphotographers = pl.read_csv(\n    photographers_file,\n    separator=\" \",\n    has_header=False,\n    new_columns=[\"UUID\", \"photographer\"]\n)\n\nsizes = pl.read_csv(\n    sizes_file,\n    separator=\" \",\n    has_header=False,\n    new_columns=[\"UUID\", \"img_width\", \"img_height\"]\n)\n\ntrain_test_split = pl.read_csv(\n    train_test_split_file,\n    separator=\" \",\n    has_header=False,\n    new_columns=[\"UUID\", \"is_training_img\"]\n)\n\nclasses_metadata = (\n    classes.join(classes_translation, on=\"class\")\n)\n\nmetadata = (\n    bb.join(classes_metadata, on=\"UUID\")\n    .join(img_paths, on=\"UUID\")\n    .join(photographers, on=\"UUID\")\n    .join(sizes, on=\"UUID\")\n    .join(train_test_split, on=\"UUID\")\n)\n\nmetadata_train = metadata.filter(pl.col(\"is_training_img\") == 1)\n\nclass NABirdsDataset:\n    \"\"\"NABirds dataset class.\"\"\"\n    def __init__(self, metadata_file, data_dir):\n        self.metadata = metadata_file\n        self.data_dir = data_dir\n    def __len__(self):\n        return len(self.metadata)\n    def __getitem__(self, idx):\n        img_path = os.path.join(\n            self.data_dir,\n            self.metadata.get_column('path')[idx]\n        )\n        img = iio.imread(img_path)\n        img_id = self.metadata.get_column('id')[idx].replace('_', ' ')\n        img_photographer = self.metadata.get_column('photographer')[idx].replace('_', ' ')\n        img_bb_x = self.metadata.get_column('bb_x')[idx]\n        img_bb_y = self.metadata.get_column('bb_y')[idx]\n        img_bb_width = self.metadata.get_column('bb_width')[idx]\n        img_bb_height = self.metadata.get_column('bb_height')[idx]\n        sample = {\n            'image': img,\n            'id': img_id,\n            'photographer': img_photographer,\n            'bbx' : img_bb_x,\n            'bby' : img_bb_y,\n            'bbwidth' : img_bb_width,\n            'bbheight' : img_bb_height\n        }\n        return sample\n\nnabirds_train = NABirdsDataset(\n    metadata_train,\n    img_dir\n)\n```\n:::\n\n\n:::\n\n## Needed transformations\n\nRaw data seldom works without being transformed.\n\nIt is a classic normalization technique to divide the pixel values of an image by 255: the color values range from 0 to 255; we bring them in the 0-1 range. This improves performance and stability in training.\n\nWe should also turn our image into a JAX array.\n\nWe also should get rid of the parts of the images that are outside of the bounding boxes containing the birds.\n\nFinally, a neural network will need images of the same size and our images come in all sorts of sizes.\n\n## Transforms\n\nDeep learning frameworks provide Transforms—classes that modify your images. You can also write your own. Here, as in the Dataset section, we will use [Grain](https://github.com/google/grain), part of the JAX AI stack, but you can use the framework of your choice, even when you use JAX.\n\nLet's first write a Transform that will normalize our images and turn them to a JAX array of dtype Float32:\n\n::: {#6147f62c .cell execution_count=3}\n``` {.python .cell-code}\nimport grain.python as grain\nimport jax.numpy as jnp\n\nclass NormAndCast(grain.MapTransform):\n    \"\"\"Transform class to normalize and cast images to float32.\"\"\"\n    def map(self, element):\n        element['image'] = jnp.array(element['image'], dtype=jnp.float32) / 255.0\n        return element\n```\n:::\n\n\nHere is another Transform that crops our images at the bounding boxes:\n\n::: {#d94ba07c .cell execution_count=4}\n``` {.python .cell-code}\nclass BbCrop(grain.MapTransform):\n    \"\"\"Transform class to crop images to their bounding boxes.\"\"\"\n    def map(self, element):\n        img = element['image']\n        bbx = element['bbx']\n        bby = element['bby']\n        bbwidth = element['bbwidth']\n        bbheight = element['bbheight']\n        img_cropped = img[bby:bby+bbheight, bbx:bbx+bbwidth]\n        element['image'] = img_cropped\n        return element\n```\n:::\n\n\nWe also need to resize the images to the same size.\n\nwhat size to chose?\n\n::: {#0b92e78f .cell execution_count=5}\n``` {.python .cell-code}\nfrom skimage.transform import resize\n\nnew_height = new_width = 224\n\nclass Resize(grain.MapTransform):\n    \"\"\"Transform class to resize all images to a given size.\"\"\"\n    def map(self, element):\n        element['image'] = resize(\n            element['image'],\n            (new_height, new_width),\n            anti_aliasing=True\n        )\n        return element\n```\n:::\n\n\nWe can combine Transforms together very easily with Grain (no need of a Compose class with Grain as with TorchVision):\n\n::: {#a4fc4a51 .cell execution_count=6}\n``` {.python .cell-code}\ntransformations = [NormAndCast(), BbCrop(), Resize()]\n```\n:::\n\n\n:::{.callout-tip collapse=\"true\"}\n\n## Equivalent using PyTorch\n\nCustom transformations can also be written in PyTorch. Here is an example for a simple normalization and casting of images:\n\n```{.python}\nclass NormAndCastPyTorch(object):\n    \"\"\"Transform class to normalize and cast images to float32.\"\"\"\n    def __call__(self, sample):\n        sample['image'] = jnp.array(sample['image'], dtype=jnp.float32) / 255.0\n        return sample\n```\n\nTo combine multiple Transforms, you use [`torchvision.transforms.Compose`](https://docs.pytorch.org/vision/0.8/transforms.html#torchvision.transforms.Compose), one of the [TorchVision Transforms](https://docs.pytorch.org/vision/0.8/transforms.html) (or you can use the newer [transforms v2](https://docs.pytorch.org/vision/main/auto_examples/transforms/plot_transforms_getting_started.html)).\n\nIn PyTorch, you would apply the Transforms as you instantiate your Dataset class since the Dataset class has a `transform` argument in which to pass them:\n\n```{.python}\nnabirds_norm_train_pytorch = NABirdsDataset(\n    metadata_train,\n    os.path.join(base_dir, img_dir),\n    transform=NormAndCastPyTorch()\n)\n```\n\nWith Grain, you pass the combined Transforms as an argument of the DataLoader class, as we will see later.\n\n:::\n\n",
    "supporting": [
      "jxai_data_files"
    ],
    "filters": [],
    "includes": {}
  }
}