{
  "hash": "8528504970ba7f2a721c305ee7b89f55",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Hyperparameters\nauthor:\n  - Marie-Hélène Burle\n  - Code adapted from JAX's [Implement ViT from scratch](https://docs.jaxstack.ai/en/latest/JAX_Vision_transformer.html)\n---\n\n:::{.def}\n\nIn this section, we set the hyperparameters that will be used during training: the optimizer, the loss function, the number of epochs, the momentum, the initial learning rate and a learning rate schedule, the training and evaluation steps, and the metrics to evaluate training.\n\n:::\n\n## Context\n\n```{dot}\n//| echo: false\n//| fig-width: 700px\n\ndigraph {\n\nbgcolor=\"transparent\"\nnode [fontname=\"Inconsolata, sans-serif\", color=gray55, fontsize=\"18pt\"]\nedge [color=gray55]\n\nload [label=\"Load data\", shape=plaintext, group=g1, fontcolor=gray55]\nproc [label=\"Process data\", shape=plaintext, group=g1, fontcolor=gray55]\nnn [label=\"Define architecture\", shape=plaintext, group=g1, fontcolor=gray55]\npretr [label=\"Pre-trained model\", shape=plaintext, group=g1, fontcolor=gray55]\nopt [label=\"Hyperparameters\", shape=plaintext, group=g1]\ntrain [label=\"Train\", shape=plaintext, group=g1, fontcolor=gray55]\ncp [label=\"Checkpoint\", shape=plaintext, group=g1, fontcolor=gray55]\n\npt [label=torchdata, fontcolor=gray55, color=gray55]\ntfds [label=tfds, group=g2, fontcolor=gray55, color=gray55]\ndt [label=datasets, fontcolor=gray55, color=gray55]\n\ngr [label=grain, fontcolor=gray55, color=gray55]\ntv [label=torchvision, fontcolor=gray55, color=gray55]\n\ntr [label=transformers, fontcolor=gray55, color=gray55]\n\nfl1 [label=flax, group=g2, fontcolor=gray55, color=gray55]\nfl2 [label=flax, group=g2, fontcolor=gray55, color=gray55]\n\noa [label=optax, group=g2, fontcolor=\"#21A89B\", color=\"#21A89B\"]\n\njx [label=\"JAX\", fontcolor=gray55, color=gray55]\n\nob [label=orbax, group=g2, fontcolor=gray55, color=gray55]\n\n{rank=same; gr load tv tr}\ngr -> load -> tv -> tr [style=invis]\n\n{rank=same; fl1 proc pretr}\nfl1 -> proc -> pretr [style=invis]\n\n{rank=same; jx fl2 opt}\nfl1 -> proc -> pretr [style=invis]\n\n{pt tfds dt} -> load [color=gray55]\n{gr tv} -> proc [color=gray55]\nfl1 -> nn [color=gray55]\npretr -> nn [dir=none]\ntr -> pretr [color=gray55]\noa -> opt [color=\"#21A89B\"]\njx -> fl2 -> train [color=gray55]\nob -> cp [color=gray55]\n\nload -> proc -> nn -> opt -> train -> cp [dir=none]\n\n}\n```\n\n:::{.callout-note collapse=\"true\"}\n\n## Minimal necessary code from previous sections\n\n::: {#e534dd4c .cell execution_count=2}\n``` {.python .cell-code}\nfrom datasets import load_dataset\nimport numpy as np\nfrom torchvision.transforms import v2 as T\nimport grain.python as grain\nimport jax\nimport jax.numpy as jnp\nfrom flax import nnx\nfrom transformers import FlaxViTForImageClassification\n\ntrain_size = 5 * 750\nval_size = 5 * 250\n\ntrain_dataset = load_dataset(\"food101\",\n                             split=f\"train[:{train_size}]\")\n\nval_dataset = load_dataset(\"food101\",\n                           split=f\"validation[:{val_size}]\")\n\nlabels_mapping = {}\nindex = 0\nfor i in range(0, len(val_dataset), 250):\n    label = val_dataset[i][\"label\"]\n    if label not in labels_mapping:\n        labels_mapping[label] = index\n        index += 1\n\ninv_labels_mapping = {v: k for k, v in labels_mapping.items()}\n\nimg_size = 224\n\ndef to_np_array(pil_image):\n  return np.asarray(pil_image.convert(\"RGB\"))\n\ndef normalize(image):\n    mean = np.array([0.5, 0.5, 0.5], dtype=np.float32)\n    std = np.array([0.5, 0.5, 0.5], dtype=np.float32)\n    image = image.astype(np.float32) / 255.0\n    return (image - mean) / std\n\ntv_train_transforms = T.Compose([\n    T.RandomResizedCrop((img_size, img_size), scale=(0.7, 1.0)),\n    T.RandomHorizontalFlip(),\n    T.ColorJitter(0.2, 0.2, 0.2),\n    T.Lambda(to_np_array),\n    T.Lambda(normalize),\n])\n\ntv_test_transforms = T.Compose([\n    T.Resize((img_size, img_size)),\n    T.Lambda(to_np_array),\n    T.Lambda(normalize),\n])\n\ndef get_transform(fn):\n    def wrapper(batch):\n        batch[\"image\"] = [\n            fn(pil_image) for pil_image in batch[\"image\"]\n        ]\n        batch[\"label\"] = [\n            labels_mapping[label] for label in batch[\"label\"]\n        ]\n        return batch\n    return wrapper\n\ntrain_transforms = get_transform(tv_train_transforms)\nval_transforms = get_transform(tv_test_transforms)\n\ntrain_dataset = train_dataset.with_transform(train_transforms)\nval_dataset = val_dataset.with_transform(val_transforms)\n\nseed = 12\ntrain_batch_size = 32\nval_batch_size = 2 * train_batch_size\n\ntrain_sampler = grain.IndexSampler(\n    len(train_dataset),\n    shuffle=True,\n    seed=seed,\n    shard_options=grain.NoSharding(),\n    num_epochs=1,\n)\n\nval_sampler = grain.IndexSampler(\n    len(val_dataset),\n    shuffle=False,\n    seed=seed,\n    shard_options=grain.NoSharding(),\n    num_epochs=1,\n)\n\ntrain_loader = grain.DataLoader(\n    data_source=train_dataset,\n    sampler=train_sampler,\n    worker_count=4,\n    worker_buffer_size=2,\n    operations=[\n        grain.Batch(train_batch_size, drop_remainder=True),\n    ]\n)\n\nval_loader = grain.DataLoader(\n    data_source=val_dataset,\n    sampler=val_sampler,\n    worker_count=4,\n    worker_buffer_size=2,\n    operations=[\n        grain.Batch(val_batch_size),\n    ]\n)\n\nclass VisionTransformer(nnx.Module):\n    def __init__(\n        self,\n        num_classes: int = 1000,\n        in_channels: int = 3,\n        img_size: int = 224,\n        patch_size: int = 16,\n        num_layers: int = 12,\n        num_heads: int = 12,\n        mlp_dim: int = 3072,\n        hidden_size: int = 768,\n        dropout_rate: float = 0.1,\n        *,\n        rngs: nnx.Rngs = nnx.Rngs(0),\n    ):\n        # Patch and position embedding\n        n_patches = (img_size // patch_size) ** 2\n        self.patch_embeddings = nnx.Conv(\n            in_channels,\n            hidden_size,\n            kernel_size=(patch_size, patch_size),\n            strides=(patch_size, patch_size),\n            padding=\"VALID\",\n            use_bias=True,\n            rngs=rngs,\n        )\n\n        initializer = jax.nn.initializers.truncated_normal(stddev=0.02)\n        self.position_embeddings = nnx.Param(\n            initializer(\n                rngs.params(),\n                (1, n_patches + 1, hidden_size),\n                jnp.float32\n            )\n        )\n        self.dropout = nnx.Dropout(dropout_rate, rngs=rngs)\n\n        self.cls_token = nnx.Param(jnp.zeros((1, 1, hidden_size)))\n\n        # Transformer Encoder blocks\n        self.encoder = nnx.Sequential(*[\n            TransformerEncoder(\n                hidden_size,\n                mlp_dim,\n                num_heads,\n                dropout_rate,\n                rngs=rngs\n            )\n            for i in range(num_layers)\n        ])\n        self.final_norm = nnx.LayerNorm(hidden_size, rngs=rngs)\n\n        # Classification head\n        self.classifier = nnx.Linear(hidden_size, num_classes, rngs=rngs)\n\n    def __call__(self, x: jax.Array) -> jax.Array:\n        # Patch and position embedding\n        patches = self.patch_embeddings(x)\n        batch_size = patches.shape[0]\n        patches = patches.reshape(batch_size, -1, patches.shape[-1])\n\n        cls_token = jnp.tile(self.cls_token, [batch_size, 1, 1])\n        x = jnp.concat([cls_token, patches], axis=1)\n        embeddings = x + self.position_embeddings\n        embeddings = self.dropout(embeddings)\n\n        # Encoder blocks\n        x = self.encoder(embeddings)\n        x = self.final_norm(x)\n\n        # fetch the first token\n        x = x[:, 0]\n\n        # Classification\n        return self.classifier(x)\n\nclass TransformerEncoder(nnx.Module):\n    def __init__(\n        self,\n        hidden_size: int,\n        mlp_dim: int,\n        num_heads: int,\n        dropout_rate: float = 0.0,\n        *,\n        rngs: nnx.Rngs = nnx.Rngs(0),\n    ) -> None:\n\n        self.norm1 = nnx.LayerNorm(hidden_size, rngs=rngs)\n        self.attn = nnx.MultiHeadAttention(\n            num_heads=num_heads,\n            in_features=hidden_size,\n            dropout_rate=dropout_rate,\n            broadcast_dropout=False,\n            decode=False,\n            deterministic=False,\n            rngs=rngs,\n        )\n        self.norm2 = nnx.LayerNorm(hidden_size, rngs=rngs)\n\n        self.mlp = nnx.Sequential(\n            nnx.Linear(hidden_size, mlp_dim, rngs=rngs),\n            nnx.gelu,\n            nnx.Dropout(dropout_rate, rngs=rngs),\n            nnx.Linear(mlp_dim, hidden_size, rngs=rngs),\n            nnx.Dropout(dropout_rate, rngs=rngs),\n        )\n\n    def __call__(self, x: jax.Array) -> jax.Array:\n        x = x + self.attn(self.norm1(x))\n        x = x + self.mlp(self.norm2(x))\n        return x\n\nmodel = VisionTransformer(num_classes=1000)\n\ntf_model = FlaxViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\n\ndef vit_inplace_copy_weights(*, src_model, dst_model):\n    assert isinstance(src_model, FlaxViTForImageClassification)\n    assert isinstance(dst_model, VisionTransformer)\n\n    tf_model_params = src_model.params\n    tf_model_params_fstate = nnx.traversals.flatten_mapping(tf_model_params)\n\n    flax_model_params = nnx.state(dst_model, nnx.Param)\n    flax_model_params_fstate = flax_model_params.flat_state()\n\n    params_name_mapping = {\n        (\"cls_token\",): (\"vit\", \"embeddings\", \"cls_token\"),\n        (\"position_embeddings\",): (\n            \"vit\",\n            \"embeddings\",\n            \"position_embeddings\"\n        ),\n        **{\n            (\"patch_embeddings\", x): (\n                \"vit\",\n                \"embeddings\",\n                \"patch_embeddings\",\n                \"projection\",\n                x\n            )\n            for x in [\"kernel\", \"bias\"]\n        },\n        **{\n            (\"encoder\", \"layers\", i, \"attn\", y, x): (\n                \"vit\",\n                \"encoder\",\n                \"layer\",\n                str(i),\n                \"attention\",\n                \"attention\",\n                y,\n                x\n            )\n            for x in [\"kernel\", \"bias\"]\n            for y in [\"key\", \"value\", \"query\"]\n            for i in range(12)\n        },\n        **{\n            (\"encoder\", \"layers\", i, \"attn\", \"out\", x): (\n                \"vit\",\n                \"encoder\",\n                \"layer\",\n                str(i),\n                \"attention\",\n                \"output\",\n                \"dense\",\n                x\n            )\n            for x in [\"kernel\", \"bias\"]\n            for i in range(12)\n        },\n        **{\n            (\"encoder\", \"layers\", i, \"mlp\", \"layers\", y1, x): (\n                \"vit\",\n                \"encoder\",\n                \"layer\",\n                str(i),\n                y2,\n                \"dense\",\n                x\n            )\n            for x in [\"kernel\", \"bias\"]\n            for y1, y2 in [(0, \"intermediate\"), (3, \"output\")]\n            for i in range(12)\n        },\n        **{\n            (\"encoder\", \"layers\", i, y1, x): (\n                \"vit\", \"encoder\", \"layer\", str(i), y2, x\n            )\n            for x in [\"scale\", \"bias\"]\n            for y1, y2 in [\n                    (\"norm1\", \"layernorm_before\"),\n                    (\"norm2\", \"layernorm_after\")\n            ]\n            for i in range(12)\n        },\n        **{\n            (\"final_norm\", x): (\"vit\", \"layernorm\", x)\n            for x in [\"scale\", \"bias\"]\n        },\n        **{\n            (\"classifier\", x): (\"classifier\", x)\n            for x in [\"kernel\", \"bias\"]\n        }\n    }\n\n    nonvisited = set(flax_model_params_fstate.keys())\n\n    for key1, key2 in params_name_mapping.items():\n        assert key1 in flax_model_params_fstate, key1\n        assert key2 in tf_model_params_fstate, (key1, key2)\n\n        nonvisited.remove(key1)\n\n        src_value = tf_model_params_fstate[key2]\n        if key2[-1] == \"kernel\" and key2[-2] in (\"key\", \"value\", \"query\"):\n            shape = src_value.shape\n            src_value = src_value.reshape((shape[0], 12, 64))\n\n        if key2[-1] == \"bias\" and key2[-2] in (\"key\", \"value\", \"query\"):\n            src_value = src_value.reshape((12, 64))\n\n        if key2[-4:] == (\"attention\", \"output\", \"dense\", \"kernel\"):\n            shape = src_value.shape\n            src_value = src_value.reshape((12, 64, shape[-1]))\n\n        dst_value = flax_model_params_fstate[key1]\n        assert src_value.shape == dst_value.value.shape, (\n            key2, src_value.shape, key1, dst_value.value.shape\n        )\n        dst_value.value = src_value.copy()\n        assert dst_value.value.mean() == src_value.mean(), (\n            dst_value.value, src_value.mean()\n        )\n\n    assert len(nonvisited) == 0, nonvisited\n\n    nnx.update(dst_model, nnx.State.from_flat_path(flax_model_params_fstate))\n\nvit_inplace_copy_weights(src_model=tf_model, dst_model=model)\n\nmodel.classifier = nnx.Linear(model.classifier.in_features, 5, rngs=nnx.Rngs(0))\n```\n:::\n\n\n:::\n\n## Load packages\n\nPackages and modules necessary for this section:\n\n::: {#504f0626 .cell execution_count=3}\n``` {.python .cell-code}\n# to set the learning rate and optimizer\nimport optax\n\n# to plot the evolution of learning rate\nimport matplotlib.pyplot as plt\n```\n:::\n\n\n## Optimizer and learning rate schedule\n\n::: {#6fad8cef .cell execution_count=4}\n``` {.python .cell-code}\nnum_epochs = 3\nlearning_rate = 0.001\nmomentum = 0.8\ntotal_steps = len(train_dataset) // train_batch_size\n\nlr_schedule = optax.linear_schedule(learning_rate, 0.0, num_epochs * total_steps)\n\niterate_subsample = np.linspace(0, num_epochs * total_steps, 100)\nplt.plot(\n    np.linspace(0, num_epochs, len(iterate_subsample)),\n    [lr_schedule(i) for i in iterate_subsample],\n    lw=3,\n)\nplt.title(\"Learning rate\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Learning rate\")\nplt.grid()\nplt.xlim((0, num_epochs))\nplt.show()\n\noptimizer = nnx.Optimizer(model, optax.sgd(lr_schedule, momentum, nesterov=True))\n```\n\n::: {.cell-output .cell-output-display}\n![](fl_hyperparameters_files/figure-html/cell-4-output-1.png){}\n:::\n:::\n\n\n## Loss function\n\n::: {#dc973687 .cell execution_count=5}\n``` {.python .cell-code}\ndef compute_losses_and_logits(model: nnx.Module, images: jax.Array, labels: jax.Array):\n    logits = model(images)\n\n    loss = optax.softmax_cross_entropy_with_integer_labels(\n        logits=logits, labels=labels\n    ).mean()\n    return loss, logits\n```\n:::\n\n\n## Train and evaluation steps\n\nThis is the part that is computationally intensive and where we want to use JAX and its efficiency. In particularly, we want to JIT-compile the functions that will do the training and evaluation.\n\nJAX requires a strictly functional programming version of Python. This is what allows its internal representations (the Jaxprs) to perform [transformations](https://docs.jax.dev/en/latest/key-concepts.html#transformations) (`jax.jit`, `jax.vmap`, `jax.pmap`, and `jax.grad` and the convenience decorators `@jax.jit`, `@jax.vmap`, `@jax.pmap`, and `@jax.grad`).\n\nFlax does not respect this anymore with the new NNX API. The JAX transformations can thus not be applied directly in Flax (as they were in the Linen API) and require [adapted versions](https://flax.readthedocs.io/en/latest/guides/jax_and_nnx_transforms.html) that handle objects' states under the hood. The NNX versions of these transformations are called `nnx.jit`, `nnx.vmap`, `nnx.pmap`, and `nnx.grad` (and the convenience decorators `@nnx.jit`, `@nnx.vmap`, `@nnx.pmap`, and `@nnx.grad`).\n\n::: {#e4283b09 .cell execution_count=6}\n``` {.python .cell-code}\n@nnx.jit\ndef train_step(\n    model: nnx.Module, optimizer: nnx.Optimizer, batch: dict[str, np.ndarray]\n):\n    # Convert np.ndarray to jax.Array on GPU\n    images = jnp.array(batch[\"image\"])\n    labels = jnp.array(batch[\"label\"], dtype=jnp.int32)\n\n    grad_fn = nnx.value_and_grad(compute_losses_and_logits, has_aux=True)\n    (loss, logits), grads = grad_fn(model, images, labels)\n\n    optimizer.update(grads)  # In-place updates.\n\n    return loss\n\n@nnx.jit\ndef eval_step(\n    model: nnx.Module, batch: dict[str, np.ndarray], eval_metrics: nnx.MultiMetric\n):\n    # Convert np.ndarray to jax.Array on GPU\n    images = jnp.array(batch[\"image\"])\n    labels = jnp.array(batch[\"label\"], dtype=jnp.int32)\n    loss, logits = compute_losses_and_logits(model, images, labels)\n\n    eval_metrics.update(\n        loss=loss,\n        logits=logits,\n        labels=labels,\n    )\n```\n:::\n\n\n## Training metrics\n\n::: {#b5e1c1b3 .cell execution_count=7}\n``` {.python .cell-code}\neval_metrics = nnx.MultiMetric(\n    loss=nnx.metrics.Average('loss'),\n    accuracy=nnx.metrics.Accuracy(),\n)\n\ntrain_metrics_history = {\n    \"train_loss\": [],\n}\n\neval_metrics_history = {\n    \"val_loss\": [],\n    \"val_accuracy\": [],\n}\n```\n:::\n\n\n",
    "supporting": [
      "fl_hyperparameters_files"
    ],
    "filters": [],
    "includes": {}
  }
}