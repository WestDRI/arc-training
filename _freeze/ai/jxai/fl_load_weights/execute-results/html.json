{
  "hash": "2a61e550b06ae260050a77bb82f6aeab",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Loading pre-trained weights\nbibliography: fl.bib\ncsl: diabetologia.csl\nauthor:\n  - Marie-Hélène Burle\n  - Code adapted from JAX's [Implement ViT from scratch](https://docs.jaxstack.ai/en/latest/JAX_Vision_transformer.html)\n---\n\n:::{.def}\n\n\n:::\n\n## Context\n\n```{dot}\n//| echo: false\n//| fig-width: 700px\n\ndigraph {\n\nbgcolor=\"transparent\"\nnode [fontname=\"Inconsolata, sans-serif\", color=gray55, fontsize=\"18pt\"]\nedge [color=gray55]\n\nload [label=\"Load data\", shape=plaintext, group=g1, fontcolor=gray55]\nproc [label=\"Process data\", shape=plaintext, group=g1, fontcolor=gray55]\nnn [label=\"Define architecture\", shape=plaintext, group=g1, fontcolor=gray55]\npretr [label=\"Pre-trained model\", shape=plaintext, group=g1]\nopt [label=\"Hyperparameters\", shape=plaintext, group=g1, fontcolor=gray55]\ntrain [label=\"Train\", shape=plaintext, group=g1, fontcolor=gray55]\ncp [label=\"Checkpoint\", shape=plaintext, group=g1, fontcolor=gray55]\n\npt [label=torchdata, fontcolor=gray55, color=gray55]\ntfds [label=tfds, group=g2, fontcolor=gray55, color=gray55]\ndt [label=datasets, fontcolor=gray55, color=gray55]\n\ngr [label=grain, fontcolor=gray55, color=gray55]\ntv [label=torchvision, fontcolor=gray55, color=gray55]\n\ntr [label=transformers, fontcolor=\"#669900\", color=\"#669900\"]\n\nfl1 [label=flax, group=g2, fontcolor=gray55, color=gray55]\nfl2 [label=flax, group=g2, fontcolor=gray55, color=gray55]\n\noa [label=optax, group=g2, fontcolor=gray55, color=gray55]\n\njx [label=\"JAX\", fontcolor=gray55, color=gray55]\n\nob [label=orbax, group=g2, fontcolor=gray55, color=gray55]\n\n{rank=same; gr load tv tr}\ngr -> load -> tv -> tr [style=invis]\n\n{rank=same; fl1 proc pretr}\nfl1 -> proc -> pretr [style=invis]\n\n{rank=same; jx fl2 opt}\nfl1 -> proc -> pretr [style=invis]\n\n{pt tfds dt} -> load [color=gray55]\n{gr tv} -> proc [color=gray55]\nfl1 -> nn [color=gray55]\npretr -> nn [dir=none]\ntr -> pretr [color=\"#669900\"]\noa -> opt [color=gray55]\njx -> fl2 -> train [color=gray55]\nob -> cp [color=gray55]\n\nload -> proc -> nn -> opt -> train -> cp [dir=none]\n\n}\n```\n\n:::{.callout-note collapse=\"true\"}\n\n## Minimal necessary code from previous sections\n\n::: {#713dd48d .cell execution_count=1}\n``` {.python .cell-code}\nimport jax\nimport jax.numpy as jnp\nfrom flax import nnx\n\nclass VisionTransformer(nnx.Module):\n    def __init__(\n        self,\n        num_classes: int = 1000,\n        in_channels: int = 3,\n        img_size: int = 224,\n        patch_size: int = 16,\n        num_layers: int = 12,\n        num_heads: int = 12,\n        mlp_dim: int = 3072,\n        hidden_size: int = 768,\n        dropout_rate: float = 0.1,\n        *,\n        rngs: nnx.Rngs = nnx.Rngs(0),\n    ):\n        # Patch and position embedding\n        n_patches = (img_size // patch_size) ** 2\n        self.patch_embeddings = nnx.Conv(\n            in_channels,\n            hidden_size,\n            kernel_size=(patch_size, patch_size),\n            strides=(patch_size, patch_size),\n            padding=\"VALID\",\n            use_bias=True,\n            rngs=rngs,\n        )\n\n        initializer = jax.nn.initializers.truncated_normal(stddev=0.02)\n        self.position_embeddings = nnx.Param(\n            initializer(\n                rngs.params(),\n                (1, n_patches + 1, hidden_size),\n                jnp.float32\n            )\n        )\n        self.dropout = nnx.Dropout(dropout_rate, rngs=rngs)\n\n        self.cls_token = nnx.Param(jnp.zeros((1, 1, hidden_size)))\n\n        # Transformer Encoder blocks\n        self.encoder = nnx.Sequential(*[\n            TransformerEncoder(\n                hidden_size,\n                mlp_dim,\n                num_heads,\n                dropout_rate,\n                rngs=rngs\n            )\n            for i in range(num_layers)\n        ])\n        self.final_norm = nnx.LayerNorm(hidden_size, rngs=rngs)\n\n        # Classification head\n        self.classifier = nnx.Linear(hidden_size, num_classes, rngs=rngs)\n\n    def __call__(self, x: jax.Array) -> jax.Array:\n        # Patch and position embedding\n        patches = self.patch_embeddings(x)\n        batch_size = patches.shape[0]\n        patches = patches.reshape(batch_size, -1, patches.shape[-1])\n\n        cls_token = jnp.tile(self.cls_token, [batch_size, 1, 1])\n        x = jnp.concat([cls_token, patches], axis=1)\n        embeddings = x + self.position_embeddings\n        embeddings = self.dropout(embeddings)\n\n        # Encoder blocks\n        x = self.encoder(embeddings)\n        x = self.final_norm(x)\n\n        # fetch the first token\n        x = x[:, 0]\n\n        # Classification\n        return self.classifier(x)\n\nclass TransformerEncoder(nnx.Module):\n    def __init__(\n        self,\n        hidden_size: int,\n        mlp_dim: int,\n        num_heads: int,\n        dropout_rate: float = 0.0,\n        *,\n        rngs: nnx.Rngs = nnx.Rngs(0),\n    ) -> None:\n\n        self.norm1 = nnx.LayerNorm(hidden_size, rngs=rngs)\n        self.attn = nnx.MultiHeadAttention(\n            num_heads=num_heads,\n            in_features=hidden_size,\n            dropout_rate=dropout_rate,\n            broadcast_dropout=False,\n            decode=False,\n            deterministic=False,\n            rngs=rngs,\n        )\n        self.norm2 = nnx.LayerNorm(hidden_size, rngs=rngs)\n\n        self.mlp = nnx.Sequential(\n            nnx.Linear(hidden_size, mlp_dim, rngs=rngs),\n            nnx.gelu,\n            nnx.Dropout(dropout_rate, rngs=rngs),\n            nnx.Linear(mlp_dim, hidden_size, rngs=rngs),\n            nnx.Dropout(dropout_rate, rngs=rngs),\n        )\n\n    def __call__(self, x: jax.Array) -> jax.Array:\n        x = x + self.attn(self.norm1(x))\n        x = x + self.mlp(self.norm2(x))\n        return x\n\nmodel = VisionTransformer(num_classes=1000)\n```\n:::\n\n\n:::\n\n## Load packages\n\nPackages and modules necessary for this section:\n\n::: {#5a51f4c2 .cell execution_count=2}\n``` {.python .cell-code}\n# Hugging Face ViT Model transformer with image classification head\nfrom transformers import FlaxViTForImageClassification\n\n# Packages to test our model after weight transfer\nimport matplotlib.pyplot as plt\nfrom transformers import ViTImageProcessor\nfrom PIL import Image\nimport requests\n```\n:::\n\n\n[FlaxViTForImageClassification](https://huggingface.co/docs/transformers/v4.51.3/en/model_doc/vit#transformers.FlaxViTForImageClassification) instantiates a pretrained Flax model with an image classification head from a pre-trained ViT model configuration.\n\n## Load pre-trained weights\n\nWe want to load the weights from [Google's ViT model pre-trained on ImageNet-21k at resolution 224x224 and fine-tuned on ImageNet 2012 at resolution 224x224](https://github.com/google-research/vision_transformer) introduced by Dosovitskiy et al. [-@dosovitskiy2021imageworth16x16words] in our model.\n\nFor this, we use the `from_pretrained` method of `FlaxViTForImageClassification` and get the weights from Google's model stored as [google/vit-base-patch16-224](https://huggingface.co/google/vit-base-patch16-224) on the Hugging Face model Hub.\n\n::: {#8a3d283f .cell execution_count=3}\n``` {.python .cell-code}\ntf_model = FlaxViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\n```\n:::\n\n\n## Copy weights to our model\n\n`tf_model` is a transformer ViT model with the pre-trained weights. We want to copy those weights to our ViT Flax model called `model`:\n\n::: {#d3dfaa76 .cell execution_count=4}\n``` {.python .cell-code}\ndef vit_inplace_copy_weights(*, src_model, dst_model):\n    assert isinstance(src_model, FlaxViTForImageClassification)\n    assert isinstance(dst_model, VisionTransformer)\n\n    tf_model_params = src_model.params\n    tf_model_params_fstate = nnx.traversals.flatten_mapping(tf_model_params)\n\n    flax_model_params = nnx.state(dst_model, nnx.Param)\n    flax_model_params_fstate = flax_model_params.flat_state()\n\n    params_name_mapping = {\n        (\"cls_token\",): (\"vit\", \"embeddings\", \"cls_token\"),\n        (\"position_embeddings\",): (\n            \"vit\",\n            \"embeddings\",\n            \"position_embeddings\"\n        ),\n        **{\n            (\"patch_embeddings\", x): (\n                \"vit\",\n                \"embeddings\",\n                \"patch_embeddings\",\n                \"projection\",\n                x\n            )\n            for x in [\"kernel\", \"bias\"]\n        },\n        **{\n            (\"encoder\", \"layers\", i, \"attn\", y, x): (\n                \"vit\",\n                \"encoder\",\n                \"layer\",\n                str(i),\n                \"attention\",\n                \"attention\",\n                y,\n                x\n            )\n            for x in [\"kernel\", \"bias\"]\n            for y in [\"key\", \"value\", \"query\"]\n            for i in range(12)\n        },\n        **{\n            (\"encoder\", \"layers\", i, \"attn\", \"out\", x): (\n                \"vit\",\n                \"encoder\",\n                \"layer\",\n                str(i),\n                \"attention\",\n                \"output\",\n                \"dense\",\n                x\n            )\n            for x in [\"kernel\", \"bias\"]\n            for i in range(12)\n        },\n        **{\n            (\"encoder\", \"layers\", i, \"mlp\", \"layers\", y1, x): (\n                \"vit\",\n                \"encoder\",\n                \"layer\",\n                str(i),\n                y2,\n                \"dense\",\n                x\n            )\n            for x in [\"kernel\", \"bias\"]\n            for y1, y2 in [(0, \"intermediate\"), (3, \"output\")]\n            for i in range(12)\n        },\n        **{\n            (\"encoder\", \"layers\", i, y1, x): (\n                \"vit\", \"encoder\", \"layer\", str(i), y2, x\n            )\n            for x in [\"scale\", \"bias\"]\n            for y1, y2 in [\n                    (\"norm1\", \"layernorm_before\"),\n                    (\"norm2\", \"layernorm_after\")\n            ]\n            for i in range(12)\n        },\n        **{\n            (\"final_norm\", x): (\"vit\", \"layernorm\", x)\n            for x in [\"scale\", \"bias\"]\n        },\n        **{\n            (\"classifier\", x): (\"classifier\", x)\n            for x in [\"kernel\", \"bias\"]\n        }\n    }\n\n    nonvisited = set(flax_model_params_fstate.keys())\n\n    for key1, key2 in params_name_mapping.items():\n        assert key1 in flax_model_params_fstate, key1\n        assert key2 in tf_model_params_fstate, (key1, key2)\n\n        nonvisited.remove(key1)\n\n        src_value = tf_model_params_fstate[key2]\n        if key2[-1] == \"kernel\" and key2[-2] in (\"key\", \"value\", \"query\"):\n            shape = src_value.shape\n            src_value = src_value.reshape((shape[0], 12, 64))\n\n        if key2[-1] == \"bias\" and key2[-2] in (\"key\", \"value\", \"query\"):\n            src_value = src_value.reshape((12, 64))\n\n        if key2[-4:] == (\"attention\", \"output\", \"dense\", \"kernel\"):\n            shape = src_value.shape\n            src_value = src_value.reshape((12, 64, shape[-1]))\n\n        dst_value = flax_model_params_fstate[key1]\n        assert src_value.shape == dst_value.value.shape, (\n            key2, src_value.shape, key1, dst_value.value.shape\n        )\n        dst_value.value = src_value.copy()\n        assert dst_value.value.mean() == src_value.mean(), (\n            dst_value.value, src_value.mean()\n        )\n\n    assert len(nonvisited) == 0, nonvisited\n\n    nnx.update(dst_model, nnx.State.from_flat_path(flax_model_params_fstate))\n\nvit_inplace_copy_weights(src_model=tf_model, dst_model=model)\n```\n:::\n\n\n## Test our model\n\nOur model should now be able to classify objects if they belong to the 1000 classes of ImageNet-1K.\n\nLet's test it by passing [the URL of the image of a Song Sparrow (Melospiza melodia)](https://www.allaboutbirds.org/guide/assets/photo/308771371-480px.jpg):\n\n::: {#99bf714e .cell execution_count=5}\n``` {.python .cell-code}\nurl = \"https://www.allaboutbirds.org/guide/assets/photo/308771371-480px.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\nprocessor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n\ninputs = processor(images=image, return_tensors=\"np\")\noutputs = tf_model(**inputs)\nlogits = outputs.logits\n\nmodel.eval()\nx = jnp.transpose(inputs[\"pixel_values\"], axes=(0, 2, 3, 1))\noutput = model(x)\n\n# Model predicts one of the 1000 ImageNet classes.\nref_class_idx = logits.argmax(-1).item()\npred_class_idx = output.argmax(-1).item()\nassert jnp.abs(logits[0, :] - output[0, :]).max() < 0.1\n\nfig, axs = plt.subplots(1, 2, figsize=(12, 8))\naxs[0].set_title(\n    f\"Reference model:\\n{tf_model.config.id2label[ref_class_idx]}\\nP={nnx.softmax(logits, axis=-1)[0, ref_class_idx]:.3f}\"\n)\naxs[0].imshow(image)\naxs[1].set_title(\n    f\"Our model:\\n{tf_model.config.id2label[pred_class_idx]}\\nP={nnx.softmax(output, axis=-1)[0, pred_class_idx]:.3f}\"\n)\naxs[1].imshow(image)\n```\n\n::: {.cell-output .cell-output-display}\n![](fl_load_weights_files/figure-html/cell-6-output-1.png){}\n:::\n:::\n\n\nSo, the [Song Sparrow](https://en.wikipedia.org/wiki/Song_sparrow) is not in the 1000 classes. But the good news is that our model with the transferred weights gave exactly the same result ([Brambling](https://en.wikipedia.org/wiki/Brambling)) as the `google/vit-base-patch16-224` model, with the same probability. So all looks good.\n\n## Reduce number of classes\n\nOur model now returns 1000 categories, but we want to fine-tune it on the [Food-101 dataset](https://data.vision.ee.ethz.ch/cvl/datasets_extra/food-101/) [@bossard14] that we have reduced to only 5 classes. So we need to replace the model classifier with one returning 5 classes:\n\n::: {#05f67ac3 .cell execution_count=6}\n``` {.python .cell-code}\nmodel.classifier = nnx.Linear(model.classifier.in_features, 5, rngs=nnx.Rngs(0))\n\nx = jnp.ones((4, 224, 224, 3))\ny = model(x)\nprint(\"Predictions shape: \", y.shape)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPredictions shape:  (4, 5)\n```\n:::\n:::\n\n\n",
    "supporting": [
      "fl_load_weights_files"
    ],
    "filters": [],
    "includes": {}
  }
}