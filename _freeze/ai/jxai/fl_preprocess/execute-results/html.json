{
  "hash": "6230d452b0a2dd2a737172657bfea41b",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Preprocessing data\nbibliography: fl.bib\ncsl: diabetologia.csl\nauthor:\n  - Marie-Hélène Burle\n  - Code adapted from JAX's [Implement ViT from scratch](https://docs.jaxstack.ai/en/latest/JAX_Vision_transformer.html)\n---\n\n:::{.def}\n\nThis section covers an example of the second step of a classic workflow: preprocessing the data.\n\n:::\n\n## Context\n\nThere are many tools and options. In this example, we use [TorchVision](https://github.com/pytorch/pytorch) to transform and augment images and [Grain](https://github.com/google/grain) to create data loaders.\n\n```{dot}\n//| echo: false\n//| fig-width: 700px\n\ndigraph {\n\nbgcolor=\"transparent\"\nnode [fontname=\"Inconsolata, sans-serif\", color=gray55, fontsize=\"18pt\"]\nedge [color=gray55]\n\nload [label=\"Load data\", shape=plaintext, group=g1, fontcolor=gray55]\nproc [label=\"Process data\", shape=plaintext, group=g1]\nnn [label=\"Define architecture\", shape=plaintext, group=g1, fontcolor=gray55]\npretr [label=\"Pre-trained model\", shape=plaintext, group=g1, fontcolor=gray55]\nopt [label=\"Hyperparameters\", shape=plaintext, group=g1, fontcolor=gray55]\ntrain [label=\"Train\", shape=plaintext, group=g1, fontcolor=gray55]\ncp [label=\"Checkpoint\", shape=plaintext, group=g1, fontcolor=gray55]\n\npt [label=torchdata, fontcolor=gray55, color=gray55]\ntfds [label=tfds, group=g2, fontcolor=gray55, color=gray55]\ndt [label=datasets, fontcolor=gray55, color=gray55]\n\ngr [label=grain, fontcolor=orangered3, color=orangered3]\ntv [label=torchvision, fontcolor=orangered3, color=orangered3]\n\ntr [label=transformers, fontcolor=gray55, color=gray55]\n\nfl1 [label=flax, group=g2, fontcolor=gray55, color=gray55]\nfl2 [label=flax, group=g2, fontcolor=gray55, color=gray55]\n\noa [label=optax, group=g2, fontcolor=gray55, color=gray55]\n\njx [label=jax, group=g2, fontcolor=gray55, color=gray55]\n\nob [label=orbax, group=g2, fontcolor=gray55, color=gray55]\n\n{rank=same; gr load tv tr}\ngr -> load -> tv -> tr [style=invis]\n\n{rank=same; fl1 proc pretr}\nfl1 -> proc -> pretr [style=invis]\n\n{rank=same; jx fl2 opt}\nfl1 -> proc -> pretr [style=invis]\n\n{pt tfds dt} -> load [color=gray55]\n{gr tv} -> proc [color=orangered3]\nfl1 -> nn [color=gray55]\npretr -> nn [dir=none]\ntr -> pretr [color=gray55]\noa -> opt [color=gray55]\njx -> fl2 -> train [color=gray55]\nob -> cp [color=gray55]\n\nload -> proc -> nn -> opt -> train -> cp [dir=none]\n\n}\n```\n\n:::{.callout-note collapse=\"true\"}\n\n## Minimal necessary code from previous section\n\n::: {#162ae7ef .cell execution_count=1}\n``` {.python .cell-code}\nfrom datasets import load_dataset\nimport matplotlib.pyplot as plt\n\ntrain_size = 5 * 750\nval_size = 5 * 250\n\ntrain_dataset = load_dataset(\"food101\",\n                             split=f\"train[:{train_size}]\")\n\nval_dataset = load_dataset(\"food101\",\n                           split=f\"validation[:{val_size}]\")\n\nlabels_mapping = {}\nindex = 0\nfor i in range(0, len(val_dataset), 250):\n    label = val_dataset[i][\"label\"]\n    if label not in labels_mapping:\n        labels_mapping[label] = index\n        index += 1\n\ninv_labels_mapping = {v: k for k, v in labels_mapping.items()}\n\ndef display_datapoints(*datapoints, tag=\"\", names_map=None):\n    num_samples = len(datapoints)\n\n    fig, axs = plt.subplots(1, num_samples, figsize=(20, 10))\n    for i, datapoint in enumerate(datapoints):\n        if isinstance(datapoint, dict):\n            img, label = datapoint[\"image\"], datapoint[\"label\"]\n        else:\n            img, label = datapoint\n\n        if hasattr(img, \"dtype\") and img.dtype in (np.float32, ):\n            img = ((img - img.min()) / (img.max() - img.min()) * 255.0).astype(np.uint8)\n\n        label_str = f\" ({names_map[label]})\" if names_map is not None else \"\"\n        axs[i].set_title(f\"{tag} Label: {label}{label_str}\")\n        axs[i].imshow(img)\n```\n:::\n\n\n:::\n\n## Load packages\n\nPackages necessary for this section:\n\n::: {#3e00053e .cell execution_count=2}\n``` {.python .cell-code}\n# general array manipulation\nimport numpy as np\n\n# for image transformation and augmentation\nfrom torchvision.transforms import v2 as T\n\n# to create data loaders\nimport grain.python as grain\n```\n:::\n\n\n## Data normalization and augmentation\n\nLet's preprocess our images to match the methods used in the [vision transformer (ViT)](https://en.wikipedia.org/wiki/Vision_transformer) introduced by Dosovitskiy A et al. [-@dosovitskiy2021imageworth16x16words] and [implemented in JAX](https://github.com/google-research/vision_transformer/). This will be useful when we fine tune this model with the Food dataset in another section.\n\nThe preprocessing involves normalization and random augmentation (to prevent overfitting) with [TorchVision](https://github.com/pytorch/pytorch):\n\n::: {#2eb9b491 .cell execution_count=3}\n``` {.python .cell-code}\nimg_size = 224\n\ndef to_np_array(pil_image):\n  return np.asarray(pil_image.convert(\"RGB\"))\n\ndef normalize(image):\n    mean = np.array([0.5, 0.5, 0.5], dtype=np.float32)\n    std = np.array([0.5, 0.5, 0.5], dtype=np.float32)\n    image = image.astype(np.float32) / 255.0\n    return (image - mean) / std\n\ntv_train_transforms = T.Compose([\n    T.RandomResizedCrop((img_size, img_size), scale=(0.7, 1.0)),\n    T.RandomHorizontalFlip(),\n    T.ColorJitter(0.2, 0.2, 0.2),\n    T.Lambda(to_np_array),\n    T.Lambda(normalize),\n])\n\ntv_test_transforms = T.Compose([\n    T.Resize((img_size, img_size)),\n    T.Lambda(to_np_array),\n    T.Lambda(normalize),\n])\n\ndef get_transform(fn):\n    def wrapper(batch):\n        batch[\"image\"] = [\n            fn(pil_image) for pil_image in batch[\"image\"]\n        ]\n        batch[\"label\"] = [\n            labels_mapping[label] for label in batch[\"label\"]\n        ]\n        return batch\n    return wrapper\n\ntrain_transforms = get_transform(tv_train_transforms)\nval_transforms = get_transform(tv_test_transforms)\n\ntrain_dataset = train_dataset.with_transform(train_transforms)\nval_dataset = val_dataset.with_transform(val_transforms)\n```\n:::\n\n\n## Data loaders\n\nWe use [Grain](https://github.com/google/grain) to create efficient data loaders:\n\n::: {#c3693676 .cell execution_count=4}\n``` {.python .cell-code}\nseed = 12\ntrain_batch_size = 32\nval_batch_size = 2 * train_batch_size\n\ntrain_sampler = grain.IndexSampler(\n    len(train_dataset),\n    shuffle=True,\n    seed=seed,\n    shard_options=grain.NoSharding(),\n    num_epochs=1,\n)\n\nval_sampler = grain.IndexSampler(\n    len(val_dataset),\n    shuffle=False,\n    seed=seed,\n    shard_options=grain.NoSharding(),\n    num_epochs=1,\n)\n\ntrain_loader = grain.DataLoader(\n    data_source=train_dataset,\n    sampler=train_sampler,\n    worker_count=4,\n    worker_buffer_size=2,\n    operations=[\n        grain.Batch(train_batch_size, drop_remainder=True),\n    ]\n)\n\nval_loader = grain.DataLoader(\n    data_source=val_dataset,\n    sampler=val_sampler,\n    worker_count=4,\n    worker_buffer_size=2,\n    operations=[\n        grain.Batch(val_batch_size),\n    ]\n)\n```\n:::\n\n\n## Inspect batches\n\n::: {#d5539027 .cell execution_count=5}\n``` {.python .cell-code}\ntrain_batch = next(iter(train_loader))\nval_batch = next(iter(val_loader))\n\nprint(\n    \"Training batch info:\",\n      train_batch[\"image\"].shape,\n      train_batch[\"image\"].dtype,\n      train_batch[\"label\"].shape,\n      train_batch[\"label\"].dtype\n)\n\nprint(\n    \"Validation batch info:\",\n      val_batch[\"image\"].shape,\n      val_batch[\"image\"].dtype,\n      val_batch[\"label\"].shape,\n      val_batch[\"label\"].dtype\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTraining batch info: (32, 224, 224, 3) float32 (32,) int64\nValidation batch info: (64, 224, 224, 3) float32 (64,) int64\n```\n:::\n:::\n\n\nDisplay the first three training and validation items:\n\n::: {#c02739c5 .cell execution_count=6}\n``` {.python .cell-code}\ndisplay_datapoints(\n    *[(train_batch[\"image\"][i], train_batch[\"label\"][i]) for i in range(3)],\n    tag=\"(Training) \",\n    names_map={\n        k: train_dataset.features[\"label\"].names[v]\n               for k, v in inv_labels_mapping.items()\n    }\n)\n\ndisplay_datapoints(\n    *[(val_batch[\"image\"][i], val_batch[\"label\"][i]) for i in range(3)],\n    tag=\"(Validation) \",\n    names_map={\n        k: val_dataset.features[\"label\"].names[v]\n               for k, v in inv_labels_mapping.items()\n    }\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/tmp/ipykernel_57435/1098619187.py:34: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n  img = ((img - img.min()) / (img.max() - img.min()) * 255.0).astype(np.uint8)\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](fl_preprocess_files/figure-html/cell-7-output-2.png){}\n:::\n\n::: {.cell-output .cell-output-display}\n![](fl_preprocess_files/figure-html/cell-7-output-3.png){}\n:::\n:::\n\n\n",
    "supporting": [
      "fl_preprocess_files"
    ],
    "filters": [],
    "includes": {}
  }
}