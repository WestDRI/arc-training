{
  "hash": "1941dd99e9bdcee9f08cf7fea30dde90",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Preprocessing data\nbibliography: fl.bib\ncsl: diabetologia.csl\nauthor:\n  - Marie-Hélène Burle\n  - Code adapted from JAX's [Implement ViT from scratch](https://docs.jaxstack.ai/en/latest/JAX_Vision_transformer.html)\n---\n\n:::{.def}\n\nThis section covers an example of the second step of a classic workflow: preprocessing the data.\n\n:::\n\n## Context\n\nThere are many tools and options. In this example, we use [TorchVision](https://github.com/pytorch/pytorch) to transform and augment images and [Grain](https://github.com/google/grain) to create data loaders.\n\n```{dot}\n//| echo: false\n//| fig-width: 700px\n\ndigraph {\n\nbgcolor=\"transparent\"\nnode [fontname=\"Inconsolata, sans-serif\", color=gray55]\nedge [color=gray55]\n\nload [label=\"Load data\", shape=plaintext, group=g1, fontcolor=gray55]\nproc [label=\"Process data\", shape=plaintext, group=g1]\nnn [label=\"Define architecture\", shape=plaintext, group=g1, fontcolor=gray55]\npretr [label=\"Pre-trained model\", shape=plaintext, group=g1, fontcolor=gray55]\nopt [label=\"Optimize\", shape=plaintext, group=g1, fontcolor=gray55]\ncp [label=\"Checkpoint\", shape=plaintext, group=g1, fontcolor=gray55]\n\npt [label=torchdata, fontcolor=gray55, color=gray55]\ntfds [label=tfds, group=g2, fontcolor=gray55, color=gray55]\ndt [label=datasets, fontcolor=gray55, color=gray55]\n\ngr [label=grain, fontcolor=orangered3, color=orangered3]\ntv [label=torchvision, fontcolor=orangered3, color=orangered3]\n\ntr [label=transformers, fontcolor=gray55, color=gray55]\n\nfl [label=flax, group=g2, fontcolor=gray55, color=gray55]\n\noa [label=optax, group=g2, fontcolor=gray55, color=gray55]\n\nob [label=orbax, group=g2, fontcolor=gray55, color=gray55]\n\n{rank=same; gr load tv}\ngr -> load -> tv [style=invis]\n\n{rank=same; fl proc pretr}\nfl -> proc -> pretr [style=invis]\n\n{pt tfds dt} -> load [color=gray55]\n{gr tv} -> proc [color=orangered3]\nfl -> nn [color=gray55]\npretr -> nn [dir=none]\ntr -> pretr [color=gray55]\noa -> opt [color=gray55]\nob -> cp [color=gray55]\n\nload -> proc -> nn -> opt -> cp [dir=none]\n\n}\n```\n\n:::{.callout-note collapse=\"true\"}\n\n## Minimal necessary code from previous section\n\n::: {#18aaa84e .cell execution_count=2}\n``` {.python .cell-code}\nfrom datasets import load_dataset\n\ntrain_size = 5 * 750\nval_size = 5 * 250\n\ntrain_dataset = load_dataset(\"food101\",\n                             split=f\"train[:{train_size}]\")\n\nval_dataset = load_dataset(\"food101\",\n                           split=f\"validation[:{val_size}]\")\n\nlabels_mapping = {}\nindex = 0\nfor i in range(0, len(val_dataset), 250):\n    label = val_dataset[i][\"label\"]\n    if label not in labels_mapping:\n        labels_mapping[label] = index\n        index += 1\n\ninv_labels_mapping = {v: k for k, v in labels_mapping.items()}\n```\n:::\n\n\n## Load packages\n\nPackages necessary for this section:\n\n::: {#2c5959af .cell execution_count=3}\n``` {.python .cell-code}\n# general array manipulation\nimport numpy as np\n\n# for image transformation and augmentation\nfrom torchvision.transforms import v2 as T\n\n# to create data loaders\nimport grain.python as grain\n```\n:::\n\n\n:::\n\n## Data normalization and augmentation\n\nLet's preprocess our images to match the methods used in the [vision transformer (ViT)](https://en.wikipedia.org/wiki/Vision_transformer) introduced by Dosovitskiy A et al. [-@dosovitskiy2021imageworth16x16words] and [implemented in JAX](https://github.com/google-research/vision_transformer/). This will be useful when we fine tune this model with the Food dataset in another section.\n\nThe preprocessing involves normalization and random augmentation (to prevent overfitting) with [TorchVision](https://github.com/pytorch/pytorch):\n\n::: {#6fe02939 .cell execution_count=4}\n``` {.python .cell-code}\nimg_size = 224\n\ndef to_np_array(pil_image):\n  return np.asarray(pil_image.convert(\"RGB\"))\n\ndef normalize(image):\n    # Image preprocessing matches the one of pretrained ViT\n    mean = np.array([0.5, 0.5, 0.5], dtype=np.float32)\n    std = np.array([0.5, 0.5, 0.5], dtype=np.float32)\n    image = image.astype(np.float32) / 255.0\n    return (image - mean) / std\n\ntv_train_transforms = T.Compose([\n    T.RandomResizedCrop((img_size, img_size), scale=(0.7, 1.0)),\n    T.RandomHorizontalFlip(),\n    T.ColorJitter(0.2, 0.2, 0.2),\n    T.Lambda(to_np_array),\n    T.Lambda(normalize),\n])\n\ntv_test_transforms = T.Compose([\n    T.Resize((img_size, img_size)),\n    T.Lambda(to_np_array),\n    T.Lambda(normalize),\n])\n\ndef get_transform(fn):\n    def wrapper(batch):\n        batch[\"image\"] = [\n            fn(pil_image) for pil_image in batch[\"image\"]\n        ]\n        # map label index between 0 - 19\n        batch[\"label\"] = [\n            labels_mapping[label] for label in batch[\"label\"]\n        ]\n        return batch\n    return wrapper\n\ntrain_transforms = get_transform(tv_train_transforms)\nval_transforms = get_transform(tv_test_transforms)\n\ntrain_dataset = train_dataset.with_transform(train_transforms)\nval_dataset = val_dataset.with_transform(val_transforms)\n```\n:::\n\n\n## Data loaders\n\nWe use [Grain](https://github.com/google/grain) to create efficient data loaders:\n\n::: {#56c873a2 .cell execution_count=5}\n``` {.python .cell-code}\nseed = 12\ntrain_batch_size = 32\nval_batch_size = 2 * train_batch_size\n\ntrain_sampler = grain.IndexSampler(\n    len(train_dataset),\n    shuffle=True,\n    seed=seed,\n    shard_options=grain.NoSharding(),\n    num_epochs=1,\n)\n\nval_sampler = grain.IndexSampler(\n    len(val_dataset),\n    shuffle=False,\n    seed=seed,\n    shard_options=grain.NoSharding(),\n    num_epochs=1,\n)\n\ntrain_loader = grain.DataLoader(\n    data_source=train_dataset,\n    sampler=train_sampler,\n    worker_count=4,\n    worker_buffer_size=2,\n    operations=[\n        grain.Batch(train_batch_size, drop_remainder=True),\n    ]\n)\n\nval_loader = grain.DataLoader(\n    data_source=val_dataset,\n    sampler=val_sampler,\n    worker_count=4,\n    worker_buffer_size=2,\n    operations=[\n        grain.Batch(val_batch_size),\n    ]\n)\n```\n:::\n\n\n",
    "supporting": [
      "fl_preprocess_files"
    ],
    "filters": [],
    "includes": {}
  }
}