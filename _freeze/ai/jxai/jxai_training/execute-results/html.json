{
  "hash": "e3c73bf48c9caed55c8258a9b9828aef",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Training\nauthor: Marie-Hélène Burle\n---\n\n:::{.def}\n\nIt is now time to train our model.\n\n:::\n\n:::{.callout-note collapse=\"true\"}\n\n## Minimal necessary code from previous sections\n\n```{.python}\nbase_dir = '<path-of-the-nabirds-dir>'\n```\n\n:::{.notenoit}\n\nTo be replaced by actual path: in our training cluster, the `base_dir` is at `/project/def-sponsor00/nabirds`:\n\n```{.python}\nbase_dir = '/project/def-sponsor00/nabirds'\n```\n\n:::\n\n\n\n::: {#bc7d8f2f .cell execution_count=3}\n``` {.python .cell-code}\nimport os\nimport polars as pl\nimport imageio.v3 as iio\nimport grain.python as grain\nfrom jax import random\nimport dm_pix as pix\nimport numpy as np\nimport jax\nimport jax.numpy as jnp\nfrom flax import nnx\nfrom transformers import FlaxViTForImageClassification\n\n\nmetadata = pl.read_parquet('metadata.parquet')\nmetadata_train = metadata.filter(pl.col('is_training_img') == 1)\nmetadata_val = metadata.filter(pl.col('is_training_img') == 0)\ncleaned_img_dir = os.path.join(base_dir, 'cleaned_images')\n\n\nclass NABirdsDataset:\n    \"\"\"NABirds dataset class.\"\"\"\n\n    def __init__(self, metadata_file, data_dir):\n        self.metadata_file = metadata_file\n        self.data_dir = data_dir\n\n    def __len__(self):\n        return len(self.metadata_file)\n\n    def __getitem__(self, idx):\n        path = os.path.join(self.data_dir, self.metadata_file.get_column('path')[idx])\n        img = iio.imread(path)\n        species_name = self.metadata_file.get_column('species_name')[idx]\n        species_id = self.metadata_file.get_column('species_id')[idx]\n        photographer = self.metadata_file.get_column('photographer')[idx]\n\n        return {\n            'img': img,\n            'species_name': species_name,\n            'species_id': species_id,\n            'photographer': photographer,\n        }\n\n\nnabirds_train = NABirdsDataset(metadata_train, cleaned_img_dir)\nnabirds_val = NABirdsDataset(metadata_val, cleaned_img_dir)\n\n\nclass Normalize(grain.MapTransform):\n    def map(self, element):\n        img = element['img']\n        mean = np.array([0.5, 0.5, 0.5], dtype=np.float32)\n        std = np.array([0.5, 0.5, 0.5], dtype=np.float32)\n        img = img.astype(np.float32) / 255.0\n        img_norm = (img - mean) / std\n        element['img'] = img_norm\n        return element\n\n\nclass ToFloat(grain.MapTransform):\n    def map(self, element):\n        element['img'] = element['img'].astype(np.float32) / 255.0\n        return element\n\n\nkey = random.key(31)\nkey, subkey1, subkey2, subkey3, subkey4 = random.split(key, num=5)\n\n\nclass RandomCrop(grain.MapTransform):\n    def map(self, element):\n        element['img'] = pix.random_crop(\n            key=subkey1,\n            image=element['img'],\n            crop_sizes=(224, 224, 3)\n        )\n        return element\n\n\nclass RandomFlip(grain.MapTransform):\n    def map(self, element):\n        element['img'] = pix.random_flip_left_right(\n            key=subkey2,\n            image=element['img']\n        )\n        return element\n\n\nclass RandomContrast(grain.MapTransform):\n    def map(self, element):\n        element['img'] = pix.random_contrast(\n            key=subkey3,\n            image=element['img'],\n            lower=0.8,\n            upper=1.2\n        )\n        return element\n\n\nclass RandomGamma(grain.MapTransform):\n    def map(self, element):\n        element['img'] = pix.random_gamma(\n            key=subkey4,\n            image=element['img'],\n            min_gamma=0.6,\n            max_gamma=1.2\n        )\n        return element\n\n\nclass ZScore(grain.MapTransform):\n    def map(self, element):\n        img = element['img']\n        mean = np.array([0.5, 0.5, 0.5], dtype=np.float32)\n        std = np.array([0.5, 0.5, 0.5], dtype=np.float32)\n        img = (img - mean) / std\n        element['img'] = img\n        return element\n\n\nseed = 123\ntrain_batch_size = 32\nval_batch_size = 2 * train_batch_size\n\ntrain_sampler = grain.IndexSampler(\n    num_records=len(nabirds_train),\n    shuffle=True,\n    seed=seed,\n    shard_options=grain.NoSharding(),\n    num_epochs=None\n)\n\ntrain_loader = grain.DataLoader(\n    data_source=nabirds_train,\n    sampler=train_sampler,\n    operations=[\n        ToFloat(),\n        RandomCrop(),\n        RandomFlip(),\n        RandomContrast(),\n        RandomGamma(),\n        ZScore(),\n        grain.Batch(train_batch_size, drop_remainder=True)\n    ]\n)\n\nval_sampler = grain.IndexSampler(\n    num_records=len(nabirds_val),\n    shuffle=False,\n    seed=seed,\n    shard_options=grain.NoSharding(),\n    num_epochs=1\n)\n\nval_loader = grain.DataLoader(\n    data_source=nabirds_val,\n    sampler=val_sampler,\n    operations=[\n        Normalize(),\n        grain.Batch(val_batch_size)\n    ]\n)\n\nclass VisionTransformer(nnx.Module):\n    def __init__(\n        self,\n        num_classes: int = 1000,\n        in_channels: int = 3,\n        img_size: int = 224,\n        patch_size: int = 16,\n        num_layers: int = 12,\n        num_heads: int = 12,\n        mlp_dim: int = 3072,\n        hidden_size: int = 768,\n        dropout_rate: float = 0.1,\n        *,\n        rngs: nnx.Rngs = nnx.Rngs(0),\n    ):\n        n_patches = (img_size // patch_size) ** 2\n        self.patch_embeddings = nnx.Conv(\n            in_channels,\n            hidden_size,\n            kernel_size=(patch_size, patch_size),\n            strides=(patch_size, patch_size),\n            padding='VALID',\n            use_bias=True,\n            rngs=rngs,\n        )\n\n        initializer = jax.nn.initializers.truncated_normal(stddev=0.02)\n        self.position_embeddings = nnx.Param(\n            initializer(rngs.params(), (1, n_patches + 1, hidden_size), jnp.float32)\n        )\n        self.dropout = nnx.Dropout(dropout_rate, rngs=rngs)\n        self.cls_token = nnx.Param(jnp.zeros((1, 1, hidden_size)))\n        self.encoder = nnx.Sequential(*[\n            TransformerEncoder(hidden_size, mlp_dim, num_heads, dropout_rate, rngs=rngs)\n            for i in range(num_layers)\n        ])\n        self.final_norm = nnx.LayerNorm(hidden_size, rngs=rngs)\n        self.classifier = nnx.Linear(hidden_size, num_classes, rngs=rngs)\n\n    def __call__(self, x: jax.Array) -> jax.Array:\n        patches = self.patch_embeddings(x)\n        batch_size = patches.shape[0]\n        patches = patches.reshape(batch_size, -1, patches.shape[-1])\n        cls_token = jnp.tile(self.cls_token, [batch_size, 1, 1])\n        x = jnp.concat([cls_token, patches], axis=1)\n        embeddings = x + self.position_embeddings\n        embeddings = self.dropout(embeddings)\n        x = self.encoder(embeddings)\n        x = self.final_norm(x)\n        x = x[:, 0]\n\n        return self.classifier(x)\n\n\nclass TransformerEncoder(nnx.Module):\n    def __init__(\n        self,\n        hidden_size: int,\n        mlp_dim: int,\n        num_heads: int,\n        dropout_rate: float = 0.0,\n        *,\n        rngs: nnx.Rngs = nnx.Rngs(0),\n    ) -> None:\n        self.norm1 = nnx.LayerNorm(hidden_size, rngs=rngs)\n        self.attn = nnx.MultiHeadAttention(\n            num_heads=num_heads,\n            in_features=hidden_size,\n            dropout_rate=dropout_rate,\n            broadcast_dropout=False,\n            decode=False,\n            deterministic=False,\n            rngs=rngs,\n        )\n        self.norm2 = nnx.LayerNorm(hidden_size, rngs=rngs)\n\n        self.mlp = nnx.Sequential(\n            nnx.Linear(hidden_size, mlp_dim, rngs=rngs),\n            nnx.gelu,\n            nnx.Dropout(dropout_rate, rngs=rngs),\n            nnx.Linear(mlp_dim, hidden_size, rngs=rngs),\n            nnx.Dropout(dropout_rate, rngs=rngs),\n        )\n\n    def __call__(self, x: jax.Array) -> jax.Array:\n        x = x + self.attn(self.norm1(x))\n        x = x + self.mlp(self.norm2(x))\n        return x\n\n\nmodel = VisionTransformer(num_classes=1000)\ntf_model = FlaxViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\n\n\ndef vit_inplace_copy_weights(*, src_model, dst_model):\n    assert isinstance(src_model, FlaxViTForImageClassification)\n    assert isinstance(dst_model, VisionTransformer)\n\n    tf_model_params = src_model.params\n    tf_model_params_fstate = nnx.traversals.flatten_mapping(tf_model_params)\n\n    flax_model_params = nnx.state(dst_model, nnx.Param)\n    flax_model_params_fstate = dict(flax_model_params.flat_state())\n\n    params_name_mapping = {\n        ('cls_token',): ('vit', 'embeddings', 'cls_token'),\n        ('position_embeddings',): ('vit', 'embeddings', 'position_embeddings'),\n        **{\n            ('patch_embeddings', x): ('vit', 'embeddings', 'patch_embeddings', 'projection', x)\n            for x in ['kernel', 'bias']\n        },\n        **{\n            ('encoder', 'layers', i, 'attn', y, x): (\n                'vit', 'encoder', 'layer', str(i), 'attention', 'attention', y, x\n            )\n            for x in ['kernel', 'bias']\n            for y in ['key', 'value', 'query']\n            for i in range(12)\n        },\n        **{\n            ('encoder', 'layers', i, 'attn', 'out', x): (\n                'vit', 'encoder', 'layer', str(i), 'attention', 'output', 'dense', x\n            )\n            for x in ['kernel', 'bias']\n            for i in range(12)\n        },\n        **{\n            ('encoder', 'layers', i, 'mlp', 'layers', y1, x): (\n                'vit', 'encoder', 'layer', str(i), y2, 'dense', x\n            )\n            for x in ['kernel', 'bias']\n            for y1, y2 in [(0, 'intermediate'), (3, 'output')]\n            for i in range(12)\n        },\n        **{\n            ('encoder', 'layers', i, y1, x): (\n                'vit', 'encoder', 'layer', str(i), y2, x\n            )\n            for x in ['scale', 'bias']\n            for y1, y2 in [('norm1', 'layernorm_before'), ('norm2', 'layernorm_after')]\n            for i in range(12)\n        },\n        **{\n            ('final_norm', x): ('vit', 'layernorm', x)\n            for x in ['scale', 'bias']\n        },\n        **{\n            ('classifier', x): ('classifier', x)\n            for x in ['kernel', 'bias']\n        }\n    }\n\n    nonvisited = set(flax_model_params_fstate.keys())\n\n    for key1, key2 in params_name_mapping.items():\n        assert key1 in flax_model_params_fstate, key1\n        assert key2 in tf_model_params_fstate, (key1, key2)\n\n        nonvisited.remove(key1)\n\n        src_value = tf_model_params_fstate[key2]\n        if key2[-1] == 'kernel' and key2[-2] in ('key', 'value', 'query'):\n            shape = src_value.shape\n            src_value = src_value.reshape((shape[0], 12, 64))\n\n        if key2[-1] == 'bias' and key2[-2] in ('key', 'value', 'query'):\n            src_value = src_value.reshape((12, 64))\n\n        if key2[-4:] == ('attention', 'output', 'dense', 'kernel'):\n            shape = src_value.shape\n            src_value = src_value.reshape((12, 64, shape[-1]))\n\n        dst_value = flax_model_params_fstate[key1]\n        assert src_value.shape == dst_value.value.shape, (key2, src_value.shape, key1, dst_value.value.shape)\n        dst_value.value = src_value.copy()\n        assert dst_value.value.mean() == src_value.mean(), (dst_value.value, src_value.mean())\n\n    assert len(nonvisited) == 0, nonvisited\n    # Notice the use of `flax.nnx.update` and `flax.nnx.State`.\n    nnx.update(dst_model, nnx.State.from_flat_path(flax_model_params_fstate))\n\nvit_inplace_copy_weights(src_model=tf_model, dst_model=model)\n\nmodel.classifier = nnx.Linear(model.classifier.in_features, 405, rngs=nnx.Rngs(0))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nTensorFlow and JAX classes are deprecated and will be removed in Transformers v5. We recommend migrating to PyTorch classes or pinning your version of Transformers.\n```\n:::\n:::\n\n\n:::\n\n## Hyperparameters\n\nWe need to define the hyperparameters that will control the training process:\n\n::: {#c27ec959 .cell execution_count=4}\n``` {.python .cell-code}\nimport optax\n\nnum_epochs = 3\nlearning_rate = 0.001\nmomentum = 0.8\ntotal_steps = len(nabirds_train) // train_batch_size\n\nlr_schedule = optax.linear_schedule(learning_rate, 0.0, num_epochs * total_steps)\n\niterate_subsample = np.linspace(0, num_epochs * total_steps, 100)\n\noptimizer = nnx.ModelAndOptimizer(model, optax.sgd(lr_schedule, momentum, nesterov=True))\n```\n:::\n\n\nWe can plot the learning rate schedule:\n\n::: {#726f5567 .cell execution_count=5}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\n\nplt.plot(\n    np.linspace(0, num_epochs, len(iterate_subsample)),\n    [lr_schedule(i) for i in iterate_subsample],\n    lw=3,\n)\nplt.title('Learning rate')\nplt.xlabel('Epochs')\nplt.ylabel('Learning rate')\nplt.grid()\nplt.xlim((0, num_epochs))\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](jxai_training_files/figure-html/cell-5-output-1.png){width=625 height=449}\n:::\n:::\n\n\n### Loss function\n\n::: {#a7326d3d .cell execution_count=6}\n``` {.python .cell-code}\ndef compute_losses_and_logits(model: nnx.Module, imgs: jax.Array, species: jax.Array):\n    logits = model(imgs)\n\n    loss = optax.softmax_cross_entropy_with_integer_labels(\n        logits=logits, species=species\n    ).mean()\n    return loss, logits\n```\n:::\n\n\n## Define train and eval steps\n\n::: {#1bc8ad25 .cell execution_count=7}\n``` {.python .cell-code}\ndef train_step(\n    model: nnx.Module, optimizer: nnx.Optimizer, batch: dict[str, np.ndarray]\n):\n    # Convert np.ndarray to jax.Array on GPU\n    imgs = jnp.array(batch['img'])\n    species = jnp.array(batch['species_id'], dtype=jnp.int32)\n\n    grad_fn = nnx.value_and_grad(compute_losses_and_logits, has_aux=True)\n    (loss, logits), grads = grad_fn(model, imgs, species)\n\n    optimizer.update(grads)  # In-place updates.\n\n    return loss\n\ndef eval_step(\n    model: nnx.Module, batch: dict[str, np.ndarray], eval_metrics: nnx.MultiMetric\n):\n    # Convert np.ndarray to jax.Array on GPU\n    imgs = jnp.array(batch['img'])\n    species = jnp.array(batch['species_id'], dtype=jnp.int32)\n    loss, logits = compute_losses_and_logits(model, imgs, species)\n\n    eval_metrics.update(\n        loss=loss,\n        logits=logits,\n        species=species,\n    )\n```\n:::\n\n\n## Metrics\n\n::: {#9ed46672 .cell execution_count=8}\n``` {.python .cell-code}\neval_metrics = nnx.MultiMetric(\n    loss=nnx.metrics.Average('loss'),\n    accuracy=nnx.metrics.Accuracy(),\n)\n\ntrain_metrics_history = {\n    'train_loss': [],\n}\n\neval_metrics_history = {\n    'val_loss': [],\n    'val_accuracy': [],\n}\n```\n:::\n\n\n## Training and evaluation functions\n\n::: {#5d1120e3 .cell execution_count=9}\n``` {.python .cell-code}\nimport tqdm\n\nbar_format = '{desc}[{n_fmt}/{total_fmt}]{postfix} [{elapsed}<{remaining}]'\n\n@nnx.jit              # To JIT compile and automatically use GPU/TPU if available\ndef train_one_epoch(epoch):\n    model.train()  # Set model to the training mode: e.g. update batch statistics\n    with tqdm.tqdm(\n        desc=f\"[train] epoch: {epoch}/{num_epochs}, \",\n        total=total_steps,\n        bar_format=bar_format,\n        leave=True,\n    ) as pbar:\n        for batch in train_loader:\n            loss = train_step(model, optimizer, batch)\n            train_metrics_history['train_loss'].append(loss.item())\n            pbar.set_postfix({'loss': loss.item()})\n            pbar.update(1)\n\n@nnx.jit\ndef evaluate_model(epoch):\n    # Computes the metrics on the training and test sets after each training epoch.\n    model.eval()  # Sets model to evaluation model: e.g. use stored batch statistics.\n\n    eval_metrics.reset()  # Reset the eval metrics\n    for val_batch in val_loader:\n        eval_step(model, val_batch, eval_metrics)\n\n    for metric, value in eval_metrics.compute().items():\n        eval_metrics_history[f'val_{metric}'].append(value)\n\n    print(f\"[val] epoch: {epoch + 1}/{num_epochs}\")\n    print(f\"- total loss: {eval_metrics_history['val_loss'][-1]:0.4f}\")\n    print(f\"- Accuracy: {eval_metrics_history['val_accuracy'][-1]:0.4f}\")\n```\n:::\n\n\n## Training\n\nTime to run the training loop:\n\n```{.python}\n%%time\n\nfor epoch in range(num_epochs):\n    train_one_epoch(epoch)\n    evaluate_model(epoch)\n```\n\n```\nValueError: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 77463552 bytes.\n```\n\nAs you can see, I ran into an [out of memory (OOM)](https://en.wikipedia.org/wiki/Out_of_memory) problem running this code on my machine (on CPU and on GPU).\n\nThis is where you would have to take the problem to the clusters.\n\nIf this happens to you, you can also reduce the batch size from 32 to 16 [as we saw earlier](jxai_dataloader#transformations).\n\n## Plot metrics\n\nIf you have the hardware to do the training, you can plot the metrics.\n\nEvolution of training loss:\n\n```{.python}\nplt.plot(train_metrics_history['train_loss'], label='Loss value during the training')\nplt.legend()\n```\n\nEvolution of validation loss and accuracy:\n\n```{.python}\nfig, axs = plt.subplots(1, 2, figsize=(10, 10))\naxs[0].set_title('Loss value on validation set')\naxs[0].plot(eval_metrics_history['val_loss'])\naxs[1].set_title('Accuracy on validation set')\naxs[1].plot(eval_metrics_history['val_accuracy'])\n```\n\nYou can also use [TensorBoard](https://github.com/tensorflow/tensorboard) for this or—even better—experiment tracking tools such as [MLflow](/ai/mlops/wb_mlflow.qmd) that will allow you to compare various training experiments.\n\n## Next steps for harder problems\n\nFine-tuning a classification model shouldn't take many epochs and this approach should be sufficient for our example.\n\nFor some problems however, you will need A LOT more training. In that case, there are additional steps you need to take.\n\n### Checkpointing\n\nCheckpointing becomes essential if you train for a long time. This will save you from loosing days of training if something happens (cluster issue, power outage, computer failure, training interruption...)\n\n[Orbax](https://github.com/google/orbax) provides checkpointing utilities for JAX.\n\n### Stopping\n\nIf the training is very long, how do you know how many epochs you need to choose?\n\nYou don't want to take a random guess. You also don't want to spend your days and nights staring at TensorBoard or MLflow. So you need to set things up to have the training stop automatically.\n\n#### Monitoring\n\nThe training loss always goes down (eventually to 0) as the accuracy keeps going up. So **what you want to monitor is the validation loss**.\n\nThe validation loss should go down as the model improves, then flatten out, and eventually start going up again as you start overfitting. At that point, the model is learning all the idiosyncrasies of the training set (which is why it is doing better and better on it), to a point that it is learning features that are not applicable to images that are not part of the training set (hence why the validation loss starts to deteriorate).\n\n#### Early stopping\n\nThe way to set things up for automatic stopping is to choose a large number of epochs (e.g. 50)—many more than you expect to need, and stop the model when the validation loss goes up.\n\nYou can use, for instance, an if/else statement to automatically stop training when you get to that point.\n\nTo be more sophisticated, instead of stopping as soon as this happens, one hyperparameter that can be set is **patience**.\n\n**Patience** is the number of epochs the model should continue training after the validation loss goes up (usually 3 to 5). This allows the model to recover from temporary plateaus and avoids the problem of [double descent](https://en.wikipedia.org/wiki/Double_descent).\n\nOnce that patience value is reached, you should use the checkpoint for the best validation loss (before the patience period) as your trained model.\n\n",
    "supporting": [
      "jxai_training_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}