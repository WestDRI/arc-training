{
  "hash": "640e298a3dd700c2a146b4a1e2e8d5e0",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Training\nauthor: Marie-Hélène Burle\n---\n\n:::{.def}\n\nIt is now time to train our model.\n\n:::\n\n:::{.callout-note collapse=\"true\"}\n\n## Minimal necessary code from previous sections\n\n```{.python}\nbase_dir = '<path-of-the-nabirds-dir>'\n```\n\n:::{.notenoit}\n\nTo be replaced by actual path: in our training cluster, the `base_dir` is at `/project/def-sponsor00/nabirds`:\n\n```{.python}\nbase_dir = '/project/def-sponsor00/nabirds'\n```\n\n:::\n\n\n\n::: {#f62a65ad .cell execution_count=3}\n``` {.python .cell-code}\nimport os\nimport polars as pl\nimport imageio.v3 as iio\nimport grain.python as grain\nfrom jax import random\nimport dm_pix as pix\nimport numpy as np\nimport jax\nimport jax.numpy as jnp\nfrom flax import nnx\nfrom transformers import FlaxViTForImageClassification\n\n\nmetadata = pl.read_parquet('metadata.parquet')\nmetadata_train = metadata.filter(pl.col('is_training_img') == 1)\nmetadata_val = metadata.filter(pl.col('is_training_img') == 0)\ncleaned_img_dir = os.path.join(base_dir, 'cleaned_images')\n\n\nclass NABirdsDataset:\n    \"\"\"NABirds dataset class.\"\"\"\n\n    def __init__(self, metadata_file, data_dir):\n        self.metadata_file = metadata_file\n        self.data_dir = data_dir\n\n    def __len__(self):\n        return len(self.metadata_file)\n\n    def __getitem__(self, idx):\n        path = os.path.join(self.data_dir, self.metadata_file.get_column('path')[idx])\n        img = iio.imread(path)\n        species_name = self.metadata_file.get_column('species_name')[idx]\n        species_id = self.metadata_file.get_column('species_id')[idx]\n        photographer = self.metadata_file.get_column('photographer')[idx]\n\n        return {\n            'img': img,\n            'species_name': species_name,\n            'species_id': species_id,\n            'photographer': photographer,\n        }\n\n\nnabirds_train = NABirdsDataset(metadata_train, cleaned_img_dir)\nnabirds_val = NABirdsDataset(metadata_val, cleaned_img_dir)\n\n\nclass Normalize(grain.MapTransform):\n    def map(self, element):\n        img = element['img']\n        mean = np.array([0.5, 0.5, 0.5], dtype=np.float32)\n        std = np.array([0.5, 0.5, 0.5], dtype=np.float32)\n        img = img.astype(np.float32) / 255.0\n        img_norm = (img - mean) / std\n        element['img'] = img_norm\n        return element\n\n\nclass ToFloat(grain.MapTransform):\n    def map(self, element):\n        element['img'] = element['img'].astype(np.float32) / 255.0\n        return element\n\n\nkey = random.key(31)\nkey, subkey1, subkey2, subkey3, subkey4 = random.split(key, num=5)\n\n\nclass RandomCrop(grain.MapTransform):\n    def map(self, element):\n        element['img'] = pix.random_crop(\n            key=subkey1,\n            image=element['img'],\n            crop_sizes=(224, 224, 3)\n        )\n        return element\n\n\nclass RandomFlip(grain.MapTransform):\n    def map(self, element):\n        element['img'] = pix.random_flip_left_right(\n            key=subkey2,\n            image=element['img']\n        )\n        return element\n\n\nclass RandomContrast(grain.MapTransform):\n    def map(self, element):\n        element['img'] = pix.random_contrast(\n            key=subkey3,\n            image=element['img'],\n            lower=0.8,\n            upper=1.2\n        )\n        return element\n\n\nclass RandomGamma(grain.MapTransform):\n    def map(self, element):\n        element['img'] = pix.random_gamma(\n            key=subkey4,\n            image=element['img'],\n            min_gamma=0.6,\n            max_gamma=1.2\n        )\n        return element\n\n\nclass ZScore(grain.MapTransform):\n    def map(self, element):\n        img = element['img']\n        mean = np.array([0.5, 0.5, 0.5], dtype=np.float32)\n        std = np.array([0.5, 0.5, 0.5], dtype=np.float32)\n        img = (img - mean) / std\n        element['img'] = img\n        return element\n\n\nseed = 123\ntrain_batch_size = 32\nval_batch_size = 2 * train_batch_size\n\ntrain_sampler = grain.IndexSampler(\n    num_records=len(nabirds_train),\n    shuffle=True,\n    seed=seed,\n    shard_options=grain.NoSharding(),\n    num_epochs=1\n)\n\ntrain_loader = grain.DataLoader(\n    data_source=nabirds_train,\n    sampler=train_sampler,\n    operations=[\n        ToFloat(),\n        RandomCrop(),\n        RandomFlip(),\n        RandomContrast(),\n        RandomGamma(),\n        ZScore(),\n        grain.Batch(train_batch_size, drop_remainder=True)\n    ]\n)\n\nval_sampler = grain.IndexSampler(\n    num_records=len(nabirds_val),\n    shuffle=False,\n    seed=seed,\n    shard_options=grain.NoSharding(),\n    num_epochs=1\n)\n\nval_loader = grain.DataLoader(\n    data_source=nabirds_val,\n    sampler=val_sampler,\n    operations=[\n        Normalize(),\n        grain.Batch(val_batch_size)\n    ]\n)\n\nclass VisionTransformer(nnx.Module):\n    def __init__(\n        self,\n        num_classes: int = 1000,\n        in_channels: int = 3,\n        img_size: int = 224,\n        patch_size: int = 16,\n        num_layers: int = 12,\n        num_heads: int = 12,\n        mlp_dim: int = 3072,\n        hidden_size: int = 768,\n        dropout_rate: float = 0.1,\n        *,\n        rngs: nnx.Rngs = nnx.Rngs(0),\n    ):\n        n_patches = (img_size // patch_size) ** 2\n        self.patch_embeddings = nnx.Conv(\n            in_channels,\n            hidden_size,\n            kernel_size=(patch_size, patch_size),\n            strides=(patch_size, patch_size),\n            padding='VALID',\n            use_bias=True,\n            rngs=rngs,\n        )\n\n        initializer = jax.nn.initializers.truncated_normal(stddev=0.02)\n        self.position_embeddings = nnx.Param(\n            initializer(rngs.params(), (1, n_patches + 1, hidden_size), jnp.float32)\n        )\n        self.dropout = nnx.Dropout(dropout_rate, rngs=rngs)\n        self.cls_token = nnx.Param(jnp.zeros((1, 1, hidden_size)))\n        self.encoder = nnx.Sequential(*[\n            TransformerEncoder(hidden_size, mlp_dim, num_heads, dropout_rate, rngs=rngs)\n            for i in range(num_layers)\n        ])\n        self.final_norm = nnx.LayerNorm(hidden_size, rngs=rngs)\n        self.classifier = nnx.Linear(hidden_size, num_classes, rngs=rngs)\n\n    def __call__(self, x: jax.Array) -> jax.Array:\n        patches = self.patch_embeddings(x)\n        batch_size = patches.shape[0]\n        patches = patches.reshape(batch_size, -1, patches.shape[-1])\n        cls_token = jnp.tile(self.cls_token, [batch_size, 1, 1])\n        x = jnp.concat([cls_token, patches], axis=1)\n        embeddings = x + self.position_embeddings\n        embeddings = self.dropout(embeddings)\n        x = self.encoder(embeddings)\n        x = self.final_norm(x)\n        x = x[:, 0]\n\n        return self.classifier(x)\n\n\nclass TransformerEncoder(nnx.Module):\n    def __init__(\n        self,\n        hidden_size: int,\n        mlp_dim: int,\n        num_heads: int,\n        dropout_rate: float = 0.0,\n        *,\n        rngs: nnx.Rngs = nnx.Rngs(0),\n    ) -> None:\n        self.norm1 = nnx.LayerNorm(hidden_size, rngs=rngs)\n        self.attn = nnx.MultiHeadAttention(\n            num_heads=num_heads,\n            in_features=hidden_size,\n            dropout_rate=dropout_rate,\n            broadcast_dropout=False,\n            decode=False,\n            deterministic=False,\n            rngs=rngs,\n        )\n        self.norm2 = nnx.LayerNorm(hidden_size, rngs=rngs)\n\n        self.mlp = nnx.Sequential(\n            nnx.Linear(hidden_size, mlp_dim, rngs=rngs),\n            nnx.gelu,\n            nnx.Dropout(dropout_rate, rngs=rngs),\n            nnx.Linear(mlp_dim, hidden_size, rngs=rngs),\n            nnx.Dropout(dropout_rate, rngs=rngs),\n        )\n\n    def __call__(self, x: jax.Array) -> jax.Array:\n        x = x + self.attn(self.norm1(x))\n        x = x + self.mlp(self.norm2(x))\n        return x\n\n\nmodel = VisionTransformer(num_classes=1000)\ntf_model = FlaxViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\n\n\ndef vit_inplace_copy_weights(*, src_model, dst_model):\n    assert isinstance(src_model, FlaxViTForImageClassification)\n    assert isinstance(dst_model, VisionTransformer)\n\n    tf_model_params = src_model.params\n    tf_model_params_fstate = nnx.traversals.flatten_mapping(tf_model_params)\n\n    flax_model_params = nnx.state(dst_model, nnx.Param)\n    flax_model_params_fstate = dict(flax_model_params.flat_state())\n\n    params_name_mapping = {\n        ('cls_token',): ('vit', 'embeddings', 'cls_token'),\n        ('position_embeddings',): ('vit', 'embeddings', 'position_embeddings'),\n        **{\n            ('patch_embeddings', x): ('vit', 'embeddings', 'patch_embeddings', 'projection', x)\n            for x in ['kernel', 'bias']\n        },\n        **{\n            ('encoder', 'layers', i, 'attn', y, x): (\n                'vit', 'encoder', 'layer', str(i), 'attention', 'attention', y, x\n            )\n            for x in ['kernel', 'bias']\n            for y in ['key', 'value', 'query']\n            for i in range(12)\n        },\n        **{\n            ('encoder', 'layers', i, 'attn', 'out', x): (\n                'vit', 'encoder', 'layer', str(i), 'attention', 'output', 'dense', x\n            )\n            for x in ['kernel', 'bias']\n            for i in range(12)\n        },\n        **{\n            ('encoder', 'layers', i, 'mlp', 'layers', y1, x): (\n                'vit', 'encoder', 'layer', str(i), y2, 'dense', x\n            )\n            for x in ['kernel', 'bias']\n            for y1, y2 in [(0, 'intermediate'), (3, 'output')]\n            for i in range(12)\n        },\n        **{\n            ('encoder', 'layers', i, y1, x): (\n                'vit', 'encoder', 'layer', str(i), y2, x\n            )\n            for x in ['scale', 'bias']\n            for y1, y2 in [('norm1', 'layernorm_before'), ('norm2', 'layernorm_after')]\n            for i in range(12)\n        },\n        **{\n            ('final_norm', x): ('vit', 'layernorm', x)\n            for x in ['scale', 'bias']\n        },\n        **{\n            ('classifier', x): ('classifier', x)\n            for x in ['kernel', 'bias']\n        }\n    }\n\n    nonvisited = set(flax_model_params_fstate.keys())\n\n    for key1, key2 in params_name_mapping.items():\n        assert key1 in flax_model_params_fstate, key1\n        assert key2 in tf_model_params_fstate, (key1, key2)\n\n        nonvisited.remove(key1)\n\n        src_value = tf_model_params_fstate[key2]\n        if key2[-1] == 'kernel' and key2[-2] in ('key', 'value', 'query'):\n            shape = src_value.shape\n            src_value = src_value.reshape((shape[0], 12, 64))\n\n        if key2[-1] == 'bias' and key2[-2] in ('key', 'value', 'query'):\n            src_value = src_value.reshape((12, 64))\n\n        if key2[-4:] == ('attention', 'output', 'dense', 'kernel'):\n            shape = src_value.shape\n            src_value = src_value.reshape((12, 64, shape[-1]))\n\n        dst_value = flax_model_params_fstate[key1]\n        assert src_value.shape == dst_value.value.shape, (key2, src_value.shape, key1, dst_value.value.shape)\n        dst_value.value = src_value.copy()\n        assert dst_value.value.mean() == src_value.mean(), (dst_value.value, src_value.mean())\n\n    assert len(nonvisited) == 0, nonvisited\n    # Notice the use of `flax.nnx.update` and `flax.nnx.State`.\n    nnx.update(dst_model, nnx.State.from_flat_path(flax_model_params_fstate))\n\nvit_inplace_copy_weights(src_model=tf_model, dst_model=model)\n\nmodel.classifier = nnx.Linear(model.classifier.in_features, 405, rngs=nnx.Rngs(0))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nTensorFlow and JAX classes are deprecated and will be removed in Transformers v5. We recommend migrating to PyTorch classes or pinning your version of Transformers.\n```\n:::\n:::\n\n\n:::\n\n## Hyperparameters\n\nWe need to define the hyperparameters that will control the training process.\n\nFirst, we need to set the number of epochs. While fine-tuning a ViT model for classification, we can expect the following:\n\n- Epochs 1-5: Massive improvements. The model adapts from \"identifying objects\" to \"identifying birds.\"\n- Epochs 5-15: Small, incremental gains.\n- Epochs 15+: Often starts overfitting (unless you have a massive dataset or very strong augmentation).\n\nLet's do 3 epochs:\n\n::: {#eedda5a7 .cell execution_count=4}\n``` {.python .cell-code}\nnum_epochs = 3\n```\n:::\n\n\nThe **learning rate** controls the size of the steps the optimizer takes in the direction of the gradient. You don't want to overshoot the minimum, so it is a good idea to decrease the learning rate during training. We are doing this with an [Optax](https://github.com/google-deepmind/optax) scheduler, reducing it linearly from 0.001 to 0.\n\n::: {#3e80b48d .cell execution_count=5}\n``` {.python .cell-code}\nimport optax\n\nlearning_rate = 0.001\ntotal_steps = len(nabirds_train) // train_batch_size\nlr_schedule = optax.linear_schedule(learning_rate, 0.0, num_epochs * total_steps)\niterate_subsample = np.linspace(0, num_epochs * total_steps, 100)\n```\n:::\n\n\nWe can plot the learning rate schedule:\n\n::: {#60adceb6 .cell execution_count=6}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\n\nplt.plot(\n    np.linspace(0, num_epochs, len(iterate_subsample)),\n    [lr_schedule(i) for i in iterate_subsample],\n    lw=3,\n)\nplt.title('Learning rate')\nplt.xlabel('Epochs')\nplt.ylabel('Learning rate')\nplt.grid()\nplt.xlim((0, num_epochs))\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](jxai_training_files/figure-html/cell-6-output-1.png){width=625 height=449}\n:::\n:::\n\n\nThe **momentum** controls the inertia of the optimizer. It helps accelerate the optimizer in the right direction and dampens oscillations (instead of using only the current gradient to update weights, momentum adds a fraction of the previous update factor to the current one. So if the gradient keeps pointing in the same direction, the momentum increases the speed of updates. If the gradient bounces back and forth, the momentum decreases the speed of updates).\n\n::: {#e3589bf1 .cell execution_count=7}\n``` {.python .cell-code}\nmomentum = 0.8\n```\n:::\n\n\nFinally, we pass our learning rate schedule and momentum to a [stochastic gradient descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent) function from [Optax](https://github.com/google-deepmind/optax) and create our optimizer:\n\n::: {#53b51290 .cell execution_count=8}\n``` {.python .cell-code}\noptimizer = nnx.ModelAndOptimizer(model, optax.sgd(lr_schedule, momentum, nesterov=True))\n```\n:::\n\n\n### Loss function\n\n::: {#4ea0cef1 .cell execution_count=9}\n``` {.python .cell-code}\ndef compute_losses_and_logits(model: nnx.Module, imgs: jax.Array, species: jax.Array):\n    logits = model(imgs)\n\n    loss = optax.softmax_cross_entropy_with_integer_labels(\n        logits=logits, labels=species\n    ).mean()\n    return loss, logits\n```\n:::\n\n\n## Define train and eval steps\n\n::: {#87db7fa1 .cell execution_count=10}\n``` {.python .cell-code}\n@nnx.jit              # To JIT compile and automatically use GPU/TPU if available\ndef train_step(\n    model: nnx.Module, optimizer: nnx.Optimizer, imgs: np.ndarray, species_id: np.ndarray\n):\n    # Convert np.ndarray to jax.Array on GPU\n    imgs = jnp.array(imgs)\n    species = jnp.array(species_id, dtype=jnp.int32)\n\n    grad_fn = nnx.value_and_grad(compute_losses_and_logits, has_aux=True)\n    (loss, logits), grads = grad_fn(model, imgs, species)\n\n    optimizer.update(grads)  # In-place updates.\n\n    return loss\n\n@nnx.jit\ndef eval_step(\n    model: nnx.Module, eval_metrics: nnx.MultiMetric, imgs: np.ndarray, species_id: np.ndarray\n):\n    # Convert np.ndarray to jax.Array on GPU\n    imgs = jnp.array(imgs)\n    species = jnp.array(species_id, dtype=jnp.int32)\n    loss, logits = compute_losses_and_logits(model, imgs, species)\n\n    eval_metrics.update(\n        loss=loss,\n        logits=logits,\n        labels=species,\n    )\n```\n:::\n\n\n## Metrics\n\n::: {#4ffee662 .cell execution_count=11}\n``` {.python .cell-code}\neval_metrics = nnx.MultiMetric(\n    loss=nnx.metrics.Average('loss'),\n    accuracy=nnx.metrics.Accuracy(),\n)\n\ntrain_metrics_history = {\n    'train_loss': [],\n}\n\neval_metrics_history = {\n    'val_loss': [],\n    'val_accuracy': [],\n}\n```\n:::\n\n\n## Training and evaluation functions\n\n::: {#d69241d3 .cell execution_count=12}\n``` {.python .cell-code}\nimport tqdm\n\nbar_format = '{desc}[{n_fmt}/{total_fmt}]{postfix} [{elapsed}<{remaining}]'\n\ndef train_one_epoch(epoch):\n    model.train()  # Set model to the training mode: e.g. update batch statistics\n    with tqdm.tqdm(\n        desc=f\"[train] epoch: {epoch}/{num_epochs}, \",\n        total=total_steps,\n        bar_format=bar_format,\n        leave=True,\n    ) as pbar:\n        for batch in train_loader:\n            loss = train_step(model, optimizer, batch['img'], batch['species_id'])\n            train_metrics_history['train_loss'].append(loss.item())\n            pbar.set_postfix({'loss': loss.item()})\n            pbar.update(1)\n\ndef evaluate_model(epoch):\n    # Computes the metrics on the training and test sets after each training epoch.\n    model.eval()  # Sets model to evaluation model: e.g. use stored batch statistics.\n\n    eval_metrics.reset()  # Reset the eval metrics\n    for val_batch in val_loader:\n        eval_step(model, eval_metrics, val_batch['img'], val_batch['species_id'])\n\n    for metric, value in eval_metrics.compute().items():\n        eval_metrics_history[f'val_{metric}'].append(value)\n\n    print(f\"[val] epoch: {epoch + 1}/{num_epochs}\")\n    print(f\"- total loss: {eval_metrics_history['val_loss'][-1]:0.4f}\")\n    print(f\"- Accuracy: {eval_metrics_history['val_accuracy'][-1]:0.4f}\")\n```\n:::\n\n\n## Training\n\nTime to run the training loop:\n\n```{.python}\n%%time\n\nfor epoch in range(num_epochs):\n    train_one_epoch(epoch)\n    evaluate_model(epoch)\n```\n\n```\nValueError: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 77463552 bytes.\n```\n\nAs you can see, I initially ran into an [out of memory (OOM)](https://en.wikipedia.org/wiki/Out_of_memory) problem running this code on my machine.\n\n:::{.exo}\n\n:::{.yourturn}\n\nYour turn:\n\n:::\n\nWhat are my options at this point?\n\n:::\n\n:::{.callout-caution collapse=\"true\"}\n\n## Answer\n\nOne option of course is to get more hardware to have access to more [VRAM](https://en.wikipedia.org/wiki/Video_random-access_memory) (the GPU memory): you can take the problem to the Alliance clusters or to a commercial cloud service. But training a classification model is something that you should be able to do on a laptop that has a dedicated GPU (which is what I have). How?\n\nWe mentioned the solution [in an earlier section](jxai_dataloader#transformations): reduce the batch size. Do so by dividing it by 2 until you stop running out of memory.\n\nIn my case, I tried to go from 32 to 16 and still ran out of memory, then I tried with a batch size of 8 and it worked.\n\nSo in the code above I replaced:\n\n```{.python}\ntrain_batch_size = 32\n```\n\nby:\n\n```{.python}\ntrain_batch_size = 8\n```\n\nand ran the rest as is.\n\n:::\n\n## Plot metrics\n\nLet's plot the training metrics.\n\nEvolution of training loss:\n\n\n\n::: {#8b8c9a68 .cell execution_count=14}\n``` {.python .cell-code}\nplt.plot(train_metrics_history['train_loss'], label='Loss value during the training')\nplt.legend()\n```\n\n::: {.cell-output .cell-output-display}\n![](jxai_training_files/figure-html/cell-14-output-1.png){width=558 height=411}\n:::\n:::\n\n\nThe loss decreased in a very jagged fashion due to my small batch size. With more memory and a bigger batch size, the descent would have been smoother. To get a smoother descent with such small batches, I could have increased the momentum or calculated the metrics for several batches before passing them to the next step.\n\nI also had some NaN (not a number) for the loss at the start of training. This didn't matter because the training kept going and after a bit of training they became less frequent and eventually disappeared. To avoid this, I could have started with a lower learning rate.\n\nEvolution of validation loss and accuracy:\n\n::: {#8ccc54cc .cell execution_count=15}\n``` {.python .cell-code}\nfig, axs = plt.subplots(1, 2, figsize=(8, 8))\naxs[0].set_title('Loss value on validation set')\naxs[0].plot(eval_metrics_history['val_loss'])\naxs[1].set_title('Accuracy on validation set')\naxs[1].plot(eval_metrics_history['val_accuracy'])\n```\n\n::: {.cell-output .cell-output-display}\n![](jxai_training_files/figure-html/cell-15-output-1.png){width=664 height=653}\n:::\n:::\n\n\nYou can also use [TensorBoard](https://github.com/tensorflow/tensorboard) for this or—even better—experiment tracking tools such as [MLflow](/ai/mlops/wb_mlflow.qmd) that will allow you to compare various training experiments.\n\n:::{.note}\n\n[MLflow is now available on our JupyterHubs!](https://docs.alliancecan.ca/wiki/MLflow)\n\n:::\n\n## Test a few samples\n\n::: {#9d4d7f1e .cell execution_count=16}\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-bright-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-bright-red-fg\">NameError</span>                                 Traceback (most recent call last)\n<span class=\"ansi-bright-cyan-fg\">Cell</span><span class=\"ansi-bright-cyan-fg\"> </span><span class=\"ansi-green-fg\">In[15]</span><span class=\"ansi-green-fg\">, line 16</span>\n<span class=\"ansi-bright-green-fg\">     13</span> <span class=\"ansi-bright-white-fg\">CHECKPOINT_DIR</span><span class=\"ansi-bright-white-fg\"> </span><span style=\"color:rgb(255,95,135)\">=</span><span class=\"ansi-bright-white-fg\"> </span><span class=\"ansi-bright-white-fg\">os</span><span style=\"color:rgb(255,95,135)\">.</span><span class=\"ansi-bright-white-fg\">path</span><span style=\"color:rgb(255,95,135)\">.</span><span class=\"ansi-bright-white-fg\">abspath</span><span class=\"ansi-bright-white-fg\">(</span><span style=\"color:rgb(215,215,135)\">'</span><span style=\"color:rgb(215,215,135)\">checkpoint</span><span style=\"color:rgb(215,215,135)\">'</span><span class=\"ansi-bright-white-fg\">)</span>\n<span class=\"ansi-bright-green-fg\">     14</span> <span class=\"ansi-bright-white-fg\">METADATA_PATH</span><span class=\"ansi-bright-white-fg\"> </span><span style=\"color:rgb(255,95,135)\">=</span><span class=\"ansi-bright-white-fg\"> </span><span style=\"color:rgb(215,215,135)\">'</span><span style=\"color:rgb(215,215,135)\">metadata.parquet</span><span style=\"color:rgb(215,215,135)\">'</span>\n<span class=\"ansi-bright-green-fg\">---&gt; </span><span class=\"ansi-bright-green-fg\">16</span> <span style=\"color:rgb(95,215,255)\">def</span><span class=\"ansi-bright-white-fg\"> </span><span style=\"color:rgb(175,215,0)\">load_model</span><span class=\"ansi-bright-white-fg\">(</span><span class=\"ansi-bright-white-fg\">checkpoint_dir</span><span style=\"color:rgb(255,95,135)\">=</span><span class=\"ansi-bright-white-fg ansi-yellow-bg\">CHECKPOINT</span><span class=\"ansi-bright-white-fg\">)</span><span class=\"ansi-bright-white-fg\">:</span>\n<span class=\"ansi-bright-green-fg\">     17</span> <span class=\"ansi-bright-white-fg\">    </span><span style=\"color:rgb(215,215,135)\">\"\"\"Loads the model from the checkpoint directory.\"\"\"</span>\n<span class=\"ansi-bright-green-fg\">     18</span> <span class=\"ansi-bright-white-fg\">    </span><span class=\"ansi-bright-white-fg\">print</span><span class=\"ansi-bright-white-fg\">(</span><span style=\"color:rgb(215,215,135)\">f</span><span style=\"color:rgb(215,215,135)\">\"</span><span style=\"color:rgb(215,215,135)\">Loading model from </span><span style=\"color:rgb(215,215,135)\">{</span><span class=\"ansi-bright-white-fg\">checkpoint_dir</span><span style=\"color:rgb(215,215,135)\">}</span><span style=\"color:rgb(215,215,135)\">...</span><span style=\"color:rgb(215,215,135)\">\"</span><span class=\"ansi-bright-white-fg\">)</span>\n\n<span class=\"ansi-bright-red-fg\">NameError</span>: name 'CHECKPOINT' is not defined</pre>\n```\n:::\n\n:::\n:::\n\n\nWe can now run inference on a few test images.\n\nSelect a subset of test images and their labels:\n\n::: {#fc3fc4fd .cell execution_count=17}\n``` {.python .cell-code}\ntest_indices = [250, 500, 750, 1000]\n\ntest_images = jnp.array([nabirds_val[i]['img'] for i in test_indices])\nexpected_labels = [nabirds_val[i]['species_name'] for i in test_indices]\n```\n:::\n\n\nRun the model to get the predictions for this subset:\n\n::: {#47177cb4 .cell execution_count=18}\n``` {.python .cell-code}\nmodel.eval()\npreds = model(test_images)\nprobas = nnx.softmax(preds, axis=1)\npred_labels = probas.argmax(axis=1)\n```\n:::\n\n\nWe need to define a translation function to get the species names from the species ids for the predictions:\n\n::: {#89419489 .cell execution_count=19}\n``` {.python .cell-code}\ndef translator(df, species_id):\n    species_name = df.unique(subset='species_id').filter(\n        pl.col('species_id') == species_id\n    ).select(pl.col('species_name')).item()\n\n    return species_name\n```\n:::\n\n\nLet's print the subset with their predicted vs expected labels:\n\n::: {#68dbd796 .cell execution_count=20}\n``` {.python .cell-code}\nnum_samples = len(test_indices)\n\nfig, axs = plt.subplots(1, num_samples, figsize=(7, 8))\n\nfor i in range(num_samples):\n    img, expected_label = test_images[i], expected_labels[i]\n\n    pred_label_id = pred_labels[i].item()\n    pred_label_name = translator(metadata, pred_label_id)\n    proba = probas[i, pred_label_id].item()\n    if img.dtype in (np.float32, ):\n        img = ((img - img.min()) / (img.max() - img.min()) * 255.0).astype(np.uint8)\n\n    plt.tight_layout()\n\n    axs[i].set_title(\n        f\"\"\"\n        Expected: {expected_labels[i]} vs\n        Predicted: {pred_label_name}, P={proba:.2f}\n        \"\"\",\n        fontsize=6,\n        linespacing=1.5\n    )\n\n    axs[i].axis('off')\n\n    axs[i].imshow(img)\n```\n\n::: {.cell-output .cell-output-display}\n![](jxai_training_files/figure-html/cell-20-output-1.png){width=695 height=189}\n:::\n:::\n\n\n",
    "supporting": [
      "jxai_training_files"
    ],
    "filters": [],
    "includes": {}
  }
}