{
  "hash": "b918c2bc92928873701aa22ca68f1aff",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Relation to NumPy\nauthor: Marie-Hélène Burle\n---\n\n:::{.def}\n\n[NumPy](https://numpy.org/) is a popular Python scientific API at the core of many libraries. JAX uses a NumPy-inspired API. There are however important differences that we will explore in this section.\n\n:::\n\n## A NumPy-inspired API\n\nNumPy being so popular, JAX comes with a convenient high-level wrapper to NumPy: `jax.numpy`.\n\n:::{.note}\n\nBeing familiar with NumPy is thus an advantage to get started with JAX. [The NumPy quickstart](https://numpy.org/doc/stable/user/quickstart.html) is a useful resource.\n\n:::\n\n:::{.note}\n\nFor a more efficient usage, JAX also comes with a lower-level API: `jax.lax`.\n\n:::\n\n:::{.panel-tabset}\n\n### NumPy\n\n::: {#6dce0af1 .cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\n```\n:::\n\n\n::: {#4b52ed16 .cell execution_count=2}\n``` {.python .cell-code}\nprint(np.array([(1, 2, 3), (4, 5, 6)]))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[1 2 3]\n [4 5 6]]\n```\n:::\n:::\n\n\n::: {#b86faee9 .cell execution_count=3}\n``` {.python .cell-code}\nprint(np.zeros((2, 3)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[0. 0. 0.]\n [0. 0. 0.]]\n```\n:::\n:::\n\n\n::: {#5664a061 .cell execution_count=4}\n``` {.python .cell-code}\nprint(np.ones((2, 3, 2)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[[1. 1.]\n  [1. 1.]\n  [1. 1.]]\n\n [[1. 1.]\n  [1. 1.]\n  [1. 1.]]]\n```\n:::\n:::\n\n\n::: {#ccea7564 .cell execution_count=5}\n``` {.python .cell-code}\nprint(np.arange(24).reshape(2, 3, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[[ 0  1  2  3]\n  [ 4  5  6  7]\n  [ 8  9 10 11]]\n\n [[12 13 14 15]\n  [16 17 18 19]\n  [20 21 22 23]]]\n```\n:::\n:::\n\n\n::: {#69cf6827 .cell execution_count=6}\n``` {.python .cell-code}\nprint(np.linspace(0, 2, 9))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[0.   0.25 0.5  0.75 1.   1.25 1.5  1.75 2.  ]\n```\n:::\n:::\n\n\n::: {#04a577ec .cell execution_count=7}\n``` {.python .cell-code}\nprint(np.linspace(0, 2, 9)[::-1])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[2.   1.75 1.5  1.25 1.   0.75 0.5  0.25 0.  ]\n```\n:::\n:::\n\n\n### JAX NumPy\n\n```{.python}\nimport jax.numpy as jnp\n```\n\n```{.python}\nprint(jnp.array([(1, 2, 3), (4, 5, 6)]))\n```\n\n```\n[[1 2 3]\n [4 5 6]]\n```\n\n```{.python}\nprint(jnp.zeros((2, 3)))\n```\n\n```\n[[0. 0. 0.]\n [0. 0. 0.]]\n```\n\n```{.python}\nprint(jnp.ones((2, 3, 2)))\n```\n\n```\n[[[1. 1.]\n  [1. 1.]\n  [1. 1.]]\n\n [[1. 1.]\n  [1. 1.]\n  [1. 1.]]]\n```\n\n```{.python}\nprint(jnp.arange(24).reshape(2, 3, 4))\n```\n\n```\n[[[ 0  1  2  3]\n  [ 4  5  6  7]\n  [ 8  9 10 11]]\n\n [[12 13 14 15]\n  [16 17 18 19]\n  [20 21 22 23]]]\n```\n\n```{.python}\nprint(jnp.linspace(0, 2, 9))\n```\n\n```\n[0.   0.25 0.5  0.75 1.   1.25 1.5  1.75 2.  ]\n```\n\n```{.python}\nprint(jnp.linspace(0, 2, 9)[::-1])\n```\n\n```\n[2.   1.75 1.5  1.25 1.   0.75 0.5  0.25 0.  ]\n```\n\n:::\n\nDespite the similarities, there are important differences between JAX and NumPy.\n\n## Differences with NumPy\n\n### Different types\n\n```{.python}\ntype(np.zeros((2, 3))) == type(jnp.zeros((2, 3)))\n```\n\n```\nFalse\n```\n\n:::{.panel-tabset}\n\n#### Numpy\n\n::: {#953efe54 .cell execution_count=8}\n``` {.python .cell-code}\ntype(np.zeros((2, 3)))\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```\nnumpy.ndarray\n```\n:::\n:::\n\n\n#### JAX NumPy\n\n```{.python}\ntype(jnp.zeros((2, 3)))\n```\n\n```\njaxlib.xla_extension.ArrayImpl\n```\n\n:::\n\n### Different default data types\n\n:::{.panel-tabset}\n\n#### Numpy\n\n::: {#a05006c9 .cell execution_count=9}\n``` {.python .cell-code}\nnp.zeros((2, 3)).dtype\n```\n\n::: {.cell-output .cell-output-display execution_count=9}\n```\ndtype('float64')\n```\n:::\n:::\n\n\n#### JAX NumPy\n\n```{.python}\njnp.zeros((2, 3)).dtype\n```\n\n```\ndtype('float32')\n```\n\n:::\n\n:::{.note}\n\nLower numerical precision improves speed and reduces memory usage at no cost while training neural networks and is thus a net benefit. Having been built with deep learning in mind, JAX defaults align with that of other DL libraries (e.g. PyTorch, TensorFlow).\n\n:::\n\n### Immutable arrays\n\n:::{.panel-tabset}\n\n#### Numpy\n\nIn NumPy, you can modify ndarrays:\n\n::: {#5d824ec8 .cell execution_count=10}\n``` {.python .cell-code}\na = np.arange(5)\na[0] = 9\nprint(a)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[9 1 2 3 4]\n```\n:::\n:::\n\n\n#### JAX NumPy\n\nJAX arrays are immutable:\n\n```{.python}\na = jnp.arange(5)\na[0] = 9\n```\n\n```\nTypeError: '<class 'jaxlib.xla_extension.ArrayImpl'>' object does not support item assignment. JAX arrays are immutable. Instead of ``x[idx] = y``, use ``x = x.at[idx].set(y)`` or another .at[] method: https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.ndarray.at.html\n```\n\nInstead, you need to create a copy of the array with the mutation. This is done with:\n\n```{.python}\nb = a.at[0].set(9)\nprint(b)\n```\n\n```\n[9 1 2 3 4]\n```\n\nOf course, you can overwrite `a`:\n\n```{.python}\na = a.at[0].set(9)\n```\n\n:::\n\n### Pseudorandom number generation\n\nProgramming languages usually come with automated [pseudorandom number generator (PRNG)](https://en.wikipedia.org/wiki/Pseudorandom_number_generator) based on nondeterministic data from the operating system. They are extremely convenient, but slow, based on repeats, and problematic in parallel executions.\n\nJAX relies on an explicitly set random state called a *key*.\n\n```{.python}\nfrom jax import random\n\nkey = random.key(18)\nprint(key)\n```\n\n```\n[ 0 18]\n```\n\nEach time you call a random function, you need a subkey split from your key. **Keys should only ever be used once in your code.** The key is what makes your code reproducible, but you don't want to reuse it within your code as it would create spurious correlations.\n\nHere is the workflow:\n\n- you split your key into a new key and one or multiple subkeys,\n- you discard the old key (because it was used to do the split—so its entropy budget, so to speak, has been used),\n- you use the subkey(s) to run your random function(s) and keep the new key for a future split.\n\n:::{.note}\n\nSubkeys are of the same nature as keys. This is just a terminology.\n\n:::\n\nTo make sure not to reuse the old key, you can overwrite it by the new one:\n\n```{.python}\nkey, subkey = random.split(key)\n```\n\n```{.python}\nprint(key)\n```\n\n```\n[4197003906 1654466292]\n```\n\n:::{.note}\n\nThat's the value of our new key for future splits.\n\n:::\n\n```{.python}\nprint(subkey)\n```\n\n```\n[1685972163 1654824463]\n```\n\n:::{.note}\n\nThis is the value of the subkey that we can use to call a random function.\n\n:::\n\nLet's use that subkey now:\n\n```{.python}\nprint(random.normal(subkey))\n```\n\n```\n1.1437175\n```\n\n:::{.note}\n\nTo split your key into more subkeys, pass an argument to `random.split`:\n\n```{.python}\nkey, subkey1, subkey2, subkey3 = random.split(key, 4)\n```\n\n:::\n\n### Strict input control\n\n:::{.panel-tabset}\n\n#### Numpy\n\nNumPy's fundamental object is the ndarray, but NumPy is very tolerant as to the type of input.\n\n::: {#c9c38104 .cell execution_count=11}\n``` {.python .cell-code}\nnp.sum([1.0, 2.0])  # here we are using a list\n```\n\n::: {.cell-output .cell-output-display execution_count=11}\n```\nnp.float64(3.0)\n```\n:::\n:::\n\n\n::: {#52be204c .cell execution_count=12}\n``` {.python .cell-code}\nnp.sum((1.0, 2.0))  # here is a tuple\n```\n\n::: {.cell-output .cell-output-display execution_count=12}\n```\nnp.float64(3.0)\n```\n:::\n:::\n\n\n#### JAX NumPy\n\nTo avoid inefficiencies, JAX will only accept arrays.\n\n```{.python}\njnp.sum([1.0, 2.0])\n```\n\n```\nTypeError: sum requires ndarray or scalar arguments, got <class 'list'> at position 0.\n```\n\n```{.python}\njnp.sum((1.0, 2.0))\n```\n\n```\nTypeError: sum requires ndarray or scalar arguments, got <class 'tuple'> at position 0.\n```\n\n:::\n\n### Out of bounds indexing\n\n:::{.panel-tabset}\n\n#### Numpy\n\nNumPy will warn you with an error message if you try to index out of bounds:\n\n::: {#f9e2ba47 .cell execution_count=13}\n``` {.python .cell-code}\nprint(np.arange(5)[10])\n```\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-bright-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-bright-red-fg\">IndexError</span>                                Traceback (most recent call last)\n<span class=\"ansi-bright-cyan-fg\">Cell</span><span class=\"ansi-bright-cyan-fg\"> </span><span class=\"ansi-green-fg\">In[13]</span><span class=\"ansi-green-fg\">, line 1</span>\n<span class=\"ansi-bright-green-fg\">----&gt; </span><span class=\"ansi-bright-green-fg\">1</span> <span class=\"ansi-bright-white-fg\">print</span><span class=\"ansi-bright-white-fg\">(</span><span class=\"ansi-bright-white-fg ansi-yellow-bg\">np</span><span style=\"color:rgb(255,95,135)\" class=\"ansi-yellow-bg\">.</span><span class=\"ansi-bright-white-fg ansi-yellow-bg\">arange</span><span class=\"ansi-bright-white-fg ansi-yellow-bg\">(</span><span style=\"color:rgb(175,135,255)\" class=\"ansi-yellow-bg\">5</span><span class=\"ansi-bright-white-fg ansi-yellow-bg\">)</span><span class=\"ansi-bright-white-fg ansi-yellow-bg\">[</span><span style=\"color:rgb(175,135,255)\" class=\"ansi-yellow-bg\">10</span><span class=\"ansi-bright-white-fg ansi-yellow-bg\">]</span><span class=\"ansi-bright-white-fg\">)</span>\n\n<span class=\"ansi-bright-red-fg\">IndexError</span>: index 10 is out of bounds for axis 0 with size 5</pre>\n```\n:::\n\n:::\n:::\n\n\n#### JAX NumPy\n\n**Be aware that JAX will not raise an error.** Instead, it will silently return the closest boundary:\n\n```{.python}\nprint(jnp.arange(5)[10])\n```\n\n```\n4\n```\n\n:::\n\n### Functionally pure functions\n\nMore importantly, only functionally pure functions—that is, functions for which the outputs are only based on the inputs and which have no side effects—can be used with JAX.\n\n#### Outputs only based on inputs\n\nConsider the function:\n\n```{.python}\ndef f(x):\n    return a + x\n```\n\nwhich uses the variable `a` from the global environment.\n\nThis function is not functionally pure because the outputs (the results of the function) do not solely depend on the arguments (the values given to `x`) passed to it. They also depend on the value of `a`.\n\nRemember how tracing works: new inputs with the same shape and dtype use the cached compiled program directly. If the value of `a` changes in the global environment, a new tracing is not triggered and the cached compiled program uses the old value of `a` (the one that was used during tracing).\n\nIt is only if the code is run on an input `x` with a different shape and/or dtype that tracing happens again and that the new value for `a` takes effect.\n\n```{.python}\nfrom jax import jit\n\na = jnp.ones(3)\nprint(a)\n```\n\n```\n[1. 1. 1.]\n```\n\n```{.python}\ndef f(x):\n    return a + x\n\nprint(jit(f)(jnp.ones(3)))\n```\n\n```\n[2. 2. 2.]\n```\n\n:::{.note}\n\nAll good here because this is the first run (tracing).\n\n:::\n\nNow, let's change the value of `a` to an array of zeros:\n\n```{.python}\na = jnp.zeros(3)\nprint(a)\n```\n\n```\n[0. 0. 0.]\n```\n\nAnd rerun the same code:\n\n```{.python}\nprint(jit(f)(jnp.ones(3)))\n```\n\n```\n[2. 2. 2.]\n```\n\nWe should have an array of ones, but we get the same result we got earlier. Why? because we are running a cached program with the value that `a` had during tracing.\n\nThe new value for `a` will only take effect if we re-trigger tracing by changing the shape and/or dtype of `x`:\n\n```{.python}\na = jnp.zeros(4)\nprint(a)\n```\n\n```\n[0. 0. 0. 0.]\n```\n\n```{.python}\nprint(jit(f)(jnp.ones(4)))\n```\n\n```\n[1. 1. 1. 1.]\n```\n\nPassing an argument of a different shape to `f` forced recompilation. Using a different data type (e.g. with `jnp.arange(3)`) would have done the same.\n\n#### No side effects\n\nA function is said to have a side effect if it changes something outside of its local environment (if it does anything beside returning an output).\n\nExamples of side effects include:\n\n- printing to standard output/shell,\n- reading from file/writing to file,\n- modifying a global variable.\n\nIn JAX, the side effects will happen during the first run (tracing), but will not happen on subsequent runs. You thus cannot rely on side effects in your code.\n\n```{.python}\ndef f(a, b):\n    print(\"Calculating sum\")\n    return a + b\n\nprint(jit(f)(jnp.arange(3), jnp.arange(3)))\n```\n\n```\nCalculating sum\n[0 2 4]\n```\n\n:::{.note}\n\nPrinting (the side effect) happened here because this is the first run.\n\n:::\n\nLet's rerun the function:\n\n```{.python}\nprint(jit(f)(jnp.arange(3), jnp.arange(3)))\n```\n\n```\n[0 2 4]\n```\n\nThis time, no printing.\n\n:::{.info}\n\n**Understanding jaxprs**\n\nJaxprs are created by tracers wrapping the Python code during compilation (the first run). They contain information on the shape and data type of arrays as well as the operations performed on these arrays. Jaxprs do not however contain information on values: this allows the compiled program to be general enough to be rerun with any new arrays of the same shape and data type without having to rerun the slow Python code and recompile.\n\nJaxprs also do not contain any information on elements that are not part of the inputs such as external variables, nor do they contain information on side effects.\n\nJaxprs can be visualized with the `jax.make_jaxpr` function:\n\n```{.python}\nimport jax\n\nx = jnp.array([1., 4., 3.])\ny = jnp.array([8., 1., 2.])\n\ndef f(x, y):\n    return 2 * x**2 + y\n\njax.make_jaxpr(f)(x, y) \n```\n\n```\n{ lambda ; a:f32[3] b:f32[3]. let\n    c:f32[3] = integer_pow[y=2] a\n    d:f32[3] = mul 2.0 c\n    e:f32[3] = add d b\n  in (e,) }\n```\n\nLet's add a print function to `f`:\n\n```{.python}\ndef f(x, y):\n    print(\"This is a function with side-effect\")\n    return 2 * x**2 + y\n\njax.make_jaxpr(f)(x, y)\n```\n\n```\n{ lambda ; a:f32[3] b:f32[3]. let\n    c:f32[3] = integer_pow[y=2] a\n    d:f32[3] = mul 2.0 c\n    e:f32[3] = add d b\n  in (e,) }\n```\n\nThe jaxpr is exactly the same. This is why printing will happen during tracing (when the Python code is run), but not afterwards (when the compiled code using the jaxpr is run).\n\n:::\n\n## Why the constraints?\n\nThe more constraints you add to a programming language, the more optimization you can get from the compiler. Speed comes at the cost of convenience.\n\nFor instance, consider a Python list. It is an extremely convenient and flexible object: heterogeneous, mutable... You can do anything with it. But computations on lists are extremely slow.\n\nNumPy's ndarrays are more constrained (homogeneous), but the type constraint permits the creation of a much faster language (NumPy is written in C and Fortran as well as Python) with vectorization, optimizations, and a greatly improved performance.\n\nJAX takes it further: by using an intermediate representation and very strict constraints on type, pure functional programming, etc., yet more optimizations can be achieved and you can optimize your own functions with JIT compilation and the XLA. Ultimately, this is what makes JAX so fast.\n\n## The good news\n\nThe good news is that Flax used to rely on [the Linen API](https://flax-linen.readthedocs.io/en/latest/) which followed JAX closely. It was very elegant and respected JAX extremely closely: updating model parameters and optimizer state could not be done as a side-effect and the models were thus stateless. Stateless models frameworks follow a functional programming approach in which the parameters are separate from the model and passed as inputs to the forward pass along with the data. This is also the case of the Julia package [Lux](https://lux.csail.mit.edu/) (a modern rewrite of Flux [with explicit model parameters and a philosophy similar to JAX's](https://lux.csail.mit.edu/dev/introduction/overview)).\n\nElegant, yes, but nobody was using Flax because it was just too obscure. People were using libraries such as [Equinox](https://github.com/patrick-kidger/equinox) instead because they were a lot easier and more familiar to PyTorch users.\n\nFlax [entirely changed its API](https://flax.readthedocs.io/en/latest/why.html). The new API (NNX) now deals with stateful models *à la PyTorch*. JAX's idiosyncrasies are mostly dealt with by Flax under the hood (you still need to be aware of them though to prevent you from making mistakes when dealing with `jnp.array`) and Flax code is now not so dissimilar from PyTorch while still making use of the great AD, JIT, XLA, and same code on all devices.\n\n",
    "supporting": [
      "fl_numpy_files"
    ],
    "filters": [],
    "includes": {}
  }
}