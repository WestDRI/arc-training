{
  "hash": "07c7f94e2c0fcce045f957a3adc60392",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Data augmentation\nauthor: Marie-Hélène Burle\n---\n\n:::{.def}\n\n\n\n:::\n\n:::{.callout-note collapse=\"true\"}\n\n## Minimal necessary code from previous sections\n\n```{.python}\nbase_dir = \"<path-of-the-nabirds-dir>\"\n```\n\n:::{.note}\n\nTo be replaced by proper path.\n\n:::\n\n\n\n::: {#c457bdf7 .cell execution_count=3}\n``` {.python .cell-code}\nimport os\nimport polars as pl\nimport imageio.v3 as iio\nimport grain.python as grain\n\nmetadata = pl.read_parquet(\"metadata.parquet\")\nmetadata_train = metadata.filter(pl.col(\"is_training_img\") == 1)\n\nclass NABirdsDataset:\n    \"\"\"NABirds dataset class.\"\"\"\n    def __init__(self, metadata_file, data_dir):\n        self.metadata_file = metadata_file\n        self.data_dir = data_dir\n    def __len__(self):\n        return len(self.metadata_file)\n    def __getitem__(self, idx):\n        path = os.path.join(\n            self.data_dir,\n            self.metadata_file.get_column('path')[idx]\n        )\n        img = iio.imread(path)\n        id = self.metadata_file.get_column('id')[idx].replace('_', ' ')\n        photographer = self.metadata_file.get_column('photographer')[idx].replace('_', ' ')\n        element = {\n            'image': img,\n            'id': id,\n            'photographer': photographer,\n        }\n        return element\n\ncleaned_img_dir = os.path.join(base_dir, \"cleaned_images\")\n\nnabirds_train = NABirdsDataset(\n    metadata_train,\n    cleaned_img_dir\n)\n\nnabirds_train_sampler = grain.IndexSampler(\n    num_records=200,\n    shuffle=True,\n    seed=0\n)\n\nnabirds_train_dl = grain.DataLoader(\n    data_source=nabirds_train,\n    sampler=nabirds_train_sampler,\n    worker_count=0,\n    operations=[\n        grain.Batch(batch_size=32)\n    ]\n)\n```\n:::\n\n\n:::\n\n## What is data augmentation?\n\nThe [AlbumentationsX](https://github.com/albumentations-team/AlbumentationsX) site has a [good explanation of the concept of data augmentation](https://albumentations.ai/docs/1-introduction/what-are-image-augmentations/).\n\ncite (from bib tex file):\n- https://arxiv.org/abs/2205.01491\n\n\n## Tools\n\n[PIX](https://github.com/google-deepmind/dm_pixk) provides low-level, JAX-native image processing primitives that can be directly jitted and vmapped.\n\n\ncite (from bib tex file):\n- paper on libraries\n\n\n\n[TorchVision transforms](https://docs.pytorch.org/vision/0.8/transforms.html)\n\n[skimage.transform](https://scikit-image.org/docs/0.25.x/api/skimage.transform.html) from [scikit-image](https://github.com/scikit-image/scikit-image) (that we used previously to create a Transform that resizes our images with padding).\n\n:::{.emph}\n\nWe are shifting paradigm here and moving from the CPU to the GPU. This is where JAX comes in.\n\n:::\n\n## Techniques\n\n[PIX](https://github.com/google-deepmind/dm_pix) [list of augmentations](https://dm-pix.readthedocs.io/en/latest/api.html#)\n\n[Pseudorandom numbers in JAX](https://docs.jax.dev/en/latest/random-numbers.html)\n\n## Choosing the techniques\n\n### The standard stack\n\nFor 90% of classification tasks (e.g., ResNet/EfficientNet on natural images), use this baseline. It consists of **3 distinct operations**:\n\n1.  **Geometric (Spatial):** Random Resized Crop (The single most important augmentation).\n2.  **Orientation:** Horizontal Flip (If the object isn't text or directional).\n3.  **Photometric (Color):** Mild Color Jitter (Brightness/Contrast/Saturation).\n\n**Why this works:** It forces the model to learn that the object is the same regardless of scale, position, orientation, or lighting.\n\n### 2. The Danger of \"Too Many\"\n\nThe biggest mistake beginners make is **Sequential Stacking**.\n*   **Bad:** Apply Rotation AND Shear AND Blur AND Noise AND Gamma AND Jitter to *every* image.\n*   **Result:** The image becomes a gray blob. The model learns nothing.\n\n**The Solution:** Use **Probabilities** or **\"OneOf\"** blocks.\n*   **Probability:** Set `p=0.5` for each technique. The statistical chance of an image getting hit by *all 5* augmentations becomes very low ($0.5^5 \\approx 3\\%$).\n*   **OneOf (Albumentations):** Define a block of 3 techniques (e.g., Blur, Noise, Compression) and tell the pipeline: *\"Pick exactly one of these to apply.\"*\n\n### Modern Best Practice: \"RandAugment\"\n\nIf you are struggling to decide how many to pick, stop guessing and use **RandAugment** (or TrivialAugment).\n\nThis is the standard for State-of-the-Art models today. instead of you manually picking \"Rotation\" and \"Shear,\" you set two parameters:\n1.  **N (Number):** How many transformations to apply sequentially (usually **N=2**).\n2.  **M (Magnitude):** How strong the effect is (usually **M=9** out of 10).\n\nThe algorithm then randomly selects 2 augmentations from a bank of 14 possibilities for each batch. This saves you from tuning hyperparameters.\n\n### Adjusting based on Dataset Size\n\nThe size of your dataset dictates how aggressively you should augment.\n\n| Dataset Size | Strategy | Recommended Count |\n| :--- | :--- | :--- |\n| **Tiny (<1k images)** | **Heavy Augmentation.** You are terrified of overfitting. You need to fake more data. | **4-6 techniques.** Use strong geometric shifts (rotation, shear) and perhaps synthetic generation (MixUp). |\n| **Medium (1k - 50k)** | **Standard Augmentation.** Balance variety with training speed. | **3-4 techniques.** (Crop, Flip, Color, maybe Rotation). |\n| **Massive (>1M images)** | **Light Augmentation.** The data itself already provides diversity. Augmentation slows down the CPU. | **1-2 techniques.** (Usually just Random Resized Crop and Flip). |\n\n### Advanced Techniques (MixUp / CutMix)\n\nOnce you have your standard stack (Crop/Flip/Color), you can add **one** \"regularization\" technique on top. These are mixed into the batch training:\n*   **MixUp:** Blending two images together.\n*   **CutMix:** Cutting a square from one image and pasting it onto another.\n\n**Recommendation:** Do not count these as \"standard\" augmentations. Treat them as a final boosting layer. Use **one** of them if your validation loss is plateauing too early.\n\n### Summary Checklist\n\n1.  **Start with 3:** Random Resized Crop + Horizontal Flip + Color Jitter.\n2.  **Visualize:** Look at a batch of 32 images. If they look destroyed, reduce the intensity or probability.\n3.  **Use Automation:** If using PyTorch or TensorFlow, implement `RandAugment` (N=2, M=9) and stop worrying about manual selection.\n\n## Choosing bounds\n\nPicking the right bounds for each type of data augmentation involves balancing dataset diversity against image realism. If the range is too narrow, you don't get much benefit, if it's too wide, you might destroy critical features or create unrealistic images that confuse the model.\n\n### Default range\n\nCheck the industry-standards (look at the literature, ask an LLM, etc.).\n\n:::{.notenoline}\n\nExample for gamma:\n\nFor most computer vision tasks (natural images, object detection, classification), the industry-standard starting point is 0.8 to 1.2.\n\nThis range simulates subtle lighting variations—like a cloud passing over the sun or a slight difference in camera exposure—without washing out the image or making it too dark to see details.\n\n:::\n\n### Domain specific ranges\n\nYou might want to adjust the values based on your specific data type.\n\n:::{.notenoline}\n\nExample for gamma:\n\nYou can increase the range for OCR (document analysis) because scanned documents often have wildly varying contrast and because text usually remains legible even under extreme gamma.\n\n:::\n\n### Visual sanity check\n\nNever set augmentation parameters blindly. Visualize some tests to ensure the data you are using to train is still reasonable (and to make sure that you aren't messing something up and getting totally absurd results!).\n\n### In our case\n\nColours are critical for species differentiation, so we don't want to mess with that. So playing with hue or solarization would be a bad idea as it might invalidate the labels (making one species actually look like another). Vertical flips or 90° rotations would not be great either as they wouldn't produce realistic data...\n\nThe most important techniques in this case are geometric augmentation ones such as random crops and horizontal flips.\n\nWe can also do some photometric augmentation as long as they don't invalidate the labels: \n\n::: {#60289d6f .cell execution_count=4}\n``` {.python .cell-code}\nimport dm_pix as pix\nimport PIL.Image as Image\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport jax.numpy as jnp\n\ndef apply_gamma(img, gamma):\n    new_img = pix.adjust_gamma(\n        image=img,\n        gamma=gamma\n    )\n    pil_img = Image.fromarray(\n        np.asarray(new_img * 255.).astype(np.uint8), 'RGB'\n    )\n    gamma = round(gamma, 1)\n    return pil_img, gamma\n\ndef show_tests(img, gamma_range):\n    jnp_img = jnp.array(img, dtype=jnp.float32) / 255.\n    pics = []\n    gammas = []\n\n    for i in gamma_range:\n        pics.append(apply_gamma(jnp_img, i)[0])\n        gammas.append(apply_gamma(jnp_img, i)[1])\n\n    fig, axes = plt.subplots(3, 4, figsize=(8, 6))\n    axes = axes.flatten()\n\n    for i, ax in enumerate(axes):\n        ax.imshow(pics[i])\n        ax.axis('off')\n        ax.set_title(f'Gamma = {gammas[i]}', fontsize=9)\n    plt.tight_layout()\n    plt.show()\n```\n:::\n\n\n::: {#ce58eb44 .cell execution_count=5}\n``` {.python .cell-code}\nshow_tests(nabirds_train[0]['image'], np.linspace(0.1, 3, 12))\nshow_tests(nabirds_train[1]['image'], np.linspace(0.1, 3, 12))\nshow_tests(nabirds_train[2]['image'], np.linspace(0.1, 3, 12))\nshow_tests(nabirds_train[3]['image'], np.linspace(0.1, 3, 12))\n```\n\n::: {.cell-output .cell-output-display}\n![](jxai_augmentation_files/figure-html/cell-5-output-1.png){width=740 height=566}\n:::\n\n::: {.cell-output .cell-output-display}\n![](jxai_augmentation_files/figure-html/cell-5-output-2.png){width=740 height=566}\n:::\n\n::: {.cell-output .cell-output-display}\n![](jxai_augmentation_files/figure-html/cell-5-output-3.png){width=740 height=566}\n:::\n\n::: {.cell-output .cell-output-display}\n![](jxai_augmentation_files/figure-html/cell-5-output-4.png){width=740 height=566}\n:::\n:::\n\n\nLet's use [random gamma](https://dm-pix.readthedocs.io/en/latest/api.html#random-gamma) with the min and max values at 0.6 and 1.3 respectively.\n\n### Validation check\n\nTrain a small version of your model (or for fewer epochs) and check whether the validation loss improves with data augmentation compared to no augmentation.\n\n<!-- ```{python} -->\n<!-- from jax import random -->\n\n\n<!-- ``` -->\n\n",
    "supporting": [
      "jxai_augmentation_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}