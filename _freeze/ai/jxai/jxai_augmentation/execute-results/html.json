{
  "hash": "93587987c6ccff879ad60f22f699d26f",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Data augmentation\nauthor: Marie-Hélène Burle\nbibliography: jxai.bib\ncsl: ../../diabetologia.csl\n---\n\n:::{.def}\n\nKey to deep learning is data augmentation. This section explains what it is and what techniques we should use in our example.\n\n:::\n\n:::{.callout-note collapse=\"true\"}\n\n## Minimal necessary code from previous sections\n\n```{.python}\nbase_dir = '<path-of-the-nabirds-dir>'\n```\n\n:::{.notenoit}\n\nTo be replaced by actual path: in our training cluster, the `base_dir` is at `/project/def-sponsor00/nabirds`:\n\n```{.python}\nbase_dir = '/project/def-sponsor00/nabirds'\n```\n\n:::\n\n\n\n::: {#daf9b284 .cell execution_count=3}\n``` {.python .cell-code}\nimport os\nimport polars as pl\nimport imageio.v3 as iio\nimport grain.python as grain\n\n\nmetadata = pl.read_parquet('metadata.parquet')\nmetadata_train = metadata.filter(pl.col('is_training_img') == 1)\nmetadata_val = metadata.filter(pl.col('is_training_img') == 0)\ncleaned_img_dir = os.path.join(base_dir, 'cleaned_images')\n\n\nclass NABirdsDataset:\n    \"\"\"NABirds dataset class.\"\"\"\n\n    def __init__(self, metadata, data_dir):\n        self.metadata = metadata\n        self.data_dir = data_dir\n\n    def __len__(self):\n        return len(self.metadata)\n\n    def __getitem__(self, idx):\n        path = os.path.join(self.data_dir, self.metadata.get_column('path')[idx])\n        img = iio.imread(path)\n        species_name = self.metadata.get_column('species_name')[idx]\n        species_id = self.metadata.get_column('species_id')[idx]\n        photographer = self.metadata.get_column('photographer')[idx]\n\n        return {\n            'img': img,\n            'species_name': species_name,\n            'species_id': species_id,\n            'photographer': photographer,\n        }\n\n\nnabirds_train = NABirdsDataset(metadata_train, cleaned_img_dir)\nnabirds_val = NABirdsDataset(metadata_val, cleaned_img_dir)\n```\n:::\n\n\n:::\n\n## What is data augmentation?\n\nTraining deep learning models requires vast amounts of labelled data. The more diverse and numerous the data, the better the models will perform on new, unseen data.\n\nA number of labelled datasets exist, but they are only so big. Moreover, for specific applications, you will need to fine-tune models on specific data for which there might not be any labelled data. And labelling data is costly and time-consuming. In some cases, it might even be impossible because little unlabelled data exist (think for instance of rare tumours).\n\nData augmentation is the artificial creation of new data by modifying the existing data in small ways. It is very powerful to avoid overfitting, particularly where datasets are small (if you were lucky enough to have a huge dataset, you wouldn't need to bother with data augmentation which would actually create an additional computationally costly step with extremely little benefit).\n\nXu *et al.* performed a comprehensive survey of image augmentation techniques for deep learning in 2023 [-@Xu_2023], but the field is evolving very fast with new techniques coming out all the time.\n\n## Augmentation libraries\n\nData augmentation being such a classic technique, many libraries offer sets of augmentation tools. Among the very popular, we can list:\n\n- [AlbumentationsX](https://github.com/albumentations-team/AlbumentationsX),\n- [TorchVision transforms](https://docs.pytorch.org/vision/0.8/transforms.html),\n- [skimage.transform](https://scikit-image.org/docs/0.25.x/api/skimage.transform.html) from [scikit-image](https://github.com/scikit-image/scikit-image).\n\nThere are others.\n\nAmarù *et al.* created a curated repository of libraries for data augmentation in computer vision [-@jimaging9100232].\n\nIn this course, we will use [PIX](https://github.com/google-deepmind/dm_pix), a library built for JAX which provides low-level, JAX-native image processing primitives that can be directly jitted and vmapped.\n\n*We are shifting paradigm here and moving from the CPU to the GPU. This is where JAX comes in.*\n\n## Augmentation techniques\n\nThere are many techniques:\n\n- Geometric transformations (flips, rotations, scaling, crops...).\n- Color space transformations (brightness, contrast, gamma, hue, saturation, grayscale conversion, channel shuffling...).\n- Noise transformations (Gaussian noise, blurring...).\n- Occlusive transformations (erasing parts of the image).\n- Mixing images (various techniques mixing images and appropriately applying the same treatment to labels).\n\n## Choosing techniques\n\n### How many to use?\n\nThe size of your dataset dictates how aggressively you should augment.\n\n| Dataset size | Strategy | Recommended count |\n| :--- | :--- | :--- |\n| **Tiny (<1k images)** | **Heavy augmentation.** You should be worried about overfitting, so you need to create more artificial data | **4-6 techniques** |\n| **Medium (1k - 100k)** | **Standard augmentation.** Balance variety with training speed | **3-4 techniques** |\n| **Large (>100K images)** | **Light augmentation.** | **1-2 techniques**  |\n| **Massive (>1M images)** | **No augmentation**. The data itself provides enough diversity. Augmentation will only slow down training. | **0 technique** |\n\n<br>Our dataset contains about 50K images (including the validation set). This puts us in the medium category and a standard approach with **3 or 4 techniques** should be reasonable.\n\n### Which ones to use?\n\nThe choice depends on the problem.\n\nIn our case, we are dealing with fine-grained birds identification. Colours are critical for species differentiation, so we don't want to mess with that. This means that playing with hue, solarization, color jitter, or gray scale could be a bad idea as it might invalidate the labels (making one species actually look like another). Transformations that do not preserve the aspect ratio (squashing, warping) could be bad too as they might change the shape of discriminant features.\n\nVertical flips or 90° rotations would not be very useful as they wouldn't produce realistic data ...\n\nThere are a lot of more advanced techniques out there (CutMix, SnapMix, TransMix, Attention Drop), but let's start with easier ones. For geometric techniques, let's do **random crops** and **random horizontal flips**.\n\nWe can also do some photometric augmentation as long as they don't invalidate the labels: brightness and contrast. If we want the model to be able to identify black and white pictures, we definitely need to remove the colour dependence by using a technique turning images to gray (with some probability and level of colour removal). If, on the other hand, we are only interested in having a model able to identify colour images, we want to stay away from this.\n\nLet's consider for instance **random gamma adjustments** and **random contrast**. They simulate different exposure levels and will improve the model's robustness and performance by making it less sensitive to variations in lighting conditions.\n\n### Choosing the parameters\n\nPicking the right bounds for each type of data augmentation involves balancing dataset diversity against image realism. If the range is too narrow, you don't get much benefit, if it's too wide, you might destroy critical features or create unrealistic images that confuse the model.\n\n#### Default range\n\nCheck the industry-standards (look at the literature, ask an LLM, etc.).\n\nFor random crops, we don't want to go too hard or we will crop out the distinguishing features of the birds.\n\nFor gamma, for most computer vision tasks (natural images, object detection, classification), the industry-standard starting point is 0.8 to 1.2.\n\nThis range simulates subtle lighting variations—like a cloud passing over the sun or a slight difference in camera exposure—without washing out the image or making it too dark to see details. This should be good for us.\n\n#### Domain specific ranges\n\nYou might want to adjust the values based on your specific data type.\n\nFor instance, you can increase the gamma range for OCR (document analysis) because scanned documents often have wildly varying contrast and because text usually remains legible even under extreme gamma.\n\nNone of this applies to our example.\n\n#### Visual sanity check\n\nNever set augmentation parameters blindly. Visualize some tests to ensure the data you are using to train is still reasonable (and to make sure that you aren't messing something up and getting totally absurd results!).\n\nLet's test various gamma values on the first 4 images in the training set:\n\n::: {#8d112d72 .cell execution_count=4}\n``` {.python .cell-code}\nimport dm_pix as pix\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport jax.numpy as jnp\n```\n:::\n\n\nLet's write a helper function that applies PIX deterministic [adjust_gamma](https://dm-pix.readthedocs.io/en/latest/api.html#adjust-gamma) function to a JAX array image, then covert it to an RGB image that we can display:\n\n::: {#b82c7f9b .cell execution_count=5}\n``` {.python .cell-code}\ndef apply_gamma(img, gamma):\n    \"\"\"\n    Apply gamma transformation to a JAX array image\n    then turn it back into an RGB image for display.\n    \"\"\"\n    new_img = pix.adjust_gamma(\n        image=img,\n        gamma=gamma\n    )\n    rgb_img = (new_img * 255.0).astype(np.uint8)\n    gamma = round(gamma, 1)\n    return rgb_img, gamma\n```\n:::\n\n\nThis next function will take a NumPy image from our Dataset class, turn it to a JAX array, apply our helper function `apply_gamma`, and display the result:\n\n::: {#47fcaf29 .cell execution_count=6}\n``` {.python .cell-code}\ndef show_tests(img, gamma_range):\n    \"\"\"\n    Turn the image into a JAX array,\n    run our apply_gamma function on it\n    (apply gamma then turn back to RGB image),\n    plot the RGB image.\n    \"\"\"\n    jnp_img = jnp.array(img, dtype=jnp.float32) / 255.\n    pics = []\n    gammas = []\n\n    for i in gamma_range:\n        pics.append(apply_gamma(jnp_img, i)[0])\n        gammas.append(apply_gamma(jnp_img, i)[1])\n\n    fig, axes = plt.subplots(3, 4, figsize=(8, 6))\n    axes = axes.flatten()\n\n    for i, ax in enumerate(axes):\n        ax.imshow(pics[i])\n        ax.axis('off')\n        ax.set_title(f'Gamma = {gammas[i]}', fontsize=9)\n    plt.tight_layout()\n    plt.show()\n```\n:::\n\n\nNow we can apply it to a few images to get an idea of the effect and ensure nothing weird is going on. This will help us catch a coding mistake that could ruin the whole training process:\n\n::: {#d1db4467 .cell execution_count=7}\n``` {.python .cell-code}\nshow_tests(nabirds_train[0]['img'], np.linspace(0.1, 3, 12))\n```\n\n::: {.cell-output .cell-output-display}\n![](jxai_augmentation_files/figure-html/cell-7-output-1.png){width=740 height=566}\n:::\n:::\n\n\n0.6 to 1.2 seem ok.\n\n::: {#ede7ca3c .cell execution_count=8}\n``` {.python .cell-code}\nshow_tests(nabirds_train[1]['img'], np.linspace(0.1, 3, 12))\n```\n\n::: {.cell-output .cell-output-display}\n![](jxai_augmentation_files/figure-html/cell-8-output-1.png){width=740 height=566}\n:::\n:::\n\n\n0.6 to 2.2 seem reasonable for this image.\n\n::: {#3f9dd3a7 .cell execution_count=9}\n``` {.python .cell-code}\nshow_tests(nabirds_train[2]['img'], np.linspace(0.1, 3, 12))\n```\n\n::: {.cell-output .cell-output-display}\n![](jxai_augmentation_files/figure-html/cell-9-output-1.png){width=740 height=566}\n:::\n:::\n\n\n0.6 to 2.2 seem ok for this one.\n\n::: {#8e91c8e7 .cell execution_count=10}\n``` {.python .cell-code}\nshow_tests(nabirds_train[3]['img'], np.linspace(0.1, 3, 12))\n```\n\n::: {.cell-output .cell-output-display}\n![](jxai_augmentation_files/figure-html/cell-10-output-1.png){width=740 height=566}\n:::\n:::\n\n\n0.6 to 1.4 seem reasonable here.\n\nFor training, we will use [random gamma](https://dm-pix.readthedocs.io/en/latest/api.html#random-gamma) as one of our transformation (we don't want the deterministic function here since we want different transformations randomly applied at each epoch) with the min and max values set a bit more broadly than what is standard as our images seem to handle it OK. Let's go with a min and max of 0.6 and 1.2 respectively.\n\nYou need to do similar checks for all augmentations that might lead to unpredictable results: you don't want to try to train a model on images that are all black, crippled by artifacts, or smeared beyond recognition!\n\n#### Validation check\n\nYou can adjust the probabilities and magnitudes of the various augmentation techniques you chose based on the validation performance you get during training.\n\nYou can train a small version of your model (or for fewer epochs) and check how the validation loss improves with various variations of your augmentation strategy.\n\nI recently gave a [webinar on MLflow](/ai/mlops/wb_mlflow.qmd) which would be the perfect tool for this kind of comparison.\n\n## Our plan for the train set\n\n### Image normalization\n\nBefore anything else, we need to normalize our images.\n\nOur images are currently stored as NumPy arrays of type integers ranging from 0 to 255 (the values of the pixels):\n\n::: {#17a4dac5 .cell execution_count=11}\n``` {.python .cell-code}\nprint(nabirds_train[0]['img'].dtype)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nuint8\n```\n:::\n:::\n\n\nIt is a classic normalization technique to divide these pixel values by 255 to bring them in the 0-1 range. This improves performance and stability in training.\n\nTo ensure that all features contribute equally and improve performance, it is also a standard practice to turn the pictures into [standard scores (z-scores)](https://en.wikipedia.org/wiki/Standard_score) by subtracting each data point by the mean and dividing by the standard deviation.\n\nIn our case, we have to do it because the ViT model that we will eventually use was pretrained on images preprocessed this way and it is important to use the same normalization as was used for the pretrained model.\n\nLet's create a first transformation that does exactly that:\n\n::: {#96f504d9 .cell execution_count=12}\n``` {.python .cell-code}\nimport numpy as np\n\nclass Normalize(grain.MapTransform):\n    def map(self, element):\n        img = element['img']\n        # Image preprocessing matches the one of pretrained ViT\n        mean = np.array([0.5, 0.5, 0.5], dtype=np.float32)\n        std = np.array([0.5, 0.5, 0.5], dtype=np.float32)\n        img = img.astype(np.float32) / 255.0\n        img_norm = (img - mean) / std\n        element['img'] = img_norm\n        return element\n```\n:::\n\n\n:::{.callout-tip collapse=\"true\"}\n\n## Equivalent using PyTorch\n\nTorchVision comes with many transformations, but custom Transforms can also be written in PyTorch. Here is an example for a simple normalization and casting of images:\n\n```{.python}\nclass NormAndCastPyTorch(object):\n    \"\"\"Transform class to normalize and cast images to float32.\"\"\"\n    def __call__(self, element):\n        element['img'] = np.asarray(element['img'], dtype=np.float32) / 255.0\n        return element\n```\n\nTo combine multiple Transforms, you use [`torchvision.transforms.Compose`](https://docs.pytorch.org/vision/0.8/transforms.html#torchvision.transforms.Compose), one of the [TorchVision Transforms](https://docs.pytorch.org/vision/0.8/transforms.html) (or you can use the newer [transforms v2](https://docs.pytorch.org/vision/main/auto_examples/transforms/plot_transforms_getting_started.html)).\n\nIn PyTorch, you would apply the Transforms as you instantiate your Dataset class since the Dataset class has a `transform` argument in which to pass them:\n\n```{.python}\nnabirds_norm_train_pytorch = NABirdsDataset(\n    metadata_train,\n    os.path.join(base_dir, img_dir),\n    transform=NormAndCastPyTorch()\n)\n```\n\nWith Grain, you pass the combined Transforms as an argument of the DataLoader class, as we saw in the previous section.\n\n:::\n\n### Our augmentation strategy\n\n:::{.callout-note collapse=\"true\"}\n\n## Pseudorandom numbers in JAX\n\nBecause we apply random transformations to our data, we need to talk about [pseudorandom numbers in JAX](https://docs.jax.dev/en/latest/random-numbers.html#random-numbers-in-jax).\n\nJAX has a mechanism to handle pseudorandom number generation (PRNG) that solves the problems that the PRNG mechanisms used by NumPy, Julia, R, and many other languages suffer from: parallelization and vectorization.\n\nIt is not based on an implicit global random state; instead, it tracks the state explicitly via a random `key` (an array with a special dtype). Random functions do not modify the key. This means that you can't reuse a key (to avoid spurious correlations). But the key is splittable into as many new keys as you want. This is what allows the parallelization and vectorization of stochasticity.\n\nYou start by generating a key with:\n\n```{.python}\nfrom jax import random\n\nkey = random.key(123)  # 123 can be replaced by any other number\n                       # reuse the same number for replicability\n```\n\nYou then split the key with:\n\n```{.python}\nkey, subkey = random.split(key)\n```\n\nThis uses the initial key (the argument of the `split` method), so you can't reuse it. But it creates two new keys (the two on the left of the equal sign). You can name these new keys however you want, but to ensure that we don't reuse the initial key by accident, we re-assign its name to one of the new key.\n\nNow, you can use `subkey` in the next random function. And if later on you need another one, you split `key` (the new one) again in the same way.\n\nYou can also create multiple new keys at once:\n\n```{.python}\nkey, subkey1, subkey2, subkey3 = random.split(key, num=4)\n```\n\n:::\n\nRandom crop:\n\n::: {#c3c8ef24 .cell execution_count=13}\n``` {.python .cell-code}\nfrom jax import random\nimport dm_pix as pix\n\nkey = random.key(31)\nkey, subkey = random.split(key)\n\nclass RandomCrop(grain.MapTransform):\n    def map(self, element):\n        element['img'] = pix.random_crop(\n            key=subkey,\n            image=element['img'],\n            crop_sizes=(224, 224, 3)\n        )\n        return element\n```\n:::\n\n\nRandom flip:\n\n::: {#dc3830c9 .cell execution_count=14}\n``` {.python .cell-code}\nkey, subkey = random.split(key)\n\nclass RandomFlip(grain.MapTransform):\n    def map(self, element):\n        element['img'] = pix.random_flip_left_right(\n            key=subkey,\n            image=element['img']\n        )\n        return element\n```\n:::\n\n\nRandom contrast:\n\n::: {#8f67dc97 .cell execution_count=15}\n``` {.python .cell-code}\nkey, subkey = random.split(key)\n\nclass RandomContrast(grain.MapTransform):\n    def map(self, element):\n        element['img'] = pix.random_contrast(\n            key=subkey,\n            image=element['img'],\n            lower=0.8,\n            upper=1.2\n        )\n        return element\n```\n:::\n\n\nRandom gamma (using the bounds we tested earlier):\n\n::: {#7f5fe382 .cell execution_count=16}\n``` {.python .cell-code}\nkey, subkey = random.split(key)\n\nclass RandomGamma(grain.MapTransform):\n    def map(self, element):\n        element['img'] = pix.random_gamma(\n            key=subkey,\n            image=element['img'],\n            min_gamma=0.6,\n            max_gamma=1.2\n        )\n        return element\n```\n:::\n\n\n### Pass to DataLoader\n\nWe can now pass the transformations to the DataLoader. We can combine Transforms together very easily with Grain (no need of a Compose class with Grain as with TorchVision):\n\n::: {#e83f17c1 .cell execution_count=17}\n``` {.python .cell-code}\nseed = 123\ntrain_batch_size = 32\n\n# Train set sampler:\ntrain_sampler = grain.IndexSampler(\n    num_records=len(nabirds_train),\n    shuffle=True,                      # We shuffle the training set\n    seed=seed,\n    shard_options=grain.NoSharding(),  # No sharding for a single-device setup\n    num_epochs=1                       # The default (None) provides an infinite stream of data\n                                       # but we only want to iterate over the entire data once\n)\n\n# Train set DataLoader:\ntrain_loader = grain.DataLoader(\n    data_source=nabirds_train,\n    sampler=train_sampler,\n    operations=[\n        Normalize(),\n        RandomCrop(),\n        RandomFlip(),\n        RandomContrast(),\n        RandomGamma(),\n        grain.Batch(train_batch_size, drop_remainder=True)\n    ]\n)\n```\n:::\n\n\n### The importance of checks\n\nWe have our DataLoader for the training set and we could start training, potentially spending hours or days trying to train a model ... with terrible results.\n\n[Before starting to train, it is crutial to check that your samples look the way you expect them to.]{.emph}\n\nLet's get some info on a batch:\n\n::: {#f034b3df .cell execution_count=18}\n``` {.python .cell-code}\ntrain_batch = next(iter(train_loader))\n\nprint(f\"\"\"\ntraining batch info:\n- img shape is {train_batch['img'].shape} and data type is {train_batch['img'].dtype}\n- species_name shape is {train_batch['species_name'].shape} and data type is {train_batch['species_name'].dtype}\n- species_id shape is {train_batch['species_id'].shape} and data type is {train_batch['species_id'].dtype}\n- photographer shape is {train_batch['photographer'].shape} and data type is {train_batch['photographer'].dtype}\n\"\"\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\ntraining batch info:\n- img shape is (32, 224, 224, 3) and data type is float32\n- species_name shape is (32,) and data type is <U26\n- species_id shape is (32,) and data type is int64\n- photographer shape is (32,) and data type is <U34\n\n```\n:::\n:::\n\n\nAll looks as expected.\n\nAnd let's plot a few processed training images:\n\n::: {#f365652a .cell execution_count=19}\n``` {.python .cell-code}\nfig = plt.figure(figsize=(8, 8))\n\nfor i in range(12):\n    ax = plt.subplot(3, 4, i + 1)\n    plt.tight_layout()\n    ax.set_title(\n        f\"\"\"\n        {train_batch['species_name'][i]}\n        Picture by {train_batch['photographer'][i]}\n        \"\"\",\n        fontsize=7,\n        linespacing=1.5\n    )\n    ax.axis('off')\n    rgb_img = (\n        (train_batch['img'][i] - train_batch['img'][i].min()) / (train_batch['img'][i].max() - train_batch['img'][i].min()) * 255.0\n    ).astype(np.uint8)\n\n    plt.imshow(rgb_img)\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](jxai_augmentation_files/figure-html/cell-19-output-1.png){width=755 height=679}\n:::\n:::\n\n\nOh no!!!! What happened?\n\nWe checked carefully that our gamma adjustment bounds are reasonable and we chose very moderate brightness adjustment, so what happened?\n\nZ-score normalization creates negative values (roughly half the pixels will be negative). But `dm-pix` expects `[0, 1]` inputs, so by default, it clips all negative values to 0. This immediately turns half your image black.\n\nWhy does `dm-pix` do this clipping? Without this, the gamma math would fail: gamma correction uses exponentiation (pixel to the power of gamma) and you cannot raise negative numbers to a floating-point power.\n\nAs a result, all the pixels that were darker than average (negative z-scores) were forced to pure black (value of 0).\n\nWhat is the solution?\n\nWe could change the order of the transformations and move our `Normalize` class *after* the PIX augmentations. But this would also fail because PIX expects `[0, 1]` values and we have `[0, 255]` values.\n\nSo we have no choice but to break our normalization into 2 steps. Here is the order of transformations that we need to use:\n\n- Convert the integers 0–255 images to floats 0–1 images.\n- Apply random gamma, brightness, etc. from the PIX library.\n- Apply the specific mean/std normalization expected by the model.\n\nNote that different augmentation libraries might have different behaviours, so **you need to check carefully what inputs the augmentation library you use expects**.\n\nSo let's fix our problem.\n\nFirst, split our `Normalize` class into two steps.\n\nA first step to turn the RGB integers to 0–1 floats:\n\n::: {#841784b9 .cell execution_count=20}\n``` {.python .cell-code}\nclass ToFloat(grain.MapTransform):\n    def map(self, element):\n        element['img'] = element['img'].astype(np.float32) / 255.0\n        return element\n```\n:::\n\n\nAnd a second step to create the z-scores:\n\n::: {#1d0e643d .cell execution_count=21}\n``` {.python .cell-code}\nclass ZScore(grain.MapTransform):\n    def map(self, element):\n        img = element['img']\n        mean = np.array([0.5, 0.5, 0.5], dtype=np.float32)\n        std = np.array([0.5, 0.5, 0.5], dtype=np.float32)\n        img = (img - mean) / std\n        element['img'] = img\n        return element\n```\n:::\n\n\nFinally, we apply the transformations in the proper order:\n\n::: {#47dd8081 .cell execution_count=22}\n``` {.python .cell-code}\ntrain_loader = grain.DataLoader(\n    data_source=nabirds_train,\n    sampler=train_sampler,\n    operations=[\n        ToFloat(),\n        RandomCrop(),\n        RandomFlip(),\n        RandomContrast(),\n        RandomGamma(),\n        ZScore(),\n        grain.Batch(train_batch_size, drop_remainder=True)\n    ]\n)\n```\n:::\n\n\nWe recreate a batch:\n\n::: {#fdd4ca44 .cell execution_count=23}\n``` {.python .cell-code}\ntrain_batch = next(iter(train_loader))\n```\n:::\n\n\nLet's plot a sample again:\n\n::: {#6fbda825 .cell execution_count=24}\n``` {.python .cell-code}\nfig = plt.figure(figsize=(8, 8))\n\nfor i in range(12):\n    ax = plt.subplot(3, 4, i + 1)\n    plt.tight_layout()\n    ax.set_title(\n        f\"\"\"\n        {train_batch['species_name'][i]}\n        Picture by {train_batch['photographer'][i]}\n        \"\"\",\n        fontsize=7,\n        linespacing=1.5\n    )\n    ax.axis('off')\n    rgb_img = (\n        (train_batch['img'][i] - train_batch['img'][i].min()) / (train_batch['img'][i].max() - train_batch['img'][i].min()) * 255.0\n    ).astype(np.uint8)\n\n    plt.imshow(rgb_img)\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](jxai_augmentation_files/figure-html/cell-24-output-1.png){width=755 height=679}\n:::\n:::\n\n\nThis time, things look as expected. Phew!\n\n## What about the validation set?\n\nYou don't want to apply any augmentation to the evaluation set. However, these images must match the normalization of the training set. Here, it is ok to use our `Normalize` class that combines both normalizations.\n\n### Pass to DataLoader\n\nPass transformations to the DataLoader:\n\n::: {#dc2eb3cc .cell execution_count=25}\n``` {.python .cell-code}\nval_batch_size = 2 * train_batch_size\n\n# Validation set sampler:\nval_sampler = grain.IndexSampler(\n    num_records=len(nabirds_val),\n    shuffle=False,                     # We don't shuffle the validation set\n    seed=seed,\n    shard_options=grain.NoSharding(),\n    num_epochs=1\n)\n\n# Validation set DataLoader:\nval_loader = grain.DataLoader(\n    data_source=nabirds_val,\n    sampler=val_sampler,\n    # worker_count=4,                 # If you have the resources (I don't)\n    # worker_buffer_size=2,\n    operations=[\n        Normalize(),\n        grain.Batch(val_batch_size)\n    ]\n)\n```\n:::\n\n\n### Checks\n\nHere again, we want to make sure all is fine by gathering some info on a batch:\n\n::: {#7dcfcad8 .cell execution_count=26}\n``` {.python .cell-code}\nval_batch = next(iter(val_loader))\n\nprint(f\"\"\"\nValidation batch info:\n- img shape is {val_batch['img'].shape} and data type is {val_batch['img'].dtype}\n- species_name shape is {val_batch['species_name'].shape} and data type is {val_batch['species_name'].dtype}\n- species_id shape is {val_batch['species_id'].shape} and data type is {val_batch['species_id'].dtype}\n- photographer shape is {val_batch['photographer'].shape} and data type is {val_batch['photographer'].dtype}\n\"\"\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nValidation batch info:\n- img shape is (64, 224, 224, 3) and data type is float32\n- species_name shape is (64,) and data type is <U26\n- species_id shape is (64,) and data type is int64\n- photographer shape is (64,) and data type is <U34\n\n```\n:::\n:::\n\n\nAnd displaying some samples:\n\n::: {#42586c83 .cell execution_count=27}\n``` {.python .cell-code}\nfig = plt.figure(figsize=(8, 8))\n\nfor i in range(12):\n    ax = plt.subplot(3, 4, i + 1)\n    plt.tight_layout()\n    ax.set_title(\n        f\"\"\"\n        {val_batch['species_name'][i]}\n        Picture by {val_batch['photographer'][i]}\n        \"\"\",\n        fontsize=7,\n        linespacing=1.5\n    )\n    ax.axis('off')\n    rgb_img = (\n        (val_batch['img'][i] - val_batch['img'][i].min()) / (val_batch['img'][i].max() - val_batch['img'][i].min()) * 255.0\n    ).astype(np.uint8)\n\n    plt.imshow(rgb_img)\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](jxai_augmentation_files/figure-html/cell-27-output-1.png){width=751 height=679}\n:::\n:::\n\n\nEverything looks good here. It is now time to talk about models.\n\n",
    "supporting": [
      "jxai_augmentation_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}