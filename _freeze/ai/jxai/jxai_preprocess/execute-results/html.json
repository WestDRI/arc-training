{
  "hash": "b5cbd2aeb048ccc9c7642cb1d31b7121",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Data preprocessing\nauthor: Marie-Hélène Burle\n---\n\n:::{.def}\n\nIn this section, we look at the images and create new ones that we save to disk as a preprocessing step.\n\nWe also create a Dataset class and instantiate one instance for the training set and one for the evaluation set using the preprocessed images.\n\n:::\n\n## Load the metadata DataFrame\n\n```{.python}\nbase_dir = '<path-of-the-nabirds-dir>'\n```\n\n:::{.notenoit}\n\nTo be replaced by actual path: in our training cluster, the `base_dir` is at `/project/def-sponsor00/nabirds`:\n\n```{.python}\nbase_dir = '/project/def-sponsor00/nabirds'\n```\n\n:::\n\n\n\nLet's read our Parquet file back in:\n\n::: {#f6b6cb3b .cell execution_count=2}\n``` {.python .cell-code}\nimport polars as pl\n\nmetadata = pl.read_parquet('metadata.parquet')\n```\n:::\n\n\n## Read in images\n\nTo read in the images, there are many options, including:\n\n- [PIL.Image.open](https://pillow.readthedocs.io/en/stable/reference/Image.html#PIL.Image.open) from [Pillow](https://github.com/python-pillow/Pillow),\n- `cv2.imread` from [OpenCV](https://github.com/opencv/opencv),\n- [skimage.io.imread](https://scikit-image.org/docs/stable/api/skimage.io.html#skimage.io.imread) from [scikit-image](https://github.com/scikit-image/scikit-image).\n\nHere, we are using `imageio.imread` from [imageio](https://github.com/imageio/imageio) which is an excellent option because it automatically creates a NumPy ndarrays, choosing a dtype based on the image, and it is faster than other options ([scikit-image](https://github.com/scikit-image/scikit-image) actually uses it now instead of their own implementation).\n\n## Initial Dataset class\n\n::: {#1fd3eb10 .cell execution_count=3}\n``` {.python .cell-code}\nimport os\nimport imageio.v3 as iio\n\nclass NABirdsDataset:\n    \"\"\"NABirds dataset class.\"\"\"\n    def __init__(self, metadata_file, data_dir):\n        self.metadata_file = metadata_file\n        self.data_dir = data_dir\n    def __len__(self):\n        return len(self.metadata_file)\n    def __getitem__(self, idx):\n        path = os.path.join(\n            self.data_dir,\n            self.metadata_file.get_column('path')[idx]\n        )\n        img = iio.imread(path)\n        species = self.metadata_file.get_column('species')[idx].replace('_', ' ')\n        photographer = self.metadata_file.get_column('photographer')[idx].replace('_', ' ')\n        bbx = self.metadata_file.get_column('bb_x')[idx]\n        bby = self.metadata_file.get_column('bb_y')[idx]\n        bbw = self.metadata_file.get_column('bb_width')[idx]\n        bbh = self.metadata_file.get_column('bb_height')[idx]\n        element = {\n            'img': img,\n            'species': species,\n            'photographer': photographer,\n            'bbx' : bbx,\n            'bby' : bby,\n            'bbw' : bbw,\n            'bbh' : bbh\n        }\n        return element\n```\n:::\n\n\n:::{.callout-tip collapse=\"true\"}\n\n## Equivalent using PyTorch\n\nPyTorch provides [`torch.utils.data.Dataset`](https://docs.pytorch.org/docs/stable/data.html#torch.utils.data.Dataset), an abstract class representing a dataset. You need to write a subclass of `torch.utils.data.Dataset` (let's call it `NABirdsDataset`) so that it inherits from `torch.utils.data.Dataset`, but with characteristics matching our own dataset.\n\nA PyTorch custom Dataset class must implement three methods:\n\n- `__init__`: initializes a new instance (object) of the class,\n- `__len__`: returns the number of elements in the new dataset class, and\n- `__getitem__`: loads and returns an element from the dataset at a given index `idx`:\n\n```{.python}\nfrom torch.utils.data import Dataset\n\nclass NABirdsDatasetPyTorch(Dataset):\n    \"\"\"NABirds dataset class.\"\"\"\n    def __init__(self, metadata_file, data_dir, transform=None):\n        self.metadata_file = metadata_file\n        self.data_dir = data_dir\n        self.transform = transform\n    def __len__(self):\n        return len(self.metadata_file)\n    def __getitem__(self, idx):\n        path = os.path.join(\n            self.data_dir,\n            self.metadata_file.get_column('path')[idx]\n        )\n        img = iio.imread(path)\n        species = self.metadata_file.get_column('species')[idx].replace('_', ' ')\n        photographer = self.metadata_file.get_column('photographer')[idx].replace('_', ' ')\n        bbx = self.metadata_file.get_column('bb_x')[idx]\n        bby = self.metadata_file.get_column('bb_y')[idx]\n        bbw = self.metadata_file.get_column('bb_width')[idx]\n        bbh = self.metadata_file.get_column('bb_height')[idx]\n        element = {\n            'img': img,\n            'species': species,\n            'photographer': photographer,\n            'bbx' : bbx,\n            'bby' : bby,\n            'bbw' : bbw,\n            'bbh' : bbh\n        }\n        if self.transform:\n            element = self.transform(element)\n        return element\n```\n\n:::\n\n## Instantiate initial class\n\n::: {#e44cc6bc .cell execution_count=4}\n``` {.python .cell-code}\nimg_dir = os.path.join(base_dir, 'images')\n\nnabirds_initial = NABirdsDataset(\n    metadata,\n    img_dir\n)\n```\n:::\n\n\n## Print an element\n\n::: {#8e834fd1 .cell execution_count=5}\n``` {.python .cell-code}\nnext(iter(nabirds_initial))\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\n{'img': array([[[ 48,  46,  49],\n         [ 52,  50,  53],\n         [ 54,  52,  53],\n         ...,\n         [ 84,  84,  82],\n         [ 86,  86,  84],\n         [ 90,  90,  88]],\n \n        [[ 47,  45,  48],\n         [ 50,  48,  51],\n         [ 52,  50,  51],\n         ...,\n         [ 84,  84,  82],\n         [ 85,  85,  83],\n         [ 88,  88,  86]],\n \n        [[ 51,  49,  50],\n         [ 53,  51,  52],\n         [ 54,  52,  53],\n         ...,\n         [ 83,  83,  81],\n         [ 83,  83,  81],\n         [ 87,  87,  85]],\n \n        ...,\n \n        [[222, 221, 226],\n         [221, 220, 225],\n         [221, 220, 225],\n         ...,\n         [ 88,  88,  88],\n         [ 87,  85,  88],\n         [ 89,  87,  90]],\n \n        [[220, 219, 224],\n         [220, 219, 224],\n         [220, 219, 224],\n         ...,\n         [ 88,  88,  88],\n         [ 86,  84,  87],\n         [ 88,  86,  89]],\n \n        [[220, 219, 224],\n         [220, 219, 224],\n         [220, 219, 224],\n         ...,\n         [ 88,  88,  88],\n         [ 85,  83,  86],\n         [ 87,  85,  88]]], shape=(341, 296, 3), dtype=uint8),\n 'species': 'Oak Titmouse',\n 'photographer': 'Ruth Cantwell',\n 'bbx': 83,\n 'bby': 59,\n 'bbw': 128,\n 'bbh': 228}\n```\n:::\n:::\n\n\n:::{.note}\n\nNote the image values between 0 and 255.\n\n:::\n\n## Display a sample of data\n\nLet's display the first 4 images and their bounding boxes (remember that we have to display the photographers names as a requirement of this dataset):\n\n::: {#e8219038 .cell execution_count=6}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\nfig = plt.figure(figsize=(8, 9))\n\nfor i, element in enumerate(nabirds_initial):\n    ax = plt.subplot(2, 2, i + 1)\n    plt.tight_layout()\n    ax.set_title(\n        f\"\"\"\n        Species: {element['species']}\n        Picture by {element['photographer']}\n        \"\"\",\n        fontsize=9,\n        linespacing=1.5\n    )\n    ax.axis('off')\n    plt.imshow(element['img'])\n    rect = patches.Rectangle(\n        (element['bbx'], element['bby']),\n        element['bbw'],\n        element['bbh'],\n        linewidth=1,\n        edgecolor='r',\n        facecolor='none'\n    )\n    ax.add_patch(rect)\n    if i == 3:\n        plt.show()\n        break\n```\n\n::: {.cell-output .cell-output-display}\n![](jxai_preprocess_files/figure-html/cell-7-output-1.png){width=708 height=828}\n:::\n:::\n\n\n## Print info on a sample of data\n\n::: {#a9be1935 .cell execution_count=7}\n``` {.python .cell-code}\nfor i, element in enumerate(nabirds_initial):\n    print(f\"Image dimensions: {element['img'].shape}, data type: {element['img'].dtype}\")\n    if i == 3:\n        break\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nImage dimensions: (341, 296, 3), data type: uint8\nImage dimensions: (427, 640, 3), data type: uint8\nImage dimensions: (1024, 730, 3), data type: uint8\nImage dimensions: (680, 1024, 3), data type: uint8\n```\n:::\n:::\n\n\nNotice how the images are all of different sizes. This is a problem because neural networks need images of the same size.\n\nWe are also not making use of the bounding boxes this dataset comes with. This means that we have a large number of pixels we know do not contain any bird part.\n\nLastly, our images are fairly large (often up to 1024 pixels in width or height). Classification models often come with a few variants for a handful of different image sizes, but the most standard size is 224 by 224 (good compromise between detail and speed).\n\nIn this section, we do the first step of addressing these problems: we crop the images to the bounding boxes, with a 20% margin, and save them to disk. The rest will be addressed during the data augmentation step (and resizing for the evaluation set).\n\n## Cleaning Dataset class\n\nWe could write a function to clean our images. Another, more elegant approach is to create a class:\n\n::: {#261db82a .cell execution_count=8}\n``` {.python .cell-code}\nclass CleaningDataset:\n    \"\"\"Cleaning dataset class.\"\"\"\n    def __init__(self, metadata_file, source_dir, target_dir):\n        self.metadata_file = metadata_file\n        self.source_dir = source_dir\n        self.target_dir = target_dir\n\n    def __len__(self):\n        return len(self.metadata_file)\n\n    def __getitem__(self, idx):\n        \"\"\"Returns cropped image and save path.\"\"\"\n\n        # Build paths\n        read_path = os.path.join(\n            self.source_dir,\n            self.metadata_file.get_column('path')[idx]\n        )\n        save_path = os.path.join(\n            self.target_dir,\n            self.metadata_file.get_column('path')[idx]\n        )\n\n        # Load image\n        try:\n            img = iio.imread(read_path)\n        except Exception as e:\n            print(f\"Error loading {filename}: {e}\")\n            return None, None\n\n        # If a file has an alpha channel, drop it\n        if img.shape[2] == 4: # <1>\n            img = img[:,:,:3]\n\n        # Get metadata\n        species = self.metadata_file.get_column('species')[idx].replace('_', ' ')\n        photographer = self.metadata_file.get_column('photographer')[idx].replace('_', ' ')\n\n        # Get bounding box data\n        bbx = self.metadata_file.get_column('bb_x')[idx]\n        bby = self.metadata_file.get_column('bb_y')[idx]\n        bbw = self.metadata_file.get_column('bb_width')[idx]\n        bbh = self.metadata_file.get_column('bb_height')[idx]\n\n        # Crop image with a 20% margin:\n        # 1. Get the image dimensions (to make sure we don't go out of bounds)\n        height, width = img.shape[:2]\n\n        # 2. Calculate the margin size (20% of the box dimensions)\n        # We use int() because pixel coordinates must be integers\n        margin_w = int(bbw * 0.2)\n        margin_h = int(bbh * 0.2)\n\n        # 3. Calculate the new coordinates with the margin\n        x1 = bbx - margin_w\n        y1 = bby - margin_h\n        x2 = bbx + bbw + margin_w\n        y2 = bby + bbh + margin_h\n\n        # 4. Set limits to coordinates to ensure they stay inside the image\n        # x1 and y1 cannot be less than 0\n        # x2 and y2 cannot be larger than the image width/height\n        x1 = max(0, x1)\n        y1 = max(0, y1)\n        x2 = min(width, x2)\n        y2 = min(height, y2)\n\n        # 5. Crop\n        img_cropped = img[y1:y2, x1:x2]\n\n        return img_cropped, save_path\n```\n:::\n\n\n1. Reason for this step:\n\n:::{.notenoit}\n\nEven serious, well curated datasets often contain inconsistent or erroneous data. After playing with this dataset, I realized that at least one image has 4 channels (RGBA, i.e. RGB and the alpha channel). This means that its NumPy array version has 4 instead of 3 dimensions...\n\nThis didn't make any sense to me since all the images were JPEG (it is easy to verify that with command line utilities such as [fd](https://github.com/sharkdp/fd)) and JPEG images do not have an alpha channel.\n\nSo I wrote a function that would return the path of the (first) image with an extra channel and I got `0344/3b69ce35b9404f3eb321100c93dd2b43.jpg`.\n\nIt *appears* to be a JPEG image. However, when passing it to the [identify](https://imagemagick.org/script/identify.php) command from [ImageMagick](https://imagemagick.org/), I realized that it was in fact a PNG image *mislabelled as a JPEG*...\n\nHere is an equivalent way to show this in Python:\n\n::: {#ef33802a .cell execution_count=9}\n``` {.python .cell-code}\nfrom PIL import Image\n\nimg = os.path.join(img_dir, '0344/3b69ce35b9404f3eb321100c93dd2b43.jpg')\n\nwith Image.open(img) as img:\n    print(f\"The actual format of the image is {img.format}.\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe actual format of the image is PNG.\n```\n:::\n:::\n\n\n:::\n\n## Create the clean data in parallel\n\nBuilt-in multiprocessing in Python can be done with either of the `ProcessPoolExecutor` class from the [`concurrent.futures` module](https://docs.python.org/3/library/concurrent.futures.html) or the `Pool` class from the [`multiprocessing` package](https://docs.python.org/3/library/multiprocessing.html).\n\nLet's use the first one:\n\n::: {#2c86f4fc .cell execution_count=10}\n``` {.python .cell-code}\nfrom concurrent.futures import ProcessPoolExecutor\nfrom tqdm import tqdm  # to display a progress bar\n```\n:::\n\n\nLet's instantiate our `CleaningDataset`:\n\n::: {#8a89368d .cell execution_count=11}\n``` {.python .cell-code}\ncleaned_img_dir = os.path.join(base_dir, 'cleaned_images')\n\ndataset_to_clean = CleaningDataset(\n    metadata_file=metadata,\n    source_dir=img_dir,\n    target_dir=cleaned_img_dir\n)\n```\n:::\n\n\nWe can now create a helper function:\n\n::: {#a10209ce .cell execution_count=12}\n``` {.python .cell-code}\ndef process_idx(i):\n    \"\"\"Helper function for the parallel worker.\"\"\"\n    img, path = dataset_to_clean[i]\n    if img is not None:\n        # Create target directory if it doesn't exist\n        os.makedirs(os.path.dirname(path), exist_ok=True)\n        # Save cleaned image\n        iio.imwrite(path, img)\n        return 1 # Success\n    return 0 # Failure\n```\n:::\n\n\nAnd run it in parallel:\n\n:::{.info}\n\nNote that you will not be able to run the following chunk:\n\n- Cropping 50,000 images takes a lot on the CPUs and if we do it all, we will probably crash the cluster.\n- Additionally, we would all be trying to write to the same path, creating weird file conflicts.\n\nI ran this ahead of time and already created the cropped files and I did not give you write access to the dataset.\n\nIf you want to run the code and experiment with various numbers of CPUs, you can do this later on your machine or on a production cluster.\n\n:::\n\n```{.python filename=\"Don't try to run this chunk in the training cluster.\"}\n# Use as many workers as you have CPU cores\nwith ProcessPoolExecutor() as executor:\n    # Map indices to the process function\n    results = list(tqdm(\n        executor.map(process_idx, range(len(dataset_to_clean))),\n        total=len(dataset_to_clean),\n        desc='Cleaning Images'\n    ))\n\nprint(f\"Done. {sum(results)} images processed.\")\n```\n\n```\nDone. 48562 images processed.\n```\n\nYou can watch the parallel work live with an application such as [htop](https://github.com/htop-dev/htop/) (on your machine or the cluster) or [btop](https://github.com/aristocratos/btop) (on your machine).\n\nHere is a screenshot I took from `btop` on my machine while creating the cleaned images. You can see that my 16 cores are working in parallel:\n\n![](img/multiprocessing_img_cleaning.png){width=\"70%\"}\n\n## Final Dataset class\n\nNow we can create a new, simplified Dataset class (we don't need the bounding boxes anymore):\n\n::: {#c6b58017 .cell execution_count=13}\n``` {.python .cell-code}\nclass NABirdsDataset:\n    \"\"\"NABirds dataset class.\"\"\"\n    def __init__(self, metadata_file, data_dir):\n        self.metadata_file = metadata_file\n        self.data_dir = data_dir\n\n    def __len__(self):\n        return len(self.metadata_file)\n\n    def __getitem__(self, idx):\n        path = os.path.join(self.data_dir, self.metadata_file.get_column('path')[idx])\n        img = iio.imread(path)\n        species = self.metadata_file.get_column('species')[idx].replace('_', ' ')\n        photographer = self.metadata_file.get_column('photographer')[idx].replace('_', ' ')\n        element = {\n            'img': img,\n            'species': species,\n            'photographer': photographer,\n        }\n\n        return element\n```\n:::\n\n\n## Training set\n\nWe instantiate this Dataset class with the training set, using the cropped images:\n\n::: {#7ea7e988 .cell execution_count=14}\n``` {.python .cell-code}\n# Filter only training set from the metadata DataFrame:\nmetadata_train = metadata.filter(pl.col('is_training_img') == 1)\n\n# Create Dataset class instance:\nnabirds_train = NABirdsDataset(metadata_train, cleaned_img_dir)\n```\n:::\n\n\n## Validation set\n\nAnd for the validation set, we need to instantiate a Dataset class with our validation data on the cropped images:\n\n::: {#17c11183 .cell execution_count=15}\n``` {.python .cell-code}\n# Filter the validation metadata in our DataFrame:\nmetadata_val = metadata.filter(pl.col('is_training_img') == 0)\n\n# Instantiate a Dataset class with the validation data:\nnabirds_val = NABirdsDataset(metadata_val, cleaned_img_dir)\n```\n:::\n\n\n## Print info on samples\n\nLet's print info on a sample of our training set:\n\n::: {#c2a1c813 .cell execution_count=16}\n``` {.python .cell-code}\nfor i, element in enumerate(nabirds_train):\n    print(f\"Image new dimensions: {element['img'].shape}, data type: {element['img'].dtype}\")\n    if i == 3:\n        break\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nImage new dimensions: (312, 688, 3), data type: uint8\nImage new dimensions: (739, 1024, 3), data type: uint8\nImage new dimensions: (722, 808, 3), data type: uint8\nImage new dimensions: (753, 896, 3), data type: uint8\n```\n:::\n:::\n\n\nAnd on a sample of our evaluation set:\n\n::: {#9e9eefac .cell execution_count=17}\n``` {.python .cell-code}\nfor i, element in enumerate(nabirds_val):\n    print(f\"Image new dimensions: {element['img'].shape}, data type: {element['img'].dtype}\")\n    if i == 3:\n        break\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nImage new dimensions: (318, 178, 3), data type: uint8\nImage new dimensions: (398, 227, 3), data type: uint8\nImage new dimensions: (528, 305, 3), data type: uint8\nImage new dimensions: (546, 366, 3), data type: uint8\n```\n:::\n:::\n\n\n## Display samples\n\nLet's display the first 4 cleaned images in our training set to make sure they look like what we expect:\n\n::: {#2788d012 .cell execution_count=18}\n``` {.python .cell-code}\nfig = plt.figure(figsize=(8, 9))\n\nfor i, element in enumerate(nabirds_train):\n    ax = plt.subplot(2, 2, i + 1)\n    plt.tight_layout()\n    ax.set_title(\n        f\"\"\"\n        Species: {element['species']}\n        Picture by {element['photographer']}\n        \"\"\",\n        fontsize=9,\n        linespacing=1.5\n    )\n    ax.axis('off')\n    plt.imshow(element['img'])\n    if i == 3:\n        plt.show()\n        break\n```\n\n::: {.cell-output .cell-output-display}\n![](jxai_preprocess_files/figure-html/cell-19-output-1.png){width=748 height=732}\n:::\n:::\n\n\nAnd let's do the same for the evaluation set:\n\n::: {#f238fd4f .cell execution_count=19}\n``` {.python .cell-code}\nfig = plt.figure(figsize=(8, 9))\n\nfor i, element in enumerate(nabirds_val):\n    ax = plt.subplot(2, 2, i + 1)\n    plt.tight_layout()\n    ax.set_title(\n        f\"\"\"\n        Species: {element['species']}\n        Picture by {element['photographer']}\n        \"\"\",\n        fontsize=9,\n        linespacing=1.5\n    )\n    ax.axis('off')\n    plt.imshow(element['img'])\n    if i == 3:\n        plt.show()\n        break\n```\n\n::: {.cell-output .cell-output-display}\n![](jxai_preprocess_files/figure-html/cell-20-output-1.png){width=615 height=828}\n:::\n:::\n\n\n",
    "supporting": [
      "jxai_preprocess_files"
    ],
    "filters": [],
    "includes": {}
  }
}