{
  "hash": "6409b717990a9d29d65a9969a9e6488d",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Data preprocessing\nauthor: Marie-Hélène Burle\n---\n\n:::{.def}\n\n\n\n:::\n\n## Load the metadata DataFrame\n\n```{.python}\nbase_dir = \"<path-of-the-nabirds-dir>\"\n```\n\n:::{.note}\n\nTo be replaced by actual path.\n\n:::\n\n\n\n::: {#39af085a .cell execution_count=3}\n``` {.python .cell-code}\nimport polars as pl\n\nmetadata = pl.read_parquet(\"metadata.parquet\")\n```\n:::\n\n\n## Training DataFrame\n\nNow we can get a subset of our metadata DataFrame with the training metadata only:\n\n::: {#5e72dc3d .cell execution_count=4}\n``` {.python .cell-code}\nmetadata_train = metadata.filter(pl.col(\"is_training_img\") == 1)\n```\n:::\n\n\nQuick sanity checks:\n\n::: {#fe31b1ea .cell execution_count=5}\n``` {.python .cell-code}\nprint(metadata_train.shape)\nprint(metadata_train.row(0))\nprint(metadata_train.columns)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(23929, 12)\n('0001afd4-99a1-4a67-b940-d419413e23b3', 307, 179, 492, 224, 645, 'Eared_Grebe_(Nonbreeding/juvenile)', '0645/0001afd499a14a67b940d419413e23b3.jpg', 'Laura_Erickson', 1024, 680, 1)\n['UUID', 'bb_x', 'bb_y', 'bb_width', 'bb_height', 'class', 'id', 'path', 'photographer', 'img_width', 'img_height', 'is_training_img']\n```\n:::\n:::\n\n\n## Read in images\n\nTo read in the images, there are many options, including:\n\n- [PIL.Image.open](https://pillow.readthedocs.io/en/stable/reference/Image.html#PIL.Image.open) from [Pillow](https://github.com/python-pillow/Pillow),\n- `cv2.imread` from [OpenCV](https://github.com/opencv/opencv),\n- [skimage.io.imread](https://scikit-image.org/docs/stable/api/skimage.io.html#skimage.io.imread) from [scikit-image](https://github.com/scikit-image/scikit-image).\n\nHere, we are using `imageio.imread` from [imageio](https://github.com/imageio/imageio) which is an excellent option because it automatically creates a NumPy ndarrays, choosing a dtype based on the image, and it is faster than other options ([scikit-image](https://github.com/scikit-image/scikit-image) actually uses it now instead of their own implementation).\n\n## Initial Dataset class\n\n::: {#2579a5d0 .cell execution_count=6}\n``` {.python .cell-code}\nimport os\nimport imageio.v3 as iio\n\nclass NABirdsDatasetInitial:\n    \"\"\"NABirds dataset class.\"\"\"\n    def __init__(self, metadata_file, data_dir):\n        self.metadata = metadata_file\n        self.data_dir = data_dir\n    def __len__(self):\n        return len(self.metadata)\n    def __getitem__(self, idx):\n        img_path = os.path.join(\n            self.data_dir,\n            self.metadata.get_column('path')[idx]\n        )\n        img = iio.imread(img_path)\n        img_id = self.metadata.get_column('id')[idx].replace('_', ' ')\n        img_photographer = self.metadata.get_column('photographer')[idx].replace('_', ' ')\n        img_bb_x = self.metadata.get_column('bb_x')[idx]\n        img_bb_y = self.metadata.get_column('bb_y')[idx]\n        img_bb_width = self.metadata.get_column('bb_width')[idx]\n        img_bb_height = self.metadata.get_column('bb_height')[idx]\n        element = {\n            'image': img,\n            'id': img_id,\n            'photographer': img_photographer,\n            'bbx' : img_bb_x,\n            'bby' : img_bb_y,\n            'bbwidth' : img_bb_width,\n            'bbheight' : img_bb_height\n        }\n        return element\n```\n:::\n\n\n## Instantiate our class\n\n::: {#ec530e7b .cell execution_count=7}\n``` {.python .cell-code}\nimg_dir = os.path.join(base_dir, \"images\")\n\nnabirds_train_initial = NABirdsDatasetInitial(\n    metadata_train,\n    img_dir\n)\n```\n:::\n\n\n## Print an element\n\n::: {#6c30ffab .cell execution_count=8}\n``` {.python .cell-code}\nnext(iter(nabirds_train_initial))\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```\n{'image': array([[[ 99, 119, 143],\n         [102, 122, 146],\n         [104, 124, 148],\n         ...,\n         [105, 116, 136],\n         [106, 117, 137],\n         [106, 117, 137]],\n \n        [[102, 122, 146],\n         [101, 121, 145],\n         [100, 120, 144],\n         ...,\n         [101, 112, 132],\n         [101, 112, 132],\n         [102, 113, 133]],\n \n        [[106, 124, 148],\n         [104, 122, 146],\n         [102, 120, 144],\n         ...,\n         [ 99, 110, 130],\n         [ 99, 110, 130],\n         [101, 112, 132]],\n \n        ...,\n \n        [[112, 129, 159],\n         [115, 132, 162],\n         [116, 133, 163],\n         ...,\n         [102, 109, 127],\n         [101, 108, 126],\n         [ 97, 104, 122]],\n \n        [[115, 132, 162],\n         [117, 134, 164],\n         [118, 135, 165],\n         ...,\n         [ 98, 105, 123],\n         [101, 108, 126],\n         [101, 108, 126]],\n \n        [[116, 133, 163],\n         [117, 134, 164],\n         [118, 135, 165],\n         ...,\n         [ 98, 105, 123],\n         [102, 109, 127],\n         [101, 108, 126]]], shape=(680, 1024, 3), dtype=uint8),\n 'id': 'Eared Grebe (Nonbreeding/juvenile)',\n 'photographer': 'Laura Erickson',\n 'bbx': 307,\n 'bby': 179,\n 'bbwidth': 492,\n 'bbheight': 224}\n```\n:::\n:::\n\n\n:::{.note}\n\nNote the image values between 0 and 255.\n\n:::\n\n## Display a sample of data\n\nLet's display the first 4 images and their bounding boxes:\n\n::: {#7bf69e7e .cell execution_count=9}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\nfig = plt.figure(figsize=(8, 8))\n\nfor i, element in enumerate(nabirds_train_initial):\n    ax = plt.subplot(2, 2, i + 1)\n    plt.tight_layout()\n    ax.set_title(\n        f'Element {i}\\nIdentification: {element['id']}\\nPicture by {element['photographer']}',\n        fontsize=9\n    )\n    ax.axis('off')\n    plt.imshow(element['image'])\n    rect = patches.Rectangle(\n        (element['bbx'], element['bby']),\n        element['bbwidth'],\n        element['bbheight'],\n        linewidth=1,\n        edgecolor='r',\n        facecolor='none'\n    )\n    ax.add_patch(rect)\n    if i == 3:\n        plt.show()\n        break\n```\n\n::: {.cell-output .cell-output-display}\n![](jxai_preprocess_files/figure-html/cell-9-output-1.png){width=769 height=682}\n:::\n:::\n\n\n## Print info on a sample of data\n\n::: {#64c828ea .cell execution_count=10}\n``` {.python .cell-code}\nfor i, element in enumerate(nabirds_train_initial):\n    print(f'Image dimensions: {element['image'].shape}, data type: {element['image'].dtype}'\n    )\n    if i == 3:\n        break\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nImage dimensions: (680, 1024, 3), data type: uint8\nImage dimensions: (819, 1024, 3), data type: uint8\nImage dimensions: (768, 1024, 3), data type: uint8\nImage dimensions: (817, 1024, 3), data type: uint8\n```\n:::\n:::\n\n\nNotice how the images are all of different sizes. This is a problem. We are also not making use of the bounding boxes this dataset comes with, hence using parts of images we know do not contain any bird unnecessarily.\n\nWe will address these problems in the next section.\n\n## Needed transformations\n\nRaw data seldom works without being transformed.\n\nWe should get rid of the parts of the images that are outside of the bounding boxes containing the birds.\n\nAlso, a neural network will need images of the same size and our images come in all sorts of sizes.\n\nWhat size to chose? xxx\n\n## Cleaning Dataset class\n\n\n[](https://scikit-image.org/docs/0.25.x/api/skimage.transform.html#skimage.transform.resize)\n\n::: {#0e1b46e0 .cell execution_count=11}\n``` {.python .cell-code}\nimport imageio.v3 as iio\nfrom skimage.transform import resize\nimport numpy as np\n\nclass CleaningDataset:\n    \"\"\"Cleaning dataset class.\"\"\"\n    def __init__(self, metadata_file, source_dir, target_dir, target_size=(224, 224)):\n        self.metadata_file = metadata_file\n        self.source_dir = source_dir\n        self.target_dir = target_dir\n        self.target_size = target_size\n\n    def __len__(self):\n        return len(self.metadata_file)\n\n    def __getitem__(self, idx):\n        \"\"\"Returns (processed_img_array, save_path)\"\"\"\n\n        # Build paths\n        read_path = os.path.join(\n            self.source_dir,\n            self.metadata_file.get_column('path')[idx]\n        )\n        save_path = os.path.join(\n            self.target_dir,\n            self.metadata_file.get_column('path')[idx]\n        )\n\n        # Load image\n        try:\n            img = iio.imread(read_path)\n        except Exception as e:\n            print(f\"Error loading {filename}: {e}\")\n            return None, None\n\n        # If a file has an alpha channel, drop it\n        if img.shape[2] == 4: # <1>\n            img = img[:,:,:3]\n\n        # Get metadata\n        id = self.metadata_file.get_column('id')[idx].replace('_', ' ')\n        photographer = self.metadata_file.get_column('photographer')[idx].replace('_', ' ')\n\n        # Get bounding box data\n        bbx = self.metadata_file.get_column('bb_x')[idx]\n        bby = self.metadata_file.get_column('bb_y')[idx]\n        bbw = self.metadata_file.get_column('bb_width')[idx]\n        bbh = self.metadata_file.get_column('bb_height')[idx]\n\n        # Crop image\n        img_cropped = img[bby:bby+bbh, bbx:bbx+bbw]\n\n        # Resize img to target size with padding to avoid distortion\n        h, w, _ = img_cropped.shape\n        target_h, target_w = self.target_size\n\n        # Calculate the scaling factor to fit the image inside the box\n        scale = min(target_h / h, target_w / w)\n\n        # Calculate the new dimensions of the image\n        new_h, new_w = int(h * scale), int(w * scale)\n\n        # Resize\n        img_resized = resize(img_cropped, (new_h, new_w), anti_aliasing=True)\n\n        # Create a black canvas (zeros) of the target size\n        out_img = np.zeros((target_h, target_w, img.shape[2]), dtype=img_resized.dtype)\n\n        # Place the resized image in the center of the canvas\n        y_offset = (target_h - new_h) // 2\n        x_offset = (target_w - new_w) // 2\n        out_img[y_offset:y_offset+new_h, x_offset:x_offset+new_w] = img_resized\n\n        # Convert back to uint8 (0-255)\n        final_img = (out_img * 255).astype(np.uint8) # <2>\n\n        return final_img, save_path\n```\n:::\n\n\n1. xxx\n2. `skimage` returns elements with a `dtype(float64)` (values from 0 to 1), but we want to save space on disk.\n\n## Create the clean data in parallel\n\nBuilt-in multiprocessing in Python can be done with either of the `ProcessPoolExecutor` class from the [`concurrent.futures` module](https://docs.python.org/3/library/concurrent.futures.html) or the `Pool` class from the [`multiprocessing` package](https://docs.python.org/3/library/multiprocessing.html).\n\nLet's use the first one:\n\n::: {#cf82614a .cell execution_count=12}\n``` {.python .cell-code}\nfrom concurrent.futures import ProcessPoolExecutor\nfrom tqdm import tqdm  # to display a progress bar\n```\n:::\n\n\nLet's instantiate our `CleaningDataset`:\n\n::: {#384a1762 .cell execution_count=13}\n``` {.python .cell-code}\ncleaned_img_dir = os.path.join(base_dir, \"cleaned_images\")\n\ndataset_to_clean = CleaningDataset(\n    metadata_file=metadata_train,\n    source_dir=img_dir,\n    target_dir=cleaned_img_dir\n)\n```\n:::\n\n\nWe can now create a helper function:\n\n::: {#4d277c54 .cell execution_count=14}\n``` {.python .cell-code}\ndef process_idx(i):\n    \"\"\"Helper function for the parallel worker.\"\"\"\n    img, path = dataset_to_clean[i]\n    if img is not None:\n        # Create target directory if it doesn't exist\n        os.makedirs(os.path.dirname(path), exist_ok=True)\n        # Save cleaned image\n        iio.imwrite(path, img)\n        return 1 # Success\n    return 0 # Failure\n```\n:::\n\n\nAnd run it in parallel:\n\n```{.python}\n# Use as many workers as you have CPU cores\nwith ProcessPoolExecutor() as executor:\n    # Map indices to the process function\n    results = list(tqdm(\n        executor.map(process_idx, range(len(dataset_to_clean))),\n        total=len(dataset_to_clean),\n        desc=\"Cleaning Images\"\n    ))\n\nprint(f\"Done. {sum(results)} images processed.\")\n```\n\n```\nDone. 23929 images processed.\n```\n\nYou can watch the parallel work live with an application such as [htop](https://github.com/htop-dev/htop/) (on your machine or the cluster) or [btop](https://github.com/aristocratos/btop) (on your machine).\n\nHere is a screenshot I took from `btop` on my machine while creating the cleaned images. You can see that my 16 cores are working in parallel:\n\n![](img/multiprocessing_img_cleaning.png){width=\"70%\"}\n\n## New Dataset class\n\n::: {#cb9946ec .cell execution_count=15}\n``` {.python .cell-code}\nclass NABirdsDataset:\n    \"\"\"NABirds dataset class.\"\"\"\n    def __init__(self, metadata_file, data_dir):\n        self.metadata_file = metadata_file\n        self.data_dir = data_dir\n    def __len__(self):\n        return len(self.metadata_file)\n    def __getitem__(self, idx):\n        path = os.path.join(\n            self.data_dir,\n            self.metadata_file.get_column('path')[idx]\n        )\n        img = iio.imread(path)\n        id = self.metadata_file.get_column('id')[idx].replace('_', ' ')\n        photographer = self.metadata_file.get_column('photographer')[idx].replace('_', ' ')\n        element = {\n            'image': img,\n            'id': id,\n            'photographer': photographer,\n        }\n        return element\n```\n:::\n\n\n:::{.callout-tip collapse=\"true\"}\n\n## Equivalent using PyTorch\n\nPyTorch provides [`torch.utils.data.Dataset`](https://docs.pytorch.org/docs/stable/data.html#torch.utils.data.Dataset), an abstract class representing a dataset. You need to write a subclass of `torch.utils.data.Dataset` (let's call it `NABirdsDataset`) so that it inherits from `torch.utils.data.Dataset`, but with characteristics matching our own dataset.\n\nA PyTorch custom Dataset class must implement three methods:\n\n- `__init__`: initializes a new instance (object) of the class,\n- `__len__`: returns the number of elements in the new dataset class, and\n- `__getitem__`: loads and returns an element from the dataset at a given index `idx`:\n\n```{.python}\nfrom torch.utils.data import Dataset\n\nclass NABirdsDatasetPyTorch(Dataset):\n    \"\"\"NABirds dataset class.\"\"\"\n    def __init__(self, metadata_file, data_dir, transform=None):\n        self.metadata_file = metadata_file\n        self.data_dir = data_dir\n        self.transform = transform\n    def __len__(self):\n        return len(self.metadata_file)\n    def __getitem__(self, idx):\n        path = os.path.join(\n            self.data_dir,\n            self.metadata_file.get_column('path')[idx]\n        )\n        img = iio.imread(path)\n        id = self.metadata_file.get_column('id')[idx].replace('_', ' ')\n        photographer = self.metadata_file.get_column('photographer')[idx].replace('_', ' ')\n        element = {\n            'image': img,\n            'id': img_id,\n            'photographer': img_photographer,\n        }\n        if self.transform:\n            element = self.transform(element)\n        return element\n```\n\n:::\n\n## Instantiate\n\nNow we can instantiate the new class with the new data:\n\n::: {#439fe656 .cell execution_count=16}\n``` {.python .cell-code}\nnabirds_train = NABirdsDataset(\n    metadata_train,\n    cleaned_img_dir\n)\n```\n:::\n\n\n## Print info on a sample\n\n::: {#82582e2c .cell execution_count=17}\n``` {.python .cell-code}\nfor i, element in enumerate(nabirds_train):\n    print(f'Image new dimensions: {element['image'].shape}, data type: {element['image'].dtype}'\n    )\n    if i == 3:\n        break\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nImage new dimensions: (224, 224, 3), data type: uint8\nImage new dimensions: (224, 224, 3), data type: uint8\nImage new dimensions: (224, 224, 3), data type: uint8\nImage new dimensions: (224, 224, 3), data type: uint8\n```\n:::\n:::\n\n\n:::{.note}\n\nNotice how images are all of width and height `224` now.\n\n:::\n\n## Display a sample\n\nLet's display the first 4 cleaned images to make sure they look like what we expect:\n\n::: {#266a2680 .cell execution_count=18}\n``` {.python .cell-code}\nfig = plt.figure(figsize=(8, 8))\n\nfor i, element in enumerate(nabirds_train):\n    ax = plt.subplot(2, 2, i + 1)\n    plt.tight_layout()\n    ax.set_title(\n        f'Element {i}\\nIdentification: {element['id']}\\nPicture by {element['photographer']}',\n        fontsize=9\n    )\n    ax.axis('off')\n    plt.imshow(element['image'])\n    if i == 3:\n        plt.show()\n        break\n```\n\n::: {.cell-output .cell-output-display}\n![](jxai_preprocess_files/figure-html/cell-18-output-1.png){width=745 height=733}\n:::\n:::\n\n\n",
    "supporting": [
      "jxai_preprocess_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}