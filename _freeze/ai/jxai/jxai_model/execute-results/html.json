{
  "hash": "eb68cb9db4896b623e166223b500a7e3",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Model and training strategy\nauthor: Marie-Hélène Burle\nbibliography: jxai.bib\ncsl: diabetologia.csl\n---\n\n:::{.def}\n\n\n\n:::\n\n:::{.callout-note collapse=\"true\"}\n\n## Minimal necessary code from previous sections\n\n```{.python}\nbase_dir = '<path-of-the-nabirds-dir>'\n```\n\n:::{.notenoit}\n\nTo be replaced by actual path: in our training cluster, the `base_dir` is at `/project/def-sponsor00/nabirds`:\n\n```{.python}\nbase_dir = '/project/def-sponsor00/nabirds'\n```\n\n:::\n\n\n\n::: {#49e4f09f .cell execution_count=2}\n``` {.python .cell-code}\nimport os\nimport polars as pl\nimport imageio.v3 as iio\nimport grain.python as grain\n\n\nmetadata = pl.read_parquet('metadata.parquet')\nmetadata_train = metadata.filter(pl.col('is_training_img') == 1)\nmetadata_val = metadata.filter(pl.col('is_training_img') == 0)\ncleaned_img_dir = os.path.join(base_dir, 'cleaned_images')\n\n\nclass NABirdsDataset:\n    \"\"\"NABirds dataset class.\"\"\"\n\n    def __init__(self, metadata_file, data_dir):\n        self.metadata_file = metadata_file\n        self.data_dir = data_dir\n\n    def __len__(self):\n        return len(self.metadata_file)\n\n    def __getitem__(self, idx):\n        path = os.path.join(self.data_dir, self.metadata_file.get_column('path')[idx])\n        img = iio.imread(path)\n        species = self.metadata_file.get_column('species')[idx].replace('_', ' ')\n        photographer = self.metadata_file.get_column('photographer')[idx].replace('_', ' ')\n\n        return {\n            'img': img,\n            'species': species,\n            'photographer': photographer,\n        }\n\n\nnabirds_train = NABirdsDataset(metadata_train, cleaned_img_dir)\nnabirds_val = NABirdsDataset(metadata_val, cleaned_img_dir)\n```\n:::\n\n\n:::\n\n## Our strategy\n\n### Technique\n\n\n\n### Architecture\n\nOptions that would make sense for our example include ResNet, EfficientNet, and ViT.\n\n#### ResNet\n\n[ResNet](https://en.wikipedia.org/wiki/Residual_neural_network), or residual network, is a type of architecture in which the layers are reformulated as learning residual functions with reference to the layer inputs. This allows for deeper (and thus more performant) networks [@he2015deepresiduallearningimage]. This is the oldest of the options that make sense for us, but it is also the most robust.\n\n![from @he2015deepresiduallearningimage](img/resnet.jpg){fig-alt=\"noshadow\" width=\"70%\"}\n\n[ResNet-50](https://huggingface.co/microsoft/resnet-50) is available from [Hugging Face](https://huggingface.co/) and has become a classic CNN for image classification.\n\n#### EfficientNet\n\n[EfficientNet](https://en.wikipedia.org/wiki/EfficientNet) is a family of newer computer vision CNNs from Google that uses a compound coefficient to uniformly scale depth, width, and resolution of networks and achieves better accuracy with fewer parameters than other CNNs [@tan2020efficientnetrethinkingmodelscaling]. This makes them easier to train on fewer resources and can lead to better results. Tuning them is however harder than the more robust ResNet family.\n\n![from @tan2020efficientnetrethinkingmodelscaling](img/efficientnet.jpg){fig-alt=\"noshadow\"}\n\nThere are variations for different image sizes sizes, all available in Hugging Face. For instance:\n\n- [EfficientNet b0](https://huggingface.co/google/efficientnet-b0) for images of size 224x224\n- [EfficientNet b2](https://huggingface.co/google/efficientnet-b2) for images 260x260\n- [EfficientNet b3](https://huggingface.co/google/efficientnet-b3) for images 300x300\n- [EfficientNet b7](https://huggingface.co/google/efficientnet-b7) for images 600x600\n\n#### ViT\n\nWhile the other options were CNN, [ViT](https://en.wikipedia.org/wiki/Vision_transformer), or vision transformer, is a transformer architecture (initially created for NLP tasks) applied to computer vision tasks [@dosovitskiy2021imageworth16x16words]. This is a more recent technique that attains excellent results while training substantially fewer computational resources.\n\n![from @dosovitskiy2021imageworth16x16words](img/vit.jpg){fig-alt=\"noshadow\"}\n\n[ViT](https://huggingface.co/docs/transformers/en/model_doc/vit) is available in Hugging Face.\n\nWhich one to choose depends on the available hardware, libraries in the framework you want to use, and other practical considerations. If time permits, this is a good case of [experiment tracking](/ai/mlops/wb_mlflow) with [MLflow](https://github.com/mlflow/mlflow).\n\n### Pre-trained weights\n\n\n### Choice of library\n\n\n### Strategy summary\n\nCategory | Answer\n-- | -----\nTechnique | Transfer learning\nArchitecture | EfficientNet-B2 (EfficientNet-B0 or ResNet-50 are other reasonable options)\nPre-trained weights | ImageNet\nLibrary | \n\n## Implementation\n\n[Flax](https://github.com/google/flax) is the neural network library in the JAX AI stack.\n\n[Hugging Face transformers](https://github.com/huggingface/transformers) package.\n\n::: {#86485182 .cell execution_count=3}\n``` {.python .cell-code}\nimport jax\nimport jax.numpy as jnp\nfrom flax import nnx\nimport optax\nfrom transformers import FlaxViTForImageClassification\n```\n:::\n\n\n::: {#54a1a694 .cell execution_count=4}\n``` {.python .cell-code}\nclass VisionTransformer(nnx.Module):\n    \"\"\" Implements the ViT model, inheriting from `flax.nnx.Module`.\n\n    Args:\n        num_classes (int): Number of classes in the classification. Defaults to 1000.\n        in_channels (int): Number of input channels in the image (such as 3 for RGB). Defaults to 3.\n        img_size (int): Input image size. Defaults to 224.\n        patch_size (int): Size of the patches extracted from the image. Defaults to 16.\n        num_layers (int): Number of transformer encoder layers. Defaults to 12.\n        num_heads (int): Number of attention heads in each transformer layer. Defaults to 12.\n        mlp_dim (int): Dimension of the hidden layers in the feed-forward/MLP block. Defaults to 3072.\n        hidden_size (int): Dimensionality of the embedding vectors. Defaults to 3072.\n        dropout_rate (int): Dropout rate (for regularization). Defaults to 0.1.\n        rngs (flax.nnx.Rngs): A set of named `flax.nnx.RngStream` objects that generate a stream of JAX pseudo-random number generator (PRNG) keys. Defaults to `flax.nnx.Rngs(0)`.\n\n    \"\"\"\n    def __init__(\n        self,\n        num_classes: int = 1000,\n        in_channels: int = 3,\n        img_size: int = 224,\n        patch_size: int = 16,\n        num_layers: int = 12,\n        num_heads: int = 12,\n        mlp_dim: int = 3072,\n        hidden_size: int = 768,\n        dropout_rate: float = 0.1,\n        *,\n        rngs: nnx.Rngs = nnx.Rngs(0),\n    ):\n        # Calculate the number of patches generated from the image.\n        n_patches = (img_size // patch_size) ** 2\n        # Patch embeddings:\n        # - Extracts patches from the input image and maps them to embedding vectors\n        #   using `flax.nnx.Conv` (convolutional layer).\n        self.patch_embeddings = nnx.Conv(\n            in_channels,\n            hidden_size,\n            kernel_size=(patch_size, patch_size),\n            strides=(patch_size, patch_size),\n            padding='VALID',\n            use_bias=True,\n            rngs=rngs,\n        )\n\n        # Positional embeddings (add information about image patch positions):\n        # Set the truncated normal initializer (using `jax.nn.initializers.truncated_normal`).\n        initializer = jax.nn.initializers.truncated_normal(stddev=0.02)\n        # The learnable parameter for positional embeddings (using `flax.nnx.Param`).\n        self.position_embeddings = nnx.Param(\n            initializer(rngs.params(), (1, n_patches + 1, hidden_size), jnp.float32)\n        ) # Shape `(1, n_patches +1, hidden_size`)\n        # The dropout layer.\n        self.dropout = nnx.Dropout(dropout_rate, rngs=rngs)\n\n        # CLS token (a special token prepended to the sequence of patch embeddings)\n        # using `flax.nnx.Param`.\n        self.cls_token = nnx.Param(jnp.zeros((1, 1, hidden_size)))\n\n        # Transformer encoder (a sequence of encoder blocks for feature extraction).\n        # - Create multiple Transformer encoder blocks (with `nnx.Sequential`\n        # and `TransformerEncoder(nnx.Module)` which is defined later).\n        self.encoder = nnx.Sequential(*[\n            TransformerEncoder(hidden_size, mlp_dim, num_heads, dropout_rate, rngs=rngs)\n            for i in range(num_layers)\n        ])\n        # Layer normalization with `flax.nnx.LayerNorm`.\n        self.final_norm = nnx.LayerNorm(hidden_size, rngs=rngs)\n\n        # Classification head (maps the transformer encoder to class probabilities).\n        self.classifier = nnx.Linear(hidden_size, num_classes, rngs=rngs)\n\n    # The forward pass in the ViT model.\n    def __call__(self, x: jax.Array) -> jax.Array:\n        # Image patch embeddings.\n        # Extract image patches and embed them.\n        patches = self.patch_embeddings(x)\n        # Get the batch size of image patches.\n        batch_size = patches.shape[0]\n        # Reshape the image patches.\n        patches = patches.reshape(batch_size, -1, patches.shape[-1])\n\n        # Replicate the CLS token for each image with `jax.numpy.tile`\n        # by constructing an array by repeating `cls_token` along `[batch_size, 1, 1]` dimensions.\n        cls_token = jnp.tile(self.cls_token, [batch_size, 1, 1])\n        # Concatenate the CLS token and image patch embeddings.\n        x = jnp.concat([cls_token, patches], axis=1)\n        # Create embedded patches by adding positional embeddings to the concatenated CLS token and image patch embeddings.\n        embeddings = x + self.position_embeddings\n        # Apply the dropout layer to embedded patches.\n        embeddings = self.dropout(embeddings)\n\n        # Transformer encoder blocks.\n        # Process the embedded patches through the transformer encoder layers.\n        x = self.encoder(embeddings)\n        # Apply layer normalization\n        x = self.final_norm(x)\n\n        # Extract the CLS token (first token), which represents the overall image embedding.\n        x = x[:, 0]\n\n        # Predict class probabilities based on the CLS token embedding.\n        return self.classifier(x)\n\n\nclass TransformerEncoder(nnx.Module):\n    \"\"\"\n    A single transformer encoder block in the ViT model, inheriting from `flax.nnx.Module`.\n\n    Args:\n        hidden_size (int): Input/output embedding dimensionality.\n        mlp_dim (int): Dimension of the feed-forward/MLP block hidden layer.\n        num_heads (int): Number of attention heads.\n        dropout_rate (float): Dropout rate. Defaults to 0.0.\n        rngs (flax.nnx.Rngs): A set of named `flax.nnx.RngStream` objects that generate a stream of JAX pseudo-random number generator (PRNG) keys. Defaults to `flax.nnx.Rngs(0)`.\n    \"\"\"\n    def __init__(\n        self,\n        hidden_size: int,\n        mlp_dim: int,\n        num_heads: int,\n        dropout_rate: float = 0.0,\n        *,\n        rngs: nnx.Rngs = nnx.Rngs(0),\n    ) -> None:\n        # First layer normalization using `flax.nnx.LayerNorm`\n        # before we apply Multi-Head Attentn.\n        self.norm1 = nnx.LayerNorm(hidden_size, rngs=rngs)\n        # The Multi-Head Attention layer (using `flax.nnx.MultiHeadAttention`).\n        self.attn = nnx.MultiHeadAttention(\n            num_heads=num_heads,\n            in_features=hidden_size,\n            dropout_rate=dropout_rate,\n            broadcast_dropout=False,\n            decode=False,\n            deterministic=False,\n            rngs=rngs,\n        )\n        # Second layer normalization using `flax.nnx.LayerNorm`.\n        self.norm2 = nnx.LayerNorm(hidden_size, rngs=rngs)\n\n        # The MLP for point-wise feedforward (using `flax.nnx.Sequential`, `flax.nnx.Linear, flax.nnx.Dropout`)\n        # with the GeLU activation function (`flax.nnx.gelu`).\n        self.mlp = nnx.Sequential(\n            nnx.Linear(hidden_size, mlp_dim, rngs=rngs),\n            nnx.gelu,\n            nnx.Dropout(dropout_rate, rngs=rngs),\n            nnx.Linear(mlp_dim, hidden_size, rngs=rngs),\n            nnx.Dropout(dropout_rate, rngs=rngs),\n        )\n\n    # The forward pass through the transformer encoder block.\n    def __call__(self, x: jax.Array) -> jax.Array:\n        # The Multi-Head Attention layer with layer normalization.\n        x = x + self.attn(self.norm1(x))\n        # The feed-forward network with layer normalization.\n        x = x + self.mlp(self.norm2(x))\n        return x\n\n# Example usage for testing:\nx = jnp.ones((4, 224, 224, 3))\nmodel = VisionTransformer(num_classes=1000)\ny = model(x)\nprint('Predictions shape: ', y.shape)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPredictions shape:  (4, 1000)\n```\n:::\n:::\n\n\n::: {#cbdc2f10 .cell execution_count=5}\n``` {.python .cell-code}\ntf_model = FlaxViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nTensorFlow and JAX classes are deprecated and will be removed in Transformers v5. We recommend migrating to PyTorch classes or pinning your version of Transformers.\n```\n:::\n:::\n\n\n::: {#3fa14b48 .cell execution_count=6}\n``` {.python .cell-code}\n# Copies weights from a TF ViT model to a Flax ViT model, reshaping layers\n# to match the expected shapes in Flax.\ndef vit_inplace_copy_weights(*, src_model, dst_model):\n    assert isinstance(src_model, FlaxViTForImageClassification)\n    assert isinstance(dst_model, VisionTransformer)\n\n    tf_model_params = src_model.params\n    tf_model_params_fstate = nnx.traversals.flatten_mapping(tf_model_params)\n\n    # Notice the use of `flax.nnx.state`.\n    flax_model_params = nnx.state(dst_model, nnx.Param)\n    flax_model_params_fstate = dict(flax_model_params.flat_state())\n\n    # Mapping from Flax parameter names to TF parameter names.\n    params_name_mapping = {\n        ('cls_token',): ('vit', 'embeddings', 'cls_token'),\n        ('position_embeddings',): ('vit', 'embeddings', 'position_embeddings'),\n        **{\n            ('patch_embeddings', x): ('vit', 'embeddings', 'patch_embeddings', 'projection', x)\n            for x in ['kernel', 'bias']\n        },\n        **{\n            ('encoder', 'layers', i, 'attn', y, x): (\n                'vit', 'encoder', 'layer', str(i), 'attention', 'attention', y, x\n            )\n            for x in ['kernel', 'bias']\n            for y in ['key', 'value', 'query']\n            for i in range(12)\n        },\n        **{\n            ('encoder', 'layers', i, 'attn', 'out', x): (\n                'vit', 'encoder', 'layer', str(i), 'attention', 'output', 'dense', x\n            )\n            for x in ['kernel', 'bias']\n            for i in range(12)\n        },\n        **{\n            ('encoder', 'layers', i, 'mlp', 'layers', y1, x): (\n                'vit', 'encoder', 'layer', str(i), y2, 'dense', x\n            )\n            for x in ['kernel', 'bias']\n            for y1, y2 in [(0, 'intermediate'), (3, 'output')]\n            for i in range(12)\n        },\n        **{\n            ('encoder', 'layers', i, y1, x): (\n                'vit', 'encoder', 'layer', str(i), y2, x\n            )\n            for x in ['scale', 'bias']\n            for y1, y2 in [('norm1', 'layernorm_before'), ('norm2', 'layernorm_after')]\n            for i in range(12)\n        },\n        **{\n            ('final_norm', x): ('vit', 'layernorm', x)\n            for x in ['scale', 'bias']\n        },\n        **{\n            ('classifier', x): ('classifier', x)\n            for x in ['kernel', 'bias']\n        }\n    }\n\n    nonvisited = set(flax_model_params_fstate.keys())\n\n    for key1, key2 in params_name_mapping.items():\n        assert key1 in flax_model_params_fstate, key1\n        assert key2 in tf_model_params_fstate, (key1, key2)\n\n        nonvisited.remove(key1)\n\n        src_value = tf_model_params_fstate[key2]\n        if key2[-1] == 'kernel' and key2[-2] in ('key', 'value', 'query'):\n            shape = src_value.shape\n            src_value = src_value.reshape((shape[0], 12, 64))\n\n        if key2[-1] == 'bias' and key2[-2] in ('key', 'value', 'query'):\n            src_value = src_value.reshape((12, 64))\n\n        if key2[-4:] == ('attention', 'output', 'dense', 'kernel'):\n            shape = src_value.shape\n            src_value = src_value.reshape((12, 64, shape[-1]))\n\n        dst_value = flax_model_params_fstate[key1]\n        assert src_value.shape == dst_value.value.shape, (key2, src_value.shape, key1, dst_value.value.shape)\n        dst_value.value = src_value.copy()\n        assert dst_value.value.mean() == src_value.mean(), (dst_value.value, src_value.mean())\n\n    assert len(nonvisited) == 0, nonvisited\n    # Notice the use of `flax.nnx.update` and `flax.nnx.State`.\n    nnx.update(dst_model, nnx.State.from_flat_path(flax_model_params_fstate))\n\n\nvit_inplace_copy_weights(src_model=tf_model, dst_model=model)\n```\n:::\n\n\n::: {#46578871 .cell execution_count=7}\n``` {.python .cell-code}\nmodel.classifier = nnx.Linear(model.classifier.in_features, 405, rngs=nnx.Rngs(0))\n\nx = jnp.ones((4, 224, 224, 3))\ny = model(x)\nprint('Predictions shape: ', y.shape)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPredictions shape:  (4, 405)\n```\n:::\n:::\n\n\n<!-- ```{python} -->\n<!-- import jax -->\n<!-- import jax.numpy as jnp -->\n<!-- from flax import nnx -->\n<!-- from flax.nnx import bridge -->\n<!-- from transformers import FlaxViTForImageClassification -->\n\n<!-- class NABirdsViT(nnx.Module): -->\n<!--     def __init__(self, num_classes, *, rngs: nnx.Rngs): -->\n<!--         # 1. Load the pre-trained Linen model structure & weights from Hugging Face -->\n<!--         hf_model = FlaxViTForImageClassification.from_pretrained( -->\n<!--             'google/vit-base-patch16-224', -->\n<!--             num_labels=num_classes, -->\n<!--             ignore_mismatched_sizes=True # Necessary to overwrite the 1000-class head -->\n<!--         ) -->\n\n<!--         # 2. Bridge the Linen module to NNX -->\n<!--         # 'hf_model.module' is the underlying Linen module -->\n<!--         # 'hf_model.params' contains the pre-trained weights -->\n<!--         self.backbone = bridge.ToNNX(hf_model.module, rngs=rngs) -->\n\n<!--         # 3. Initialize and Load Weights -->\n<!--         # ToNNX requires a lazy initialization to structure the variables -->\n<!--         dummy_input = jnp.zeros((1, 3, 224, 224)) # ResNet expects NCHW by default in HF -->\n<!--         self.backbone.lazy_init(dummy_input) -->\n\n<!--         # Extract the empty NNX state -->\n<!--         _, backbone_state = nnx.split(self.backbone) -->\n\n<!--         # Copy weights from the loaded HF model into the NNX state -->\n<!--         # We define a helper to recursively copy the dictionary structure -->\n<!--         def copy_weights(target_state, source_dict): -->\n<!--             for key, value in source_dict.items(): -->\n<!--                 if isinstance(value, dict) or hasattr(value, 'items'): -->\n<!--                     copy_weights(target_state[key], value) -->\n<!--                 else: -->\n<!--                     # Assign the weight to the NNX Variable -->\n<!--                     target_state[key].value = value -->\n\n<!--         # HF stores weights in .params and batch stats in .batch_stats (if applicable) -->\n<!--         # We merge them to match the structure expected by the bridge -->\n<!--         full_linen_vars = {**hf_model.params} -->\n<!--         if hasattr(hf_model, 'batch_stats'): -->\n<!--             full_linen_vars['batch_stats'] = hf_model.batch_stats -->\n\n<!--         copy_weights(backbone_state, full_linen_vars) -->\n\n<!--         # Update the bridge with the loaded weights -->\n<!--         nnx.update(self.backbone, backbone_state) -->\n\n<!--     def __call__(self, x): -->\n<!--         # HF ResNet expects NCHW format (channels first) -->\n<!--         # If your data is NHWC (standard for JAX), transpose it: -->\n<!--         # x = jnp.transpose(x, (0, 3, 1, 2)) -->\n\n<!--         # Run the bridged model -->\n<!--         logits = self.backbone(x).logits -->\n<!--         return logits -->\n<!-- ``` -->\n\n<!-- Initialize the model: -->\n\n<!-- ```{python} -->\n<!-- rngs = nnx.Rngs(params=0, dropout=1) -->\n<!-- model = NABirdsViT(num_classes=405, rngs=rngs) -->\n<!-- ``` -->\n\n<!-- Test forward pass: -->\n\n<!-- ```{python} -->\n<!-- dummy_img = jax.random.normal(jax.random.key(0), (1, 3, 224, 224)) -->\n<!-- logits = model(dummy_img) -->\n<!-- print(f\"Output shape: {logits.shape}\") # (1, 555) -->\n<!-- ``` -->\n\n<!-- Forward pass: -->\n\n<!-- ```{python} -->\n<!-- x = jax.random.normal(jax.random.key(0), (1, 224, 224, 3)) -->\n<!-- logits = model(x) -->\n\n<!-- print(f\"Logits shape: {logits.shape}\") # (1, 555) -->\n<!-- print(\"Model initialized and pre-trained weights loaded via NNX bridge.\") -->\n<!-- ``` -->\n\n",
    "supporting": [
      "jxai_model_files"
    ],
    "filters": [],
    "includes": {}
  }
}