{
  "hash": "801067e9f944c719eb25aa26fbda01f6",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Defining a model architecture\nbibliography: fl.bib\ncsl: diabetologia.csl\nauthor:\n  - Marie-Hélène Burle\n  - Code adapted from JAX's [Implement ViT from scratch](https://docs.jaxstack.ai/en/latest/JAX_Vision_transformer.html)\n---\n\n:::{.def}\n\nIn this section, we define a model with [Flax](https://github.com/google/flax)'s new API called NNX.\n\n:::\n\n## Context\n\n```{dot}\n//| echo: false\n//| fig-width: 700px\n\ndigraph {\n\nbgcolor=\"transparent\"\nnode [fontname=\"Inconsolata, sans-serif\", color=gray55, fontsize=\"18pt\"]\nedge [color=gray55]\n\nload [label=\"Load data\", shape=plaintext, group=g1, fontcolor=gray55]\nproc [label=\"Process data\", shape=plaintext, group=g1, fontcolor=gray55]\nnn [label=\"Define architecture\", shape=plaintext, group=g1]\npretr [label=\"Pre-trained model\", shape=plaintext, group=g1, fontcolor=gray55]\nopt [label=\"Hyperparameters\", shape=plaintext, group=g1, fontcolor=gray55]\ntrain [label=\"Train\", shape=plaintext, group=g1, fontcolor=gray55]\ncp [label=\"Checkpoint\", shape=plaintext, group=g1, fontcolor=gray55]\n\npt [label=torchdata, fontcolor=gray55, color=gray55]\ntfds [label=tfds, group=g2, fontcolor=gray55, color=gray55]\ndt [label=datasets, fontcolor=gray55, color=gray55]\n\ngr [label=grain, fontcolor=gray55, color=gray55]\ntv [label=torchvision, fontcolor=gray55, color=gray55]\n\ntr [label=transformers, fontcolor=gray55, color=gray55]\n\nfl1 [label=flax, group=g2, fontcolor=\"#00695C\", color=\"#00695C\"]\nfl2 [label=flax, group=g2, fontcolor=gray55, color=gray55]\n\noa [label=optax, group=g2, fontcolor=gray55, color=gray55]\n\njx [label=jax, group=g2, fontcolor=gray55, color=gray55]\n\nob [label=orbax, group=g2, fontcolor=gray55, color=gray55]\n\n{rank=same; gr load tv tr}\ngr -> load -> tv -> tr [style=invis]\n\n{rank=same; fl1 proc pretr}\nfl1 -> proc -> pretr [style=invis]\n\n{rank=same; jx fl2 opt}\nfl1 -> proc -> pretr [style=invis]\n\n{pt tfds dt} -> load [color=gray55]\n{gr tv} -> proc [color=gray55]\nfl1 -> nn [color=\"#00695C\"]\npretr -> nn [dir=none]\ntr -> pretr [color=gray55]\noa -> opt [color=gray55]\njx -> fl2 -> train [color=gray55]\nob -> cp [color=gray55]\n\nload -> proc -> nn -> opt -> train -> cp [dir=none]\n\n}\n```\n\n## Load packages\n\nPackage and module necessary for this section:\n\n::: {#b498db0c .cell execution_count=1}\n``` {.python .cell-code}\n# to define the jax.Array type\nimport jax\n\n# general JAX array manipulations\nimport jax.numpy as jnp\n\n# to define the model architecture\nfrom flax import nnx\n\n# to get callables from functions with fewer arguments\nfrom functools import partial\n```\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-bright-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-bright-red-fg\">ModuleNotFoundError</span>                       Traceback (most recent call last)\n<span class=\"ansi-bright-cyan-fg\">Cell</span><span class=\"ansi-bright-cyan-fg\"> </span><span class=\"ansi-green-fg\">In[1]</span><span class=\"ansi-green-fg\">, line 2</span>\n<span class=\"ansi-bright-green-fg\">      1</span> <span style=\"color:rgb(138,138,138)\"># to define the jax.Array type</span>\n<span class=\"ansi-bright-green-fg\">----&gt; </span><span class=\"ansi-bright-green-fg\">2</span> <span style=\"color:rgb(255,95,135)\">import</span><span class=\"ansi-bright-white-fg\"> </span><span class=\"ansi-bright-white-fg\">jax</span>\n<span class=\"ansi-bright-green-fg\">      4</span> <span style=\"color:rgb(138,138,138)\"># general JAX array manipulations</span>\n<span class=\"ansi-bright-green-fg\">      5</span> <span style=\"color:rgb(255,95,135)\">import</span><span class=\"ansi-bright-white-fg\"> </span><span class=\"ansi-bright-white-fg\">jax</span><span class=\"ansi-bright-white-fg\">.</span><span class=\"ansi-bright-white-fg\">numpy</span><span class=\"ansi-bright-white-fg\"> </span><span style=\"color:rgb(95,215,255)\">as</span><span class=\"ansi-bright-white-fg\"> </span><span class=\"ansi-bright-white-fg\">jnp</span>\n\n<span class=\"ansi-bright-red-fg\">ModuleNotFoundError</span>: No module named 'jax'</pre>\n```\n:::\n\n:::\n:::\n\n\n## The Flax NNX API\n\nFlax went through several APIs.\n\nThe initial `nn` API—now retired—got replaced in 2020 by the [Linen API](https://flax-linen.readthedocs.io/en/latest/), still available with the Flax package. In 2024, [they launched the NNX API](https://flax.readthedocs.io/en/latest/why.html).\n\nEach iteration has moved further from JAX and closer to Python, with a syntax increasingly similar to PyTorch.\n\nWhile the Linen API still exists, new users are advised to learn the new NNX API.\n\n### Stateful models\n\nThe old Linen API is a stateless model framework similar to the Julia package [Lux.jl](https://github.com/LuxDL/Lux.jl). It follows a strict functional programming approach in which the parameters are separate from the model and are passed as inputs to the forward pass along with the data. This is much closer to the JAX sublanguage, more optimized, but restrictive and unpopular in the deep learning community and among Python users.\n\nBy contrast, the new NNX API is a stateful model framework similar to [PyTorch](https://github.com/pytorch/pytorch) and the older Julia package [Flux.jl](https://github.com/FluxML/Flux.jl): model parameters and optimizer state are stored within the model instance. Flax handles a lot of JAX's constraints under the hood, making the code more familiar to Python/PyTorch users, simpler, and more forgiving.\n\nThe dynamic state handled by NNX is stored in `nnx.Params` and the static state (all types not handled by NNX) are stored directly as Python object attributes. This follows the classic Python [object-oriented](https://en.wikipedia.org/wiki/Object-oriented_programming) paradigm.\n\n### No shape inference\n\nAll model dimensions need to be explicitly stated.\n\n### Handling of PRNG\n\n[We saw that JAX has a complex way to handle pseudo-random number generation](fl_numpy#pseudorandom-number-generation). While the Linen API required PRNG to be done explicitly in JAX by the user, the new NNX API defines the random state as an object state stored in a variable and carried by the model.\n\n### What this looks like\n\nDefine the model architecture:\n\n::: {#528c8e90 .cell execution_count=2}\n``` {.python .cell-code}\nclass Linear(nnx.Module):\n  def __init__(self, din: int, dout: int, *, rngs: nnx.Rngs):\n    key = rngs.params()\n    self.w = nnx.Param(jax.random.uniform(key, (din, dout)))\n    self.b = nnx.Param(jnp.zeros((dout,)))\n    self.din, self.dout = din, dout\n\n  def __call__(self, x: jax.Array):\n    return x @ self.w + self.b\n```\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-bright-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-bright-red-fg\">NameError</span>                                 Traceback (most recent call last)\n<span class=\"ansi-bright-cyan-fg\">Cell</span><span class=\"ansi-bright-cyan-fg\"> </span><span class=\"ansi-green-fg\">In[2]</span><span class=\"ansi-green-fg\">, line 1</span>\n<span class=\"ansi-bright-green-fg\">----&gt; </span><span class=\"ansi-bright-green-fg\">1</span> <span style=\"color:rgb(95,215,255)\">class</span><span class=\"ansi-bright-white-fg\"> </span><span style=\"color:rgb(175,215,0)\">Linear</span><span class=\"ansi-bright-white-fg\">(</span><span class=\"ansi-bright-white-fg ansi-yellow-bg\">nnx</span><span style=\"color:rgb(255,95,135)\">.</span><span class=\"ansi-bright-white-fg\">Module</span><span class=\"ansi-bright-white-fg\">)</span><span class=\"ansi-bright-white-fg\">:</span>\n<span class=\"ansi-bright-green-fg\">      2</span> <span class=\"ansi-bright-white-fg\">  </span><span style=\"color:rgb(95,215,255)\">def</span><span class=\"ansi-bright-white-fg\"> </span><span style=\"color:rgb(175,215,0)\">__init__</span><span class=\"ansi-bright-white-fg\">(</span><span class=\"ansi-bright-white-fg\">self</span><span class=\"ansi-bright-white-fg\">,</span><span class=\"ansi-bright-white-fg\"> </span><span class=\"ansi-bright-white-fg\">din</span><span class=\"ansi-bright-white-fg\">:</span><span class=\"ansi-bright-white-fg\"> </span><span class=\"ansi-bright-white-fg\">int</span><span class=\"ansi-bright-white-fg\">,</span><span class=\"ansi-bright-white-fg\"> </span><span class=\"ansi-bright-white-fg\">dout</span><span class=\"ansi-bright-white-fg\">:</span><span class=\"ansi-bright-white-fg\"> </span><span class=\"ansi-bright-white-fg\">int</span><span class=\"ansi-bright-white-fg\">,</span><span class=\"ansi-bright-white-fg\"> </span><span style=\"color:rgb(255,95,135)\">*</span><span class=\"ansi-bright-white-fg\">,</span><span class=\"ansi-bright-white-fg\"> </span><span class=\"ansi-bright-white-fg\">rngs</span><span class=\"ansi-bright-white-fg\">:</span><span class=\"ansi-bright-white-fg\"> </span><span class=\"ansi-bright-white-fg\">nnx</span><span style=\"color:rgb(255,95,135)\">.</span><span class=\"ansi-bright-white-fg\">Rngs</span><span class=\"ansi-bright-white-fg\">)</span><span class=\"ansi-bright-white-fg\">:</span>\n<span class=\"ansi-bright-green-fg\">      3</span> <span class=\"ansi-bright-white-fg\">    </span><span class=\"ansi-bright-white-fg\">key</span><span class=\"ansi-bright-white-fg\"> </span><span style=\"color:rgb(255,95,135)\">=</span><span class=\"ansi-bright-white-fg\"> </span><span class=\"ansi-bright-white-fg\">rngs</span><span style=\"color:rgb(255,95,135)\">.</span><span class=\"ansi-bright-white-fg\">params</span><span class=\"ansi-bright-white-fg\">(</span><span class=\"ansi-bright-white-fg\">)</span>\n\n<span class=\"ansi-bright-red-fg\">NameError</span>: name 'nnx' is not defined</pre>\n```\n:::\n\n:::\n:::\n\n\nInstantiate the model:\n\n::: {#729e9ac0 .cell execution_count=3}\n``` {.python .cell-code}\nmodel = Linear(2, 5, rngs=nnx.Rngs(params=0))\n```\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-bright-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-bright-red-fg\">NameError</span>                                 Traceback (most recent call last)\n<span class=\"ansi-bright-cyan-fg\">Cell</span><span class=\"ansi-bright-cyan-fg\"> </span><span class=\"ansi-green-fg\">In[3]</span><span class=\"ansi-green-fg\">, line 1</span>\n<span class=\"ansi-bright-green-fg\">----&gt; </span><span class=\"ansi-bright-green-fg\">1</span> <span class=\"ansi-bright-white-fg\">model</span><span class=\"ansi-bright-white-fg\"> </span><span style=\"color:rgb(255,95,135)\">=</span><span class=\"ansi-bright-white-fg\"> </span><span class=\"ansi-bright-white-fg ansi-yellow-bg\">Linear</span><span class=\"ansi-bright-white-fg\">(</span><span style=\"color:rgb(175,135,255)\">2</span><span class=\"ansi-bright-white-fg\">,</span><span class=\"ansi-bright-white-fg\"> </span><span style=\"color:rgb(175,135,255)\">5</span><span class=\"ansi-bright-white-fg\">,</span><span class=\"ansi-bright-white-fg\"> </span><span class=\"ansi-bright-white-fg\">rngs</span><span style=\"color:rgb(255,95,135)\">=</span><span class=\"ansi-bright-white-fg\">nnx</span><span style=\"color:rgb(255,95,135)\">.</span><span class=\"ansi-bright-white-fg\">Rngs</span><span class=\"ansi-bright-white-fg\">(</span><span class=\"ansi-bright-white-fg\">params</span><span style=\"color:rgb(255,95,135)\">=</span><span style=\"color:rgb(175,135,255)\">0</span><span class=\"ansi-bright-white-fg\">)</span><span class=\"ansi-bright-white-fg\">)</span>\n\n<span class=\"ansi-bright-red-fg\">NameError</span>: name 'Linear' is not defined</pre>\n```\n:::\n\n:::\n:::\n\n\nDisplay the model structure:\n\n::: {#cf2a8a4b .cell execution_count=4}\n``` {.python .cell-code}\nnnx.display(model)\n```\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-bright-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-bright-red-fg\">NameError</span>                                 Traceback (most recent call last)\n<span class=\"ansi-bright-cyan-fg\">Cell</span><span class=\"ansi-bright-cyan-fg\"> </span><span class=\"ansi-green-fg\">In[4]</span><span class=\"ansi-green-fg\">, line 1</span>\n<span class=\"ansi-bright-green-fg\">----&gt; </span><span class=\"ansi-bright-green-fg\">1</span> <span class=\"ansi-bright-white-fg ansi-yellow-bg\">nnx</span><span style=\"color:rgb(255,95,135)\">.</span><span class=\"ansi-bright-white-fg\">display</span><span class=\"ansi-bright-white-fg\">(</span><span class=\"ansi-bright-white-fg\">model</span><span class=\"ansi-bright-white-fg\">)</span>\n\n<span class=\"ansi-bright-red-fg\">NameError</span>: name 'nnx' is not defined</pre>\n```\n:::\n\n:::\n:::\n\n\n:::{.note}\n\nIf you have the [penzai package](https://github.com/google-deepmind/penzai) installed, you will see an interactive display of the model.\n\n:::\n\n::: {#4b7a8d3d .cell execution_count=5}\n``` {.python .cell-code}\ny = model(x=jnp.ones((1, 2)))\nprint(\"Predictions shape: \", y.shape)\n```\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-bright-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-bright-red-fg\">NameError</span>                                 Traceback (most recent call last)\n<span class=\"ansi-bright-cyan-fg\">Cell</span><span class=\"ansi-bright-cyan-fg\"> </span><span class=\"ansi-green-fg\">In[5]</span><span class=\"ansi-green-fg\">, line 1</span>\n<span class=\"ansi-bright-green-fg\">----&gt; </span><span class=\"ansi-bright-green-fg\">1</span> <span class=\"ansi-bright-white-fg\">y</span><span class=\"ansi-bright-white-fg\"> </span><span style=\"color:rgb(255,95,135)\">=</span><span class=\"ansi-bright-white-fg\"> </span><span class=\"ansi-bright-white-fg ansi-yellow-bg\">model</span><span class=\"ansi-bright-white-fg\">(</span><span class=\"ansi-bright-white-fg\">x</span><span style=\"color:rgb(255,95,135)\">=</span><span class=\"ansi-bright-white-fg\">jnp</span><span style=\"color:rgb(255,95,135)\">.</span><span class=\"ansi-bright-white-fg\">ones</span><span class=\"ansi-bright-white-fg\">(</span><span class=\"ansi-bright-white-fg\">(</span><span style=\"color:rgb(175,135,255)\">1</span><span class=\"ansi-bright-white-fg\">,</span><span class=\"ansi-bright-white-fg\"> </span><span style=\"color:rgb(175,135,255)\">2</span><span class=\"ansi-bright-white-fg\">)</span><span class=\"ansi-bright-white-fg\">)</span><span class=\"ansi-bright-white-fg\">)</span>\n<span class=\"ansi-bright-green-fg\">      2</span> <span class=\"ansi-bright-white-fg\">print</span><span class=\"ansi-bright-white-fg\">(</span><span style=\"color:rgb(215,215,135)\">\"</span><span style=\"color:rgb(215,215,135)\">Predictions shape: </span><span style=\"color:rgb(215,215,135)\">\"</span><span class=\"ansi-bright-white-fg\">,</span><span class=\"ansi-bright-white-fg\"> </span><span class=\"ansi-bright-white-fg\">y</span><span style=\"color:rgb(255,95,135)\">.</span><span class=\"ansi-bright-white-fg\">shape</span><span class=\"ansi-bright-white-fg\">)</span>\n\n<span class=\"ansi-bright-red-fg\">NameError</span>: name 'model' is not defined</pre>\n```\n:::\n\n:::\n:::\n\n\n## Example MLP with Flax NNX\n\n[Multilayer perceptrons (MLPs)](https://en.wikipedia.org/wiki/Multilayer_perceptron) are fully-connected feed-forward neural networks.\n\nHere is an example of MLP with a single hidden layer for the [MNIST dataset](https://en.wikipedia.org/wiki/MNIST_database) by LeCun et al. [@lecun2010mnist]:\n\n![[Image source](https://www.researchgate.net/figure/A-one-hidden-layer-MLP-network-applied-to-MNIST-dataset_fig2_350543967)](img/mlp.jpg)\n\nAnd here is the implementation in Flax NNX:\n\n::: {#583ceb45 .cell execution_count=6}\n``` {.python .cell-code}\nclass MLP(nnx.Module):\n\n  def __init__(\n          self,\n          # 28x28 pixel images with 1 channel\n          n_features: int = 784,\n          n_hidden: int = 300,\n          # 10 digits\n          n_targets: int = 10,\n          *,\n          rngs: nnx.Rngs\n  ):\n    self.n_features = n_features\n    self.layer1 = nnx.Linear(n_features, n_hidden, rngs=rngs)\n    self.layer2 = nnx.Linear(n_hidden, n_hidden, rngs=rngs)\n    self.layer3 = nnx.Linear(n_hidden, n_targets, rngs=rngs)\n\n  def __call__(self, x):\n    x = x.reshape(x.shape[0], self.n_features) # flatten\n    x = nnx.selu(self.layer1(x))\n    x = nnx.selu(self.layer2(x))\n    x = self.layer3(x)\n    return x\n\n# instantiate the model\nmodel = MLP(rngs=nnx.Rngs(0))\n\n# visualize it\nnnx.display(model)\n```\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-bright-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-bright-red-fg\">NameError</span>                                 Traceback (most recent call last)\n<span class=\"ansi-bright-cyan-fg\">Cell</span><span class=\"ansi-bright-cyan-fg\"> </span><span class=\"ansi-green-fg\">In[6]</span><span class=\"ansi-green-fg\">, line 1</span>\n<span class=\"ansi-bright-green-fg\">----&gt; </span><span class=\"ansi-bright-green-fg\">1</span> <span style=\"color:rgb(95,215,255)\">class</span><span class=\"ansi-bright-white-fg\"> </span><span style=\"color:rgb(175,215,0)\">MLP</span><span class=\"ansi-bright-white-fg\">(</span><span class=\"ansi-bright-white-fg ansi-yellow-bg\">nnx</span><span style=\"color:rgb(255,95,135)\">.</span><span class=\"ansi-bright-white-fg\">Module</span><span class=\"ansi-bright-white-fg\">)</span><span class=\"ansi-bright-white-fg\">:</span>\n<span class=\"ansi-bright-green-fg\">      3</span> <span class=\"ansi-bright-white-fg\">  </span><span style=\"color:rgb(95,215,255)\">def</span><span class=\"ansi-bright-white-fg\"> </span><span style=\"color:rgb(175,215,0)\">__init__</span><span class=\"ansi-bright-white-fg\">(</span>\n<span class=\"ansi-bright-green-fg\">      4</span> <span class=\"ansi-bright-white-fg\">          </span><span class=\"ansi-bright-white-fg\">self</span><span class=\"ansi-bright-white-fg\">,</span>\n<span class=\"ansi-bright-green-fg\">      5</span> <span class=\"ansi-bright-white-fg\">          </span><span style=\"color:rgb(138,138,138)\"># 28x28 pixel images with 1 channel</span>\n<span class=\"ansi-bright-green-fg\">   (...)</span><span class=\"ansi-bright-green-fg\">     11</span> <span class=\"ansi-bright-white-fg\">          </span><span class=\"ansi-bright-white-fg\">rngs</span><span class=\"ansi-bright-white-fg\">:</span><span class=\"ansi-bright-white-fg\"> </span><span class=\"ansi-bright-white-fg\">nnx</span><span style=\"color:rgb(255,95,135)\">.</span><span class=\"ansi-bright-white-fg\">Rngs</span>\n<span class=\"ansi-bright-green-fg\">     12</span> <span class=\"ansi-bright-white-fg\">  </span><span class=\"ansi-bright-white-fg\">)</span><span class=\"ansi-bright-white-fg\">:</span>\n<span class=\"ansi-bright-green-fg\">     13</span> <span class=\"ansi-bright-white-fg\">    </span><span class=\"ansi-bright-white-fg\">self</span><span style=\"color:rgb(255,95,135)\">.</span><span class=\"ansi-bright-white-fg\">n_features</span><span class=\"ansi-bright-white-fg\"> </span><span style=\"color:rgb(255,95,135)\">=</span><span class=\"ansi-bright-white-fg\"> </span><span class=\"ansi-bright-white-fg\">n_features</span>\n\n<span class=\"ansi-bright-red-fg\">NameError</span>: name 'nnx' is not defined</pre>\n```\n:::\n\n:::\n:::\n\n\n:::{.notenoit}\n\nNNX API references:\n\n- [flax.nnx.Linear](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/linear.html#flax.nnx.Linear) layer class\n- [flax.nnx.selu](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/activations.html#flax.nnx.selu) SELU activation function\n\n:::\n\n## Example CNN with Flax NNX\n\n[Convolutional neural networks (CNNs)](https://en.wikipedia.org/wiki/Convolutional_neural_network) take advantage of the spacial correlations that exist in images and allow to greatly reduce the number of neurons in vision networks.\n\n[LeNet](https://en.wikipedia.org/wiki/LeNet)-5 [@lecun1998gradient] model, initially used on the [MNIST dataset](https://en.wikipedia.org/wiki/MNIST_database) by LeCun et al. [@lecun2010mnist], is an early and simple CNN. The architecture of this model is explained in details in [this kaggle post](https://www.kaggle.com/code/blurredmachine/lenet-architecture-a-complete-guide) and here is a schematic:\n\n![Image source: LeCun et al. [@lecun1998gradient]](img/lenet-5.png)\n\nYou can find the [keras](https://github.com/keras-team/keras) code [here](https://www.kaggle.com/code/blurredmachine/lenet-architecture-a-complete-guide?scriptVersionId=38673209&cellId=8) and the [PyTorch](https://github.com/pytorch/pytorch) code (slightly modified) [here](https://pytorch.org/tutorials/intermediate/tensorboard_tutorial.html) for comparison.\n\n::: {#8c644a14 .cell execution_count=7}\n``` {.python .cell-code}\nclass LeNet(nnx.Module):\n  \"\"\"An adapted LeNet-5 model.\"\"\"\n\n  def __init__(self, *, rngs: nnx.Rngs):\n    self.conv1 = nnx.Conv(1, 6, kernel_size=(5, 5), rngs=rngs)\n    self.max_pool = partial(nnx.max_pool, window_shape=(2, 2), strides=(2, 2))\n    self.conv2 = nnx.Conv(6, 16, kernel_size=(5, 5), rngs=rngs)\n    self.linear1 = nnx.Linear(16 * 4 * 4, 120, rngs=rngs)\n    self.linear2 = nnx.Linear(120, 84, rngs=rngs)\n    self.linear3 = nnx.Linear(84, 10, rngs=rngs)\n\n  def __call__(self, x):\n    x = self.max_pool(nnx.relu(self.conv1(x)))\n    x = self.max_pool(nnx.relu(self.conv2(x)))\n    x = x.reshape(x.shape[0], -1)  # flatten\n    x = nnx.relu(self.linear1(x))\n    x = nnx.relu(self.linear2(x))\n    x = self.linear3(x)\n    return x\n\n# instantiate the model\nmodel = LeNet(rngs=nnx.Rngs(0))\n\n# visualize it\nnnx.display(model)\n```\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-bright-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-bright-red-fg\">NameError</span>                                 Traceback (most recent call last)\n<span class=\"ansi-bright-cyan-fg\">Cell</span><span class=\"ansi-bright-cyan-fg\"> </span><span class=\"ansi-green-fg\">In[7]</span><span class=\"ansi-green-fg\">, line 1</span>\n<span class=\"ansi-bright-green-fg\">----&gt; </span><span class=\"ansi-bright-green-fg\">1</span> <span style=\"color:rgb(95,215,255)\">class</span><span class=\"ansi-bright-white-fg\"> </span><span style=\"color:rgb(175,215,0)\">LeNet</span><span class=\"ansi-bright-white-fg\">(</span><span class=\"ansi-bright-white-fg ansi-yellow-bg\">nnx</span><span style=\"color:rgb(255,95,135)\">.</span><span class=\"ansi-bright-white-fg\">Module</span><span class=\"ansi-bright-white-fg\">)</span><span class=\"ansi-bright-white-fg\">:</span>\n<span class=\"ansi-bright-green-fg\">      2</span> <span class=\"ansi-bright-white-fg\">  </span><span style=\"color:rgb(215,215,135)\">\"\"\"An adapted LeNet-5 model.\"\"\"</span>\n<span class=\"ansi-bright-green-fg\">      4</span> <span class=\"ansi-bright-white-fg\">  </span><span style=\"color:rgb(95,215,255)\">def</span><span class=\"ansi-bright-white-fg\"> </span><span style=\"color:rgb(175,215,0)\">__init__</span><span class=\"ansi-bright-white-fg\">(</span><span class=\"ansi-bright-white-fg\">self</span><span class=\"ansi-bright-white-fg\">,</span><span class=\"ansi-bright-white-fg\"> </span><span style=\"color:rgb(255,95,135)\">*</span><span class=\"ansi-bright-white-fg\">,</span><span class=\"ansi-bright-white-fg\"> </span><span class=\"ansi-bright-white-fg\">rngs</span><span class=\"ansi-bright-white-fg\">:</span><span class=\"ansi-bright-white-fg\"> </span><span class=\"ansi-bright-white-fg\">nnx</span><span style=\"color:rgb(255,95,135)\">.</span><span class=\"ansi-bright-white-fg\">Rngs</span><span class=\"ansi-bright-white-fg\">)</span><span class=\"ansi-bright-white-fg\">:</span>\n\n<span class=\"ansi-bright-red-fg\">NameError</span>: name 'nnx' is not defined</pre>\n```\n:::\n\n:::\n:::\n\n\n:::{.notenoit}\n\nNNX API references:\n\n- [flax.nnx.Conv](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/linear.html#flax.nnx.Conv) convolution module\n- [flax.nnx.Linear](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/linear.html#flax.nnx.Linear) layer class\n- `flax.nnx.max_pool` is [missing in the API documentation](https://github.com/google/flax/issues/4271) as of April 2025\n- [flax.nnx.relu](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/activations.html#flax.nnx.relu) activation function\n\n:::\n\n## ViT with Flax NNX\n\nLeNet (various iterations until 1998) was followed by [AlexNet](https://en.wikipedia.org/wiki/AlexNet) in 2011 and many increasingly complex CNNs, until multi-head [attention](https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)) and transformers changed everything.\n\n[Transformers](https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)) are a complex neural network architecture developed by Google in 2017, after the seminal paper \"Attention Is All You Need\" [@vaswani2017attention]—cited 175,083 times as of April 2025 (!!!)—came out. They were initially only used in [natural language processing (NLP)](https://en.wikipedia.org/wiki/Natural_language_processing), but have since been applied to vision.\n\nTo classify our food dataset, we will use the [vision transformer (ViT)](https://en.wikipedia.org/wiki/Vision_transformer) introduced by Dosovitskiy et al. [-@dosovitskiy2021imageworth16x16words] (that we will fine-tune in a later section).\n\nHere is a schematic of the model:\n\n![Image source: Dosovitskiy et al. [@dosovitskiy2021imageworth16x16words]](img/vit.png)\n\nAnd here is the [JAX implementation](https://github.com/google-research/vision_transformer/) by [Google Research](https://github.com/google-research):\n\n::: {#50c6338d .cell execution_count=8}\n``` {.python .cell-code}\nclass VisionTransformer(nnx.Module):\n    def __init__(\n        self,\n        num_classes: int = 1000,\n        in_channels: int = 3,\n        img_size: int = 224,\n        patch_size: int = 16,\n        num_layers: int = 12,\n        num_heads: int = 12,\n        mlp_dim: int = 3072,\n        hidden_size: int = 768,\n        dropout_rate: float = 0.1,\n        *,\n        rngs: nnx.Rngs = nnx.Rngs(0),\n    ):\n        # Patch and position embedding\n        n_patches = (img_size // patch_size) ** 2\n        self.patch_embeddings = nnx.Conv(\n            in_channels,\n            hidden_size,\n            kernel_size=(patch_size, patch_size),\n            strides=(patch_size, patch_size),\n            padding=\"VALID\",\n            use_bias=True,\n            rngs=rngs,\n        )\n\n        initializer = jax.nn.initializers.truncated_normal(stddev=0.02)\n        self.position_embeddings = nnx.Param(\n            initializer(\n                rngs.params(),\n                (1, n_patches + 1, hidden_size),\n                jnp.float32\n            )\n        )\n        self.dropout = nnx.Dropout(dropout_rate, rngs=rngs)\n\n        self.cls_token = nnx.Param(jnp.zeros((1, 1, hidden_size)))\n\n        # Transformer Encoder blocks\n        self.encoder = nnx.Sequential(*[\n            TransformerEncoder(\n                hidden_size,\n                mlp_dim,\n                num_heads,\n                dropout_rate,\n                rngs=rngs\n            )\n            for i in range(num_layers)\n        ])\n        self.final_norm = nnx.LayerNorm(hidden_size, rngs=rngs)\n\n        # Classification head\n        self.classifier = nnx.Linear(hidden_size, num_classes, rngs=rngs)\n\n    def __call__(self, x: jax.Array) -> jax.Array:\n        # Patch and position embedding\n        patches = self.patch_embeddings(x)\n        batch_size = patches.shape[0]\n        patches = patches.reshape(batch_size, -1, patches.shape[-1])\n\n        cls_token = jnp.tile(self.cls_token, [batch_size, 1, 1])\n        x = jnp.concat([cls_token, patches], axis=1)\n        embeddings = x + self.position_embeddings\n        embeddings = self.dropout(embeddings)\n\n        # Encoder blocks\n        x = self.encoder(embeddings)\n        x = self.final_norm(x)\n\n        # fetch the first token\n        x = x[:, 0]\n\n        # Classification\n        return self.classifier(x)\n\nclass TransformerEncoder(nnx.Module):\n    def __init__(\n        self,\n        hidden_size: int,\n        mlp_dim: int,\n        num_heads: int,\n        dropout_rate: float = 0.0,\n        *,\n        rngs: nnx.Rngs = nnx.Rngs(0),\n    ) -> None:\n\n        self.norm1 = nnx.LayerNorm(hidden_size, rngs=rngs)\n        self.attn = nnx.MultiHeadAttention(\n            num_heads=num_heads,\n            in_features=hidden_size,\n            dropout_rate=dropout_rate,\n            broadcast_dropout=False,\n            decode=False,\n            deterministic=False,\n            rngs=rngs,\n        )\n        self.norm2 = nnx.LayerNorm(hidden_size, rngs=rngs)\n\n        self.mlp = nnx.Sequential(\n            nnx.Linear(hidden_size, mlp_dim, rngs=rngs),\n            nnx.gelu,\n            nnx.Dropout(dropout_rate, rngs=rngs),\n            nnx.Linear(mlp_dim, hidden_size, rngs=rngs),\n            nnx.Dropout(dropout_rate, rngs=rngs),\n        )\n\n    def __call__(self, x: jax.Array) -> jax.Array:\n        x = x + self.attn(self.norm1(x))\n        x = x + self.mlp(self.norm2(x))\n        return x\n\nx = jnp.ones((4, 224, 224, 3))\nmodel = VisionTransformer(num_classes=1000)\ny = model(x)\nprint(\"Predictions shape: \", y.shape)\n```\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-bright-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-bright-red-fg\">NameError</span>                                 Traceback (most recent call last)\n<span class=\"ansi-bright-cyan-fg\">Cell</span><span class=\"ansi-bright-cyan-fg\"> </span><span class=\"ansi-green-fg\">In[8]</span><span class=\"ansi-green-fg\">, line 1</span>\n<span class=\"ansi-bright-green-fg\">----&gt; </span><span class=\"ansi-bright-green-fg\">1</span> <span style=\"color:rgb(95,215,255)\">class</span><span class=\"ansi-bright-white-fg\"> </span><span style=\"color:rgb(175,215,0)\">VisionTransformer</span><span class=\"ansi-bright-white-fg\">(</span><span class=\"ansi-bright-white-fg ansi-yellow-bg\">nnx</span><span style=\"color:rgb(255,95,135)\">.</span><span class=\"ansi-bright-white-fg\">Module</span><span class=\"ansi-bright-white-fg\">)</span><span class=\"ansi-bright-white-fg\">:</span>\n<span class=\"ansi-bright-green-fg\">      2</span> <span class=\"ansi-bright-white-fg\">    </span><span style=\"color:rgb(95,215,255)\">def</span><span class=\"ansi-bright-white-fg\"> </span><span style=\"color:rgb(175,215,0)\">__init__</span><span class=\"ansi-bright-white-fg\">(</span>\n<span class=\"ansi-bright-green-fg\">      3</span> <span class=\"ansi-bright-white-fg\">        </span><span class=\"ansi-bright-white-fg\">self</span><span class=\"ansi-bright-white-fg\">,</span>\n<span class=\"ansi-bright-green-fg\">      4</span> <span class=\"ansi-bright-white-fg\">        </span><span class=\"ansi-bright-white-fg\">num_classes</span><span class=\"ansi-bright-white-fg\">:</span><span class=\"ansi-bright-white-fg\"> </span><span class=\"ansi-bright-white-fg\">int</span><span class=\"ansi-bright-white-fg\"> </span><span style=\"color:rgb(255,95,135)\">=</span><span class=\"ansi-bright-white-fg\"> </span><span style=\"color:rgb(175,135,255)\">1000</span><span class=\"ansi-bright-white-fg\">,</span>\n<span class=\"ansi-bright-green-fg\">   (...)</span><span class=\"ansi-bright-green-fg\">     15</span> <span class=\"ansi-bright-white-fg\">    </span><span class=\"ansi-bright-white-fg\">)</span><span class=\"ansi-bright-white-fg\">:</span>\n<span class=\"ansi-bright-green-fg\">     16</span> <span class=\"ansi-bright-white-fg\">        </span><span style=\"color:rgb(138,138,138)\"># Patch and position embedding</span>\n<span class=\"ansi-bright-green-fg\">     17</span> <span class=\"ansi-bright-white-fg\">        </span><span class=\"ansi-bright-white-fg\">n_patches</span><span class=\"ansi-bright-white-fg\"> </span><span style=\"color:rgb(255,95,135)\">=</span><span class=\"ansi-bright-white-fg\"> </span><span class=\"ansi-bright-white-fg\">(</span><span class=\"ansi-bright-white-fg\">img_size</span><span class=\"ansi-bright-white-fg\"> </span><span style=\"color:rgb(255,95,135)\">/</span><span style=\"color:rgb(255,95,135)\">/</span><span class=\"ansi-bright-white-fg\"> </span><span class=\"ansi-bright-white-fg\">patch_size</span><span class=\"ansi-bright-white-fg\">)</span><span class=\"ansi-bright-white-fg\"> </span><span style=\"color:rgb(255,95,135)\">*</span><span style=\"color:rgb(255,95,135)\">*</span><span class=\"ansi-bright-white-fg\"> </span><span style=\"color:rgb(175,135,255)\">2</span>\n\n<span class=\"ansi-bright-red-fg\">NameError</span>: name 'nnx' is not defined</pre>\n```\n:::\n\n:::\n:::\n\n\n:::{.notenoit}\n\nNNX API references:\n\n- [flax.nnx.Conv](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/linear.html#flax.nnx.Conv) convolution module\n- [flax.nnx.Dropout](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/stochastic.html#flax.nnx.Dropout) dropout class\n- [flax.nnx.LayerNorm](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/normalization.html#flax.nnx.LayerNorm) layer normalization class\n- [flax.nnx.Linear](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/linear.html#flax.nnx.Linear) linear layer class\n- [flax.nnx.MultiHeadAttention](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/attention.html#flax.nnx.MultiHeadAttention) multi-head attention class\n- [flax.nnx.Param](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/variables.html#flax.nnx.Param) parameter class\n- [flax.nnx.Sequential](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/helpers.html#flax.nnx.Sequential) helper class\n- [flax.nnx.gelu](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/activations.html#flax.nnx.gelu) GELU activation function\n\n:::\n\n",
    "supporting": [
      "fl_nn_files"
    ],
    "filters": [],
    "includes": {}
  }
}