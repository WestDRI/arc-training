{
  "hash": "6a64772fefcfc0eec785993e480eec17",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Relation to NumPy\nauthor: Marie-Hélène Burle\n---\n\n:::{.def}\n\n[NumPy](https://numpy.org/) is a popular Python scientific API at the core of many libraries. JAX uses a NumPy-inspired API. There are however important differences that we will explore in this section.\n\n:::\n\n## Using IPython for interactive sessions\n\nNowadays, IPython (Interactive Python) is known as the kernel used by Jupyter when running Python. Before the existence of Jupyter however, this kernel was created as a better command shell than the default Python shell. For interactive Python sessions in the command line, it is nicer and faster than plain Python with no downside. So we will use it for this course.\n\nFor this, we need to load the `ipython-kernel` module. To see what versions are available, you can run:\n\n```{.bash}\nmodule spider ipython-kernel\n```\n\nLet's load the latest module:\n\n```{.bash}\nmodule load ipython-kernel/3.11\n```\n\nNow we can launch IPython:\n\n```{.bash}\nipython\n```\n\n## A NumPy-inspired API\n\nNumPy being so popular, JAX comes with a convenient high-level wrapper to NumPy: `jax.numpy`.\n\n:::{.note}\n\nBeing familiar with NumPy is thus an advantage to get started with JAX. [The NumPy quickstart](https://numpy.org/doc/stable/user/quickstart.html) is a useful resource.\n\n:::\n\n:::{.note}\n\nFor a more efficient usage, JAX also comes with a lower-level API: `jax.lax`.\n\n:::\n\n:::{.panel-tabset}\n\n### NumPy\n\n::: {#e11d3150 .cell execution_count=2}\n``` {.python .cell-code}\nimport numpy as np\n```\n:::\n\n\n::: {#1a0e64a8 .cell execution_count=3}\n``` {.python .cell-code}\nprint(np.array([(1, 2, 3), (4, 5, 6)]))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[1 2 3]\n [4 5 6]]\n```\n:::\n:::\n\n\n::: {#b6eac149 .cell execution_count=4}\n``` {.python .cell-code}\nprint(np.zeros((2, 3)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[0. 0. 0.]\n [0. 0. 0.]]\n```\n:::\n:::\n\n\n::: {#8be5589a .cell execution_count=5}\n``` {.python .cell-code}\nprint(np.ones((2, 3, 2)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[[1. 1.]\n  [1. 1.]\n  [1. 1.]]\n\n [[1. 1.]\n  [1. 1.]\n  [1. 1.]]]\n```\n:::\n:::\n\n\n::: {#e81995ea .cell execution_count=6}\n``` {.python .cell-code}\nprint(np.arange(24).reshape(2, 3, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[[ 0  1  2  3]\n  [ 4  5  6  7]\n  [ 8  9 10 11]]\n\n [[12 13 14 15]\n  [16 17 18 19]\n  [20 21 22 23]]]\n```\n:::\n:::\n\n\n::: {#5caf73f7 .cell execution_count=7}\n``` {.python .cell-code}\nprint(np.linspace(0, 2, 9))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[0.   0.25 0.5  0.75 1.   1.25 1.5  1.75 2.  ]\n```\n:::\n:::\n\n\n::: {#9a9dbb2c .cell execution_count=8}\n``` {.python .cell-code}\nprint(np.linspace(0, 2, 9)[::-1])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[2.   1.75 1.5  1.25 1.   0.75 0.5  0.25 0.  ]\n```\n:::\n:::\n\n\n### JAX NumPy\n\n```{.python}\nimport jax.numpy as jnp\n```\n\n```{.python}\nprint(jnp.array([(1, 2, 3), (4, 5, 6)]))\n```\n\n```\n[[1 2 3]\n [4 5 6]]\n```\n\n```{.python}\nprint(jnp.zeros((2, 3)))\n```\n\n```\n[[0. 0. 0.]\n [0. 0. 0.]]\n```\n\n```{.python}\nprint(jnp.ones((2, 3, 2)))\n```\n\n```\n[[[1. 1.]\n  [1. 1.]\n  [1. 1.]]\n\n [[1. 1.]\n  [1. 1.]\n  [1. 1.]]]\n```\n\n```{.python}\nprint(jnp.arange(24).reshape(2, 3, 4))\n```\n\n```\n[[[ 0  1  2  3]\n  [ 4  5  6  7]\n  [ 8  9 10 11]]\n\n [[12 13 14 15]\n  [16 17 18 19]\n  [20 21 22 23]]]\n```\n\n```{.python}\nprint(jnp.linspace(0, 2, 9))\n```\n\n```\n[0.   0.25 0.5  0.75 1.   1.25 1.5  1.75 2.  ]\n```\n\n```{.python}\nprint(jnp.linspace(0, 2, 9)[::-1])\n```\n\n```\n[2.   1.75 1.5  1.25 1.   0.75 0.5  0.25 0.  ]\n```\n\n:::\n\nDespite the similarities, there are important differences between JAX and NumPy.\n\n## Differences with NumPy\n\n### Different types\n\n```{.python}\ntype(np.zeros((2, 3))) == type(jnp.zeros((2, 3)))\n```\n\n```\nFalse\n```\n\n:::{.panel-tabset}\n\n#### Numpy\n\n::: {#b8fc98c2 .cell execution_count=9}\n``` {.python .cell-code}\ntype(np.zeros((2, 3)))\n```\n\n::: {.cell-output .cell-output-display execution_count=39}\n```\nnumpy.ndarray\n```\n:::\n:::\n\n\n#### JAX NumPy\n\n```{.python}\ntype(jnp.zeros((2, 3)))\n```\n\n```\njaxlib.xla_extension.ArrayImpl\n```\n\n:::\n\n### Different default data types\n\n:::{.panel-tabset}\n\n#### Numpy\n\n::: {#05fcf78e .cell execution_count=10}\n``` {.python .cell-code}\nnp.zeros((2, 3)).dtype\n```\n\n::: {.cell-output .cell-output-display execution_count=40}\n```\ndtype('float64')\n```\n:::\n:::\n\n\n#### JAX NumPy\n\n```{.python}\njnp.zeros((2, 3)).dtype\n```\n\n```\ndtype('float32')\n```\n\n:::\n\n:::{.note}\n\nLower numerical precision improves speed and reduces memory usage at no cost while training neural networks and is thus a net benefit. Having been built with deep learning in mind, JAX defaults align with that of other DL libraries (e.g. PyTorch, TensorFlow).\n\n:::\n\n### Functionally pure functions\n\nMore importantly, only functionally pure functions—that is, functions for which the outputs are only based on the inputs and which have no side effects—can be used with JAX.\n\n#### Outputs only based on inputs\n\nConsider the function:\n\n```{.python}\ndef impure_func(x):\n    return a + x\n```\n\nwhich uses the variable `a` from the global environment.\n\nThis function is not functionally pure because the outputs (the results of the function) do not solely depend on the arguments (the values given to `x`) passed to it. They also depend on the value of `a`.\n\nRemember how tracing works: new inputs with the same shape and dtype use the cached compiled program directly. If the value of `a` changes in the global environment, a new tracing is not triggered and the cached compiled program uses the old value of `a` (the one that was used during tracing).\n\nIt is only if the code is run on an input `x` with a different shape and/or dtype that tracing happens again and that the new value for `a` takes effect.\n\n:::{.note}\n\nTo demo this, we need to use JIT compilation that we will explain in a later section.\n\n:::\n\n```{.python}\nfrom jax import jit\n\na = jnp.ones(3)\nprint(a)\n```\n\n```\n[1. 1. 1.]\n```\n\n```{.python}\ndef impure_func(x):\n    return a + x\n\nprint(jit(impure_func)(jnp.ones(3)))\n```\n\n```\n[2. 2. 2.]\n```\n\n:::{.note}\n\nAll good here because this is the first run (tracing).\n\n:::\n\nNow, let's change the value of `a` to an array of zeros:\n\n```{.python}\na = jnp.zeros(3)\nprint(a)\n```\n\n```\n[0. 0. 0.]\n```\n\nAnd rerun the same code:\n\n```{.python}\nprint(jit(impure_func)(jnp.ones(3)))\n```\n\n```\n[2. 2. 2.]\n```\n\nWe should have an array of ones, but we get the same result we got earlier. Why? because we are running a cached program with the value that `a` had during tracing.\n\nThe new value for `a` will only take effect if we re-trigger tracing by changing the shape and/or dtype of `x`:\n\n```{.python}\na = jnp.zeros(4)\nprint(a)\n```\n\n```\n[0. 0. 0. 0.]\n```\n\n```{.python}\nprint(jit(impure_func)(jnp.ones(4)))\n```\n\n```\n[1. 1. 1. 1.]\n```\n\nPassing to `impure_func()` an argument of a different shape forced retracing.\n\n#### No side effects\n\nA function is said to have a side effect if it changes something outside of its local environment (if it does anything beside returning an output).\n\nExamples of side effects include:\n\n- printing to standard output/shell,\n- reading from file/writing to file,\n- modifying a global variable.\n\nIn JAX, the side effects will happen during the first run (tracing), but will not happen on subsequent runs. You thus cannot rely on side effects in your code.\n\n```{.python}\ndef impure_func(a, b):\n    print(\"Calculating sum\")\n    return a + b\n\nprint(jit(impure_func)(jnp.arange(3), jnp.arange(3)))\n```\n\n```\nCalculating sum\n[0 2 4]\n```\n\n:::{.note}\n\nPrinting (the side effect) happened here because this is the first run.\n\n:::\n\nLet's rerun the function:\n\n```{.python}\nprint(jit(impure_func)(jnp.arange(3), jnp.arange(3)))\n```\n\n```\n[0 2 4]\n```\n\nThis time, no printing...\n\n### Pseudorandom number generation\n\nProgramming languages usually come with automated [pseudorandom number generator (PRNG)](https://en.wikipedia.org/wiki/Pseudorandom_number_generator) based on nondeterministic data from the operating system. They are extremely convenient, but slow, based on repeats, and problematic in parallel executions.\n\nJAX relies on an explicitly set random state called a *key*.\n\n```{.python}\nfrom jax import random\n\nkey = random.PRNGKey(18)\nprint(key)\n```\n\n```\n[ 0 18]\n```\n\nEach time you call a random function, you need a subkey split from your key. **Keys should only ever be used once in your code.** The key is what makes your code reproducible, but you don't want to reuse it within your code as it would create spurious correlations.\n\nHere is the workflow:\n\n- you split your key into a new key and a subkey,\n- you discard the old key (because it was used to do the split—so its entropy budget, so to speak, has been used),\n- you use the subkey to run your random function and keep the new key for a future split.\n\nTo make sure not to reuse the old key, you can overwrite it by the new one:\n\n```{.python}\nkey, subkey = random.split(key)\n```\n\n```{.python}\nprint(key)\n```\n\n```\n[4197003906 1654466292]\n```\n\n:::{.note}\n\nThat's the value of our new key for future splits.\n\n:::\n\n```{.python}\nprint(subkey)\n```\n\n```\n[1685972163 1654824463]\n```\n\n:::{.note}\n\nThis is the value of the subkey that we can use to call a random function.\n\n:::\n\nLet's use that subkey now:\n\n```{.python}\nprint(random.normal(subkey))\n```\n\n```\n1.1437175\n```\n\n### Immutable arrays\n\n:::{.panel-tabset}\n\n#### Numpy\n\nIn NumPy, you can modify ndarrays:\n\n::: {#ea9011ec .cell execution_count=11}\n``` {.python .cell-code}\na = np.arange(5)\na[0] = 9\nprint(a)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[9 1 2 3 4]\n```\n:::\n:::\n\n\n#### JAX NumPy\n\nJAX arrays are immutable:\n\n```{.python}\na = jnp.arange(5)\na[0] = 9\n```\n\n```\nTypeError: '<class 'jaxlib.xla_extension.ArrayImpl'>' object does not support item assignment. JAX arrays are immutable. Instead of ``x[idx] = y``, use ``x = x.at[idx].set(y)`` or another .at[] method: https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.ndarray.at.html\n```\n\nInstead, you need to create a copy of the array with the mutation. This is done with:\n\n```{.python}\nb = a.at[0].set(9)\nprint(b)\n```\n\n```\n[9 1 2 3 4]\n```\n\nOf course, if you want to modify the array in place, you can overwrite `a`:\n\n```{.python}\na = a.at[0].set(9)\n```\n\n:::\n\n### Strict input control\n\n:::{.panel-tabset}\n\n#### Numpy\n\nNumPy's fundamental object is the ndarray, but NumPy is very tolerant as to the type of input.\n\n::: {#6c2bcd88 .cell execution_count=12}\n``` {.python .cell-code}\nnp.sum([1.0, 2.0])  # here we are using a list\n```\n\n::: {.cell-output .cell-output-display execution_count=42}\n```\n3.0\n```\n:::\n:::\n\n\n::: {#da40c238 .cell execution_count=13}\n``` {.python .cell-code}\nnp.sum((1.0, 2.0))  # here is a tuple\n```\n\n::: {.cell-output .cell-output-display execution_count=43}\n```\n3.0\n```\n:::\n:::\n\n\n#### JAX NumPy\n\nTo avoid inefficiencies, JAX will only accept arrays.\n\n```{.python}\njnp.sum([1.0, 2.0])\n```\n\n```\nTypeError: sum requires ndarray or scalar arguments, got <class 'list'> at position 0.\n```\n\n```{.python}\njnp.sum((1.0, 2.0))\n```\n\n```\nTypeError: sum requires ndarray or scalar arguments, got <class 'tuple'> at position 0.\n```\n\n:::\n\n### Out of bounds indexing\n\n:::{.panel-tabset}\n\n#### Numpy\n\nNumPy will warn you with an error message if you try to index out of bounds:\n\n::: {#a3a67314 .cell execution_count=14}\n``` {.python .cell-code}\nprint(np.arange(5)[10])\n```\n\n::: {.cell-output .cell-output-error}\n```\nIndexError: index 10 is out of bounds for axis 0 with size 5\n```\n:::\n:::\n\n\n#### JAX NumPy\n\n**Be aware that JAX will not raise an error.** Instead, it will silently return the closest boundary:\n\n```{.python}\nprint(jnp.arange(5)[10])\n```\n\n```\n4\n```\n\n:::\n\n## Why all these constraints?\n\nThe more constraints you add to a programming language, the more optimization you can get from the compiler. Speed comes at the cost of convenience.\n\nFor instance, consider a Python list. It is an extremely convenient and flexible object: heterogeneous, mutable... You can do anything with it. But computations on lists are extremely slow.\n\nNumPy's ndarrays are more constrained (homogeneous), but the type constraint permits the creation of a much faster language (NumPy is written in C and Fortran as well as Python) with vectorization, optimizations, and a greatly improved performance.\n\nJAX takes it further: by using an intermediate representation and very strict constraints on type, pure functional programming, etc., yet more optimizations can be achieved and you can optimize your own functions with JIT compilation and the XLA. Ultimately, this is what makes JAX so fast.\n\n<!-- ## Integration with NumPy -->\n\n<!-- `device_put()` -->\n\n<!-- Local Variables: -->\n<!-- pyvenv-activate: \"/home/marie/parvus/prog/mint/ai/jx_env\" -->\n<!-- End: -->\n\n",
    "supporting": [
      "jx_numpy_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}