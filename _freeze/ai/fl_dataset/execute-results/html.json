{
  "hash": "d87f564cefdeb1106a8d19aae74567e8",
  "result": {
    "markdown": "---\ntitle: Loading datasets\nauthor: Marie-HÃ©lÃ¨ne Burle\nbibliography: fl.bib\n---\n\n:::{.def}\n\nNeither JAX nor Flax implement methods to load datasets since [PyTorch](https://github.com/pytorch/pytorch), [TensorFlow](https://github.com/tensorflow/datasets), and [Hugging Face](https://github.com/huggingface/datasets) already provide great APIs for this.\n\nLet's use one of the most classic of all deep learning datasetsâ€”the MNIST [@lecun2010mnist]â€”to see how these APIs work.\n\n:::\n\n## Hugging Face Datasets\n\nThe [Datasets](https://github.com/huggingface/datasets) library from ðŸ¤— is a lightweight, framework-agnostic, and easy to use API to download datasets from the [Hugging Face Hub](https://huggingface.co/datasets). It uses [Apache Arrow](https://arrow.apache.org/)'s efficient caching system, allowing large datasets to be used on machines with small memory [@lhoest-etal-2021-datasets].\n\n### Search dataset\n\nGo to the [Hugging Face Hub](https://huggingface.co/datasets) and search through thousands of open source datasets provided by the community.\n\n### Inspect dataset\n\nYou can get information on a dataset before downloading it.\n\nLoad the dataset builder for the dataset you are interested in:\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nfrom datasets import load_dataset_builder\nds_builder = load_dataset_builder(\"mnist\")\n```\n:::\n\n\nGet a description of the dataset:\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nds_builder.info.description\n```\n\n::: {.cell-output .cell-output-display execution_count=14}\n```\n'The MNIST dataset consists of 70,000 28x28 black-and-white images in 10 classes (one for each digits), with 7,000\\nimages per class. There are 60,000 training images and 10,000 test images.\\n'\n```\n:::\n:::\n\n\nGet information on the features:\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nds_builder.info.features\n```\n\n::: {.cell-output .cell-output-display execution_count=15}\n```\n{'image': Image(decode=True, id=None),\n 'label': ClassLabel(names=['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'], id=None)}\n```\n:::\n:::\n\n\n### Download dataset and load in session\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nfrom datasets import load_dataset\n\nds = load_dataset(\"mnist\")\nds\n```\n\n::: {.cell-output .cell-output-display execution_count=16}\n```\nDatasetDict({\n    train: Dataset({\n        features: ['image', 'label'],\n        num_rows: 60000\n    })\n    test: Dataset({\n        features: ['image', 'label'],\n        num_rows: 10000\n    })\n})\n```\n:::\n:::\n\n\n:::{.note}\n\nYou need to have the [Pillow](https://python-pillow.org/) package installed for this to work since the MNIST is an image dataset.\n\n:::\n\nLet's explore our dataset dictionary:\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nlen(ds)\n```\n\n::: {.cell-output .cell-output-display execution_count=17}\n```\n2\n```\n:::\n:::\n\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nds.keys()\n```\n\n::: {.cell-output .cell-output-display execution_count=18}\n```\ndict_keys(['train', 'test'])\n```\n:::\n:::\n\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nds['train']\n```\n\n::: {.cell-output .cell-output-display execution_count=19}\n```\nDataset({\n    features: ['image', 'label'],\n    num_rows: 60000\n})\n```\n:::\n:::\n\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nlen(ds['train'])\n```\n\n::: {.cell-output .cell-output-display execution_count=20}\n```\n60000\n```\n:::\n:::\n\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nds['train'][0]\n```\n\n::: {.cell-output .cell-output-display execution_count=21}\n```\n{'image': <PIL.PngImagePlugin.PngImageFile image mode=L size=28x28>,\n 'label': 5}\n```\n:::\n:::\n\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\nlen(ds['train'][0])\n```\n\n::: {.cell-output .cell-output-display execution_count=22}\n```\n2\n```\n:::\n:::\n\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\nds['train'][0].keys()\n```\n\n::: {.cell-output .cell-output-display execution_count=23}\n```\ndict_keys(['image', 'label'])\n```\n:::\n:::\n\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\nds['train'][0]['image']\n```\n\n::: {.cell-output .cell-output-display execution_count=24}\n![](fl_dataset_files/figure-html/cell-13-output-1.png){}\n:::\n:::\n\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\nds['train'][0]['label']\n```\n\n::: {.cell-output .cell-output-display execution_count=25}\n```\n5\n```\n:::\n:::\n\n\n### Convert to JAX object\n\nWe need to convert our dataset to a JAX `Array` object:\n\n```{.python}\ndsj = ds.with_format(\"jax\")\ndsj\n```\n\nPrinting `dsj` looks the same, but:\n\n```{.python}\ndsj['train'][0]\n```\n\n```\n{'image': Array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n           0,   0],\n        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n           0,   0],\n       ...,\n        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n           0,   0]], dtype=uint8),\n 'label': Array(5, dtype=int32)}\n```\n\n```{.python}\ndsj['train'][0]['image']\n```\n\n```\nArray([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0],\n       ...,\n       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n          0,   0]], dtype=uint8)\n```\n\n```{.python}\ndsj['train'][0]['label']\n```\n\n```\nArray(5, dtype=int32)\n```\n\n```{.python}\ndsj['train']['label'][:10]\n```\n\n```\nArray([5, 0, 4, 1, 9, 2, 1, 3, 1, 4], dtype=int32)\n```\n\nWe can shuffle the data:\n\n```{.python}\nds_shuffled = dsj.shuffle(seed=123)\nds_shuffled['train']['label'][:10]\n```\n\n```\nArray([4, 4, 4, 1, 7, 8, 5, 2, 8, 3], dtype=int32)\n```\n\nFor normalization and more complex operations, you will need the Hugging Face [transformers](https://github.com/huggingface/transformers) package.\n\n## PyTorch\n\nIf you are familiar with PyTorch DataLoaders, this is an equally great option:\n\n```{.python}\nimport torch\nfrom torchvision import datasets, transforms\n\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.1307,), (0.3081,))\n])\n\ntrain_data = datasets.MNIST(\n    '~/projects/def-sponsor00/data',\n    train=True, download=True, transform=transform)\n\ntest_data = datasets.MNIST(\n    '~/projects/def-sponsor00/data',\n    train=False, transform=transform)\n\n# create DataLoaders\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=20, shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(\n    test_data, batch_size=20, shuffle=False)\n```\n\nYou can find more details in [our PyTorch course](https://mint.westdri.ca/ai/pt_mnist).\n\n## TensorFlow Datasets\n\nFor those familiar with TensorFlow, here is an example from the Flax [Quick start](https://flax.readthedocs.io/en/latest/quick_start.html):\n\n```{.python}\nimport tensorflow_datasets as tfds\nimport tensorflow as tf\n\ndef get_datasets(num_epochs, batch_size):\n    \"\"\"Load MNIST train and test datasets into memory.\"\"\"\n\n    train_ds = tfds.load('mnist', split='train')\n    test_ds = tfds.load('mnist', split='test')\n    \n    # normalize train set\n    train_ds = train_ds.map(\n        lambda sample: {'image': tf.cast(sample['image'], tf.float32) / 255.0,\n                        'label': sample['label']})\n    # normalize test set\n    test_ds = test_ds.map(\n        lambda sample: {'image': tf.cast(sample['image'], tf.float32) / 255.0,\n                        'label': sample['label']})\n    \n    # create shuffled dataset by allocating a buffer size of 1024\n    # to randomly draw elements from\n    train_ds = train_ds.repeat(num_epochs).shuffle(1024)\n\n    # group into batches of batch_size and skip incomplete batch,\n    # prefetch the next sample to improve latency\n    train_ds = train_ds.batch(batch_size, drop_remainder=True).prefetch(1)\n\n    # create shuffled dataset by allocating a buffer size of 1024\n    # to randomly draw elements from\n    test_ds = test_ds.shuffle(1024)\n\n    # group into batches of batch_size and skip incomplete batch,\n    # prefetch the next sample to improve latency\n    test_ds = test_ds.batch(batch_size, drop_remainder=True).prefetch(1)\n\n    return train_ds, test_ds\n```\n\nThere is nothing wrong with using utilities from different libraries! Pick and choose the tools that serve your needs best.\n\n",
    "supporting": [
      "fl_dataset_files"
    ],
    "filters": [],
    "includes": {}
  }
}