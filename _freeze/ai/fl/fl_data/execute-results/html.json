{
  "hash": "459630a91cf2aeee991d31819aac7493",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Loading data\naliases:\n  - fl_dataset\n  - /ai/jx/fl_data\n  - /ai/jxai/fl_data\nbibliography: fl.bib\ncsl: diabetologia.csl\nauthor:\n  - Marie-Hélène Burle\n  - Code adapted from JAX's [Implement ViT from scratch](https://docs.jaxstack.ai/en/latest/JAX_Vision_transformer.html)\n---\n\n:::{.def}\n\nIn this section, we download the [Food-101 dataset](https://data.vision.ee.ethz.ch/cvl/datasets_extra/food-101/) [@bossard14] that we will later use to train and fine-tune models.\n\n:::\n\n## Context\n\nStep one of a classic deep learning workflow: getting the data. There are several options. In this example, we use [Datasets](https://github.com/huggingface/datasets) from [Hugging Face](https://github.com/huggingface).\n\n```{dot}\n//| echo: false\n//| fig-width: 700px\n\ndigraph {\n\nbgcolor=\"transparent\"\nnode [fontname=\"Inconsolata, sans-serif\", color=gray55, fontsize=\"18pt\"]\nedge [color=gray55]\n\nload [label=\"Load data\", shape=plaintext, group=g1]\nproc [label=\"Process data\", shape=plaintext, group=g1, fontcolor=gray55]\nnn [label=\"Define architecture\", shape=plaintext, group=g1, fontcolor=gray55]\npretr [label=\"Pre-trained model\", shape=plaintext, group=g1, fontcolor=gray55]\nopt [label=\"Hyperparameters\", shape=plaintext, group=g1, fontcolor=gray55]\ntrain [label=\"Train\", shape=plaintext, group=g1, fontcolor=gray55]\ncp [label=\"Checkpoint\", shape=plaintext, group=g1, fontcolor=gray55]\n\npt [label=torchdata, fontcolor=gray55, color=gray55]\ntfds [label=tfds, group=g2, fontcolor=gray55, color=gray55]\ndt [label=datasets, fontcolor=darkorange4, color=darkorange4]\n\ngr [label=grain, fontcolor=gray55, color=gray55]\ntv [label=torchvision, fontcolor=gray55, color=gray55]\n\ntr [label=transformers, fontcolor=gray55, color=gray55]\n\nfl1 [label=flax, group=g2, fontcolor=gray55, color=gray55]\nfl2 [label=flax, group=g2, fontcolor=gray55, color=gray55]\n\noa [label=optax, group=g2, fontcolor=gray55, color=gray55]\n\njx [label=jax, group=g2, fontcolor=gray55, color=gray55]\n\nob [label=orbax, group=g2, fontcolor=gray55, color=gray55]\n\n{rank=same; gr load tv tr}\ngr -> load -> tv -> tr [style=invis]\n\n{rank=same; fl1 proc pretr}\nfl1 -> proc -> pretr [style=invis]\n\n{rank=same; jx fl2 opt}\nfl1 -> proc -> pretr [style=invis]\n\n{pt tfds} -> load [color=gray55]\ndt -> load [color=darkorange4]\n{gr tv} -> proc [color=gray55]\nfl1 -> nn [color=gray55]\npretr -> nn [dir=none]\ntr -> pretr [color=gray55]\noa -> opt [color=gray55]\njx -> fl2 -> train [color=gray55]\nob -> cp [color=gray55]\n\nload -> proc -> nn -> opt -> train -> cp [dir=none]\n\n}\n```\n\n## Load packages\n\nPackages necessary for this section:\n\n::: {#98a8d58a .cell execution_count=2}\n``` {.python .cell-code}\n# to get information about a dataset before downloading it\nfrom datasets import load_dataset_builder\n\n# to load dataset from Hugging Face Hub\nfrom datasets import load_dataset\n\n# to display a few samples\nimport matplotlib.pyplot as plt\n```\n:::\n\n\n## Choosing a library\n\nData can be downloaded and processed manually, but many datasets are available via [Hugging Face datasets](https://github.com/huggingface/datasets), [torchvision](https://github.com/pytorch/vision), and [TensorFlow datasets](https://github.com/tensorflow/datasets). Remember that JAX does not implement domain-specific utilities and is not a deep learning library. Flax is a deep learning library, but, because there are already so many good options to load and process data, they did not implement a method of their own.\n\nChoose the library you are the most familiar with, or the one for which you found code somewhere, or the one that seems the easiest to you, or provides the exact functionality that you want for your project.\n\nThe [Food-101](https://data.vision.ee.ethz.ch/cvl/datasets_extra/food-101/) dataset for instance can be accessed with `torchvision.datasets.Food101` since it is [one of TorchVision datasets](https://pytorch.org/vision/main/generated/torchvision.datasets.Food101.html) or with `tfds.image_classification.Food101` since it is also [one of TFDS datasets](https://www.tensorflow.org/datasets/catalog/food101).\n\n[It is also](https://huggingface.co/datasets/ethz/food101) in the [Hugging Face Hub](https://huggingface.co/datasets) and that's the method that we will use here.\n\n## Hugging Face datasets\n\nThe [Datasets](https://github.com/huggingface/datasets) library from [Hugging Face](https://github.com/huggingface) is a lightweight, framework-agnostic, and easy to use API to download datasets from the [Hugging Face Hub](https://huggingface.co/datasets). It uses [Apache Arrow](https://arrow.apache.org/)'s efficient caching system, allowing large datasets to be used on machines with small memory [@lhoest-etal-2021-datasets].\n\n### Search dataset\n\nGo to the [Hugging Face Hub](https://huggingface.co/datasets) and search through thousands of open source datasets provided by the community.\n\n### Inspect dataset\n\nYou can get information on a dataset before downloading it.\n\nLoad the dataset builder for the dataset you are interested in:\n\n::: {#383f5b84 .cell execution_count=3}\n``` {.python .cell-code}\nds_builder = load_dataset_builder(\"food101\")\n```\n:::\n\n\nGet a description of the dataset (if it exists—here it doesn't):\n\n::: {#d99994df .cell execution_count=4}\n``` {.python .cell-code}\nds_builder.info.description\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\n''\n```\n:::\n:::\n\n\nGet information on the features:\n\n::: {#05d100d3 .cell execution_count=5}\n``` {.python .cell-code}\nds_builder.info.features\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\n{'image': Image(mode=None, decode=True),\n 'label': ClassLabel(names=['apple_pie', 'baby_back_ribs', 'baklava', 'beef_carpaccio', 'beef_tartare', 'beet_salad', 'beignets', 'bibimbap', 'bread_pudding', 'breakfast_burrito', 'bruschetta', 'caesar_salad', 'cannoli', 'caprese_salad', 'carrot_cake', 'ceviche', 'cheesecake', 'cheese_plate', 'chicken_curry', 'chicken_quesadilla', 'chicken_wings', 'chocolate_cake', 'chocolate_mousse', 'churros', 'clam_chowder', 'club_sandwich', 'crab_cakes', 'creme_brulee', 'croque_madame', 'cup_cakes', 'deviled_eggs', 'donuts', 'dumplings', 'edamame', 'eggs_benedict', 'escargots', 'falafel', 'filet_mignon', 'fish_and_chips', 'foie_gras', 'french_fries', 'french_onion_soup', 'french_toast', 'fried_calamari', 'fried_rice', 'frozen_yogurt', 'garlic_bread', 'gnocchi', 'greek_salad', 'grilled_cheese_sandwich', 'grilled_salmon', 'guacamole', 'gyoza', 'hamburger', 'hot_and_sour_soup', 'hot_dog', 'huevos_rancheros', 'hummus', 'ice_cream', 'lasagna', 'lobster_bisque', 'lobster_roll_sandwich', 'macaroni_and_cheese', 'macarons', 'miso_soup', 'mussels', 'nachos', 'omelette', 'onion_rings', 'oysters', 'pad_thai', 'paella', 'pancakes', 'panna_cotta', 'peking_duck', 'pho', 'pizza', 'pork_chop', 'poutine', 'prime_rib', 'pulled_pork_sandwich', 'ramen', 'ravioli', 'red_velvet_cake', 'risotto', 'samosa', 'sashimi', 'scallops', 'seaweed_salad', 'shrimp_and_grits', 'spaghetti_bolognese', 'spaghetti_carbonara', 'spring_rolls', 'steak', 'strawberry_shortcake', 'sushi', 'tacos', 'takoyaki', 'tiramisu', 'tuna_tartare', 'waffles'])}\n```\n:::\n:::\n\n\n### Download dataset\n\nWe will only use the first 5 classes of food (instead of 101) to test our code. To prevent us from all downloading the data (by default in `~/.cache/huggingface`), we will use a joint cache directory at `/project/60055/data`.\n\n```{.python}\ntrain_size = 5 * 750\nval_size = 5 * 250\n\ntrain_dataset = load_dataset(\"food101\",\n                             split=f\"train[:{train_size}]\",\n                             cache_dir=\"/project/60055/data\")\n\nval_dataset = load_dataset(\"food101\",\n                           split=f\"validation[:{val_size}]\",\n                           cache_dir=\"/project/60055/data\")\n```\n\n\n\n## Explore data\n\nLet's inspect our data:\n\n::: {#24c50d0a .cell execution_count=7}\n``` {.python .cell-code}\nprint(\"Training set size:\", len(train_dataset))\nprint(\"Validation set size:\", len(val_dataset))\nprint(\"Training set shape:\", train_dataset.shape)\nprint(\"Validation set shape:\", val_dataset.shape)\nprint(\"First item of training set:\", train_dataset[0])\nprint(\"Firt image of training set:\", train_dataset[0][\"image\"])\nprint(\"First label of training set:\", train_dataset[0][\"label\"])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTraining set size: 3750\nValidation set size: 1250\nTraining set shape: (3750, 2)\nValidation set shape: (1250, 2)\nFirst item of training set: {'image': <PIL.Image.Image image mode=RGB size=384x512 at 0x7F6CE1C26CF0>, 'label': 6}\nFirt image of training set: <PIL.Image.Image image mode=RGB size=384x512 at 0x7F6CE1748410>\nFirst label of training set: 6\n```\n:::\n:::\n\n\nHere is the beginning of the list of foods:\n\n::: {#3c50a099 .cell execution_count=8}\n``` {.python .cell-code}\nprint(train_dataset.features[\"label\"].names[:5])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n['apple_pie', 'baby_back_ribs', 'baklava', 'beef_carpaccio', 'beef_tartare']\n```\n:::\n:::\n\n\nAnd here is the food of the first item in the training set (label `6`):\n\n::: {#6f9436bf .cell execution_count=9}\n``` {.python .cell-code}\nprint(\"First food of training set:\", train_dataset.features[\"label\"].names[6])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFirst food of training set: beignets\n```\n:::\n:::\n\n\nTo make this simpler, we can create a mapping of the labels matching their order:\n\n::: {#8d73aba0 .cell execution_count=10}\n``` {.python .cell-code}\nlabels_mapping = {}\nindex = 0\nfor i in range(0, len(val_dataset), 250):\n    label = val_dataset[i][\"label\"]\n    if label not in labels_mapping:\n        labels_mapping[label] = index\n        index += 1\n\ninv_labels_mapping = {v: k for k, v in labels_mapping.items()}\n\nprint(inv_labels_mapping)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n{0: 6, 1: 79, 2: 81, 3: 53, 4: 10}\n```\n:::\n:::\n\n\nAnd a mapping of the names:\n\n::: {#c7a0bbe4 .cell execution_count=11}\n``` {.python .cell-code}\nnames_map={k: train_dataset.features[\"label\"].names[v] for k, v in inv_labels_mapping.items()}\nprint(names_map)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n{0: 'beignets', 1: 'prime_rib', 2: 'ramen', 3: 'hamburger', 4: 'bruschetta'}\n```\n:::\n:::\n\n\nNow, to get the food of the first item, we just have to do:\n\n::: {#e4107be1 .cell execution_count=12}\n``` {.python .cell-code}\nprint(names_map[0])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nbeignets\n```\n:::\n:::\n\n\nHere is a function to display some samples (their images, label, and food type):\n\n::: {#397844a0 .cell execution_count=13}\n``` {.python .cell-code}\ndef display_datapoints(*datapoints, tag=\"\", names_map=None):\n    num_samples = len(datapoints)\n\n    fig, axs = plt.subplots(1, num_samples, figsize=(20, 10))\n    for i, datapoint in enumerate(datapoints):\n        if isinstance(datapoint, dict):\n            img, label = datapoint[\"image\"], datapoint[\"label\"]\n        else:\n            img, label = datapoint\n\n        if hasattr(img, \"dtype\") and img.dtype in (np.float32, ):\n            img = ((img - img.min()) / (img.max() - img.min()) * 255.0).astype(np.uint8)\n\n        label_str = f\" ({names_map[label]})\" if names_map is not None else \"\"\n        axs[i].set_title(f\"{tag} Label: {label}{label_str}\")\n        axs[i].imshow(img)\n```\n:::\n\n\nLet's display the first 3 items (images and labels) of both the training and validation sets:\n\n::: {#e965b916 .cell execution_count=14}\n``` {.python .cell-code}\ndisplay_datapoints(\n    train_dataset[0],\n    train_dataset[1],\n    train_dataset[2],\n    tag=\"(Training)\",\n    names_map=train_dataset.features[\"label\"].names,\n)\n\ndisplay_datapoints(\n    val_dataset[0],\n    val_dataset[1],\n    val_dataset[2],\n    tag=\"(Validation)\",\n    names_map=val_dataset.features[\"label\"].names,\n)\n```\n\n::: {.cell-output .cell-output-display}\n![](fl_data_files/figure-html/cell-14-output-1.png){width=1545 height=645}\n:::\n\n::: {.cell-output .cell-output-display}\n![](fl_data_files/figure-html/cell-14-output-2.png){width=1545 height=499}\n:::\n:::\n\n\n",
    "supporting": [
      "fl_data_files"
    ],
    "filters": [],
    "includes": {}
  }
}