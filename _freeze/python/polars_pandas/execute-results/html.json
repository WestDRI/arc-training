{
  "hash": "3992fcee3f293d20b8e8333ad667add5",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Comparison with pandas\nauthor: Marie-Hélène Burle\n---\n\n\n\n\n:::{.def}\n\nAs pandas was the only data frame library for Python for a long time, many Python users are familiar with it and a comparison with Polars might be useful.\n\n:::\n\n## Overview\n\n| | pandas | Polars |\n|--|--|--|\n| Available for | Python | Rust, Python, R, NodeJS |\n| Written in | Cython | Rust |\n| Multithreading | Some operations | Yes (GIL released) |\n| Index | Rows are indexed | Integer positions are used |\n| Evaluation | Eager | Eager and lazy |\n| Query optimizer | No | Yes |\n| Out-of-core | No | Yes |\n| [SIMD](https://en.wikipedia.org/wiki/Single_instruction,_multiple_data) vectorization | Yes | Yes |\n| Data in memory | With [NumPy](https://github.com/numpy/numpy) arrays | With [Apache Arrow](https://github.com/apache/arrow) arrays |\n| Memory efficiency | Poor | Excellent |\n| Handling of missing data | Inconsistent | Consistent, promotes type stability |\n\n## Performance\n\n### Example 1\n\nLet's use the [FizzBuzz](https://en.wikipedia.org/wiki/Fizz_buzz#:~:text=Fizz%20buzz%20is%20a%20group,with%20the%20word%20%22fizzbuzz%22.) problem.\n\nIn his pandas course, Alex compares multiple methods and shows that [the best method uses masks](https://wgpages.netlify.app/python2/python-13-pandas/#three-solutions-to-a-classification-problem). Let's see how Polars fares in comparison to pandas' best method.\n\nFirst, let's load the packages we will need:\n\n::: {#7bfbd55d .cell execution_count=2}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport polars as pl\n```\n:::\n\n\nAnd let's make sure that the code works.\n\nWith pandas:\n\n::: {#ba24f745 .cell execution_count=3}\n``` {.python .cell-code}\ndf_pd = pd.DataFrame()\nsize = 10_000\ndf_pd[\"number\"] = np.arange(1, size+1)\ndf_pd[\"response\"] = df_pd[\"number\"].astype(str)\ndf_pd.loc[df_pd[\"number\"] % 3 == 0, \"response\"] = \"Fizz\"\ndf_pd.loc[df_pd[\"number\"] % 5 == 0, \"response\"] = \"Buzz\"\ndf_pd.loc[df_pd[\"number\"] % 15 == 0, \"response\"] = \"FizzBuzz\"\n\nprint(df_pd)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      number response\n0          1        1\n1          2        2\n2          3     Fizz\n3          4        4\n4          5     Buzz\n...      ...      ...\n9995    9996     Fizz\n9996    9997     9997\n9997    9998     9998\n9998    9999     Fizz\n9999   10000     Buzz\n\n[10000 rows x 2 columns]\n```\n:::\n:::\n\n\nWith Polars:\n\n::: {#bcfc5476 .cell execution_count=4}\n``` {.python .cell-code}\nsize = 10_000\ndf_pl = pl.DataFrame({\"number\": np.arange(1, size+1)})\ndf_pl.with_columns(pl.col(\"number\").cast(pl.String).alias(\"response\"))\ndf_pl = df_pl.with_columns(\n    pl.when(pl.col(\"number\") % 3 == 0)\n    .then(pl.lit(\"Fizz\"))\n    .when(pl.col(\"number\") % 5 == 0)\n    .then(pl.lit(\"Buzz\"))\n    .when(pl.col(\"number\") % 15 == 0)\n    .then(pl.lit(\"FizzBuzz\"))\n    .otherwise(pl.col(\"number\"))\n    .alias(\"response\")\n)\n\nprint(df_pl)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nshape: (10_000, 2)\n┌────────┬──────────┐\n│ number ┆ response │\n│ ---    ┆ ---      │\n│ i64    ┆ str      │\n╞════════╪══════════╡\n│ 1      ┆ 1        │\n│ 2      ┆ 2        │\n│ 3      ┆ Fizz     │\n│ 4      ┆ 4        │\n│ 5      ┆ Buzz     │\n│ …      ┆ …        │\n│ 9996   ┆ Fizz     │\n│ 9997   ┆ 9997     │\n│ 9998   ┆ 9998     │\n│ 9999   ┆ Fizz     │\n│ 10000  ┆ Buzz     │\n└────────┴──────────┘\n```\n:::\n:::\n\n\nNow, let's time them.\n\npandas:\n\n::: {#8a226118 .cell execution_count=5}\n``` {.python .cell-code}\n%%timeit\n\ndf_pd = pd.DataFrame()\nsize = 10_000\ndf_pd[\"number\"] = np.arange(1, size+1)\ndf_pd[\"response\"] = df_pd[\"number\"].astype(str)\ndf_pd.loc[df_pd[\"number\"] % 3 == 0, \"response\"] = \"Fizz\"\ndf_pd.loc[df_pd[\"number\"] % 5 == 0, \"response\"] = \"Buzz\"\ndf_pd.loc[df_pd[\"number\"] % 15 == 0, \"response\"] = \"FizzBuzz\"\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n9.11 ms ± 145 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n```\n:::\n:::\n\n\nPolars:\n\n::: {#d2683b15 .cell execution_count=6}\n``` {.python .cell-code}\n%%timeit\n\nsize = 10_000\ndf_pl = pl.DataFrame({\"number\": np.arange(1, size+1)})\ndf_pl.with_columns(pl.col(\"number\").cast(pl.String).alias(\"response\"))\ndf_pl.with_columns(\n    pl.when(pl.col(\"number\") % 3 == 0)\n    .then(pl.lit(\"Fizz\"))\n    .when(pl.col(\"number\") % 5 == 0)\n    .then(pl.lit(\"Buzz\"))\n    .when(pl.col(\"number\") % 15 == 0)\n    .then(pl.lit(\"FizzBuzz\"))\n    .otherwise(pl.col(\"number\"))\n    .alias(\"response\")\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n930 μs ± 16.9 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n```\n:::\n:::\n\n\nThat's a speedup of 9 (the longer the series, the larger this speedup will be).\n\n### Example 2\n\nFor a second example, let's go back to [the jeopardy example with a large file](https://wgpages.netlify.app/python2/python-13-pandas/#example-with-a-larger-dataframe) and compare the timing of pandas and Polar.\n\nFirst, let's make sure that the code works.\n\npandas:\n\n::: {#0fe17d55 .cell execution_count=7}\n``` {.python .cell-code}\ndf_pd = pd.read_csv(\"https://raw.githubusercontent.com/razoumov/publish/master/jeopardy.csv\")\ndf_pd.loc[df_pd[\"Category\"] == \"HISTORY\"].shape\n```\n\n::: {.cell-output .cell-output-display execution_count=16}\n```\n(349, 7)\n```\n:::\n:::\n\n\nPolars:\n\n::: {#1df4ca21 .cell execution_count=8}\n``` {.python .cell-code}\ndf_pl = pl.read_csv(\"https://raw.githubusercontent.com/razoumov/publish/master/jeopardy.csv\")\ndf_pl.filter(pl.col(\"Category\") == \"HISTORY\").shape\n```\n\n::: {.cell-output .cell-output-display execution_count=17}\n```\n(349, 7)\n```\n:::\n:::\n\n\nAnd now for timings.\n\npandas:\n\n::: {#3587f073 .cell execution_count=9}\n``` {.python .cell-code}\n%%timeit\n\ndf_pd = pd.read_csv(\"https://raw.githubusercontent.com/razoumov/publish/master/jeopardy.csv\")\ndf_pd.loc[df_pd[\"Category\"] == \"HISTORY\"].shape\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1.49 s ± 50.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n```\n:::\n:::\n\n\nPolars:\n\n::: {#c00c9b19 .cell execution_count=10}\n``` {.python .cell-code}\n%%timeit\n\ndf_pl = pl.read_csv(\"https://raw.githubusercontent.com/razoumov/publish/master/jeopardy.csv\")\ndf_pl.filter(pl.col(\"Category\") == \"HISTORY\").shape\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n817 ms ± 26.6 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n```\n:::\n:::\n\n\nThat's a speedup of 2.\n\nBut it gets much better with [lazy evaluation](polars_lazy.qmd). First, we create a LazyFrame instead of a DataFrame by using `scan_csv` instead of `read_csv`. The query is not evaluated but a graph is created. This allows the query optimizer to combine operations and perform [optimizations](https://docs.pola.rs/user-guide/lazy/optimizations/) where possible, very much the way compilers work. To evaluate the query and get a result, we use the `collect` method.\n\nLet's make sure that the lazy Polars code gives us the same result:\n\n::: {#5ecaca97 .cell execution_count=11}\n``` {.python .cell-code}\ndf_pl = pl.scan_csv(\"https://raw.githubusercontent.com/razoumov/publish/master/jeopardy.csv\")\ndf_pl.filter(pl.col(\"Category\") == \"HISTORY\").collect().shape\n```\n\n::: {.cell-output .cell-output-display execution_count=20}\n```\n(349, 7)\n```\n:::\n:::\n\n\nLazy timing:\n\n::: {#7cb7430a .cell execution_count=12}\n``` {.python .cell-code}\n%%timeit\n\ndf_pl = pl.scan_csv(\"https://raw.githubusercontent.com/razoumov/publish/master/jeopardy.csv\")\ndf_pl.filter(pl.col(\"Category\") == \"HISTORY\").collect().shape\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n72.2 ms ± 14.6 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n```\n:::\n:::\n\n\nThat's a speedup of 20 (the larger the file, the larger this speedup will be).\n\n:::{.hnote}\n\nPandas is trying to fight back: v 2.0 came with optional Arrow support instead of NumPy, then [it became the default engine](https://dataalgo.medium.com/pandas-2-0-ditches-numpy-for-pyarrow-what-you-need-to-know-cbba4cb60249), but performance remains way below that of Polars (e.g. in [DataCamp benchmarks](https://www.datacamp.com/tutorial/high-performance-data-manipulation-in-python-pandas2-vs-polars), [official benchmarks](https://pola.rs/posts/benchmarks/), many blog posts for [whole scripts](https://medium.com/@asimandia/benchmarking-performance-polars-vs-vaex-vs-pandas-f1c889dccc12) or [individual tasks](https://medium.com/cuenex/pandas-2-0-vs-polars-the-ultimate-battle-a378eb75d6d1)).\n\n:::\n\n:::{.info}\n\n**Comparison with other frameworks**\n\nComparisons between Polars and distributed (Dask, Ray, Spark) or GPU (RAPIDS) libraries aren't the most pertinent since they can be used in *combination with* Polars and the benefits can thus be combined.\n\nIt only makes sense to compare Polars with other libraries occupying the same \"niche\" such as pandas or Vaex.\n\nFor [Vaex](https://github.com/vaexio/vaex), [some benchmark found it twice slower](https://medium.com/@asimandia/benchmarking-performance-polars-vs-vaex-vs-pandas-f1c889dccc12), but this could have changed with recent developments.\n\nOne framework performing better than Polars in some benchmarks is [datatable](https://github.com/h2oai/datatable) (derived from the R package [data.table](https://cran.r-project.org/web/packages/data.table/index.html)), but it hasn't been developed for a year—a sharp contrast with the fast development of Polars.\n\n:::\n\n## Migrating from Pandas\n\nRead [the migration guide](https://docs.pola.rs/user-guide/migration/pandas/#selecting-data): it will help you write Polars code rather than \"literally translated\" Pandas code that runs, but doesn't make use of Polars' strengths. The differences in style mostly come from the fact that Polars runs in parallel.\n\n",
    "supporting": [
      "polars_pandas_files"
    ],
    "filters": [],
    "includes": {}
  }
}