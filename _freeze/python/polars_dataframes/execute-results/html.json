{
  "hash": "8f4b025dacc410a2ae4ab816bb14ace2",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: The world of data frames\nauthor: Marie-Hélène Burle\n---\n\n:::{.def}\n\nLet's talk about data frames, how they came to the world of programming, how Pandas had the monopoly for many years in Python, and how things are changing very quickly at the moment.\n\n:::\n\n## Tabular data\n\nMany fields of machine learning, data science, and humanities rely on tabular data where\n\n- columns hold variables and are homogeneous (same data type),\n- rows contain observations and can be heterogeneous.\n\nEarly computer options to manipulate such data were limited to [spreadsheets](https://en.wikipedia.org/wiki/Spreadsheet) (e.g. Microsoft Excel).\n\nDataframes (data frames or DataFrames) are two dimensional objects that brought tabular data to programming.\n\n## Early history of dataframes\n\nAfter data frames emerged in S, then R, they were added to Python with the library [Pandas](https://pandas.pydata.org/) in 2008.\n\n\n```{dot}\n//| echo: false\n//| fig-height: 250px\n\nstrict graph {\n  \nbgcolor=\"transparent\"\ngraph [fontname=\"Inconsolata, sans-serif\"]\nnode [fontname=\"Inconsolata, sans-serif\", fontsize=15]\n\ny1 [label=1990, shape=plaintext, group=g1, group=g1]\ny2 [label=2000, shape=plaintext, group=g1, group=g1]\ny3 [label=2008, shape=plaintext, group=g1]\n\nl1 [label=\"S programming language\", href=\"https://en.wikipedia.org/wiki/S_(programming_language)\", shape=plaintext, group=g2, fontcolor=\"#5592FD\"]\nl2 [label=\"R\", href=\"https://en.wikipedia.org/wiki/R_(programming_language)\", shape=plaintext, group=g2, fontcolor=\"#5592FD\"]\nl3 [label=\"Pandas (Python)\", href=\"https://en.wikipedia.org/wiki/Pandas_(software)\", shape=plaintext, group=g2, fontcolor=\"#5592FD\"]\n\n{rank=same; y1 l1}\n\ny1 -- y2 -- y3\nl1 -- l2 -- l3 [style=invis]\n\n}\n```\n\n\nThe world was simple ... but slow with high memory usage and it remains thus for a long time.\n\n## Issues with Pandas\n\nWhile Pandas was *the* Python data frame library and is now at the core of many other libraries, [Wes McKinney](https://wesmckinney.com/) (pandas creator) himself [has complaints about it](https://wesmckinney.com/blog/apache-arrow-pandas-internals/), mostly:\n\n- the internals are too far from the hardware,\n- no support for memory-mapped datasets,\n- poor performance in database and file ingest / export,\n- lack of proper support for missing data,\n- lack of memory use and RAM management transparency,\n- weak support for categorical data,\n- `groupby` operations are complex, awkward, and slow,\n- appending data to a DataFrame tedious and very costly,\n- limited, non-extensible type metadata,\n- eager evaluation model with no query planning,\n- slow and limited multicore algorithms for large datasets.\n\n## A rich new field\n\nAfter years of Pandas as *the* Python dataframe library, there is currently an exuberant explosion of faster alternatives.\n\n### Parallel computing\n\nThe Python [global interpreter lock (GIL)](https://en.wikipedia.org/wiki/Global_interpreter_lock) gets in the way of multi-threading.\n\nLibraries such as [Ray](https://github.com/ray-project/ray), [Dask](https://github.com/dask/dask), and [Apache Spark](https://github.com/apache/spark) allow the use of multiple cores and bring dataframes to clusters.\n\nDask and Spark have APIs for Pandas and [Modin](https://docs.ray.io/en/latest/ray-more-libs/modin/index.html) makes this even more trivial by providing a drop-in replacement for Pandas on Dask, Spark, and Ray.\n\n[fugue](https://github.com/fugue-project/fugue/) provides a unified interface for distributed computing that works on Spark, Dask, and Ray.\n\n### Accelerators\n\n[RAPIDS](https://rapids.ai/) brings dataframes on the GPUs with the [cuDF library](https://github.com/rapidsai/cudf).\n\nIntegration with pandas is easy.\n\n### Lazy out-of-core\n\n[Vaex](https://github.com/vaexio/vaex) exists as an alternative to pandas.\n\n### SQL\n\n[Structured query language (SQL)](https://en.wikipedia.org/wiki/SQL) handles [relational databases](https://en.wikipedia.org/wiki/Relational_database), but the distinction between SQL and dataframe software is getting increasingly blurry with most libraries now able to handle both.\n\n[DuckDB](https://github.com/duckdb/duckdb) is a very fast and popular option with good integration with pandas.\n\nMany additional options such as [dbt](https://github.com/dbt-labs/dbt-core) and the [snowflake snowpark Python API](https://github.com/snowflakedb/snowpark-python) exist, although integration with pandas is not always as good.\n\n## Polars\n\nAnd then came [Polars](https://pola.rs/).\n\nThe new memory standard is [Apache Arrow](https://arrow.apache.org/) and the most efficient library making use of it is Polars.\n\nIn addition, most libraries are developing an integration with Polars, lodging it nicely in the Python ecosystem.\n\nFor maximum dataframe efficiency, the best strategy currently seems to be:\n\n- single machine&nbsp; ➔ &nbsp;use Polars,\n- cluster&nbsp; ➔ &nbsp;use Polars + [fugue](https://github.com/fugue-project/fugue/) ([example benchmark](https://medium.com/fugue-project/benchmarking-pyspark-pandas-pandas-udfs-and-fugue-polars-198c3109a226), [documentation of Polars integration](https://fugue-tutorials.readthedocs.io/tutorials/integrations/backends/polars.html)),\n- GPUs available&nbsp; ➔ &nbsp;use Polars + [RAPIDS](https://rapids.ai/) library [cuDF](https://github.com/rapidsai/cudf) ([Polars integration coming soon](https://pola.rs/posts/polars-on-gpu/)),\n- SQL&nbsp; ➔ &nbsp;use Polars + [DuckDB](https://github.com/duckdb/duckdb) ([documentation of Polars integration](https://duckdb.org/docs/guides/python/polars.html)),\n- combination of the above (e.g. cluster with GPUs)&nbsp; ➔ &nbsp;use a combination of the above tools (e.g. Polars + fugue + RAPIDS).\n\n### Comparison with Pandas\n\n#### Overview\n\n| | Pandas | Polars |\n|--|--|--|\n| Available for | Python | Rust, Python, R, NodeJS |\n| Written in | Cython | Rust |\n| Multithreading | Some operations | Yes (GIL released) |\n| Index | Rows are indexed | Integer positions are used |\n| Evaluation | Eager only | Lazy and eager |\n| Query optimizer | No | Yes |\n| Out-of-core | No | Yes |\n| [SIMD](https://en.wikipedia.org/wiki/Single_instruction,_multiple_data) vectorization | Yes | Yes |\n| Data in memory | With [NumPy](https://github.com/numpy/numpy) arrays | With [Apache Arrow](https://github.com/apache/arrow) arrays |\n| Memory efficiency | Poor | Excellent |\n| Handling of missing data | Inconsistent | Consistent, promotes type stability |\n\n<!-- #### Table visualization -->\n\n<!-- While Pandas comes with internal capabilities [to make publication ready tables](https://pandas.pydata.org/docs/user_guide/style.html), Polars [integrates very well](https://posit-dev.github.io/great-tables/blog/polars-styling/) with [great-tables](https://github.com/posit-dev/great-tables) to achieve the same goal. -->\n\n#### Performance\n\n:::{.info}\n\nYou can run these on our training cluster by either of two methods:\n\n##### Method 1\n\n[Launching JupyterHub](https://mint.westdri.ca/python/intro_run#jupyter-1).\n\n##### Method 2\n\nLogging on the cluster through SSH, and running the following in your command line:\n\n```{.bash}\nmodule load ipykernel/2023b\nsource /project/def-sponsor00/shared/scientificpython-env/bin/activate\nsalloc --time=2:00:0 --mem-per-cpu=3600\nipython\n```\n\n:::\n\nLet's go back to the [FizzBuzz](https://en.wikipedia.org/wiki/Fizz_buzz#:~:text=Fizz%20buzz%20is%20a%20group,with%20the%20word%20%22fizzbuzz%22.) problem.\n\n[The best method with Pandas used masks](https://wgpages.netlify.app/python2/python-13-pandas/#three-solutions-to-a-classification-problem). Let's see how Polars fares in comparison.\n\nFirst, let's load the packages we will need:\n\n::: {#410a4b80 .cell execution_count=2}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport polars as pl\n```\n:::\n\n\nAnd let's make sure that the code works.\n\nWith Pandas:\n\n```{.python}\ndf_pd = pd.DataFrame()\nsize = 10_000\ndf_pd[\"number\"] = np.arange(1, size+1)\ndf_pd[\"response\"] = df_pd[\"number\"].astype(str)\ndf_pd.loc[df_pd[\"number\"] % 3 == 0, \"response\"] = \"Fizz\"\ndf_pd.loc[df_pd[\"number\"] % 5 == 0, \"response\"] = \"Buzz\"\ndf_pd.loc[df_pd[\"number\"] % 15 == 0, \"response\"] = \"FizzBuzz\"\n\ndf_pd\n```\n\n```\n      number response\n0          1        1\n1          2        2\n2          3     Fizz\n3          4        4\n4          5     Buzz\n...      ...      ...\n9995    9996     Fizz\n9996    9997     9997\n9997    9998     9998\n9998    9999     Fizz\n9999   10000     Buzz\n\n[10000 rows x 2 columns]\n```\n\nWith Polars:\n\n```{.python}\nsize = 10_000\ndf_pl = pl.DataFrame({\"number\": np.arange(1, size+1)})\ndf_pl.with_columns(pl.col(\"number\").cast(pl.String).alias(\"response\"))\ndf_pl.with_columns(\n    pl.when(pl.col(\"number\") % 3 == 0)\n    .then(pl.lit(\"Fizz\"))\n    .when(pl.col(\"number\") % 5 == 0)\n    .then(pl.lit(\"Buzz\"))\n    .when(pl.col(\"number\") % 15 == 0)\n    .then(pl.lit(\"FizzBuzz\"))\n    .otherwise(pl.col(\"number\"))\n    .alias(\"response\")\n)\n```\n\n```\nshape: (10_000, 2)\n┌────────┬──────────┐\n│ number ┆ response │\n│ ---    ┆ ---      │\n│ i64    ┆ str      │\n╞════════╪══════════╡\n│ 1      ┆ 1        │\n│ 2      ┆ 2        │\n│ 3      ┆ Fizz     │\n│ 4      ┆ 4        │\n│ 5      ┆ Buzz     │\n│ …      ┆ …        │\n│ 9996   ┆ Fizz     │\n│ 9997   ┆ 9997     │\n│ 9998   ┆ 9998     │\n│ 9999   ┆ Fizz     │\n│ 10000  ┆ Buzz     │\n└────────┴──────────┘\n```\n\nNow, let's time them.\n\nPandas:\n\n```{.python}\n%%timeit\n\ndf_pd = pd.DataFrame()\nsize = 10_000\ndf_pd[\"number\"] = np.arange(1, size+1)\ndf_pd[\"response\"] = df_pd[\"number\"].astype(str)\ndf_pd.loc[df_pd[\"number\"] % 3 == 0, \"response\"] = \"Fizz\"\ndf_pd.loc[df_pd[\"number\"] % 5 == 0, \"response\"] = \"Buzz\"\ndf_pd.loc[df_pd[\"number\"] % 15 == 0, \"response\"] = \"FizzBuzz\"\n```\n\n```\n4.75 ms ± 9.76 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n```\n\nPolars:\n\n```{.python}\n%%timeit\n\nsize = 10_000\ndf_pl = pl.DataFrame({\"number\": np.arange(1, size+1)})\ndf_pl.with_columns(pl.col(\"number\").cast(pl.String).alias(\"response\"))\ndf_pl.with_columns(\n    pl.when(pl.col(\"number\") % 3 == 0)\n    .then(pl.lit(\"Fizz\"))\n    .when(pl.col(\"number\") % 5 == 0)\n    .then(pl.lit(\"Buzz\"))\n    .when(pl.col(\"number\") % 15 == 0)\n    .then(pl.lit(\"FizzBuzz\"))\n    .otherwise(pl.col(\"number\"))\n    .alias(\"response\")\n)\n```\n\n```\n518 μs ± 580 ns per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n```\n\nThat's a speedup of almost 10 (the longer the series, the larger this speedup will be).\n\nPolars: 1, Pandas: 0 🙂\n\n---\n\nLet's go back to [the jeopardy example with a large file](https://wgpages.netlify.app/python2/python-13-pandas/#example-with-a-larger-dataframe) and compare the timing of Pandas and Polar.\n\nPandas:\n\n```{.python}\n%%timeit\n\ndf_pd = pd.read_csv(\"https://raw.githubusercontent.com/razoumov/publish/master/jeopardy.csv\")\ndf_pd.loc[df_pd[\"Category\"] == \"HISTORY\"].shape\n```\n\n```\n887 ms ± 164 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n```\n\nPolars:\n\n```{.python}\n%%timeit\n\ndf_pl = pl.read_csv(\"https://raw.githubusercontent.com/razoumov/publish/master/jeopardy.csv\")\ndf_pl.filter(pl.col(\"Category\") == \"HISTORY\").shape\n```\n\n```\n446 ms ± 89.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n```\n\nThat's a speedup of 2.\n\nBut it gets even better: **Polars support [lazy evaluation](https://en.wikipedia.org/wiki/Lazy_evaluation)**. \n\n[Lazy evaluation is not yet implemented when reading files from the cloud](https://github.com/pola-rs/polars/issues/13115) (Polars is a very new tool, but its functionalities are expanding very fast). This means that we cannot test the benefit of lazy evaluation in our example by using the CSV file in its current location (<https://github.com/pola-rs/polars/issues/13115>).\n\nI downloaded it on our training cluster however so that we can run the test.\n\nFirst, let's make sure that the code works.\n\nPandas:\n\n```{.python}\ndf_pd = pd.read_csv(\"/project/def-sponsor00/data/jeopardy.csv\")\ndf_pd.loc[df_pd[\"Category\"] == \"HISTORY\"].shape\n```\n\n```\n(349, 7)\n```\n\nPolars:\n\n```{.python}\ndf_pl = pl.scan_csv(\"/project/def-sponsor00/data/jeopardy.csv\")\ndf_pl.filter(pl.col(\"Category\") == \"HISTORY\").collect().shape\n```\n\n```\n(349, 7)\n```\n\nAnd now for the timing.\n\nPandas:\n\n```{.python}\n%%timeit\n\ndf_pd = pd.read_csv(\"/project/def-sponsor00/data/jeopardy.csv\")\ndf_pd.loc[df_pd[\"Category\"] == \"HISTORY\"].shape\n```\n\n```\n331 ms ± 2.29 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n```\n\nPolars:\n\n```{.python}\n%%timeit\n\ndf_pl = pl.scan_csv(\"/project/def-sponsor00/data/jeopardy.csv\")\ndf_pl.filter(pl.col(\"Category\") == \"HISTORY\").collect().shape\n```\n\n```\n13.1 ms ± 175 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n```\n\nThat's a speedup of 25 (the larger the file, the larger this speedup will be). This is because `pl.scan_csv` doesn't read the file. Instead, it creates a future. By using a lazy query, only the part of the file that is necessary actually gets read in. This potentially saves a lot of time for very large files and it even allows to work with files too large to fit in memory.\n\nLazy evaluation also allows the query optimizer to combine operations where possible, very much the way compiled languages work.\n\nTo evaluate the future and get a result, we use the `collect` method.\n\n:::{.note}\n\nNote that Polars also has a `pl.read_csv` function if you want to use eager evaluation.\n\n:::\n\nPolars: 2, Pandas: 0 🙂\n\n<!-- Comparisons between Polars and distributed (Dask, Ray, Spark) or GPU (RAPIDS) libraries aren't the most pertinent since they can be used in *combination with* Polars and the benefits can thus be combined. -->\n\n<!-- It makes most sense to compare Polars with another library occupying the same \"niche\" such as Pandas or Vaex. -->\n\n<!-- The net is full of benchmarks with consistent results: Polars is 3 to 150 times faster than Pandas. -->\n\n<!-- Pandas is trying to fight back: v 2.0 came with optional Arrow support instead of NumPy, then [it became the default engine](https://dataalgo.medium.com/pandas-2-0-ditches-numpy-for-pyarrow-what-you-need-to-know-cbba4cb60249), but performance remains way below that of Polars (e.g. in [DataCamp benchmarks](https://www.datacamp.com/tutorial/high-performance-data-manipulation-in-python-pandas2-vs-polars), [official benchmarks](https://pola.rs/posts/benchmarks/), many blog posts for [whole scripts](https://medium.com/@asimandia/benchmarking-performance-polars-vs-vaex-vs-pandas-f1c889dccc12) or [individual tasks](https://medium.com/cuenex/pandas-2-0-vs-polars-the-ultimate-battle-a378eb75d6d1)). -->\n\n<!-- As for Vaex, [it seems twice slower](https://medium.com/@asimandia/benchmarking-performance-polars-vs-vaex-vs-pandas-f1c889dccc12) and [development has stalled over the past 10 months](https://github.com/vaexio/vaex). -->\n\n<!-- The only framework performing better than Polars in some benchmarks is [datatable](https://github.com/h2oai/datatable) (derived from the R package [data.table](https://cran.r-project.org/web/packages/data.table/index.html)), but it hasn't been developed for 6 months—a sharp contrast with the fast development of Polars. -->\n\n",
    "supporting": [
      "polars_dataframes_files"
    ],
    "filters": [],
    "includes": {}
  }
}