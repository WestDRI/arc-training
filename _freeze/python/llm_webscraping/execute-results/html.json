{
  "hash": "88ee8a908debd7fafd8177fff28a3f23",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Webscraping with an LLM\nauthor: Marie-Hélène Burle\n---\n\n:::{.def}\n\nThe internet is a trove of information. A lot of it is publicly available and thus suitable for use in research. Extracting that information and putting it in an organized format for analysis can however be extremely tedious.\n\nSome websites have an API that makes it easy to extract information. These are websites that were built with the intention of being scraped (e.g. sites that contain databases, museums, art collections, etc.). When this is the case, this is definitely the way to go. Most websites however do not contain an API that can be queried.\n\nWeb scraping tools allow to automate parts of that process and Python is a popular language for the task.\n\nOf note, an increasing number of websites use JavaScript to add cookies, interactivity, etc. to websites. This makes them a lot harder to scrape and require more sophisticated tools.\n\nIn this section, we will scrape a simple site that does not contain any JavaScript using the package [Beautiful Soup](https://beautiful-soup-4.readthedocs.io/en/latest/).\n\nWe will use an LLM to help us in this process.\n\n:::\n\n## Background information\n\n### HTML and CSS\n\n[HyperText Markup Language](https://en.wikipedia.org/wiki/HTML) (HTML) is the standard markup language for websites: it encodes the information related to the formatting and structure of webpages. Additionally, some of the customization can be stored in [Cascading Style Sheets](https://en.wikipedia.org/wiki/CSS) (CSS) files.\n\nHTML uses tags of the form:\n\n```{.html}\n<some_tag>Your content</some_tag>\n```\n\nSome tags have attributes:\n\n```{.html}\n<some_tag attribute_name=\"attribute value\">Your content</some_tag>\n```\n\n:::{.example}\n\nExamples:\n\n:::\n\nSite structure:\n\n- `<h2>This is a heading of level 2</h2>`\n- `<p>This is a paragraph</p>`\n\nFormatting:\n\n- `<b>This is bold</b>`\n- `<a href=\"https://some.url\">This is the text for a link</a>`\n\n### Web scrapping\n\nWeb scraping is a general term for a set of tools which allow for the extraction of data from the web automatically.\n\nWhile most of the data on the internet is publicly available, it is illegal to scrape some sites and you should always look into the policy of a site before attempting to scrape it. Some sites will also block you if you submit too many requests in a short amount of time, so remember to scrape responsibly.\n\n## Webscraping example\n\nWe will use [a website](https://trace.tennessee.edu/utk_graddiss/index.html) from the [University of Tennessee](https://www.utk.edu/) containing a database of PhD theses from that university which uses the [Digital Commons Network](https://network.bepress.com/).\n\nOur goal is to scrape data from this site to produce a dataframe with the date, major, and advisor for each dissertation. Scraping any other site which uses the [Digital Commons Network](https://network.bepress.com/) can be done following a similar workflow.\n\n### Site exploration\n\nFirst of all, let's have a close look at the website we want to scrape to think carefully about what we want to do. Before starting to write code, it is always a good idea to think about what you are trying to achieve with your code.\n\nTo see how the website is built, you can open the web inspector. Most browsers allow to right-click on an element and select \"inspect\" in the menu. Alternatively, Chrome opens the web inspector with F12 and Firefox and Edge with Ctrl+Shift+I (Windows, Linux) or Command+Option+I (macOS).\n\nTo create a dataframe with the data for all the dissertations on the first page, we need to do two things:\n\n- Step 1: from the [dissertations database first page](https://trace.tennessee.edu/utk_graddiss/index.html), we want to scrape the list of URLs for the dissertation pages.\n\n- Step 2: once we have the URLs, we want to scrape those pages too to get the date, major, and advisor for each dissertation.\n\n:::{.note}\n\nNote that we are only scraping the first page (with the 100 most recent theses) to keep things simple. If you wanted to get all the data from the site, you would have to create a loop and repeat this over all 117 pages of the site.\n\n:::\n\n### LLM code\n\nLet's use an LLM to get us started.\n\nHere is the prompt I used:\n\n:::{.llm}\n\nWrite Python code to scrape the following website: https://trace.tennessee.edu/utk_graddiss/index.html.\n\nAs the end result, I want to have a polars dataframe with the following columns:\n\n- Date\n- Major of the thesis\n- Advisor (PI)\n\n:::\n\n::: {#67cdd041 .cell execution_count=2}\n``` {.python .cell-code}\nimport requests\nfrom bs4 import BeautifulSoup\nimport polars as pl\nfrom urllib.parse import urljoin\n\nbase_url = \"https://trace.tennessee.edu/utk_graddiss/index.html\"\n\ndef get_dissertation_data():\n\t# Get main page and extract dissertation URLs\n\tmain_page = requests.get(base_url)\n\tsoup = BeautifulSoup(main_page.content, \"html.parser\")\n\n\t# Extract individual dissertation links\n\tlinks = [urljoin(base_url, a[\"href\"])\n\t\t\t for a in soup.select(\".article-listing a\")]\n\n\tdata = []\n\n\tfor link in links[:10]:  # Reduced to 10 for testing; remove slice for full 100\n\t\ttry:\n\t\t\t# Get individual dissertation page\n\t\t\tpage = requests.get(link)\n\t\t\tpage_soup = BeautifulSoup(page.content, \"html.parser\")\n\n\t\t\t# Extract required fields\n\t\t\tdate = page_soup.select_one(\"#publication_date p\").text.strip()\n\t\t\tmajor = page_soup.select_one(\"#department p\").text.strip()\n\t\t\tadvisor = page_soup.select_one(\"#advisor1 p\").text.strip()\n\n\t\t\tdata.append({\n\t\t\t\t\"Date\": date,\n\t\t\t\t\"Major\": major,\n\t\t\t\t\"Advisor\": advisor\n\t\t\t})\n\n\t\texcept Exception as e:\n\t\t\tprint(f\"Error processing {link}: {str(e)}\")\n\n\treturn pl.DataFrame(data)\n\n# Usage\ndf = get_dissertation_data()\nprint(df)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nshape: (10, 3)\n┌─────────┬─────────────────────────────────┬────────────────────────┐\n│ Date    ┆ Major                           ┆ Advisor                │\n│ ---     ┆ ---                             ┆ ---                    │\n│ str     ┆ str                             ┆ str                    │\n╞═════════╪═════════════════════════════════╪════════════════════════╡\n│ 8-2024  ┆ History                         ┆ Jay Rubenstein         │\n│ 12-2024 ┆ Biochemistry and Cellular and … ┆ Dr. Rebecca A. Prosser │\n│ 5-2024  ┆ Business Administration         ┆ Larry A. Fauver        │\n│ 8-2024  ┆ Electrical Engineering          ┆ Dan Wilson             │\n│ 12-2024 ┆ Mechanical Engineering          ┆ Prashant Singh         │\n│ 5-2024  ┆ Chemical Engineering            ┆ Steven M. Abel         │\n│ 12-2024 ┆ Industrial Engineering          ┆ Anahita Khojandi       │\n│ 12-2024 ┆ Education                       ┆ Clara Lee Brown        │\n│ 8-2024  ┆ Geography                       ┆ Sally P Horn           │\n│ 12-2024 ┆ Education                       ┆ Enlida J Romero-Hall   │\n└─────────┴─────────────────────────────────┴────────────────────────┘\n```\n:::\n:::\n\n\n:::{.note}\n\nThe package [Beautiful Soup](https://beautiful-soup-4.readthedocs.io/en/latest/)—loaded in Python as `bs4`—transforms (parses) HTML data into a parse tree, which makes extracting information easier.\n\n:::\n\n:::{.exo}\n\n:::{.yourturn}\n\nYour turn:\n\n:::\n\n- Did it work? Go to the site and verify the data.\n\n- What is the problem? Can you fix it?\n\n:::\n\n### Code improvements\n\nLLMs can be very helpful in getting you started, but you will often have to tweak the code to improve it—even when it works.\n\nWe now have code that works, but its downside is that we created a function that can only work on one webpage... so it is of limited use. If, for instance, we wanted to apply that function to the second page of the site (<https://trace.tennessee.edu/utk_graddiss/index.2.html>), we can't because it doesn't accept any argument. The URL of the site is *inside* the function. This is called [hard coding](https://en.wikipedia.org/wiki/Hard_coding) and it isn't a good coding practice.\n\nA better approach would be to create a function accepting the URL of the page we want to scrape as argument. It is actually really easy to modify the code to get this:\n\n::: {#2dea7a8e .cell execution_count=3}\n``` {.python .cell-code}\ndef get_dissertation_data(base_url):\n    # Get main page and extract dissertation URLs\n    main_page = requests.get(base_url)\n    soup = BeautifulSoup(main_page.content, \"html.parser\")\n\n    # Extract individual dissertation links\n    links = [urljoin(base_url, a[\"href\"])\n             for a in soup.select(\".article-listing a\")]\n\n    data = []\n\n    for link in links:\n        try:\n            # Get individual dissertation page\n            page = requests.get(link)\n            page_soup = BeautifulSoup(page.content, \"html.parser\")\n\n            # Extract required fields\n            date = page_soup.select_one(\"#publication_date p\").text.strip()\n            major = page_soup.select_one(\"#department p\").text.strip()\n            advisor = page_soup.select_one(\"#advisor1 p\").text.strip()\n\n            data.append({\n                \"Date\": date,\n                \"Major\": major,\n                \"Advisor\": advisor\n            })\n\n        except Exception as e:\n            print(f\"Error processing {link}: {str(e)}\")\n\n    return pl.DataFrame(data)\n```\n:::\n\n\nNow, if we want to use the function on that first page, we need to pass the URL as an argument:\n\n::: {#d482832d .cell execution_count=4}\n``` {.python .cell-code}\ndf = get_dissertation_data(base_url)\n```\n:::\n\n\nYou can verify that the code still works:\n\n::: {#37a9fb04 .cell execution_count=5}\n``` {.python .cell-code}\nprint(df)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nshape: (100, 3)\n┌─────────┬─────────────────────────────────┬────────────────────────┐\n│ Date    ┆ Major                           ┆ Advisor                │\n│ ---     ┆ ---                             ┆ ---                    │\n│ str     ┆ str                             ┆ str                    │\n╞═════════╪═════════════════════════════════╪════════════════════════╡\n│ 8-2024  ┆ History                         ┆ Jay Rubenstein         │\n│ 12-2024 ┆ Biochemistry and Cellular and … ┆ Dr. Rebecca A. Prosser │\n│ 5-2024  ┆ Business Administration         ┆ Larry A. Fauver        │\n│ 8-2024  ┆ Electrical Engineering          ┆ Dan Wilson             │\n│ 12-2024 ┆ Mechanical Engineering          ┆ Prashant Singh         │\n│ …       ┆ …                               ┆ …                      │\n│ 5-2024  ┆ Mechanical Engineering          ┆ Feng-Yuan Zhang        │\n│ 8-2024  ┆ Computer Science                ┆ Scott Ruoti            │\n│ 8-2024  ┆ Mechanical Engineering          ┆ Tony L. Schmitz        │\n│ 8-2024  ┆ Microbiology                    ┆ Shigetoshi Eda         │\n│ 8-2024  ┆ Energy Science and Engineering  ┆ David C. Donovan       │\n└─────────┴─────────────────────────────────┴────────────────────────┘\n```\n:::\n:::\n\n\nThe code looks very similar, but it now allows us to scrape the data from any page of the website.\n\n:::{.exo}\n\n:::{.yourturn}\n\nYour turn:\n\n:::\n\nScrape the data from the sixth page (<https://trace.tennessee.edu/utk_graddiss/index.6.html>).\n\n:::\n\nAnother improvement that we can make to the code is to add a little delay between requests because some sites will block requests if they are too frequent.\n\nFor this we need to load the `time` module:\n\n::: {#91700d50 .cell execution_count=6}\n``` {.python .cell-code}\nimport time\n```\n:::\n\n\nThen add `time.sleep(0.1)` in the loop:\n\n```{.python}\ndef get_dissertation_data(base_url):\n    # Get main page and extract dissertation URLs\n    main_page = requests.get(base_url)\n    soup = BeautifulSoup(main_page.content, \"html.parser\")\n\n    # Extract individual dissertation links\n    links = [urljoin(base_url, a[\"href\"])\n             for a in soup.select(\".article-listing a\")]\n\n    data = []\n\n    for link in links:\n        try:\n            # Get individual dissertation page\n            page = requests.get(link)\n            page_soup = BeautifulSoup(page.content, \"html.parser\")\n\n            # Extract required fields\n            date = page_soup.select_one(\"#publication_date p\").text.strip()\n            major = page_soup.select_one(\"#department p\").text.strip()\n            advisor = page_soup.select_one(\"#advisor1 p\").text.strip()\n\n            data.append({\n                \"Date\": date,\n                \"Major\": major,\n                \"Advisor\": advisor\n\n            # Add 0.1 s between each request to the site\n            time.sleep(0.1)\n            })\n\n        except Exception as e:\n            print(f\"Error processing {link}: {str(e)}\")\n\n    return pl.DataFrame(data)\n```\n\n### Save data to file\n\nIf you want to export the data and save it to a CSV file, you can do this:\n\n```{.python}\ndf.write_csv(\"dissertations_data.csv\")\n```\n\n",
    "supporting": [
      "llm_webscraping_files"
    ],
    "filters": [],
    "includes": {}
  }
}