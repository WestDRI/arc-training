{
  "hash": "e68a0fc9dc8791e64c01633ab36739b2",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Webscraping with an LLM\nauthor: Marie-Hélène Burle\n---\n\n:::{.def}\n\nThe internet is a trove of information. A lot of it is publicly available and thus suitable for use in research. Extracting that information and putting it in an organized format for analysis can however be extremely tedious.\n\nSome websites have an API that makes it easy to extract information. These are websites that were built with the intention of being scraped (e.g. sites that contain databases, museums, art collections, etc.). When this is the case, this is definitely the way to go. Most websites however do not contain an API that can be queried.\n\nWeb scraping tools allow to automate parts of that process and Python is a popular language for the task.\n\nOf note, an increasing number of websites use JavaScript to add cookies, interactivity, etc. to websites. This makes them a lot harder to scrape and require more sophisticated tools.\n\nIn this section, we will scrape a simple site that does not contain any JavaScript using the package [Beautiful Soup](https://beautiful-soup-4.readthedocs.io/en/latest/).\n\nWe will use an LLM to help us in this process.\n\n:::\n\n## Background information\n\n### HTML and CSS\n\n[HyperText Markup Language](https://en.wikipedia.org/wiki/HTML) (HTML) is the standard markup language for websites: it encodes the information related to the formatting and structure of webpages. Additionally, some of the customization can be stored in [Cascading Style Sheets](https://en.wikipedia.org/wiki/CSS) (CSS) files.\n\nHTML uses tags of the form:\n\n```{.html}\n<some_tag>Your content</some_tag>\n```\n\nSome tags have attributes:\n\n```{.html}\n<some_tag attribute_name=\"attribute value\">Your content</some_tag>\n```\n\n:::{.example}\n\nExamples:\n\n:::\n\nSite structure:\n\n- `<h2>This is a heading of level 2</h2>`\n- `<p>This is a paragraph</p>`\n\nFormatting:\n\n- `<b>This is bold</b>`\n- `<a href=\"https://some.url\">This is the text for a link</a>`\n\n### Web scrapping\n\nWeb scraping is a general term for a set of tools which allow for the extraction of data from the web automatically.\n\nWhile most of the data on the internet is publicly available, it is illegal to scrape some sites and you should always look into the policy of a site before attempting to scrape it. Some sites will also block you if you submit too many requests in a short amount of time, so remember to scrape responsibly.\n\n## Example for this workshop\n\nWe will use [a website](https://trace.tennessee.edu/utk_graddiss/index.html) from the [University of Tennessee](https://www.utk.edu/) containing a database of PhD theses from that university.\n\nOur goal is to scrape data from this site to produce a dataframe with the date, major, and advisor for each dissertation.\n\n### Let's look at the site\n\nFirst of all, let's have a close look at the website we want to scrape to think carefully about what we want to do. Before starting to write code, it is always a good idea to think about what you are trying to achieve with your code.\n\nTo create a dataframe with the data for all the dissertations on that first page, we need to do two things:\n\n- Step 1: from the [dissertations database first page](https://trace.tennessee.edu/utk_graddiss/index.html), we want to scrape the list of URLs for the dissertation pages.\n\n- Step 2: once we have the URLs, we want to scrape those pages too to get the date, major, and advisor for each dissertation.\n\n### Let's try to scrape the data\n\n:::{.llm}\n\nWrite Python code to scrape the following website: https://trace.tennessee.edu/utk_graddiss/index.html.\n\n:::\n\n::: {#1e53bfa4 .cell execution_count=2}\n``` {.python .cell-code}\nimport requests\nfrom bs4 import BeautifulSoup\nimport polars as pl\nfrom urllib.parse import urljoin\n\nBASE_URL = \"https://trace.tennessee.edu/utk_graddiss/index.html\"\n\ndef get_dissertation_data():\n\t# Get main page and extract dissertation URLs\n\tmain_page = requests.get(BASE_URL)\n\tsoup = BeautifulSoup(main_page.content, \"html.parser\")\n\n\t# Extract individual dissertation links\n\tlinks = [urljoin(BASE_URL, a[\"href\"])\n\t\t\t for a in soup.select(\".article-listing a\")]\n\n\tdata = []\n\n\tfor link in links[:10]:  # Reduced to 10 for testing; remove slice for full 100\n\t\ttry:\n\t\t\t# Get individual dissertation page\n\t\t\tpage = requests.get(link)\n\t\t\tpage_soup = BeautifulSoup(page.content, \"html.parser\")\n\n\t\t\t# Extract required fields\n\t\t\tdate = page_soup.select_one(\"#publication_date p\").text.strip()\n\t\t\tmajor = page_soup.select_one(\"#department p\").text.strip()\n\t\t\tadvisor = page_soup.select_one(\"#advisor1 p\").text.strip()\n\n\t\t\tdata.append({\n\t\t\t\t\"Date\": date,\n\t\t\t\t\"Major\": major,\n\t\t\t\t\"Advisor\": advisor\n\t\t\t})\n\n\t\texcept Exception as e:\n\t\t\tprint(f\"Error processing {link}: {str(e)}\")\n\n\treturn pl.DataFrame(data)\n\n# Usage\ndf = get_dissertation_data()\nprint(df)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nshape: (10, 3)\n┌─────────┬─────────────────────────────────┬────────────────────────┐\n│ Date    ┆ Major                           ┆ Advisor                │\n│ ---     ┆ ---                             ┆ ---                    │\n│ str     ┆ str                             ┆ str                    │\n╞═════════╪═════════════════════════════════╪════════════════════════╡\n│ 8-2024  ┆ History                         ┆ Jay Rubenstein         │\n│ 12-2024 ┆ Biochemistry and Cellular and … ┆ Dr. Rebecca A. Prosser │\n│ 5-2024  ┆ Business Administration         ┆ Larry A. Fauver        │\n│ 8-2024  ┆ Electrical Engineering          ┆ Dan Wilson             │\n│ 12-2024 ┆ Mechanical Engineering          ┆ Prashant Singh         │\n│ 5-2024  ┆ Chemical Engineering            ┆ Steven M. Abel         │\n│ 12-2024 ┆ Industrial Engineering          ┆ Anahita Khojandi       │\n│ 12-2024 ┆ Education                       ┆ Clara Lee Brown        │\n│ 8-2024  ┆ Geography                       ┆ Sally P Horn           │\n│ 12-2024 ┆ Education                       ┆ Enlida J Romero-Hall   │\n└─────────┴─────────────────────────────────┴────────────────────────┘\n```\n:::\n:::\n\n\n:::{.exo}\n\n:::{.yourturn}\n\nYour turn:\n\n:::\n\n- Did it work? Go to the site and verify the data.\n\n- What is the problem?\n\n:::\n\n:::{.llm}\n\nThis works for the first 10 dissertations, but I want all of them.\n\n:::\n\n::: {#d51830d6 .cell execution_count=3}\n``` {.python .cell-code}\nimport requests\nfrom bs4 import BeautifulSoup\nimport polars as pl\nfrom urllib.parse import urljoin\nimport time\n\nBASE_URL = \"https://trace.tennessee.edu/utk_graddiss/\"\n\ndef get_all_dissertation_links():\n\t\"\"\"Handles pagination to collect all dissertation links\"\"\"\n\tall_links = []\n\tpage_num = 1\n\n\twhile True:\n\t\tpage_url = f\"{BASE_URL}index{page_num}.html\" if page_num > 1 else BASE_URL\n\t\ttry:\n\t\t\tresponse = requests.get(page_url)\n\t\t\tresponse.raise_for_status()\n\n\t\t\tsoup = BeautifulSoup(response.content, \"html.parser\")\n\t\t\tlinks = [urljoin(BASE_URL, a[\"href\"])\n\t\t\t\t\tfor a in soup.select(\".article-listing a\")]\n\n\t\t\tif not links:\n\t\t\t\tbreak\n\n\t\t\tall_links.extend(links)\n\t\t\tpage_num += 1\n\t\t\ttime.sleep(1)  # Be polite to the server\n\n\t\texcept Exception as e:\n\t\t\tprint(f\"Stopped at page {page_num}: {str(e)}\")\n\t\t\tbreak\n\n\treturn all_links\n\ndef get_dissertation_data(link):\n\t\"\"\"Scrapes individual dissertation page with error handling\"\"\"\n\ttry:\n\t\tpage = requests.get(link, timeout=10)\n\t\tpage.raise_for_status()\n\t\tsoup = BeautifulSoup(page.content, \"html.parser\")\n\n\t\treturn {\n\t\t\t\"Date\": soup.select_one(\"#publication_date p\").text.strip(),\n\t\t\t\"Major\": soup.select_one(\"#department p\").text.strip(),\n\t\t\t\"Advisor\": soup.select_one(\"#advisor1 p\").text.strip(),\n\t\t\t\"URL\": link\n\t\t}\n\n\texcept Exception as e:\n\t\tprint(f\"Failed on {link}: {str(e)}\")\n\t\treturn None\n\ndef build_full_dataset():\n\t\"\"\"Main function to create complete dataset\"\"\"\n\tlinks = get_all_dissertation_links()\n\tprint(f\"Found {len(links)} dissertations to process\")\n\n\tdata = []\n\tfor link in links:\n\t\tresult = get_dissertation_data(link)\n\t\tif result:\n\t\t\tdata.append(result)\n\t\ttime.sleep(0.5)  # Throttle requests\n\n\treturn pl.DataFrame(data).unique()  # Remove duplicates\n\n# Usage\nfull_df = build_full_dataset()\nprint(f\"Final dataset contains {len(full_df)} records\")\nfull_df.head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nStopped at page 2: 404 Client Error: Not Found for url: https://trace.tennessee.edu/utk_graddiss/index2.html\nFound 100 dissertations to process\nFinal dataset contains 100 records\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=2}\n```{=html}\n<div><style>\n.dataframe > thead > tr,\n.dataframe > tbody > tr {\n  text-align: right;\n  white-space: pre-wrap;\n}\n</style>\n<small>shape: (5, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>Date</th><th>Major</th><th>Advisor</th><th>URL</th></tr><tr><td>str</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;5-2024&quot;</td><td>&quot;Business Administration&quot;</td><td>&quot;Matthew Serfling&quot;</td><td>&quot;https://trace.tennessee.edu/ut…</td></tr><tr><td>&quot;8-2024&quot;</td><td>&quot;History&quot;</td><td>&quot;Jay Rubenstein&quot;</td><td>&quot;https://trace.tennessee.edu/ut…</td></tr><tr><td>&quot;8-2024&quot;</td><td>&quot;Educational Psychology and Res…</td><td>&quot;Mitsunori Misawa&quot;</td><td>&quot;https://trace.tennessee.edu/ut…</td></tr><tr><td>&quot;8-2024&quot;</td><td>&quot;Physics&quot;</td><td>&quot;George Siopsis&quot;</td><td>&quot;https://trace.tennessee.edu/ut…</td></tr><tr><td>&quot;5-2024&quot;</td><td>&quot;Industrial Engineering&quot;</td><td>&quot;Hoon Hwangbo&quot;</td><td>&quot;https://trace.tennessee.edu/ut…</td></tr></tbody></table></div>\n```\n:::\n:::\n\n\nHere, things fall apart. The first code worked, but didn't scrape all the data. This one is just a mess.\n\nSo let's go back to the first code and try to work through it. As you can see LLMs are impressive for easy tasks, but far from perfect at this point when things become more elaborate. But they can still truly help you get there.\n\n> # Reduced to 10 for testing; remove slice for full 100\n\n\"What is a slice in Python?\"\n\"How do I remove the slice in the following Python code: `for link in links[:10]:`\"\n\n::: {#13fef9fe .cell execution_count=4}\n``` {.python .cell-code}\nimport requests\nfrom bs4 import BeautifulSoup\nimport polars as pl\nfrom urllib.parse import urljoin\n\nBASE_URL = \"https://trace.tennessee.edu/utk_graddiss/index.html\"\n\ndef get_dissertation_data():\n\t# Get main page and extract dissertation URLs\n\tmain_page = requests.get(BASE_URL)\n\tsoup = BeautifulSoup(main_page.content, \"html.parser\")\n\n\t# Extract individual dissertation links\n\tlinks = [urljoin(BASE_URL, a[\"href\"])\n\t\t\t for a in soup.select(\".article-listing a\")]\n\n\tdata = []\n\n\tfor link in links:\n\t\ttry:\n\t\t\t# Get individual dissertation page\n\t\t\tpage = requests.get(link)\n\t\t\tpage_soup = BeautifulSoup(page.content, \"html.parser\")\n\n\t\t\t# Extract required fields\n\t\t\tdate = page_soup.select_one(\"#publication_date p\").text.strip()\n\t\t\tmajor = page_soup.select_one(\"#department p\").text.strip()\n\t\t\tadvisor = page_soup.select_one(\"#advisor1 p\").text.strip()\n\n\t\t\tdata.append({\n\t\t\t\t\"Date\": date,\n\t\t\t\t\"Major\": major,\n\t\t\t\t\"Advisor\": advisor\n\t\t\t})\n\n\t\texcept Exception as e:\n\t\t\tprint(f\"Error processing {link}: {str(e)}\")\n\n\treturn pl.DataFrame(data)\n\n# Usage\ndf = get_dissertation_data()\nprint(df)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nshape: (100, 3)\n┌─────────┬─────────────────────────────────┬────────────────────────┐\n│ Date    ┆ Major                           ┆ Advisor                │\n│ ---     ┆ ---                             ┆ ---                    │\n│ str     ┆ str                             ┆ str                    │\n╞═════════╪═════════════════════════════════╪════════════════════════╡\n│ 8-2024  ┆ History                         ┆ Jay Rubenstein         │\n│ 12-2024 ┆ Biochemistry and Cellular and … ┆ Dr. Rebecca A. Prosser │\n│ 5-2024  ┆ Business Administration         ┆ Larry A. Fauver        │\n│ 8-2024  ┆ Electrical Engineering          ┆ Dan Wilson             │\n│ 12-2024 ┆ Mechanical Engineering          ┆ Prashant Singh         │\n│ …       ┆ …                               ┆ …                      │\n│ 5-2024  ┆ Mechanical Engineering          ┆ Feng-Yuan Zhang        │\n│ 8-2024  ┆ Computer Science                ┆ Scott Ruoti            │\n│ 8-2024  ┆ Mechanical Engineering          ┆ Tony L. Schmitz        │\n│ 8-2024  ┆ Microbiology                    ┆ Shigetoshi Eda         │\n│ 8-2024  ┆ Energy Science and Engineering  ┆ David C. Donovan       │\n└─────────┴─────────────────────────────────┴────────────────────────┘\n```\n:::\n:::\n\n\n::: {#ee666101 .cell execution_count=5}\n``` {.python .cell-code}\nimport requests\nfrom bs4 import BeautifulSoup\nimport polars as pl\nfrom urllib.parse import urljoin\nimport time\n\nBASE_URL = \"https://trace.tennessee.edu/utk_graddiss/\"\nFIRST_PAGE = BASE_URL + \"index.html\"\n\ndef get_all_dissertation_links():\n\tlinks = []\n\tpage_num = 1\n\n\twhile True:\n\t\tif page_num == 1:\n\t\t\tpage_url = FIRST_PAGE\n\t\telse:\n\t\t\tpage_url = f\"{BASE_URL}index{page_num}.html\"\n\n\t\tprint(f\"Scraping: {page_url}\")\n\t\tresponse = requests.get(page_url)\n\t\tif response.status_code != 200:\n\t\t\tbreak  # No more pages\n\n\t\tsoup = BeautifulSoup(response.content, \"html.parser\")\n\t\tpage_links = [urljoin(page_url, a[\"href\"])\n\t\t\t\t\t  for a in soup.select(\".article-listing a\")]\n\n\t\tif not page_links:\n\t\t\tbreak  # No more dissertation links found\n\n\t\tlinks.extend(page_links)\n\t\tpage_num += 1\n\t\ttime.sleep(0.5)  # Be polite to the server\n\n\treturn links\n\ndef get_dissertation_data():\n\tlinks = get_all_dissertation_links()\n\tdata = []\n\n\tfor link in links:\n\t\ttry:\n\t\t\tpage = requests.get(link)\n\t\t\tpage_soup = BeautifulSoup(page.content, \"html.parser\")\n\n\t\t\tdate = page_soup.select_one(\"#publication_date p\")\n\t\t\tmajor = page_soup.select_one(\"#department p\")\n\t\t\tadvisor = page_soup.select_one(\"#advisor1 p\")\n\n\t\t\t# Handle missing fields gracefully\n\t\t\tdate = date.text.strip() if date else \"\"\n\t\t\tmajor = major.text.strip() if major else \"\"\n\t\t\tadvisor = advisor.text.strip() if advisor else \"\"\n\n\t\t\tdata.append({\n\t\t\t\t\"Date\": date,\n\t\t\t\t\"Major\": major,\n\t\t\t\t\"Advisor\": advisor\n\t\t\t})\n\n\t\texcept Exception as e:\n\t\t\tprint(f\"Error processing {link}: {str(e)}\")\n\t\ttime.sleep(0.2)  # Be polite to the server\n\n\treturn pl.DataFrame(data)\n\n# Usage\ndf = get_dissertation_data()\nprint(df)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nScraping: https://trace.tennessee.edu/utk_graddiss/index.html\nScraping: https://trace.tennessee.edu/utk_graddiss/index2.html\nshape: (100, 3)\n┌─────────┬─────────────────────────────────┬────────────────────────┐\n│ Date    ┆ Major                           ┆ Advisor                │\n│ ---     ┆ ---                             ┆ ---                    │\n│ str     ┆ str                             ┆ str                    │\n╞═════════╪═════════════════════════════════╪════════════════════════╡\n│ 8-2024  ┆ History                         ┆ Jay Rubenstein         │\n│ 12-2024 ┆ Biochemistry and Cellular and … ┆ Dr. Rebecca A. Prosser │\n│ 5-2024  ┆ Business Administration         ┆ Larry A. Fauver        │\n│ 8-2024  ┆ Electrical Engineering          ┆ Dan Wilson             │\n│ 12-2024 ┆ Mechanical Engineering          ┆ Prashant Singh         │\n│ …       ┆ …                               ┆ …                      │\n│ 5-2024  ┆ Mechanical Engineering          ┆ Feng-Yuan Zhang        │\n│ 8-2024  ┆ Computer Science                ┆ Scott Ruoti            │\n│ 8-2024  ┆ Mechanical Engineering          ┆ Tony L. Schmitz        │\n│ 8-2024  ┆ Microbiology                    ┆ Shigetoshi Eda         │\n│ 8-2024  ┆ Energy Science and Engineering  ┆ David C. Donovan       │\n└─────────┴─────────────────────────────────┴────────────────────────┘\n```\n:::\n:::\n\n\n",
    "supporting": [
      "llm_webscraping_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}