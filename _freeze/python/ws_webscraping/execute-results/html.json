{
  "hash": "980107263baf4663be93c3a5699383b5",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Web scraping with Python\naliases:\n  - webscraping.html\nauthor: Marie-Hélène Burle\n---\n\n:::{.def}\n\nThe internet is a trove of information. A lot of it is publicly available and thus suitable for use in research. Extracting that information and putting it in an organized format for analysis can however be extremely tedious.\n\nWeb scraping tools allow to automate parts of that process and Python is a popular language for the task.\n\nIn this workshop, I will guide you through a simple example using the package [Beautiful Soup](https://beautiful-soup-4.readthedocs.io/en/latest/).\n\n:::\n\n## HTML and CSS\n\n[HyperText Markup Language](https://en.wikipedia.org/wiki/HTML) (HTML) is the standard markup language for websites: it encodes the information related to the formatting and structure of webpages. Additionally, some of the customization can be stored in [Cascading Style Sheets](https://en.wikipedia.org/wiki/CSS) (CSS) files.\n\nHTML uses tags of the form:\n\n```{.html}\n<some_tag>Your content</some_tag>\n```\n\nSome tags have attributes:\n\n```{.html}\n<some_tag attribute_name=\"attribute value\">Your content</some_tag>\n```\n\n:::{.example}\n\nExamples:\n\n:::\n\nSite structure:\n\n- `<h2>This is a heading of level 2</h2>`\n- `<p>This is a paragraph</p>`\n\nFormatting:\n\n- `<b>This is bold</b>`\n- `<a href=\"https://some.url\">This is the text for a link</a>`\n\n## Web scrapping\n\nWeb scraping is a general term for a set of tools which allow for the extraction of data from the web automatically.\n\nWhile most of the data on the internet is publicly available, it is illegal to scrape some sites and you should always look into the policy of a site before attempting to scrape it. Some sites will also block you if you submit too many requests in a short amount of time, so remember to scrape responsibly.\n\n## Example for this workshop\n\nWe will use [a website](https://trace.tennessee.edu/utk_graddiss/index.html) from the [University of Tennessee](https://www.utk.edu/) containing a database of PhD theses from that university.\n\nOur goal is to scrape data from this site to produce a dataframe with the date, major, and advisor for each dissertation.\n\n:::{.note}\n\nWe will only do this for the first page which contains the links to the 100 most recent theses. If you really wanted to gather all the data, you would have to do this for all pages.\n\n:::\n\n## Let's look at the sites\n\nFirst of all, let's have a close look at the websites we want to scrape to think carefully about what we want to do. Before starting to write code, it is always a good idea to think about what you are trying to achieve with your code.\n\nTo create a dataframe with the data for all the dissertations on that first page, we need to do two things:\n\n- Step 1: from the [dissertations database first page](https://trace.tennessee.edu/utk_graddiss/index.html), we want to scrape the list of URLs for the dissertation pages.\n\n- Step 2: once we have the URLs, we want to scrape those pages too to get the date, major, and advisor for each dissertation.\n\n## Load packages\n\nLet's load the packages that will make scraping websites with Python easier:\n\n::: {#8733e27a .cell execution_count=2}\n``` {.python .cell-code}\nimport requests                 # To download the html data from a site\nfrom bs4 import BeautifulSoup   # To parse the html data\nimport time                     # To add a delay between each requests\nimport pandas as pd             # To store our data in a DataFrame\n```\n:::\n\n\n## Send request to the main site\n\nAs mentioned above, our site is the [database of PhD dissertations from the University of Tennessee](https://trace.tennessee.edu/utk_graddiss/index.html).\n\nLet's create a string with the URL:\n\n::: {#b6d171aa .cell execution_count=3}\n``` {.python .cell-code}\nurl = \"https://trace.tennessee.edu/utk_graddiss/index.html\"\n```\n:::\n\n\nFirst, we send a request to that URL and save the response in a variable called `r`:\n\n::: {#df614f75 .cell execution_count=4}\n``` {.python .cell-code}\nr = requests.get(url)\n```\n:::\n\n\nLet's see what our response looks like:\n\n::: {#346c8446 .cell execution_count=5}\n``` {.python .cell-code}\nr\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\n<Response [200]>\n```\n:::\n:::\n\n\nIf you look in the [list of HTTP status codes](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes), you can see that a response with a code of `200` means that the request was successful.\n\n## Explore the raw data\n\nTo get the actual content of the response as unicode (text), we can use the `text` property of the response. This will give us the raw HTML markup from the webpage.\n\nLet's print the first 200 characters:\n\n::: {#026c5f51 .cell execution_count=6}\n``` {.python .cell-code}\nprint(r.text[:200])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n<!DOCTYPE html>\n<html lang=\"en\">\n<head><!-- inj yui3-seed: --><script type='text/javascript' src='//cdnjs.cloudflare.com/ajax/libs/yui/3.6.0/yui/yui-min.js'></script><script type='text/javascript' sr\n```\n:::\n:::\n\n\n## Parse the data\n\nThe package [Beautiful Soup](https://beautiful-soup-4.readthedocs.io/en/latest/) transforms (parses) such HTML data into a parse tree, which will make extracting information easier.\n\nLet's create an object called `mainpage` with the parse tree:\n\n::: {#aed1f9f0 .cell execution_count=7}\n``` {.python .cell-code}\nmainpage = BeautifulSoup(r.text, \"html.parser\")\n```\n:::\n\n\n:::{.note}\n\n`html.parser` is the name of the parser that we are using here. It is better to use a specific parser to get consistent results across environments.\n\n:::\n\nWe can print the beginning of the parsed result:\n\n::: {#82cf6c10 .cell execution_count=8}\n``` {.python .cell-code}\nprint(mainpage.prettify()[:200])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<!DOCTYPE html>\n<html lang=\"en\">\n <head>\n  <!-- inj yui3-seed: -->\n  <script src=\"//cdnjs.cloudflare.com/ajax/libs/yui/3.6.0/yui/yui-min.js\" type=\"text/javascript\">\n  </script>\n  <script src=\"//ajax.g\n```\n:::\n:::\n\n\n:::{.note}\n\nThe `prettify` method turns the BeautifulSoup object we created into a string (which is needed for slicing).\n\n:::\n\nIt doesn't look any more clear to us, but it is now in a format the Beautiful Soup package can work with.\n\nFor instance, we can get the HTML segment containing the title with three methods:\n\n- using the title tag name:\n\n::: {#c1ee27bb .cell execution_count=9}\n``` {.python .cell-code}\nmainpage.title\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```\n<title>\nDoctoral Dissertations | Graduate School | University of Tennessee, Knoxville\n</title>\n```\n:::\n:::\n\n\n- using `find` to look for HTML markers (tags, attributes, etc.):\n\n::: {#13d076df .cell execution_count=10}\n``` {.python .cell-code}\nmainpage.find(\"title\")\n```\n\n::: {.cell-output .cell-output-display execution_count=9}\n```\n<title>\nDoctoral Dissertations | Graduate School | University of Tennessee, Knoxville\n</title>\n```\n:::\n:::\n\n\n- using `select` which accepts CSS selectors:\n\n::: {#33a59e07 .cell execution_count=11}\n``` {.python .cell-code}\nmainpage.select(\"title\")\n```\n\n::: {.cell-output .cell-output-display execution_count=10}\n```\n[<title>\n Doctoral Dissertations | Graduate School | University of Tennessee, Knoxville\n </title>]\n```\n:::\n:::\n\n\n`find` will only return the first element. `find_all` will return all elements. `select` will also return all elements. Which one you chose depends on what you need to extract. There often several ways to get you there.\n\nHere are other examples of data extraction:\n\n::: {#ee2c7d03 .cell execution_count=12}\n``` {.python .cell-code}\nmainpage.head\n```\n\n::: {.cell-output .cell-output-display execution_count=11}\n```\n<head><!-- inj yui3-seed: --><script src=\"//cdnjs.cloudflare.com/ajax/libs/yui/3.6.0/yui/yui-min.js\" type=\"text/javascript\"></script><script src=\"//ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js\" type=\"text/javascript\"></script><!-- Adobe Analytics --><script src=\"https://assets.adobedtm.com/4a848ae9611a/d0e96722185b/launch-d525bb0064d8.min.js\" type=\"text/javascript\"></script><!-- Cookies --><link href=\"//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.0.3/cookieconsent.min.css\" rel=\"stylesheet\" type=\"text/css\"/><script src=\"//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.0.3/cookieconsent.min.js\" type=\"text/javascript\"></script><script src=\"/assets/nr_browser_production.js\" type=\"text/javascript\"></script>\n<!-- def.1 -->\n<meta charset=\"utf-8\"/>\n<meta content=\"width=device-width\" name=\"viewport\"/>\n<title>\nDoctoral Dissertations | Graduate School | University of Tennessee, Knoxville\n</title>\n<!-- FILE meta-tags.inc --><!-- FILE: /srv/sequoia/main/data/assets/site/meta-tags.inc -->\n<!-- FILE: meta-tags.inc (cont) -->\n<!-- sh.1 -->\n<link href=\"/ir-style.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/>\n<link href=\"/ir-custom.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/>\n<link href=\"ir-custom.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/>\n<link href=\"/ir-local.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/>\n<link href=\"ir-local.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/>\n<link href=\"/ir-print.css\" media=\"print\" rel=\"stylesheet\" type=\"text/css\"/>\n<link href=\"/assets/floatbox/floatbox.css\" rel=\"stylesheet\" type=\"text/css\"/>\n<link href=\"/recent.rss\" rel=\"alternate\" title=\"Site Feed\" type=\"application/rss+xml\"/>\n<link href=\"/favicon.ico\" rel=\"shortcut icon\" type=\"image/x-icon\"/>\n<!--[if IE]>\n<link rel=\"stylesheet\" href=\"/ir-ie.css\" type=\"text/css\" media=\"screen\">\n<![endif]-->\n<!-- JS -->\n<script src=\"/assets/jsUtilities.js\" type=\"text/javascript\"></script>\n<script src=\"/assets/footnoteLinks.js\" type=\"text/javascript\"></script>\n<script src=\"/assets/scripts/yui-init.pack.js\" type=\"text/javascript\"></script>\n<script src=\"/assets/scripts/bepress-init.pack.js\" type=\"text/javascript\"></script>\n<script src=\"/assets/scripts/JumpListYUI.pack.js\" type=\"text/javascript\"></script>\n<!-- end sh.1 -->\n<script type=\"text/javascript\">var pageData = {\"page\":{\"environment\":\"prod\",\"productName\":\"bpdg\",\"language\":\"en\",\"name\":\"ir_etd\",\"businessUnit\":\"els:rp:st\"},\"visitor\":{}};</script>\n</head>\n```\n:::\n:::\n\n\n::: {#2d43056a .cell execution_count=13}\n``` {.python .cell-code}\nmainpage.a\n```\n\n::: {.cell-output .cell-output-display execution_count=12}\n```\n<a data-scroll=\"\" href=\"https://trace.tennessee.edu\" title=\"Home\">Home</a>\n```\n:::\n:::\n\n\n::: {#1311180d .cell execution_count=14}\n``` {.python .cell-code}\nmainpage.find_all(\"a\")[:5]\n```\n\n::: {.cell-output .cell-output-display execution_count=13}\n```\n[<a data-scroll=\"\" href=\"https://trace.tennessee.edu\" title=\"Home\">Home</a>,\n <a data-scroll=\"\" href=\"https://trace.tennessee.edu/do/search/advanced/\" title=\"Search\"><i class=\"icon-search\"></i> Search</a>,\n <a data-scroll=\"\" href=\"https://trace.tennessee.edu/communities.html\" title=\"Browse\">Browse Collections</a>,\n <a data-scroll=\"\" href=\"/cgi/myaccount.cgi?context=utk_graddiss\" title=\"My Account\">My Account</a>,\n <a data-scroll=\"\" href=\"https://trace.tennessee.edu/about.html\" title=\"About\">About</a>]\n```\n:::\n:::\n\n\n::: {#4c49d482 .cell execution_count=15}\n``` {.python .cell-code}\nmainpage.select(\"a\")[:5]\n```\n\n::: {.cell-output .cell-output-display execution_count=14}\n```\n[<a data-scroll=\"\" href=\"https://trace.tennessee.edu\" title=\"Home\">Home</a>,\n <a data-scroll=\"\" href=\"https://trace.tennessee.edu/do/search/advanced/\" title=\"Search\"><i class=\"icon-search\"></i> Search</a>,\n <a data-scroll=\"\" href=\"https://trace.tennessee.edu/communities.html\" title=\"Browse\">Browse Collections</a>,\n <a data-scroll=\"\" href=\"/cgi/myaccount.cgi?context=utk_graddiss\" title=\"My Account\">My Account</a>,\n <a data-scroll=\"\" href=\"https://trace.tennessee.edu/about.html\" title=\"About\">About</a>]\n```\n:::\n:::\n\n\n## Test run\n\n### Identify relevant markers\n\nThe html code for this webpage contains the data we are interested in, but it is mixed in with a lot of HTML formatting and data we don't care about. We need to extract the data relevant to us and turn it into a workable format.\n\nThe first step is to find the HTML markers that contain our data. One option is to use a web inspector or—even easier—the [SelectorGadget](https://selectorgadget.com/), a JavaScript bookmarklet built by [Andrew Cantino](https://andrewcantino.com/).\n\nTo use this tool, go to the [SelectorGadget](https://selectorgadget.com/) website and drag the link of the bookmarklet to your bookmarks bar.\n\nNow, go to the [dissertations database first page](https://trace.tennessee.edu/utk_graddiss/index.html) and click on the bookmarklet in your bookmarks bar. You will see a floating box at the bottom of your screen. As you move your mouse across the screen, an orange rectangle appears around each element over which you pass.\n\nClick on one of the dissertation links: now, there is an `a` appearing in the box at the bottom as well as the number of elements selected. The selected elements are highlighted in yellow. Those elements are links (in HTML, `a` tags define hyperlinks).\n\nAs you can see, all the links we want are selected. However, there are many other links we don't want that are also highlighted. In fact, *all* links in the document are selected. We need to remove the categories of links that we don't want. To do this, hover above any of the links we don't want. You will see a red rectangle around it. Click on it: now all similar links are gone. You might have to do this a few times until only the relevant links (i.e. those that lead to the dissertation information pages) remain highlighted.\n\nAs there are 100 such links per page, the count of selected elements in the bottom floating box should be down to 100.\n\nIn the main section of the floating box, you can now see: `.article-listing a`. This means that the data we want are under the HTML elements `.article-listing a` (the class `.article-listing` and the tag `a`).\n\n### Extract test URL\n\nIt is a good idea to test things out on a single element before doing a massive batch scraping of a site, so let's test our method for the first dissertation.\n\nTo start, we need to extract the first URL. Here, we will use the CSS selectors (we can get there using `find` too). `mainpage.select(\".article-listing a\")` would give us all the results (100 links):\n\n::: {#07a9ffaf .cell execution_count=16}\n``` {.python .cell-code}\nlen(mainpage.select(\".article-listing a\"))\n```\n\n::: {.cell-output .cell-output-display execution_count=15}\n```\n100\n```\n:::\n:::\n\n\nTo get the first one, we index it:\n\n::: {#8aa22874 .cell execution_count=17}\n``` {.python .cell-code}\nmainpage.select(\".article-listing a\")[0]\n```\n\n::: {.cell-output .cell-output-display execution_count=16}\n```\n<a href=\"https://trace.tennessee.edu/utk_graddiss/8076\">Understanding host-microbe interactions in maize kernel and sweetpotato leaf metagenomic profiles.</a>\n```\n:::\n:::\n\n\nThe actual URL is contained in the `href` attribute. Attributes can be extracted with the `get` method:\n\n::: {#f4963938 .cell execution_count=18}\n``` {.python .cell-code}\nmainpage.select(\".article-listing a\")[0].get(\"href\")\n```\n\n::: {.cell-output .cell-output-display execution_count=17}\n```\n'https://trace.tennessee.edu/utk_graddiss/8076'\n```\n:::\n:::\n\n\nWe now have our URL as a string. We can double-check that it is indeed a string:\n\n::: {#e5a59641 .cell execution_count=19}\n``` {.python .cell-code}\ntype(mainpage.select(\".article-listing a\")[0].get(\"href\"))\n```\n\n::: {.cell-output .cell-output-display execution_count=18}\n```\nstr\n```\n:::\n:::\n\n\nThis is exactly what we need to send a request to that site, so let's create an object `url_test` with it:\n\n::: {#ae729b15 .cell execution_count=20}\n``` {.python .cell-code}\nurl_test = mainpage.select(\".article-listing a\")[0].get(\"href\")\n```\n:::\n\n\nWe have our first thesis URL:\n\n::: {#1ca32675 .cell execution_count=21}\n``` {.python .cell-code}\nprint(url_test)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nhttps://trace.tennessee.edu/utk_graddiss/8076\n```\n:::\n:::\n\n\n### Send request to test URL\n\nNow that we have the URL for the first dissertation information page, we want to extract the date, major, and advisor for that dissertation.\n\nThe first thing to do—as we did earlier with the database site—is to send a request to that page. Let's assign it to a new object that we will call `r_test`:\n\n::: {#ce9f18cd .cell execution_count=22}\n``` {.python .cell-code}\nr_test = requests.get(url_test)\n```\n:::\n\n\nThen we can parse it with Beautiful Soup (as we did before). Let's create a `dissertpage_test` object:\n\n::: {#b367a153 .cell execution_count=23}\n``` {.python .cell-code}\ndissertpage_test = BeautifulSoup(r_test.text, \"html.parser\")\n```\n:::\n\n\n### Get data for test URL\n\nIt is time to extract the publication date, major, and advisor for our test URL.\n\nLet's start with the date. Thanks to the [SelectorGadget](https://selectorgadget.com/), following [the method we saw earlier](#identify-the-relevant-html-markers), we can see that we now need elements marked by `#publication_date p`.\n\nWe can use `select` as we did earlier:\n\n::: {#9e7e094d .cell execution_count=24}\n``` {.python .cell-code}\ndissertpage_test.select(\"#publication_date p\")\n```\n\n::: {.cell-output .cell-output-display execution_count=23}\n```\n[<p>5-2023</p>]\n```\n:::\n:::\n\n\nNotice the square brackets around our result: this is import. It shows us that we have a ResultSet (a list of results specific to Beautiful Soup). This is because `select` returns all the results. Here, we have a single result, but the format is still list-like. Before we can go further, we need to index the value out of it:\n\n::: {#f50b975b .cell execution_count=25}\n``` {.python .cell-code}\ndissertpage_test.select(\"#publication_date p\")[0]\n```\n\n::: {.cell-output .cell-output-display execution_count=24}\n```\n<p>5-2023</p>\n```\n:::\n:::\n\n\nWe can now get the text out of this paragraph with the `text` attribute:\n\n::: {#9e292cfc .cell execution_count=26}\n``` {.python .cell-code}\ndissertpage_test.select(\"#publication_date p\")[0].text\n```\n\n::: {.cell-output .cell-output-display execution_count=25}\n```\n'5-2023'\n```\n:::\n:::\n\n\nWe could save it in a variable `date_test`:\n\n::: {#26b21c2b .cell execution_count=27}\n``` {.python .cell-code}\ndate_test = dissertpage_test.select(\"#publication_date p\")[0].text\n```\n:::\n\n\n:::{.exo}\n\n:::{.yourturn}\n\nYour turn:\n\n:::\n\nGet the major and advisor for our test URL.\n\n:::\n\n## Full run\n\nOnce everything is working for a test site, we can do some bulk scraping.\n\n### Extract all URLs\n\nWe already know how to get the 100 dissertations links from the main page: `mainpage.select(\".article-listing a\")`. Let's assign it to a variable:\n\n::: {#d337f180 .cell execution_count=28}\n``` {.python .cell-code}\ndissertlinks = mainpage.select(\".article-listing a\")\n```\n:::\n\n\nThis ResultSet is an [iterable](https://docs.python.org/3/glossary.html#term-iterable), meaning that it can be used in a loop.\n\nLet's write a loop to extract all the URLs from this ResultSet of links:\n\n::: {#88e7a82e .cell execution_count=29}\n``` {.python .cell-code}\n# Create an empty list before filling it during the loop\nurls = []\n\nfor link in dissertlinks:\n    urls.append(link.get(\"href\"))\n```\n:::\n\n\nLet's see our first 5 URLs:\n\n::: {#fda505a0 .cell execution_count=30}\n``` {.python .cell-code}\nurls[:5]\n```\n\n::: {.cell-output .cell-output-display execution_count=29}\n```\n['https://trace.tennessee.edu/utk_graddiss/8076',\n 'https://trace.tennessee.edu/utk_graddiss/9158',\n 'https://trace.tennessee.edu/utk_graddiss/8080',\n 'https://trace.tennessee.edu/utk_graddiss/8086',\n 'https://trace.tennessee.edu/utk_graddiss/8078']\n```\n:::\n:::\n\n\n### Extract data from each page\n\nFor each element of `urls` (i.e. for each dissertation URL), we can now get our information.\n\n::: {#702e5f2f .cell execution_count=31}\n``` {.python .cell-code}\n# Create an empty list\nls = []\n\n# For each element of our list of sites\nfor url in urls:\n    # Send a request to the site\n    r = requests.get(url)\n    # Parse the result\n    dissertpage = BeautifulSoup(r.text, \"html.parser\")\n    # Get the date\n    date = dissertpage.select(\"#publication_date p\")[0].text\n    # Get the major\n    major = dissertpage.select(\"#department p\")[0].text\n    # Get the advisor\n    advisor = dissertpage.select(\"#advisor1 p\")[0].text\n    # Store the results in the list\n    ls.append((date, major, advisor))\n    # Add a delay at each iteration\n    time.sleep(0.1)\n```\n:::\n\n\n:::{.note}\n\nSome sites will block requests if they are too frequent. Adding a little delay between requests is often a good idea.\n\n:::\n\n## Store results in DataFrame\n\nA DataFrame would be a lot more convenient than a list to hold our results.\n\nFirst, we create a list with the column names for our future DataFrame:\n\n::: {#a232a0b0 .cell execution_count=32}\n``` {.python .cell-code}\ncols = [\"Date\", \"Major\", \"Advisor\"]\n```\n:::\n\n\nThen we create our DataFrame:\n\n::: {#b1ecdf01 .cell execution_count=33}\n``` {.python .cell-code}\ndf = pd.DataFrame(ls, columns=cols)\n```\n:::\n\n\n::: {#f65f2802 .cell execution_count=34}\n``` {.python .cell-code}\ndf\n```\n\n::: {.cell-output .cell-output-display execution_count=33}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Date</th>\n      <th>Major</th>\n      <th>Advisor</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5-2023</td>\n      <td>Life Sciences</td>\n      <td>Bode A. Olukolu</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>12-2023</td>\n      <td>Industrial Engineering</td>\n      <td>Hugh Medal</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5-2023</td>\n      <td>Nuclear Engineering</td>\n      <td>Erik Lukosi</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>5-2023</td>\n      <td>Energy Science and Engineering</td>\n      <td>Kyle R. Gluesenkamp</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5-2023</td>\n      <td>English</td>\n      <td>Margaret Lazarus Dean</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>8-2023</td>\n      <td>Educational Psychology and Research</td>\n      <td>Qi Sun</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>12-2023</td>\n      <td>Nuclear Engineering</td>\n      <td>Lawrence H. Heilbronn</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>5-2023</td>\n      <td>Geology</td>\n      <td>Bradley Thomson</td>\n    </tr>\n    <tr>\n      <th>98</th>\n      <td>5-2023</td>\n      <td>Natural Resources</td>\n      <td>Sharon R. Jean-Philippe</td>\n    </tr>\n    <tr>\n      <th>99</th>\n      <td>12-2023</td>\n      <td>Psychology</td>\n      <td>Greg Stuart</td>\n    </tr>\n  </tbody>\n</table>\n<p>100 rows × 3 columns</p>\n</div>\n```\n:::\n:::\n\n\n## Save results to file\n\nAs a final step, we will save our data to a CSV file:\n\n```{.python}\ndf.to_csv('dissertations_data.csv', index=False)\n```\n\n:::{.note}\n\nThe default `index=True` writes the row numbers. We are not writing these indices in our file by changing the value of this argument to `False`.\n\n:::\n\nIf you are using a Jupyter notebook or the [IPython](https://en.wikipedia.org/wiki/IPython) shell, you can type `!ls` to see that the file is there and `!cat dissertations_data.csv` to print its content.\n\n:::{.note}\n\n`!` is a magic command that allows to run Unix shell commands in a notebook or IPython shell.\n\n:::\n\n",
    "supporting": [
      "ws_webscraping_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}