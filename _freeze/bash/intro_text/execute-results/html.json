{
  "hash": "57ea00110dff281d372bfa73450a3f40",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Searching & manipulating text\naliases:\n  - text.html\nauthor:\n  - Marie-Hélène Burle\n  - Alex Razoumov\n---\n\n:::{.def}\n\n\n\n:::\n\n\n::: {.cell}\n\n:::\n\n\n## Searching inside files with grep\n\n```{.bash}\ncd ~/Desktop/data-shell/writing\nmore haiku.txt\n```\n\nFirst let's search for text in files:\n\n```{.bash}\ngrep not haiku.txt     # let's find all lines that contain the word 'not'\ngrep day haiku.txt     # now search for word 'day'\ngrep -w day haiku.txt     # search for a separate word 'day' (not 'today', etc.)\ngrep -w today haiku.txt   # search for 'today'\ngrep -w Today haiku.txt   # search for 'Today'\ngrep -i -w today haiku.txt       # both upper and lower case 'today'\ngrep -n -i -w today haiku.txt    # -n prints out numbers the matching lines\ngrep -n -i -w -v the haiku.txt   # -v searches for lines that do not contain 'the'\nman grep\n```\n\nMore than two arguments to grep:\n\n```{.bash}\ngrep pattern file1 file2 file3   # all argument after the first one are assumed to be filenames\ngrep pattern *.txt   # the last argument will expand to the list of *.txt files\n```\n\n<!-- {{< question num=\"`dissecting a haiku`\" >}} -->\n```txt\nThe Tao that is seen\nIs not the true Tao, until\nYou bring fresh toner.\nWith searching comes loss\nand the presence of absence:\n\"My Thesis\" not found.\nYesterday it worked.\nToday it is not working.\nSoftware is like that.\n```\nFrom the above text, contained in the file `haiku.txt`, which command would result in the following output:\n```txt\nand the presence of absence:\n```\n1. `grep of haiku.txt`\n2. `grep -E of haiku.txt`\n3. `grep -w of haiku.txt`\n<!-- {{< /question >}} -->\n\n[Here](https://youtu.be/mbZ8nB-V4zQ) is a video on this topic.\n\n## Text manipulation\n\n(This example was kindly provided by John Simpson.)\n\nIn this section we'll use two tools for text manipulation: *sed* and *tr*. Our goal is to calculate the\nfrequency of all dictionary words in the novel \"The Invisible Man\" by Herbert Wells (public\ndomain). First, let's apply our knowledge of grep to this text:\n\n```sh\ncd ~/Desktop/data-shell\nls   # shows wellsInvisibleMan.txt\nwc wellsInvisibleMan.txt                          # number of lines, words, characters\ngrep invisible wellsInvisibleMan.txt              # see the invisible man\ngrep invisible wellsInvisibleMan.txt | wc -l      # returns 60; adding -w gives the same count\ngrep -i invisible wellsInvisibleMan.txt | wc -l   # returns 176 (includes: invisible Invisible INVISIBLE)\n```\n\nLet's sidetrack for a second and see how we can use the \"stream editor\" `sed`:\n\n```sh\nsed 's/[iI]nvisible/supervisible/g' wellsInvisibleMan.txt > visibleMan.txt   # make him visible\ncat wellsInvisibleMan.txt | sed 's/[iI]nvisible/supervisible/g' > visibleMan.txt   # this also works (standard input)\ngrep supervisible visibleMan.txt   # see what happened to the now visible man\ngrep -i invisible visibleMan.txt   # see what was not converted\nman sed\n```\n\nNow let's remove punctuation from the original file using \"tr\" (translate) command:\n\n```sh\ncat wellsInvisibleMan.txt | tr -d \"[:punct:]\" > invisibleNoPunct.txt    # tr only takes standard input\ntail wellsInvisibleMan.txt\ntail invisibleNoPunct.txt\n```\n\nNext convert all upper case to lower case:\n\n```sh\ncat invisibleNoPunct.txt | tr '[:upper:]' '[:lower:]' > invisibleClean.txt\ntail invisibleClean.txt\n```\n\nNext replace spaces with new lines:\n\n```sh\ncat invisibleClean.txt | sed 's/ /\\'$'\\n/g' > invisibleList.txt   # \\'$'\\n is a shortcut for a new line\nmore invisibleList.txt\n```\n\nNext remove empty lines:\n\n```sh\nsed '/^$/d' invisibleList.txt  > invisibleCompact.txt\n```\n\nNext sort the list alphabetically, count each word's occurrence, and remove duplicate words:\n\n```sh\ncat invisibleCompact.txt | sort | uniq -c > invisibleWords.txt\nmore invisibleWords.txt\n```\n\nNext sort the list into most frequent words:\n\n```sh\ncat invisibleWords.txt | sort -gr > invisibleFrequencyList.txt   # use 'man sort'\nmore invisibleFrequencyList.txt\n```\n\n<!-- > **Exercise:** write a script 'countWords.sh' that takes a text file name as an argument, and returns -->\n<!-- > the list of its 100 most common words, i.e. the script should be used as `./countWords.sh -->\n<!-- > wellsInvisibleMan.txt`. The script should not leave any intermediate files. Or even better, write a -->\n<!-- > function 'countWords()' taking a text file name as an argument. -->\n\n<!-- 10-textManipulation.mkv -->\n<!-- {{< yt 4IkHY84uUss 63 >}} -->\nYou can [watch a video for this topic](https://youtu.be/4IkHY84uUss) after the workshop.\n\n**Quick reference:**\n```sh\nsed 's/pattern1/pattern2/' filename    # replace pattern1 with pattern2, one per line\nsed 's/pattern1/pattern2/g' filename   # same but multiple per line\nsed 's|pattern1|pattern2|g' filename   # same\n\ncat wellsInvisibleMan.txt | tr -d \"[:punct:]\" > invisibleNoPunct.txt       # remove punctuation; tr only takes standard input\ncat invisibleNoPunct.txt | tr '[:upper:]' '[:lower:]' > invisibleClean.txt # convert all upper case to lower case:\ncat invisibleClean.txt | sed 's/ /\\'$'\\n/g' > invisibleList.txt            # replace spaces with new lines;\n                                                                           # \\'$'\\n is a shortcut for a new line\nsed '/^$/d' invisibleList.txt  > invisibleCompact.txt   # remove empty lines\ncat invisibleCompact.txt | sort | uniq -c > invisibleWords.txt   # sort the list alphabetically, count each word's occurrence\ncat invisibleWords.txt | sort -gr > invisibleFrequencyList.txt   # sort the list into most frequent words\n```\n\n<!-- {{< question num=39a >}} -->\nWrite a script that takes an English-language file and print the list of its 100 most common words, along with the word\ncount. Hint: use the workflow from the text manipulation video. Finally, convert this script into a bash function. (no\nneed to type any answer)\n<!-- {{< /question >}} -->\n\n## AWK scripting language\n\n[AWK](https://en.wikipedia.org/wiki/AWK) is a scripting language that can be used inside Bash which allows for column-based text manipulation.\n\n```sh\ncd .../data-shell/writing\ncat haiku.txt   # 11 lines\n```\n\nYou can define inline awk scripts with braces surrounded by single quotation:\n\n```sh\nawk '{print $1}' haiku.txt       # $1 is the first field (word) in each line => processing columns\nawk '{print $0}' haiku.txt       # $0 is the whole line\nawk '{print}' haiku.txt          # the whole line is the default action\nawk -Fa '{print $1}' haiku.txt   # can specify another separator with -F (\"a\" in this case)\n```\n\nYou can use multiple commands inside your awk script:\n\n```sh\necho Hello Tom > hello.txt\necho Hello John >> hello.txt\nawk '{$2=\"Adam\"; print $0}' hello.txt   # we replaced the second word in each line with \"Adam\"\n```\n\nMost common `awk` usage is to postprocess output of other commands:\n\n```sh\n/bin/ps aux    # display all running processes as multi-column output\n/bin/ps aux | awk '{print $2 \" \" $11}'   # print only the process number and the command\n```\n\nAwk also takes patterns in addition to scripts:\n\n```sh\nawk '/Yesterday|Today/' haiku.txt   # print the lines that contain the words Yesterday or Today\n```\n\nAnd then you act on these patterns: if the pattern evaluates to True, then run the script:\n\n```sh\nawk '/Yesterday|Today/{print $3}' haiku.txt\nawk '/Yesterday|Today/' haiku.txt | awk '{print $3}'   # same as previous line\n```\n\nAwk has a number of built-in variables; the most commonly used is NR:\n\n```sh\nawk 'NR>1' haiku.txt    # if NumberRecord >1 then print it (default action), i.e. skip the first line\nawk 'NR>1{print $0}' haiku.txt   # last command expanded\nawk 'NR>1 && NR < 5' haiku.txt   # print lines 2-4\n```\n\n> **Exercise:** write a awk script to process `cities.csv` to print only town/city names and their\n> population and store it in a separate file `populations.csv`. Try to do everything in a single-line\n> command.\n\n**Quick reference:**\n```sh\nls -l | awk 'NR>3 {print $5 \"  \" $9}'   # print 5th and 9th columns starting with line 4\nawk 'NR>1 && NR < 5' haiku.txt          # print lines 2-4\nawk '/Yesterday|Today/' haiku.txt       # print lines that contain Yesterday or Today\n```\n\n<!-- {{< question num=41a >}} -->\nWrite a one-line command that finds 5 largest files in the current directory and prints only their names and file sizes\nin the human-readable format (indicating bytes, kB, MB, GB, ...) in the decreasing file-size order. Hint: use `find`,\n`xargs`, and `awk`.\n<!-- {{< /question >}} -->\n\n<!-- {{< question num=42 >}} -->\nLet's study together these commands:\n```sh\nsource ~/projects/def-sponsor00/shared/fzf/.fzf.bash\nkill -9 `/bin/ps aux | fzf | awk '{print $2}'`\n```\n<!-- {{< /question >}} -->\n\n[Here](https://youtu.be/BMrL7zoyJH8) is a video on this topic.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}