<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>gemini_chat – Research Computing</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../..//img/favicon_sfu.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-48758e0ea246f80a9d80e8901e3a7c2a.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-3ab34fd50eaf4c251bc27ae6cc09dd28.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-7E8D6VP6FH"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-7E8D6VP6FH', { 'anonymize_ip': true});
</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../img/favicon_sfudrac.png" alt="SFU &amp; DRAC logos" class="navbar-logo light-content">
    <img src="../../img/favicon_sfudrac.png" alt="SFU &amp; DRAC logos" class="navbar-logo dark-content">
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../ai/index.html"> 
<span class="menu-text">AI</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../r/index.html"> 
<span class="menu-text">R</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../python/index.html"> 
<span class="menu-text">Python</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../julia/index.html"> 
<span class="menu-text">Julia</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../git/index.html"> 
<span class="menu-text">Git</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../bash/index.html"> 
<span class="menu-text">Bash/Zsh</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../emacs/index.html"> 
<span class="menu-text">Emacs</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../tools/index.html"> 
<span class="menu-text">Tools</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools tools-wide">
    <a href="https://github.com/WestDRI" title="" class="quarto-navigation-tool px-1" aria-label="GitHub organization" target="_blank"><i class="bi bi-github"></i></a>
    <a href="https://docs.alliancecan.ca/wiki/Technical_documentation" title="" class="quarto-navigation-tool px-1" aria-label="Alliance wikipedia" target="_blank"><i class="bi bi-wikipedia"></i></a>
    <a href="https://www.youtube.com/channel/UCfgds4Qf7VFOv4ORRvFFmhw" title="" class="quarto-navigation-tool px-1" aria-label="YouTube channel" target="_blank"><i class="bi bi-youtube"></i></a>
    <a href="../../calendar.html" title="Upcoming training events" class="quarto-navigation-tool px-1" aria-label="Training calendar"><i class="bi bi-calendar-week"></i></a>
    <a href="../../newsletter.html" title="Training events mailing list" class="quarto-navigation-tool px-1" aria-label="Newsletter"><i class="bi bi-envelope-fill"></i></a>
    <a href="../../contact.html" title="Contact" class="quarto-navigation-tool px-1" aria-label="Contact us"><i class="bi bi-mailbox2"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title"><em>On this page:</em></h2>
   
  <ul>
  <li><a href="#how-to-use-pix-with-grain" id="toc-how-to-use-pix-with-grain" class="nav-link active" data-scroll-target="#how-to-use-pix-with-grain">How to use PIX with Grain</a>
  <ul>
  <li><a href="#option-1-on-device-augmentation-recommended" id="toc-option-1-on-device-augmentation-recommended" class="nav-link" data-scroll-target="#option-1-on-device-augmentation-recommended">Option 1: On-Device Augmentation (Recommended)</a></li>
  <li><a href="#option-2-inside-grain-pipeline-cpu-preprocessing" id="toc-option-2-inside-grain-pipeline-cpu-preprocessing" class="nav-link" data-scroll-target="#option-2-inside-grain-pipeline-cpu-preprocessing">Option 2: Inside Grain Pipeline (CPU Preprocessing)</a></li>
  <li><a href="#summary-of-differences" id="toc-summary-of-differences" class="nav-link" data-scroll-target="#summary-of-differences">Summary of Differences</a></li>
  </ul></li>
  <li><a href="#is-grain-really-cpu-only" id="toc-is-grain-really-cpu-only" class="nav-link" data-scroll-target="#is-grain-really-cpu-only">Is Grain really CPU only?</a>
  <ul>
  <li><a href="#best-practice-augment-after-device_put-on-device" id="toc-best-practice-augment-after-device_put-on-device" class="nav-link" data-scroll-target="#best-practice-augment-after-device_put-on-device">Best Practice: Augment <em>After</em> <code>device_put</code> (On-Device)</a></li>
  <li><a href="#why-not-use-pix-inside-grain.dataloader" id="toc-why-not-use-pix-inside-grain.dataloader" class="nav-link" data-scroll-target="#why-not-use-pix-inside-grain.dataloader">Why not use PIX inside <code>grain.DataLoader</code>?</a></li>
  </ul></li>
  <li><a href="#should-i-save-the-cleaned-images-to-file" id="toc-should-i-save-the-cleaned-images-to-file" class="nav-link" data-scroll-target="#should-i-save-the-cleaned-images-to-file">Should I save the cleaned images to file?</a>
  <ul>
  <li><a href="#why-your-current-approach-is-slow" id="toc-why-your-current-approach-is-slow" class="nav-link" data-scroll-target="#why-your-current-approach-is-slow">Why your current approach is slow</a></li>
  <li><a href="#the-recommended-workflow" id="toc-the-recommended-workflow" class="nav-link" data-scroll-target="#the-recommended-workflow">The Recommended Workflow</a>
  <ul>
  <li><a href="#step-1-offline-pre-processing-run-this-once" id="toc-step-1-offline-pre-processing-run-this-once" class="nav-link" data-scroll-target="#step-1-offline-pre-processing-run-this-once">Step 1: Offline Pre-processing (Run this once)</a></li>
  <li><a href="#step-2-optimized-training-pipeline" id="toc-step-2-optimized-training-pipeline" class="nav-link" data-scroll-target="#step-2-optimized-training-pipeline">Step 2: Optimized Training Pipeline</a></li>
  </ul></li>
  <li><a href="#summary-of-wins" id="toc-summary-of-wins" class="nav-link" data-scroll-target="#summary-of-wins">Summary of Wins</a></li>
  </ul></li>
  <li><a href="#shouldnt-i-use-a-class-rather-than-a-for-loop" id="toc-shouldnt-i-use-a-class-rather-than-a-for-loop" class="nav-link" data-scroll-target="#shouldnt-i-use-a-class-rather-than-a-for-loop">Shouldn’t I use a class rather than a for loop?</a>
  <ul>
  <li><a href="#the-cleaningdataset-class" id="toc-the-cleaningdataset-class" class="nav-link" data-scroll-target="#the-cleaningdataset-class">1. The <code>CleaningDataset</code> Class</a></li>
  <li><a href="#the-execution-make-it-fast" id="toc-the-execution-make-it-fast" class="nav-link" data-scroll-target="#the-execution-make-it-fast">2. The Execution (Make it Fast)</a></li>
  <li><a href="#why-this-is-better-than-your-previous-approach" id="toc-why-this-is-better-than-your-previous-approach" class="nav-link" data-scroll-target="#why-this-is-better-than-your-previous-approach">Why this is better than your previous approach:</a></li>
  </ul></li>
  <li><a href="#should-i-use-to-torchvision-then-since-grain-only-works-on-cpu" id="toc-should-i-use-to-torchvision-then-since-grain-only-works-on-cpu" class="nav-link" data-scroll-target="#should-i-use-to-torchvision-then-since-grain-only-works-on-cpu">Should I use to TorchVision then since Grain only works on CPU?</a>
  <ul>
  <li><a href="#regarding-the-offline-cleaning-step-resizing-saving" id="toc-regarding-the-offline-cleaning-step-resizing-saving" class="nav-link" data-scroll-target="#regarding-the-offline-cleaning-step-resizing-saving">1. Regarding the Offline Cleaning Step (Resizing &amp; Saving)</a></li>
  <li><a href="#regarding-the-training-dataloader" id="toc-regarding-the-training-dataloader" class="nav-link" data-scroll-target="#regarding-the-training-dataloader">2. Regarding the Training DataLoader</a></li>
  <li><a href="#summary-the-fastest-architecture" id="toc-summary-the-fastest-architecture" class="nav-link" data-scroll-target="#summary-the-fastest-architecture">Summary: The Fastest Architecture</a></li>
  </ul></li>
  <li><a href="#what-is-the-best-strategy-for-data-augmentation-with-the-pix-httpsgithub.comgoogle-deepminddm_pix-library-to-train-a-classification-model-in-jax-using-the-nabirds-httpsdl.allaboutbirds.orgnabirds-dataset" id="toc-what-is-the-best-strategy-for-data-augmentation-with-the-pix-httpsgithub.comgoogle-deepminddm_pix-library-to-train-a-classification-model-in-jax-using-the-nabirds-httpsdl.allaboutbirds.orgnabirds-dataset" class="nav-link" data-scroll-target="#what-is-the-best-strategy-for-data-augmentation-with-the-pix-httpsgithub.comgoogle-deepminddm_pix-library-to-train-a-classification-model-in-jax-using-the-nabirds-httpsdl.allaboutbirds.orgnabirds-dataset">What is the best strategy for data augmentation with the PIX (https://github.com/google-deepmind/dm_pix) library to train a classification model in JAX using the NABirds (https://dl.allaboutbirds.org/nabirds) dataset?</a>
  <ul>
  <li><a href="#the-strategy-look-closer-dont-change-colors" id="toc-the-strategy-look-closer-dont-change-colors" class="nav-link" data-scroll-target="#the-strategy-look-closer-dont-change-colors">The Strategy: “Look Closer, Don’t Change Colors”</a></li>
  <li><a href="#implementation-with-dm-pix" id="toc-implementation-with-dm-pix" class="nav-link" data-scroll-target="#implementation-with-dm-pix">Implementation with <code>dm-pix</code></a></li>
  <li><a href="#why-this-configuration" id="toc-why-this-configuration" class="nav-link" data-scroll-target="#why-this-configuration">Why this configuration?</a></li>
  <li><a href="#advanced-tip-jax-performance" id="toc-advanced-tip-jax-performance" class="nav-link" data-scroll-target="#advanced-tip-jax-performance">Advanced Tip: JAX Performance</a></li>
  </ul></li>
  <li><a href="#how-to-choose-which-augmentation-to-use" id="toc-how-to-choose-which-augmentation-to-use" class="nav-link" data-scroll-target="#how-to-choose-which-augmentation-to-use">How to choose which augmentation to use?</a>
  <ul>
  <li><a href="#the-standard-stack" id="toc-the-standard-stack" class="nav-link" data-scroll-target="#the-standard-stack">The standard stack</a></li>
  <li><a href="#advanced-techniques-mixup-cutmix" id="toc-advanced-techniques-mixup-cutmix" class="nav-link" data-scroll-target="#advanced-techniques-mixup-cutmix">Advanced Techniques (MixUp / CutMix)</a></li>
  <li><a href="#summary-checklist" id="toc-summary-checklist" class="nav-link" data-scroll-target="#summary-checklist">Summary Checklist</a></li>
  <li><a href="#the-danger-of-too-many" id="toc-the-danger-of-too-many" class="nav-link" data-scroll-target="#the-danger-of-too-many">The danger of too many</a></li>
  <li><a href="#modern-best-practice-randaugment" id="toc-modern-best-practice-randaugment" class="nav-link" data-scroll-target="#modern-best-practice-randaugment">Modern Best Practice: “RandAugment”</a></li>
  </ul></li>
  <li><a href="#should-i-cropresize-my-eval-set-the-same-way" id="toc-should-i-cropresize-my-eval-set-the-same-way" class="nav-link" data-scroll-target="#should-i-cropresize-my-eval-set-the-same-way">Should I crop/resize my eval set the same way?</a>
  <ul>
  <li><a href="#the-standard-protocols-for-fine-grained-classification-nabirds-cub-200" id="toc-the-standard-protocols-for-fine-grained-classification-nabirds-cub-200" class="nav-link" data-scroll-target="#the-standard-protocols-for-fine-grained-classification-nabirds-cub-200">The Standard Protocols for Fine-Grained Classification (NABirds / CUB-200)</a></li>
  <li><a href="#recommended-preprocessing-steps" id="toc-recommended-preprocessing-steps" class="nav-link" data-scroll-target="#recommended-preprocessing-steps">Recommended Preprocessing Steps</a></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary">Summary</a></li>
  <li><a href="#the-save-to-disk-phase-preprocessing" id="toc-the-save-to-disk-phase-preprocessing" class="nav-link" data-scroll-target="#the-save-to-disk-phase-preprocessing">1. The “Save to Disk” Phase (Preprocessing)</a></li>
  <li><a href="#the-load-augment-phase-runtime" id="toc-the-load-augment-phase-runtime" class="nav-link" data-scroll-target="#the-load-augment-phase-runtime">2. The “Load &amp; Augment” Phase (Runtime)</a></li>
  <li><a href="#a-critical-warning-on-resolution" id="toc-a-critical-warning-on-resolution" class="nav-link" data-scroll-target="#a-critical-warning-on-resolution">A Critical Warning on Resolution</a></li>
  </ul></li>
  <li><a href="#should-i-random-crop" id="toc-should-i-random-crop" class="nav-link" data-scroll-target="#should-i-random-crop">Should I random crop?</a>
  <ul>
  <li><a href="#the-short-answer" id="toc-the-short-answer" class="nav-link" data-scroll-target="#the-short-answer">1. The Short Answer</a></li>
  <li><a href="#why-random-cropping-helps-even-for-birds" id="toc-why-random-cropping-helps-even-for-birds" class="nav-link" data-scroll-target="#why-random-cropping-helps-even-for-birds">2. Why Random Cropping Helps (Even for Birds)</a></li>
  <li><a href="#the-trap-in-your-current-workflow" id="toc-the-trap-in-your-current-workflow" class="nav-link" data-scroll-target="#the-trap-in-your-current-workflow">3. The “Trap” in Your Current Workflow</a></li>
  <li><a href="#recommended-pipeline-for-best-results" id="toc-recommended-pipeline-for-best-results" class="nav-link" data-scroll-target="#recommended-pipeline-for-best-results">4. Recommended Pipeline for Best Results</a>
  <ul>
  <li><a href="#a.-the-gold-standard-pipeline" id="toc-a.-the-gold-standard-pipeline" class="nav-link" data-scroll-target="#a.-the-gold-standard-pipeline">A. The “Gold Standard” Pipeline</a></li>
  <li><a href="#b.-the-fixed-files-pipeline-if-you-must-stick-to-your-saved-224x224-files" id="toc-b.-the-fixed-files-pipeline-if-you-must-stick-to-your-saved-224x224-files" class="nav-link" data-scroll-target="#b.-the-fixed-files-pipeline-if-you-must-stick-to-your-saved-224x224-files">B. The “Fixed Files” Pipeline (If you must stick to your saved 224x224 files)</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#should-i-save-to-disk-after-cropping-to-bb" id="toc-should-i-save-to-disk-after-cropping-to-bb" class="nav-link" data-scroll-target="#should-i-save-to-disk-after-cropping-to-bb">Should I save to disk after cropping to bb?</a>
  <ul>
  <li><a href="#the-recommended-workflow-1" id="toc-the-recommended-workflow-1" class="nav-link" data-scroll-target="#the-recommended-workflow-1">The Recommended Workflow</a></li>
  <li><a href="#why-this-is-better-than-your-current-plan" id="toc-why-this-is-better-than-your-current-plan" class="nav-link" data-scroll-target="#why-this-is-better-than-your-current-plan">Why this is better than your current plan</a></li>
  </ul></li>
  <li><a href="#should-i-just-skip-the-cropping-part-entirely" id="toc-should-i-just-skip-the-cropping-part-entirely" class="nav-link" data-scroll-target="#should-i-just-skip-the-cropping-part-entirely">Should I just skip the cropping part entirely?</a>
  <ul>
  <li><a href="#training-speed-winner-save-to-disk" id="toc-training-speed-winner-save-to-disk" class="nav-link" data-scroll-target="#training-speed-winner-save-to-disk">1. Training Speed (Winner: Save to Disk)</a></li>
  <li><a href="#flexibility-winner-on-the-fly" id="toc-flexibility-winner-on-the-fly" class="nav-link" data-scroll-target="#flexibility-winner-on-the-fly">2. Flexibility (Winner: On-the-Fly)</a></li>
  <li><a href="#accuracy-tie" id="toc-accuracy-tie" class="nav-link" data-scroll-target="#accuracy-tie">3. Accuracy (Tie)</a></li>
  <li><a href="#the-sweet-spot-recommendation" id="toc-the-sweet-spot-recommendation" class="nav-link" data-scroll-target="#the-sweet-spot-recommendation">The “Sweet Spot” Recommendation</a></li>
  </ul></li>
  <li><a href="#details-for-the-eval-set" id="toc-details-for-the-eval-set" class="nav-link" data-scroll-target="#details-for-the-eval-set">Details for the eval set</a>
  <ul>
  <li><a href="#phase-1-preprocessing-saving-to-disk" id="toc-phase-1-preprocessing-saving-to-disk" class="nav-link" data-scroll-target="#phase-1-preprocessing-saving-to-disk">Phase 1: Preprocessing (Saving to Disk)</a></li>
  <li><a href="#phase-2-the-runtime-pipeline-the-code" id="toc-phase-2-the-runtime-pipeline-the-code" class="nav-link" data-scroll-target="#phase-2-the-runtime-pipeline-the-code">Phase 2: The Runtime Pipeline (The Code)</a>
  <ul>
  <li><a href="#the-code-pytorch-example" id="toc-the-code-pytorch-example" class="nav-link" data-scroll-target="#the-code-pytorch-example">The Code (PyTorch Example)</a></li>
  </ul></li>
  <li><a href="#why-direct-resize-instead-of-resize-crop" id="toc-why-direct-resize-instead-of-resize-crop" class="nav-link" data-scroll-target="#why-direct-resize-instead-of-resize-crop">Why “Direct Resize” instead of “Resize &amp; Crop”?</a></li>
  <li><a href="#alternative-the-black-bars-method-padding" id="toc-alternative-the-black-bars-method-padding" class="nav-link" data-scroll-target="#alternative-the-black-bars-method-padding">Alternative: The “Black Bars” Method (Padding)</a></li>
  <li><a href="#summary-of-eval-workflow" id="toc-summary-of-eval-workflow" class="nav-link" data-scroll-target="#summary-of-eval-workflow">Summary of Eval Workflow</a></li>
  </ul></li>
  <li><a href="#which-model-to-use" id="toc-which-model-to-use" class="nav-link" data-scroll-target="#which-model-to-use">Which model to use?</a>
  <ul>
  <li><a href="#the-recommendation-resnet-50" id="toc-the-recommendation-resnet-50" class="nav-link" data-scroll-target="#the-recommendation-resnet-50">The Recommendation: ResNet-50</a></li>
  <li><a href="#the-pro-alternative-efficientnet-b0-or-b2" id="toc-the-pro-alternative-efficientnet-b0-or-b2" class="nav-link" data-scroll-target="#the-pro-alternative-efficientnet-b0-or-b2">The “Pro” Alternative: EfficientNet (B0 or B2)</a></li>
  <li><a href="#step-by-step-strategy-for-nabirds" id="toc-step-by-step-strategy-for-nabirds" class="nav-link" data-scroll-target="#step-by-step-strategy-for-nabirds">Step-by-Step Strategy for NABirds</a>
  <ul>
  <li><a href="#use-pre-trained-weights-crucial" id="toc-use-pre-trained-weights-crucial" class="nav-link" data-scroll-target="#use-pre-trained-weights-crucial">1. Use Pre-trained Weights (Crucial)</a></li>
  <li><a href="#resolution-matters" id="toc-resolution-matters" class="nav-link" data-scroll-target="#resolution-matters">2. Resolution Matters</a></li>
  <li><a href="#data-augmentation" id="toc-data-augmentation" class="nav-link" data-scroll-target="#data-augmentation">3. Data Augmentation</a></li>
  </ul></li>
  <li><a href="#code-snippet-pytorch-example" id="toc-code-snippet-pytorch-example" class="nav-link" data-scroll-target="#code-snippet-pytorch-example">Code Snippet (PyTorch Example)</a></li>
  </ul></li>
  <li><a href="#how-to-do-it-in-jax" id="toc-how-to-do-it-in-jax" class="nav-link" data-scroll-target="#how-to-do-it-in-jax">How to do it in JAX?</a>
  <ul>
  <li><a href="#the-challenge-with-bonsai" id="toc-the-challenge-with-bonsai" class="nav-link" data-scroll-target="#the-challenge-with-bonsai">1. The Challenge with Bonsai</a></li>
  <li><a href="#the-solution-efficientnet-jax-or-jimm" id="toc-the-solution-efficientnet-jax-or-jimm" class="nav-link" data-scroll-target="#the-solution-efficientnet-jax-or-jimm">2. The Solution: <code>efficientnet-jax</code> (or <code>jimm</code>)</a></li>
  <li><a href="#step-by-step-implementation-guide" id="toc-step-by-step-implementation-guide" class="nav-link" data-scroll-target="#step-by-step-implementation-guide">3. Step-by-Step Implementation Guide</a>
  <ul>
  <li><a href="#step-1-install-the-necessary-libraries" id="toc-step-1-install-the-necessary-libraries" class="nav-link" data-scroll-target="#step-1-install-the-necessary-libraries">Step 1: Install the necessary libraries</a></li>
  <li><a href="#step-2-load-the-pre-trained-model" id="toc-step-2-load-the-pre-trained-model" class="nav-link" data-scroll-target="#step-2-load-the-pre-trained-model">Step 2: Load the Pre-trained Model</a></li>
  </ul></li>
  <li><a href="#summary-of-strategy-for-nabirds" id="toc-summary-of-strategy-for-nabirds" class="nav-link" data-scroll-target="#summary-of-strategy-for-nabirds">4. Summary of Strategy for NABirds</a></li>
  </ul></li>
  <li><a href="#option-without-outdated-libraries" id="toc-option-without-outdated-libraries" class="nav-link" data-scroll-target="#option-without-outdated-libraries">Option without outdated libraries</a>
  <ul>
  <li><a href="#the-strategy" id="toc-the-strategy" class="nav-link" data-scroll-target="#the-strategy">The Strategy</a></li>
  <li><a href="#the-code" id="toc-the-code" class="nav-link" data-scroll-target="#the-code">The Code</a></li>
  <li><a href="#why-this-is-the-correct-approach" id="toc-why-this-is-the-correct-approach" class="nav-link" data-scroll-target="#why-this-is-the-correct-approach">Why this is the correct approach</a></li>
  <li><a href="#fine-tuning-advice" id="toc-fine-tuning-advice" class="nav-link" data-scroll-target="#fine-tuning-advice">Fine-Tuning Advice</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"></header>




<section id="how-to-use-pix-with-grain" class="level2">
<h2 class="anchored" data-anchor-id="how-to-use-pix-with-grain">How to use PIX with Grain</h2>
<p>Using PIX (dm-pix) with Grain requires bridging two different paradigms: Grain uses <strong>NumPy/Python</strong> on the CPU for loading, while PIX is designed for <strong>JAX</strong> on accelerators (GPU/TPU).</p>
<p>You have two main options to integrate them. <strong>Option 1 (Best Practice)</strong> is to apply PIX after the dataloader yields a batch, leveraging JAX’s speed. <strong>Option 2</strong> is to apply PIX inside the Grain pipeline, which is useful if you need to augment images before batching (e.g., for variable-size images).</p>
<section id="option-1-on-device-augmentation-recommended" class="level3">
<h3 class="anchored" data-anchor-id="option-1-on-device-augmentation-recommended">Option 1: On-Device Augmentation (Recommended)</h3>
<p>This method is fastest. Use Grain to load and batch clean NumPy arrays, then use <code>jax.vmap</code> and <code>jax.jit</code> to apply PIX transformations on the GPU/TPU.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> grain.python <span class="im">as</span> grain</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> dm_pix <span class="im">as</span> pix</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> jax</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> jax.numpy <span class="im">as</span> jnp</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Define your augmentation function (runs on GPU/TPU)</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> augment_batch(batch, rng_key):</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Split rng key for different operations</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>    rng1, rng2 <span class="op">=</span> jax.random.split(rng_key)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>    images <span class="op">=</span> batch[<span class="st">'image'</span>]</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Example: Random flip and brightness</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># PIX functions expect (H, W, C), so we use vmap to handle (Batch, H, W, C)</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>    images <span class="op">=</span> jax.vmap(pix.flip_left_right)(images)</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>    images <span class="op">=</span> jax.vmap(<span class="kw">lambda</span> i, k: pix.random_brightness(k, i, max_delta<span class="op">=</span><span class="fl">0.2</span>))(images, jax.random.split(rng1, images.shape[<span class="dv">0</span>]))</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> {<span class="op">**</span>batch, <span class="st">'image'</span>: images}</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="co"># JIT compile the augmentation for maximum speed</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>augment_batch_jit <span class="op">=</span> jax.jit(augment_batch)</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Setup standard Grain loader (loads NumPy arrays)</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Assume 'source' is your DataSource</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>loader <span class="op">=</span> grain.DataLoader(</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>    data_source<span class="op">=</span>source,</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>    sampler<span class="op">=</span>grain.IndexSampler(</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>        num_records<span class="op">=</span><span class="bu">len</span>(source),</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>        shard_options<span class="op">=</span>grain.ShardOptions(shard_count<span class="op">=</span><span class="dv">1</span>, shard_index<span class="op">=</span><span class="dv">0</span>),</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>        shuffle<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>        seed<span class="op">=</span><span class="dv">42</span></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>    operations<span class="op">=</span>[</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>        grain.Batch(batch_size<span class="op">=</span><span class="dv">32</span>), <span class="co"># Batching happens here</span></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. Training Loop</span></span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>rng <span class="op">=</span> jax.random.key(<span class="dv">0</span>)</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> batch <span class="kw">in</span> loader:</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Split RNG for this step</span></span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>    rng, step_rng <span class="op">=</span> jax.random.split(rng)</span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Move data to device (GPU/TPU)</span></span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Note: Grain yields numpy arrays, so we convert them to JAX arrays here</span></span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a>    batch_jax <span class="op">=</span> jax.tree.<span class="bu">map</span>(jnp.array, batch)</span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Apply augmentations on device</span></span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>    augmented_batch <span class="op">=</span> augment_batch_jit(batch_jax, step_rng)</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ... proceed to model training ...</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="option-2-inside-grain-pipeline-cpu-preprocessing" class="level3">
<h3 class="anchored" data-anchor-id="option-2-inside-grain-pipeline-cpu-preprocessing">Option 2: Inside Grain Pipeline (CPU Preprocessing)</h3>
<p>If you must augment <em>before</em> batching (e.g., cropping high-res images to a smaller fixed size), you can use <code>grain.RandomMapTransform</code>.</p>
<p><strong>Critical Detail:</strong> Grain provides a NumPy <code>rng</code> (generator), but PIX needs a JAX <code>PRNGKey</code>. You must convert the seed manually.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> grain.python <span class="im">as</span> grain</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> dm_pix <span class="im">as</span> pix</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> jax</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> PixAugmentation(grain.RandomMapTransform):</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> random_map(<span class="va">self</span>, element, rng: np.random.Generator):</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 1. Convert Grain's Numpy RNG to a JAX PRNGKey</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>        <span class="co"># We generate a large integer seed from the Numpy generator</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>        seed <span class="op">=</span> rng.integers(<span class="dv">0</span>, <span class="dv">2</span><span class="op">**</span><span class="dv">32</span> <span class="op">-</span> <span class="dv">1</span>)</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>        jax_key <span class="op">=</span> jax.random.key(seed)</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>        image <span class="op">=</span> element[<span class="st">'image'</span>] <span class="co"># Expecting numpy array</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 2. Apply PIX transformations</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Note: PIX accepts numpy arrays but converts them to JAX arrays internally.</span></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># This runs on CPU because Grain workers are CPU processes.</span></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Random Crop example</span></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># PIX random functions need a key</span></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>        key1, key2 <span class="op">=</span> jax.random.split(jax_key)</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>        image <span class="op">=</span> pix.random_crop(</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>            key<span class="op">=</span>key1,</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>            image<span class="op">=</span>image,</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>            crop_sizes<span class="op">=</span>(<span class="dv">224</span>, <span class="dv">224</span>, <span class="dv">3</span>)</span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>        image <span class="op">=</span> pix.random_flip_left_right(</span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>            key<span class="op">=</span>key2,</span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>            image<span class="op">=</span>image</span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 3. Return as numpy array (Grain expects standard Python/Numpy types)</span></span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> {<span class="op">**</span>element, <span class="st">'image'</span>: np.array(image)}</span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Usage in loader</span></span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a>loader <span class="op">=</span> grain.DataLoader(</span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a>    data_source<span class="op">=</span>source,</span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a>    sampler<span class="op">=</span>grain.IndexSampler(...),</span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a>    operations<span class="op">=</span>[</span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a>        PixAugmentation(), <span class="co"># Apply per-item augmentation</span></span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a>        grain.Batch(batch_size<span class="op">=</span><span class="dv">32</span>)</span>
<span id="cb2-44"><a href="#cb2-44" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb2-45"><a href="#cb2-45" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="summary-of-differences" class="level3">
<h3 class="anchored" data-anchor-id="summary-of-differences">Summary of Differences</h3>
<table class="caption-top table">
<colgroup>
<col style="width: 16%">
<col style="width: 44%">
<col style="width: 39%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Feature</th>
<th style="text-align: left;">Option 1 (On-Device)</th>
<th style="text-align: left;">Option 2 (In-Loader)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Execution</strong></td>
<td style="text-align: left;">GPU / TPU</td>
<td style="text-align: left;">CPU (Multiprocessing)</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Speed</strong></td>
<td style="text-align: left;">Extremely Fast (Vectorized)</td>
<td style="text-align: left;">Slower (CPU bound)</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Batching</strong></td>
<td style="text-align: left;">Augments entire batch at once</td>
<td style="text-align: left;">Augments one image at a time</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Use Case</strong></td>
<td style="text-align: left;">Color jitters, standard resizing, flips</td>
<td style="text-align: left;">Variable size inputs -&gt; Fixed size</td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="is-grain-really-cpu-only" class="level2">
<h2 class="anchored" data-anchor-id="is-grain-really-cpu-only">Is Grain really CPU only?</h2>
<p>Use of the documentation you provided is appreciated. However, looking closely at the <a href="https://docs.jaxstack.ai/en/latest/data_loaders_on_gpu_with_jax.html#loading-data-with-grain">code in that tutorial</a>, it actually confirms that Grain runs on the CPU and explicitly transfers data to the GPU afterwards.</p>
<p>In the “Loading Data with Grain” section of your link, the training loop explicitly calls:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>x, y <span class="op">=</span> device_put(x), device_put(y)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>This demonstrates the standard workflow: <strong>Grain (CPU) → RAM → <code>device_put</code> → GPU</strong>.</p>
<p>Because Grain workers are Python <code>multiprocessing</code> processes (which run on the CPU), using PIX <strong>inside</strong> the Grain pipeline forces JAX to run on the CPU. This is significantly slower than using standard libraries like OpenCV or Pillow.</p>
<p>The recommendation remains the same, but here is the clarified context based on your link:</p>
<section id="best-practice-augment-after-device_put-on-device" class="level3">
<h3 class="anchored" data-anchor-id="best-practice-augment-after-device_put-on-device">Best Practice: Augment <em>After</em> <code>device_put</code> (On-Device)</h3>
<p>Since your goal with PIX is to leverage JAX’s speed on the GPU/TPU, you should apply PIX transformations <strong>after</strong> the data has left the Grain loader and arrived on the device.</p>
<ol type="1">
<li><strong>Grain (CPU):</strong> Loads clean, raw images (using Pillow/OpenCV). Batches them.</li>
<li><strong>Transfer:</strong> You move the batch to GPU (e.g., <code>jax.device_put</code> or implicitly via <code>jit</code>).</li>
<li><strong>PIX (GPU):</strong> You apply the augmentations inside your training step (jit-compiled).</li>
</ol>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Grain Loader (Runs on CPU)</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>loader <span class="op">=</span> grain.DataLoader(</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>    data_source<span class="op">=</span>source,</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    operations<span class="op">=</span>[grain.Batch(batch_size<span class="op">=</span><span class="dv">32</span>)] </span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Augmentation Function (Runs on GPU via JIT)</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="at">@jax.jit</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> augment_and_train(state, batch, key):</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>    images <span class="op">=</span> batch[<span class="st">'image'</span>]</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Split key for PIX</span></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>    key, subkey <span class="op">=</span> jax.random.split(key)</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># PIX runs blindingly fast here because it's vectorized on the GPU</span></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>    images <span class="op">=</span> jax.vmap(pix.flip_left_right)(images)</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>    images <span class="op">=</span> jax.vmap(<span class="kw">lambda</span> i, k: pix.random_brightness(k, i, max_delta<span class="op">=</span><span class="fl">0.2</span>))(</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>        images, jax.random.split(subkey, images.shape[<span class="dv">0</span>])</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ... rest of your training step ...</span></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> state, key</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. Loop</span></span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> batch <span class="kw">in</span> loader:</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Data is moved to GPU here (implicitly or explicitly)</span></span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>    state, key <span class="op">=</span> augment_and_train(state, batch, key)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-not-use-pix-inside-grain.dataloader" class="level3">
<h3 class="anchored" data-anchor-id="why-not-use-pix-inside-grain.dataloader">Why not use PIX inside <code>grain.DataLoader</code>?</h3>
<p>If you put PIX inside a <code>grain.MapTransform</code>, it executes inside a CPU worker process. * <strong>Performance Hit:</strong> JAX is optimized for accelerators. Running JAX image ops on a CPU one-by-one is often slower than using Pillow or OpenCV. * <strong>Resource Conflict:</strong> JAX pre-allocates memory. Spawning multiple Grain workers that all try to initialize JAX can lead to memory contention or crashes (OOM) unless you carefully manage <code>JAX_PLATFORMS=cpu</code>.</p>
<p><strong>Summary:</strong> Grain handles the “feed” (CPU), and PIX handles the “chew” (GPU). Keep them separate for maximum performance.</p>
</section>
</section>
<section id="should-i-save-the-cleaned-images-to-file" class="level2">
<h2 class="anchored" data-anchor-id="should-i-save-the-cleaned-images-to-file">Should I save the cleaned images to file?</h2>
<p>Yes, it would be <strong>significantly faster</strong> to save the cleaned (cropped and resized) images to files as a separate step.</p>
<p>Here is the breakdown of why your current approach is likely creating a bottleneck and how to fix it.</p>
<section id="why-your-current-approach-is-slow" class="level3">
<h3 class="anchored" data-anchor-id="why-your-current-approach-is-slow">Why your current approach is slow</h3>
<ol type="1">
<li><strong>Repeated Deterministic Work:</strong> Your <code>BbCrop</code> and <code>PaddingResize</code> operations are deterministic (the result is always the same for a specific image). Currently, you are forcing the CPU to re-calculate the crop and the expensive <code>skimage.resize</code> (which involves convolutions) for every single image, <strong>in every single epoch</strong>.</li>
<li><strong>I/O Inefficiency:</strong> You are likely loading large original images (e.g., 1920x1080) just to shrink them down to 224x224. This wastes disk bandwidth reading unnecessary pixels.</li>
<li><strong>Multiprocessing Overhead (Float32):</strong> In <code>NormAndCast</code>, you convert images to <code>float32</code>. A 224x224x3 image in <code>uint8</code> is ~150KB. In <code>float32</code>, it is ~600KB. Grain uses multiprocessing; moving larger <code>float32</code> data between worker processes and the main process is 4x slower than moving <code>uint8</code>.</li>
<li><strong>JAX in CPU Workers:</strong> You are using <code>jnp.array</code> inside <code>NormAndCast</code>. Since Grain workers run on the CPU, this initializes JAX on the CPU inside every worker process. This is not only slower than NumPy for simple element-wise division but can cause memory contention issues.</li>
</ol>
</section>
<section id="the-recommended-workflow" class="level3">
<h3 class="anchored" data-anchor-id="the-recommended-workflow">The Recommended Workflow</h3>
<p>You should split your pipeline into two stages: <strong>Offline Pre-processing</strong> and <strong>Training</strong>.</p>
<section id="step-1-offline-pre-processing-run-this-once" class="level4">
<h4 class="anchored" data-anchor-id="step-1-offline-pre-processing-run-this-once">Step 1: Offline Pre-processing (Run this once)</h4>
<p>Write a script that iterates through your dataset and performs the <strong>deterministic</strong> steps: Crop and Resize. * Save the output as standard image files (PNG or JPG) or <code>.npy</code> files. * <strong>Crucial:</strong> Save them as <code>uint8</code> (0-255), not <code>float32</code>.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Pseudo-code for a cleaning script</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> skimage.io <span class="im">import</span> imread, imsave</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> skimage.transform <span class="im">import</span> resize</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Define your logic here (using standard Numpy, not JAX)</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> process_and_save(image_path, bbox, save_path):</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>    img <span class="op">=</span> imread(image_path)</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 1. Crop (BbCrop logic)</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>    img_cropped <span class="op">=</span> img[bbox.y:bbox.y<span class="op">+</span>bbox.h, bbox.x:bbox.x<span class="op">+</span>bbox.w]</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 2. Resize with Padding (PaddingResize logic)</span></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ... (Your resize logic) ...</span></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Ensure output is uint8 to save space</span></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>    img_final <span class="op">=</span> (resized_img_float <span class="op">*</span> <span class="dv">255</span>).astype(np.uint8)</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 3. Save</span></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>    imsave(save_path, img_final)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="step-2-optimized-training-pipeline" class="level4">
<h4 class="anchored" data-anchor-id="step-2-optimized-training-pipeline">Step 2: Optimized Training Pipeline</h4>
<p>Now your Grain loader only needs to read small, pre-cropped files. This changes your transforms significantly.</p>
<p><strong>Remove</strong> <code>BbCrop</code> and <code>PaddingResize</code> from Grain. <strong>Move</strong> <code>NormAndCast</code> to the GPU (after batching).</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Grain Loader (Runs on CPU)</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Only responsible for I/O and Batching</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>loader <span class="op">=</span> grain.DataLoader(</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>    data_source<span class="op">=</span>cleaned_source, <span class="co"># Points to your pre-processed files</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>    operations<span class="op">=</span>[</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>        grain.Batch(batch_size<span class="op">=</span><span class="dv">32</span>) </span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Note: No map transforms needed if data is pre-saved!</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. GPU Step (JAX)</span></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a><span class="at">@jax.jit</span></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_step(state, batch):</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># a. Move to GPU (implicitly done when passing to JIT)</span></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>    images <span class="op">=</span> batch[<span class="st">'image'</span>]</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># b. Normalize and Cast (Done on GPU = Blazing Fast)</span></span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># This replaces your 'NormAndCast' Grain transform</span></span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>    images <span class="op">=</span> images.astype(jnp.float32) <span class="op">/</span> <span class="fl">255.0</span></span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># c. Augmentation (PIX)</span></span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Apply random things that change every epoch</span></span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>    images <span class="op">=</span> jax.vmap(pix.flip_left_right)(images)</span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ... model training ...</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
</section>
<section id="summary-of-wins" class="level3">
<h3 class="anchored" data-anchor-id="summary-of-wins">Summary of Wins</h3>
<ol type="1">
<li><strong>Storage Read Speed:</strong> Reading a 224x224 file is instant compared to reading a full-res image.</li>
<li><strong>CPU Load:</strong> You eliminate the heavy <code>resize</code> math during training, freeing up the CPU to just feed data.</li>
<li><strong>Throughput:</strong> Your GPU will no longer wait for the CPU to finish resizing images.</li>
</ol>
</section>
</section>
<section id="shouldnt-i-use-a-class-rather-than-a-for-loop" class="level2">
<h2 class="anchored" data-anchor-id="shouldnt-i-use-a-class-rather-than-a-for-loop">Shouldn’t I use a class rather than a for loop?</h2>
<p>Yes, your intuition is correct. Creating a simple <strong>Dataset class</strong> is much cleaner and easier to manage than writing a raw loop over a DataFrame. It encapsulates all the “messy” logic (paths, cropping math, resizing) in one place.</p>
<p>Since this is an <strong>offline cleaning step</strong> (running once to save files to disk), you don’t even need to use Grain or PyTorch. You can use a standard Python class and <code>multiprocessing</code> to make it fly.</p>
<p>Here is the best way to structure this.</p>
<section id="the-cleaningdataset-class" class="level3">
<h3 class="anchored" data-anchor-id="the-cleaningdataset-class">1. The <code>CleaningDataset</code> Class</h3>
<p>This class handles one single job: “Give me an index, and I will give you the cleaned image and where to save it.”</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> skimage.transform <span class="im">import</span> resize</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CleaningDataset:</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, df, source_dir, target_dir, target_size<span class="op">=</span>(<span class="dv">224</span>, <span class="dv">224</span>)):</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.df <span class="op">=</span> df</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.source_dir <span class="op">=</span> source_dir</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.target_dir <span class="op">=</span> target_dir</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.target_size <span class="op">=</span> target_size</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Create target directory if it doesn't exist</span></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>        os.makedirs(<span class="va">self</span>.target_dir, exist_ok<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">len</span>(<span class="va">self</span>.df)</span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__getitem__</span>(<span class="va">self</span>, idx):</span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Returns (processed_image_array, save_path)"""</span></span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>        row <span class="op">=</span> <span class="va">self</span>.df.iloc[idx]</span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 1. Build Paths</span></span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Assuming your DF has a column 'filename'</span></span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a>        filename <span class="op">=</span> row[<span class="st">'filename'</span>] </span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a>        img_path <span class="op">=</span> os.path.join(<span class="va">self</span>.source_dir, filename)</span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a>        save_path <span class="op">=</span> os.path.join(<span class="va">self</span>.target_dir, filename)</span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 2. Load Image (Use PIL or skimage)</span></span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a>        <span class="cf">try</span>:</span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true" tabindex="-1"></a>            <span class="co"># converting to RGB prevents issues with Alpha channels or Grayscale</span></span>
<span id="cb7-33"><a href="#cb7-33" aria-hidden="true" tabindex="-1"></a>            img <span class="op">=</span> np.array(Image.<span class="bu">open</span>(img_path).convert(<span class="st">'RGB'</span>))</span>
<span id="cb7-34"><a href="#cb7-34" aria-hidden="true" tabindex="-1"></a>        <span class="cf">except</span> <span class="pp">Exception</span> <span class="im">as</span> e:</span>
<span id="cb7-35"><a href="#cb7-35" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"Error loading </span><span class="sc">{</span>filename<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>e<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb7-36"><a href="#cb7-36" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> <span class="va">None</span>, <span class="va">None</span></span>
<span id="cb7-37"><a href="#cb7-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-38"><a href="#cb7-38" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 3. Crop (BbCrop Logic)</span></span>
<span id="cb7-39"><a href="#cb7-39" aria-hidden="true" tabindex="-1"></a>        bbx, bby <span class="op">=</span> <span class="bu">int</span>(row[<span class="st">'bbx'</span>]), <span class="bu">int</span>(row[<span class="st">'bby'</span>])</span>
<span id="cb7-40"><a href="#cb7-40" aria-hidden="true" tabindex="-1"></a>        bbw, bbh <span class="op">=</span> <span class="bu">int</span>(row[<span class="st">'bbwidth'</span>]), <span class="bu">int</span>(row[<span class="st">'bbheight'</span>])</span>
<span id="cb7-41"><a href="#cb7-41" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb7-42"><a href="#cb7-42" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Safety check for bounds</span></span>
<span id="cb7-43"><a href="#cb7-43" aria-hidden="true" tabindex="-1"></a>        img_cropped <span class="op">=</span> img[bby:bby<span class="op">+</span>bbh, bbx:bbx<span class="op">+</span>bbw]</span>
<span id="cb7-44"><a href="#cb7-44" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb7-45"><a href="#cb7-45" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Handle cases where crop is empty (bad bbox data)</span></span>
<span id="cb7-46"><a href="#cb7-46" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> img_cropped.size <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb7-47"><a href="#cb7-47" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> <span class="va">None</span>, <span class="va">None</span></span>
<span id="cb7-48"><a href="#cb7-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-49"><a href="#cb7-49" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 4. Resize with Padding (Your logic)</span></span>
<span id="cb7-50"><a href="#cb7-50" aria-hidden="true" tabindex="-1"></a>        h, w, _ <span class="op">=</span> img_cropped.shape</span>
<span id="cb7-51"><a href="#cb7-51" aria-hidden="true" tabindex="-1"></a>        target_h, target_w <span class="op">=</span> <span class="va">self</span>.target_size</span>
<span id="cb7-52"><a href="#cb7-52" aria-hidden="true" tabindex="-1"></a>        scale <span class="op">=</span> <span class="bu">min</span>(target_h <span class="op">/</span> h, target_w <span class="op">/</span> w)</span>
<span id="cb7-53"><a href="#cb7-53" aria-hidden="true" tabindex="-1"></a>        new_h, new_w <span class="op">=</span> <span class="bu">int</span>(h <span class="op">*</span> scale), <span class="bu">int</span>(w <span class="op">*</span> scale)</span>
<span id="cb7-54"><a href="#cb7-54" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb7-55"><a href="#cb7-55" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Resize</span></span>
<span id="cb7-56"><a href="#cb7-56" aria-hidden="true" tabindex="-1"></a>        resized <span class="op">=</span> resize(img_cropped, (new_h, new_w), anti_aliasing<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb7-57"><a href="#cb7-57" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb7-58"><a href="#cb7-58" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Pad</span></span>
<span id="cb7-59"><a href="#cb7-59" aria-hidden="true" tabindex="-1"></a>        out_img <span class="op">=</span> np.zeros((target_h, target_w, <span class="dv">3</span>), dtype<span class="op">=</span>resized.dtype)</span>
<span id="cb7-60"><a href="#cb7-60" aria-hidden="true" tabindex="-1"></a>        y_off <span class="op">=</span> (target_h <span class="op">-</span> new_h) <span class="op">//</span> <span class="dv">2</span></span>
<span id="cb7-61"><a href="#cb7-61" aria-hidden="true" tabindex="-1"></a>        x_off <span class="op">=</span> (target_w <span class="op">-</span> new_w) <span class="op">//</span> <span class="dv">2</span></span>
<span id="cb7-62"><a href="#cb7-62" aria-hidden="true" tabindex="-1"></a>        out_img[y_off:y_off<span class="op">+</span>new_h, x_off:x_off<span class="op">+</span>new_w] <span class="op">=</span> resized</span>
<span id="cb7-63"><a href="#cb7-63" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb7-64"><a href="#cb7-64" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Convert back to uint8 (0-255) for saving! </span></span>
<span id="cb7-65"><a href="#cb7-65" aria-hidden="true" tabindex="-1"></a>        <span class="co"># skimage returns float (0-1), but we want to save space on disk</span></span>
<span id="cb7-66"><a href="#cb7-66" aria-hidden="true" tabindex="-1"></a>        final_img <span class="op">=</span> (out_img <span class="op">*</span> <span class="dv">255</span>).astype(np.uint8)</span>
<span id="cb7-67"><a href="#cb7-67" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb7-68"><a href="#cb7-68" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> final_img, save_path</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="the-execution-make-it-fast" class="level3">
<h3 class="anchored" data-anchor-id="the-execution-make-it-fast">2. The Execution (Make it Fast)</h3>
<p>Since you are resizing images, this is <strong>CPU bound</strong>. If you just use a <code>for</code> loop, it will run on a single core. To speed this up by 8x-16x, use <code>multiprocessing</code>.</p>
<p>Here is the script to run the dataset:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tqdm <span class="im">import</span> tqdm</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> concurrent.futures <span class="im">import</span> ProcessPoolExecutor</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Setup</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> CleaningDataset(</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>    df<span class="op">=</span>my_dataframe,</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>    source_dir<span class="op">=</span><span class="st">"./raw_images"</span>,</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>    target_dir<span class="op">=</span><span class="st">"./cleaned_images"</span></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> process_idx(i):</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Helper function for the parallel worker"""</span></span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>    img, path <span class="op">=</span> dataset[i]</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> img <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Save using PIL for easy compression control</span></span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>        Image.fromarray(img).save(path, quality<span class="op">=</span><span class="dv">95</span>)</span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="dv">1</span> <span class="co"># Success</span></span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">0</span> <span class="co"># Failure</span></span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Run in Parallel</span></span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">"__main__"</span>:</span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Use as many workers as you have CPU cores</span></span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> ProcessPoolExecutor() <span class="im">as</span> executor:</span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Map indices to the process function</span></span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a>        results <span class="op">=</span> <span class="bu">list</span>(tqdm(</span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a>            executor.<span class="bu">map</span>(process_idx, <span class="bu">range</span>(<span class="bu">len</span>(dataset))), </span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a>            total<span class="op">=</span><span class="bu">len</span>(dataset),</span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a>            desc<span class="op">=</span><span class="st">"Cleaning Images"</span></span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a>        ))</span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Done! Processed </span><span class="sc">{</span><span class="bu">sum</span>(results)<span class="sc">}</span><span class="ss"> images."</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-this-is-better-than-your-previous-approach" class="level3">
<h3 class="anchored" data-anchor-id="why-this-is-better-than-your-previous-approach">Why this is better than your previous approach:</h3>
<ol type="1">
<li><strong>Isolation:</strong> If your resizing logic changes, you edit one place (<code>__getitem__</code>).</li>
<li><strong>Speed:</strong> The <code>ProcessPoolExecutor</code> allows you to process 8, 16, or 32 images simultaneously (depending on your CPU cores), which is massive for image resizing.</li>
<li><strong>Memory:</strong> It processes one image, saves it, and dumps the memory. It never holds the whole dataset in RAM.</li>
</ol>
</section>
</section>
<section id="should-i-use-to-torchvision-then-since-grain-only-works-on-cpu" class="level2">
<h2 class="anchored" data-anchor-id="should-i-use-to-torchvision-then-since-grain-only-works-on-cpu">Should I use to TorchVision then since Grain only works on CPU?</h2>
<p>The short answer is <strong>no</strong>. You should <strong>not</strong> switch to TorchVision for this specific task.</p>
<p>Using TorchVision would likely make your <strong>offline cleaning step slower</strong> and your <strong>training setup more complicated</strong> (since you are using JAX/Grain).</p>
<p>Here is the breakdown of why using GPU/TorchVision is the wrong tool for these specific jobs.</p>
<section id="regarding-the-offline-cleaning-step-resizing-saving" class="level3">
<h3 class="anchored" data-anchor-id="regarding-the-offline-cleaning-step-resizing-saving">1. Regarding the Offline Cleaning Step (Resizing &amp; Saving)</h3>
<p>You asked if TorchVision on GPU would make this faster. <strong>It will generally be slower.</strong></p>
<ul>
<li><strong>The Bottleneck is Disk I/O, not Math:</strong> In a cleaning pipeline, the computer spends most of its time reading the file from the hard drive and writing the new file back. The actual math of resizing a 224x224 image is trivial for a CPU.</li>
<li><strong>The “Transfer Tax”:</strong> To use the GPU, you must:
<ol type="1">
<li>Load image to RAM (CPU).</li>
<li>Transfer image to GPU (PCIe bus).</li>
<li>Resize (GPU - fast).</li>
<li>Transfer back to RAM (PCIe bus).</li>
<li>Save to disk (CPU). For simple resizing, the time spent moving the data back and forth to the GPU takes longer than just doing the math on the CPU.</li>
</ol></li>
<li><strong>JPEG Encoding is mostly CPU:</strong> Standard TorchVision <code>save_image</code> still uses CPU libraries (like libpng or libjpeg) to encode the final file. You don’t gain speed there.</li>
</ul>
<p><strong>Winner:</strong> The <strong>CPU Multiprocessing</strong> script provided in the previous answer. It saturates your disk bandwidth, which is the physical speed limit.</p>
</section>
<section id="regarding-the-training-dataloader" class="level3">
<h3 class="anchored" data-anchor-id="regarding-the-training-dataloader">2. Regarding the Training DataLoader</h3>
<p>You are using <strong>JAX</strong> (indicated by your use of <code>dm-pix</code> and <code>jax.numpy</code>).</p>
<ul>
<li><strong>Ecosystem Clash:</strong> TorchVision is built for PyTorch. Grain is built for JAX/Flax. While you <em>can</em> use TorchVision with JAX, you have to manually convert PyTorch Tensors to Numpy/JAX arrays. This adds overhead and extra dependencies.</li>
<li><strong>GPU Transforms:</strong>
<ul>
<li><strong>TorchVision:</strong> Can run transforms on GPU (using <code>v2.transforms</code>).</li>
<li><strong>JAX (Your setup):</strong> You are <em>already</em> planning to use <strong>PIX</strong> (dm-pix) on the GPU.</li>
<li><strong>Result:</strong> Switching to TorchVision gives you no advantage because PIX already handles GPU acceleration natively in JAX.</li>
</ul></li>
</ul>
</section>
<section id="summary-the-fastest-architecture" class="level3">
<h3 class="anchored" data-anchor-id="summary-the-fastest-architecture">Summary: The Fastest Architecture</h3>
<p>Do not change your tools; just organize them correctly.</p>
<ol type="1">
<li><strong>Offline Cleaning (Step 1):</strong>
<ul>
<li><strong>Tool:</strong> Python + PIL/Numpy + Multiprocessing (CPU).</li>
<li><strong>Why:</strong> Parallel CPU cores handle disk I/O and simple resizing fastest without GPU transfer overhead.</li>
</ul></li>
<li><strong>Data Loading (Step 2):</strong>
<ul>
<li><strong>Tool:</strong> Grain (CPU).</li>
<li><strong>Why:</strong> Optimized for JAX, handles pre-fetching efficiently.</li>
</ul></li>
<li><strong>Augmentation (Step 3):</strong>
<ul>
<li><strong>Tool:</strong> DM-PIX (GPU).</li>
<li><strong>Why:</strong> Happens <em>after</em> the batch is loaded onto the GPU, leveraging JAX’s speed for complex math (rotations, color jitters).</li>
</ul></li>
</ol>
<p><strong>Stick with the plan:</strong> Use the <code>ProcessPoolExecutor</code> script for cleaning, and use Grain + PIX for training.</p>
</section>
</section>
<section id="what-is-the-best-strategy-for-data-augmentation-with-the-pix-httpsgithub.comgoogle-deepminddm_pix-library-to-train-a-classification-model-in-jax-using-the-nabirds-httpsdl.allaboutbirds.orgnabirds-dataset" class="level2">
<h2 class="anchored" data-anchor-id="what-is-the-best-strategy-for-data-augmentation-with-the-pix-httpsgithub.comgoogle-deepminddm_pix-library-to-train-a-classification-model-in-jax-using-the-nabirds-httpsdl.allaboutbirds.orgnabirds-dataset">What is the best strategy for data augmentation with the PIX (https://github.com/google-deepmind/dm_pix) library to train a classification model in JAX using the NABirds (https://dl.allaboutbirds.org/nabirds) dataset?</h2>
<p>For the <strong>NABirds</strong> dataset—a fine-grained classification task where distinguishing features are often subtle color patterns, beak shapes, or plumage textures—the “best” strategy balances increasing data diversity with preserving the precise visual cues required to distinguish species.</p>
<p>Since <strong><code>dm-pix</code></strong> (PIX) provides low-level, JAX-native image processing primitives, you must compose them into a pipeline yourself. Unlike libraries like <code>imgaug</code> or <code>torchvision</code>, PIX requires explicit management of JAX random keys (<code>PRNGKey</code>).</p>
<p>Here is the recommended strategy and a concrete implementation pattern.</p>
<section id="the-strategy-look-closer-dont-change-colors" class="level3">
<h3 class="anchored" data-anchor-id="the-strategy-look-closer-dont-change-colors">The Strategy: “Look Closer, Don’t Change Colors”</h3>
<ol type="1">
<li><strong>Geometric Augmentation (Crucial):</strong>
<ul>
<li><strong>Random Resized Crop:</strong> This is the most important augmentation for fine-grained tasks. It forces the model to learn from specific parts of the bird (head, wing, tail) rather than just the whole silhouette.</li>
<li><strong>Horizontal Flip:</strong> Birds are bilaterally symmetric (mostly); flipping is a safe way to double your dataset.</li>
<li><strong>Avoid Vertical Flips/90° Rotations:</strong> Unless the bird is flying, these orientations are unnatural and can confuse the model regarding gravity-dependent features.</li>
</ul></li>
<li><strong>Photometric Augmentation (Caution Needed):</strong>
<ul>
<li><strong>Brightness/Contrast:</strong> Safe and recommended to simulate different lighting conditions.</li>
<li><strong>Saturation:</strong> Use moderately.</li>
<li><strong>Hue:</strong> <strong>Avoid or use extremely sparingly.</strong> Distinguishing a “Scarlet Tanager” from a “Summer Tanager” relies heavily on exact shades of red. Shifting the hue too far invalidates the label.</li>
</ul></li>
<li><strong>Advanced (Batch-Level):</strong>
<ul>
<li><strong>MixUp / CutMix:</strong> While not strictly <em>inside</em> PIX, these are standard for fine-grained classification to prevent the model from memorizing backgrounds. You implement these on the batch <em>after</em> the PIX pipeline.</li>
</ul></li>
</ol>
<hr>
</section>
<section id="implementation-with-dm-pix" class="level3">
<h3 class="anchored" data-anchor-id="implementation-with-dm-pix">Implementation with <code>dm-pix</code></h3>
<p>Below is a JAX-compatible augmentation pipeline using <code>dm-pix</code>. This setup assumes you are using <code>pmap</code> or <code>vmap</code> for parallelism.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> jax</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> jax.numpy <span class="im">as</span> jnp</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> dm_pix <span class="im">as</span> pix</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> augment_image(key, image, training<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a><span class="co">    Applies data augmentation to a single image for NABirds.</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a><span class="co">        key: A jax.random.PRNGKey.</span></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a><span class="co">        image: A float32 JAX array of shape [H, W, 3] in range [0, 1].</span></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a><span class="co">        training: Boolean, whether to apply random augmentations.</span></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a><span class="co">        Augmented image of shape [224, 224, 3].</span></span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 1. Define target size (standard for ResNet/ViT)</span></span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>    target_h, target_w <span class="op">=</span> <span class="dv">224</span>, <span class="dv">224</span></span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> training:</span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Split keys for each stochastic operation</span></span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>        k_crop, k_flip, k_color <span class="op">=</span> jax.random.split(key, <span class="dv">3</span>)</span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a>        k_bright, k_cont, k_sat <span class="op">=</span> jax.random.split(k_color, <span class="dv">3</span>)</span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a>        <span class="co"># --- Geometric Transformations ---</span></span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Random Resized Crop (Simulated)</span></span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a>        <span class="co"># PIX's random_crop produces a crop of fixed size. To get "Inception-style" </span></span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a>        <span class="co"># random resized cropping, we typically crop randomly then resize.</span></span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Ideally, you crop a random region covering 50%-100% of the image.</span></span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Here is a simplified version using PIX primitives:</span></span>
<span id="cb9-32"><a href="#cb9-32" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb9-33"><a href="#cb9-33" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Random flip left/right (probability 0.5 is default)</span></span>
<span id="cb9-34"><a href="#cb9-34" aria-hidden="true" tabindex="-1"></a>        image <span class="op">=</span> pix.random_flip_left_right(k_flip, image)</span>
<span id="cb9-35"><a href="#cb9-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-36"><a href="#cb9-36" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Random Crop: For fine-grained, we want to zoom in.</span></span>
<span id="cb9-37"><a href="#cb9-37" aria-hidden="true" tabindex="-1"></a>        <span class="co"># We'll take a crop slightly larger than target and resize, </span></span>
<span id="cb9-38"><a href="#cb9-38" aria-hidden="true" tabindex="-1"></a>        <span class="co"># or just crop to target if image is large enough.</span></span>
<span id="cb9-39"><a href="#cb9-39" aria-hidden="true" tabindex="-1"></a>        <span class="co"># (Assuming input images are larger than 224x224)</span></span>
<span id="cb9-40"><a href="#cb9-40" aria-hidden="true" tabindex="-1"></a>        image <span class="op">=</span> pix.random_crop(k_crop, image, (target_h, target_w, <span class="dv">3</span>))</span>
<span id="cb9-41"><a href="#cb9-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-42"><a href="#cb9-42" aria-hidden="true" tabindex="-1"></a>        <span class="co"># --- Photometric Transformations (Color Jitter) ---</span></span>
<span id="cb9-43"><a href="#cb9-43" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb9-44"><a href="#cb9-44" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Adjust Brightness (delta usually roughly 0.1 to 0.2)</span></span>
<span id="cb9-45"><a href="#cb9-45" aria-hidden="true" tabindex="-1"></a>        image <span class="op">=</span> pix.random_brightness(k_bright, image, max_delta<span class="op">=</span><span class="fl">0.2</span>)</span>
<span id="cb9-46"><a href="#cb9-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-47"><a href="#cb9-47" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Adjust Contrast (factor between 0.8 and 1.2)</span></span>
<span id="cb9-48"><a href="#cb9-48" aria-hidden="true" tabindex="-1"></a>        image <span class="op">=</span> pix.random_contrast(k_cont, image, lower<span class="op">=</span><span class="fl">0.8</span>, upper<span class="op">=</span><span class="fl">1.2</span>)</span>
<span id="cb9-49"><a href="#cb9-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-50"><a href="#cb9-50" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Adjust Saturation (factor between 0.8 and 1.2)</span></span>
<span id="cb9-51"><a href="#cb9-51" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Avoid removing color entirely (factor=0) for birds</span></span>
<span id="cb9-52"><a href="#cb9-52" aria-hidden="true" tabindex="-1"></a>        image <span class="op">=</span> pix.random_saturation(k_sat, image, lower<span class="op">=</span><span class="fl">0.8</span>, upper<span class="op">=</span><span class="fl">1.2</span>)</span>
<span id="cb9-53"><a href="#cb9-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-54"><a href="#cb9-54" aria-hidden="true" tabindex="-1"></a>        <span class="co"># </span><span class="al">NOTE</span><span class="co">: Intentionally skipping random_hue to preserve species colors.</span></span>
<span id="cb9-55"><a href="#cb9-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-56"><a href="#cb9-56" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb9-57"><a href="#cb9-57" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Validation Strategy: Resize then Center Crop</span></span>
<span id="cb9-58"><a href="#cb9-58" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Note: 'resize' is usually done via jax.image.resize before calling this,</span></span>
<span id="cb9-59"><a href="#cb9-59" aria-hidden="true" tabindex="-1"></a>        <span class="co"># or we use pix.resize_with_crop_or_pad for simple centering.</span></span>
<span id="cb9-60"><a href="#cb9-60" aria-hidden="true" tabindex="-1"></a>        image <span class="op">=</span> pix.resize_with_crop_or_pad(image, target_h, target_w)</span>
<span id="cb9-61"><a href="#cb9-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-62"><a href="#cb9-62" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Standard ImageNet normalization (if model expects it)</span></span>
<span id="cb9-63"><a href="#cb9-63" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Mean and Std for ImageNet in [0, 1] range</span></span>
<span id="cb9-64"><a href="#cb9-64" aria-hidden="true" tabindex="-1"></a>    mean <span class="op">=</span> jnp.array([<span class="fl">0.485</span>, <span class="fl">0.456</span>, <span class="fl">0.406</span>])</span>
<span id="cb9-65"><a href="#cb9-65" aria-hidden="true" tabindex="-1"></a>    std <span class="op">=</span> jnp.array([<span class="fl">0.229</span>, <span class="fl">0.224</span>, <span class="fl">0.225</span>])</span>
<span id="cb9-66"><a href="#cb9-66" aria-hidden="true" tabindex="-1"></a>    image <span class="op">=</span> (image <span class="op">-</span> mean) <span class="op">/</span> std</span>
<span id="cb9-67"><a href="#cb9-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-68"><a href="#cb9-68" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> image</span>
<span id="cb9-69"><a href="#cb9-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-70"><a href="#cb9-70" aria-hidden="true" tabindex="-1"></a><span class="co"># Usage with vmap for batch processing</span></span>
<span id="cb9-71"><a href="#cb9-71" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> augment_batch(key, images, training<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb9-72"><a href="#cb9-72" aria-hidden="true" tabindex="-1"></a>    batch_size <span class="op">=</span> images.shape[<span class="dv">0</span>]</span>
<span id="cb9-73"><a href="#cb9-73" aria-hidden="true" tabindex="-1"></a>    keys <span class="op">=</span> jax.random.split(key, batch_size)</span>
<span id="cb9-74"><a href="#cb9-74" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> jax.vmap(augment_image, in_axes<span class="op">=</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="va">None</span>))(keys, images, training)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-this-configuration" class="level3">
<h3 class="anchored" data-anchor-id="why-this-configuration">Why this configuration?</h3>
<ol type="1">
<li><strong><code>random_crop</code> vs <code>center_crop</code></strong>: NABirds images often have the bird in the center, but not always. Random cropping helps the model become invariant to the bird’s position and, critically, acts as a “zoom” augmentation, training the model to recognize a bird by just its head or wing texture.</li>
<li><strong>Explicit Key Splitting</strong>: JAX is stateless. You generally cannot just call “random” functions. You must split the <code>PRNGKey</code> and pass a unique sub-key to every random operation (<code>random_brightness</code>, <code>random_flip</code>, etc.).</li>
<li><strong>Hue Exclusion</strong>: In generic object detection, a green car is still a car. In bird classification, a Red-winged Blackbird without the red wing is just a Blackbird (or a different species entirely). Omitting <code>random_hue</code> prevents generating false negatives.</li>
</ol>
</section>
<section id="advanced-tip-jax-performance" class="level3">
<h3 class="anchored" data-anchor-id="advanced-tip-jax-performance">Advanced Tip: JAX Performance</h3>
<p>PIX functions are written to be compiled. Ensure you wrap your training step (which includes this augmentation) in <code>jax.jit</code>. Because image augmentation creates a lot of intermediate arrays, JAX’s XLA compiler is very good at fusing these operations into a single GPU kernel, making this pipeline extremely fast compared to CPU-based loaders like standard PyTorch <code>DataLoader</code>.</p>
</section>
</section>
<section id="how-to-choose-which-augmentation-to-use" class="level2">
<h2 class="anchored" data-anchor-id="how-to-choose-which-augmentation-to-use">How to choose which augmentation to use?</h2>
<section id="the-standard-stack" class="level3">
<h3 class="anchored" data-anchor-id="the-standard-stack">The standard stack</h3>
<p>For 90% of classification tasks (e.g., ResNet/EfficientNet on natural images), use this baseline. It consists of <strong>3 distinct operations</strong>:</p>
<ol type="1">
<li><strong>Geometric (Spatial):</strong> Random Resized Crop (The single most important augmentation).</li>
<li><strong>Orientation:</strong> Horizontal Flip (If the object isn’t text or directional).</li>
<li><strong>Photometric (Color):</strong> Mild Color Jitter (Brightness/Contrast/Saturation).</li>
</ol>
<p><strong>Why this works:</strong> It forces the model to learn that the object is the same regardless of scale, position, orientation, or lighting.</p>
</section>
<section id="advanced-techniques-mixup-cutmix" class="level3">
<h3 class="anchored" data-anchor-id="advanced-techniques-mixup-cutmix">Advanced Techniques (MixUp / CutMix)</h3>
<p>Once you have your standard stack (Crop/Flip/Color), you can add <strong>one</strong> “regularization” technique on top. These are mixed into the batch training: * <strong>MixUp:</strong> Blending two images together. * <strong>CutMix:</strong> Cutting a square from one image and pasting it onto another.</p>
<p><strong>Recommendation:</strong> Do not count these as “standard” augmentations. Treat them as a final boosting layer. Use <strong>one</strong> of them if your validation loss is plateauing too early.</p>
</section>
<section id="summary-checklist" class="level3">
<h3 class="anchored" data-anchor-id="summary-checklist">Summary Checklist</h3>
<ol type="1">
<li><strong>Start with 3:</strong> Random Resized Crop + Horizontal Flip + Color Jitter.</li>
<li><strong>Visualize:</strong> Look at a batch of 32 images. If they look destroyed, reduce the intensity or probability.</li>
<li><strong>Use Automation:</strong> If using PyTorch or TensorFlow, implement <code>RandAugment</code> (N=2, M=9) and stop worrying about manual selection.</li>
</ol>
</section>
<section id="the-danger-of-too-many" class="level3">
<h3 class="anchored" data-anchor-id="the-danger-of-too-many">The danger of too many</h3>
<p>The biggest mistake beginners make is <strong>Sequential Stacking</strong>. * <strong>Bad:</strong> Apply Rotation AND Shear AND Blur AND Noise AND Gamma AND Jitter to <em>every</em> image. * <strong>Result:</strong> The image becomes a gray blob. The model learns nothing.</p>
<p><strong>The Solution:</strong> Use <strong>Probabilities</strong> or <strong>“OneOf”</strong> blocks. * <strong>Probability:</strong> Set <code>p=0.5</code> for each technique. The statistical chance of an image getting hit by <em>all 5</em> augmentations becomes very low (<span class="math inline">\(0.5^5 \approx 3\%\)</span>). * <strong>OneOf (Albumentations):</strong> Define a block of 3 techniques (e.g., Blur, Noise, Compression) and tell the pipeline: <em>“Pick exactly one of these to apply.”</em></p>
</section>
<section id="modern-best-practice-randaugment" class="level3">
<h3 class="anchored" data-anchor-id="modern-best-practice-randaugment">Modern Best Practice: “RandAugment”</h3>
<p>If you are struggling to decide how many to pick, stop guessing and use <strong>RandAugment</strong> (or TrivialAugment).</p>
<p>This is the standard for State-of-the-Art models today. instead of you manually picking “Rotation” and “Shear,” you set two parameters: 1. <strong>N (Number):</strong> How many transformations to apply sequentially (usually <strong>N=2</strong>). 2. <strong>M (Magnitude):</strong> How strong the effect is (usually <strong>M=9</strong> out of 10).</p>
<p>The algorithm then randomly selects 2 augmentations from a bank of 14 possibilities for each batch. This saves you from tuning hyperparameters.</p>
</section>
</section>
<section id="should-i-cropresize-my-eval-set-the-same-way" class="level2">
<h2 class="anchored" data-anchor-id="should-i-cropresize-my-eval-set-the-same-way">Should I crop/resize my eval set the same way?</h2>
<p>Yes, you <strong>should</strong> crop and resize the evaluation set to the bounding boxes, just as you did for the training set.</p>
<p>Since you trained your model on <strong>cropped images</strong> (showing only the bird), your model has learned to expect close-up, centered features. If you evaluate on full images (which include trees, sky, and background noise), your model will likely fail because the input data distribution will be completely different from what it learned.</p>
<section id="the-standard-protocols-for-fine-grained-classification-nabirds-cub-200" class="level3">
<h3 class="anchored" data-anchor-id="the-standard-protocols-for-fine-grained-classification-nabirds-cub-200">The Standard Protocols for Fine-Grained Classification (NABirds / CUB-200)</h3>
<p>In fine-grained visual classification research (like NABirds), there are two distinct evaluation settings. You are currently in the <strong>“Oracle”</strong> setting.</p>
<ol type="1">
<li><strong>Oracle Setting (Your current path):</strong>
<ul>
<li><strong>Method:</strong> You use the ground-truth bounding boxes during <em>both</em> training and evaluation.</li>
<li><strong>Goal:</strong> This tests the <strong>classification capacity</strong> of your model in isolation. It asks: <em>“Assuming we perfectly located the bird, can the model identify the species?”</em></li>
<li><strong>Action:</strong> Crop the evaluation set using the provided bounding boxes, then resize to <span class="math inline">\(224 \times 224\)</span>.</li>
</ul></li>
<li><strong>Realistic / “In the Wild” Setting:</strong>
<ul>
<li><strong>Method:</strong> You use bounding boxes during training, but <strong>not</strong> during evaluation.</li>
<li><strong>Goal:</strong> This tests the entire system’s ability to find and identify birds in raw images.</li>
<li><strong>Action:</strong> You would need a separate object detection model (like YOLO or Faster R-CNN) to predict bounding boxes on the test set first, and then feed those predicted crops into your classifier.</li>
</ul></li>
</ol>
</section>
<section id="recommended-preprocessing-steps" class="level3">
<h3 class="anchored" data-anchor-id="recommended-preprocessing-steps">Recommended Preprocessing Steps</h3>
<p>To get the most accurate evaluation of your current model, follow this pipeline for your evaluation set:</p>
<ol type="1">
<li><strong>Crop:</strong> Crop the image using the ground-truth bounding box coordinates.</li>
<li><strong>Resize (with Aspect Ratio preservation):</strong>
<ul>
<li><em>Ideally:</em> Resize the cropped bird image so the smaller edge is 256 pixels, then <strong>Center Crop</strong> to <span class="math inline">\(224 \times 224\)</span>. This prevents distorting the bird (e.g., making a fat bird look skinny).</li>
<li><em>Alternatively (if you did this in training):</em> Direct resize (squish) to <span class="math inline">\(224 \times 224\)</span>. If you squished images during training, you must squish them during evaluation.</li>
</ul></li>
<li><strong>Normalize:</strong> Apply the exact same mean and standard deviation normalization you used for the training set (usually ImageNet defaults).</li>
</ol>
</section>
<section id="summary" class="level3">
<h3 class="anchored" data-anchor-id="summary">Summary</h3>
<p>If you do not crop the evaluation set, your accuracy will drop significantly, not because the model is bad, but because it is looking at “background” data it was taught to ignore. <strong>Stick to the crops for a fair evaluation.</strong></p>
<p>Yes, you absolutely <strong>should</strong> apply the exact same “Crop <span class="math inline">\(\rightarrow\)</span> Proportional Resize <span class="math inline">\(\rightarrow\)</span> Save” steps to your evaluation set.</p>
<p>Because you have already baked a specific geometric transformation (proportional resizing with no squishing) into your training data files, your evaluation data must match that structure exactly to be valid.</p>
<p>Here is the breakdown of how your pipeline should look to ensure your evaluation is fair and accurate:</p>
</section>
<section id="the-save-to-disk-phase-preprocessing" class="level3">
<h3 class="anchored" data-anchor-id="the-save-to-disk-phase-preprocessing">1. The “Save to Disk” Phase (Preprocessing)</h3>
<p><strong>Action:</strong> Perform <strong>identical</strong> operations on both Training and Evaluation sets.</p>
<ul>
<li><strong>Crop:</strong> Crop to the bird’s bounding box.</li>
<li><strong>Resize:</strong> Perform your “proportional resize” to <span class="math inline">\(224 \times 224\)</span>.
<ul>
<li><em>Note:</em> Since you mentioned “no squishing,” you likely padded the images with black borders (or another color) to make them square, or you saved them as non-square files. Whatever logic you used (e.g., “fit within <span class="math inline">\(224 \times 224\)</span> and pad”), you <strong>must</strong> use the exact same logic for the evaluation set.</li>
<li><strong>Why?</strong> If your model learned to see birds with “black bars” (padding) or specific aspect ratios during training, it will be confused if the evaluation images don’t have them.</li>
</ul></li>
</ul>
</section>
<section id="the-load-augment-phase-runtime" class="level3">
<h3 class="anchored" data-anchor-id="the-load-augment-phase-runtime">2. The “Load &amp; Augment” Phase (Runtime)</h3>
<p><strong>Action:</strong> This is where the two sets <strong>diverge</strong>.</p>
<p>Even though the files on disk were created the same way, how you load them into your model differs:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Feature</th>
<th style="text-align: left;">Training Pipeline</th>
<th style="text-align: left;">Evaluation Pipeline</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Input</strong></td>
<td style="text-align: left;">Your pre-processed <span class="math inline">\(224 \times 224\)</span> files</td>
<td style="text-align: left;">Your pre-processed <span class="math inline">\(224 \times 224\)</span> files</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Augmentation</strong></td>
<td style="text-align: left;">Random Flip, Color Jitter, etc.</td>
<td style="text-align: left;"><strong>None</strong> (or Center Crop if needed)</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Normalization</strong></td>
<td style="text-align: left;">Standard (e.g., ImageNet mean/std)</td>
<td style="text-align: left;"><strong>Same</strong> (ImageNet mean/std)</td>
</tr>
</tbody>
</table>
</section>
<section id="a-critical-warning-on-resolution" class="level3">
<h3 class="anchored" data-anchor-id="a-critical-warning-on-resolution">A Critical Warning on Resolution</h3>
<p>You mentioned you resized to <span class="math inline">\(224 \times 224\)</span> <em>before</em> saving to disk. * <strong>The Risk:</strong> Standard training often uses <strong>RandomResizedCrop</strong>, which zooms into a random part of the image. If your starting image is already only <span class="math inline">\(224 \times 224\)</span>, zooming in further will result in very blurry, low-resolution crops (e.g., a <span class="math inline">\(150 \times 150\)</span> crop stretched back to <span class="math inline">\(224 \times 224\)</span>). * <strong>The Fix:</strong> If you plan to use <code>RandomResizedCrop</code> during training, it is usually better to save your images at a <strong>larger size</strong> (e.g., <span class="math inline">\(400 \times 400\)</span> or <span class="math inline">\(512 \times 512\)</span>) and let the data loader handle the final resize to <span class="math inline">\(224\)</span>. * <strong>If you are NOT cropping during training:</strong> If your augmentation is just flipping/rotating (no zooming), then your current method is perfectly fine.</p>
<p><strong>Summary:</strong> Yes, run the exact same <code>skimage</code> proportional resize script on your evaluation folder before running your evaluation script.</p>
</section>
</section>
<section id="should-i-random-crop" class="level2">
<h2 class="anchored" data-anchor-id="should-i-random-crop">Should I random crop?</h2>
<p>For the <strong>NABirds</strong> dataset, using the “Random Resized Crop” (RRC) augmentation during training typically yields <strong>better results</strong> than training on static, perfectly centered crops, but you must be careful with how aggressive it is.</p>
<p>Since you are working with fine-grained classification (where the difference between two species might just be the beak shape or wing bar), here is the specific breakdown of how to handle this for the best performance.</p>
<section id="the-short-answer" class="level3">
<h3 class="anchored" data-anchor-id="the-short-answer">1. The Short Answer</h3>
<ul>
<li><strong>Training Set:</strong> Yes, use <strong>Random Resized Crop</strong>, but with a “gentler” scale range (e.g., 0.5 to 1.0) so you don’t cut off too many identifying features.</li>
<li><strong>Evaluation Set:</strong> No.&nbsp;Use <strong>Deterministic Resize</strong> (resize the bounding box crop directly to 224x224).</li>
</ul>
</section>
<section id="why-random-cropping-helps-even-for-birds" class="level3">
<h3 class="anchored" data-anchor-id="why-random-cropping-helps-even-for-birds">2. Why Random Cropping Helps (Even for Birds)</h3>
<p>You might think that because you have bounding boxes, you should just show the model the perfect “bird box” every time. However, this often leads to overfitting. * <strong>The Problem with Static Crops:</strong> If you only train on the perfect bounding box, the model expects the bird to always be perfectly framed. If your test set crop is slightly off (or if you eventually deploy this model on real-world images where your detector isn’t perfect), the model will fail. * <strong>The Solution:</strong> By using RandomResizedCrop on the training set, you simulate “imperfect” detection. The model learns to recognize the bird even if it’s slightly zoomed in, slightly off-center, or if the tail is cut off.</p>
</section>
<section id="the-trap-in-your-current-workflow" class="level3">
<h3 class="anchored" data-anchor-id="the-trap-in-your-current-workflow">3. The “Trap” in Your Current Workflow</h3>
<p>You mentioned you <strong>already saved</strong> your training images as 224x224 files. * <strong>The Issue:</strong> Standard <code>RandomResizedCrop</code> works by taking a crop (e.g., 50% of the image) and resizing it <em>down</em> to 224x224. * <strong>Your Constraint:</strong> Since your files are <em>already</em> 224x224, if you try to crop 50% of the image, you are taking a 112x112 patch and stretching it back up to 224x224. This creates a blurry, pixelated image that destroys the fine details (feather patterns) necessary for identifying bird species.</p>
</section>
<section id="recommended-pipeline-for-best-results" class="level3">
<h3 class="anchored" data-anchor-id="recommended-pipeline-for-best-results">4. Recommended Pipeline for Best Results</h3>
<p>To get state-of-the-art results on NABirds, you should ideally modify your pipeline to perform cropping <strong>online</strong> (during training) rather than <strong>offline</strong> (saving to disk).</p>
<section id="a.-the-gold-standard-pipeline" class="level4">
<h4 class="anchored" data-anchor-id="a.-the-gold-standard-pipeline">A. The “Gold Standard” Pipeline</h4>
<p>If you can reload your original images, do this: 1. <strong>Load</strong> the original high-res image. 2. <strong>Crop</strong> to the bounding box (plus a small margin, e.g., 10-20% context, which helps significantly with birds). 3. <strong>Apply Augmentation (Train only):</strong> * <code>RandomResizedCrop(224, scale=(0.7, 1.0))</code> -&gt; <em>Note the high minimum scale. Don’t go down to 0.08 like ImageNet.</em> * <code>RandomHorizontalFlip()</code> 4. <strong>Evaluation Phase:</strong> * Crop to Bounding Box. * <code>Resize(256)</code> -&gt; <code>CenterCrop(224)</code> OR just <code>Resize((224, 224))</code>.</p>
</section>
<section id="b.-the-fixed-files-pipeline-if-you-must-stick-to-your-saved-224x224-files" class="level4">
<h4 class="anchored" data-anchor-id="b.-the-fixed-files-pipeline-if-you-must-stick-to-your-saved-224x224-files">B. The “Fixed Files” Pipeline (If you must stick to your saved 224x224 files)</h4>
<p>If you cannot redo the dataset creation, do <strong>not</strong> use <code>RandomResizedCrop</code>. It will blur your images too much. Instead use: * <strong>Training:</strong> <code>RandomCrop(224, padding=28)</code> (this pads the image slightly and shifts it around without losing resolution) + <code>RandomHorizontalFlip</code>. * <strong>Evaluation:</strong> Standard <code>Resize((224, 224))</code> of the bounding box crop.</p>
</section>
</section>
</section>
<section id="should-i-save-to-disk-after-cropping-to-bb" class="level2">
<h2 class="anchored" data-anchor-id="should-i-save-to-disk-after-cropping-to-bb">Should I save to disk after cropping to bb?</h2>
<p>Yes, you <strong>should</strong> save the cropped images to disk to speed up training, but you must be careful about <strong>when</strong> you resize them.</p>
<p>For the best results on the NABirds dataset, use this specific “Hybrid” approach:</p>
<section id="the-recommended-workflow-1" class="level3">
<h3 class="anchored" data-anchor-id="the-recommended-workflow-1">The Recommended Workflow</h3>
<ol type="1">
<li><strong>Preprocessing Step (Run Once):</strong>
<ul>
<li><strong>Crop:</strong> Crop the original high-resolution image to the bounding box (plus a small margin, e.g., 10-20%).</li>
<li><strong>Action:</strong> Save this cropped image to disk at its <strong>native resolution</strong> (e.g., if the crop is <span class="math inline">\(600 \times 600\)</span>, save it as <span class="math inline">\(600 \times 600\)</span>).</li>
<li><strong>Do NOT Resize yet:</strong> Do <em>not</em> shrink these to <span class="math inline">\(224 \times 224\)</span> during this step.</li>
</ul></li>
<li><strong>Training Step (Run Many Times):</strong>
<ul>
<li><strong>Load:</strong> Your dataloader reads the saved (smaller, faster) crop files.</li>
<li><strong>Augment:</strong> Apply <code>RandomResizedCrop(224)</code> on these loaded images.</li>
<li><em>Why this works:</em> Since your saved files are still large (e.g., <span class="math inline">\(500\)</span>+ pixels), the random cropper has plenty of pixels to zoom in and out of without making the image blurry.</li>
</ul></li>
<li><strong>Evaluation Step:</strong>
<ul>
<li><strong>Load:</strong> Read the same saved crop files.</li>
<li><strong>Resize:</strong> Apply <code>Resize(256)</code> <span class="math inline">\(\rightarrow\)</span> <code>CenterCrop(224)</code>.</li>
</ul></li>
</ol>
</section>
<section id="why-this-is-better-than-your-current-plan" class="level3">
<h3 class="anchored" data-anchor-id="why-this-is-better-than-your-current-plan">Why this is better than your current plan</h3>
<p>If you resize to <span class="math inline">\(224 \times 224\)</span> <em>before</em> saving to disk, you lock your data into a low resolution. If you then try to use data augmentation (like zooming in on the head), you are zooming into an already pixelated image, which will look blurry and hurt your model’s accuracy on fine-grained bird features.</p>
<p><strong>Summary:</strong> Save the <strong>crops</strong> to disk (for speed), but keep them <strong>large</strong> (for quality).</p>
</section>
</section>
<section id="should-i-just-skip-the-cropping-part-entirely" class="level2">
<h2 class="anchored" data-anchor-id="should-i-just-skip-the-cropping-part-entirely">Should I just skip the cropping part entirely?</h2>
<p>It is generally <strong>better to crop and save to disk</strong> (the “offline” method) rather than doing it on-the-fly, primarily for <strong>training speed</strong>.</p>
<p>However, in terms of <strong>model accuracy</strong>, both methods are identical if implemented correctly.</p>
<p>Here is the breakdown of the trade-offs so you can decide which fits your setup:</p>
<section id="training-speed-winner-save-to-disk" class="level3">
<h3 class="anchored" data-anchor-id="training-speed-winner-save-to-disk">1. Training Speed (Winner: Save to Disk)</h3>
<p>This is the main reason researchers pre-crop datasets like NABirds or CUB-200. * <strong>The Problem with “On-the-Fly”:</strong> If your original image is 4000x3000 pixels (12MP) but the bird only occupies a 500x500 region, your computer has to load a huge file, decode the entire JPEG, and then throw away 90% of the pixels <em>every single time</em> the model sees that image (thousands of times during training). This creates a massive bottleneck on your CPU and Hard Drive/SSD, often leaving your GPU waiting for data. * <strong>The Benefit of “Save to Disk”:</strong> By saving the crop (e.g., a 50KB file instead of a 5MB file), you reduce the I/O load significantly. Your training epochs will likely run 2x to 5x faster.</p>
</section>
<section id="flexibility-winner-on-the-fly" class="level3">
<h3 class="anchored" data-anchor-id="flexibility-winner-on-the-fly">2. Flexibility (Winner: On-the-Fly)</h3>
<ul>
<li><strong>The Benefit:</strong> If you decide later that you want to include <em>more</em> background context (e.g., you want the crop to be 1.5x the size of the bounding box instead of 1.1x), you can just change a number in your code.</li>
<li><strong>The Downside of “Save to Disk”:</strong> If you saved tight crops and realize later you needed more background, you have to re-run your preprocessing script to generate the dataset again.</li>
</ul>
</section>
<section id="accuracy-tie" class="level3">
<h3 class="anchored" data-anchor-id="accuracy-tie">3. Accuracy (Tie)</h3>
<p>As long as the pixels feeding into the neural network are the same, the model doesn’t care if they were cropped 5 minutes ago or 5 milliseconds ago.</p>
<hr>
</section>
<section id="the-sweet-spot-recommendation" class="level3">
<h3 class="anchored" data-anchor-id="the-sweet-spot-recommendation">The “Sweet Spot” Recommendation</h3>
<p>To get the best of both worlds (Speed + Flexibility), stick to the <strong>“High-Res Crop with Margin”</strong> strategy I mentioned earlier:</p>
<ol type="1">
<li><strong>Script:</strong> Load Original Image.</li>
<li><strong>Crop:</strong> Extract the bounding box <strong>+ 20% margin</strong> (extra background).</li>
<li><strong>Save:</strong> Save this as a high-quality JPG/PNG to a new folder.</li>
<li><strong>Train:</strong> Point your dataloader at this new folder.</li>
</ol>
<p><strong>Why this wins:</strong> * <strong>Speed:</strong> You get the speed boost because you aren’t loading the full megapixels of the original photo. * <strong>Flexibility:</strong> The 20% margin gives your <code>RandomResizedCrop</code> augmentation enough “wiggle room” to shift the bird around or zoom out slightly without hitting the edge of the image.</p>
<p><strong>Verdict:</strong> Don’t skip the “save to disk” step unless your dataset is very small or you have an incredibly fast CPU/SSD combo that can handle decoding full images in real-time without slowing down the GPU.</p>
</section>
</section>
<section id="details-for-the-eval-set" class="level2">
<h2 class="anchored" data-anchor-id="details-for-the-eval-set">Details for the eval set</h2>
<p>Here is the detailed breakdown for the <strong>Evaluation Set</strong> within the “High-Res Crop <span class="math inline">\(\rightarrow\)</span> Save <span class="math inline">\(\rightarrow\)</span> Load” workflow.</p>
<p>This approach ensures you evaluate on the highest quality data while avoiding the risk of cutting off the bird’s head or tail (which can happen with standard center cropping on rectangular birds).</p>
<section id="phase-1-preprocessing-saving-to-disk" class="level3">
<h3 class="anchored" data-anchor-id="phase-1-preprocessing-saving-to-disk">Phase 1: Preprocessing (Saving to Disk)</h3>
<p><strong>Rule:</strong> You must treat the evaluation set <strong>exactly</strong> like the training set during the saving phase.</p>
<p>If you saved your training data with a 20% margin, you must save your evaluation data with a 20% margin. If you don’t, the model will see the bird “closer up” during evaluation than it did during training, which will skew your results.</p>
<ul>
<li><strong>Input:</strong> Original Test Images + Bounding Boxes.</li>
<li><strong>Action:</strong>
<ol type="1">
<li>Calculate Bounding Box coordinates.</li>
<li>Expand coordinates by your chosen margin (e.g., 20%).</li>
<li>Crop the image.</li>
<li><strong>Save to disk at native resolution</strong> (Do not resize to 224 yet).</li>
</ol></li>
</ul>
</section>
<section id="phase-2-the-runtime-pipeline-the-code" class="level3">
<h3 class="anchored" data-anchor-id="phase-2-the-runtime-pipeline-the-code">Phase 2: The Runtime Pipeline (The Code)</h3>
<p>This is where the Evaluation strategy deviates from Training.</p>
<p>Since you are working with <strong>crops of birds</strong> (which are often rectangular—think of a standing heron vs.&nbsp;a flying duck), using the standard <code>CenterCrop</code> can be dangerous because it might chop off the head or tail.</p>
<p>Instead, for fine-grained Bounding Box classification, the standard approach is a <strong>Direct Resize (Squish)</strong>.</p>
<section id="the-code-pytorch-example" class="level4">
<h4 class="anchored" data-anchor-id="the-code-pytorch-example">The Code (PyTorch Example)</h4>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision <span class="im">import</span> transforms</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Training Transforms (For context)</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="co"># We use RandomResizedCrop here. This teaches the model to handle</span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="co"># zoom, missing parts, and ASPECT RATIO DISTORTION (squishing).</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>train_transforms <span class="op">=</span> transforms.Compose([</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>    transforms.RandomResizedCrop(<span class="dv">224</span>, scale<span class="op">=</span>(<span class="fl">0.7</span>, <span class="fl">1.0</span>)), </span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>    transforms.RandomHorizontalFlip(),</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>    transforms.ToTensor(),</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>    transforms.Normalize(mean<span class="op">=</span>[<span class="fl">0.485</span>, <span class="fl">0.456</span>, <span class="fl">0.406</span>], std<span class="op">=</span>[<span class="fl">0.229</span>, <span class="fl">0.224</span>, <span class="fl">0.225</span>])</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Evaluation Transforms (The Recommendation)</span></span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a><span class="co"># We use a direct Resize.</span></span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>val_transforms <span class="op">=</span> transforms.Compose([</span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Resize directly to 224x224. </span></span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># This ensures the WHOLE bird (and your margin) is visible.</span></span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>    transforms.Resize((<span class="dv">224</span>, <span class="dv">224</span>)), </span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a>    transforms.ToTensor(),</span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a>    transforms.Normalize(mean<span class="op">=</span>[<span class="fl">0.485</span>, <span class="fl">0.456</span>, <span class="fl">0.406</span>], std<span class="op">=</span>[<span class="fl">0.229</span>, <span class="fl">0.224</span>, <span class="fl">0.225</span>])</span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
</section>
<section id="why-direct-resize-instead-of-resize-crop" class="level3">
<h3 class="anchored" data-anchor-id="why-direct-resize-instead-of-resize-crop">Why “Direct Resize” instead of “Resize &amp; Crop”?</h3>
<p>You might be worried that <code>Resize((224, 224))</code> distorts the aspect ratio (squishing a wide bird to be square). Here is why this is actually the best choice for this specific workflow:</p>
<ol type="1">
<li><strong>Safety:</strong> If you use the standard ImageNet method (<code>Resize(256) -&gt; CenterCrop(224)</code>), you risk cutting off features.
<ul>
<li><em>Example:</em> You have a wide image of a flying eagle (<span class="math inline">\(400 \times 200\)</span>).</li>
<li>If you resize the short edge to <span class="math inline">\(256\)</span>, the image becomes <span class="math inline">\(512 \times 256\)</span>.</li>
<li>If you then <code>CenterCrop(224)</code>, you only see the middle feathers. The head and tail are gone. The model cannot identify the bird.</li>
</ul></li>
<li><strong>Robustness:</strong> Because your <strong>Training Set</strong> uses <code>RandomResizedCrop</code>, your model has already learned to be robust to aspect ratio changes. <code>RandomResizedCrop</code> inherently stretches and squishes images as it selects random areas. Therefore, the model will not be confused by the “squish” in the evaluation set.</li>
</ol>
</section>
<section id="alternative-the-black-bars-method-padding" class="level3">
<h3 class="anchored" data-anchor-id="alternative-the-black-bars-method-padding">Alternative: The “Black Bars” Method (Padding)</h3>
<p>If you absolutely cannot tolerate squishing (distortion) because you feel the beak shape is being ruined, you can use a “Letterbox” resize: 1. Resize the longest edge to 224. 2. Pad the shorter edge with black pixels (zeros) to reach 224.</p>
<p><em>However, for most modern CNNs (ResNet, EfficientNet, ViT) trained with standard augmentation, the <strong>Direct Resize</strong> method described above usually yields higher accuracy than padding.</em></p>
</section>
<section id="summary-of-eval-workflow" class="level3">
<h3 class="anchored" data-anchor-id="summary-of-eval-workflow">Summary of Eval Workflow</h3>
<ol type="1">
<li><strong>File on Disk:</strong> High-res crop of the bird + 20% background.</li>
<li><strong>Dataloader:</strong> Loads file <span class="math inline">\(\rightarrow\)</span> Squishes to <span class="math inline">\(224 \times 224\)</span> <span class="math inline">\(\rightarrow\)</span> Normalizes.</li>
<li><strong>Model:</strong> Predicts class.</li>
</ol>
</section>
</section>
<section id="which-model-to-use" class="level2">
<h2 class="anchored" data-anchor-id="which-model-to-use">Which model to use?</h2>
<p>To train a model on the NABirds dataset, you should start with <strong>ResNet-50</strong> pre-trained on ImageNet.</p>
<p>For a specific task like identifying 400+ species of birds, you need a balance between a model that is powerful enough to see fine details (like beak shape or wing patterns) and one that is easy to train without requiring a supercomputer.</p>
<section id="the-recommendation-resnet-50" class="level3">
<h3 class="anchored" data-anchor-id="the-recommendation-resnet-50">The Recommendation: ResNet-50</h3>
<ul>
<li><strong>Why start here?</strong> ResNet-50 is the “gold standard” baseline in computer vision. It is deep enough to capture complex features but stable enough that it rarely fails to converge.</li>
<li><strong>Availability:</strong> Every major deep learning library (PyTorch, TensorFlow/Keras, FastAI) has it built-in with a single line of code.</li>
<li><strong>Performance:</strong> You can expect remarkably high accuracy (often &gt;85-90%) on bird datasets just by fine-tuning this model.</li>
</ul>
</section>
<section id="the-pro-alternative-efficientnet-b0-or-b2" class="level3">
<h3 class="anchored" data-anchor-id="the-pro-alternative-efficientnet-b0-or-b2">The “Pro” Alternative: EfficientNet (B0 or B2)</h3>
<p>If you are comfortable with slightly more modern architectures, <strong>EfficientNet-B0</strong> or <strong>EfficientNet-B2</strong> are excellent choices. * <strong>Pros:</strong> They often achieve higher accuracy than ResNet while using fewer parameters (making the model smaller and faster). * <strong>Cons:</strong> They can sometimes be a bit trickier to tune (e.g., they are sensitive to learning rates and optimizers) compared to the robust ResNet.</p>
<hr>
</section>
<section id="step-by-step-strategy-for-nabirds" class="level3">
<h3 class="anchored" data-anchor-id="step-by-step-strategy-for-nabirds">Step-by-Step Strategy for NABirds</h3>
<p>Since NABirds is a <strong>fine-grained classification</strong> task (distinguishing between very similar sub-species), training from scratch will likely fail. You must use <strong>Transfer Learning</strong>.</p>
<section id="use-pre-trained-weights-crucial" class="level4">
<h4 class="anchored" data-anchor-id="use-pre-trained-weights-crucial">1. Use Pre-trained Weights (Crucial)</h4>
<p>Do not initialize your model with random weights. Start with weights pre-trained on <strong>ImageNet</strong>. * <strong>Context:</strong> ImageNet contains 1,000 classes, including many birds (e.g., “goldfinch,” “robin,” “eagle”). Your model will start already knowing what a feather, a beak, and a wing look like. * <strong>Advanced Tip:</strong> If you want to squeeze out the absolute best performance later, look for model weights pre-trained on the <strong>iNaturalist</strong> dataset. This dataset is entirely nature-focused and handles the “long tail” of rare species better than ImageNet, but these weights are harder to find and load than standard ImageNet ones.</p>
</section>
<section id="resolution-matters" class="level4">
<h4 class="anchored" data-anchor-id="resolution-matters">2. Resolution Matters</h4>
<p>Birds are often small in the frame or distinguished by tiny markings. * <strong>Standard:</strong> Standard models use <span class="math inline">\(224 \times 224\)</span> pixel inputs. * <strong>Better for Birds:</strong> If your GPU memory allows, try increasing the input size to <strong><span class="math inline">\(448 \times 448\)</span></strong>. This is often the single most effective trick for boosting accuracy in fine-grained bird classification.</p>
</section>
<section id="data-augmentation" class="level4">
<h4 class="anchored" data-anchor-id="data-augmentation">3. Data Augmentation</h4>
<p>Bird photos are taken in various lighting conditions and angles. You should use aggressive data augmentation during training to prevent overfitting: * <strong>Essential:</strong> Random rotation, horizontal flips (a bird facing left is the same species as one facing right), and brightness/contrast adjustments. * <strong>Useful:</strong> Random cropping (zooming in) to simulate different distances.</p>
</section>
</section>
<section id="code-snippet-pytorch-example" class="level3">
<h3 class="anchored" data-anchor-id="code-snippet-pytorch-example">Code Snippet (PyTorch Example)</h3>
<p>Here is how you would load the recommended starting point in PyTorch:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchvision.models <span class="im">as</span> models</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Load the ResNet50 model with ImageNet weights</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>weights <span class="op">=</span> models.ResNet50_Weights.DEFAULT</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> models.resnet50(weights<span class="op">=</span>weights)</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Modify the final layer for NABirds (which has roughly 400-555 classes depending on version)</span></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Replace 'num_classes' with the exact number of species in your dataset split</span></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>num_classes <span class="op">=</span> <span class="dv">555</span> </span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>model.fc <span class="op">=</span> nn.Linear(model.fc.in_features, num_classes)</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. Now you are ready to train (fine-tune)</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Summary:</strong> Start with <strong>ResNet-50</strong> using <strong>ImageNet weights</strong>. It is the path of least resistance and will give you a high-quality baseline. If you need more speed, swap to EfficientNet-B0. If you need more accuracy, increase image resolution.</p>
</section>
</section>
<section id="how-to-do-it-in-jax" class="level2">
<h2 class="anchored" data-anchor-id="how-to-do-it-in-jax">How to do it in JAX?</h2>
<p>It is excellent that you are looking to use <strong>JAX</strong>. It is a powerful framework, but for a task like NABirds (fine-grained classification), you face a specific challenge: <strong>Transfer Learning</strong>.</p>
<p>To get high accuracy on birds, you cannot train from scratch; you must start with a model pre-trained on ImageNet. While <strong>Bonsai</strong> contains an implementation of EfficientNet, it is a “minimal, lightweight” library. As of late 2024, it often lacks the robust “model zoo” features (like automatic downloading and aligning of pre-trained weights) that libraries like PyTorch’s <code>timm</code> provide.</p>
<p>Here is the best way to get started with EfficientNet in the JAX ecosystem for your specific dataset.</p>
<section id="the-challenge-with-bonsai" class="level3">
<h3 class="anchored" data-anchor-id="the-challenge-with-bonsai">1. The Challenge with Bonsai</h3>
<p>If you use the <code>jax-ml/bonsai</code> repository directly, you will get the EfficientNet <em>architecture</em>, but likely initialized with random weights. Training this on NABirds will probably result in poor performance (likely &lt;50% accuracy) because the model hasn’t learned the basic features of “seeing” (edges, textures, shapes) that it gets from ImageNet.</p>
<p><strong>Recommendation:</strong> Unless you want to manually write a script to port weights from PyTorch to JAX, <strong>do not start with Bonsai alone</strong>.</p>
</section>
<section id="the-solution-efficientnet-jax-or-jimm" class="level3">
<h3 class="anchored" data-anchor-id="the-solution-efficientnet-jax-or-jimm">2. The Solution: <code>efficientnet-jax</code> (or <code>jimm</code>)</h3>
<p>The standard, battle-tested way to use EfficientNet in JAX with pre-trained weights is the <strong><a href="https://github.com/rwightman/efficientnet-jax">efficientnet-jax</a></strong> repository. It is maintained by Ross Wightman (the creator of the famous <code>timm</code> library) and specifically supports <strong>Flax</strong> (the neural network library for JAX).</p>
<p>This library allows you to load ImageNet weights that have been ported to JAX, which is exactly what you need.</p>
</section>
<section id="step-by-step-implementation-guide" class="level3">
<h3 class="anchored" data-anchor-id="step-by-step-implementation-guide">3. Step-by-Step Implementation Guide</h3>
<section id="step-1-install-the-necessary-libraries" class="level4">
<h4 class="anchored" data-anchor-id="step-1-install-the-necessary-libraries">Step 1: Install the necessary libraries</h4>
<p>You will need JAX, Flax, Optax (for optimization), and the EfficientNet library.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb12"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install jax jaxlib flax optax efficientnet-jax</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="step-2-load-the-pre-trained-model" class="level4">
<h4 class="anchored" data-anchor-id="step-2-load-the-pre-trained-model">Step 2: Load the Pre-trained Model</h4>
<p>Here is how you initialize the model with ImageNet weights and modify it for NABirds (which has ~555 classes).</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> jax.numpy <span class="im">as</span> jnp</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> efficientnet_jax <span class="im">import</span> EfficientNetB0</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Initialize the model with pre-trained ImageNet weights</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="co"># 'pretrained=True' downloads and loads the weights automatically.</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> EfficientNetB0(pretrained<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Inspect the model structure (optional)</span></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a><span class="co"># In Flax/JAX, models are stateless. You get a 'params' dictionary.</span></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a><span class="co"># To use it for NABirds, we need to replace the final layer.</span></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a><span class="co"># However, efficientnet-jax is often used via its functional API or Linen wrapper.</span></span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a><span class="co"># A more robust Flax pattern for Transfer Learning:</span></span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> flax <span class="im">import</span> linen <span class="im">as</span> nn</span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> BirdModel(nn.Module):</span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>    num_classes: <span class="bu">int</span></span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a>    <span class="at">@nn.compact</span></span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, x, training: <span class="bu">bool</span>):</span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Load the base EfficientNet (features only)</span></span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Note: In a real script, you would load the pretrained backbone state here</span></span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># For simplicity, many JAX users use the 'rwightman/efficientnet-jax' </span></span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># specifically for the .apply function or similar logic.</span></span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a>        <span class="co"># ... (There is slightly more boilerplate in JAX than PyTorch for this) ...</span></span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">pass</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>Wait—Simpler Alternative for Beginners:</strong> Because JAX is “purely functional,” handling state dictionaries for transfer learning can be verbose. If you are new to JAX, the easiest path that still uses the JAX stack is to use <strong>Hugging Face Transformers</strong> with a Flax backend, <em>if available</em>, or simply <strong>PyTorch for data loading + JAX for training</strong>.</p>
<p>However, since you want a pure JAX stack, here is the cleanest current snippet using <code>efficientnet-jax</code>:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> jax</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> jax.numpy <span class="im">as</span> jnp</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> efficientnet_jax <span class="im">import</span> load_pretrained_model</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Load EfficientNet-B0 with ImageNet weights</span></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a><span class="co"># This function handles the messy weight porting for you</span></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>model, params <span class="op">=</span> load_pretrained_model(<span class="st">"efficientnet-b0"</span>)</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a><span class="co"># The 'model' is a Flax Linen module.</span></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a><span class="co"># You can now wrap this or modify the 'params' to change the final layer.</span></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a><span class="co"># The final layer weights in EfficientNet are usually keys like 'predictions/kernel' </span></span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a><span class="co"># or 'head/dense/kernel'.</span></span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Strategy:</span></span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Keep 'params' for all layers EXCEPT the last one.</span></span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Randomly initialize a new last layer for 555 bird classes.</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
</section>
<section id="summary-of-strategy-for-nabirds" class="level3">
<h3 class="anchored" data-anchor-id="summary-of-strategy-for-nabirds">4. Summary of Strategy for NABirds</h3>
<ol type="1">
<li><strong>Don’t use <code>jax-ml/bonsai</code> yet:</strong> It is better for reading code than for practical transfer learning.</li>
<li><strong>Use <code>efficientnet-jax</code>:</strong> This gives you the pre-trained weights you need.</li>
<li><strong>Resolution:</strong> Train at <strong><span class="math inline">\(224 \times 224\)</span></strong> first, but for birds, bumping up to <strong><span class="math inline">\(300 \times 300\)</span></strong> (EfficientNet-B3 size) often yields better results if your GPU fits it.</li>
<li><strong>Learning Rate:</strong> When fine-tuning in JAX, use a <strong>low learning rate</strong> (e.g., <span class="math inline">\(1e-4\)</span>) so you don’t destroy the pre-trained weights.</li>
</ol>
</section>
</section>
<section id="option-without-outdated-libraries" class="level2">
<h2 class="anchored" data-anchor-id="option-without-outdated-libraries">Option without outdated libraries</h2>
<p>Since <strong>Flax NNX</strong> is very new (released late 2024), there is no official “NNX Model Zoo” yet. Native NNX implementations of EfficientNet with pre-trained weights simply do not exist in a mature, ready-to-use library.</p>
<p>However, you can achieve exactly what you want by combining <strong>Hugging Face Transformers</strong> (which is active and maintains pre-trained Linen weights) with the <strong><code>flax.nnx.bridge</code></strong> API. This allows you to run a modern NNX training loop while utilizing the battle-tested weights from the older Linen API.</p>
<p>Here is the modern, non-archived solution.</p>
<section id="the-strategy" class="level3">
<h3 class="anchored" data-anchor-id="the-strategy">The Strategy</h3>
<ol type="1">
<li><strong>Source:</strong> Use <code>transformers</code> to download the pre-trained EfficientNet (Linen version).</li>
<li><strong>Bridge:</strong> Use <code>flax.nnx.bridge.ToNNX</code> to convert the Linen module into an NNX graph.</li>
<li><strong>Surgery:</strong> Graph the pre-trained weights into the new NNX module variables.</li>
<li><strong>Model:</strong> Wrap it all in a clean <code>nnx.Module</code> with your custom classifier head.</li>
</ol>
</section>
<section id="the-code" class="level3">
<h3 class="anchored" data-anchor-id="the-code">The Code</h3>
<p>You will need <code>transformers</code> and <code>flax</code> installed.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> jax</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> jax.numpy <span class="im">as</span> jnp</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> flax <span class="im">import</span> nnx</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> flax.nnx <span class="im">import</span> bridge</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> FlaxEfficientNetModel</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> NABirdsEfficientNet(nnx.Module):</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_classes<span class="op">=</span><span class="dv">555</span>, <span class="op">*</span>, rngs: nnx.Rngs):</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 1. Load the pre-trained Linen model structure &amp; weights from Hugging Face</span></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>        hf_model <span class="op">=</span> FlaxEfficientNetModel.from_pretrained(<span class="st">"google/efficientnet-b0"</span>)</span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 2. Extract the underlying Linen module and its variables</span></span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>        linen_module <span class="op">=</span> hf_model.module</span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>        linen_variables <span class="op">=</span> hf_model.params</span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># HF stores batch_stats in 'batch_stats' if they exist, or inside params.</span></span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># EfficientNet usually has 'batch_stats'. We merge them for the bridge.</span></span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">hasattr</span>(hf_model, <span class="st">'batch_stats'</span>):</span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a>            linen_variables <span class="op">=</span> {<span class="op">**</span>linen_variables, <span class="op">**</span>hf_model.batch_stats}</span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 3. Create the NNX Bridge</span></span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># We wrap the Linen module. ToNNX creates the structure.</span></span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.backbone <span class="op">=</span> bridge.ToNNX(linen_module, rngs<span class="op">=</span>rngs)</span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb15-24"><a href="#cb15-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 4. WEIGHT SURGERY (The Critical Step)</span></span>
<span id="cb15-25"><a href="#cb15-25" aria-hidden="true" tabindex="-1"></a>        <span class="co"># We must initialize the bridge to create the NNX variable structure,</span></span>
<span id="cb15-26"><a href="#cb15-26" aria-hidden="true" tabindex="-1"></a>        <span class="co"># then replace those random variables with the pre-trained ones.</span></span>
<span id="cb15-27"><a href="#cb15-27" aria-hidden="true" tabindex="-1"></a>        dummy_input <span class="op">=</span> jnp.ones((<span class="dv">1</span>, <span class="dv">224</span>, <span class="dv">224</span>, <span class="dv">3</span>))</span>
<span id="cb15-28"><a href="#cb15-28" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.backbone.lazy_init(dummy_input)</span>
<span id="cb15-29"><a href="#cb15-29" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb15-30"><a href="#cb15-30" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Transfer weights: Linen dict -&gt; NNX State</span></span>
<span id="cb15-31"><a href="#cb15-31" aria-hidden="true" tabindex="-1"></a>        <span class="co"># The bridge maps Linen collections to NNX variable types automatically.</span></span>
<span id="cb15-32"><a href="#cb15-32" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 'params' -&gt; nnx.Param, 'batch_stats' -&gt; nnx.BatchStat</span></span>
<span id="cb15-33"><a href="#cb15-33" aria-hidden="true" tabindex="-1"></a>        _, backbone_state <span class="op">=</span> nnx.split(<span class="va">self</span>.backbone)</span>
<span id="cb15-34"><a href="#cb15-34" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb15-35"><a href="#cb15-35" aria-hidden="true" tabindex="-1"></a>        <span class="co"># This function recursively matches keys and updates the state</span></span>
<span id="cb15-36"><a href="#cb15-36" aria-hidden="true" tabindex="-1"></a>        <span class="kw">def</span> copy_weights(target_state, source_dict):</span>
<span id="cb15-37"><a href="#cb15-37" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> key, value <span class="kw">in</span> source_dict.items():</span>
<span id="cb15-38"><a href="#cb15-38" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> <span class="bu">isinstance</span>(value, <span class="bu">dict</span>) <span class="kw">or</span> <span class="bu">hasattr</span>(value, <span class="st">'items'</span>):</span>
<span id="cb15-39"><a href="#cb15-39" aria-hidden="true" tabindex="-1"></a>                    <span class="co"># Traverse deeper if it's a dict/FrozenDict</span></span>
<span id="cb15-40"><a href="#cb15-40" aria-hidden="true" tabindex="-1"></a>                    copy_weights(target_state[key], value)</span>
<span id="cb15-41"><a href="#cb15-41" aria-hidden="true" tabindex="-1"></a>                <span class="cf">else</span>:</span>
<span id="cb15-42"><a href="#cb15-42" aria-hidden="true" tabindex="-1"></a>                    <span class="co"># We found a leaf (array). Update the NNX Variable's value.</span></span>
<span id="cb15-43"><a href="#cb15-43" aria-hidden="true" tabindex="-1"></a>                    <span class="co"># Note: target_state[key] is a Variable (Param/BatchStat) wrapper</span></span>
<span id="cb15-44"><a href="#cb15-44" aria-hidden="true" tabindex="-1"></a>                    target_state[key].value <span class="op">=</span> value</span>
<span id="cb15-45"><a href="#cb15-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-46"><a href="#cb15-46" aria-hidden="true" tabindex="-1"></a>        copy_weights(backbone_state, linen_variables)</span>
<span id="cb15-47"><a href="#cb15-47" aria-hidden="true" tabindex="-1"></a>        nnx.update(<span class="va">self</span>.backbone, backbone_state)</span>
<span id="cb15-48"><a href="#cb15-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-49"><a href="#cb15-49" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 5. Define your new Custom Head (Pure NNX)</span></span>
<span id="cb15-50"><a href="#cb15-50" aria-hidden="true" tabindex="-1"></a>        <span class="co"># EfficientNet-B0 output is 1280 dim</span></span>
<span id="cb15-51"><a href="#cb15-51" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.head <span class="op">=</span> nnx.Linear(<span class="dv">1280</span>, num_classes, rngs<span class="op">=</span>rngs)</span>
<span id="cb15-52"><a href="#cb15-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-53"><a href="#cb15-53" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, x):</span>
<span id="cb15-54"><a href="#cb15-54" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Run backbone (bridge)</span></span>
<span id="cb15-55"><a href="#cb15-55" aria-hidden="true" tabindex="-1"></a>        <span class="co"># HF models output a generic object; we need 'last_hidden_state'</span></span>
<span id="cb15-56"><a href="#cb15-56" aria-hidden="true" tabindex="-1"></a>        <span class="co"># shape: [batch, 7, 7, 1280] for B0</span></span>
<span id="cb15-57"><a href="#cb15-57" aria-hidden="true" tabindex="-1"></a>        outputs <span class="op">=</span> <span class="va">self</span>.backbone(x)</span>
<span id="cb15-58"><a href="#cb15-58" aria-hidden="true" tabindex="-1"></a>        features <span class="op">=</span> outputs.last_hidden_state</span>
<span id="cb15-59"><a href="#cb15-59" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb15-60"><a href="#cb15-60" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Global Average Pooling (standard for EfficientNet)</span></span>
<span id="cb15-61"><a href="#cb15-61" aria-hidden="true" tabindex="-1"></a>        features <span class="op">=</span> jnp.mean(features, axis<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb15-62"><a href="#cb15-62" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb15-63"><a href="#cb15-63" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Classification</span></span>
<span id="cb15-64"><a href="#cb15-64" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.head(features)</span>
<span id="cb15-65"><a href="#cb15-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-66"><a href="#cb15-66" aria-hidden="true" tabindex="-1"></a><span class="co"># --- Usage Example ---</span></span>
<span id="cb15-67"><a href="#cb15-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-68"><a href="#cb15-68" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Initialize</span></span>
<span id="cb15-69"><a href="#cb15-69" aria-hidden="true" tabindex="-1"></a>rngs <span class="op">=</span> nnx.Rngs(params<span class="op">=</span><span class="dv">0</span>, dropout<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb15-70"><a href="#cb15-70" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> NABirdsEfficientNet(num_classes<span class="op">=</span><span class="dv">555</span>, rngs<span class="op">=</span>rngs)</span>
<span id="cb15-71"><a href="#cb15-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-72"><a href="#cb15-72" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Forward Pass</span></span>
<span id="cb15-73"><a href="#cb15-73" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> jax.random.normal(jax.random.key(<span class="dv">0</span>), (<span class="dv">1</span>, <span class="dv">224</span>, <span class="dv">224</span>, <span class="dv">3</span>))</span>
<span id="cb15-74"><a href="#cb15-74" aria-hidden="true" tabindex="-1"></a>logits <span class="op">=</span> model(x)</span>
<span id="cb15-75"><a href="#cb15-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-76"><a href="#cb15-76" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Logits shape: </span><span class="sc">{</span>logits<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>) <span class="co"># (1, 555)</span></span>
<span id="cb15-77"><a href="#cb15-77" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Model initialized and pre-trained weights loaded via NNX bridge."</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="why-this-is-the-correct-approach" class="level3">
<h3 class="anchored" data-anchor-id="why-this-is-the-correct-approach">Why this is the correct approach</h3>
<ul>
<li><strong>No Archived Libraries:</strong> It uses <code>transformers</code> (very active) and <code>flax.nnx</code> (brand new).</li>
<li><strong>Stateful:</strong> The <code>NABirdsEfficientNet</code> class is fully stateful. You don’t need to pass <code>params</code> dictionaries around manually; <code>model.w</code> holds the data, just like in PyTorch.</li>
<li><strong>Hybrid Power:</strong> You get the mature, accurate weights from the Linen ecosystem but can write your training loop, loss functions, and new layers using the clean NNX API.</li>
</ul>
</section>
<section id="fine-tuning-advice" class="level3">
<h3 class="anchored" data-anchor-id="fine-tuning-advice">Fine-Tuning Advice</h3>
<p>When training this <code>model</code>: 1. <strong>Freeze the Backbone (Optional):</strong> In NNX, you can easily filter parameters. If you want to train only the head first: ```python # Split model into params and graph graph, params = nnx.split(model)</p>
<pre><code># Filter to get only the head parameters
head_params = nnx.filter(params, lambda path, _: 'head' in path)

# Pass ONLY head_params to your optimizer
```</code></pre>
<ol start="2" type="1">
<li><strong>Batch Statistics:</strong> Remember that EfficientNet has BatchNorm layers. In NNX, you need to handle <code>BatchStat</code> updates. Ensure your training step returns the updated state for these statistics alongside the gradients.</li>
</ol>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/mint\.westdri\.ca\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
            // target, if specified
            link.setAttribute("target", "_blank");
            if (link.getAttribute("rel") === null) {
              link.setAttribute("rel", "noopener");
            }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
    <a class="nav-link" href="../../about.html">
<p>About</p>
</a>
  </li>  
    <li class="nav-item">
    <a class="nav-link" href="../../talks/index.html">
<p>Talks</p>
</a>
  </li>  
</ul>
    </div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/WestDRI" target="_blank">
      <i class="bi bi-github" role="img" aria-label="GitHub organization">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://docs.alliancecan.ca/wiki/Technical_documentation" target="_blank">
      <i class="bi bi-wikipedia" role="img" aria-label="Alliance wikipedia">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://www.youtube.com/channel/UCfgds4Qf7VFOv4ORRvFFmhw" target="_blank">
      <i class="bi bi-youtube" role="img" aria-label="YouTube channel">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://mint.westdri.ca/calendar">
      <i class="bi bi-calendar-week" role="img" aria-label="Training calendar">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://mint.westdri.ca/newsletter">
      <i class="bi bi-envelope-fill" role="img" aria-label="Newsletter">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://mint.westdri.ca/contact">
      <i class="bi bi-mailbox2" role="img" aria-label="Contact us">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>




</body></html>