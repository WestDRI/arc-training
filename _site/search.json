[
  {
    "objectID": "tools/ws_quarto.html",
    "href": "tools/ws_quarto.html",
    "title": "Authoring scientific documents with Markdown and Quarto",
    "section": "",
    "text": "This workshop will show you how to easily create beautiful scientific documents (html, pdf, websites, books…)—complete with formatted text, dynamic code, and figures with Quarto, an open-source tool combining the powers of Jupyter or knitr with Pandoc to turn your text and code blocks into fully dynamic and formatted documents.",
    "crumbs": [
      "Tools",
      "<em><b>Workshops</b></em>"
    ]
  },
  {
    "objectID": "tools/ws_quarto.html#markup-and-markdown",
    "href": "tools/ws_quarto.html#markup-and-markdown",
    "title": "Authoring scientific documents with Markdown and Quarto",
    "section": "Markup and Markdown",
    "text": "Markup and Markdown\n\nMarkup languages\nMarkup languages control the formatting of text documents. They are powerful but complex and the raw text (before it is rendered into its formatted version) is visually cluttered and hard to read.\nExamples of markup languages include LaTeX and HTML.\n\nTex (often with the macro package LaTeX) is used to create pdf.\n\n\nExample LaTeX:\n\n\\documentclass{article}\n\\title{My title}\n\\author{My name}\n\\usepackage{datetime}\n\\newdate{date}{24}{11}{2022}\n\\date{\\displaydate{date}}\n\\begin{document}\n \\maketitle\n \\section{First section}\n Some text in the first section.\n\\end{document}\n\nHTML (often with css or scss files to customize the format) is used to create webpages.\n\n\nExample HTML:\n\n&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en-US\"&gt;\n  &lt;head&gt;\n    &lt;meta charset=\"utf-8\" /&gt;\n    &lt;meta name=\"viewport\" content=\"width=device-width\" /&gt;\n    &lt;title&gt;My title&lt;/title&gt;\n    &lt;address class=\"author\"&gt;My name&lt;/address&gt;\n    &lt;input type=\"date\" value=\"2022-11-24\" /&gt;\n  &lt;/head&gt;\n  &lt;h1&gt;First section&lt;/h1&gt;\n  &lt;body&gt;\n    Some text in the first section.\n  &lt;/body&gt;\n&lt;/html&gt;\n\n\nMarkdown\nA number of minimalist markup languages intend to remove all the visual clutter and complexity to create raw texts that are readable prior to rendering. Markdown (note the pun with “markup”), created in 2004, is the most popular of them. Due to its simplicity, it has become quasi-ubiquitous. Many implementations exist which add a varying number of features (as you can imagine, a very simple markup language is also fairly limited).\nMarkdown files are simply text files and they use the .md extension.\n\n\nBasic Markdown syntax\nIn its basic form, Markdown is mostly used to create webpages. Conveniently, raw HTML can be included whenever the limited markdown syntax isn’t sufficient.\nHere is an overview of the Markdown syntax supported by many applications.\n\n\nPandoc and its extended Markdown syntax\nWhile the basic syntax is good enough for HTML outputs, it is very limited for other formats.\nPandoc is a free and open-source markup format converter. Pandoc supports an extended Markdown syntax with functionality for figures, tables, callout blocks, LaTeX mathematical equations, citations, and YAML metadata blocks. In short, everything needed for the creation of scientific documents.\nSuch documents remain as readable as basic Markdown documents (thus respecting the Markdown philosophy), but they can now be rendered in sophisticated pdf, books, entire websites, Word documents, etc.\nAnd of course, as such documents remain text files, you can put them under version control with Git.\n\nPrevious example using Pandoc’s Markdown:\n\n---\ntitle: My title\nauthor: My name\ndate: 2022-11-24\n---\n# First section\nSome text in the first section.",
    "crumbs": [
      "Tools",
      "<em><b>Workshops</b></em>"
    ]
  },
  {
    "objectID": "tools/ws_quarto.html#literate-programming",
    "href": "tools/ws_quarto.html#literate-programming",
    "title": "Authoring scientific documents with Markdown and Quarto",
    "section": "Literate programming",
    "text": "Literate programming\nLiterate programming is a methodology that combines snippets of code and written text. While first introduced in 1984, this approach to the creation of documents has truly exploded in popularity in recent years thanks to the development of new tools such as R Markdown and, later, Jupyter notebooks.",
    "crumbs": [
      "Tools",
      "<em><b>Workshops</b></em>"
    ]
  },
  {
    "objectID": "tools/ws_quarto.html#quarto",
    "href": "tools/ws_quarto.html#quarto",
    "title": "Authoring scientific documents with Markdown and Quarto",
    "section": "Quarto",
    "text": "Quarto\n\nHow it works\nQuarto files are transformed into Pandoc’s extended Markdown by Jupyter (when used with Python or Julia) or by knitr (when used with R), then pandoc turns the Markdown document into the output of your choice.\n\nJulia and Python make use of the Jupyter engine:\n\n From Quarto documentation\n\nR uses the knitr engine:\n\n From Quarto documentation\nQuarto files use the extension .qmd.\nWhen using R, you can use Quarto directly from RStudio: if you are used to R Markdown, Quarto is the new and better R Markdown.\nWhen using Python or Julia, you can use Quarto directly from a Jupyter notebook (with .ipynb extension).\n\nUsing Quarto directly from a Jupyter notebook:\n\n From Quarto documentation\nIn this workshop, we will see the most general workflow: simply using a text editor.\n\n\n\n\n\n\nSupported languages\n\n\n\n\n\nQuarto renders highlighting in countless languages and generates dynamic output for code blocks in:\n\nPython\nR\nJulia\nObservable JS\n\nYou can render documents in a wide variety of formats:\n\nHTML\nPDF\nMS Word\nOpenOffice\nePub\nRevealjs\nPowerPoint\nBeamer\nGitHub Markdown\nCommonMark\nHugo\nDocusaurus\nMarkua\nMediaWiki\nDokuWiki\nZimWiki\nJira Wiki\nXWiki\nJATS\nJupyter\nConTeXt\nRTF\nreST\nAsciiDoc\nOrg-Mode\nMuse\nGNU\nGroff\n\nThis training website is actually built with Quarto!\n\n\n\n\n\nInstallation\n\nDownload Quarto here.\nDownload the language(s) (R, Python, or Julia) you will want to use with Quarto as well as their corresponding engine (knitr for R; Jupyter for Python and Julia):\n\nIf you want to use Quarto with R, you will need:\n\nR (download here if you don’t have R already on your system),\nthe rmarkdown package. For this, launch R and run:\n\ninstall.packages(\"rmarkdown\")\nIf you want to use it with Python, you will need:\n\nPython 3 (download here if don’t have it on your system),\nJupyterLab. For this, open a terminal and run:\n\npython3 -m pip install jupyter  # if you are on MacOS or Linux\npython -m pip install jupyter   # if you are on Windows\nFinally, if you want to use Quarto with Julia, you will need:\n\nJulia (download here if you don’t have Julia),\nthe IJulia and Revise packages. For this, launch Julia and run:\n\n] add IJulia Revise\n# &lt;Backspace&gt;\nusing IJulia\nnotebook()      # to install a minimal Python+Jupyter distribution\nRunning notebook() allows you to install Jupyter if you don’t already have it.\n\n\nDocument structure and syntax\n\nFront matter\nWritten in YAML. Sets the options for the document. Let’s see a few examples.\n\nHTML output:\n\n---\ntitle: \"My title\"\nauthor: \"My name\"\nformat: html\n---\n\nHTML output with a few options:\n\n---\ntitle: \"My title\"\nauthor: \"My name\"\nformat:\n  html:\n    toc: true\n    css: &lt;my_file&gt;.css\n---\n\nMS Word output with Python code blocks:\n\n---\ntitle: \"My title\"\nauthor: \"My name\"\nformat: docx\njupyter: python3\n---\n\nrevealjs output with some options and Julia code blocks:\n\n---\ntitle: \"Some title\"\nsubtitle: \"Some subtitle\"\ninstitute: \"Simon Fraser University\"\ndate: \"2022-11-24\"\nexecute:\n  error: true\n  echo: true\nformat:\n  revealjs:\n    theme: [default, custom.scss]\n    highlight-style: monokai\n    code-line-numbers: false\n    embed-resources: true\njupyter: julia-1.8\n---\nSee the Quarto documentation for an exhaustive list of options for all formats.\n\n\nWritten sections\nWritten sections are written in Pandoc’s extended Markdown.\n\n\nCode blocks\nIf all you want is syntax highlighting of the code blocks, use this syntax:\n{.language} &lt;some code&gt;\nIf you want syntax highlighting of the blocks and for the code to run, use instead:\n```{language}\n&lt;some code&gt;\n```\nIn addition, options can be added to individual code blocks:\n```{language}\n#| &lt;some option&gt;: &lt;some option value&gt;\n\n&lt;some code&gt;\n```\n\n\n\nRendering\nUsing Quarto is very simple: there are only two commands you need to know.\nIn a terminal, simply run either of:\nquarto render &lt;file&gt;.qmd     # Render the document\nquarto preview &lt;file&gt;.qmd    # Display a live preview",
    "crumbs": [
      "Tools",
      "<em><b>Workshops</b></em>"
    ]
  },
  {
    "objectID": "tools/ws_quarto.html#lets-create-a-webpage-together",
    "href": "tools/ws_quarto.html#lets-create-a-webpage-together",
    "title": "Authoring scientific documents with Markdown and Quarto",
    "section": "Let’s create a webpage together",
    "text": "Let’s create a webpage together\nFirst, create a file called test.qmd with the text editor of your choice.\n\nExample:\n\nnano test.qmd\nAdd a minimal front matter with the title of your document and the output format (html here since we are creating a webpage):\n---\ntitle: Test webpage\nformat: html\n---\nThen open a new terminal, cd to the location of the file, and run the command:\nquarto preview test.qmd\nThis will open the rendered document in your browser.\nWe will play with this test.qmd file and see how it is rendered by Quarto as we go.",
    "crumbs": [
      "Tools",
      "<em><b>Workshops</b></em>"
    ]
  },
  {
    "objectID": "tools/ws_quarto.html#examples",
    "href": "tools/ws_quarto.html#examples",
    "title": "Authoring scientific documents with Markdown and Quarto",
    "section": "Examples",
    "text": "Examples\nBelow are a few basic example files and their outputs.\n\nRevealjs presentation\n\n\n\n\n\n\nCode\n\n\n\n\n\n---\ntitle: \"My title\"\nauthor: \"My name\"\ninstitute: \"Simon Fraser University\"\nformat:\n  revealjs:\n    highlight-style: monokai\n    code-line-numbers: false\n    embed-resources: true\n---\n\n## First section\n\nWhen exporting to revealjs, second level sections mark the start of new slides,\nwith a slide title.\n\nThis can be changed in options.\n\n---\n\nNew slides can be started without titles this way.\n\n# There are title slides\n\n## Formatting\n\nText can be rendered *in italic* or **in bold** as well as [underlined]{.underline}.\n\nYou can use superscripts^2^, subscripts~test~, ~~strikethrough~~, and `inline code`.\n\n&gt; This is a quote.\n\n## Columns\n\n:::: {.columns}\n\n::: {.column width=\"30%\"}\n\nYou can create columns.\n\n:::\n\n::: {.column width=\"70%\"}\n\nAnd you can set their respective width.\n\n:::\n\n::::\n\n## Lists\n\n::: {.incremental}\n\n- List can happen one line at a time\n- like\n- this\n\n:::\n\n## Lists\n\n- Or all at the same time\n- like\n- that\n\n## Ordered lists\n\n1. Item 1\n2. Item 2\n3. Item 3\n\n## Images\n\n![Example image](qmd_jupyter.png)\n\n## Tables\n\n| Col 1 | Col 2 | Col 3  |\n|-------|-------|--------|\n| a     | 1     | red    |\n| b     | 2     | orange | \n| c     | 3     | yellow |\n\n:::{.callout-note}\n\nTables can be fully customized (or you could use raw html).\n\n:::\n\n## Equations\n\n$$\n\\frac{\\partial \\mathrm C}{ \\partial \\mathrm t } + \\frac{1}{2}\\sigma^{2} \\mathrm S^{2}\n\\frac{\\partial^{2} \\mathrm C}{\\partial \\mathrm C^2}\n  + \\mathrm r \\mathrm S \\frac{\\partial \\mathrm C}{\\partial \\mathrm S}\\ =\n  \\mathrm r \\mathrm C \n$$\n\n\n\nRendered document (click on it to open it in a new tab):\n\n\n\npdf\n\n\n\n\n\n\nCode\n\n\n\n\n\n---\ntitle: \"My title\"\nauthor: \"My name\"\nformat:\n  pdf:\n    toc: true\n---\n\n## Heading\n\nSome text.\n\n### Subheading\n\nMore text.\n\n## Formatting\n\nText can be rendered *in italic* or **in bold** as well as [underlined]{.underline}.\n\nYou can use superscripts^2^, subscripts~test~, ~~strikethrough~~, and `inline code`.\n\n&gt; This is a quote.\n\n## Lists\n\n### Unordered\n\n- Item 1\n- Item 2\n- Item 3\n\n### Ordered\n\n1. Item 1\n2. Item 2\n3. Item 3\n\n## Images\n\n![Example image](qmd_jupyter.png)\n\n## Tables\n\n| Col 1 | Col 2 | Col 3  |\n|-------|-------|--------|\n| a     | 1     | red    |\n| b     | 2     | orange | \n| c     | 3     | yellow |\n\n:::{.callout-note}\n\nTables can be fully customized (or you could use raw html).\n\n:::\n\n## Equations\n\n$$\n\\frac{\\partial \\mathrm C}{ \\partial \\mathrm t } + \\frac{1}{2}\\sigma^{2} \\mathrm S^{2}\n\\frac{\\partial^{2} \\mathrm C}{\\partial \\mathrm C^2}\n  + \\mathrm r \\mathrm S \\frac{\\partial \\mathrm C}{\\partial \\mathrm S}\\ =\n  \\mathrm r \\mathrm C \n$$\n\n\n\nRendered document (click on it to open it in a new tab):\n\n\nIn order to export to pdf, you need a TeX distribution. You probably already have one installed on your machine, so you should first try to render or preview a document to pdf to see whether it works. If it doesn’t work, you can install the minimalist distribution TinyTex by running in your terminal:\n\nquarto install tool tinytex\n\n\nHTML with R code blocks\n\n\n\n\n\n\nCode\n\n\n\n\n\n---\ntitle: \"My title\"\nauthor: \"My name\"\ninstitute: \"Simon Fraser University\"\nformat: html\n---\n\n## Heading\n\n### Subheading\n\nSome text.\n\n## Formatting  {#sec-formatting}\n\n::: aside\n\nNote that each heading automatically creates an anchor, making it easy to link to specific sections of your documents.\n\n:::\n\nText can be rendered *in italic* or **in bold** as well as [underlined]{.underline}.\n\nYou can use superscripts^2^, subscripts~test~, ~~strikethrough~~, and `inline code`.\n\n&gt; This is a quote.\n\n## Columns\n\n:::: {.columns}\n\n::: {.column width=\"30%\"}\n\nYou can create columns.\n\n:::\n\n::: {.column width=\"70%\"}\n\nAnd you can set their respective width.\n\n:::\n\n::::\n\n## Lists\n\n- Item 1\n- Item 2\n- Item 3\n\n## Ordered lists\n\n1. Item 1\n2. Item 2\n3. Item 3\n\n## Images\n\n![Example image](qmd_jupyter.png)\n\n## Tables\n\n| Col 1 | Col 2 | Col 3  |\n|-------|-------|--------|\n| a     | 1     | red    |\n| b     | 2     | orange | \n| c     | 3     | yellow |\n\n:::{.callout-note}\n\nTables can be fully customized (or you could use raw html).\n\n:::\n\n## Equations\n\n$$\n\\frac{\\partial \\mathrm C}{ \\partial \\mathrm t } + \\frac{1}{2}\\sigma^{2} \\mathrm S^{2}\n\\frac{\\partial^{2} \\mathrm C}{\\partial \\mathrm C^2}\n  + \\mathrm r \\mathrm S \\frac{\\partial \\mathrm C}{\\partial \\mathrm S}\\ =\n  \\mathrm r \\mathrm C \n$$\n\n## Cross-references\n\nSee @sec-formatting.\n\n*Note that you can add bibliographies, flow charts, the equivalent of HTML \"div\",\nand just so much more. Remember that this is a tiny overview.*\n\n## Let's try some code blocks now\n\n```{r}\n# This is a block that runs\n2 + 3\n```\n\n::: aside\n\nDid you notice that the content of your code blocks can be copied with a click?\nOf course, this is customizable.\n\n:::\n\n```{.r}\n# This is a block that doesn't run\n2 + 3\n```\n\n```{r}\n#| echo: false\n# And this is a block showing only the output\ndata.frame(\n  country = c(\"Canada\", \"USA\", \"Mexico\"),\n  var = c(2.9, 3.1, 4.5)\n)\n```\n\n## Plots\n\n```{r}\nplot(cars)\n```\n\n&lt;br&gt;\nYou can play with options to add a title:\n\n```{r}\n#| fig-cap: \"Stopping distance as a function of speed in cars\"\n\nplot(cars)\n```\n\n&lt;br&gt;\nYou can have more complex multi-plot layouts:\n\n```{r}\n#| layout-ncol: 2\n#| fig-cap: \n#|   - \"Stopping distance as a function of speed in cars\"\n#|   - \"Vapor pressure of mercury as a function of temperature\"\n\nplot(cars)\nplot(pressure)\n```\n\nFor those who have `ggplot2`[^1], you can try that too:\n\n```{r}\nlibrary(ggplot2)\n\nggplot(data = mpg, mapping = aes(x = displ, y = hwy)) + \n  geom_point(mapping = aes(color = class)) + \n  geom_smooth()\n```\n\n[^1]: You can install it with:\n    ```{.r}\n    install.packages(\"ggplot2\")\n    ```\n\n\n\nRendered document (click on it to open it in a new tab):\n\n\n\nBeamer with Python code blocks\nBeamer is LaTeX presentation framework: a way to create beautiful pdf slides.\n\n\n\n\n\n\nCode\n\n\n\n\n\n---\ntitle: \"Some title\"\nauthor: \"Some name\"\nformat: beamer\njupyter: python3\n---\n\n## First slide\n\nWith some content\n\n## Formatting\n\nText can be rendered *in italic* or **in bold** as well as [underlined]{.underline}.\n\nYou can use superscripts^2^, subscripts~test~, ~~strikethrough~~, and `inline code`.\n\n## Lists\n\n- Item 1\n- Item 2\n- Item 3\n\n## Ordered lists\n\n1. Item 1\n2. Item 2\n3. Item 3\n\n## Images\n\n![Example image](qmd_jupyter.png)\n\n## Tables\n\n| Col 1 | Col 2 | Col 3  |\n|-------|-------|--------|\n| a     | 1     | red    |\n| b     | 2     | orange | \n| c     | 3     | yellow |\n\n:::{.callout-note}\n\nTables can be fully customized (or you could use raw html).\n\n:::\n\n## Equations\n\n$$\n\\frac{\\partial \\mathrm C}{ \\partial \\mathrm t } + \\frac{1}{2}\\sigma^{2} \\mathrm S^{2}\n\\frac{\\partial^{2} \\mathrm C}{\\partial \\mathrm C^2}\n  + \\mathrm r \\mathrm S \\frac{\\partial \\mathrm C}{\\partial \\mathrm S}\\ =\n  \\mathrm r \\mathrm C \n$$\n\n## Some basic code block\n\n```{python}\n#| echo: true\n\n2 + 3\n```\n\n## Some plot\n\n```{python}\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Data for plotting\nt = np.arange(0.0, 2.0, 0.01)\ns = 1 + np.sin(2 * np.pi * t)\n\nfig, ax = plt.subplots()\nax.plot(t, s)\n\nax.set(xlabel='time (s)', ylabel='voltage (mV)',\n       title='Here goes the title')\nax.grid()\n\nfig.savefig(\"test.png\")\nplt.show()\n```\n\n\n\nRendered document (click on it to open it in a new tab):",
    "crumbs": [
      "Tools",
      "<em><b>Workshops</b></em>"
    ]
  },
  {
    "objectID": "tools/ws_quarto.html#recording",
    "href": "tools/ws_quarto.html#recording",
    "title": "Authoring scientific documents with Markdown and Quarto",
    "section": "Recording",
    "text": "Recording",
    "crumbs": [
      "Tools",
      "<em><b>Workshops</b></em>"
    ]
  },
  {
    "objectID": "tools/wb_tools1.html",
    "href": "tools/wb_tools1.html",
    "title": "Fun tools to simplify your life in the shell",
    "section": "",
    "text": "Please find this webinar in the Bash section.",
    "crumbs": [
      "Tools",
      "<b><em>Webinars</em></b>",
      "Fun tools for the command line"
    ]
  },
  {
    "objectID": "tools/wb_quarto.html",
    "href": "tools/wb_quarto.html",
    "title": "The new R Markdown:",
    "section": "",
    "text": "This webinar will show you how to easily create beautiful publications (webpages, pdf, websites, presentations, books…)—complete with formatted text, dynamic code and figures with Quarto.\nQuarto is the successor to R Markdown. By combining the powers of Jupyter or knitr with Pandoc, it works with R, but also with Python and Julia code blocks and it adds new functionalities to the old tool.\n\nSlides (Click and wait: this reveal.js presentation may take a little time to load.) \n\n\n\nVariation of the talk as a staff to staff webinar:",
    "crumbs": [
      "Tools",
      "<b><em>Webinars</em></b>",
      "Quarto: the new R Markdown"
    ]
  },
  {
    "objectID": "tools/wb_help.html",
    "href": "tools/wb_help.html",
    "title": "So, you are stuck … now what?",
    "section": "",
    "text": "Stack Overflow, Stack Exchange, Discourse forums, and other online platforms … the internet is a treasure trove of online communities where you can find solutions to your coding problems. To have a positive experience and get the answers you need however, you have to know where to ask, how to ask, and when not to ask: if countless people are willing to give you their time for free, they usually expect that you do your part.\nIn this webinar, I will present key online sites, their functioning, and their culture; then I will go over the magic trick to get answers to your questions: knowing how to create minimum reproducible examples. I will not focus on any particular language as the principles (how to create a minimal dataset, how to deal with private data, how to create self-sufficient code, how to reproduce the problem, etc.) can apply to any language.\n\nSlides (Click and wait: this reveal.js presentation may take a little time to load.)",
    "crumbs": [
      "Tools",
      "<b><em>Webinars</em></b>",
      "Getting help"
    ]
  },
  {
    "objectID": "tools/top_wb.html",
    "href": "tools/top_wb.html",
    "title": "Tools webinars",
    "section": "",
    "text": "Quarto: the new R Markdown\n\n\n\n\nData version control\n\n\n\n\nFun tools for the command line\n\n\n\n\nMore command line tools\n\n\n\n\n\n\nGetting help",
    "crumbs": [
      "Tools",
      "<b><em>Webinars</em></b>"
    ]
  },
  {
    "objectID": "talks/index.html",
    "href": "talks/index.html",
    "title": "The instruments for advanced research computing are here, but are researchers ready?",
    "section": "",
    "text": "The current times are exciting: we are witnessing a growth of computing power while the open source community is vigorously building impressive machine learning and scientific programming tools.\nThis boom of hardware and software assets cannot however translate into research if graduate students aren’t able to take advantage of it. Curricula often lack training pertinent to the use of such resources. Worse yet, in many fields faculties and PIs don’t have the necessary background to help their students with high-performance programming. The training team at Simon Fraser University Research Computing Group aims to fill this gap in the West on behalf of the Alliance and all Western Canadian universities.\nThis talk will present an overview of the training we provide, from introductory skill sets for researchers new to ARC and HPC to advanced topics in parallel programming.\n\nSlides"
  },
  {
    "objectID": "talks/2023_driconnect.html",
    "href": "talks/2023_driconnect.html",
    "title": "The instruments for advanced research computing are here, but are researchers ready?",
    "section": "",
    "text": "The current times are exciting: we are witnessing a growth of computing power while the open source community is vigorously building impressive machine learning and scientific programming tools.\nThis boom of hardware and software assets cannot however translate into research if graduate students aren’t able to take advantage of it. Curricula often lack training pertinent to the use of such resources. Worse yet, in many fields faculties and PIs don’t have the necessary background to help their students with high-performance programming. The training team at Simon Fraser University Research Computing Group aims to fill this gap in the West on behalf of the Alliance and all Western Canadian universities.\nThis talk will present an overview of the training we provide, from introductory skill sets for researchers new to ARC and HPC to advanced topics in parallel programming.\n\nSlides",
    "crumbs": [
      "<em><b>Conference talks</b></em>"
    ]
  },
  {
    "objectID": "r/ws_r_demo_slides.html#history",
    "href": "r/ws_r_demo_slides.html#history",
    "title": "A little demo of programming in",
    "section": "History",
    "text": "History\nCreated by academic statisticians Ross Ihaka and Robert Gentleman\nThe name comes from the language S which was a great influence as well as the first initial of the developers\nLaunched in 1993\nA GNU Project since 1997"
  },
  {
    "objectID": "r/ws_r_demo_slides.html#why-r",
    "href": "r/ws_r_demo_slides.html#why-r",
    "title": "A little demo of programming in",
    "section": "Why R?",
    "text": "Why R?\nFree and open source\nHigh-level and easy to learn\nLarge community\nVery well documented\nUnequalled number of statistics and modelling packages\nIntegrated package manager\nEasy connection with fast compiled languages such as C and C++\nPowerful IDEs (e.g. RStudio, ESS, Jupyter)"
  },
  {
    "objectID": "r/ws_r_demo_slides.html#for-whom",
    "href": "r/ws_r_demo_slides.html#for-whom",
    "title": "A little demo of programming in",
    "section": "For whom?",
    "text": "For whom?\nFields with heavy statistics, modelling, or Bayesian inference such as biology, linguistics, economics, or statistics\nData science"
  },
  {
    "objectID": "r/ws_r_demo_slides.html#downsides",
    "href": "r/ws_r_demo_slides.html#downsides",
    "title": "A little demo of programming in",
    "section": "Downsides",
    "text": "Downsides\nInconsistent syntax full of quirks\nSlow\nLarge memory usage"
  },
  {
    "objectID": "r/ws_r_demo_slides.html#an-interpreted-language",
    "href": "r/ws_r_demo_slides.html#an-interpreted-language",
    "title": "A little demo of programming in",
    "section": "An interpreted language",
    "text": "An interpreted language\nR being an interpreted language, it can be run non-interactively or interactively"
  },
  {
    "objectID": "r/ws_r_demo_slides.html#running-r-non-interactively",
    "href": "r/ws_r_demo_slides.html#running-r-non-interactively",
    "title": "A little demo of programming in",
    "section": "Running R non-interactively",
    "text": "Running R non-interactively\nIf you write code in a text file (called a script), you can then execute it with:\nRscript my_script.R\n\nThe command to execute scripts is Rscript rather than R\nBy convention, R scripts take the extension .R"
  },
  {
    "objectID": "r/ws_r_demo_slides.html#running-r-interactively",
    "href": "r/ws_r_demo_slides.html#running-r-interactively",
    "title": "A little demo of programming in",
    "section": "Running R interactively",
    "text": "Running R interactively\nThere are several ways to run R interactively:\n\ndirectly in the console (the name for the R shell)\nin Jupyter with the R kernel (IRkernel package)\nin another IDE (e.g. in Emacs with ESS)\nin the RStudio IDE"
  },
  {
    "objectID": "r/ws_r_demo_slides.html#documentation",
    "href": "r/ws_r_demo_slides.html#documentation",
    "title": "A little demo of programming in",
    "section": "Documentation",
    "text": "Documentation\nThe R documentation is excellent. Get info on any function with ? (e.g. ?sum)"
  },
  {
    "objectID": "r/ws_r_demo_slides.html#basic-operations",
    "href": "r/ws_r_demo_slides.html#basic-operations",
    "title": "A little demo of programming in",
    "section": "Basic operations",
    "text": "Basic operations\n\na &lt;- 5\n4 + a\n\n[1] 9\n\nc &lt;- c(2, 4, 1)\nc * 5\n\n[1] 10 20  5\n\nsum(c)\n\n[1] 7"
  },
  {
    "objectID": "r/ws_r_demo_slides.html#statistics-probabilities-and-modelling",
    "href": "r/ws_r_demo_slides.html#statistics-probabilities-and-modelling",
    "title": "A little demo of programming in",
    "section": "Statistics, probabilities, and modelling",
    "text": "Statistics, probabilities, and modelling\nR really shines when it comes to statistics and modelling\nWe will spend the rest of the hour diving into very complex and heavy Bayesian statistics"
  },
  {
    "objectID": "r/ws_r_demo_slides.html#just-kidding",
    "href": "r/ws_r_demo_slides.html#just-kidding",
    "title": "A little demo of programming in",
    "section": "Just kidding 🙂",
    "text": "Just kidding 🙂\nIn this demo, I will stick to fun topics"
  },
  {
    "objectID": "r/ws_r_demo_slides.html#datasets",
    "href": "r/ws_r_demo_slides.html#datasets",
    "title": "A little demo of programming in",
    "section": "Datasets",
    "text": "Datasets\nR comes with a number of datasets. You can get a list by running data()"
  },
  {
    "objectID": "r/ws_r_demo_slides.html#datasets-1",
    "href": "r/ws_r_demo_slides.html#datasets-1",
    "title": "A little demo of programming in",
    "section": "Datasets",
    "text": "Datasets\nThe ggplot2 package provides additional ones, such as the mpg dataset:\n\nlibrary(ggplot2)\nhead(mpg)\n\n# A tibble: 6 × 11\n  manufacturer model displ  year   cyl trans      drv     cty   hwy fl   \n  &lt;chr&gt;        &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;      &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;\n1 audi         a4      1.8  1999     4 auto(l5)   f        18    29 p    \n2 audi         a4      1.8  1999     4 manual(m5) f        21    29 p    \n3 audi         a4      2    2008     4 manual(m6) f        20    31 p    \n4 audi         a4      2    2008     4 auto(av)   f        21    30 p    \n5 audi         a4      2.8  1999     6 auto(l5)   f        16    26 p    \n6 audi         a4      2.8  1999     6 manual(m5) f        18    26 p    \n  class  \n  &lt;chr&gt;  \n1 compact\n2 compact\n3 compact\n4 compact\n5 compact\n6 compact"
  },
  {
    "objectID": "r/ws_r_demo_slides.html#the-canvas",
    "href": "r/ws_r_demo_slides.html#the-canvas",
    "title": "A little demo of programming in",
    "section": "The canvas",
    "text": "The canvas\n The first component is the data:\n\nggplot(data = mpg)"
  },
  {
    "objectID": "r/ws_r_demo_slides.html#the-canvas-1",
    "href": "r/ws_r_demo_slides.html#the-canvas-1",
    "title": "A little demo of programming in",
    "section": "The canvas",
    "text": "The canvas\nThe second component sets the way variables are mapped on the axes. This is done with the aes() (aesthetics) function:\n\nggplot(data = mpg, mapping = aes(x = displ, y = hwy))"
  },
  {
    "objectID": "r/ws_r_demo_slides.html#geometric-representations-of-the-data",
    "href": "r/ws_r_demo_slides.html#geometric-representations-of-the-data",
    "title": "A little demo of programming in",
    "section": "Geometric representations of the data",
    "text": "Geometric representations of the data\nOnto this canvas, we can add “geoms” (geometrical objects) representing the data.\nTo represent the data as a scatterplot, we use the geom_point() function:\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point()"
  },
  {
    "objectID": "r/ws_r_demo_slides.html#colour-coding-based-on-variables",
    "href": "r/ws_r_demo_slides.html#colour-coding-based-on-variables",
    "title": "A little demo of programming in",
    "section": "Colour-coding based on variables",
    "text": "Colour-coding based on variables\nWe can colour-code the points in the scatterplot based on the drv variable, showing the lower fuel efficiency of 4WD vehicles:\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = drv))"
  },
  {
    "objectID": "r/ws_r_demo_slides.html#colour-coding-based-on-variables-1",
    "href": "r/ws_r_demo_slides.html#colour-coding-based-on-variables-1",
    "title": "A little demo of programming in",
    "section": "Colour-coding based on variables",
    "text": "Colour-coding based on variables\nOr we can colour-code them based on the class variable:\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = class))"
  },
  {
    "objectID": "r/ws_r_demo_slides.html#multiple-geoms",
    "href": "r/ws_r_demo_slides.html#multiple-geoms",
    "title": "A little demo of programming in",
    "section": "Multiple geoms",
    "text": "Multiple geoms\nMultiple “geoms” can be added on top of each other. For instance, we can add a smoothed conditional means function that aids at seeing patterns in the data with geom_smooth():\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = class)) +\n  geom_smooth()"
  },
  {
    "objectID": "r/ws_r_demo_slides.html#colour-scales",
    "href": "r/ws_r_demo_slides.html#colour-scales",
    "title": "A little demo of programming in",
    "section": "Colour scales",
    "text": "Colour scales"
  },
  {
    "objectID": "r/ws_r_demo_slides.html#ggplot-extensions",
    "href": "r/ws_r_demo_slides.html#ggplot-extensions",
    "title": "A little demo of programming in",
    "section": "ggplot extensions",
    "text": "ggplot extensions\nMany packages build on ggplot2 and add functionality"
  },
  {
    "objectID": "r/ws_r_demo_slides.html#combining-plots",
    "href": "r/ws_r_demo_slides.html#combining-plots",
    "title": "A little demo of programming in",
    "section": "Combining plots",
    "text": "Combining plots\nOne ggplot extension is the patchwork package which allows to combine multiple plots on the same frame"
  },
  {
    "objectID": "r/ws_r_demo_slides.html#html-and-css",
    "href": "r/ws_r_demo_slides.html#html-and-css",
    "title": "A little demo of programming in",
    "section": "HTML and CSS",
    "text": "HTML and CSS\nHyperText Markup Language (HTML) is the standard markup language for websites: it encodes the information related to the formatting and structure of webpages. Additionally, some of the customization can be stored in Cascading Style Sheets (CSS) files.\nHTML uses tags of the form:\n&lt;some_tag&gt;Your content&lt;/some_tag&gt;\nSome tags have attributes:\n&lt;some_tag attribute_name=\"attribute value\"&gt;Your content&lt;/some_tag&gt;\n\nExamples:\n\n\n&lt;h2&gt;This is a heading of level 2&lt;/h2&gt;\n&lt;b&gt;This is bold&lt;/b&gt;\n&lt;a href=\"https://some.url\"&gt;This is the text for a link&lt;/a&gt;"
  },
  {
    "objectID": "r/ws_r_demo_slides.html#example-for-this-workshop",
    "href": "r/ws_r_demo_slides.html#example-for-this-workshop",
    "title": "A little demo of programming in",
    "section": "Example for this workshop",
    "text": "Example for this workshop\nWe will use a website from the University of Tennessee containing a database of PhD theses from that university\nOur goal is to scrape data from this site to produce a dataframe with the date, major, and advisor for each dissertation\n\nWe will only do this for the first page which contains the links to the 100 most recent theses. If you really wanted to gather all the data, you would have to do this for all pages"
  },
  {
    "objectID": "r/ws_r_demo_slides.html#package",
    "href": "r/ws_r_demo_slides.html#package",
    "title": "A little demo of programming in",
    "section": "Package",
    "text": "Package\nTo do all this, we will use the package rvest, part of the tidyverse (a modern set of R packages). It is a package influenced by the popular Python package Beautiful Soup and it makes scraping websites with R really easy\nLet’s load it:\n\nlibrary(rvest)"
  },
  {
    "objectID": "r/ws_r_demo_slides.html#read-in-html-from-main-site",
    "href": "r/ws_r_demo_slides.html#read-in-html-from-main-site",
    "title": "A little demo of programming in",
    "section": "Read in HTML from main site",
    "text": "Read in HTML from main site\nAs mentioned above, our site is the database of PhD dissertations from the University of Tennessee\nLet’s create a character vector with the URL:\n\nurl &lt;- \"https://trace.tennessee.edu/utk_graddiss/index.html\"\n\nFirst, we read in the html data from that page:\n\nhtml &lt;- read_html(url)\n\nLet’s have a look at the raw data:\n\nhtml\n\n{html_document}\n&lt;html lang=\"en\"&gt;\n[1] &lt;head&gt;\\n&lt;meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8 ...\n[2] &lt;body&gt;\\n&lt;!-- FILE /srv/sequoia/main/data/trace.tennessee.edu/assets/heade ..."
  },
  {
    "objectID": "r/ws_r_demo_slides.html#extract-all-urls",
    "href": "r/ws_r_demo_slides.html#extract-all-urls",
    "title": "A little demo of programming in",
    "section": "Extract all URLs",
    "text": "Extract all URLs\n\ndat &lt;- html %&gt;% html_elements(\".article-listing a\")\ndat\n\n{xml_nodeset (100)}\n [1] &lt;a href=\"https://trace.tennessee.edu/utk_graddiss/8076\"&gt;Understanding ho ...\n [2] &lt;a href=\"https://trace.tennessee.edu/utk_graddiss/9158\"&gt;Generating Diver ...\n [3] &lt;a href=\"https://trace.tennessee.edu/utk_graddiss/8080\"&gt;FABRICATION, MEA ...\n [4] &lt;a href=\"https://trace.tennessee.edu/utk_graddiss/8086\"&gt;Development and  ...\n [5] &lt;a href=\"https://trace.tennessee.edu/utk_graddiss/8078\"&gt;The Light from P ...\n [6] &lt;a href=\"https://trace.tennessee.edu/utk_graddiss/9185\"&gt;Image Deblurring ...\n [7] &lt;a href=\"https://trace.tennessee.edu/utk_graddiss/8584\"&gt;Clickable Lipid  ...\n [8] &lt;a href=\"https://trace.tennessee.edu/utk_graddiss/8703\"&gt;Retinoic Acid, I ...\n [9] &lt;a href=\"https://trace.tennessee.edu/utk_graddiss/8987\"&gt;Development and  ...\n[10] &lt;a href=\"https://trace.tennessee.edu/utk_graddiss/8734\"&gt;Defining Systemi ...\n[11] &lt;a href=\"https://trace.tennessee.edu/utk_graddiss/8073\"&gt;Investigating th ...\n[12] &lt;a href=\"https://trace.tennessee.edu/utk_graddiss/8088\"&gt;The Disparate Ef ...\n[13] &lt;a href=\"https://trace.tennessee.edu/utk_graddiss/9077\"&gt;Social Wellness  ...\n[14] &lt;a href=\"https://trace.tennessee.edu/utk_graddiss/8077\"&gt;A HIERARCHICAL P ...\n[15] &lt;a href=\"https://trace.tennessee.edu/utk_graddiss/8094\"&gt;Nurse Staffing a ...\n[16] &lt;a href=\"https://trace.tennessee.edu/utk_graddiss/8714\"&gt;Ruinous Natures: ...\n[17] &lt;a href=\"https://trace.tennessee.edu/utk_graddiss/9050\"&gt;Toward Accelerat ...\n[18] &lt;a href=\"https://trace.tennessee.edu/utk_graddiss/8737\"&gt;Implementing the ...\n[19] &lt;a href=\"https://trace.tennessee.edu/utk_graddiss/8074\"&gt;Riding the Wave: ...\n[20] &lt;a href=\"https://trace.tennessee.edu/utk_graddiss/9177\"&gt;A TWO-DIAMETER H ...\n..."
  },
  {
    "objectID": "r/ws_r_demo_slides.html#extract-all-urls-1",
    "href": "r/ws_r_demo_slides.html#extract-all-urls-1",
    "title": "A little demo of programming in",
    "section": "Extract all URLs",
    "text": "Extract all URLs\nWe now have a list of lists\nBefore running for loops, it is important to initialize empty loops. It is much more efficient than growing the result at each iteration\nSo let’s initialize an empty list that we call list_urls of the appropriate size:\n\nlist_urls &lt;- vector(\"list\", length(dat))"
  },
  {
    "objectID": "r/ws_r_demo_slides.html#extract-all-urls-2",
    "href": "r/ws_r_demo_slides.html#extract-all-urls-2",
    "title": "A little demo of programming in",
    "section": "Extract all URLs",
    "text": "Extract all URLs\nNow we can run a loop to fill in our list:\n\nfor (i in seq_along(dat)) {\n  list_urls[[i]] &lt;- dat[[i]] %&gt;% html_attr(\"href\")\n}\n\nLet’s print again the first element of list_urls to make sure all looks good:\n\nlist_urls[[1]]\n\n[1] \"https://trace.tennessee.edu/utk_graddiss/8076\"\n\n\nWe now have a list of URLs (in the form of character vectors) as we wanted"
  },
  {
    "objectID": "r/ws_r_demo_slides.html#extract-data-from-each-page",
    "href": "r/ws_r_demo_slides.html#extract-data-from-each-page",
    "title": "A little demo of programming in",
    "section": "Extract data from each page",
    "text": "Extract data from each page\nWe will now extract the data (date, major, and advisor) for all URLs in our list.\nAgain, before running a for loop, we need to allocate memory first by creating an empty container (here a list):\n\nlist_data &lt;- vector(\"list\", length(list_urls))\n\nfor (i in seq_along(list_urls)) {\n  html &lt;- read_html(list_urls[[i]])\n  date &lt;- html %&gt;%\n    html_element(\"#publication_date p\") %&gt;%\n    html_text2()\n  major &lt;- html %&gt;%\n    html_element(\"#department p\") %&gt;%\n    html_text2()\n  advisor &lt;- html %&gt;%\n    html_element(\"#advisor1 p\") %&gt;%\n    html_text2()\n  Sys.sleep(0.1)  # Add a little delay\n  list_data[[i]] &lt;- cbind(date, major, advisor)\n}"
  },
  {
    "objectID": "r/ws_r_demo_slides.html#store-results-in-dataframe",
    "href": "r/ws_r_demo_slides.html#store-results-in-dataframe",
    "title": "A little demo of programming in",
    "section": "Store results in DataFrame",
    "text": "Store results in DataFrame\nWe can turn this big list into a dataframe:\n\nresult &lt;- do.call(rbind.data.frame, list_data)\n\nWe can capitalize the headers:\n\nnames(result) &lt;- c(\"Date\", \"Major\", \"Advisor\")"
  },
  {
    "objectID": "r/ws_r_demo_slides.html#our-final-data",
    "href": "r/ws_r_demo_slides.html#our-final-data",
    "title": "A little demo of programming in",
    "section": "Our final data",
    "text": "Our final data\nresult is a long dataframe, so we will only print the first few elements:\n\nhead(result, 15)\n\n      Date                          Major               Advisor\n1   5-2023                  Life Sciences       Bode A. Olukolu\n2  12-2023         Industrial Engineering            Hugh Medal\n3   5-2023            Nuclear Engineering           Erik Lukosi\n4   5-2023 Energy Science and Engineering   Kyle R. Gluesenkamp\n5   5-2023                        English Margaret Lazarus Dean\n6  12-2023         Industrial Engineering          Hoon Hwangbo\n7   8-2023                      Chemistry       Michael D. Best\n8   8-2023           Nutritional Sciences         Jiangang Chen\n9  12-2023         Mechanical Engineering      Dustin L. Crouch\n10  8-2023            Counselor Education    Melinda M. Gibbons\n11  5-2023         Mechanical Engineering            Doug Aaron\n12  5-2023        Business Administration           Linda Myers\n13 12-2023            Counselor Education   Joel Foster Diambra\n14  5-2023         Industrial Engineering         John E. Kobza\n15  5-2023                        Nursing       Carole R. Myers"
  },
  {
    "objectID": "r/ws_r_demo_slides.html#save-results-to-file",
    "href": "r/ws_r_demo_slides.html#save-results-to-file",
    "title": "A little demo of programming in",
    "section": "Save results to file",
    "text": "Save results to file\nIf we wanted, we could save our data to a CSV file:\nwrite.csv(result, \"dissertations_data.csv\", row.names = FALSE)"
  },
  {
    "objectID": "r/ws_r_demo_slides.html#data-reading-and-manipulation",
    "href": "r/ws_r_demo_slides.html#data-reading-and-manipulation",
    "title": "A little demo of programming in",
    "section": "Data reading and manipulation",
    "text": "Data reading and manipulation\n\nSpatial vectors: great modern packages are sf or terra\nRaster data: the package terra\n\nI will skip the data preparation due to lack of time, but you can look at the code in this webinar or this workshop"
  },
  {
    "objectID": "r/ws_r_demo_slides.html#mapping-data",
    "href": "r/ws_r_demo_slides.html#mapping-data",
    "title": "A little demo of programming in",
    "section": "Mapping data",
    "text": "Mapping data\nGood options to create maps include ggplot2 (the package we already used for plotting) or tmap"
  },
  {
    "objectID": "r/ws_r_demo_slides.html#map-of-glaciers-in-western-north-america",
    "href": "r/ws_r_demo_slides.html#map-of-glaciers-in-western-north-america",
    "title": "A little demo of programming in",
    "section": "Map of glaciers in western North America",
    "text": "Map of glaciers in western North America\ntm_shape(states, bbox = nwa_bbox) +\n  tm_polygons(col = \"#f2f2f2\", lwd = 0.2) +\n  tm_shape(ak) +\n  tm_borders(col = \"#3399ff\") +\n  tm_fill(col = \"#86baff\") +\n  tm_shape(wes) +\n  tm_borders(col = \"#3399ff\") +\n  tm_fill(col = \"#86baff\") +\n  tm_layout(\n    title = \"Glaciers of Western North America\",\n    title.position = c(\"center\", \"top\"),\n    title.size = 1.1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.06, 0.01, 0.09, 0.01),\n    outer.margins = 0,\n    frame.lwd = 0.2\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    size = 1.2,\n    text.size = 0.6\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 1000, 2000),\n    position = c(\"right\", \"BOTTOM\")\n  )"
  },
  {
    "objectID": "r/ws_r_demo_slides.html#multi-layer-map-of-the-retreat-of-a-glacier",
    "href": "r/ws_r_demo_slides.html#multi-layer-map-of-the-retreat-of-a-glacier",
    "title": "A little demo of programming in",
    "section": "Multi-layer map of the retreat of a glacier",
    "text": "Multi-layer map of the retreat of a glacier\ntm_shape(ag) +\n  tm_polygons(\"year\", palette = \"Blues\") +\n  tm_layout(\n    title = \"Agassiz Glacier\",\n    title.position = c(\"center\", \"top\"),\n    legend.position = c(\"left\", \"bottom\"),\n    legend.title.color = \"#fcfcfc\",\n    legend.text.size = 1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.07, 0.03, 0.07, 0.03),\n    outer.margins = 0\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    text.size = 0.7\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 0.5, 1),\n    position = c(\"right\", \"BOTTOM\"),\n    text.size = 1\n  )"
  },
  {
    "objectID": "r/ws_r_demo_slides.html#animated-map-of-the-retreat-of-a-glacier",
    "href": "r/ws_r_demo_slides.html#animated-map-of-the-retreat-of-a-glacier",
    "title": "A little demo of programming in",
    "section": "Animated map of the retreat of a glacier",
    "text": "Animated map of the retreat of a glacier\ntmap_animation(tm_shape(ag) +\n                 tm_polygons(col = \"#86baff\") +\n                 tm_layout(\n                   title = \"Agassiz Glacier\",\n                   title.position = c(\"center\", \"top\"),\n                   legend.position = c(\"left\", \"bottom\"),\n                   legend.title.color = \"#fcfcfc\",\n                   legend.text.size = 1,\n                   bg.color = \"#fcfcfc\",\n                   inner.margins = c(0.08, 0, 0.08, 0),\n                   outer.margins = 0,\n                   panel.label.bg.color = \"#fcfcfc\"\n                 ) +\n                 tm_compass(\n                   type = \"arrow\",\n                   position = c(\"right\", \"top\"),\n                   text.size = 0.7\n                 ) +\n                 tm_scale_bar(\n                   breaks = c(0, 0.5, 1),\n                   position = c(\"right\", \"BOTTOM\"),\n                   text.size = 1\n                 ) +\n                 tm_facets(\n                   along = \"year\",\n                   free.coords = F\n                 )filename = \"ag.gif\",\n               dpi = 300,\n               inner.margins = c(0.08, 0, 0.08, 0),\n               delay = 100\n               )"
  },
  {
    "objectID": "r/ws_r_demo_slides.html#three-day-introductory-workshop-for-the-hss",
    "href": "r/ws_r_demo_slides.html#three-day-introductory-workshop-for-the-hss",
    "title": "A little demo of programming in",
    "section": "Three-day introductory workshop for the HSS",
    "text": "Three-day introductory workshop for the HSS\nAs a follow-up to this year HSS Series, we will be offering a free three-day hands-on introduction to R for researchers in the humanities, arts, and social sciences\nYou can register here"
  },
  {
    "objectID": "r/ws_r_demo_slides.html#beyond-the-hss-series",
    "href": "r/ws_r_demo_slides.html#beyond-the-hss-series",
    "title": "A little demo of programming in",
    "section": "Beyond the HSS series",
    "text": "Beyond the HSS series\nEach region under the Alliance offers regular courses and workshops in R (and many other topics)\nIn the west, Alex Razoumov and myself offer regular free workshops, courses, and webinars for researchers in Canadian academic institutions\nYou can find our program here or join our mailing list here\n\n\n\n\n Back to workshop page"
  },
  {
    "objectID": "r/ws_hss_intro.html",
    "href": "r/ws_hss_intro.html",
    "title": "Introduction to R for the humanities",
    "section": "",
    "text": "R is a free and open-source programming language for statistical computing, modelling, and graphics, with an unbeatable collection of statistical packages. It is extremely popular in some academic fields such as statistics, biology, bioinformatics, data mining, data analysis, and linguistics.\nThis introductory course does not assume any prior knowledge.",
    "crumbs": [
      "R",
      "<b><em>Workshops</em></b>",
      "Intro R for the humanities"
    ]
  },
  {
    "objectID": "r/ws_hss_intro.html#running-r",
    "href": "r/ws_hss_intro.html#running-r",
    "title": "Introduction to R for the humanities",
    "section": "Running R",
    "text": "Running R\nR being an interpreted language, it can be run non-interactively or interactively.\n\nRunning R non-interactively\nIf you write code in a text file (called a script), you can then execute it with:\nRscript my_script.R\n\nThe command to execute scripts is Rscript rather than R.\nBy convention, R scripts take the extension .R.\n\n\n\nRunning R interactively\nThere are several ways to run R interactively.\n\nDirectly in the console (the name for the R shell):\n\n\n\nIn Jupyter with the R kernel (IRkernel package).\nIn another IDE (e.g. in Emacs with ESS).\nIn the RStudio IDE.\n\nThe RStudio IDE is popular and this is what we will use today. RStudio can can be run locally, but for this course, we will use an RStudio server.\n\n\nAccessing our RStudio server\nFor this workshop, we will use a temporary RStudio server.\nTo access it, go to the website given during the workshop and sign in using the username and password you will be given (you can ignore the OTP entry).\nThis will take you to our JupyterHub. There, click on the “RStudio” button and our RStudio server will open in a new tab.\n\n\nUsing RStudio\nFor those unfamiliar with the RStudio IDE, you can download the following cheatsheet:\n\n\n\nfrom Posit Cheatsheets",
    "crumbs": [
      "R",
      "<b><em>Workshops</em></b>",
      "Intro R for the humanities"
    ]
  },
  {
    "objectID": "r/ws_hss_intro.html#help-and-documentation",
    "href": "r/ws_hss_intro.html#help-and-documentation",
    "title": "Introduction to R for the humanities",
    "section": "Help and documentation",
    "text": "Help and documentation\nFor some general documentation on R, you can run:\nhelp.start()\nTo get help on a function (e.g. sum), you can run:\nhelp(sum)\nDepending on your settings, this will open a documentation for sum in a pager or in your browser.",
    "crumbs": [
      "R",
      "<b><em>Workshops</em></b>",
      "Intro R for the humanities"
    ]
  },
  {
    "objectID": "r/ws_hss_intro.html#basic-syntax",
    "href": "r/ws_hss_intro.html#basic-syntax",
    "title": "Introduction to R for the humanities",
    "section": "Basic syntax",
    "text": "Basic syntax\n\nAssignment\nR can accept the equal sign (=) for assignments, but it is more idiomatic to use the assignment sign (&lt;-) whenever you bind a name to a value and to use the equal sign everywhere else.\n\na &lt;- 3\n\nOnce you have bound a name to a value, you can recall the value with that name:\n\na  # Note that you do not need to use a print() function in R\n\n[1] 3\n\n\nYou can remove an object from the environment by deleting its name:\n\nrm(a)\na\n\nError in eval(expr, envir, enclos): object 'a' not found\n\n\nThe garbage collector will take care of deleting the object itself from memory.\n\n\nComments\nAnything to the left of # is a comment and is ignored by R:\n\n# This is an inline comment\n\na &lt;- 3  # This is also a comment",
    "crumbs": [
      "R",
      "<b><em>Workshops</em></b>",
      "Intro R for the humanities"
    ]
  },
  {
    "objectID": "r/ws_hss_intro.html#data-types-and-structures",
    "href": "r/ws_hss_intro.html#data-types-and-structures",
    "title": "Introduction to R for the humanities",
    "section": "Data types and structures",
    "text": "Data types and structures\n\n\n\nDimension\nHomogeneous\nHeterogeneous\n\n\n\n\n1 d\nAtomic vector\nList\n\n\n2 d\nMatrix\nData frame\n\n\n3 d\nArray\n\n\n\n\n\nAtomic vectors\n\nvec &lt;- c(2, 4, 1)\nvec\n\n[1] 2 4 1\n\ntypeof(vec)\n\n[1] \"double\"\n\nstr(vec)\n\n num [1:3] 2 4 1\n\n\n\nvec &lt;- c(TRUE, TRUE, NA, FALSE)\nvec\n\n[1]  TRUE  TRUE    NA FALSE\n\ntypeof(vec)\n\n[1] \"logical\"\n\nstr(vec)\n\n logi [1:4] TRUE TRUE NA FALSE\n\n\n\nNA (“Not Available”) is a logical constant of length one. It is an indicator for a missing value.\n\nVectors are homogeneous, so all elements need to be of the same type.\nIf you use elements of different types, R will convert some of them to ensure that they become of the same type:\n\nvec &lt;- c(\"This is a string\", 3, \"test\")\nvec\n\n[1] \"This is a string\" \"3\"                \"test\"            \n\ntypeof(vec)\n\n[1] \"character\"\n\nstr(vec)\n\n chr [1:3] \"This is a string\" \"3\" \"test\"\n\n\n\nvec &lt;- c(TRUE, 3, FALSE)\nvec\n\n[1] 1 3 0\n\ntypeof(vec)\n\n[1] \"double\"\n\nstr(vec)\n\n num [1:3] 1 3 0\n\n\n\n\nData frames\nData frames contain tabular data. Under the hood, a data frame is a list of vectors.\n\ndat &lt;- data.frame(\n  country = c(\"Canada\", \"USA\", \"Mexico\"),\n  var = c(2.9, 3.1, 4.5)\n)\ndat\n\n  country var\n1  Canada 2.9\n2     USA 3.1\n3  Mexico 4.5\n\ntypeof(dat)\n\n[1] \"list\"\n\nstr(dat)\n\n'data.frame':   3 obs. of  2 variables:\n $ country: chr  \"Canada\" \"USA\" \"Mexico\"\n $ var    : num  2.9 3.1 4.5\n\nlength(dat)\n\n[1] 2\n\ndim(dat)\n\n[1] 3 2",
    "crumbs": [
      "R",
      "<b><em>Workshops</em></b>",
      "Intro R for the humanities"
    ]
  },
  {
    "objectID": "r/ws_hss_intro.html#function-definition",
    "href": "r/ws_hss_intro.html#function-definition",
    "title": "Introduction to R for the humanities",
    "section": "Function definition",
    "text": "Function definition\n\ncompare &lt;- function(x, y) {\n  x == y\n}\n\nWe can now use our function:\n\ncompare(2, 3)\n\n[1] FALSE\n\n\nNote that the result of the last statement is printed automatically:\n\ntest &lt;- function(x, y) {\n  x\n  y\n}\ntest(2, 3)\n\n[1] 3\n\n\nIf you want to return other results, you need to explicitly use the print() function:\n\ntest &lt;- function(x, y) {\n  print(x)\n  y\n}\ntest(2, 3)\n\n[1] 2\n\n\n[1] 3",
    "crumbs": [
      "R",
      "<b><em>Workshops</em></b>",
      "Intro R for the humanities"
    ]
  },
  {
    "objectID": "r/ws_hss_intro.html#control-flow",
    "href": "r/ws_hss_intro.html#control-flow",
    "title": "Introduction to R for the humanities",
    "section": "Control flow",
    "text": "Control flow\n\nConditionals\n\ntest_sign &lt;- function(x) {\n  if (x &gt; 0) {\n    \"x is positif\"\n  } else if (x &lt; 0) {\n    \"x is negatif\"\n  } else {\n    \"x is equal to zero\"\n  }\n}\n\n\ntest_sign(3)\n\n[1] \"x is positif\"\n\ntest_sign(-2)\n\n[1] \"x is negatif\"\n\ntest_sign(0)\n\n[1] \"x is equal to zero\"\n\n\n\n\nLoops\n\nfor (i in 1:10) {\n  print(i)\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n[1] 7\n[1] 8\n[1] 9\n[1] 10\n\n\nNotice that here we need to use the print() function.",
    "crumbs": [
      "R",
      "<b><em>Workshops</em></b>",
      "Intro R for the humanities"
    ]
  },
  {
    "objectID": "r/ws_hss_intro.html#packages",
    "href": "r/ws_hss_intro.html#packages",
    "title": "Introduction to R for the humanities",
    "section": "Packages",
    "text": "Packages\nPackages are a set of functions and/or data that add functionality to R.\n\nLooking for packages\n\nPackage finder\nYour peers and the literature\n\n\n\nPackage documentation\n\nList of CRAN packages\nPackage documentation\n\n\n\nManaging R packages\nR packages can be installed, updated, and removed from within R:\ninstall.packages(\"package-name\")\nremove.packages(\"package-name\")\nupdate_packages()\n\n\nLoading packages\nTo make a package available in an R session, you load it with the library() function.\n\nExample:\n\nlibrary(readxl)\nAlternatively, you can access a function from a package without loading it with the syntax: package::function().\n\nExample:\n\nreadxl::read_excel(\"file.xlsx\")",
    "crumbs": [
      "R",
      "<b><em>Workshops</em></b>",
      "Intro R for the humanities"
    ]
  },
  {
    "objectID": "r/ws_hss_intro.html#publishing",
    "href": "r/ws_hss_intro.html#publishing",
    "title": "Introduction to R for the humanities",
    "section": "Publishing",
    "text": "Publishing\nYou might have heard of R Markdown. It allows for the creation of dynamic publication-quality documents mixing code blocks, text, graphs…\nThe team which created R Markdown has now created an even better tool: Quarto. If you are interested in an introduction to this tool, you can have a look at our workshop or our webinar on Quarto.",
    "crumbs": [
      "R",
      "<b><em>Workshops</em></b>",
      "Intro R for the humanities"
    ]
  },
  {
    "objectID": "r/ws_hss_intro.html#resources",
    "href": "r/ws_hss_intro.html#resources",
    "title": "Introduction to R for the humanities",
    "section": "Resources",
    "text": "Resources\n\nAlliance wiki\n\nR page\n\n\n\nR main site\n\nDownload page\n\n\n\nRStudio\n\nPosit site (Posit is the brand new name of the RStudio company)\nPosit cheatsheets\n\n\n\nSoftware Carpentry online workshop\n\nData analysis using R in the digital humanities\n\n\n\nOnline book\n\nR for Data Science (heavily based on the tidyverse)",
    "crumbs": [
      "R",
      "<b><em>Workshops</em></b>",
      "Intro R for the humanities"
    ]
  },
  {
    "objectID": "r/ws_hss_intro.html#recording",
    "href": "r/ws_hss_intro.html#recording",
    "title": "Introduction to R for the humanities",
    "section": "Recording",
    "text": "Recording\n\nVideos of this workshop for the Digital Research Alliance of Canada HSS Winter Series 2023:\n\n\nFirst part\n\n\n\nSecond part",
    "crumbs": [
      "R",
      "<b><em>Workshops</em></b>",
      "Intro R for the humanities"
    ]
  },
  {
    "objectID": "r/wb_hpc_slides.html#intel-vs-gcc-compilers",
    "href": "r/wb_hpc_slides.html#intel-vs-gcc-compilers",
    "title": "High-performance research computing in ",
    "section": "Intel vs GCC compilers",
    "text": "Intel vs GCC compilers\nTo compile R packages, you need a C compiler.\nIn theory, you could use the proprietary Intel compiler which is loaded by default on the Alliance clusters, but it is recommended to replace it with the GCC compiler (R packages can even be compiled with Clang and LLVM, but the default GCC compiler is the best way to avoid headaches).\nIt is thus much simpler to always load a gcc module before loading an r module."
  },
  {
    "objectID": "r/wb_hpc_slides.html#r-module",
    "href": "r/wb_hpc_slides.html#r-module",
    "title": "High-performance research computing in ",
    "section": "R module",
    "text": "R module\nTo see what versions of R are available on a cluster, run:\nmodule spider r\nTo see the dependencies of a particular version (e.g. r/4.2.1), run:\nmodule spider r/4.2.1\n\nStdEnv/2020 is a required module for this version.\nOn most Alliance clusters, it is automatically loaded, so you don’t need to include it. You can double-check with module list or you can include it (before r/4.2.1) just to be sure.\n\nFinally, load your modules:\nmodule load StdEnv/2020 gcc/11.3.0 r/4.2.1"
  },
  {
    "objectID": "r/wb_hpc_slides.html#scripts",
    "href": "r/wb_hpc_slides.html#scripts",
    "title": "High-performance research computing in ",
    "section": "Scripts",
    "text": "Scripts\nTo run an R script called &lt;your_script&gt;.R, you first need to write a job script:\n\nExample:\n\n\n&lt;your_job&gt;.sh\n\n#!/bin/bash\n#SBATCH --account=def-&lt;your_account&gt;\n#SBATCH --time=15\n#SBATCH --mem-per-cpu=3000M\n#SBATCH --cpus-per-task=4\n#SBATCH --job-name=\"&lt;your_job&gt;\"\nmodule load StdEnv/2020 gcc/11.3.0 r/4.2.1\nRscript &lt;your_script&gt;.R   # Note that R scripts are run with the command `Rscript`\n\n\nThen launch your job with:\nsbatch &lt;your_job&gt;.sh\nYou can monitor your job with sq (an alias for squeue -u $USER $@)."
  },
  {
    "objectID": "r/wb_hpc_slides.html#interactive-jobs",
    "href": "r/wb_hpc_slides.html#interactive-jobs",
    "title": "High-performance research computing in ",
    "section": "Interactive jobs",
    "text": "Interactive jobs\n\nWhile it is fine to run R on the login node when you install packages, you must start a SLURM job before any heavy computation.\n\nTo run R interactively, you should launch an salloc session.\nHere is what I will use for this webinar:\nsalloc --time=1:10:00 --mem-per-cpu=7000M --ntasks=8\nThis takes me to a compute node where I can launch R to run computations:\nR"
  },
  {
    "objectID": "r/wb_hpc_slides.html#profiling",
    "href": "r/wb_hpc_slides.html#profiling",
    "title": "High-performance research computing in ",
    "section": "Profiling",
    "text": "Profiling\nThe first thing to do if you want to improve your code efficiency is to identify bottlenecks in your code. Common tools are:\n\nthe base R function Rprof()\nthe package profvis\n\nprofvis is a newer tool, built by posit (formerly RStudio). Under the hood, it runs Rprof() to collect data, then produces an interactive html widget with a flame graph that allows for an easy visual identification of slow sections of code. While this tool integrates well within the RStudio IDE or the RPubs ecosystem, it is not very well suited for remote work on a cluster. One option is to profile your code with small data on your own machine. Another option is to use the base profiler with Rprof() directly as in this example."
  },
  {
    "objectID": "r/wb_hpc_slides.html#benchmarking",
    "href": "r/wb_hpc_slides.html#benchmarking",
    "title": "High-performance research computing in ",
    "section": "Benchmarking",
    "text": "Benchmarking\nOnce you have identified expressions that are particularly slow, you can use benchmarking tools to compare variations of the code.\nIn the most basic fashion, you can use system.time(), but this is limited and imprecise.\nThe microbenchmark package is a popular option.\nIt gives the minimum time, lower quartile, mean, median, upper quartile, and maximum time of R expressions.\nThe newer bench package has less overhead, is more accurate, and—for sequential code—gives information on memory usage and garbage collections. This is the package I will use today."
  },
  {
    "objectID": "r/wb_hpc_slides.html#multi-threading",
    "href": "r/wb_hpc_slides.html#multi-threading",
    "title": "High-performance research computing in ",
    "section": "Multi-threading",
    "text": "Multi-threading\nWe talk about multi-threading when a single process (with its own memory) runs multiple threads.\nThe execution can happen in parallel—if each thread has access to a CPU core—or by alternating some of the threads on some CPU cores.\nBecause all threads in a process write to the same memory addresses, multi-threading can lead to race conditions.\nMulti-threading does not seem to be a common approach to parallelizing R code."
  },
  {
    "objectID": "r/wb_hpc_slides.html#multi-processing-in-shared-memory",
    "href": "r/wb_hpc_slides.html#multi-processing-in-shared-memory",
    "title": "High-performance research computing in ",
    "section": "Multi-processing in shared memory",
    "text": "Multi-processing in shared memory\nMulti-processing in shared memory happens when multiple processes execute code on multiple CPU cores of a single node (or a single machine).\nThe different processes need to communicate with each other, but because they are all running on the CPU cores of a single node, messages can pass via shared memory."
  },
  {
    "objectID": "r/wb_hpc_slides.html#multi-processing-in-distributed-memory",
    "href": "r/wb_hpc_slides.html#multi-processing-in-distributed-memory",
    "title": "High-performance research computing in ",
    "section": "Multi-processing in distributed memory",
    "text": "Multi-processing in distributed memory\nWhen processes involved in the execution of some code run on multiple nodes of a cluster, messages between them need to travel over the cluster interconnect. In that case, we talk about distributed memory."
  },
  {
    "objectID": "r/wb_hpc_slides.html#package-parallel-base-r",
    "href": "r/wb_hpc_slides.html#package-parallel-base-r",
    "title": "High-performance research computing in ",
    "section": "Package parallel (base R)",
    "text": "Package parallel (base R)\nThe parallel package has been part of the “base” package group since version 2.14.0.\nThis means that it is comes with R.\nMost parallel approaches in R build on this package.\nWe will make use of it to create and close an ad-hoc cluster.\n\nThe parallelly package adds functionality to the parallel package."
  },
  {
    "objectID": "r/wb_hpc_slides.html#package-foreach",
    "href": "r/wb_hpc_slides.html#package-foreach",
    "title": "High-performance research computing in ",
    "section": "Package foreach",
    "text": "Package foreach\nThe foreach package implements a looping construct without an explicit counter. It doesn’t require the preallocation of an output container, it brings to R an equivalent of the Python or Julia list comprehensions, and mostly, it allows for an easy execution of loops in parallel. Unlike loops, it creates variables (loops are used for their side-effect).\nLet’s look at an example to calculate the sum of 1e4 random vectors of length 3.\nWe will use foreach and iterators (which creates convenient iterators for foreach):\n\nlibrary(foreach)\nlibrary(iterators)"
  },
  {
    "objectID": "r/wb_hpc_slides.html#package-future",
    "href": "r/wb_hpc_slides.html#package-future",
    "title": "High-performance research computing in ",
    "section": "Package future",
    "text": "Package future\nA future is an object that acts as an abstract representation for a value in the future. A future can be resolved (if the value has been computed) or unresolved. If the value is queried while the future is unresolved, the process is blocked until the future is resolved.\nFutures allow for asynchronous and parallel evaluations. The future package provides a simple and unified API to evaluate futures."
  },
  {
    "objectID": "r/wb_hpc_slides.html#plans",
    "href": "r/wb_hpc_slides.html#plans",
    "title": "High-performance research computing in ",
    "section": "Plans",
    "text": "Plans\nThe future package does this thanks to the plan function:\n\nplan(sequential): futures are evaluated sequentially in the current R session\nplan(multisession): futures are evaluated by new R sessions spawned in the background (multi-processing in shared memory)\nplan(multicore): futures are evaluated in processes forked from the existing process (multi-processing in shared memory)\nplan(cluster): futures are evaluated on an ad-hoc cluster (allows for distributed parallelism across multiple nodes)"
  },
  {
    "objectID": "r/wb_hpc_slides.html#consistency",
    "href": "r/wb_hpc_slides.html#consistency",
    "title": "High-performance research computing in ",
    "section": "Consistency",
    "text": "Consistency\nTo ensure a consistent behaviour across plans, all evaluations are done in a local environment:\n\nlibrary(future)\n\na &lt;- 1\n\nb %&lt;-% {\n  a &lt;- 2\n}\n\na\n\n[1] 1"
  },
  {
    "objectID": "r/wb_hpc_slides.html#lets-return-to-our-example",
    "href": "r/wb_hpc_slides.html#lets-return-to-our-example",
    "title": "High-performance research computing in ",
    "section": "Let’s return to our example",
    "text": "Let’s return to our example\nWe had:\nset.seed(2)\nresult2 &lt;- foreach(icount(1e4), .combine = '+') %do% runif(3)\nWe can replace %do% with %dopar%:\nset.seed(2)\nresult3 &lt;- foreach(icount(1e4), .combine = '+') %dopar% runif(3)\nSince we haven’t registered any parallel backend, the expression will still be evaluated sequentially."
  },
  {
    "objectID": "r/wb_hpc_slides.html#load-packages",
    "href": "r/wb_hpc_slides.html#load-packages",
    "title": "High-performance research computing in ",
    "section": "Load packages",
    "text": "Load packages\nFor this toy example, I will use a modified version of one of the examples in the foreach vignette: I will b uild a classification model made of a forest of decision trees thanks to the randomForest package.\nBecause the code includes randomly generated numbers, I will use the doRNG package which replaces foreach::%dopar% wit h doRNG::%dorng%. This follows the recommendations of Pierre L’Ecuyer (1999)1 and ensures reproducibility.\nlibrary(doFuture)       # This will also load the `future` package\nlibrary(doRNG)          # This will also load the `foreach` package\nlibrary(randomForest)\nlibrary(bench)          # To do some benchmarking\nLoading required package: foreach\nLoading required package: future\nLoading required package: rngtools\nL’Ecuyer, P. (1999). Good parameters and implementations for combined multiple recursive random number generators. Operations Research, 47, 159–164."
  },
  {
    "objectID": "r/wb_hpc_slides.html#the-code-to-parallelize",
    "href": "r/wb_hpc_slides.html#the-code-to-parallelize",
    "title": "High-performance research computing in ",
    "section": "The code to parallelize",
    "text": "The code to parallelize\nThe goal is to create a classifier based on some data (here a matrix of random numbers for simplicity) and a response variable (as factor). This model could then be passed in the predict() function with novel data to generate predictions of classification. But here we are only interested in the creation of the model as this is the part that is computationally intensive. We aren’t interested in actually using it.\nset.seed(11)\ntraindata &lt;- matrix(runif(1e5), 100)\nfac &lt;- gl(2, 50)\n\nrf &lt;- foreach(ntree = rep(250, 8), .combine = combine) %do%\n  randomForest(x = traindata, y = fac, ntree = ntree)\n\nrf\nCall:\n randomForest(x = traindata, y = fac, ntree = ntree)\n               Type of random forest: classification\n                     Number of trees: 2000\nNo. of variables tried at each split: 31"
  },
  {
    "objectID": "r/wb_hpc_slides.html#reference-timing",
    "href": "r/wb_hpc_slides.html#reference-timing",
    "title": "High-performance research computing in ",
    "section": "Reference timing",
    "text": "Reference timing\nThis is the non parallelizable code with %do%:\ntref &lt;- mark(\n  rf1 &lt;- foreach(ntree = rep(250, 8), .combine = combine) %do%\n    randomForest(x = traindata, y = fac, ntree = ntree),\n  memory = FALSE\n)\n\ntref$median\n[1] 5.66s"
  },
  {
    "objectID": "r/wb_hpc_slides.html#plan-sequential",
    "href": "r/wb_hpc_slides.html#plan-sequential",
    "title": "High-performance research computing in ",
    "section": "Plan sequential",
    "text": "Plan sequential\nThis is the parallelizable foreach code, but run sequentially:\nregisterDoFuture()   # Set the parallel backend\nplan(sequential)     # Set the evaluation strategy\n\n# Using bench::mark()\ntseq &lt;- mark(\n  rf2 &lt;- foreach(ntree = rep(250, 8), .combine = combine) %dorng%\n    randomForest(x = traindata, y = fac, ntree = ntree),\n  memory = FALSE\n)\n\ntseq$median\n[1] 5.78s\n\nNo surprise: those are similar."
  },
  {
    "objectID": "r/wb_hpc_slides.html#multi-processing-in-shared-memory-1",
    "href": "r/wb_hpc_slides.html#multi-processing-in-shared-memory-1",
    "title": "High-performance research computing in ",
    "section": "Multi-processing in shared memory",
    "text": "Multi-processing in shared memory\nfuture provides availableCores() to detect the number of available cores:\navailableCores()\nsystem\n     4\n\nSimilar to parallel::detectCores().\n\nThis detects the number of CPU cores available to me on the current compute node, that is, what I can use for shared memory multi-processing."
  },
  {
    "objectID": "r/wb_hpc_slides.html#plan-multisession",
    "href": "r/wb_hpc_slides.html#plan-multisession",
    "title": "High-performance research computing in ",
    "section": "Plan multisession",
    "text": "Plan multisession\nShared memory multi-processing can be run with plan(multisession) that will spawn new R sessions in the background to evaluate futures:\nplan(multisession)\n\ntms &lt;- mark(\n  rf2 &lt;- foreach(ntree = rep(250, 8), .combine = combine) %dorng%\n    randomForest(x = traindata, y = fac, ntree = ntree),\n  memory = FALSE\n)\n\ntms$median\n[1] 2s\n\nWe got a speedup of 5.78 / 2 = 2.9."
  },
  {
    "objectID": "r/wb_hpc_slides.html#plan-multicore",
    "href": "r/wb_hpc_slides.html#plan-multicore",
    "title": "High-performance research computing in ",
    "section": "Plan multicore",
    "text": "Plan multicore\nShared memory multi-processing can also be run with plan(multicore) (except on Windows) that will fork the current R process to evaluate futures:\nplan(multicore)\n\ntmc &lt;- mark(\n  rf2 &lt;- foreach(ntree = rep(250, 8), .combine = combine) %dorng%\n    randomForest(x = traindata, y = fac, ntree = ntree),\n  memory = FALSE\n)\n\ntmc$median\n[1] 1.9s\n\nWe got a very similar speedup of 5.78 / 1.9 = 3.0."
  },
  {
    "objectID": "r/wb_hpc_slides.html#multi-processing-in-distributed-memory-1",
    "href": "r/wb_hpc_slides.html#multi-processing-in-distributed-memory-1",
    "title": "High-performance research computing in ",
    "section": "Multi-processing in distributed memory",
    "text": "Multi-processing in distributed memory\nI requested 8 tasks from Slurm on a training cluster made of nodes with 4 CPU cores each. Let’s verify that I got them by accessing the SLURM_NTASKS environment variable from within R:\nas.numeric(Sys.getenv(\"SLURM_NTASKS\"))\n[1] 8\nI can create a character vector with the name of the node each task is running on:\n(hosts &lt;- system(\"srun hostname | cut -f 1 -d '.'\", intern = TRUE))\nchr [1:8] \"node1\" \"node1\" \"node1\" \"node1\" \"node2\" \"node2\" \"node2\" \"node2\"\nThis allows me to create a cluster of workers:\n(cl &lt;- parallel::makeCluster(hosts))      # Defaults to type=\"PSOCK\"\nsocket cluster with 8 nodes on hosts ‘node1’, ‘node2’"
  },
  {
    "objectID": "r/wb_hpc_slides.html#plan-cluster",
    "href": "r/wb_hpc_slides.html#plan-cluster",
    "title": "High-performance research computing in ",
    "section": "Plan cluster",
    "text": "Plan cluster\nI can now try the code with distributed parallelism using all 8 CPU cores across both nodes:\nplan(cluster, workers = cl)\n\ntdis &lt;- mark(\n  rf2 &lt;- foreach(ntree = rep(250, 8), .combine = combine) %dorng%\n    randomForest(x = traindata, y = fac, ntree = ntree),\n  memory = FALSE\n)\n\ntdis$median\n[1] 1.14s\n\nSpeedup: 5.78 / 1.14 = 5.1.\n\nThe cluster of workers can be stopped with:\nparallel::stopCluster(cl)"
  },
  {
    "objectID": "r/wb_hpc_slides.html#alternative-approaches",
    "href": "r/wb_hpc_slides.html#alternative-approaches",
    "title": "High-performance research computing in ",
    "section": "Alternative approaches",
    "text": "Alternative approaches\nThe multidplyr package partitions data frames across worker processes, allows you to run the usual tidyverse functions on each partition, then collects the processed data.\nThe furrr package is a parallel equivalent to the purrr package from the tidyverse.\nIf you work with genomic data, you might want to have a look at the BiocParallel package from Bioconductor.\nYet another option to run distributed R code is to use the sparklyr package (an R interface to Spark).\nRmpi is a wrapper to MPI (Message-Passing Interface). It has proved slow and problematic on Cedar though.\nThe boot package provides functions and datasets specifically for bootstrapping in parallel."
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#types-of-spatial-data",
    "href": "r/wb_gis_mapping_slides.html#types-of-spatial-data",
    "title": "GIS mapping with R",
    "section": "Types of spatial data",
    "text": "Types of spatial data\nVector data\nDiscrete objects\nContain:  - geometry:  shape & location of the objects\n    - attributes:  additional variables (e.g. name, year, type)\nCommon file format:  GeoJSON, shapefile\n\nExamples: countries, roads, rivers, towns"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#types-of-spatial-data-1",
    "href": "r/wb_gis_mapping_slides.html#types-of-spatial-data-1",
    "title": "GIS mapping with R",
    "section": "Types of spatial data",
    "text": "Types of spatial data\nRaster data\nContinuous phenomena or spatial fields\nCommon file formats:  TIFF, GeoTIFF, NetCDF, Esri grid\n\nExamples: temperature, air quality, elevation, water depth"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#vector-data-1",
    "href": "r/wb_gis_mapping_slides.html#vector-data-1",
    "title": "GIS mapping with R",
    "section": "Vector data",
    "text": "Vector data\nTypes\n\npoint:       single set of coordinates\nmulti-point:   multiple sets of coordinates\npolyline:      multiple sets for which the order matters\nmulti-polyline:  multiple of the above\npolygon:      same as polyline but first & last sets are the same\nmulti-polygon:  multiple of the above"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#raster-data-1",
    "href": "r/wb_gis_mapping_slides.html#raster-data-1",
    "title": "GIS mapping with R",
    "section": "Raster data",
    "text": "Raster data\nGrid of equally sized rectangular cells containing values for some variables\nSize of cells = resolution\nFor computing efficiency, rasters do not have coordinates of each cell, but the bounding box & the number of rows & columns"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#coordinate-reference-systems-crs",
    "href": "r/wb_gis_mapping_slides.html#coordinate-reference-systems-crs",
    "title": "GIS mapping with R",
    "section": "Coordinate Reference Systems (CRS)",
    "text": "Coordinate Reference Systems (CRS)\nA location on Earth’s surface can be identified by its coordinates & some reference system called CRS\nThe coordinates (x, y) are called longitude & latitude\nThere can be a 3rd coordinate (z) for elevation or other measurement—usually a vertical one\nAnd a 4th (m) for some other data attribute—usually a horizontal measurement\nIn 3D, longitude & latitude are expressed in angular units (e.g. degrees) & the reference system needed is an angular CRS or geographic coordinate system (GCS)\nIn 2D, they are expressed in linear units (e.g. meters) & the reference system needed is a planar CRS or projected coordinate system (PCS)"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#datums",
    "href": "r/wb_gis_mapping_slides.html#datums",
    "title": "GIS mapping with R",
    "section": "Datums",
    "text": "Datums\nSince the Earth is not a perfect sphere, we use spheroidal models to represent its surface. Those are called geodetic datums\nSome datums are global, others local (more accurate in a particular area of the globe, but only useful there)\n\nExamples of commonly used global datums:\n\nWGS84 (World Geodesic System 1984)\nNAD83 (North American Datum of 1983)"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#angular-crs",
    "href": "r/wb_gis_mapping_slides.html#angular-crs",
    "title": "GIS mapping with R",
    "section": "Angular CRS",
    "text": "Angular CRS\nAn angular CRS contains a datum, an angular unit & references such as a prime meridian (e.g. the Royal Observatory, Greenwich, England)\nIn an angular CRS or GCS:\n\nLongitude (\\(\\lambda\\)) represents the angle between the prime meridian & the meridian that passes through that location\nLatitude (\\(\\phi\\)) represents the angle between the line that passes through the center of the Earth & that location & its projection on the equatorial plane\n\nLongitude & latitude are thus angular coordinates"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#projections",
    "href": "r/wb_gis_mapping_slides.html#projections",
    "title": "GIS mapping with R",
    "section": "Projections",
    "text": "Projections\nTo create a two-dimensional map, you need to project this 3D angular CRS into a 2D one\nVarious projections offer different characteristics. For instance:\n\nsome respect areas (equal-area)\nsome respect the shape of geographic features (conformal)\nsome almost respect both for small areas\n\nIt is important to choose one with sensible properties for your goals\n\nExamples of projections:\n\nMercator\nUTM\nRobinson"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#planar-crs",
    "href": "r/wb_gis_mapping_slides.html#planar-crs",
    "title": "GIS mapping with R",
    "section": "Planar CRS",
    "text": "Planar CRS\nA planar CRS is defined by a datum, a projection & a set of parameters such as a linear unit & the origins\nCommon planar CRS have been assigned a unique ID called EPSG code which is much more convenient to use\nIn a planar CRS, coordinates will not be in degrees anymore but in meters (or other length unit)"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#projecting-into-a-new-crs",
    "href": "r/wb_gis_mapping_slides.html#projecting-into-a-new-crs",
    "title": "GIS mapping with R",
    "section": "Projecting into a new CRS",
    "text": "Projecting into a new CRS\nYou can change the projection of your data\nVector data won’t suffer any loss of precision, but raster data will\n→  best to try to avoid reprojecting rasters: if you want to combine various datasets which have different projections, reproject vector data instead"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#resources",
    "href": "r/wb_gis_mapping_slides.html#resources",
    "title": "GIS mapping with R",
    "section": "Resources",
    "text": "Resources\nOpen GIS data\nFree GIS Data: list of free GIS datasets\nBooks\nGeocomputation with R by Robin Lovelace, Jakub Nowosad & Jannes Muenchow\nSpatial Data Science by Edzer Pebesma & Roger Bivand\nSpatial Data Science with R by Robert J. Hijmans\nUsing Spatial Data with R by Claudia A. Engel\nTutorial\nAn Introduction to Spatial Data Analysis and Visualisation in R by the CDRC"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#resources-1",
    "href": "r/wb_gis_mapping_slides.html#resources-1",
    "title": "GIS mapping with R",
    "section": "Resources",
    "text": "Resources\nWebsite\nr-spatial by Edzer Pebesma, Marius Appel & Daniel Nüst\nCRAN package list\nAnalysis of Spatial Data\nMailing list\nR Special Interest Group on using Geographical data and Mapping"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#data-manipulation",
    "href": "r/wb_gis_mapping_slides.html#data-manipulation",
    "title": "GIS mapping with R",
    "section": "Data manipulation",
    "text": "Data manipulation\nOlder packages\n\nsp\nraster\nrgdal\nrgeos\n\nNewer generation\n\nsf: vector data\nterra: raster data (also has vector data capabilities)"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#mapping",
    "href": "r/wb_gis_mapping_slides.html#mapping",
    "title": "GIS mapping with R",
    "section": "Mapping",
    "text": "Mapping\nStatic maps\n\nggplot2 + ggspatial\ntmap\n\nDynamic maps\n\nleaflet\nggplot2 + gganimate\nmapview\nggmap\ntmap"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#sf-simple-features-in-r",
    "href": "r/wb_gis_mapping_slides.html#sf-simple-features-in-r",
    "title": "GIS mapping with R",
    "section": "sf: Simple Features in R",
    "text": "sf: Simple Features in R\nGeospatial vectors: points, lines, polygons\nSimple Features—defined by the Open Geospatial Consortium (OGC) & formalized by ISO—is a set of standards now used by most GIS libraries\nWell-known text (WKT) is a markup language for representing vector geometry objects according to those standards\nA compact computer version also exists—well-known binary (WKB)—used by spatial databases\nThe package sp predates Simple Features\nsf—launched in 2016—implements these standards in R in the form of sf objects: data.frames (or tibbles) containing the attributes, extended by sfc objects or simple feature geometries list-columns"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#sf",
    "href": "r/wb_gis_mapping_slides.html#sf",
    "title": "GIS mapping with R",
    "section": "sf",
    "text": "sf\nUseful links\n\nGitHub repo\nPaper\nResources\nCheatsheet\n6 vignettes: 1, 2, 3, 4, 5, 6"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#sf-objects",
    "href": "r/wb_gis_mapping_slides.html#sf-objects",
    "title": "GIS mapping with R",
    "section": "sf objects",
    "text": "sf objects"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#sf-objects-1",
    "href": "r/wb_gis_mapping_slides.html#sf-objects-1",
    "title": "GIS mapping with R",
    "section": "sf objects",
    "text": "sf objects"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#sf-objects-2",
    "href": "r/wb_gis_mapping_slides.html#sf-objects-2",
    "title": "GIS mapping with R",
    "section": "sf objects",
    "text": "sf objects"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#sf-objects-3",
    "href": "r/wb_gis_mapping_slides.html#sf-objects-3",
    "title": "GIS mapping with R",
    "section": "sf objects",
    "text": "sf objects"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#sf-objects-4",
    "href": "r/wb_gis_mapping_slides.html#sf-objects-4",
    "title": "GIS mapping with R",
    "section": "sf objects",
    "text": "sf objects"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#sf-functions",
    "href": "r/wb_gis_mapping_slides.html#sf-functions",
    "title": "GIS mapping with R",
    "section": "sf functions",
    "text": "sf functions\nMost functions start with st_ (which refers to “spatial type”)"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#terra-geospatial-rasters",
    "href": "r/wb_gis_mapping_slides.html#terra-geospatial-rasters",
    "title": "GIS mapping with R",
    "section": "terra: Geospatial rasters",
    "text": "terra: Geospatial rasters\nFaster and simpler replacement for the raster package by the same team\nMostly implemented in C++\nCan work with datasets too large to be loaded into memory"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#terra",
    "href": "r/wb_gis_mapping_slides.html#terra",
    "title": "GIS mapping with R",
    "section": "terra",
    "text": "terra\nUseful links\n\nGitHub repo\nResources\nFull manual"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#tmap-layered-grammar-of-graphics-gis-maps",
    "href": "r/wb_gis_mapping_slides.html#tmap-layered-grammar-of-graphics-gis-maps",
    "title": "GIS mapping with R",
    "section": "tmap: Layered grammar of graphics GIS maps",
    "text": "tmap: Layered grammar of graphics GIS maps"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#tmap",
    "href": "r/wb_gis_mapping_slides.html#tmap",
    "title": "GIS mapping with R",
    "section": "tmap",
    "text": "tmap\nUseful links\n\nGitHub repo\nResources\n\nHelp pages and vignettes\n?tmap-element\nvignette(\"tmap-getstarted\")\n# All the usual help pages, e.g.:\n?tm_layout"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#tmap-functions",
    "href": "r/wb_gis_mapping_slides.html#tmap-functions",
    "title": "GIS mapping with R",
    "section": "tmap functions",
    "text": "tmap functions\nMain functions start with tmap_\nFunctions creating map elements start with tm_"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#tmap-functioning",
    "href": "r/wb_gis_mapping_slides.html#tmap-functioning",
    "title": "GIS mapping with R",
    "section": "tmap functioning",
    "text": "tmap functioning\nVery similar to ggplot2\nTypically, a map contains:\n\nOne or multiple layer(s) (the order matters as they stack on top of each other)\nSome layout (e.g. customization of title, background, margins): tm_layout\nA compass: tm_compass\nA scale bar: tm_scale_bar\n\nEach layer contains:\n\nSome data: tm_shape\nHow that data will be represented: e.g. tm_polygons, tm_lines, tm_raster"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#tmap-example",
    "href": "r/wb_gis_mapping_slides.html#tmap-example",
    "title": "GIS mapping with R",
    "section": "tmap example",
    "text": "tmap example"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#tmap-example-1",
    "href": "r/wb_gis_mapping_slides.html#tmap-example-1",
    "title": "GIS mapping with R",
    "section": "tmap example",
    "text": "tmap example"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#tmap-example-2",
    "href": "r/wb_gis_mapping_slides.html#tmap-example-2",
    "title": "GIS mapping with R",
    "section": "tmap example",
    "text": "tmap example"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#tmap-example-3",
    "href": "r/wb_gis_mapping_slides.html#tmap-example-3",
    "title": "GIS mapping with R",
    "section": "tmap example",
    "text": "tmap example"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#tmap-example-4",
    "href": "r/wb_gis_mapping_slides.html#tmap-example-4",
    "title": "GIS mapping with R",
    "section": "tmap example",
    "text": "tmap example"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#tmap-example-5",
    "href": "r/wb_gis_mapping_slides.html#tmap-example-5",
    "title": "GIS mapping with R",
    "section": "tmap example",
    "text": "tmap example"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#tmap-example-6",
    "href": "r/wb_gis_mapping_slides.html#tmap-example-6",
    "title": "GIS mapping with R",
    "section": "tmap example",
    "text": "tmap example"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#tmap-example-7",
    "href": "r/wb_gis_mapping_slides.html#tmap-example-7",
    "title": "GIS mapping with R",
    "section": "tmap example",
    "text": "tmap example"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#ggplot2-the-standard-in-r-plots",
    "href": "r/wb_gis_mapping_slides.html#ggplot2-the-standard-in-r-plots",
    "title": "GIS mapping with R",
    "section": "ggplot2 (the standard in R plots)",
    "text": "ggplot2 (the standard in R plots)\nUseful links\n\nGitHub repo\nResources\nCheatsheet"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#ggplot2",
    "href": "r/wb_gis_mapping_slides.html#ggplot2",
    "title": "GIS mapping with R",
    "section": "ggplot2",
    "text": "ggplot2\ngeom_sf allows to plot sf objects (i.e. make maps)"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#data",
    "href": "r/wb_gis_mapping_slides.html#data",
    "title": "GIS mapping with R",
    "section": "Data",
    "text": "Data\nFor this webinar, we will use:\n\nthe Alaska as well as the Western Canada & USA subsets of the Randolph Glacier Inventory version 6.01\nthe USGS time series of the named glaciers of Glacier National Park2\nthe Alaska as well as the Western Canada & USA subsets of the consensus estimate for the ice thickness distribution of all glaciers on Earth dataset3\n\nThe datasets can be downloaded as zip files from these websites\nRGI Consortium (2017). Randolph Glacier Inventory – A Dataset of Global Glacier Outlines: Version 6.0: Technical Report, Global Land Ice Measurements from Space, Colorado, USA. Digital Media. DOI: https://doi.org/10.7265/N5-RGI-60.Fagre, D.B., McKeon, L.A., Dick, K.A. & Fountain, A.G., 2017, Glacier margin time series (1966, 1998, 2005, 2015) of the named glaciers of Glacier National Park, MT, USA: U.S. Geological Survey data release. DOI: https://doi.org/10.5066/F7P26WB1.Farinotti, Daniel, 2019, A consensus estimate for the ice thickness distribution of all glaciers on Earth - dataset, Zurich. ETH Zurich. DOI: https://doi.org/10.3929/ethz-b-000315707."
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#packages-1",
    "href": "r/wb_gis_mapping_slides.html#packages-1",
    "title": "GIS mapping with R",
    "section": "Packages",
    "text": "Packages\nPackages need to be installed before they can be loaded in a session\nPackages on CRAN can be installed with:\ninstall.packages(\"&lt;package-name&gt;\")\n basemaps is not on CRAN & needs to be installed from GitHub thanks to devtools:\ninstall.packages(\"devtools\")\ndevtools::install_github(\"16EAGLE/basemaps\")"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#packages-2",
    "href": "r/wb_gis_mapping_slides.html#packages-2",
    "title": "GIS mapping with R",
    "section": "Packages",
    "text": "Packages\nWe load all the packages that we will need at the top of the script:\nlibrary(sf)                 # spatial vector data manipulation\nlibrary(tmap)               # map production & tiled web map\nlibrary(dplyr)              # non GIS specific (tabular data manipulation)\nlibrary(magrittr)           # non GIS specific (pipes)\nlibrary(purrr)              # non GIS specific (functional programming)\nlibrary(rnaturalearth)      # basemap data access functions\nlibrary(rnaturalearthdata)  # basemap data\nlibrary(mapview)            # tiled web map\nlibrary(grid)               # (part of base R) used to create inset map\nlibrary(ggplot2)            # alternative to tmap for map production\nlibrary(ggspatial)          # spatial framework for ggplot2\nlibrary(terra)              # gridded spatial data manipulation\nlibrary(ggmap)              # download basemap data\nlibrary(basemaps)           # download basemap data\nlibrary(magick)             # wrapper around ImageMagick STL\nlibrary(leaflet)            # integrate Leaflet JS in R"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#randolph-glacier-inventory",
    "href": "r/wb_gis_mapping_slides.html#randolph-glacier-inventory",
    "title": "GIS mapping with R",
    "section": "Randolph Glacier Inventory",
    "text": "Randolph Glacier Inventory\nThis dataset contains the contour of all glaciers on Earth\nWe will focus on glaciers in Western North America\nYou can download & unzip 02_rgi60_WesternCanadaUS & 01_rgi60_Alaska from the Randolph Glacier Inventory version 6.0"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#reading-in-data",
    "href": "r/wb_gis_mapping_slides.html#reading-in-data",
    "title": "GIS mapping with R",
    "section": "Reading in data",
    "text": "Reading in data\nData get imported & turned into sf objects with the function sf::st_read:\nak &lt;- st_read(\"data/01_rgi60_Alaska\")\n\nMake sure to use the absolute paths or the paths relative to your working directory (which can be obtained with getwd)"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#reading-in-data-1",
    "href": "r/wb_gis_mapping_slides.html#reading-in-data-1",
    "title": "GIS mapping with R",
    "section": "Reading in data",
    "text": "Reading in data\nak &lt;- st_read(\"data/01_rgi60_Alaska\")\nReading layer `01_rgi60_Alaska' from data source `./data/01_rgi60_Alaska'\n               using driver `ESRI Shapefile'\nSimple feature collection with 27108 features and 22 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -176.1425 ymin: 52.05727 xmax: -126.8545 ymax: 69.35167\nGeodetic CRS:  WGS 84"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#reading-in-data-2",
    "href": "r/wb_gis_mapping_slides.html#reading-in-data-2",
    "title": "GIS mapping with R",
    "section": "Reading in data",
    "text": "Reading in data\n\n\nYour turn:\n\nRead in the data for the rest of north western America (from 02_rgi60_WesternCanadaUS) and create an sf object called wes"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#first-look-at-the-data",
    "href": "r/wb_gis_mapping_slides.html#first-look-at-the-data",
    "title": "GIS mapping with R",
    "section": "First look at the data",
    "text": "First look at the data\nak\nSimple feature collection with 27108 features and 22 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -176.1425 ymin: 52.05727 xmax: -126.8545 ymax: 69.35167\nGeodetic CRS:  WGS 84\nFirst 10 features:\n           RGIId        GLIMSId  BgnDate  EndDate    CenLon   CenLat O1Region\n1  RGI60-01.00001 G213177E63689N 20090703 -9999999 -146.8230 63.68900        1\n2  RGI60-01.00002 G213332E63404N 20090703 -9999999 -146.6680 63.40400        1\n3  RGI60-01.00003 G213920E63376N 20090703 -9999999 -146.0800 63.37600        1\n4  RGI60-01.00004 G213880E63381N 20090703 -9999999 -146.1200 63.38100        1\n5  RGI60-01.00005 G212943E63551N 20090703 -9999999 -147.0570 63.55100        1\n6  RGI60-01.00006 G213756E63571N 20090703 -9999999 -146.2440 63.57100        1\n7  RGI60-01.00007 G213771E63551N 20090703 -9999999 -146.2295 63.55085        1\n8  RGI60-01.00008 G213704E63543N 20090703 -9999999 -146.2960 63.54300        1\n9  RGI60-01.00009 G212400E63659N 20090703 -9999999 -147.6000 63.65900        1\n10 RGI60-01.00010 G212830E63513N 20090703 -9999999 -147.1700 63.51300        1\nO2Region   Area Zmin Zmax Zmed Slope Aspect  Lmax Status Connect Form\n1         2  0.360 1936 2725 2385    42    346   839      0       0    0\n2         2  0.558 1713 2144 2005    16    162  1197      0       0    0\n3         2  1.685 1609 2182 1868    18    175  2106      0       0    0\n4         2  3.681 1273 2317 1944    19    195  4175      0       0    0\n5         2  2.573 1494 2317 1914    16    181  2981      0       0    0\n6         2 10.470 1201 3547 1740    22     33 10518      0       0    0\n7         2  0.649 1918 2811 2194    23    151  1818      0       0    0\n8         2  0.200 2826 3555 3195    45     80   613      0       0    0\n9         2  1.517 1750 2514 1977    18    274  2255      0       0    0\n10        2  3.806 1280 1998 1666    17     35  3332      0       0    0\nTermType Surging Linkages Name                       geometry\n1         0       9        9 &lt;NA&gt; POLYGON ((-146.818 63.69081...\n2         0       9        9 &lt;NA&gt; POLYGON ((-146.6635 63.4076...\n3         0       9        9 &lt;NA&gt; POLYGON ((-146.0723 63.3834...\n4         0       9        9 &lt;NA&gt; POLYGON ((-146.149 63.37919...\n5         0       9        9 &lt;NA&gt; POLYGON ((-147.0431 63.5502...\n6         0       9        9 &lt;NA&gt; POLYGON ((-146.2436 63.5562...\n7         0       9        9 &lt;NA&gt; POLYGON ((-146.2495 63.5531...\n8         0       9        9 &lt;NA&gt; POLYGON ((-146.2992 63.5443...\n9         0       9        9 &lt;NA&gt; POLYGON ((-147.6147 63.6643...\n10        0       9        9 &lt;NA&gt; POLYGON ((-147.1494 63.5098..."
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#structure-of-the-data",
    "href": "r/wb_gis_mapping_slides.html#structure-of-the-data",
    "title": "GIS mapping with R",
    "section": "Structure of the data",
    "text": "Structure of the data\nstr(ak)\nClasses ‘sf’ and 'data.frame':  27108 obs. of  23 variables:\n$ RGIId   : chr  \"RGI60-01.00001\" \"RGI60-01.00002\" \"RGI60-01.00003\" ...\n$ GLIMSId : chr  \"G213177E63689N\" \"G213332E63404N\" \"G213920E63376N\" ...\n$ BgnDate : chr  \"20090703\" \"20090703\" \"20090703\" \"20090703\" ...\n$ EndDate : chr  \"-9999999\" \"-9999999\" \"-9999999\" \"-9999999\" ...\n$ CenLon  : num  -147 -147 -146 -146 -147 ...\n$ CenLat  : num  63.7 63.4 63.4 63.4 63.6 ...\n$ O1Region: chr  \"1\" \"1\" \"1\" \"1\" ...\n$ O2Region: chr  \"2\" \"2\" \"2\" \"2\" ...\n$ Area    : num  0.36 0.558 1.685 3.681 2.573 ...\n$ Zmin    : int  1936 1713 1609 1273 1494 1201 1918 2826 1750 1280 ...\n$ Zmax    : int  2725 2144 2182 2317 2317 3547 2811 3555 2514 1998 ...\n$ Zmed    : int  2385 2005 1868 1944 1914 1740 2194 3195 1977 1666 ...\n$ Slope   : num  42 16 18 19 16 22 23 45 18 17 ...\n$ Aspect  : int  346 162 175 195 181 33 151 80 274 35 ...\n$ Lmax    : int  839 1197 2106 4175 2981 10518 1818 613 2255 3332 ...\n$ Status  : int  0 0 0 0 0 0 0 0 0 0 ...\n$ Connect : int  0 0 0 0 0 0 0 0 0 0 ...\n$ Form    : int  0 0 0 0 0 0 0 0 0 0 ...\n$ TermType: int  0 0 0 0 0 0 0 0 0 0 ...\n$ Surging : int  9 9 9 9 9 9 9 9 9 9 ...\n$ Linkages: int  9 9 9 9 9 9 9 9 9 9 ...\n$ Name    : chr  NA NA NA NA ...\n$ geometry:sfc_POLYGON of length 27108; first list element: List of 1\n..$ : num [1:65, 1:2] -147 -147 -147 -147 -147 ...\n..- attr(*, \"class\")= chr [1:3] \"XY\" \"POLYGON\" \"sfg\"\n- attr(*, \"sf_column\")= chr \"geometry\"\n- attr(*, \"agr\")= Factor w/ 3 levels \"constant\",\"aggregate\",..: NA NA NA ...\n..- attr(*, \"names\")= chr [1:22] \"RGIId\" \"GLIMSId\" \"BgnDate\" \"EndDate\" ..."
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#inspect-your-data",
    "href": "r/wb_gis_mapping_slides.html#inspect-your-data",
    "title": "GIS mapping with R",
    "section": "Inspect your data",
    "text": "Inspect your data\n\n\nYour turn:\n\nInspect the wes object you created."
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#glacier-national-park-dataset",
    "href": "r/wb_gis_mapping_slides.html#glacier-national-park-dataset",
    "title": "GIS mapping with R",
    "section": "Glacier National Park dataset",
    "text": "Glacier National Park dataset\nThis dataset contains a time series of the retreat of 39 glaciers of Glacier National Park, MT, USA\nfor the years 1966, 1998, 2005 & 2015\nYou can download and unzip the 4 sets of files from the USGS website"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#read-in-and-clean-datasets",
    "href": "r/wb_gis_mapping_slides.html#read-in-and-clean-datasets",
    "title": "GIS mapping with R",
    "section": "Read in and clean datasets",
    "text": "Read in and clean datasets\nCreate a function that reads and cleans the data:\nprep &lt;- function(dir) {\n  g &lt;- st_read(dir)\n  g %&lt;&gt;% rename_with(~ tolower(gsub(\"Area....\", \"area\", .x)))\n  g %&lt;&gt;% dplyr::select(\n    year,\n    objectid,\n    glacname,\n    area,\n    shape_leng,\n    x_coord,\n    y_coord,\n    source_sca,\n    source\n  )\n}"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#combine-datasets-into-one-sf-object",
    "href": "r/wb_gis_mapping_slides.html#combine-datasets-into-one-sf-object",
    "title": "GIS mapping with R",
    "section": "Combine datasets into one sf object",
    "text": "Combine datasets into one sf object\nCheck that the CRS are all the same:\nall(sapply(\n  list(st_crs(gnp[[1]]),\n       st_crs(gnp[[2]]),\n       st_crs(gnp[[3]]),\n       st_crs(gnp[[4]])),\n  function(x) x == st_crs(gnp[[1]])\n))\n[1] TRUE"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#combine-datasets-into-one-sf-object-1",
    "href": "r/wb_gis_mapping_slides.html#combine-datasets-into-one-sf-object-1",
    "title": "GIS mapping with R",
    "section": "Combine datasets into one sf object",
    "text": "Combine datasets into one sf object\nWe can rbind the elements of our list:\ngnp &lt;- do.call(\"rbind\", gnp)\nYou can inspect your new sf object by calling it or with str"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#estimate-for-ice-thickness",
    "href": "r/wb_gis_mapping_slides.html#estimate-for-ice-thickness",
    "title": "GIS mapping with R",
    "section": "Estimate for ice thickness",
    "text": "Estimate for ice thickness\nThis dataset contains an estimate for the ice thickness of all glaciers on Earth\nThe nomenclature follows the Randolph Glacier Inventory\nIce thickness being a spatial field, this is raster data\nWe will use data in RGI60-02.16664_thickness.tif from the ETH Zürich Research Collection which corresponds to one of the glaciers (Agassiz) of Glacier National Park"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#load-raster-data",
    "href": "r/wb_gis_mapping_slides.html#load-raster-data",
    "title": "GIS mapping with R",
    "section": "Load raster data",
    "text": "Load raster data\nRead in data and create a SpatRaster object:\nras &lt;- rast(\"data/RGI60-02/RGI60-02.16664_thickness.tif\")"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#inspect-our-spatraster-object",
    "href": "r/wb_gis_mapping_slides.html#inspect-our-spatraster-object",
    "title": "GIS mapping with R",
    "section": "Inspect our SpatRaster object",
    "text": "Inspect our SpatRaster object\nras\nclass       : SpatRaster \ndimensions  : 93, 74, 1  (nrow, ncol, nlyr)\nresolution  : 25, 25  (x, y)\nextent      : 707362.5, 709212.5, 5422962, 5425288  (xmin, xmax, ymin, ymax)\ncoord. ref. : +proj=utm +zone=11 +datum=WGS84 +units=m +no_defs \nsource      : RGI60-02.16664_thickness.tif \nname        : RGI60-02.16664_thickness \nnlyr gives us the number of bands (a single one here). You can also run str(ras)"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#our-data",
    "href": "r/wb_gis_mapping_slides.html#our-data",
    "title": "GIS mapping with R",
    "section": "Our data",
    "text": "Our data\nWe now have 3 sf objects & 1 SpatRaster object:\n\nak:  contour of glaciers in AK\nwes:  contour of glaciers in the rest of Western North America\ngnp:  time series of 39 glaciers in Glacier National Park, MT, USA\nras:  ice thickness of the Agassiz Glacier from Glacier National Park"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#lets-map-our-sf-object-ak",
    "href": "r/wb_gis_mapping_slides.html#lets-map-our-sf-object-ak",
    "title": "GIS mapping with R",
    "section": "Let’s map our sf object ak",
    "text": "Let’s map our sf object ak\nAt a bare minimum, we need tm_shape with the data & some info as to how to represent that data:\ntm_shape(ak) +\n  tm_polygons()"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#we-need-to-label-customize-it",
    "href": "r/wb_gis_mapping_slides.html#we-need-to-label-customize-it",
    "title": "GIS mapping with R",
    "section": "We need to label & customize it",
    "text": "We need to label & customize it\ntm_shape(ak) +\n  tm_polygons() +\n  tm_layout(\n    title = \"Glaciers of Alaska\",\n    title.position = c(\"center\", \"top\"),\n    title.size = 1.1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.06, 0.01, 0.09, 0.01),\n    outer.margins = 0,\n    frame.lwd = 0.2\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    size = 1.2,\n    text.size = 0.6\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 500, 1000),\n    position = c(\"right\", \"BOTTOM\")\n  )"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#make-a-map-of-the-wes-object",
    "href": "r/wb_gis_mapping_slides.html#make-a-map-of-the-wes-object",
    "title": "GIS mapping with R",
    "section": "Make a map of the wes object",
    "text": "Make a map of the wes object\n\n\nYour turn:\n\nMake a map with the wes object you created with the data for Western North America excluding AK"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#now-lets-make-a-map-with-ak-wes",
    "href": "r/wb_gis_mapping_slides.html#now-lets-make-a-map-with-ak-wes",
    "title": "GIS mapping with R",
    "section": "Now, let’s make a map with ak & wes",
    "text": "Now, let’s make a map with ak & wes\n\nThe Coordinate Reference Systems (CRS) must be the same\n\n\nsf has a function to retrieve the CRS of an sf object: st_crs\n\n\nst_crs(ak) == st_crs(wes)\n[1] TRUE\n\n\nSo we’re good (we will see later what to do if this is not the case)"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#our-combined-map",
    "href": "r/wb_gis_mapping_slides.html#our-combined-map",
    "title": "GIS mapping with R",
    "section": "Our combined map",
    "text": "Our combined map\nLet’s start again with a minimum map without any layout to test things out:\ntm_shape(ak) +\n  tm_polygons() +\n  tm_shape(wes) +\n  tm_polygons()\n\n\nUh … oh …"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#what-went-wrong",
    "href": "r/wb_gis_mapping_slides.html#what-went-wrong",
    "title": "GIS mapping with R",
    "section": "What went wrong?",
    "text": "What went wrong?\nMaps are bound by “bounding boxes”. In tmap, they are called bbox\ntmap sets the bbox the first time tm_shape is called. In our case, the bbox was thus set to the bbox of the ak object\nWe need to create a new bbox for our new map"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#retrieving-bounding-boxes",
    "href": "r/wb_gis_mapping_slides.html#retrieving-bounding-boxes",
    "title": "GIS mapping with R",
    "section": "Retrieving bounding boxes",
    "text": "Retrieving bounding boxes\nsf has a function to retrieve the bbox of an sf object: st_bbox\nThe bbox of ak is:\nst_bbox(ak)\nxmin         ymin       xmax         ymax\n-176.14247   52.05727   -126.85450   69.35167"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#combining-bounding-boxes",
    "href": "r/wb_gis_mapping_slides.html#combining-bounding-boxes",
    "title": "GIS mapping with R",
    "section": "Combining bounding boxes",
    "text": "Combining bounding boxes\nbbox objects can’t be combined directly\nHere is how we can create a new bbox encompassing both of our bboxes:\n\nFirst, we transform our bboxes to sfc objects with st_as_sfc\nThen we combine those objects into a new sfc object with st_union\nFinally, we retrieve the bbox of that object with st_bbox:\n\nnwa_bbox &lt;- st_bbox(\n  st_union(\n    st_as_sfc(st_bbox(wes)),\n    st_as_sfc(st_bbox(ak))\n  )\n)"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#back-to-our-map",
    "href": "r/wb_gis_mapping_slides.html#back-to-our-map",
    "title": "GIS mapping with R",
    "section": "Back to our map",
    "text": "Back to our map\nWe can now use our new bounding box for the map of Western North America:\ntm_shape(ak, bbox = nwa_bbox) +\n  tm_polygons() +\n  tm_shape(wes) +\n  tm_polygons() +\n  tm_layout(\n    title = \"Glaciers of Western North America\",\n    title.position = c(\"center\", \"top\"),\n    title.size = 1.1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.06, 0.01, 0.09, 0.01),\n    outer.margins = 0,\n    frame.lwd = 0.2\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    size = 1.2,\n    text.size = 0.6\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 1000, 2000),\n    position = c(\"right\", \"BOTTOM\")\n  )"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#lets-add-a-basemap",
    "href": "r/wb_gis_mapping_slides.html#lets-add-a-basemap",
    "title": "GIS mapping with R",
    "section": "Let’s add a basemap",
    "text": "Let’s add a basemap\nWe will use data from Natural Earth, a public domain map dataset\nThere are much more fancy options, but they usually involve creating accounts (e.g. with Google) to access some API\nIn addition, this dataset can be accessed direction from within R thanks to the rOpenSci packages:\n\nrnaturalearth: provides the functions\nrnaturalearthdata: provides the data"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#create-an-sf-object-with-statesprovinces",
    "href": "r/wb_gis_mapping_slides.html#create-an-sf-object-with-statesprovinces",
    "title": "GIS mapping with R",
    "section": "Create an sf object with states/provinces",
    "text": "Create an sf object with states/provinces\nstates_all &lt;- ne_states(\n  country = c(\"canada\", \"united states of america\"),\n  returnclass = \"sf\"\n)\n\nne_ stands for “Natural Earth”"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#select-relevant-statesprovinces",
    "href": "r/wb_gis_mapping_slides.html#select-relevant-statesprovinces",
    "title": "GIS mapping with R",
    "section": "Select relevant states/provinces",
    "text": "Select relevant states/provinces\nstates &lt;- states_all %&gt;%\n  filter(name_en == \"Alaska\" |\n           name_en == \"British Columbia\" |\n           name_en == \"Yukon\" |\n           name_en == \"Northwest Territories\" |\n           name_en ==  \"Alberta\" |\n           name_en == \"California\" |\n           name_en == \"Washington\" |\n           name_en == \"Oregon\" |\n           name_en == \"Idaho\" |\n           name_en == \"Montana\" |\n           name_en == \"Wyoming\" |\n           name_en == \"Colorado\" |\n           name_en == \"Nevada\" |\n           name_en == \"Utah\"\n         )"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#add-the-basemap-to-our-map",
    "href": "r/wb_gis_mapping_slides.html#add-the-basemap-to-our-map",
    "title": "GIS mapping with R",
    "section": "Add the basemap to our map",
    "text": "Add the basemap to our map\n\n\nWhat do we need to make sure of first?\n\n\n\nst_crs(states) == st_crs(ak)\n[1] TRUE"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#add-the-basemap-to-our-map-1",
    "href": "r/wb_gis_mapping_slides.html#add-the-basemap-to-our-map-1",
    "title": "GIS mapping with R",
    "section": "Add the basemap to our map",
    "text": "Add the basemap to our map\nWe add the basemap as a 3rd layer\nMind the order! If you put the basemap last, it will cover your data\nOf course, we will use our nwa_bbox bounding box again\nWe will also break tm_polygons into tm_borders and tm_fill for ak and wes in order to colourise them with slightly different colours"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#add-the-basemap-to-our-map-2",
    "href": "r/wb_gis_mapping_slides.html#add-the-basemap-to-our-map-2",
    "title": "GIS mapping with R",
    "section": "Add the basemap to our map",
    "text": "Add the basemap to our map\ntm_shape(states, bbox = nwa_bbox) +\n  tm_polygons(col = \"#f2f2f2\", lwd = 0.2) +\n  tm_shape(ak) +\n  tm_borders(col = \"#3399ff\") +\n  tm_fill(col = \"#86baff\") +\n  tm_shape(wes) +\n  tm_borders(col = \"#3399ff\") +\n  tm_fill(col = \"#86baff\") +\n  tm_layout(\n    title = \"Glaciers of Western North America\",\n    title.position = c(\"center\", \"top\"),\n    title.size = 1.1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.06, 0.01, 0.09, 0.01),\n    outer.margins = 0,\n    frame.lwd = 0.2\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    size = 1.2,\n    text.size = 0.6\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 1000, 2000),\n    position = c(\"right\", \"BOTTOM\")\n  )"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#tmap-styles",
    "href": "r/wb_gis_mapping_slides.html#tmap-styles",
    "title": "GIS mapping with R",
    "section": "tmap styles",
    "text": "tmap styles\ntmap has a number of styles that you can try\nFor instance, to set the style to “classic”, run the following before making your map:\ntmap_style(\"classic\")\n\nOther options are:\n“white” (default), “gray”, “natural”, “cobalt”, “col_blind”, “albatross”, “beaver”, “bw”, “watercolor”"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#tmap-styles-1",
    "href": "r/wb_gis_mapping_slides.html#tmap-styles-1",
    "title": "GIS mapping with R",
    "section": "tmap styles",
    "text": "tmap styles\nTo return to the default, you need to run\ntmap_style(\"white\")\nor\ntmap_options_reset()\nwhich will reset every tmap option"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#first-lets-map-it",
    "href": "r/wb_gis_mapping_slides.html#first-lets-map-it",
    "title": "GIS mapping with R",
    "section": "First, let’s map it",
    "text": "First, let’s map it\nLet’s use the same tm_borders and tm_fill we just used:\ntm_shape(gnp) +\n  tm_borders(col = \"#3399ff\") +\n  tm_fill(col = \"#86baff\") +\n  tm_layout(\n    title = \"Glaciers of Glacier National Park\",\n    title.position = c(\"center\", \"top\"),\n    legend.title.color = \"#fcfcfc\",\n    legend.text.size = 1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.07, 0.03, 0.07, 0.03),\n    outer.margins = 0\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    text.size = 0.7\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 10, 20),\n    position = c(\"right\", \"BOTTOM\"),\n    text.size = 1\n  )"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#create-an-inset-map",
    "href": "r/wb_gis_mapping_slides.html#create-an-inset-map",
    "title": "GIS mapping with R",
    "section": "Create an inset map",
    "text": "Create an inset map\nAs always, first we check that the CRS are the same:\nst_crs(gnp) == st_crs(ak)\n[1] FALSE\n\nAH!"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#crs-transformation",
    "href": "r/wb_gis_mapping_slides.html#crs-transformation",
    "title": "GIS mapping with R",
    "section": "CRS transformation",
    "text": "CRS transformation\nWe need to reproject gnp into the CRS of our other sf objects (e.g. ak):\ngnp &lt;- st_transform(gnp, st_crs(ak))\n\nWe can verify that the CRS are now the same:\nst_crs(gnp) == st_crs(ak)\n[1] TRUE"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#inset-maps-first-step",
    "href": "r/wb_gis_mapping_slides.html#inset-maps-first-step",
    "title": "GIS mapping with R",
    "section": "Inset maps: first step",
    "text": "Inset maps: first step\nAdd a rectangle showing the location of the GNP map in the main North America map\nWe need to create a new sfc object from the gnp bbox so that we can add it to our previous map as a new layer:\ngnp_zone &lt;- st_bbox(gnp) %&gt;%\n  st_as_sfc()"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#inset-maps-second-step",
    "href": "r/wb_gis_mapping_slides.html#inset-maps-second-step",
    "title": "GIS mapping with R",
    "section": "Inset maps: second step",
    "text": "Inset maps: second step\nCreate a tmap object of the main map. Of course, we need to edit the title. Also, note the presence of our new layer:\nmain_map &lt;- tm_shape(states, bbox = nwa_bbox) +\n  tm_polygons(col = \"#f2f2f2\", lwd = 0.2) +\n  tm_shape(ak) +\n  tm_borders(col = \"#3399ff\") +\n  tm_fill(col = \"#86baff\") +\n  tm_shape(wes) +\n  tm_borders(col = \"#3399ff\") +\n  tm_fill(col = \"#86baff\") +\n  tm_shape(gnp_zone) +\n  tm_borders(lwd = 1.5, col = \"#ff9900\") +\n  tm_layout(\n    title = \"Glaciers of Glacier National Park\",\n    title.position = c(\"center\", \"top\"),\n    title.size = 1.1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.06, 0.01, 0.09, 0.01),\n    outer.margins = 0,\n    frame.lwd = 0.2\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    size = 1.2,\n    text.size = 0.6\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 500, 1000),\n    position = c(\"right\", \"BOTTOM\")\n  )"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#inset-maps-third-step",
    "href": "r/wb_gis_mapping_slides.html#inset-maps-third-step",
    "title": "GIS mapping with R",
    "section": "Inset maps: third step",
    "text": "Inset maps: third step\nCreate a tmap object of the inset map\nWe make sure to matching colours & edit the layouts for better readability:\ninset_map &lt;- tm_shape(gnp) +\n  tm_borders(col = \"#3399ff\") +\n  tm_fill(col = \"#86baff\") +\n  tm_layout(\n    legend.show = F,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.03, 0.03, 0.03, 0.03),\n    outer.margins = 0,\n    frame = \"#ff9900\",\n    frame.lwd = 3\n  )"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#inset-maps-final-step",
    "href": "r/wb_gis_mapping_slides.html#inset-maps-final-step",
    "title": "GIS mapping with R",
    "section": "Inset maps: final step",
    "text": "Inset maps: final step\nCombine the two tmap objects\nWe print the main map & add the inset map with grid::viewport:\nmain_map\nprint(inset_map, vp = viewport(0.41, 0.26, width = 0.5, height = 0.5))"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#map-of-the-agassiz-glacier",
    "href": "r/wb_gis_mapping_slides.html#map-of-the-agassiz-glacier",
    "title": "GIS mapping with R",
    "section": "Map of the Agassiz Glacier",
    "text": "Map of the Agassiz Glacier\nSelect the data points corresponding to the Agassiz Glacier:\nag &lt;- gnp %&gt;% filter(glacname == \"Agassiz Glacier\")"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#map-of-the-agassiz-glacier-1",
    "href": "r/wb_gis_mapping_slides.html#map-of-the-agassiz-glacier-1",
    "title": "GIS mapping with R",
    "section": "Map of the Agassiz Glacier",
    "text": "Map of the Agassiz Glacier\ntm_shape(ag) +\n  tm_polygons() +\n  tm_layout(\n    title = \"Agassiz Glacier\",\n    title.position = c(\"center\", \"top\"),\n    legend.position = c(\"left\", \"bottom\"),\n    legend.title.color = \"#fcfcfc\",\n    legend.text.size = 1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.07, 0.03, 0.07, 0.03),\n    outer.margins = 0\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    text.size = 0.7\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 0.5, 1),\n    position = c(\"right\", \"BOTTOM\"),\n    text.size = 1\n  )"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#map-of-the-agassiz-glacier-2",
    "href": "r/wb_gis_mapping_slides.html#map-of-the-agassiz-glacier-2",
    "title": "GIS mapping with R",
    "section": "Map of the Agassiz Glacier",
    "text": "Map of the Agassiz Glacier\n\n\nNot great …"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#map-based-on-attribute-variables",
    "href": "r/wb_gis_mapping_slides.html#map-based-on-attribute-variables",
    "title": "GIS mapping with R",
    "section": "Map based on attribute variables",
    "text": "Map based on attribute variables\ntm_shape(ag) +\n  tm_polygons(\"year\", palette = \"Blues\") +\n  tm_layout(\n    title = \"Agassiz Glacier\",\n    title.position = c(\"center\", \"top\"),\n    legend.position = c(\"left\", \"bottom\"),\n    legend.title.color = \"#fcfcfc\",\n    legend.text.size = 1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.07, 0.03, 0.07, 0.03),\n    outer.margins = 0\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    text.size = 0.7\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 0.5, 1),\n    position = c(\"right\", \"BOTTOM\"),\n    text.size = 1\n  )"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#using-ggplot2-instead-of-tmap",
    "href": "r/wb_gis_mapping_slides.html#using-ggplot2-instead-of-tmap",
    "title": "GIS mapping with R",
    "section": "Using ggplot2 instead of tmap",
    "text": "Using ggplot2 instead of tmap\nAs an alternative to tmap, ggplot2 can plot maps with the geom_sf function:\nggplot(ag) +\n  geom_sf(aes(fill = year)) +\n  scale_fill_brewer(palette = \"Blues\") +\n  labs(title = \"Agassiz Glacier\") +\n  annotation_scale(location = \"bl\", width_hint = 0.4) +\n  annotation_north_arrow(location = \"tr\", which_north = \"true\",\n                         pad_x = unit(0.75, \"in\"), pad_y = unit(0.5, \"in\"),\n                         style = north_arrow_fancy_orienteering) +\n  theme_bw() +\n  theme(plot.title = element_text(hjust = 0.5))\nThe package ggspatial adds a lot of functionality to ggplot2 for spatial data"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#faceted-map-of-the-retreat-of-agassiz",
    "href": "r/wb_gis_mapping_slides.html#faceted-map-of-the-retreat-of-agassiz",
    "title": "GIS mapping with R",
    "section": "Faceted map of the retreat of Agassiz",
    "text": "Faceted map of the retreat of Agassiz\ntm_shape(ag) +\n  tm_polygons(col = \"#86baff\") +\n  tm_layout(\n    main.title = \"Agassiz Glacier\",\n    main.title.position = c(\"center\", \"top\"),\n    main.title.size = 1.2,\n    legend.position = c(\"left\", \"bottom\"),\n    legend.title.color = \"#fcfcfc\",\n    legend.text.size = 1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0, 0.03, 0, 0.03),\n    outer.margins = 0,\n    panel.label.bg.color = \"#fcfcfc\",\n    frame = F,\n    asp = 0.6\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    size = 1,\n    text.size = 0.6\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 0.5, 1),\n    position = c(\"right\", \"BOTTOM\"),\n    text.size = 0.6\n  ) +\n  tm_facets(\n    by = \"year\",\n    free.coords = F,\n    ncol = 4\n  )"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#animated-map-of-the-retreat-of-agassiz",
    "href": "r/wb_gis_mapping_slides.html#animated-map-of-the-retreat-of-agassiz",
    "title": "GIS mapping with R",
    "section": "Animated map of the Retreat of Agassiz",
    "text": "Animated map of the Retreat of Agassiz\nFirst, we need to create a tmap object with facets:\nagassiz_anim &lt;- tm_shape(ag) +\n  tm_polygons(col = \"#86baff\") +\n  tm_layout(\n    title = \"Agassiz Glacier\",\n    title.position = c(\"center\", \"top\"),\n    legend.position = c(\"left\", \"bottom\"),\n    legend.title.color = \"#fcfcfc\",\n    legend.text.size = 1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.08, 0, 0.08, 0),\n    outer.margins = 0,\n    panel.label.bg.color = \"#fcfcfc\"\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    text.size = 0.7\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 0.5, 1),\n    position = c(\"right\", \"BOTTOM\"),\n    text.size = 1\n  ) +\n  tm_facets(\n    along = \"year\",\n    free.coords = F\n  )"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#animated-map-of-the-retreat-of-agassiz-1",
    "href": "r/wb_gis_mapping_slides.html#animated-map-of-the-retreat-of-agassiz-1",
    "title": "GIS mapping with R",
    "section": "Animated map of the Retreat of Agassiz",
    "text": "Animated map of the Retreat of Agassiz\nThen we can pass that object to tmap_animation:\ntmap_animation(\n  agassiz_anim,\n  filename = \"ag.gif\",\n  dpi = 300,\n  inner.margins = c(0.08, 0, 0.08, 0),\n  delay = 100\n)"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#map-of-ice-thickness-of-agassiz",
    "href": "r/wb_gis_mapping_slides.html#map-of-ice-thickness-of-agassiz",
    "title": "GIS mapping with R",
    "section": "Map of ice thickness of Agassiz",
    "text": "Map of ice thickness of Agassiz\nNow, let’s map the estimated ice thickness on Agassiz Glacier. This time, we use tm_raster:\ntm_shape(ras) +\n  tm_raster(title = \"\") +\n  tm_layout(\n    title = \"Ice thickness (m) of Agassiz Glacier\",\n    title.position = c(\"center\", \"top\"),\n    legend.position = c(\"left\", \"bottom\"),\n    legend.bg.color = \"#ffffff\",\n    legend.text.size = 1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.07, 0.03, 0.07, 0.03),\n    outer.margins = 0\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    text.size = 0.7\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 0.5, 1),\n    position = c(\"right\", \"BOTTOM\"),\n    text.size = 1\n  )"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#combining-with-randolph-data",
    "href": "r/wb_gis_mapping_slides.html#combining-with-randolph-data",
    "title": "GIS mapping with R",
    "section": "Combining with Randolph data",
    "text": "Combining with Randolph data\nAs always, we check whether the CRS are the same:\nst_crs(ag) == st_crs(ras)\n[1] FALSE\nWe need to reproject ag (remember that it is best to avoid reprojecting raster data):\nag %&lt;&gt;% st_transform(st_crs(ras))"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#combining-with-randolph-data-1",
    "href": "r/wb_gis_mapping_slides.html#combining-with-randolph-data-1",
    "title": "GIS mapping with R",
    "section": "Combining with Randolph data",
    "text": "Combining with Randolph data\nThe layers hide each other (the order matters!). You can use tm_borders for one of them or transparency (alpha). We also adjust the legend:\ntm_shape(ras) +\n  tm_raster(title = \"Ice (m)\") +\n  tm_shape(ag) +\n  tm_polygons(\"year\", palette = \"Blues\", alpha = 0.2, title = \"Contour\") +\n  tm_layout(\n    title = \"Ice thickness (m) and retreat of Agassiz Glacier\",\n    title.position = c(\"center\", \"top\"),\n    legend.position = c(\"left\", \"bottom\"),\n    legend.bg.color = \"#ffffff\",\n    legend.text.size = 0.7,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.07, 0.03, 0.07, 0.03),\n    outer.margins = 0\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    text.size = 0.7\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 0.5, 1),\n    position = c(\"right\", \"BOTTOM\"),\n    text.size = 1\n  )"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#refining-raster-maps",
    "href": "r/wb_gis_mapping_slides.html#refining-raster-maps",
    "title": "GIS mapping with R",
    "section": "Refining raster maps",
    "text": "Refining raster maps\nLet’s go back to our ice thickness map:"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#basemap-with-ggmap",
    "href": "r/wb_gis_mapping_slides.html#basemap-with-ggmap",
    "title": "GIS mapping with R",
    "section": "Basemap with ggmap",
    "text": "Basemap with ggmap\nbasemap &lt;- get_map(\n  bbox = c(\n    left = st_bbox(ag)[1],\n    bottom = st_bbox(ag)[2],\n    right = st_bbox(ag)[3],\n    top = st_bbox(ag)[4]\n  ),\n  source = \"osm\"\n)\n\nggmap is a powerful package, but Google now requires an API key obtained through registration"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#basemap-with-basemaps",
    "href": "r/wb_gis_mapping_slides.html#basemap-with-basemaps",
    "title": "GIS mapping with R",
    "section": "Basemap with basemaps",
    "text": "Basemap with basemaps\nThe package basemaps allows to download open source basemap data from several sources, but those cannot easily be combined with sf objects\nThis plots a satellite image of the Agassiz Glacier:\nbasemap_plot(ag, map_service = \"esri\", map_type = \"world_imagery\")"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#satellite-image-of-the-agassiz-glacier",
    "href": "r/wb_gis_mapping_slides.html#satellite-image-of-the-agassiz-glacier",
    "title": "GIS mapping with R",
    "section": "Satellite image of the Agassiz Glacier",
    "text": "Satellite image of the Agassiz Glacier"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#mapview",
    "href": "r/wb_gis_mapping_slides.html#mapview",
    "title": "GIS mapping with R",
    "section": "mapview",
    "text": "mapview\nmapview(gnp)"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#tmap-1",
    "href": "r/wb_gis_mapping_slides.html#tmap-1",
    "title": "GIS mapping with R",
    "section": "tmap",
    "text": "tmap\nSo far, we have used the plot mode of tmap. There is also a view mode which allows interactive viewing in a browser through Leaflet\nChange to view mode:\ntmap_mode(\"view\")\n\nYou can also toggle between modes with ttm\n\nRe-plot the last map we plotted with tmap:\ntmap_last()"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#leaflet",
    "href": "r/wb_gis_mapping_slides.html#leaflet",
    "title": "GIS mapping with R",
    "section": "leaflet",
    "text": "leaflet\nleaflet creates a map widget to which you add layers\nmap &lt;- leaflet()\naddTiles(map)"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#resources-2",
    "href": "r/wb_gis_mapping_slides.html#resources-2",
    "title": "GIS mapping with R",
    "section": "Resources",
    "text": "Resources\nHere are some resources on the topic to get started.\n\nR companion to Geographic Information Analysis\nSpatial data analysis"
  },
  {
    "objectID": "r/top_ws.html",
    "href": "r/top_ws.html",
    "title": "R workshops",
    "section": "",
    "text": "Web scraping with rvest\n\n\n\n\nGIS with R\n\n\n\n\nIntro R for the humanities\n\n\n\n\nR: a demo",
    "crumbs": [
      "R",
      "<b><em>Workshops</em></b>"
    ]
  },
  {
    "objectID": "r/top_intro.html",
    "href": "r/top_intro.html",
    "title": "Getting started with R",
    "section": "",
    "text": "R is a free and open-source programming language for statistical computing, modelling, and graphics, with an unbeatable collection of statistical packages. It is extremely popular in some academic fields such as statistics, biology, bioinformatics, data mining, data analysis, and linguistics.\nThis introductory course does not assume any prior knowledge.\n\n Start course ➤",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>"
    ]
  },
  {
    "objectID": "r/top_hpc.html",
    "href": "r/top_hpc.html",
    "title": "High-performance R",
    "section": "",
    "text": "R is not famous for its speed. With code optimization and parallelization, it can however be used for heavy computations.\nThis course will introduce you to working with R from the command line on the Alliance clusters with a focus on performance. We will discuss code profiling and benchmarking, various parallelization techniques, as well as using C++ from inside R to speed up calculations.\nA basic knowledge of R will be useful for this course.\n\n Start course ➤",
    "crumbs": [
      "R",
      "<b><em>High-performance R</em></b>"
    ]
  },
  {
    "objectID": "r/intro_tidyverse.html",
    "href": "r/intro_tidyverse.html",
    "title": "The tidyverse",
    "section": "",
    "text": "The tidyverse is a set of packages which attempts to make R more consistent. R was written by statisticians and it is a bit quirky. The tidyverse makes it look more like other programming languages which were developed by computer scientists. It is a different style of writing R code and it is by no means necessary.",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "Introduction to the tidyverse"
    ]
  },
  {
    "objectID": "r/intro_tidyverse.html#a-glimpse-at-the-tidyverse",
    "href": "r/intro_tidyverse.html#a-glimpse-at-the-tidyverse",
    "title": "The tidyverse",
    "section": "A glimpse at the tidyverse",
    "text": "A glimpse at the tidyverse\nThe best introduction to the tidyverse is probably the book R for Data Science by Hadley Wickham and Garrett Grolemund.\nPosit (the company formerly known as RStudio Inc. behind the tidyverse) developed a series of useful cheatsheets. Below are links to the ones you are the most likely to use as you get started with R.\n\nData import\nThe first thing you often need to do is to import your data into R. This is done with readr.\n\n\n\nfrom Posit Cheatsheets\n\n\n\nData transformation\nYou then often need to transformation your data into the right format. This is done with the packages dplyr and tidyr.\n\n\n\nfrom Posit Cheatsheets\n\n\n\n\nfrom Posit Cheatsheets\n\n\n\nVisualization\nVisualization in the tidyverse is done with the ggplot2 package which we will explore in the next section.\n\n\n\nfrom Posit Cheatsheets\n\n\n\nWorking with factors\nThe package forcats offers the tidyverse approach to working with factors.\n\n\nfrom Posit Cheatsheets\n\n\n\nWorking with strings\nstringr is for strings.\n\n\n\nfrom Posit Cheatsheets\n\n\n\nWorking with dates\nlubridate will help you deal with dates.\n\n\n\nfrom Posit Cheatsheets\n\n\n\nFunctional programming\nFinally, purrr is the tidyverse equivalent to the apply functions in base R: a way to run functions on functions.\n\n\n\nfrom Posit Cheatsheets",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "Introduction to the tidyverse"
    ]
  },
  {
    "objectID": "r/intro_tidyverse.html#base-r-or-tidyverse",
    "href": "r/intro_tidyverse.html#base-r-or-tidyverse",
    "title": "The tidyverse",
    "section": "Base R or tidyverse?",
    "text": "Base R or tidyverse?\n“Base R” refers to the use of the standard R library. The expression is often used in contrast to the tidyverse.\nThere are a many things that you can do with either base R or the tidyverse. Because the syntaxes are quite different, it almost feels like using two different languages and people tend to favour one or the other.\nWhich one you should use is really up to you.\n\n\n\n\n\n\n\nBase R\nTidyverse\n\n\n\n\nPreferred by old-schoolers\nIncreasingly becoming the norm with newer R users\n\n\nMore stable\nMore consistent syntax and behaviour\n\n\nDoesn’t require installing and loading packages\nMore and more resources and documentation available\n\n\n\nIn truth, even though the tidyverse has many detractors amongst old R users, it is increasingly becoming the norm.",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "Introduction to the tidyverse"
    ]
  },
  {
    "objectID": "r/intro_resources.html",
    "href": "r/intro_resources.html",
    "title": "Resources",
    "section": "",
    "text": "The R community is dynamic and offers a lot of online resources, from IDEs to Q&A, to workshops, books, or publications.\nThis section provides a selection of useful sites.\n\n\nMain sites\n\nR website\nComprehensive R Archive Network (CRAN): R versions and packages\n\n\n\nPosit and RStudio IDE\n\nPosit website (Posit was formerly called RStudio Inc.)\nPosit cheatsheets\n\n\n\nForums and Q&A\n\nStack Overflow [r] tag wiki\nStack Overflow [r] tag questions\nPosit Discourse\n\n\n\nDocumentation as pdf\n\nContributed documentation\nIntro books\n\n\n\nSoftware Carpentry online workshops\n\nProgramming with R\nR for Reproducible Scientific Analysis\nData analysis using R in the digital humanities\n\n\n\nOnline books\n\nR for Data Science (heavily based on the tidyverse)\nR Packages (how to create packages)\nR Programming for Data Science\nMastering Software Development in R\n\n\n\nR research\n\nThe R Journal",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "Resources"
    ]
  },
  {
    "objectID": "r/intro_plotting.html",
    "href": "r/intro_plotting.html",
    "title": "Plotting",
    "section": "",
    "text": "This section focuses on plotting in R with the package ggplot2 from the tidyverse.",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "Plotting"
    ]
  },
  {
    "objectID": "r/intro_plotting.html#the-data",
    "href": "r/intro_plotting.html#the-data",
    "title": "Plotting",
    "section": "The data",
    "text": "The data\nR comes with a number of datasets. You can get a list by running data(). The ggplot2 package provides additional ones. We will use the mpg dataset from ggplot2.\nTo access the data, let’s load the package:\n\nlibrary(ggplot2)\n\nHere is what that dataset looks like:\n\nmpg\n\n# A tibble: 234 × 11\n   manufacturer model      displ  year   cyl trans      drv     cty   hwy fl   \n   &lt;chr&gt;        &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;      &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;\n 1 audi         a4           1.8  1999     4 auto(l5)   f        18    29 p    \n 2 audi         a4           1.8  1999     4 manual(m5) f        21    29 p    \n 3 audi         a4           2    2008     4 manual(m6) f        20    31 p    \n 4 audi         a4           2    2008     4 auto(av)   f        21    30 p    \n 5 audi         a4           2.8  1999     6 auto(l5)   f        16    26 p    \n 6 audi         a4           2.8  1999     6 manual(m5) f        18    26 p    \n 7 audi         a4           3.1  2008     6 auto(av)   f        18    27 p    \n 8 audi         a4 quattro   1.8  1999     4 manual(m5) 4        18    26 p    \n 9 audi         a4 quattro   1.8  1999     4 auto(l5)   4        16    25 p    \n10 audi         a4 quattro   2    2008     4 manual(m6) 4        20    28 p    \n   class  \n   &lt;chr&gt;  \n 1 compact\n 2 compact\n 3 compact\n 4 compact\n 5 compact\n 6 compact\n 7 compact\n 8 compact\n 9 compact\n10 compact\n# ℹ 224 more rows\n\n\n?mpg will give you information on the variables. In particular:\n\ndispl contains data on engine displacement (a measure of engine size and thus power) in litres (L).\nhwy contains data on fuel economy while driving on highways in miles per gallon (mpg).\ndrv represents the type of drive train (front-wheel drive, rear wheel drive, 4WD).\nclass represents the type of car.\n\nWe are interested in the relationship between engine size and fuel economy and see how the type of drive train and/or the type of car might affect this relationship.",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "Plotting"
    ]
  },
  {
    "objectID": "r/intro_plotting.html#base-r-plotting",
    "href": "r/intro_plotting.html#base-r-plotting",
    "title": "Plotting",
    "section": "Base R plotting",
    "text": "Base R plotting\nR contains built-in plotting capability thanks to the plot() function.\nA basic version of our plot would be:\n\nplot(\n  mpg$displ,\n  mpg$hwy,\n  main = \"Fuel consumption per engine size on highways\",\n  xlab = \"Engine size (L)\",\n  ylab = \"Fuel economy (mpg) on highways\"\n)",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "Plotting"
    ]
  },
  {
    "objectID": "r/intro_plotting.html#grammar-of-graphics",
    "href": "r/intro_plotting.html#grammar-of-graphics",
    "title": "Plotting",
    "section": "Grammar of graphics",
    "text": "Grammar of graphics\nLeland Wilkinson developed the concept of grammar of graphics in his 2005 book The Grammar of Graphics. By breaking down statistical graphs into components following a set of rules, any plot can be described and constructed in a rigorous fashion.\nThis was further refined by Hadley Wickham in his 2010 article A Layered Grammar of Graphics and implemented in the package ggplot2 (that’s what the 2 “g” stand for in “ggplot”).\nggplot2 has become the dominant graphing package in R. Let’s see how to construct a plot with this package.",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "Plotting"
    ]
  },
  {
    "objectID": "r/intro_plotting.html#plotting-with-ggplot2",
    "href": "r/intro_plotting.html#plotting-with-ggplot2",
    "title": "Plotting",
    "section": "Plotting with ggplot2",
    "text": "Plotting with ggplot2\n\nYou can find the ggplot2 cheatsheet here.\n\n\nThe Canvas\nThe first component is the data:\n\nggplot(data = mpg)\n\n\n\n\n\n\n\n\n\nThis can be simplified into ggplot(mpg).\n\nThe second component sets the way variables are mapped on the axes. This is done with the aes() (aesthetics) function:\n\nggplot(data = mpg, mapping = aes(x = displ, y = hwy))\n\n\n\n\n\n\n\n\n\nThis can be simplified into ggplot(mpg, aes(displ, hwy)).\n\n\n\nGeometric representations of the data\nOnto this canvas, we can add “geoms” (geometrical objects) representing the data. The type of “geom” defines the type of representation (e.g. boxplot, histogram, bar chart).\nTo represent the data as a scatterplot, we use the geom_point() function:\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point()\n\n\n\n\n\n\n\n\nWe can colour-code the points in the scatterplot based on the drv variable, showing the lower fuel efficiency of 4WD vehicles:\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = drv))\n\n\n\n\n\n\n\n\nOr we can colour-code them based on the class variable:\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = class))\n\n\n\n\n\n\n\n\nMultiple “geoms” can be added on top of each other. For instance, we can add a smoothed conditional means function that aids at seeing patterns in the data with geom_smooth():\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = class)) +\n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThanks to the colour-coding of the types of car, we can see that the cluster of points in the top right corner all belong to the same type: 2 seaters. Those are outliers with high power, yet high few efficiency due to their smaller size.\nThe default smoothing function uses the LOESS (locally estimated scatterplot smoothing) method, which is a nonlinear regression. But maybe a linear model would actually show the general trend better. We can change the method by passing it as an argument to geom_smooth():\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = class)) +\n  geom_smooth(method = lm)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nOf course, we could apply the smoothing function to each class instead of the entire data. It creates a busy plot but shows that the downward trend remains true within each type of car:\n\nggplot(mpg, aes(x = displ, y = hwy, color = class)) +\n  geom_point(aes(color = class)) +\n  geom_smooth(method = lm)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nOther arguments to geom_smooth() can set the line width, color, or whether or not the standard error (se) is shown:\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = class)) +\n  geom_smooth(\n    method = lm,\n    se = FALSE,\n    color = \"#999999\",\n    linewidth = 0.5\n  )\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nColour scales\nIf we want to change the colour scale, we add another layer for this:\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = class)) +\n  scale_color_brewer(palette = \"Dark2\") +\n  geom_smooth(\n    method = lm,\n    se = FALSE,\n    color = \"#999999\",\n    linewidth = 0.5\n  )\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nscale_color_brewer(), based on color brewer 2.0, is one of many methods to change the color scale. Here is the list of available scales for this particular method:\n\n\n\nLabels\nWe can keep on adding layers. For instance, the labs() function allows to set title, subtitle, captions, tags, axes labels, etc.\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = class)) +\n  scale_color_brewer(palette = \"Dark2\") +\n  geom_smooth(\n    method = lm,\n    se = FALSE,\n    color = \"#999999\",\n    linewidth = 0.5\n  ) +\n  labs(\n    title = \"Fuel consumption per engine size on highways\",\n    x = \"Engine size (L)\",\n    y = \"Fuel economy (mpg) on highways\",\n    color = \"Type of car\",\n    caption = \"EPA data from https://fueleconomy.gov/\"\n  )\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nThemes\nAnother optional layer sets one of several preset themes.\nEdward Tufte developed, amongst others, the principle of data-ink ratio which emphasizes that ink should be used primarily where it communicates meaningful messages. It is indeed common to see charts where more ink is used in labels or background than in the actual representation of the data.\nThe default ggplot2 theme could be criticized as not following this principle. Let’s change it:\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = class)) +\n  scale_color_brewer(palette = \"Dark2\") +\n  geom_smooth(\n    method = lm,\n    se = FALSE,\n    color = \"#999999\",\n    linewidth = 0.5\n  ) +\n  labs(\n    title = \"Fuel consumption per engine size on highways\",\n    x = \"Engine size (L)\",\n    y = \"Fuel economy (mpg) on highways\",\n    color = \"Type of car\",\n    caption = \"EPA data from https://fueleconomy.gov/\"\n  ) +\n  theme_classic()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThe theme() function allows to tweak the theme in any number of ways. For instance, what if we don’t like the default position of the title and we would rather have it centered?\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = class)) +\n  scale_color_brewer(palette = \"Dark2\") +\n  geom_smooth(\n    method = lm,\n    se = FALSE,\n    color = \"#999999\",\n    linewidth = 0.5\n  ) +\n  labs(\n    title = \"Fuel consumption per engine size on highways\",\n    x = \"Engine size (L)\",\n    y = \"Fuel economy (mpg) on highways\",\n    color = \"Type of car\",\n    caption = \"EPA data from https://fueleconomy.gov/\"\n  ) +\n  theme_classic() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nWe can also move the legend to give more space to the actual graph:\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = class)) +\n  scale_color_brewer(palette = \"Dark2\") +\n  geom_smooth(\n    method = lm,\n    se = FALSE,\n    color = \"#999999\",\n    linewidth = 0.5\n  ) +\n  labs(\n    title = \"Fuel consumption per engine size on highways\",\n    x = \"Engine size (L)\",\n    y = \"Fuel economy (mpg) on highways\",\n    color = \"Type of car\",\n    caption = \"EPA data from https://fueleconomy.gov/\"\n  ) +\n  theme_classic() +\n  theme(plot.title = element_text(hjust = 0.5), legend.position = \"bottom\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nAs you could see, ggplot2 works by adding a number of layers on top of each other, all following a standard set of rules, or “grammar”. This way, a vast array of graphs can be created by organizing simple components.",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "Plotting"
    ]
  },
  {
    "objectID": "r/intro_plotting.html#ggplot2-extensions",
    "href": "r/intro_plotting.html#ggplot2-extensions",
    "title": "Plotting",
    "section": "ggplot2 extensions",
    "text": "ggplot2 extensions\nThanks to its vast popularity, ggplot2 has seen a proliferation of packages extending its capabilities.\n\nCombining plots\nFor instance the patchwork package allows to easily combine multiple plots on the same frame.\nLet’s add a second plot next to our plot. To add plots side by side, we simply add them to each other. We also make a few changes to the labels to improve the plots integration:\n\nlibrary(patchwork)\n\nggplot(mpg, aes(x = displ, y = hwy)) +        # First plot\n  geom_point(aes(color = class)) +\n  scale_color_brewer(palette = \"Dark2\") +\n  geom_smooth(\n    method = lm,\n    se = FALSE,\n    color = \"#999999\",\n    linewidth = 0.5\n  ) +\n  labs(\n    x = \"Engine size (L)\",\n    y = \"Fuel economy (mpg) on highways\",\n    color = \"Type of car\"\n  ) +\n  theme_classic() +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    legend.position = c(0.7, 0.75),           # Better legend position\n    legend.background = element_rect(         # Add a frame to the legend\n      linewidth = 0.1,\n      linetype = \"solid\",\n      colour = \"black\"\n    )\n  ) +\n  ggplot(mpg, aes(x = displ, y = hwy)) +      # Second plot\n  geom_point(aes(color = drv)) +\n  scale_color_brewer(palette = \"Dark2\") +\n  labs(\n    x = \"Engine size (L)\",\n    y = element_blank(),                      # Remove redundant label\n    color = \"Type of drive train\",\n    caption = \"EPA data from https://fueleconomy.gov/\"\n  ) +\n  theme_classic() +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    legend.position = c(0.7, 0.87),\n    legend.background = element_rect(\n      linewidth = 0.1,\n      linetype = \"solid\",\n      colour = \"black\"\n    )\n  )\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nExtensions list\nAnother popular extension is the gganimate package which allows to create data animations.\nA full list of extensions for ggplot2 is shown below (here is the website):",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "Plotting"
    ]
  },
  {
    "objectID": "r/intro_indexing.html",
    "href": "r/intro_indexing.html",
    "title": "Indexing",
    "section": "",
    "text": "This section covers indexing from the various data structures.",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "Indexing"
    ]
  },
  {
    "objectID": "r/intro_indexing.html#indexing-atomic-vectors",
    "href": "r/intro_indexing.html#indexing-atomic-vectors",
    "title": "Indexing",
    "section": "Indexing atomic vectors",
    "text": "Indexing atomic vectors\n\nHere is an example with an atomic vector of size one:\n\nIndexing in R starts at 1 and is done with square brackets next to the element to index:\n\nx &lt;- 2\nx\n\n[1] 2\n\nx[1]\n\n[1] 2\n\n\nWhat happens if we index out of range?\n\nx[2]\n\n[1] NA\n\n\n\nExample for an atomic vector with multiple elements:\n\n\nx &lt;- c(2, 4, 1)\nx\n\n[1] 2 4 1\n\nx[2]\n\n[1] 4\n\nx[2:4]\n\n[1]  4  1 NA\n\n\n\nModifying mutable objects\nIndexing also allows to modify some of the values of mutable objects:\n\nx\n\n[1] 2 4 1\n\nx[2] &lt;- 0\nx\n\n[1] 2 0 1\n\n\n\n\nCopy-on-modify\nNot all languages behave the same when you assign the same mutable object to several variables, then modify one of them.\n\nIn Python: no copy-on-modify\n\nDon’t try to run this code in R. This is for information only.\n\n\n\nPython\n\na = [1, 2, 3]\nb = a\nb\n\n[1, 2, 3]\n\n\nPython\n\na[0] = 4           # In Python, indexing starts at 0\na\n\n[4, 2, 3]\n\n\nPython\n\nb\n\n[4, 2, 3]\nModifying a also modifies b: this is because no copy is made when you modify a. If you want to keep b unchanged, you need to assign an explicit copy of a to it with b = copy.copy(a).\n\n\nIn R: copy-on-modify\n\na &lt;- c(1, 2, 3)\nb &lt;- a\nb\n\n[1] 1 2 3\n\na[1] &lt;- 4          # In R, indexing starts at 1\na\n\n[1] 4 2 3\n\nb\n\n[1] 1 2 3\n\n\nHere, the default is to create a new copy in memory when a is transformed so that b remains unchanged.",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "Indexing"
    ]
  },
  {
    "objectID": "r/intro_indexing.html#indexing-matrices-and-arrays",
    "href": "r/intro_indexing.html#indexing-matrices-and-arrays",
    "title": "Indexing",
    "section": "Indexing matrices and arrays",
    "text": "Indexing matrices and arrays\n\nx &lt;- matrix(1:12, nrow = 3, ncol = 4)\nx\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    4    7   10\n[2,]    2    5    8   11\n[3,]    3    6    9   12\n\nx[2, 3]\n\n[1] 8\n\nx &lt;- array(as.double(1:24), c(3, 2, 4))\nx\n\n, , 1\n\n     [,1] [,2]\n[1,]    1    4\n[2,]    2    5\n[3,]    3    6\n\n, , 2\n\n     [,1] [,2]\n[1,]    7   10\n[2,]    8   11\n[3,]    9   12\n\n, , 3\n\n     [,1] [,2]\n[1,]   13   16\n[2,]   14   17\n[3,]   15   18\n\n, , 4\n\n     [,1] [,2]\n[1,]   19   22\n[2,]   20   23\n[3,]   21   24\n\nx[2, 1, 3]\n\n[1] 14",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "Indexing"
    ]
  },
  {
    "objectID": "r/intro_indexing.html#indexing-lists",
    "href": "r/intro_indexing.html#indexing-lists",
    "title": "Indexing",
    "section": "Indexing lists",
    "text": "Indexing lists\n\nx &lt;- list(2L, 3:8, c(2, 1), FALSE, \"string\")\nx\n\n[[1]]\n[1] 2\n\n[[2]]\n[1] 3 4 5 6 7 8\n\n[[3]]\n[1] 2 1\n\n[[4]]\n[1] FALSE\n\n[[5]]\n[1] \"string\"\n\n\nIndexing a list returns a list:\n\nx[3]\n\n[[1]]\n[1] 2 1\n\ntypeof(x[3])\n\n[1] \"list\"\n\n\nTo extract elements of a list, double square brackets are required:\n\nx[[3]]\n\n[1] 2 1\n\ntypeof(x[[3]])\n\n[1] \"double\"\n\n\n\n\nYour turn:\n\nTry to extract the number 7 from this list.",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "Indexing"
    ]
  },
  {
    "objectID": "r/intro_indexing.html#indexing-data-frames",
    "href": "r/intro_indexing.html#indexing-data-frames",
    "title": "Indexing",
    "section": "Indexing data frames",
    "text": "Indexing data frames\n\nx &lt;- data.frame(\n  country = c(\"Canada\", \"USA\", \"Mexico\"),\n  var = c(2.9, 3.1, 4.5)\n)\nx\n\n  country var\n1  Canada 2.9\n2     USA 3.1\n3  Mexico 4.5\n\n\nIndexing dataframes can be done by using indices, as we saw for matrices:\n\nx[2, 1]\n\n[1] \"USA\"\n\n\nIt can also be done using column names thanks to the $ symbol (a column is a vector, so indexing from a column is the same as indexing from a vector):\n\nx$country[2]\n\n[1] \"USA\"\n\n\nA data frame is actually a list of vectors representing the various columns:\n\ntypeof(x)\n\n[1] \"list\"\n\n\nIndexing a column can thus also be done by indexing the element of the list with double square brackets (although this is a slower method).\nWe get the same result with:\n\nx[[1]][2]\n\n[1] \"USA\"",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "Indexing"
    ]
  },
  {
    "objectID": "r/intro_data_structure.html",
    "href": "r/intro_data_structure.html",
    "title": "Data types and structures",
    "section": "",
    "text": "This section covers the various data types and structures available in R.",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "Data types and structures"
    ]
  },
  {
    "objectID": "r/intro_data_structure.html#summary-of-structures",
    "href": "r/intro_data_structure.html#summary-of-structures",
    "title": "Data types and structures",
    "section": "Summary of structures",
    "text": "Summary of structures\n\n\n\nDimension\nHomogeneous\nHeterogeneous\n\n\n\n\n1 d\nAtomic vector\nList\n\n\n2 d\nMatrix\nData frame\n\n\n3 d\nArray",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "Data types and structures"
    ]
  },
  {
    "objectID": "r/intro_data_structure.html#atomic-vectors",
    "href": "r/intro_data_structure.html#atomic-vectors",
    "title": "Data types and structures",
    "section": "Atomic vectors",
    "text": "Atomic vectors\n\nWith a single element\n\na &lt;- 2\na\n\n[1] 2\n\ntypeof(a)\n\n[1] \"double\"\n\nstr(a)\n\n num 2\n\nlength(a)\n\n[1] 1\n\ndim(a)\n\nNULL\n\n\nThe dim attribute of a vector doesn’t exist (hence the NULL). This makes vectors different from one-dimensional arrays which have a dim of 1.\nYou might have noticed that 2 is a double (double precision floating point number, equivalent of “float” in other languages). In R, this is the default, even if you don’t type 2.0. This prevents the kind of weirdness you can find in, for instance, Python.\nIn Python:\n&gt;&gt;&gt; 2 == 2.0\nTrue\n&gt;&gt;&gt; type(2) == type(2.0)\nFalse\n&gt;&gt;&gt; type(2)\n&lt;class 'int'&gt;\n&gt;&gt;&gt; type(2.0)\n&lt;class 'float'&gt;\nIn R:\n&gt; 2 == 2.0\n[1] TRUE\n&gt; typeof(2) == typeof(2.0)\n[1] TRUE\n&gt; typeof(2)\n[1] \"double\"\n&gt; typeof(2.0)\n[1] \"double\"\nIf you want to define an integer variable, you use:\n\nb &lt;- 2L\nb\n\n[1] 2\n\ntypeof(b)\n\n[1] \"integer\"\n\nmode(b)\n\n[1] \"numeric\"\n\nstr(b)\n\n int 2\n\n\nThere are six vector types:\n\nlogical\ninteger\ndouble\ncharacter\ncomplex\nraw\n\n\n\nWith multiple elements\n\nc &lt;- c(2, 4, 1)\nc\n\n[1] 2 4 1\n\ntypeof(c)\n\n[1] \"double\"\n\nmode(c)\n\n[1] \"numeric\"\n\nstr(c)\n\n num [1:3] 2 4 1\n\n\n\nd &lt;- c(TRUE, TRUE, NA, FALSE)\nd\n\n[1]  TRUE  TRUE    NA FALSE\n\ntypeof(d)\n\n[1] \"logical\"\n\nstr(d)\n\n logi [1:4] TRUE TRUE NA FALSE\n\n\n\nNA (“Not Available”) is a logical constant of length one. It is an indicator for a missing value.\n\nVectors are homogeneous, so all elements need to be of the same type.\nIf you use elements of different types, R will convert some of them to ensure that they become of the same type:\n\ne &lt;- c(\"This is a string\", 3, \"test\")\ne\n\n[1] \"This is a string\" \"3\"                \"test\"            \n\ntypeof(e)\n\n[1] \"character\"\n\nstr(e)\n\n chr [1:3] \"This is a string\" \"3\" \"test\"\n\n\n\nf &lt;- c(TRUE, 3, FALSE)\nf\n\n[1] 1 3 0\n\ntypeof(f)\n\n[1] \"double\"\n\nstr(f)\n\n num [1:3] 1 3 0\n\n\n\ng &lt;- c(2L, 3, 4L)\ng\n\n[1] 2 3 4\n\ntypeof(g)\n\n[1] \"double\"\n\nstr(g)\n\n num [1:3] 2 3 4\n\n\n\nh &lt;- c(\"string\", TRUE, 2L, 3.1)\nh\n\n[1] \"string\" \"TRUE\"   \"2\"      \"3.1\"   \n\ntypeof(h)\n\n[1] \"character\"\n\nstr(h)\n\n chr [1:4] \"string\" \"TRUE\" \"2\" \"3.1\"\n\n\nThe binary operator : is equivalent to the seq() function and generates a regular sequence of integers:\n\ni &lt;- 1:5\ni\n\n[1] 1 2 3 4 5\n\ntypeof(i)\n\n[1] \"integer\"\n\nstr(i)\n\n int [1:5] 1 2 3 4 5\n\nidentical(2:8, seq(2, 8))\n\n[1] TRUE",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "Data types and structures"
    ]
  },
  {
    "objectID": "r/intro_data_structure.html#matrices",
    "href": "r/intro_data_structure.html#matrices",
    "title": "Data types and structures",
    "section": "Matrices",
    "text": "Matrices\n\nj &lt;- matrix(1:12, nrow = 3, ncol = 4)\nj\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    4    7   10\n[2,]    2    5    8   11\n[3,]    3    6    9   12\n\ntypeof(j)\n\n[1] \"integer\"\n\nstr(j)\n\n int [1:3, 1:4] 1 2 3 4 5 6 7 8 9 10 ...\n\nlength(j)\n\n[1] 12\n\ndim(j)\n\n[1] 3 4\n\n\nThe default is byrow = FALSE. If you want the matrix to be filled in by row, you need to set this argument to TRUE:\n\nk &lt;- matrix(1:12, nrow = 3, ncol = 4, byrow = TRUE)\nk\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    2    3    4\n[2,]    5    6    7    8\n[3,]    9   10   11   12",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "Data types and structures"
    ]
  },
  {
    "objectID": "r/intro_data_structure.html#arrays",
    "href": "r/intro_data_structure.html#arrays",
    "title": "Data types and structures",
    "section": "Arrays",
    "text": "Arrays\n\nl &lt;- array(as.double(1:24), c(3, 2, 4))\nl\n\n, , 1\n\n     [,1] [,2]\n[1,]    1    4\n[2,]    2    5\n[3,]    3    6\n\n, , 2\n\n     [,1] [,2]\n[1,]    7   10\n[2,]    8   11\n[3,]    9   12\n\n, , 3\n\n     [,1] [,2]\n[1,]   13   16\n[2,]   14   17\n[3,]   15   18\n\n, , 4\n\n     [,1] [,2]\n[1,]   19   22\n[2,]   20   23\n[3,]   21   24\n\ntypeof(l)\n\n[1] \"double\"\n\nstr(l)\n\n num [1:3, 1:2, 1:4] 1 2 3 4 5 6 7 8 9 10 ...\n\nlength(l)\n\n[1] 24\n\ndim(l)\n\n[1] 3 2 4",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "Data types and structures"
    ]
  },
  {
    "objectID": "r/intro_data_structure.html#lists",
    "href": "r/intro_data_structure.html#lists",
    "title": "Data types and structures",
    "section": "Lists",
    "text": "Lists\n\nm &lt;- list(2, 3)\nm\n\n[[1]]\n[1] 2\n\n[[2]]\n[1] 3\n\ntypeof(m)\n\n[1] \"list\"\n\nstr(m)\n\nList of 2\n $ : num 2\n $ : num 3\n\nlength(m)\n\n[1] 2\n\ndim(m)\n\nNULL\n\n\nAs with atomic vectors, lists do not have a dim attribute. Lists are in fact a different type of vectors.\nLists can be heterogeneous:\n\nn &lt;- list(2L, 3, c(2, 1), FALSE, \"string\")\nn\n\n[[1]]\n[1] 2\n\n[[2]]\n[1] 3\n\n[[3]]\n[1] 2 1\n\n[[4]]\n[1] FALSE\n\n[[5]]\n[1] \"string\"\n\ntypeof(n)\n\n[1] \"list\"\n\nstr(n)\n\nList of 5\n $ : int 2\n $ : num 3\n $ : num [1:2] 2 1\n $ : logi FALSE\n $ : chr \"string\"\n\nlength(n)\n\n[1] 5",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "Data types and structures"
    ]
  },
  {
    "objectID": "r/intro_data_structure.html#data-frames",
    "href": "r/intro_data_structure.html#data-frames",
    "title": "Data types and structures",
    "section": "Data frames",
    "text": "Data frames\nData frames contain tabular data. Under the hood, a data frame is a list of vectors.\n\no &lt;- data.frame(\n  country = c(\"Canada\", \"USA\", \"Mexico\"),\n  var = c(2.9, 3.1, 4.5)\n)\no\n\n  country var\n1  Canada 2.9\n2     USA 3.1\n3  Mexico 4.5\n\ntypeof(o)\n\n[1] \"list\"\n\nstr(o)\n\n'data.frame':   3 obs. of  2 variables:\n $ country: chr  \"Canada\" \"USA\" \"Mexico\"\n $ var    : num  2.9 3.1 4.5\n\nlength(o)\n\n[1] 2\n\ndim(o)\n\n[1] 3 2",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "Data types and structures"
    ]
  },
  {
    "objectID": "r/intro_basics.html",
    "href": "r/intro_basics.html",
    "title": "First steps in R",
    "section": "",
    "text": "In this section, we take our first few steps in R: we will access the R documentation, see how to set R options, and talk about a few concepts.",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "First steps in R"
    ]
  },
  {
    "objectID": "r/intro_basics.html#help-and-documentation",
    "href": "r/intro_basics.html#help-and-documentation",
    "title": "First steps in R",
    "section": "Help and documentation",
    "text": "Help and documentation\nFor some general documentation on R, you can run:\nhelp.start()\nTo get help on a function (e.g. sum), you can run:\nhelp(sum)\nDepending on your settings, this will open a documentation for sum in a pager or in your browser.",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "First steps in R"
    ]
  },
  {
    "objectID": "r/intro_basics.html#r-settings",
    "href": "r/intro_basics.html#r-settings",
    "title": "First steps in R",
    "section": "R settings",
    "text": "R settings\nSettings are saved in a .Rprofile file. You can edit the file directly in any text editor or from within R.\nList all options:\noptions()\nReturn the value of a particular option:\n\ngetOption(\"help_type\")\n\n[1] \"text\"\n\n\nSet an option:\noptions(help_type = \"html\")",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "First steps in R"
    ]
  },
  {
    "objectID": "r/intro_basics.html#assignment",
    "href": "r/intro_basics.html#assignment",
    "title": "First steps in R",
    "section": "Assignment",
    "text": "Assignment\nR can accept the equal sign (=) for assignments, but it is more idiomatic to use the assignment sign (&lt;-) whenever you bind a name to a value and to use the equal sign everywhere else.\n\na &lt;- 3\n\nOnce you have bound a name to a value, you can recall the value with that name:\n\na  # Note that you do not need to use a print() function in R\n\n[1] 3\n\n\nYou can remove an object from the environment by deleting its name:\n\nrm(a)\n\nLet’s confirm that a doesn’t exist anymore in the environment:\n\na\n\nError in eval(expr, envir, enclos): object 'a' not found\n\n\nThe garbage collector will take care of deleting the object itself from memory.",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "First steps in R"
    ]
  },
  {
    "objectID": "r/intro_basics.html#comments",
    "href": "r/intro_basics.html#comments",
    "title": "First steps in R",
    "section": "Comments",
    "text": "Comments\nAnything to the left of # is a comment and is ignored by R:\n\n# This is an inline comment\n\na &lt;- 3  # This is also a comment",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "First steps in R"
    ]
  },
  {
    "objectID": "r/hss_why.html",
    "href": "r/hss_why.html",
    "title": "R: why and for whom?",
    "section": "",
    "text": "There are other high level programming languages such as Python or Julia, so when might it make sense for you to turn to R?",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "R: why and for whom?"
    ]
  },
  {
    "objectID": "r/hss_why.html#why-r",
    "href": "r/hss_why.html#why-r",
    "title": "R: why and for whom?",
    "section": "Why R?",
    "text": "Why R?\nHere are a number of reasons why you might want to consider using R:\n\nFree and open source\nHigh-level and easy to learn\nLarge community\nVery well documented\nUnequalled number of statistics and modelling packages\nIntegrated package manager\nEasy connection with fast compiled languages such as C and C++\nPowerful IDEs (e.g. RStudio, ESS, Jupyter)",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "R: why and for whom?"
    ]
  },
  {
    "objectID": "r/hss_why.html#for-whom",
    "href": "r/hss_why.html#for-whom",
    "title": "R: why and for whom?",
    "section": "For whom?",
    "text": "For whom?\nFor whom is R particularly well suited?\n\nFields with heavy statistics, modelling, or Bayesian analysis such as biology, linguistics, economics, or statistics\nData science using a lot of tabular data",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "R: why and for whom?"
    ]
  },
  {
    "objectID": "r/hss_why.html#downsides-of-r",
    "href": "r/hss_why.html#downsides-of-r",
    "title": "R: why and for whom?",
    "section": "Downsides of R",
    "text": "Downsides of R\nOf course, R also has its downsides:\n\nInconsistent syntax full of quirks\nSlow\nLarge memory usage",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "R: why and for whom?"
    ]
  },
  {
    "objectID": "r/hss_tidyverse.html",
    "href": "r/hss_tidyverse.html",
    "title": "The tidyverse",
    "section": "",
    "text": "The tidyverse is a set of packages which attempts to make R more consistent. R was written by statisticians and it is a bit quirky. The tidyverse makes it look more like other programming languages which were developed by computer scientists. It is a different style of writing R code and it is by no means necessary.",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "The tidyverse"
    ]
  },
  {
    "objectID": "r/hss_tidyverse.html#a-glimpse-at-the-tidyverse",
    "href": "r/hss_tidyverse.html#a-glimpse-at-the-tidyverse",
    "title": "The tidyverse",
    "section": "A glimpse at the tidyverse",
    "text": "A glimpse at the tidyverse\nThe best introduction to the tidyverse is probably the book R for Data Science by Hadley Wickham and Garrett Grolemund.\nPosit (the company formerly known as RStudio Inc. behind the tidyverse) developed a series of useful cheatsheets. Below are links to the ones you are the most likely to use as you get started with R.\n\nData import\nThe first thing you often need to do is to import your data into R. This is done with readr.\n\n\n\nfrom Posit Cheatsheets\n\n\n\nData transformation\nYou then often need to transformation your data into the right format. This is done with the packages dplyr and tidyr.\n\n\n\nfrom Posit Cheatsheets\n\n\n\n\nfrom Posit Cheatsheets\n\n\n\nVisualization\nVisualization in the tidyverse is done with the ggplot2 package which we will explore in the next section.\n\n\n\nfrom Posit Cheatsheets\n\n\n\nWorking with factors\nThe package forcats offers the tidyverse approach to working with factors.\n\n\nfrom Posit Cheatsheets\n\n\n\nWorking with strings\nstringr is for strings.\n\n\n\nfrom Posit Cheatsheets\n\n\n\nWorking with dates\nlubridate will help you deal with dates.\n\n\n\nfrom Posit Cheatsheets\n\n\n\nFunctional programming\nFinally, purrr is the tidyverse equivalent to the apply functions in base R: a way to run functions on functions.\n\n\n\nfrom Posit Cheatsheets",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "The tidyverse"
    ]
  },
  {
    "objectID": "r/hss_tidyverse.html#base-r-or-tidyverse",
    "href": "r/hss_tidyverse.html#base-r-or-tidyverse",
    "title": "The tidyverse",
    "section": "Base R or tidyverse?",
    "text": "Base R or tidyverse?\n“Base R” refers to the use of the standard R library. The expression is often used in contrast to the tidyverse.\nThere are a many things that you can do with either base R or the tidyverse. Because the syntaxes are quite different, it almost feels like using two different languages and people tend to favour one or the other.\nWhich one you should use is really up to you.\n\n\n\n\n\n\n\nBase R\nTidyverse\n\n\n\n\nPreferred by old-schoolers\nIncreasingly becoming the norm with newer R users\n\n\nMore stable\nMore consistent syntax and behaviour\n\n\nDoesn’t require installing and loading packages\nMore and more resources and documentation available\n\n\n\nIn truth, even though the tidyverse has many detractors amongst old R users, it is increasingly becoming the norm.",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "The tidyverse"
    ]
  },
  {
    "objectID": "r/hss_resources.html",
    "href": "r/hss_resources.html",
    "title": "Resources",
    "section": "",
    "text": "The R community is dynamic and offers a lot of online resources, from IDEs to Q&A, to workshops, books, or publications.\nThis section provides a selection of useful sites.\n\n\nMain sites\n\nR website\nComprehensive R Archive Network (CRAN): R versions and packages\n\n\n\nPosit and RStudio IDE\n\nPosit website (Posit was formerly called RStudio Inc.)\nPosit cheatsheets\n\n\n\nForums and Q&A\n\nStack Overflow [r] tag wiki\nStack Overflow [r] tag questions\nPosit Discourse\n\n\n\nDocumentation as pdf\n\nContributed documentation\nIntro books\n\n\n\nSoftware Carpentry online workshops\n\nProgramming with R\nR for Reproducible Scientific Analysis\nData analysis using R in the digital humanities\n\n\n\nOnline books\n\nR for Data Science (heavily based on the tidyverse)\nR Packages (how to create packages)\nR Programming for Data Science\nMastering Software Development in R\n\n\n\nR research\n\nThe R Journal",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Resources"
    ]
  },
  {
    "objectID": "r/hss_packages.html",
    "href": "r/hss_packages.html",
    "title": "Packages",
    "section": "",
    "text": "Packages are a set of functions, constants, and/or data developed by the community that add functionality to R.\nIn this section, we look at where to find packages and how to install them.",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Packages"
    ]
  },
  {
    "objectID": "r/hss_packages.html#looking-for-packages",
    "href": "r/hss_packages.html#looking-for-packages",
    "title": "Packages",
    "section": "Looking for packages",
    "text": "Looking for packages\n\nPackage finder.\nYour peers and the literature.\nList of CRAN packages.\nList of CRAN task views (list of packages with information for a large number of wide topics).",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Packages"
    ]
  },
  {
    "objectID": "r/hss_packages.html#managing-r-packages",
    "href": "r/hss_packages.html#managing-r-packages",
    "title": "Packages",
    "section": "Managing R packages",
    "text": "Managing R packages\n\nFor this course, you won’t have to install any package as they have already been installed in our RStudio server.\n\nR packages can be installed, updated, and removed from within R:\ninstall.packages(\"&lt;package_name&gt;\", repos=\"&lt;url-cran-mirror&gt;\")\nremove.packages(\"&lt;package-name&gt;\")\nupdate_packages()\n\nExample:\n\ninstall.packages(\"rvest\", repos=\"https://mirror.rcg.sfu.ca/mirror/CRAN/\")\n\nrepos argument: chose a CRAN mirror close to the location of your cluster or use https://cloud.r-project.org/.\n\n\nThe first time you install a package, R will ask you whether you want to create a personal library in your home directory. Answer yes to both questions. Your packages will now install under ~/.\n\n\nSome packages require additional modules to be loaded before they can be installed. Other packages need additional R packages as dependencies. In either case, you will get explicit error messages. Adding the argument dependencies = T helps in the second case, but you will still have to add packages manually from time to time.",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Packages"
    ]
  },
  {
    "objectID": "r/hss_packages.html#loading-packages",
    "href": "r/hss_packages.html#loading-packages",
    "title": "Packages",
    "section": "Loading packages",
    "text": "Loading packages\nTo make a package available in an R session, you load it with the library() function.\n\nExample:\n\nlibrary(readxl)\nAlternatively, you can access a function from a package without loading it with the syntax: package::function().\n\nExample:\n\nreadxl::read_excel(\"file.xlsx\")",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Packages"
    ]
  },
  {
    "objectID": "r/hss_packages.html#package-documentation",
    "href": "r/hss_packages.html#package-documentation",
    "title": "Packages",
    "section": "Package documentation",
    "text": "Package documentation\n\nSelect a package from the list of CRAN packages.\nGoogle “cran” and the name of your package (e.g. “cran dplyr”).\nLook up a package in the package documentation.\nGet a list of functions within a package with the help() function (installed, but not loaded in session):\n\n\nExample to get a list of functions in the dplyr package:\n\nhelp(package = \"dplyr\")\n\nGet help on a function within a package:\n\nIf you are using RStudio or the HTML format for your R help and you already ran the command to get the list of functions within a package (e.g. help(package = \"dplyr\")), you can get help on any function by clicking on its name.\nIf you are using the text format for help (for instance, if you are running R remotely on the command line), you can get help for any function by adding its name at as the first argument of the previous command.\n\nExample to get help on the function bind() of the package dplyr:\n\nhelp(bind, package = \"dplyr\")\nOf course, if the dplyr package is already loaded in your session, you can simply run help(bind).\n\nGet a list of all help files with alias or concept or title matching a regular expression in all installed packages:\n\n\nExample to get a list of all help files with alias or concept or title matching bind:\n\n??bind\nYou can then open those help files as seen previously.\n\nGet a list of all vignettes for all installed packages:\n\nIf you are using RStudio or the HTML help format:\nbrowseVignettes()\nIf you are using the text help format:\nvignette()\n\nGet a list of vignettes available for a package (not all packages have vignettes):\n\n\nExample to get a list of vignettes for the package dplyr:\n\nIf you are using RStudio or the HTML help format:\nvignette(package = \"dplyr\")\nIf you are using the text help format:\nbrowseVignettes(package = \"dplyr\")\nYou can then open those help vignettes as seen previously.",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Packages"
    ]
  },
  {
    "objectID": "r/hss_import.html",
    "href": "r/hss_import.html",
    "title": "Data import and export",
    "section": "",
    "text": "So far, we have used a well-formatted dataset. In the real world, things are often not this nice and tidy…\nIn this section, we will learn how to handle real data.",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Data import and export"
    ]
  },
  {
    "objectID": "r/hss_import.html#reading-in-data",
    "href": "r/hss_import.html#reading-in-data",
    "title": "Data import and export",
    "section": "Reading in data",
    "text": "Reading in data\nThe readr package from the tidyverse provides a number of functions to read in text files with tabular data (e.g. comma-separated values (CSV) or tab-separated values (TSV) files).\nLet’s load it:\n\nlibrary(readr)\n\nThe read_csv() function allows to read in CSV files that are either stored locally or from a URL.\nLet’s use it to load a CSV file with mock archaeological data which is at the URL https://mint.westdri.ca/r/hss_data/arc1.csv:\n\narc1 &lt;- read_csv(\"https://mint.westdri.ca/r/hss_data/arc1.csv\")\n\nRows: 6 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): Site, Date, Number of artifacts, Name of PI, Comments\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nIf the file was in your machine, you would provide its path instead of the URL.\n\nHere is our data:\n\narc1\n\n# A tibble: 6 × 5\n  Site  Date      `Number of artifacts` `Name of PI`\n  &lt;chr&gt; &lt;chr&gt;     &lt;chr&gt;                 &lt;chr&gt;       \n1 E1    13/2/2001 4                     John Doe    \n2 E1    14/2/2001 3                     John Doe    \n3 A2    26/3/2003 N/A                   Paul Smith  \n4 B18   4/5/2006  5                     Paul Smith  \n5 B7    4/5/2006  5                     n/a         \n6 B3    4/5/2006  5                     P. Smith    \n  Comments                          \n  &lt;chr&gt;                             \n1 &lt;NA&gt;                              \n2 &lt;NA&gt;                              \n3 Artifacts still need to be counted\n4 &lt;NA&gt;                              \n5 &lt;NA&gt;                              \n6 &lt;NA&gt;",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Data import and export"
    ]
  },
  {
    "objectID": "r/hss_import.html#improper-na",
    "href": "r/hss_import.html#improper-na",
    "title": "Data import and export",
    "section": "Improper NA",
    "text": "Improper NA\nIn R, missing values are represented by NA (not available). It is a constant that R understands and can deal with, so it is important that all missing values are represented properly.\nWhen you enter data (say in an Excel file or CSV file), leave an empty cell for missing values: R will then transform them automatically into NA.\nBecause this data was not entered properly, we have to fix our missing values. One way to go about this is to replace the characters representing missing values in the file (\"N/A\" and \"n/a\") by NA:\n\nis.na(arc1) &lt;- arc1 == \"N/A\"\nis.na(arc1) &lt;- arc1 == \"n/a\"\narc1\n\n# A tibble: 6 × 5\n  Site  Date      `Number of artifacts` `Name of PI`\n  &lt;chr&gt; &lt;chr&gt;     &lt;chr&gt;                 &lt;chr&gt;       \n1 E1    13/2/2001 4                     John Doe    \n2 E1    14/2/2001 3                     John Doe    \n3 A2    26/3/2003 &lt;NA&gt;                  Paul Smith  \n4 B18   4/5/2006  5                     Paul Smith  \n5 B7    4/5/2006  5                     &lt;NA&gt;        \n6 B3    4/5/2006  5                     P. Smith    \n  Comments                          \n  &lt;chr&gt;                             \n1 &lt;NA&gt;                              \n2 &lt;NA&gt;                              \n3 Artifacts still need to be counted\n4 &lt;NA&gt;                              \n5 &lt;NA&gt;                              \n6 &lt;NA&gt;                              \n\n\nNow, we have another problem to fix: readr is very good at guessing the types of the various variables. Unfortunately, the character \"N/A\" in the Number of artifacts column prevented it to guess the type properly: it should be a double (a numerical value) and not a character. We can fix this too:\n\narc1$`Number of artifacts` &lt;- as.double(arc1$`Number of artifacts`)\narc1\n\n# A tibble: 6 × 5\n  Site  Date      `Number of artifacts` `Name of PI`\n  &lt;chr&gt; &lt;chr&gt;                     &lt;dbl&gt; &lt;chr&gt;       \n1 E1    13/2/2001                     4 John Doe    \n2 E1    14/2/2001                     3 John Doe    \n3 A2    26/3/2003                    NA Paul Smith  \n4 B18   4/5/2006                      5 Paul Smith  \n5 B7    4/5/2006                      5 &lt;NA&gt;        \n6 B3    4/5/2006                      5 P. Smith    \n  Comments                          \n  &lt;chr&gt;                             \n1 &lt;NA&gt;                              \n2 &lt;NA&gt;                              \n3 Artifacts still need to be counted\n4 &lt;NA&gt;                              \n5 &lt;NA&gt;                              \n6 &lt;NA&gt;                              \n\n\nAlternatively, it is simpler to have read_csv() properly recognize the missing values. This can be done thanks to the na argument:\n\narc1 &lt;- read_csv(\n  \"https://mint.westdri.ca/r/hss_data/arc1.csv\",\n  na = c(\"N/A\", \"n/a\")\n)\n\nRows: 6 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): Site, Date, Name of PI, Comments\ndbl (1): Number of artifacts\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\narc1\n\n# A tibble: 6 × 5\n  Site  Date      `Number of artifacts` `Name of PI`\n  &lt;chr&gt; &lt;chr&gt;                     &lt;dbl&gt; &lt;chr&gt;       \n1 E1    13/2/2001                     4 John Doe    \n2 E1    14/2/2001                     3 John Doe    \n3 A2    26/3/2003                    NA Paul Smith  \n4 B18   4/5/2006                      5 Paul Smith  \n5 B7    4/5/2006                      5 &lt;NA&gt;        \n6 B3    4/5/2006                      5 P. Smith    \n  Comments                            \n  &lt;chr&gt;                               \n1 \"\"                                  \n2 \"\"                                  \n3 \"Artifacts still need to be counted\"\n4 \"\"                                  \n5 \"\"                                  \n6 \"\"                                  \n\n\nA benefit of this approach is that read_csv() now automatically detects the proper data type of Number of artifacts (since there is no more confusing character in what is otherwise a column of doubles).",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Data import and export"
    ]
  },
  {
    "objectID": "r/hss_import.html#dealing-with-dates",
    "href": "r/hss_import.html#dealing-with-dates",
    "title": "Data import and export",
    "section": "Dealing with dates",
    "text": "Dealing with dates\nThere is another problem in our data frame: the Date variable should be of the date type, but read_csv() failed to recognize the values as dates and processed them as characters. This is because it is not entered in our data following the ISO 8601 format which is YYYY-MM-DD. When you enter data, make sure to follow this format as it will make things work automatically. In our case, we have to convert the date.\nThe tidyverse package dealing with date is lubridate. Let’s load it:\n\nlibrary(lubridate)\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\n\nlubridate comes with many functions that can convert dates and times from many format to the ISO format. Since our date have the day, then the month, then the year, the function we need is dmy():\n\narc1$Date &lt;- dmy(arc1$Date)\n\nAlternatively, read_csv() will understand dates in a non ISO format, provided you give it the right information. This can be done with the col_types argument and the col_date() function to which the parameters corresponding to your date format are passed.\nHere are the parameters to use:\n\n\n\n\nFormat\nExample\nParameter\n\n\n\n\nYear\n4 digits\n2024\n%Y\n\n\n\n2 digits\n24\n%y\n\n\nMonth\nDecimal\n2\n%m\n\n\n\nAbbreviated name\nFeb\n%b\n\n\n\nFull name\nFebruary\n%B\n\n\nDay\nDecimal\n8\n%d\n\n\n\nIn our case, the date looks like \"%d/%m/%Y\":\n\narc1 &lt;- read_csv(\n  \"https://mint.westdri.ca/r/hss_data/arc1.csv\",\n  na = c(\"N/A\", \"n/a\"),\n  col_types = cols(Date = col_date(\"%d/%m/%Y\"))\n)\narc1\n\n# A tibble: 6 × 5\n  Site  Date       `Number of artifacts` `Name of PI`\n  &lt;chr&gt; &lt;date&gt;                     &lt;dbl&gt; &lt;chr&gt;       \n1 E1    2001-02-13                     4 John Doe    \n2 E1    2001-02-14                     3 John Doe    \n3 A2    2003-03-26                    NA Paul Smith  \n4 B18   2006-05-04                     5 Paul Smith  \n5 B7    2006-05-04                     5 &lt;NA&gt;        \n6 B3    2006-05-04                     5 P. Smith    \n  Comments                            \n  &lt;chr&gt;                               \n1 \"\"                                  \n2 \"\"                                  \n3 \"Artifacts still need to be counted\"\n4 \"\"                                  \n5 \"\"                                  \n6 \"\"",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Data import and export"
    ]
  },
  {
    "objectID": "r/hss_import.html#renaming-variables",
    "href": "r/hss_import.html#renaming-variables",
    "title": "Data import and export",
    "section": "Renaming variables",
    "text": "Renaming variables\nVariable names cannot contain spaces. Since our data did have spaces in some of the names and since those names were not quoted, R added backticks ``` to be able to make use of them. This makes for rather awkward variables. Let’s rename them.\nWe could use the camel or snake case, but we can also just simplify the names:\n\narc1 &lt;- arc1 |&gt;\n  rename(\n    Artifacts = `Number of artifacts`,\n    PI = `Name of PI`\n  )\n\nError in rename(arc1, Artifacts = `Number of artifacts`, PI = `Name of PI`): could not find function \"rename\"",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Data import and export"
    ]
  },
  {
    "objectID": "r/hss_import.html#fixing-inconsistencies",
    "href": "r/hss_import.html#fixing-inconsistencies",
    "title": "Data import and export",
    "section": "Fixing inconsistencies",
    "text": "Fixing inconsistencies\nThere is still another problem in our data: Paul Smith and P. Smith are—as far as R is concerned—2 different values. The number of PIs in our data should be two, but R currently interprets it as being three:\n\ndplyr::n_distinct(arc1$PI, na.rm = TRUE)\n\nWarning: Unknown or uninitialised column: `PI`.\n\n\n[1] 0\n\n\n\nWe remove the missing values so that they don’t get counted as an additional PI (although, more PIs could have been involved in the data collection: dealing with missing values programmatically is easy once they are properly formatted, but what to do with them methodologically depends on the situation and is part of the research question).\n\nThis can be a problem for future analysis, so let’s fix it. There are many ways to go about this, but the simplest is to use regular expressions:\n\narc1$PI &lt;- gsub(\"P\\\\.\", \"Paul\", arc1$PI)\n\nWarning: Unknown or uninitialised column: `PI`.\n\n\nError in `$&lt;-`:\n! Assigned data `gsub(\"P\\\\\\\\.\", \"Paul\", arc1$PI)` must be compatible\n  with existing data.\n✖ Existing data has 6 rows.\n✖ Assigned data has 0 rows.\nℹ Only vectors of size 1 are recycled.\nCaused by error in `vectbl_recycle_rhs_rows()`:\n! Can't recycle input of size 0 to size 6.\n\n\nOur data is finally well formatted and can be used for plotting, analyses, etc.:\n\narc1\n\n# A tibble: 6 × 5\n  Site  Date       `Number of artifacts` `Name of PI`\n  &lt;chr&gt; &lt;date&gt;                     &lt;dbl&gt; &lt;chr&gt;       \n1 E1    2001-02-13                     4 John Doe    \n2 E1    2001-02-14                     3 John Doe    \n3 A2    2003-03-26                    NA Paul Smith  \n4 B18   2006-05-04                     5 Paul Smith  \n5 B7    2006-05-04                     5 &lt;NA&gt;        \n6 B3    2006-05-04                     5 P. Smith    \n  Comments                            \n  &lt;chr&gt;                               \n1 \"\"                                  \n2 \"\"                                  \n3 \"Artifacts still need to be counted\"\n4 \"\"                                  \n5 \"\"                                  \n6 \"\"",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Data import and export"
    ]
  },
  {
    "objectID": "r/hss_import.html#writing-data-to-file",
    "href": "r/hss_import.html#writing-data-to-file",
    "title": "Data import and export",
    "section": "Writing data to file",
    "text": "Writing data to file\nNow that we have a properly formatted data frame, we could, if we needed to, export it to a new file. readr also has functions to write to text files.\nLet’s save our data frame as a new CSV file (make sure to give it a different name from the original file):\nwrite_csv(arc1, \"arc1_clean.csv\")",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Data import and export"
    ]
  },
  {
    "objectID": "r/hss_functions.html",
    "href": "r/hss_functions.html",
    "title": "Function definition",
    "section": "",
    "text": "R comes with a number of built-in functions. Packages can provide additional ones. In many cases however, you will want to create your own functions to perform exactly the computations that you need.\nIn this section, we will see how to define new functions.",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Function definition"
    ]
  },
  {
    "objectID": "r/hss_functions.html#syntax",
    "href": "r/hss_functions.html#syntax",
    "title": "Function definition",
    "section": "Syntax",
    "text": "Syntax\nHere is the syntax to define a new function:\nname &lt;- function(arguments) {\n  body\n}",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Function definition"
    ]
  },
  {
    "objectID": "r/hss_functions.html#example",
    "href": "r/hss_functions.html#example",
    "title": "Function definition",
    "section": "Example",
    "text": "Example\nLet’s define a function that we call compare which will compare the value between 2 numbers:\n\ncompare &lt;- function(x, y) {\n  x == y\n}\n\n\ncompare is the name of our function.\nx and y are the placeholders for the arguments that our function will accept (our function will need 2 arguments to run successfully).\nx == y is the body of the function, that is, the computation performed by our function.\n\nWe can now use our function:\n\ncompare(2, 3)\n\n[1] FALSE",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Function definition"
    ]
  },
  {
    "objectID": "r/hss_functions.html#what-is-returned-by-a-function",
    "href": "r/hss_functions.html#what-is-returned-by-a-function",
    "title": "Function definition",
    "section": "What is returned by a function?",
    "text": "What is returned by a function?\nIn R, the result of the last statement is printed automatically:\n\ntest &lt;- function(x, y) {\n  x\n  y\n}\ntest(2, 3)\n\n[1] 3\n\n\nIf you want to also print other results, you need to explicitly use the print() function:\n\ntest &lt;- function(x, y) {\n  print(x)\n  y\n}\ntest(2, 3)\n\n[1] 2\n\n\n[1] 3\n\n\nNote that, unlike print(), the function return() exits the function:\n\ntest &lt;- function(x, y) {\n  return(x)\n  y\n}\ntest(2, 3)\n\n[1] 2\n\n\n\ntest &lt;- function(x, y) {\n  return(x)\n  return(y)\n}\ntest(2, 3)\n\n[1] 2",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Function definition"
    ]
  },
  {
    "objectID": "r/hss_data_types.html",
    "href": "r/hss_data_types.html",
    "title": "Data types and structures",
    "section": "",
    "text": "It might be time to talk a bit more formally about the various data types and structures available in R. The goal of this course is not to get bogged down in the nitty-gritty of R syntax, so this section is kept very short.",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Data types and structures"
    ]
  },
  {
    "objectID": "r/hss_data_types.html#data-types",
    "href": "r/hss_data_types.html#data-types",
    "title": "Data types and structures",
    "section": "Data types",
    "text": "Data types\n\ntypeof(\"Some words\")\n\n[1] \"character\"\n\ntypeof(2)\n\n[1] \"double\"\n\ntypeof(2.0)\n\n[1] \"double\"\n\ntypeof(2L)\n\n[1] \"integer\"\n\ntypeof(TRUE)\n\n[1] \"logical\"",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Data types and structures"
    ]
  },
  {
    "objectID": "r/hss_data_types.html#data-structures",
    "href": "r/hss_data_types.html#data-structures",
    "title": "Data types and structures",
    "section": "Data structures",
    "text": "Data structures\n\n\n\nDimension\nHomogeneous\nHeterogeneous\n\n\n\n\n1 d\nAtomic vector\nList\n\n\n2 d\nMatrix\nData frame\n\n\n3 d\nArray\n\n\n\n\n\nAtomic vectors\n\nc(2, 4, 1)\n\n[1] 2 4 1\n\nstr(c(2, 4, 1))\n\n num [1:3] 2 4 1\n\nc(2.2, 4.4, 1.0)\n\n[1] 2.2 4.4 1.0\n\nstr(c(2.2, 4.4, 1.0))\n\n num [1:3] 2.2 4.4 1\n\n1:3\n\n[1] 1 2 3\n\nstr(1:3)\n\n int [1:3] 1 2 3\n\nc(\"some\", \"random\", \"words\")\n\n[1] \"some\"   \"random\" \"words\" \n\nstr(c(\"some\", \"random\", \"words\"))\n\n chr [1:3] \"some\" \"random\" \"words\"\n\n\n\n\nMatrices\n\nm &lt;- matrix(1:12, nrow = 3, ncol = 4)\nm\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    4    7   10\n[2,]    2    5    8   11\n[3,]    3    6    9   12\n\nstr(m)\n\n int [1:3, 1:4] 1 2 3 4 5 6 7 8 9 10 ...\n\n\n\n\nArrays\n\na &lt;- array(as.double(1:24), c(3, 2, 4))\na\n\n, , 1\n\n     [,1] [,2]\n[1,]    1    4\n[2,]    2    5\n[3,]    3    6\n\n, , 2\n\n     [,1] [,2]\n[1,]    7   10\n[2,]    8   11\n[3,]    9   12\n\n, , 3\n\n     [,1] [,2]\n[1,]   13   16\n[2,]   14   17\n[3,]   15   18\n\n, , 4\n\n     [,1] [,2]\n[1,]   19   22\n[2,]   20   23\n[3,]   21   24\n\nstr(a)\n\n num [1:3, 1:2, 1:4] 1 2 3 4 5 6 7 8 9 10 ...\n\n\n\n\nLists\n\nl &lt;- list(2L, 3, c(2, 1), FALSE, \"string\")\nl\n\n[[1]]\n[1] 2\n\n[[2]]\n[1] 3\n\n[[3]]\n[1] 2 1\n\n[[4]]\n[1] FALSE\n\n[[5]]\n[1] \"string\"\n\nstr(l)\n\nList of 5\n $ : int 2\n $ : num 3\n $ : num [1:2] 2 1\n $ : logi FALSE\n $ : chr \"string\"\n\n\n\n\nData frames\n\nd &lt;- data.frame(\n  country = c(\"Canada\", \"USA\", \"Mexico\"),\n  var = c(2.9, 3.1, 4.5)\n)\nd\n\n  country var\n1  Canada 2.9\n2     USA 3.1\n3  Mexico 4.5\n\nstr(d)\n\n'data.frame':   3 obs. of  2 variables:\n $ country: chr  \"Canada\" \"USA\" \"Mexico\"\n $ var    : num  2.9 3.1 4.5",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Data types and structures"
    ]
  },
  {
    "objectID": "r/hss_analyze.html",
    "href": "r/hss_analyze.html",
    "title": "Data visualization",
    "section": "",
    "text": "To understand the data, it is useful to visualize it."
  },
  {
    "objectID": "r/hpc_resources.html",
    "href": "r/hpc_resources.html",
    "title": "Resources for HPC in R",
    "section": "",
    "text": "This section contains resources specific to high-performance R. For introductory/general R resources, see this page instead.\n\n\nCRAN Task Views\n\nCRAN Task Views give information on packages relevant to certain topics.\n\n\nThe High-Performance and Parallel Computing with R task view lists a lot of packages useful for HPC in R.\n\n\n\nRunning R on the Alliance clusters\n\nThe Alliance wiki contains a lot of documentation on how to run code on the Alliance clusters. Here are pages particularly relevant for HPC in R:\n\n\nGetting started: how to get started using the Alliance supercomputers.\nRunning jobs: how to launch Slurm jobs.\nRunning R: how to use R on the Alliance supercomputers.\nTechnical support: what to do if you are stuck running code on one of the Alliance clusters.\n\nIf you are still having issues after reading the documentation, you can open a ticket by emailing support@tech.alliancecan.ca.\n\n\nOnline books\n\nAdvanced R\nEfficient R programming\n\n\n\nRcpp\n\nDocumentation and examples",
    "crumbs": [
      "R",
      "<b><em>High-performance R</em></b>",
      "Resources for HPC in R"
    ]
  },
  {
    "objectID": "r/hpc_performance.html",
    "href": "r/hpc_performance.html",
    "title": "Measuring performance:",
    "section": "",
    "text": "Before we talk about ways to improve performance, let’s see how to measure it.",
    "crumbs": [
      "R",
      "<b><em>High-performance R</em></b>",
      "Measuring performance"
    ]
  },
  {
    "objectID": "r/hpc_performance.html#when-should-you-care",
    "href": "r/hpc_performance.html#when-should-you-care",
    "title": "Measuring performance:",
    "section": "When should you care?",
    "text": "When should you care?\n\n“There is no doubt that the grail of efficiency leads to abuse. Programmers waste enormous amounts of time thinking about, or worrying about, the speed of noncritical parts of their programs, and these attempts at efficiency actually have a strong negative impact when debugging and maintenance are considered. We should forget about small efficiencies, say about 97% of the time: premature optimization is the root of all evil. Yet we should not pass up our opportunities in that critical 3%.”\n— Donald Knuth\n\nOptimizing code takes time, can lead to mistakes, and may make code harder to read. Consequently, not all code is worth optimizing and before jumping into optimizations, you need a strategy.\nYou should consider optimizations when:\n\nyou have debugged your code (optimization comes last, don’t optimize a code that doesn’t run),\nyou will run a section of code (e.g. a function) many times (your optimization efforts will really pay off),\na section of code is particularly slow.\n\nHow do you know which sections of your code are slow? Don’t rely on intuition. You need to profile your code to identify bottlenecks.",
    "crumbs": [
      "R",
      "<b><em>High-performance R</em></b>",
      "Measuring performance"
    ]
  },
  {
    "objectID": "r/hpc_performance.html#profiling",
    "href": "r/hpc_performance.html#profiling",
    "title": "Measuring performance:",
    "section": "Profiling",
    "text": "Profiling\n\n“It is often a mistake to make a priori judgments about what parts of a program are really critical, since the universal experience of programmers who have been using measurement tools has been that their intuitive guesses fail.”\n— Donald Knuth\n\n\nBase R profiler\nR comes with a profiler: Rprof.\nThe data gets collected with:\n## Start profiler\nRprof()\n\n&lt;Your code to profile&gt;\n\n## Stop profiler\nRprof(NULL)\nThis creates a Rprof.out file in your working directory (you can give it another name by passing a name into the initial call to Rprof (e.g. Rprof(\"test.out\")).\nThe raw data is dense and is better read by running summaryRprof() (or summaryRprof(\"test.out\") if you have created the file test.out rather than the default).\nAlternatively, you can run R CMD Rprof (or R CMD Rprof test.out if you named your file) from the command line.\nYou can find an example here.\n\n\nPackages\nA number of packages run Rprof under the hood and create flame graphs or provide other utilities to visualize the profiling data:\n\nprofr,\nproftools,\nprofvis built by posit (formerly RStudio Inc) is the newest tool. See here for an example.",
    "crumbs": [
      "R",
      "<b><em>High-performance R</em></b>",
      "Measuring performance"
    ]
  },
  {
    "objectID": "r/hpc_performance.html#benchmarking",
    "href": "r/hpc_performance.html#benchmarking",
    "title": "Measuring performance:",
    "section": "Benchmarking",
    "text": "Benchmarking\nOnce you have identified expressions that are particularly slow, you can use benchmarking tools to compare variations of the code.\nIn the most basic fashion, you can use system.time(), but this is limited and imprecise.\nThe microbenchmark package is a much better option. It gives the minimum time, lower quartile, mean, median, upper quartile, and maximum time of R expressions.\nThe newer bench package is very similar, but it has less overhead, is more accurate, and—for sequential code—gives information on memory usage and garbage collections. This is the package that we will use for this course.\nThe main function from this package is mark(). You can pass as argument(s) one or multiple expressions that you want to benchmark. By default, it ensures that all expressions output the same result. If you want to remove this test, add the argument check = FALSE.\nWhile mark() gives memory usage and garbage collection information for sequential code, this functionality is not yet implemented for parallel code. When benchmarking parallel expressions, we will have to use the argument memory = FALSE.\nYou will see many examples throughout this course.",
    "crumbs": [
      "R",
      "<b><em>High-performance R</em></b>",
      "Measuring performance"
    ]
  },
  {
    "objectID": "r/hpc_parallelism.html",
    "href": "r/hpc_parallelism.html",
    "title": "Parallelism: concepts",
    "section": "",
    "text": "Once all sequential optimizations have been exhausted, it is time to consider whether parallelization makes sense.\nThis section covers important concepts that are necessary to understand clearly before moving on to writing parallel code.",
    "crumbs": [
      "R",
      "<b><em>High-performance R</em></b>",
      "Parallelism: concepts"
    ]
  },
  {
    "objectID": "r/hpc_parallelism.html#hidden-parallelism",
    "href": "r/hpc_parallelism.html#hidden-parallelism",
    "title": "Parallelism: concepts",
    "section": "Hidden parallelism",
    "text": "Hidden parallelism\nAn increasing number of packages run code in parallel under the hood. It is very important to be aware of this before attempting any explicit parallelization or you may end up with recursive multicore parallelization and an explosion of running cores. This can be both inefficient with demultiplied overhead and extremely resource intensive.\nOne way to assess this is to test the package on your machine and look at the number of cores running with tools such as htop.",
    "crumbs": [
      "R",
      "<b><em>High-performance R</em></b>",
      "Parallelism: concepts"
    ]
  },
  {
    "objectID": "r/hpc_parallelism.html#embarrassingly-parallel-problems",
    "href": "r/hpc_parallelism.html#embarrassingly-parallel-problems",
    "title": "Parallelism: concepts",
    "section": "Embarrassingly parallel problems",
    "text": "Embarrassingly parallel problems\nIdeal cases for parallelization are embarrassingly parallel problems: problems which can be broken down into independent tasks without any work.\nExamples:\n\nLoops for which all iterations are independent of each others.\nResampling (e.g. bootstrapping or cross-validation).\nEnsemble learning (e.g. random forests).\n\nExamples of problems which are not embarrassingly parallel:\n\nLoops for which the result of one iteration is needed for the next iteration.\nRecursive function calls.\nProblems that are inherently sequential.\n\nFor non-embarrassingly parallel problems, one solution is to use C++ to improve speed, as we will see at the end of this course.",
    "crumbs": [
      "R",
      "<b><em>High-performance R</em></b>",
      "Parallelism: concepts"
    ]
  },
  {
    "objectID": "r/hpc_parallelism.html#types-of-parallelism",
    "href": "r/hpc_parallelism.html#types-of-parallelism",
    "title": "Parallelism: concepts",
    "section": "Types of parallelism",
    "text": "Types of parallelism\nThere are various ways to run code in parallel and it is important to have a clear understanding of what each method entails.\n\nMulti-threading\nWe talk about multi-threading when a single process (with its own memory) runs multiple threads.\nThe execution can happen in parallel—if each thread has access to a CPU core—or by alternating some of the threads on some CPU cores.\nBecause all threads in a process write to the same memory addresses, multi-threading can lead to race conditions.\nMulti-threading does not seem to be a common approach to parallelizing R code.\n\n\nMulti-processing in shared memory\nMulti-processing in shared memory happens when multiple processes execute code on multiple CPU cores of a single node (or a single machine).\nThe different processes need to communicate with each other, but because they are all running on the CPU cores of a single node, messages can pass via shared memory.\n\n\nMulti-processing in distributed memory\nWhen processes involved in the execution of some code run on multiple nodes of a cluster, messages between them need to travel over the cluster interconnect. In that case, we talk about distributed memory.",
    "crumbs": [
      "R",
      "<b><em>High-performance R</em></b>",
      "Parallelism: concepts"
    ]
  },
  {
    "objectID": "r/hpc_optimizations.html",
    "href": "r/hpc_optimizations.html",
    "title": "Optimizations",
    "section": "",
    "text": "A lot of hardware is not the answer to poorly written code. Before considering parallelization, you should think about ways to optimize your code sequentially.\nWhy?\n\nnot all code can be parallelized,\nparallelization is costly (overhead of parallelization and, if you use a supercomputer, waiting time to access an Alliance cluster or money spent on a commercial cloud),\nthe optimization of the sequential code will also benefit the parallel code.\n\nIn many cases, writing better code will save you more computing time than parallelization.\nIn this section, we will cover several principles by playing with the programmatic implementation of the fizz buzz game.",
    "crumbs": [
      "R",
      "<b><em>High-performance R</em></b>",
      "Optimizations"
    ]
  },
  {
    "objectID": "r/hpc_optimizations.html#toy-example",
    "href": "r/hpc_optimizations.html#toy-example",
    "title": "Optimizations",
    "section": "Toy example",
    "text": "Toy example\nFizz buzz is a children game to practice divisions. Players take turn counting out loud while replacing:\n\nany number divisible by 3 with the word “Fizz”,\nany number divisible by 5 with the word “Buzz”,\nany number divisible by both 3 and 5 with the word “FizzBuzz”.\n\nLet’s write functions that output series from 1 to n following these rules and time them to draw general principles about code efficiency.",
    "crumbs": [
      "R",
      "<b><em>High-performance R</em></b>",
      "Optimizations"
    ]
  },
  {
    "objectID": "r/hpc_optimizations.html#setup",
    "href": "r/hpc_optimizations.html#setup",
    "title": "Optimizations",
    "section": "Setup",
    "text": "Setup\nFirst of all, we need to load the necessary modules:\nmodule load StdEnv/2020 gcc/11.3.0 r/4.3.1\nThen we need to launch a job.\n\nInteractive job\nIf there are few of us, we will use interactive sessions with one CPU each with:\nsalloc --time=2:00:00 --mem-per-cpu=3500M\nWe can then launch R and load the benchmarking package we will use throughout this section:\n\nlibrary(bench)\n\n\n\nBatch jobs\nIf there are more of us than there are CPUs in the cluster, we will run batch jobs. In this Case:\n\nCreate an R script called optim.R with the code to run (you can reuse the same script for all sections on this page by editing it). Don’t forget to load the package bench in your script.\nCreate a bash script called optim.sh with the following:\n\n\n\n&lt;your_job&gt;.sh\n\n#!/bin/bash\n#SBATCH --account=def-&lt;your_account&gt;\n#SBATCH --time=15\n#SBATCH --mem-per-cpu=3500M\n#SBATCH --cpus-per-task=4\n#SBATCH --job-name=\"&lt;your_job&gt;\"\nmodule load StdEnv/2020 gcc/11.3.0 r/4.3.1\nRscript &lt;your_script&gt;.R\n\n\nRun the jobs with:\n\nsbatch optim.sh",
    "crumbs": [
      "R",
      "<b><em>High-performance R</em></b>",
      "Optimizations"
    ]
  },
  {
    "objectID": "r/hpc_optimizations.html#optimizations",
    "href": "r/hpc_optimizations.html#optimizations",
    "title": "Optimizations",
    "section": "Optimizations",
    "text": "Optimizations\n\nPre-allocate memory\nIn order to store the results of a loop, we need to create an object and assign to it the result of the loop at each iteration. In this first function, we create an empty object z of class integer and of length 0 for that purpose:\n\nf1 &lt;- function(n) {\n  z &lt;- integer()\n  for(i in 1:n) {\n    if(i %% 3 == 0 && i %% 5 == 0) {\n      z[i] &lt;- \"FizzBuzz\"\n    } else if(i %% 3 == 0) {\n      z[i] &lt;- \"Fizz\"\n    } else if(i %% 5 == 0) {\n      z[i] &lt;- \"Buzz\"\n    } else {\n      z[i] &lt;- i\n    }\n  }\n  z\n}\n\nThe second function is similar, but this time, we initialize z with its final length. This means that we are pre-allocating memory for the full vector before we run the loop instead of growing the vector at each iteration:\n\nf2 &lt;- function(n) {\n  z &lt;- integer(n)\n  for(i in 1:n) {\n    if(i %% 3 == 0 && i %% 5 == 0) {\n      z[i] &lt;- \"FizzBuzz\"\n    } else if(i %% 3 == 0) {\n      z[i] &lt;- \"Fizz\"\n    } else if(i %% 5 == 0) {\n      z[i] &lt;- \"Buzz\"\n    } else {\n      z[i] &lt;- i\n    }\n  }\n  z\n}\n\nLet’s make sure that our functions work by testing it on a small number:\n\nf1(20)\n\n [1] \"1\"        \"2\"        \"Fizz\"     \"4\"        \"Buzz\"     \"Fizz\"    \n [7] \"7\"        \"8\"        \"Fizz\"     \"Buzz\"     \"11\"       \"Fizz\"    \n[13] \"13\"       \"14\"       \"FizzBuzz\" \"16\"       \"17\"       \"Fizz\"    \n[19] \"19\"       \"Buzz\"    \n\nf2(20)\n\n [1] \"1\"        \"2\"        \"Fizz\"     \"4\"        \"Buzz\"     \"Fizz\"    \n [7] \"7\"        \"8\"        \"Fizz\"     \"Buzz\"     \"11\"       \"Fizz\"    \n[13] \"13\"       \"14\"       \"FizzBuzz\" \"16\"       \"17\"       \"Fizz\"    \n[19] \"19\"       \"Buzz\"    \n\n\nNow, let’s benchmark them for a large number:\n\nn &lt;- 1e5\nmark(f1(n), f2(n))\n\nWarning: Some expressions had a GC in every iteration; so filtering is\ndisabled.\n\n\n# A tibble: 2 × 6\n  expression      min   median `itr/sec` mem_alloc `gc/sec`\n  &lt;bch:expr&gt; &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt;\n1 f1(n)         155ms    219ms      4.79   16.55MB     16.0\n2 f2(n)         126ms    150ms      6.30    1.15MB     20.5\n\n\nf2() is consistently faster, although very slightly. In many cases, the difference you will find will be a lot greater.\nNote also the large difference in memory allocation.\n\n\nNo, loops are not a big ‘no no’\nBy now, you might be thinking: “Wait… aren’t loops a big ‘no no’ in R? I’ve always been told that they are slow and that one should always use functional programming! We are talking about optimization in this course and we are using loops?!?”\nThere are a lot of misconceptions around R loops. They can be very slow if you don’t pre-allocate memory. Otherwise they are almost always faster than functions (the apply() family or the tidyverse equivalent of the purrr::map() family). You can choose to use a functional programming approach for style and readability, but not for speed.\nLet’s test it.\nFirst we create a function:\n\nf3 &lt;- function(n) {\n  if(n %% 3 == 0 && n %% 5 == 0) {\n    \"FizzBuzz\"\n  } else if(n %% 3 == 0) {\n    \"Fizz\"\n  } else if(n %% 5 == 0) {\n    \"Buzz\"\n  } else {\n    n\n  }\n}\n\nThen we pass it through sapply(). We can test that it works on a small number:\n\nsapply(1:20, f3)\n\n [1] \"1\"        \"2\"        \"Fizz\"     \"4\"        \"Buzz\"     \"Fizz\"    \n [7] \"7\"        \"8\"        \"Fizz\"     \"Buzz\"     \"11\"       \"Fizz\"    \n[13] \"13\"       \"14\"       \"FizzBuzz\" \"16\"       \"17\"       \"Fizz\"    \n[19] \"19\"       \"Buzz\"    \n\n\nFinally, we compare the timing with that of f2():\n\nmark(f2(n), sapply(1:n, f3))\n\nWarning: Some expressions had a GC in every iteration; so filtering is\ndisabled.\n\n\n# A tibble: 2 × 6\n  expression           min   median `itr/sec` mem_alloc `gc/sec`\n  &lt;bch:expr&gt;      &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt;\n1 f2(n)              259ms    261ms      3.83    1.15MB     15.3\n2 sapply(1:n, f3)    188ms    199ms      4.88    3.29MB     21.2\n\n\nAs you can see, the loop is faster.\n\n\nAvoid unnecessary operations\n\nExample 1\nCalling z as the last command in our function is the same as calling return(z).\nFrom the R documentation:\n\nIf the end of a function is reached without calling return, the value of the last evaluated expression is returned.\n\nNow, what about using print() instead?\n\nf4 &lt;- function(n) {\n  z &lt;- integer(n)\n  for(i in 1:n) {\n    if(i %% 3 == 0 && i %% 5 == 0) {\n      z[i] &lt;- \"FizzBuzz\"\n    } else if(i %% 3 == 0) {\n      z[i] &lt;- \"Fizz\"\n    } else if(i %% 5 == 0) {\n      z[i] &lt;- \"Buzz\"\n    } else {\n      z[i] &lt;- i\n    }\n  }\n  print(z)\n}\n\nLet’s benchmark it against f2():\nmark(f2(n), f4(n))\n [1] \"1\"        \"2\"        \"Fizz\"     \"4\"        \"Buzz\"     \"Fizz\"    \n [7] \"7\"        \"8\"        \"Fizz\"     \"Buzz\"     \"11\"       \"Fizz\"    \n[13] \"13\"       \"14\"       \"FizzBuzz\" \"16\"       \"17\"       \"Fizz\"    \n[19] \"19\"       \"Buzz\"     \"Fizz\"     \"22\"       \"23\"       \"Fizz\"    \n[25] \"Buzz\"     \"26\"       \"Fizz\"     \"28\"       \"29\"       \"FizzBuzz\"\n[31] \"31\"       \"32\"       \"Fizz\"     \"34\"       \"Buzz\"     \"Fizz\"    \n[37] \"37\"       \"38\"       \"Fizz\"     \"Buzz\"     \"41\"       \"Fizz\"\n...\n\n  expression      min median `itr/sec` mem_alloc `gc/sec` n_itr  n_gc total_time\n  &lt;bch:expr&gt; &lt;bch:tm&gt; &lt;bch:&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;   &lt;bch:tm&gt;\n1 f2(n)         131ms  139ms      6.71        NA    21.8      4    13      596ms\n2 f4(n)         405ms  411ms      2.43        NA     8.52     2     7      822ms\n# ℹ 4 more variables: result &lt;list&gt;, memory &lt;list&gt;, time &lt;list&gt;, gc &lt;list&gt;\nWarning message:\nSome expressions had a GC in every iteration; so filtering is disabled.\nf4() is 3 times slower.\nWhat happened?\nprint() returns its argument, but it additionally prints it to the standard output. This is why the mark() function printed the output of f4() before printing the timings.\nAs you can see, printing takes a long time.\nIf you are evaluating f2() on its own (e.g. f2(20)), the returned result will also be printed to standard output and both functions will be equivalent. However, if you are using the function in another context, printing becomes an unnecessary and timely operation and f4() would be a very bad option. f4() is thus not a good function.\nHere is an example in which f4() would perform a totally unnecessary operation that f2() avoids:\n\na &lt;- f2(20)\n\n\nNo unnecessary printing.\n\n\na &lt;- f4(20)\n\n [1] \"1\"        \"2\"        \"Fizz\"     \"4\"        \"Buzz\"     \"Fizz\"    \n [7] \"7\"        \"8\"        \"Fizz\"     \"Buzz\"     \"11\"       \"Fizz\"    \n[13] \"13\"       \"14\"       \"FizzBuzz\" \"16\"       \"17\"       \"Fizz\"    \n[19] \"19\"       \"Buzz\"    \n\n\n\nUnnecessary printing.\n\nEven worse would be to use:\nf5 &lt;- function(n) {\n  for(i in 1:n) {\n    if(i %% 3 == 0 && i %% 5 == 0) {\n      print(\"FizzBuzz\")\n    } else if(i %% 3 == 0) {\n      print(\"Fizz\")\n    } else if(i %% 5 == 0) {\n      print(\"Buzz\")\n    } else {\n      print(i)\n    }\n  }\n}\n\nmark(f2(n), f4(n), check = F)\n  expression      min   median `itr/sec` mem_alloc `gc/sec` n_itr  n_gc\n  &lt;bch:expr&gt; &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;\n1 f2(n)       132.8ms 141.69ms     6.77         NA     25.4     4    15\n2 f5(n)         1.65s    1.65s     0.606        NA     12.7     1    21\n# ℹ 5 more variables: total_time &lt;bch:tm&gt;, result &lt;list&gt;, memory &lt;list&gt;,\n#   time &lt;list&gt;, gc &lt;list&gt;\nWarning message:\nSome expressions had a GC in every iteration; so filtering is disabled.\n\nWe have to disable the check here because the results of the two functions are not technically the same (but we don’t care because, in both cases, the series gets created and that’s what we want).\n\nHere the difference in timing is a factor of 12!\n\n\nExample 2\nOne modulo operation and equality test can be removed by replacing i %% 3 == 0 && i %% 5 == 0 by i %% 15 == 0. We now have three modulo operations and equality tests per iteration instead of four. This gives us a little speedup:\n\nf6 &lt;- function(n) {\n  z &lt;- integer(n)\n  for(i in 1:n) {\n    if(i %% 15 == 0) {\n      z[i] &lt;- \"FizzBuzz\"\n    } else if(i %% 3 == 0) {\n      z[i] &lt;- \"Fizz\"\n    } else if(i %% 5 == 0) {\n      z[i] &lt;- \"Buzz\"\n    } else {\n      z[i] &lt;- i\n    }\n  }\n  z\n}\n\nmark(f2(n), f6(n))\n\nWarning: Some expressions had a GC in every iteration; so filtering is\ndisabled.\n\n\n# A tibble: 2 × 6\n  expression      min   median `itr/sec` mem_alloc `gc/sec`\n  &lt;bch:expr&gt; &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt;\n1 f2(n)         196ms    205ms      4.75    1.15MB     17.4\n2 f6(n)         169ms    186ms      5.39    1.22MB     18.0\n\n\nBut we can remove an additional modulo operation and equality test at each iteration by assigning i %% 3 == 0 and i %% 5 == 0 to variables:\n\nf7 &lt;- function(n) {\n  z &lt;- integer(n)\n  for(i in 1:n) {\n    div3 &lt;- (i %% 3 == 0)\n    div5 &lt;- (i %% 5 == 0)\n    if(div3 && div5) {\n      z[i] &lt;- \"FizzBuzz\"\n    } else if(div3) {\n      z[i] &lt;- \"Fizz\"\n    } else if(div5) {\n      z[i] &lt;- \"Buzz\"\n    } else {\n      z[i] &lt;- i\n    }\n  }\n  z\n}\n\nNow we only have two modulo operations and equality tests per iteration and we get another little speedup:\n\nmark(f6(n), f7(n))\n\nWarning: Some expressions had a GC in every iteration; so filtering is\ndisabled.\n\n\n# A tibble: 2 × 6\n  expression      min   median `itr/sec` mem_alloc `gc/sec`\n  &lt;bch:expr&gt; &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt;\n1 f6(n)         178ms    179ms      5.53    1.15MB     18.4\n2 f7(n)         162ms    163ms      5.96    1.22MB     15.9\n\n\n\n\nExample 3\nWe can assign 1:n to z instead of initializing it as an empty vector, thus rendering the assignment of i to z[i] in the last else statement unnecessary:\n\nf8 &lt;- function(n) {\n  z &lt;- 1:n\n  for(i in z) {\n    div3 &lt;- (i %% 3 == 0)\n    div5 &lt;- (i %% 5 == 0)\n    if(div3 && div5) {\n      z[i] &lt;- \"FizzBuzz\"\n    } else if(div3) {\n      z[i] &lt;- \"Fizz\"\n    } else if(div5) {\n      z[i] &lt;- \"Buzz\"\n    } \n  }\n  z\n}\n\nThis function works:\n\nf8(20)\n\n [1] \"1\"        \"2\"        \"Fizz\"     \"4\"        \"Buzz\"     \"Fizz\"    \n [7] \"7\"        \"8\"        \"Fizz\"     \"Buzz\"     \"11\"       \"Fizz\"    \n[13] \"13\"       \"14\"       \"FizzBuzz\" \"16\"       \"17\"       \"Fizz\"    \n[19] \"19\"       \"Buzz\"    \n\n\nand we get a really good speedup here:\n\nmark(f7(n), f8(n))\n\nWarning: Some expressions had a GC in every iteration; so filtering is\ndisabled.\n\n\n# A tibble: 2 × 6\n  expression      min   median `itr/sec` mem_alloc `gc/sec`\n  &lt;bch:expr&gt; &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt;\n1 f7(n)       127.9ms  146.2ms      6.46    1.15MB     19.4\n2 f8(n)        76.4ms   87.7ms     11.3     1.15MB     22.6\n\n\n\n\n\nVectorize whenever possible\nWe can actually get rid of the loop and use a vectorized approach.\n\nf9 &lt;- function(n) {\n  z &lt;- 1:n\n  div3 &lt;- (z %% 3 == 0)\n  div5 &lt;- (z %% 5 == 0)\n  z[div3] &lt;- \"Fizz\"\n  z[div5] &lt;- \"Buzz\"\n  z[(div3 & div5)] &lt;- \"FizzBuzz\"\n  z\n}\n\nThis still give us the same result:\n\nf9(20)\n\n [1] \"1\"        \"2\"        \"Fizz\"     \"4\"        \"Buzz\"     \"Fizz\"    \n [7] \"7\"        \"8\"        \"Fizz\"     \"Buzz\"     \"11\"       \"Fizz\"    \n[13] \"13\"       \"14\"       \"FizzBuzz\" \"16\"       \"17\"       \"Fizz\"    \n[19] \"19\"       \"Buzz\"    \n\n\n\nmark(f8(n), f9(n))\n\nWarning: Some expressions had a GC in every iteration; so filtering is\ndisabled.\n\n\n# A tibble: 2 × 6\n  expression      min   median `itr/sec` mem_alloc `gc/sec`\n  &lt;bch:expr&gt; &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt;\n1 f8(n)          73ms     83ms      11.7    1.15MB    25.3 \n2 f9(n)        23.4ms     44ms      24.6    5.62MB     1.89\n\n\nThe speedup of 3.8 shows how important it is to use vectorization whenever possible.\n\n\nReplace costly operations where possible\nSometimes, it isn’t obvious that one method will be faster than another. Benchmarking alternative expressions can teach you which ones are faster.\nFor instance, it is much faster to index a column from a dataframe by its name (e.g. dataframe$column1) than by using list indexing (e.g. dataframe[[1]]).\nSometimes, packages exist which bring much more efficiency than can be achieved with base R. In the case of data frames for example, there is data.table.\n\n\nConclusion\nStarting from our first function f1(), we have gained a speedup of 7.4, simply by writing better code and without using parallelization and additional hardware:\n\nmark(f1(n), f9(n))\n\nWarning: Some expressions had a GC in every iteration; so filtering is\ndisabled.\n\n\n# A tibble: 2 × 6\n  expression      min   median `itr/sec` mem_alloc `gc/sec`\n  &lt;bch:expr&gt; &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt;\n1 f1(n)       174.4ms  195.6ms      4.79   16.55MB    19.1 \n2 f9(n)        26.7ms   48.2ms     21.6     5.57MB     1.96\n\n\nIf we used a silly function such as f5() as our starting function, the speedup would be 370.",
    "crumbs": [
      "R",
      "<b><em>High-performance R</em></b>",
      "Optimizations"
    ]
  },
  {
    "objectID": "r/hpc_future.html",
    "href": "r/hpc_future.html",
    "title": "The future package",
    "section": "",
    "text": "The future package is a modern package that brings a consistent and simple API for all evaluation strategies of futures in R.\nExcellent backends have been built on top of it.",
    "crumbs": [
      "R",
      "<b><em>High-performance R</em></b>",
      "The future package"
    ]
  },
  {
    "objectID": "r/hpc_future.html#classic-parallel-packages-in-r",
    "href": "r/hpc_future.html#classic-parallel-packages-in-r",
    "title": "The future package",
    "section": "Classic parallel packages in R",
    "text": "Classic parallel packages in R\nWe talked in the previous section about various types of parallelism. Several options exist in R to run code in shared-memory or distributed parallelism.\nExamples of options for shared-memory parallelism:\n\nThe foreach package with backends such as doMC, now also part of the doParallel package.\nmclapply() and mcmapply() from the parallel package (part of the core distribution of R).\n\nExamples of options for distributed parallelism:\n\nThe foreach package with backends such as doSNOW, now also part of the doParallel package.\nThe suite of clusterApply() and par*apply() functions from the parallel package.\n\n\nThe parallel package is a merger of the former multicore package for shared-memory and of the snow package for distributed parallelism.\nSimilarly, the doParallel package is merger of the doMC package for use with foreach in shared-memory and the doSNOW package for use with foreach for distributed parallelism.",
    "crumbs": [
      "R",
      "<b><em>High-performance R</em></b>",
      "The future package"
    ]
  },
  {
    "objectID": "r/hpc_future.html#the-future-package",
    "href": "r/hpc_future.html#the-future-package",
    "title": "The future package",
    "section": "The future package",
    "text": "The future package\nThe future package opened up a new landscape in the world of parallel R by providing a simple and consistent API for the evaluation of futures sequentially, through shared-memory parallelism, or through distributed parallelism.\n\nA future is an object that acts as an abstract representation for a value in the future. A future can be resolved (if the value has been computed) or unresolved. If the value is queried while the future is unresolved, the process is blocked until the future is resolved. Futures thus allow for asynchronous and parallel evaluations.\n\nThe evaluation strategy is set with the plan() function:\n\nplan(sequential):\nFutures are evaluated sequentially in the current R session.\nplan(multisession):\nFutures are evaluated by new R sessions spawned in the background (multi-processing in shared memory).\nplan(multicore):\nFutures are evaluated in processes forked from the existing process (multi-processing in shared memory).\nplan(cluster):\nFutures are evaluated on an ad-hoc cluster (distributed parallelism across multiple nodes).\n\n\nConsistency\nTo ensure a consistent behaviour across plans, all evaluations are done in a local environment:\n\nlibrary(future)\n\na &lt;- 1\n\nb %&lt;-% {      # %&lt;-% creates futures\n  a &lt;- 2\n}\n\na\n\n[1] 1",
    "crumbs": [
      "R",
      "<b><em>High-performance R</em></b>",
      "The future package"
    ]
  },
  {
    "objectID": "r/hpc_future.html#the-future-ecosystem",
    "href": "r/hpc_future.html#the-future-ecosystem",
    "title": "The future package",
    "section": "the future ecosystem",
    "text": "the future ecosystem\nSeveral great packages have been built on top of the future API.\n\nThe doFuture package allows to parallelize foreach expressions on the future evaluation strategies.\nSimilarly, the future.apply package parallelizes the *apply() functions on these strategies.\nThe furrr package provides a parallel version of purrr for those who prefer this approach to functional programming.\nThe future.callr package implements a future evaluation based on callr that resolves every future in a new R session. This removes any limitation on the number of background R parallel processes that can be active at the same time.\nThe future.batchtools package implements a future evaluation based on the batchtools package—a package that provides functions to interact with HPC systems schedulers such as Slurm.\n\nIn this course, we will cover foreach with doFuture in great details to explain all the important concepts. After that, you will be able to use any of these backends easily.",
    "crumbs": [
      "R",
      "<b><em>High-performance R</em></b>",
      "The future package"
    ]
  },
  {
    "objectID": "r/hpc_data.html",
    "href": "r/hpc_data.html",
    "title": "Data on HPC clusters",
    "section": "",
    "text": "So far, we have played with randomly created data. In your work, you will often need to work with real world data.\nHow do you move it to the cluster? Where should you store it?\nIt’s time to talk about data on HPC clusters."
  },
  {
    "objectID": "r/hpc_data.html#transferring-data-tofrom-the-cluster",
    "href": "r/hpc_data.html#transferring-data-tofrom-the-cluster",
    "title": "Data on HPC clusters",
    "section": "Transferring data to/from the cluster",
    "text": "Transferring data to/from the cluster\n\nSecure Copy Protocol\nSecure Copy Protocol (SCP) allows to copy files over the Secure Shell Protocol (SSH) with the scp utility. scp follows a syntax similar to that of the cp command.\nNote that you need to run it from your local machines (not from the cluster).\n\nCopy from your machine to the cluster\n# Copy a local file to your home directory on the cluster\nscp /local/path/file username@hostname:\n# Copy a local file to some path on the cluster\nscp /local/path/file username@hostname:/remote/path\n\n\nCopy from the cluster to your machine\n# Copy a file from the cluster to some path on your machine\nscp username@hostname:/remote/path/file /local/path\n# Copy a file from the cluster to your current location on your machine\nscp username@hostname:/remote/path/file .\nYou can also use wildcards to transfer multiple files:\n# Copy all the Bash scripts from your cluster home dir to some local path\nscp username@hostname:*.sh /local/path\n\n\nCopying directories\nTo copy a directory, you need to add the -r (recursive) flag:\nscp -r /local/path/folder username@hostname:/remote/path\n\n\nCopying for Windows users\nMobaXterm users (on Windows) can copy files by dragging them between the local and remote machines in the GUI. Alternatively, they can use the download and upload buttons.\n\n\n\nSecure File Transfer Protocol\nThe Secure File Transfer Protocol (SFTP) is more sophisticated and allows additional operations in an interactive shell. The sftp command provided by OpenSSH and other packages launches an SFTP client:\nsftp username@hostname\n\nLook at your prompt: your usual Bash/Zsh prompt has been replaced with sftp&gt;.\n\nFrom this prompt, you can access a number of SFTP commands. Type help for a list:\nsftp&gt; help\nAvailable commands:\nbye                                Quit sftp\ncd path                            Change remote directory to 'path'\nchgrp [-h] grp path                Change group of file 'path' to 'grp'\nchmod [-h] mode path               Change permissions of file 'path' to 'mode'\nchown [-h] own path                Change owner of file 'path' to 'own'\ncopy oldpath newpath               Copy remote file\ncp oldpath newpath                 Copy remote file\ndf [-hi] [path]                    Display statistics for current directory or\n                                   filesystem containing 'path'\nexit                               Quit sftp\nget [-afpR] remote [local]         Download file\nhelp                               Display this help text\nlcd path                           Change local directory to 'path'\nlls [ls-options [path]]            Display local directory listing\nlmkdir path                        Create local directory\nln [-s] oldpath newpath            Link remote file (-s for symlink)\nlpwd                               Print local working directory\nls [-1afhlnrSt] [path]             Display remote directory listing\nlumask umask                       Set local umask to 'umask'\nmkdir path                         Create remote directory\nprogress                           Toggle display of progress meter\nput [-afpR] local [remote]         Upload file\npwd                                Display remote working directory\nquit                               Quit sftp\nreget [-fpR] remote [local]        Resume download file\nrename oldpath newpath             Rename remote file\nreput [-fpR] local [remote]        Resume upload file\nrm path                            Delete remote file\nrmdir path                         Remove remote directory\nsymlink oldpath newpath            Symlink remote file\nversion                            Show SFTP version\n!command                           Execute 'command' in local shell\n!                                  Escape to local shell\n?                                  Synonym for help\nAs this list shows, you have access to a number of classic Unix command such as cd, pwd, ls, etc. These commands will be executed on the remote machine.\nIn addition, there are a number of commands of the form l&lt;command&gt;. “l” stands for “local”.\nThese commands will be executed on your local machine.\nFor instance, ls will list the files in your current directory in the remote machine while lls (“local ls”) will list the files in your current directory on your computer.\nThis means that you are now able to navigate two file systems at once: your local machine and the remote machine.\n\nHere are a few examples:\n\nsftp&gt; pwd              # print remote working directory\nsftp&gt; lpwd             # print local working directory\nsftp&gt; ls               # list files in remote working directory\nsftp&gt; lls              # list files in local working directory\nsftp&gt; cd               # change the remote directory\nsftp&gt; lcd              # change the local directory\nsftp&gt; put local_file   # upload a file\nsftp&gt; get remote_file  # download a file\n\nCopying directories\nTo upload/download directories, you first need to create them in the destination, then copy the content with the -r (recursive) flag.\n\nIf you have a local directory called dir and you want to copy it to the cluster you need to run:\n\nsftp&gt; mkdir dir    # First create the directory\nsftp&gt; put -r dir   # Then copy the content\nTo terminate the session, press &lt;Ctrl+D&gt;.\n\n\n\nSyncing\nIf, instead of an occasional copying of files between your machine and the cluster, you want to keep a directory in sync between both machines, you might want to use rsync instead. You can look at the Alliance wiki page on rsync for complete instructions.\n\n\nHeavy transfers\nWhile the methods covered above work very well for limited amounts of data, if you need to make large transfers, you should use globus instead, following the instructions in the Alliance wiki page on this service.\n\n\nWindows line endings\nOn modern Mac operating systems and on Linux, lines in files are terminated with a newline (\\n). On Windows, they are terminated with a carriage return + newline (\\r\\n).\nWhen you transfer files between Windows and Linux (the cluster uses Linux), this creates a mismatch. Most modern software handle this correctly, but you may occasionally run into problems.\nThe solution is to convert a file from Windows encoding to Unix encoding with:\ndos2unix file\nTo convert a file back to Windows encoding, run:\nunix2dos file"
  },
  {
    "objectID": "r/hpc_data.html#files-management",
    "href": "r/hpc_data.html#files-management",
    "title": "Data on HPC clusters",
    "section": "Files management",
    "text": "Files management\nThe Alliance clusters are designed to handle large files very well. They are however slowed by the presence of many small files. It is thus important to know how to handle large collections of files by archiving them with tools such as tar and dar."
  },
  {
    "objectID": "r/hpc_data.html#where-to-store-data",
    "href": "r/hpc_data.html#where-to-store-data",
    "title": "Data on HPC clusters",
    "section": "Where to store data",
    "text": "Where to store data\nSupercomputers have several filesystems and you should familiarize yourself with the quotas and policies of the clusters you use.\nAll filesystems are mounted on all nodes so that you can access the data on any network storage from any node (e.g. something in /home, /project, or /scratch will be accessible from any login node or compute node).\nA temporary folder gets created directly on the compute nodes while a job is running. In situations with heavy I/O or involving many files, it is worth considering copying data to it as part of the job. In that case, make sure to copy the results back to network storage before the end of the job."
  },
  {
    "objectID": "python/ws_webscraping.html",
    "href": "python/ws_webscraping.html",
    "title": "Web scraping with Python",
    "section": "",
    "text": "The internet is a trove of information. A lot of it is publicly available and thus suitable for use in research. Extracting that information and putting it in an organized format for analysis can however be extremely tedious.\nWeb scraping tools allow to automate parts of that process and Python is a popular language for the task.\nIn this workshop, I will guide you through a simple example using the package Beautiful Soup.",
    "crumbs": [
      "Python",
      "<em><b>Workshops</b></em>",
      "Web scraping with Python"
    ]
  },
  {
    "objectID": "python/ws_webscraping.html#html-and-css",
    "href": "python/ws_webscraping.html#html-and-css",
    "title": "Web scraping with Python",
    "section": "HTML and CSS",
    "text": "HTML and CSS\nHyperText Markup Language (HTML) is the standard markup language for websites: it encodes the information related to the formatting and structure of webpages. Additionally, some of the customization can be stored in Cascading Style Sheets (CSS) files.\nHTML uses tags of the form:\n&lt;some_tag&gt;Your content&lt;/some_tag&gt;\nSome tags have attributes:\n&lt;some_tag attribute_name=\"attribute value\"&gt;Your content&lt;/some_tag&gt;\n\nExamples:\n\nSite structure:\n\n&lt;h2&gt;This is a heading of level 2&lt;/h2&gt;\n&lt;p&gt;This is a paragraph&lt;/p&gt;\n\nFormatting:\n\n&lt;b&gt;This is bold&lt;/b&gt;\n&lt;a href=\"https://some.url\"&gt;This is the text for a link&lt;/a&gt;",
    "crumbs": [
      "Python",
      "<em><b>Workshops</b></em>",
      "Web scraping with Python"
    ]
  },
  {
    "objectID": "python/ws_webscraping.html#web-scrapping",
    "href": "python/ws_webscraping.html#web-scrapping",
    "title": "Web scraping with Python",
    "section": "Web scrapping",
    "text": "Web scrapping\nWeb scraping is a general term for a set of tools which allow for the extraction of data from the web automatically.\nWhile most of the data on the internet is publicly available, it is illegal to scrape some sites and you should always look into the policy of a site before attempting to scrape it. Some sites will also block you if you submit too many requests in a short amount of time, so remember to scrape responsibly.",
    "crumbs": [
      "Python",
      "<em><b>Workshops</b></em>",
      "Web scraping with Python"
    ]
  },
  {
    "objectID": "python/ws_webscraping.html#example-for-this-workshop",
    "href": "python/ws_webscraping.html#example-for-this-workshop",
    "title": "Web scraping with Python",
    "section": "Example for this workshop",
    "text": "Example for this workshop\nWe will use a website from the University of Tennessee containing a database of PhD theses from that university.\nOur goal is to scrape data from this site to produce a dataframe with the date, major, and advisor for each dissertation.\n\nWe will only do this for the first page which contains the links to the 100 most recent theses. If you really wanted to gather all the data, you would have to do this for all pages.",
    "crumbs": [
      "Python",
      "<em><b>Workshops</b></em>",
      "Web scraping with Python"
    ]
  },
  {
    "objectID": "python/ws_webscraping.html#lets-look-at-the-sites",
    "href": "python/ws_webscraping.html#lets-look-at-the-sites",
    "title": "Web scraping with Python",
    "section": "Let’s look at the sites",
    "text": "Let’s look at the sites\nFirst of all, let’s have a close look at the websites we want to scrape to think carefully about what we want to do. Before starting to write code, it is always a good idea to think about what you are trying to achieve with your code.\nTo create a dataframe with the data for all the dissertations on that first page, we need to do two things:\n\nStep 1: from the dissertations database first page, we want to scrape the list of URLs for the dissertation pages.\nStep 2: once we have the URLs, we want to scrape those pages too to get the date, major, and advisor for each dissertation.",
    "crumbs": [
      "Python",
      "<em><b>Workshops</b></em>",
      "Web scraping with Python"
    ]
  },
  {
    "objectID": "python/ws_webscraping.html#load-packages",
    "href": "python/ws_webscraping.html#load-packages",
    "title": "Web scraping with Python",
    "section": "Load packages",
    "text": "Load packages\nLet’s load the packages that will make scraping websites with Python easier:\n\nimport requests                 # To download the html data from a site\nfrom bs4 import BeautifulSoup   # To parse the html data\nimport time                     # To add a delay between each requests\nimport pandas as pd             # To store our data in a DataFrame",
    "crumbs": [
      "Python",
      "<em><b>Workshops</b></em>",
      "Web scraping with Python"
    ]
  },
  {
    "objectID": "python/ws_webscraping.html#send-request-to-the-main-site",
    "href": "python/ws_webscraping.html#send-request-to-the-main-site",
    "title": "Web scraping with Python",
    "section": "Send request to the main site",
    "text": "Send request to the main site\nAs mentioned above, our site is the database of PhD dissertations from the University of Tennessee.\nLet’s create a string with the URL:\n\nurl = \"https://trace.tennessee.edu/utk_graddiss/index.html\"\n\nFirst, we send a request to that URL and save the response in a variable called r:\n\nr = requests.get(url)\n\nLet’s see what our response looks like:\n\nr\n\n&lt;Response [200]&gt;\n\n\nIf you look in the list of HTTP status codes, you can see that a response with a code of 200 means that the request was successful.",
    "crumbs": [
      "Python",
      "<em><b>Workshops</b></em>",
      "Web scraping with Python"
    ]
  },
  {
    "objectID": "python/ws_webscraping.html#explore-the-raw-data",
    "href": "python/ws_webscraping.html#explore-the-raw-data",
    "title": "Web scraping with Python",
    "section": "Explore the raw data",
    "text": "Explore the raw data\nTo get the actual content of the response as unicode (text), we can use the text property of the response. This will give us the raw HTML markup from the webpage.\nLet’s print the first 200 characters:\n\nprint(r.text[:200])\n\n\n&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\"&gt;\n&lt;head&gt;&lt;!-- inj yui3-seed: --&gt;&lt;script type='text/javascript' src='//cdnjs.cloudflare.com/ajax/libs/yui/3.6.0/yui/yui-min.js'&gt;&lt;/script&gt;&lt;script type='text/javascript' sr",
    "crumbs": [
      "Python",
      "<em><b>Workshops</b></em>",
      "Web scraping with Python"
    ]
  },
  {
    "objectID": "python/ws_webscraping.html#parse-the-data",
    "href": "python/ws_webscraping.html#parse-the-data",
    "title": "Web scraping with Python",
    "section": "Parse the data",
    "text": "Parse the data\nThe package Beautiful Soup transforms (parses) such HTML data into a parse tree, which will make extracting information easier.\nLet’s create an object called mainpage with the parse tree:\n\nmainpage = BeautifulSoup(r.text, \"html.parser\")\n\n\nhtml.parser is the name of the parser that we are using here. It is better to use a specific parser to get consistent results across environments.\n\nWe can print the beginning of the parsed result:\n\nprint(mainpage.prettify()[:200])\n\n&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\"&gt;\n &lt;head&gt;\n  &lt;!-- inj yui3-seed: --&gt;\n  &lt;script src=\"//cdnjs.cloudflare.com/ajax/libs/yui/3.6.0/yui/yui-min.js\" type=\"text/javascript\"&gt;\n  &lt;/script&gt;\n  &lt;script src=\"//ajax.g\n\n\n\nThe prettify method turns the BeautifulSoup object we created into a string (which is needed for slicing).\n\nIt doesn’t look any more clear to us, but it is now in a format the Beautiful Soup package can work with.\nFor instance, we can get the HTML segment containing the title with three methods:\n\nusing the title tag name:\n\n\nmainpage.title\n\n&lt;title&gt;\nDoctoral Dissertations | Graduate School | University of Tennessee, Knoxville\n&lt;/title&gt;\n\n\n\nusing find to look for HTML markers (tags, attributes, etc.):\n\n\nmainpage.find(\"title\")\n\n&lt;title&gt;\nDoctoral Dissertations | Graduate School | University of Tennessee, Knoxville\n&lt;/title&gt;\n\n\n\nusing select which accepts CSS selectors:\n\n\nmainpage.select(\"title\")\n\n[&lt;title&gt;\n Doctoral Dissertations | Graduate School | University of Tennessee, Knoxville\n &lt;/title&gt;]\n\n\nfind will only return the first element. find_all will return all elements. select will also return all elements. Which one you chose depends on what you need to extract. There often several ways to get you there.\nHere are other examples of data extraction:\n\nmainpage.head\n\n&lt;head&gt;&lt;!-- inj yui3-seed: --&gt;&lt;script src=\"//cdnjs.cloudflare.com/ajax/libs/yui/3.6.0/yui/yui-min.js\" type=\"text/javascript\"&gt;&lt;/script&gt;&lt;script src=\"//ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js\" type=\"text/javascript\"&gt;&lt;/script&gt;&lt;!-- Adobe Analytics --&gt;&lt;script src=\"https://assets.adobedtm.com/4a848ae9611a/d0e96722185b/launch-d525bb0064d8.min.js\" type=\"text/javascript\"&gt;&lt;/script&gt;&lt;!-- Cookies --&gt;&lt;link href=\"//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.0.3/cookieconsent.min.css\" rel=\"stylesheet\" type=\"text/css\"/&gt;&lt;script src=\"//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.0.3/cookieconsent.min.js\" type=\"text/javascript\"&gt;&lt;/script&gt;&lt;script src=\"/assets/nr_browser_production.js\" type=\"text/javascript\"&gt;&lt;/script&gt;\n&lt;!-- def.1 --&gt;\n&lt;meta charset=\"utf-8\"/&gt;\n&lt;meta content=\"width=device-width\" name=\"viewport\"/&gt;\n&lt;title&gt;\nDoctoral Dissertations | Graduate School | University of Tennessee, Knoxville\n&lt;/title&gt;\n&lt;!-- FILE meta-tags.inc --&gt;&lt;!-- FILE: /srv/sequoia/main/data/assets/site/meta-tags.inc --&gt;\n&lt;!-- FILE: meta-tags.inc (cont) --&gt;\n&lt;!-- sh.1 --&gt;\n&lt;link href=\"/ir-style.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/&gt;\n&lt;link href=\"/ir-custom.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/&gt;\n&lt;link href=\"ir-custom.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/&gt;\n&lt;link href=\"/ir-local.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/&gt;\n&lt;link href=\"ir-local.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/&gt;\n&lt;link href=\"/ir-print.css\" media=\"print\" rel=\"stylesheet\" type=\"text/css\"/&gt;\n&lt;link href=\"/assets/floatbox/floatbox.css\" rel=\"stylesheet\" type=\"text/css\"/&gt;\n&lt;link href=\"/recent.rss\" rel=\"alternate\" title=\"Site Feed\" type=\"application/rss+xml\"/&gt;\n&lt;link href=\"/favicon.ico\" rel=\"shortcut icon\" type=\"image/x-icon\"/&gt;\n&lt;!--[if IE]&gt;\n&lt;link rel=\"stylesheet\" href=\"/ir-ie.css\" type=\"text/css\" media=\"screen\"&gt;\n&lt;![endif]--&gt;\n&lt;!-- JS --&gt;\n&lt;script src=\"/assets/jsUtilities.js\" type=\"text/javascript\"&gt;&lt;/script&gt;\n&lt;script src=\"/assets/footnoteLinks.js\" type=\"text/javascript\"&gt;&lt;/script&gt;\n&lt;script src=\"/assets/scripts/yui-init.pack.js\" type=\"text/javascript\"&gt;&lt;/script&gt;\n&lt;script src=\"/assets/scripts/bepress-init.pack.js\" type=\"text/javascript\"&gt;&lt;/script&gt;\n&lt;script src=\"/assets/scripts/JumpListYUI.pack.js\" type=\"text/javascript\"&gt;&lt;/script&gt;\n&lt;!-- end sh.1 --&gt;\n&lt;script type=\"text/javascript\"&gt;var pageData = {\"page\":{\"environment\":\"prod\",\"productName\":\"bpdg\",\"language\":\"en\",\"name\":\"ir_etd\",\"businessUnit\":\"els:rp:st\"},\"visitor\":{}};&lt;/script&gt;\n&lt;/head&gt;\n\n\n\nmainpage.a\n\n&lt;a data-scroll=\"\" href=\"https://trace.tennessee.edu\" title=\"Home\"&gt;Home&lt;/a&gt;\n\n\n\nmainpage.find_all(\"a\")[:5]\n\n[&lt;a data-scroll=\"\" href=\"https://trace.tennessee.edu\" title=\"Home\"&gt;Home&lt;/a&gt;,\n &lt;a data-scroll=\"\" href=\"https://trace.tennessee.edu/do/search/advanced/\" title=\"Search\"&gt;&lt;i class=\"icon-search\"&gt;&lt;/i&gt; Search&lt;/a&gt;,\n &lt;a data-scroll=\"\" href=\"https://trace.tennessee.edu/communities.html\" title=\"Browse\"&gt;Browse Collections&lt;/a&gt;,\n &lt;a data-scroll=\"\" href=\"/cgi/myaccount.cgi?context=utk_graddiss\" title=\"My Account\"&gt;My Account&lt;/a&gt;,\n &lt;a data-scroll=\"\" href=\"https://trace.tennessee.edu/about.html\" title=\"About\"&gt;About&lt;/a&gt;]\n\n\n\nmainpage.select(\"a\")[:5]\n\n[&lt;a data-scroll=\"\" href=\"https://trace.tennessee.edu\" title=\"Home\"&gt;Home&lt;/a&gt;,\n &lt;a data-scroll=\"\" href=\"https://trace.tennessee.edu/do/search/advanced/\" title=\"Search\"&gt;&lt;i class=\"icon-search\"&gt;&lt;/i&gt; Search&lt;/a&gt;,\n &lt;a data-scroll=\"\" href=\"https://trace.tennessee.edu/communities.html\" title=\"Browse\"&gt;Browse Collections&lt;/a&gt;,\n &lt;a data-scroll=\"\" href=\"/cgi/myaccount.cgi?context=utk_graddiss\" title=\"My Account\"&gt;My Account&lt;/a&gt;,\n &lt;a data-scroll=\"\" href=\"https://trace.tennessee.edu/about.html\" title=\"About\"&gt;About&lt;/a&gt;]",
    "crumbs": [
      "Python",
      "<em><b>Workshops</b></em>",
      "Web scraping with Python"
    ]
  },
  {
    "objectID": "python/ws_webscraping.html#test-run",
    "href": "python/ws_webscraping.html#test-run",
    "title": "Web scraping with Python",
    "section": "Test run",
    "text": "Test run\n\nIdentify relevant markers\nThe html code for this webpage contains the data we are interested in, but it is mixed in with a lot of HTML formatting and data we don’t care about. We need to extract the data relevant to us and turn it into a workable format.\nThe first step is to find the HTML markers that contain our data. One option is to use a web inspector or—even easier—the SelectorGadget, a JavaScript bookmarklet built by Andrew Cantino.\nTo use this tool, go to the SelectorGadget website and drag the link of the bookmarklet to your bookmarks bar.\nNow, go to the dissertations database first page and click on the bookmarklet in your bookmarks bar. You will see a floating box at the bottom of your screen. As you move your mouse across the screen, an orange rectangle appears around each element over which you pass.\nClick on one of the dissertation links: now, there is an a appearing in the box at the bottom as well as the number of elements selected. The selected elements are highlighted in yellow. Those elements are links (in HTML, a tags define hyperlinks).\nAs you can see, all the links we want are selected. However, there are many other links we don’t want that are also highlighted. In fact, all links in the document are selected. We need to remove the categories of links that we don’t want. To do this, hover above any of the links we don’t want. You will see a red rectangle around it. Click on it: now all similar links are gone. You might have to do this a few times until only the relevant links (i.e. those that lead to the dissertation information pages) remain highlighted.\nAs there are 100 such links per page, the count of selected elements in the bottom floating box should be down to 100.\nIn the main section of the floating box, you can now see: .article-listing a. This means that the data we want are under the HTML elements .article-listing a (the class .article-listing and the tag a).\n\n\nExtract test URL\nIt is a good idea to test things out on a single element before doing a massive batch scraping of a site, so let’s test our method for the first dissertation.\nTo start, we need to extract the first URL. Here, we will use the CSS selectors (we can get there using find too). mainpage.select(\".article-listing a\") would give us all the results (100 links):\n\nlen(mainpage.select(\".article-listing a\"))\n\n100\n\n\nTo get the first one, we index it:\n\nmainpage.select(\".article-listing a\")[0]\n\n&lt;a href=\"https://trace.tennessee.edu/utk_graddiss/8076\"&gt;Understanding host-microbe interactions in maize kernel and sweetpotato leaf metagenomic profiles.&lt;/a&gt;\n\n\nThe actual URL is contained in the href attribute. Attributes can be extracted with the get method:\n\nmainpage.select(\".article-listing a\")[0].get(\"href\")\n\n'https://trace.tennessee.edu/utk_graddiss/8076'\n\n\nWe now have our URL as a string. We can double-check that it is indeed a string:\n\ntype(mainpage.select(\".article-listing a\")[0].get(\"href\"))\n\nstr\n\n\nThis is exactly what we need to send a request to that site, so let’s create an object url_test with it:\n\nurl_test = mainpage.select(\".article-listing a\")[0].get(\"href\")\n\nWe have our first thesis URL:\n\nprint(url_test)\n\nhttps://trace.tennessee.edu/utk_graddiss/8076\n\n\n\n\nSend request to test URL\nNow that we have the URL for the first dissertation information page, we want to extract the date, major, and advisor for that dissertation.\nThe first thing to do—as we did earlier with the database site—is to send a request to that page. Let’s assign it to a new object that we will call r_test:\n\nr_test = requests.get(url_test)\n\nThen we can parse it with Beautiful Soup (as we did before). Let’s create a dissertpage_test object:\n\ndissertpage_test = BeautifulSoup(r_test.text, \"html.parser\")\n\n\n\nGet data for test URL\nIt is time to extract the publication date, major, and advisor for our test URL.\nLet’s start with the date. Thanks to the SelectorGadget, following the method we saw earlier, we can see that we now need elements marked by #publication_date p.\nWe can use select as we did earlier:\n\ndissertpage_test.select(\"#publication_date p\")\n\n[&lt;p&gt;5-2023&lt;/p&gt;]\n\n\nNotice the square brackets around our result: this is import. It shows us that we have a ResultSet (a list of results specific to Beautiful Soup). This is because select returns all the results. Here, we have a single result, but the format is still list-like. Before we can go further, we need to index the value out of it:\n\ndissertpage_test.select(\"#publication_date p\")[0]\n\n&lt;p&gt;5-2023&lt;/p&gt;\n\n\nWe can now get the text out of this paragraph with the text attribute:\n\ndissertpage_test.select(\"#publication_date p\")[0].text\n\n'5-2023'\n\n\nWe could save it in a variable date_test:\n\ndate_test = dissertpage_test.select(\"#publication_date p\")[0].text\n\n\n\nYour turn:\n\nGet the major and advisor for our test URL.",
    "crumbs": [
      "Python",
      "<em><b>Workshops</b></em>",
      "Web scraping with Python"
    ]
  },
  {
    "objectID": "python/ws_webscraping.html#full-run",
    "href": "python/ws_webscraping.html#full-run",
    "title": "Web scraping with Python",
    "section": "Full run",
    "text": "Full run\nOnce everything is working for a test site, we can do some bulk scraping.\n\nExtract all URLs\nWe already know how to get the 100 dissertations links from the main page: mainpage.select(\".article-listing a\"). Let’s assign it to a variable:\n\ndissertlinks = mainpage.select(\".article-listing a\")\n\nThis ResultSet is an iterable, meaning that it can be used in a loop.\nLet’s write a loop to extract all the URLs from this ResultSet of links:\n\n# Create an empty list before filling it during the loop\nurls = []\n\nfor link in dissertlinks:\n    urls.append(link.get(\"href\"))\n\nLet’s see our first 5 URLs:\n\nurls[:5]\n\n['https://trace.tennessee.edu/utk_graddiss/8076',\n 'https://trace.tennessee.edu/utk_graddiss/9158',\n 'https://trace.tennessee.edu/utk_graddiss/8080',\n 'https://trace.tennessee.edu/utk_graddiss/8086',\n 'https://trace.tennessee.edu/utk_graddiss/8078']\n\n\n\n\nExtract data from each page\nFor each element of urls (i.e. for each dissertation URL), we can now get our information.\n\n# Create an empty list\nls = []\n\n# For each element of our list of sites\nfor url in urls:\n    # Send a request to the site\n    r = requests.get(url)\n    # Parse the result\n    dissertpage = BeautifulSoup(r.text, \"html.parser\")\n    # Get the date\n    date = dissertpage.select(\"#publication_date p\")[0].text\n    # Get the major\n    major = dissertpage.select(\"#department p\")[0].text\n    # Get the advisor\n    advisor = dissertpage.select(\"#advisor1 p\")[0].text\n    # Store the results in the list\n    ls.append((date, major, advisor))\n    # Add a delay at each iteration\n    time.sleep(0.1)\n\n\nSome sites will block requests if they are too frequent. Adding a little delay between requests is often a good idea.",
    "crumbs": [
      "Python",
      "<em><b>Workshops</b></em>",
      "Web scraping with Python"
    ]
  },
  {
    "objectID": "python/ws_webscraping.html#store-results-in-dataframe",
    "href": "python/ws_webscraping.html#store-results-in-dataframe",
    "title": "Web scraping with Python",
    "section": "Store results in DataFrame",
    "text": "Store results in DataFrame\nA DataFrame would be a lot more convenient than a list to hold our results.\nFirst, we create a list with the column names for our future DataFrame:\n\ncols = [\"Date\", \"Major\", \"Advisor\"]\n\nThen we create our DataFrame:\n\ndf = pd.DataFrame(ls, columns=cols)\n\n\ndf\n\n\n\n\n\n\n\n\n\nDate\nMajor\nAdvisor\n\n\n\n\n0\n5-2023\nLife Sciences\nBode A. Olukolu\n\n\n1\n12-2023\nIndustrial Engineering\nHugh Medal\n\n\n2\n5-2023\nNuclear Engineering\nErik Lukosi\n\n\n3\n5-2023\nEnergy Science and Engineering\nKyle R. Gluesenkamp\n\n\n4\n5-2023\nEnglish\nMargaret Lazarus Dean\n\n\n...\n...\n...\n...\n\n\n95\n8-2023\nEducational Psychology and Research\nQi Sun\n\n\n96\n12-2023\nNuclear Engineering\nLawrence H. Heilbronn\n\n\n97\n5-2023\nGeology\nBradley Thomson\n\n\n98\n5-2023\nNatural Resources\nSharon R. Jean-Philippe\n\n\n99\n12-2023\nPsychology\nGreg Stuart\n\n\n\n\n100 rows × 3 columns",
    "crumbs": [
      "Python",
      "<em><b>Workshops</b></em>",
      "Web scraping with Python"
    ]
  },
  {
    "objectID": "python/ws_webscraping.html#save-results-to-file",
    "href": "python/ws_webscraping.html#save-results-to-file",
    "title": "Web scraping with Python",
    "section": "Save results to file",
    "text": "Save results to file\nAs a final step, we will save our data to a CSV file:\ndf.to_csv('dissertations_data.csv', index=False)\n\nThe default index=True writes the row numbers. We are not writing these indices in our file by changing the value of this argument to False.\n\nIf you are using a Jupyter notebook or the IPython shell, you can type !ls to see that the file is there and !cat dissertations_data.csv to print its content.\n\n! is a magic command that allows to run Unix shell commands in a notebook or IPython shell.",
    "crumbs": [
      "Python",
      "<em><b>Workshops</b></em>",
      "Web scraping with Python"
    ]
  },
  {
    "objectID": "python/ws_pandas.html",
    "href": "python/ws_pandas.html",
    "title": "DataFrames with pandas",
    "section": "",
    "text": "pandas is a Python library built to manipulate data frames and time series.\nFor this section, we will use the Covid-19 data from the Johns Hopkins University CSSE repository.\nYou can visualize this data in a dashboard created by the Johns Hopkins University Center for Systems Science and Engineering.",
    "crumbs": [
      "Python",
      "<em><b>Workshops</b></em>",
      "DataFrames with Pandas"
    ]
  },
  {
    "objectID": "python/ws_pandas.html#setup",
    "href": "python/ws_pandas.html#setup",
    "title": "DataFrames with pandas",
    "section": "Setup",
    "text": "Setup\nFirst, we need to load the pandas library and read in the data from the web:\n\n# Load the pandas library and create a shorter name for it\nimport pandas as pd\n\n# The global confirmed cases are available in CSV format at the url:\nurl = \"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv\"\n\n# pandas allows to read in data from the web directly\ncases = pd.read_csv(url)",
    "crumbs": [
      "Python",
      "<em><b>Workshops</b></em>",
      "DataFrames with Pandas"
    ]
  },
  {
    "objectID": "python/ws_pandas.html#first-look-at-the-data",
    "href": "python/ws_pandas.html#first-look-at-the-data",
    "title": "DataFrames with pandas",
    "section": "First look at the data",
    "text": "First look at the data\nWhat does our data look like?\n\ncases\n\n\n\n\n\n\n\n\n\nProvince/State\nCountry/Region\nLat\nLong\n1/22/20\n1/23/20\n1/24/20\n1/25/20\n1/26/20\n1/27/20\n...\n2/28/23\n3/1/23\n3/2/23\n3/3/23\n3/4/23\n3/5/23\n3/6/23\n3/7/23\n3/8/23\n3/9/23\n\n\n\n\n0\nNaN\nAfghanistan\n33.939110\n67.709953\n0\n0\n0\n0\n0\n0\n...\n209322\n209340\n209358\n209362\n209369\n209390\n209406\n209436\n209451\n209451\n\n\n1\nNaN\nAlbania\n41.153300\n20.168300\n0\n0\n0\n0\n0\n0\n...\n334391\n334408\n334408\n334427\n334427\n334427\n334427\n334427\n334443\n334457\n\n\n2\nNaN\nAlgeria\n28.033900\n1.659600\n0\n0\n0\n0\n0\n0\n...\n271441\n271448\n271463\n271469\n271469\n271477\n271477\n271490\n271494\n271496\n\n\n3\nNaN\nAndorra\n42.506300\n1.521800\n0\n0\n0\n0\n0\n0\n...\n47866\n47875\n47875\n47875\n47875\n47875\n47875\n47875\n47890\n47890\n\n\n4\nNaN\nAngola\n-11.202700\n17.873900\n0\n0\n0\n0\n0\n0\n...\n105255\n105277\n105277\n105277\n105277\n105277\n105277\n105277\n105288\n105288\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n284\nNaN\nWest Bank and Gaza\n31.952200\n35.233200\n0\n0\n0\n0\n0\n0\n...\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n\n\n285\nNaN\nWinter Olympics 2022\n39.904200\n116.407400\n0\n0\n0\n0\n0\n0\n...\n535\n535\n535\n535\n535\n535\n535\n535\n535\n535\n\n\n286\nNaN\nYemen\n15.552727\n48.516388\n0\n0\n0\n0\n0\n0\n...\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n\n\n287\nNaN\nZambia\n-13.133897\n27.849332\n0\n0\n0\n0\n0\n0\n...\n343012\n343012\n343079\n343079\n343079\n343135\n343135\n343135\n343135\n343135\n\n\n288\nNaN\nZimbabwe\n-19.015438\n29.154857\n0\n0\n0\n0\n0\n0\n...\n263921\n264127\n264127\n264127\n264127\n264127\n264127\n264127\n264276\n264276\n\n\n\n\n289 rows × 1147 columns\n\n\n\n\n\n# Quick summary of the data\ncases.describe()\n\n\n\n\n\n\n\n\n\nLat\nLong\n1/22/20\n1/23/20\n1/24/20\n1/25/20\n1/26/20\n1/27/20\n1/28/20\n1/29/20\n...\n2/28/23\n3/1/23\n3/2/23\n3/3/23\n3/4/23\n3/5/23\n3/6/23\n3/7/23\n3/8/23\n3/9/23\n\n\n\n\ncount\n287.000000\n287.000000\n289.000000\n289.000000\n289.000000\n289.000000\n289.000000\n289.000000\n289.000000\n289.000000\n...\n2.890000e+02\n2.890000e+02\n2.890000e+02\n2.890000e+02\n2.890000e+02\n2.890000e+02\n2.890000e+02\n2.890000e+02\n2.890000e+02\n2.890000e+02\n\n\nmean\n19.718719\n22.182084\n1.927336\n2.273356\n3.266436\n4.972318\n7.335640\n10.134948\n19.307958\n21.346021\n...\n2.336755e+06\n2.337519e+06\n2.338173e+06\n2.338805e+06\n2.338992e+06\n2.339187e+06\n2.339387e+06\n2.339839e+06\n2.340460e+06\n2.341073e+06\n\n\nstd\n25.956609\n77.870931\n26.173664\n26.270191\n32.707271\n45.523871\n63.623197\n85.724481\n210.329649\n211.628535\n...\n8.506608e+06\n8.511285e+06\n8.514488e+06\n8.518031e+06\n8.518408e+06\n8.518645e+06\n8.519346e+06\n8.521641e+06\n8.524968e+06\n8.527765e+06\n\n\nmin\n-71.949900\n-178.116500\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n\n\n25%\n4.072192\n-32.823050\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n1.456700e+04\n1.456700e+04\n1.456700e+04\n1.456700e+04\n1.456700e+04\n1.456700e+04\n1.456700e+04\n1.456700e+04\n1.456700e+04\n1.456700e+04\n\n\n50%\n21.512583\n20.939400\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n1.032480e+05\n1.032480e+05\n1.032480e+05\n1.032480e+05\n1.032480e+05\n1.032480e+05\n1.032480e+05\n1.032480e+05\n1.032480e+05\n1.032480e+05\n\n\n75%\n40.401784\n89.224350\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n1.051998e+06\n1.052122e+06\n1.052247e+06\n1.052382e+06\n1.052519e+06\n1.052664e+06\n1.052664e+06\n1.052926e+06\n1.053068e+06\n1.053213e+06\n\n\nmax\n71.706900\n178.065000\n444.000000\n444.000000\n549.000000\n761.000000\n1058.000000\n1423.000000\n3554.000000\n3554.000000\n...\n1.034435e+08\n1.035339e+08\n1.035898e+08\n1.036487e+08\n1.036508e+08\n1.036470e+08\n1.036555e+08\n1.036909e+08\n1.037558e+08\n1.038027e+08\n\n\n\n\n8 rows × 1145 columns\n\n\n\n\n\nOf course, this value is meaningless for Lat and Long!\n\n\n# Data types of the various columns\ncases.dtypes\n\nProvince/State     object\nCountry/Region     object\nLat               float64\nLong              float64\n1/22/20             int64\n                   ...   \n3/5/23              int64\n3/6/23              int64\n3/7/23              int64\n3/8/23              int64\n3/9/23              int64\nLength: 1147, dtype: object\n\n\n\ncases.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 289 entries, 0 to 288\nColumns: 1147 entries, Province/State to 3/9/23\ndtypes: float64(2), int64(1143), object(2)\nmemory usage: 2.5+ MB\n\n\n\ncases.shape\n\n(289, 1147)",
    "crumbs": [
      "Python",
      "<em><b>Workshops</b></em>",
      "DataFrames with Pandas"
    ]
  },
  {
    "objectID": "python/ws_pandas.html#cases-per-country-by-date",
    "href": "python/ws_pandas.html#cases-per-country-by-date",
    "title": "DataFrames with pandas",
    "section": "Cases per country by date",
    "text": "Cases per country by date\nThe dataset is a time series: this means that we have the cumulative numbers up to each date.\n\n# Let's get rid of the latitude and longitude to simplify our data\nsimple = cases.drop(columns=['Lat', 'Long'])\nsimple\n\n\n\n\n\n\n\n\n\nProvince/State\nCountry/Region\n1/22/20\n1/23/20\n1/24/20\n1/25/20\n1/26/20\n1/27/20\n1/28/20\n1/29/20\n...\n2/28/23\n3/1/23\n3/2/23\n3/3/23\n3/4/23\n3/5/23\n3/6/23\n3/7/23\n3/8/23\n3/9/23\n\n\n\n\n0\nNaN\nAfghanistan\n0\n0\n0\n0\n0\n0\n0\n0\n...\n209322\n209340\n209358\n209362\n209369\n209390\n209406\n209436\n209451\n209451\n\n\n1\nNaN\nAlbania\n0\n0\n0\n0\n0\n0\n0\n0\n...\n334391\n334408\n334408\n334427\n334427\n334427\n334427\n334427\n334443\n334457\n\n\n2\nNaN\nAlgeria\n0\n0\n0\n0\n0\n0\n0\n0\n...\n271441\n271448\n271463\n271469\n271469\n271477\n271477\n271490\n271494\n271496\n\n\n3\nNaN\nAndorra\n0\n0\n0\n0\n0\n0\n0\n0\n...\n47866\n47875\n47875\n47875\n47875\n47875\n47875\n47875\n47890\n47890\n\n\n4\nNaN\nAngola\n0\n0\n0\n0\n0\n0\n0\n0\n...\n105255\n105277\n105277\n105277\n105277\n105277\n105277\n105277\n105288\n105288\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n284\nNaN\nWest Bank and Gaza\n0\n0\n0\n0\n0\n0\n0\n0\n...\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n\n\n285\nNaN\nWinter Olympics 2022\n0\n0\n0\n0\n0\n0\n0\n0\n...\n535\n535\n535\n535\n535\n535\n535\n535\n535\n535\n\n\n286\nNaN\nYemen\n0\n0\n0\n0\n0\n0\n0\n0\n...\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n\n\n287\nNaN\nZambia\n0\n0\n0\n0\n0\n0\n0\n0\n...\n343012\n343012\n343079\n343079\n343079\n343135\n343135\n343135\n343135\n343135\n\n\n288\nNaN\nZimbabwe\n0\n0\n0\n0\n0\n0\n0\n0\n...\n263921\n264127\n264127\n264127\n264127\n264127\n264127\n264127\n264276\n264276\n\n\n\n\n289 rows × 1145 columns\n\n\n\n\n Some countries (e.g. Australia) are split between several provinces or states so we will have to add the values of all their provinces/states to get their totals.\nHere is how to make the sum for all Australian states:\nLet’s first select all the data for Australia: we want all the rows for which the Country/Region column is equal to Australia.\nFirst, we want to select the Country/Region column. There are several ways to index in pandas.\nWhen indexing columns, one can use square brackets directly after the DataFrame to index:\n\nsimple['Country/Region']\n\n0               Afghanistan\n1                   Albania\n2                   Algeria\n3                   Andorra\n4                    Angola\n               ...         \n284      West Bank and Gaza\n285    Winter Olympics 2022\n286                   Yemen\n287                  Zambia\n288                Zimbabwe\nName: Country/Region, Length: 289, dtype: object\n\n\nHowever, it is more efficient to use the .loc or .iloc methods.\n\nUse .loc when using labels or booleans:\n\n\nsimple.loc[:, 'Country/Region']\n\n0               Afghanistan\n1                   Albania\n2                   Algeria\n3                   Andorra\n4                    Angola\n               ...         \n284      West Bank and Gaza\n285    Winter Olympics 2022\n286                   Yemen\n287                  Zambia\n288                Zimbabwe\nName: Country/Region, Length: 289, dtype: object\n\n\n\nUse .iloc when using indices:\n\n\nsimple.iloc[:, 1]\n\n0               Afghanistan\n1                   Albania\n2                   Algeria\n3                   Andorra\n4                    Angola\n               ...         \n284      West Bank and Gaza\n285    Winter Olympics 2022\n286                   Yemen\n287                  Zambia\n288                Zimbabwe\nName: Country/Region, Length: 289, dtype: object\n\n\n\nCountry/Region is the 2nd column, but indexing starts at 0 in Python.\n\nThen we need a conditional to filter the rows for which the value is equal to Australia:\n\nsimple.loc[:, 'Country/Region'] == 'Australia'\n\n0      False\n1      False\n2      False\n3      False\n4      False\n       ...  \n284    False\n285    False\n286    False\n287    False\n288    False\nName: Country/Region, Length: 289, dtype: bool\n\n\nFinally, we index, out of our entire data frame, the rows for which that condition returns True:\n\nsimple.loc[simple.loc[:, 'Country/Region'] == 'Australia']\n\n\n\n\n\n\n\n\n\nProvince/State\nCountry/Region\n1/22/20\n1/23/20\n1/24/20\n1/25/20\n1/26/20\n1/27/20\n1/28/20\n1/29/20\n...\n2/28/23\n3/1/23\n3/2/23\n3/3/23\n3/4/23\n3/5/23\n3/6/23\n3/7/23\n3/8/23\n3/9/23\n\n\n\n\n9\nAustralian Capital Territory\nAustralia\n0\n0\n0\n0\n0\n0\n0\n0\n...\n232018\n232018\n232619\n232619\n232619\n232619\n232619\n232619\n232619\n232974\n\n\n10\nNew South Wales\nAustralia\n0\n0\n0\n0\n3\n4\n4\n4\n...\n3900969\n3900969\n3908129\n3908129\n3908129\n3908129\n3908129\n3908129\n3908129\n3915992\n\n\n11\nNorthern Territory\nAustralia\n0\n0\n0\n0\n0\n0\n0\n0\n...\n104931\n104931\n105021\n105021\n105021\n105021\n105021\n105021\n105021\n105111\n\n\n12\nQueensland\nAustralia\n0\n0\n0\n0\n0\n0\n0\n1\n...\n1796633\n1796633\n1800236\n1800236\n1800236\n1800236\n1800236\n1800236\n1800236\n1800236\n\n\n13\nSouth Australia\nAustralia\n0\n0\n0\n0\n0\n0\n0\n0\n...\n880207\n880207\n881911\n881911\n881911\n881911\n881911\n881911\n881911\n883620\n\n\n14\nTasmania\nAustralia\n0\n0\n0\n0\n0\n0\n0\n0\n...\n286264\n286264\n286264\n286897\n286897\n286897\n286897\n286897\n286897\n287507\n\n\n15\nVictoria\nAustralia\n0\n0\n0\n0\n1\n1\n1\n1\n...\n2874262\n2874262\n2877260\n2877260\n2877260\n2877260\n2877260\n2877260\n2877260\n2880559\n\n\n16\nWestern Australia\nAustralia\n0\n0\n0\n0\n0\n0\n0\n0\n...\n1291077\n1291077\n1293461\n1293461\n1293461\n1293461\n1293461\n1293461\n1293461\n1293461\n\n\n\n\n8 rows × 1145 columns\n\n\n\n\n\nHere we use .loc to index based on a boolean array.\n\nWe can now make the sum for all of Australia for each day:\n\ntotal_australia = simple.loc[simple.loc[:, 'Country/Region'] == 'Australia'].sum(numeric_only=True)\ntotal_australia\n\n1/22/20           0\n1/23/20           0\n1/24/20           0\n1/25/20           0\n1/26/20           4\n             ...   \n3/5/23     11385534\n3/6/23     11385534\n3/7/23     11385534\n3/8/23     11385534\n3/9/23     11399460\nLength: 1143, dtype: int64\n\n\nWe can do this for all countries by grouping them:\n\ntotals = simple.groupby('Country/Region').sum(numeric_only=True)\ntotals\n\n\n\n\n\n\n\n\n\n1/22/20\n1/23/20\n1/24/20\n1/25/20\n1/26/20\n1/27/20\n1/28/20\n1/29/20\n1/30/20\n1/31/20\n...\n2/28/23\n3/1/23\n3/2/23\n3/3/23\n3/4/23\n3/5/23\n3/6/23\n3/7/23\n3/8/23\n3/9/23\n\n\nCountry/Region\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAfghanistan\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n209322\n209340\n209358\n209362\n209369\n209390\n209406\n209436\n209451\n209451\n\n\nAlbania\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n334391\n334408\n334408\n334427\n334427\n334427\n334427\n334427\n334443\n334457\n\n\nAlgeria\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n271441\n271448\n271463\n271469\n271469\n271477\n271477\n271490\n271494\n271496\n\n\nAndorra\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n47866\n47875\n47875\n47875\n47875\n47875\n47875\n47875\n47890\n47890\n\n\nAngola\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n105255\n105277\n105277\n105277\n105277\n105277\n105277\n105277\n105288\n105288\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\nWest Bank and Gaza\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n\n\nWinter Olympics 2022\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n535\n535\n535\n535\n535\n535\n535\n535\n535\n535\n\n\nYemen\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n\n\nZambia\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n343012\n343012\n343079\n343079\n343079\n343135\n343135\n343135\n343135\n343135\n\n\nZimbabwe\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n263921\n264127\n264127\n264127\n264127\n264127\n264127\n264127\n264276\n264276\n\n\n\n\n201 rows × 1143 columns\n\n\n\n\n Now, we can look at the totals for any date:\n\ntotals.loc[:, '6/12/21']\n\nCountry/Region\nAfghanistan              88740\nAlbania                 132449\nAlgeria                 133070\nAndorra                  13813\nAngola                   36600\n                         ...  \nWest Bank and Gaza      311018\nWinter Olympics 2022         0\nYemen                     6857\nZambia                  110332\nZimbabwe                 39852\nName: 6/12/21, Length: 201, dtype: int64\n\n\nTo make it easier to read, let’s order those numbers by decreasing order:\n\ntotals.loc[:, '6/12/21'].sort_values(ascending=False)\n\nCountry/Region\nUS                      33573694\nIndia                   29439989\nBrazil                  17385952\nFrance                   5799565\nTurkey                   5325435\n                          ...   \nPalau                          0\nWinter Olympics 2022           0\nKorea, North                   0\nSummer Olympics 2020           0\nTonga                          0\nName: 6/12/21, Length: 201, dtype: int64\n\n\nWe can also index the data for a particular country by indexing a row instead of a column:\n\ntotals.loc['Albania', :]\n\n1/22/20         0\n1/23/20         0\n1/24/20         0\n1/25/20         0\n1/26/20         0\n            ...  \n3/5/23     334427\n3/6/23     334427\n3/7/23     334427\n3/8/23     334443\n3/9/23     334457\nName: Albania, Length: 1143, dtype: int64\n\n\nWhen indexing rows, this syntax can be simplified to:\n\ntotals.loc['Albania']\n\n1/22/20         0\n1/23/20         0\n1/24/20         0\n1/25/20         0\n1/26/20         0\n            ...  \n3/5/23     334427\n3/6/23     334427\n3/7/23     334427\n3/8/23     334443\n3/9/23     334457\nName: Albania, Length: 1143, dtype: int64",
    "crumbs": [
      "Python",
      "<em><b>Workshops</b></em>",
      "DataFrames with Pandas"
    ]
  },
  {
    "objectID": "python/ws_pandas.html#global-totals",
    "href": "python/ws_pandas.html#global-totals",
    "title": "DataFrames with pandas",
    "section": "Global totals",
    "text": "Global totals\nNow, what if we want to have the world totals for each day? We calculate the columns totals (i.e. the sum across countries):\n\ntotals.sum()\n\n1/22/20          557\n1/23/20          657\n1/24/20          944\n1/25/20         1437\n1/26/20         2120\n             ...    \n3/5/23     676024901\n3/6/23     676082941\n3/7/23     676213378\n3/8/23     676392824\n3/9/23     676570149\nLength: 1143, dtype: int64\n\n\n\n\nYour turn:\n\nHow many confirmed cases were there in Venezuela by March 10, 2021?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFirst, we need to select the data for Venezuela:\n\nvenez = totals.loc['Venezuela']\nvenez\n\n1/22/20         0\n1/23/20         0\n1/24/20         0\n1/25/20         0\n1/26/20         0\n            ...  \n3/5/23     552051\n3/6/23     552125\n3/7/23     552157\n3/8/23     552157\n3/9/23     552162\nName: Venezuela, Length: 1143, dtype: int64\n\n\nThen, we need to select for the proper date:\n\nanswer = venez.loc['3/10/21']\nanswer\n\n143321\n\n\nWe could have done it at once by indexing the row and column:\n\ntotals.loc['Venezuela', '3/10/21']\n\n143321",
    "crumbs": [
      "Python",
      "<em><b>Workshops</b></em>",
      "DataFrames with Pandas"
    ]
  },
  {
    "objectID": "python/ws_pandas.html#pandas-documentation",
    "href": "python/ws_pandas.html#pandas-documentation",
    "title": "DataFrames with pandas",
    "section": "pandas documentation",
    "text": "pandas documentation\n\nA user Guide to pandas\nFull documentation",
    "crumbs": [
      "Python",
      "<em><b>Workshops</b></em>",
      "DataFrames with Pandas"
    ]
  },
  {
    "objectID": "python/wb_polars.html",
    "href": "python/wb_polars.html",
    "title": "DataFrames on steroids with Polars",
    "section": "",
    "text": "Polars is a modern open source and very fast DataFrame framework for Python, Rust, JS, R, and Ruby.\nIn this webinar, I will demo Polars for Python and show how much faster it is compared to pandas while remaining just as convenient.\n\nComing up in spring 2024.",
    "crumbs": [
      "Python",
      "<em><b>Webinars</b></em>"
    ]
  },
  {
    "objectID": "python/top_ws.html",
    "href": "python/top_ws.html",
    "title": "Python workshops",
    "section": "",
    "text": "Web scraping with Python\n\n\n\n\nDataFrames with Pandas",
    "crumbs": [
      "Python",
      "<em><b>Workshops</b></em>"
    ]
  },
  {
    "objectID": "python/top_hpc.html",
    "href": "python/top_hpc.html",
    "title": "Faster Python",
    "section": "",
    "text": "Courses on accelerated Python:\n\nAccelerated arrays with JAX\nHigh-performance DataFrames with Polars",
    "crumbs": [
      "Python",
      "<b><em>Faster Python</em></b>"
    ]
  },
  {
    "objectID": "python/intro_resources.html",
    "href": "python/intro_resources.html",
    "title": "Resources",
    "section": "",
    "text": "Python page\n\nThere are many books on Python, several of which can be accessed online for free, either directly, or through your university."
  },
  {
    "objectID": "python/intro_resources.html#alliance-wiki",
    "href": "python/intro_resources.html#alliance-wiki",
    "title": "Resources",
    "section": "",
    "text": "Python page\n\nThere are many books on Python, several of which can be accessed online for free, either directly, or through your university."
  },
  {
    "objectID": "python/intro_resources.html#books-by-oreilly",
    "href": "python/intro_resources.html#books-by-oreilly",
    "title": "Resources",
    "section": "Books by O’Reilly",
    "text": "Books by O’Reilly\n\nThink Python, 2nd Edition, by Allen B. Downey\nPython Pocket Reference, 5th Edition, by Mark Lutz\nIntroducing Python, by Bill Lubanovic\nPython in a Nutshell, 3rd Edition, by Alex Martelli, Anna Ravenscroft, and Steve Holden\nLearning Python, 5th Edition, by Mark Lutz\nPython Cookbook, 3rd Edition, by David Beazley and Brian K. Jones\nThe Hitchhiker’s Guide to Python, by Kenneth Reitz and Tanya Schlusser\nFluent Python, by Luciano Ramalho\nHigh Performance Python, by Micha Gorelick and Ian Ozsvald\nWeb Scraping with Python, by Ryan Mitchell\nPython Data Science Handbook, by Jake VanderPlas\nPython for Data Analysis, by Wes McKinney\nFoundations for Analytics with Python, by Clinton W. Brownley\nData Wrangling with Python, by Jacquiline Kazil and Katharine Jarmul\nData Visualization with Python and Javascript, by Kyran Dale\nNatural Language Processing with Python, by Steven Bird and Ewan Klein\nThoughtful Machine Learning with Python, by Matthew Kirk\nPython for Finance, by Yves Hilpisch"
  },
  {
    "objectID": "python/intro_resources.html#books-by-no-starch-press",
    "href": "python/intro_resources.html#books-by-no-starch-press",
    "title": "Resources",
    "section": "Books by No Starch Press",
    "text": "Books by No Starch Press\n\nAutomate the Boring Stuff with Python, by Al Sweigart\nPython Crash Course, by Eric Matthews\nPython Playground, by Mahesh Venkitachalam\nDoing Math with Python, by Amit Saha\nInvent Your Own Computer Games with Python, by Al Sweigart"
  },
  {
    "objectID": "python/intro_resources.html#other-books",
    "href": "python/intro_resources.html#other-books",
    "title": "Resources",
    "section": "Other books",
    "text": "Other books\n\nPython Machine Learning, by Sebastian Raschka\nPractical Programming: An Introduction to Computer Science Using Python 3, by Paul Gries, Jennifer Campbell, and Jason Montojo\nPython for Dummies, by Stef Maruch and Aahz Maruch\nPython Essential Reference, 4th Edition, by David Beazley\nHead First Python, by Paul Barry\nPython for Data Science for Dummies, by John Paul Mueller and Luca Massaron\nBeginning Programming with Python for Dummies, by John Paul Mueller\nPython for Everybody, by Charles Severance"
  },
  {
    "objectID": "python/intro_hpc.html",
    "href": "python/intro_hpc.html",
    "title": "Introduction to high performance research computing in Python",
    "section": "",
    "text": "When you launch a Jupyter session from a JupyterHub, you are running a Slurm job on a compute node. If you want to play for 8 hours in Jupyter, you are requesting an 8 hour job. Now, most of the time you spend on Jupyter is spent typing, running bits and pieces of code, or doing nothing at all. If you ask for GPUs, many CPUs, and lots of RAM, all of it will remain idle most of the time. This is a suboptimal use of resources.\nIn addition, if you ask for lots of resources for a long time, you will have to wait for a while before they get allocated to you.\nLastly, you will go through your allocations quickly.\nAll of this applies equally for interactive sessions launched from an SSH session with salloc."
  },
  {
    "objectID": "python/intro_hpc.html#interactive-sessions-for-high-performance-computing",
    "href": "python/intro_hpc.html#interactive-sessions-for-high-performance-computing",
    "title": "Introduction to high performance research computing in Python",
    "section": "",
    "text": "When you launch a Jupyter session from a JupyterHub, you are running a Slurm job on a compute node. If you want to play for 8 hours in Jupyter, you are requesting an 8 hour job. Now, most of the time you spend on Jupyter is spent typing, running bits and pieces of code, or doing nothing at all. If you ask for GPUs, many CPUs, and lots of RAM, all of it will remain idle most of the time. This is a suboptimal use of resources.\nIn addition, if you ask for lots of resources for a long time, you will have to wait for a while before they get allocated to you.\nLastly, you will go through your allocations quickly.\nAll of this applies equally for interactive sessions launched from an SSH session with salloc."
  },
  {
    "objectID": "python/intro_hpc.html#a-better-approach",
    "href": "python/intro_hpc.html#a-better-approach",
    "title": "Introduction to high performance research computing in Python",
    "section": "A better approach",
    "text": "A better approach\nA more efficient strategy is to develop and test your code with small samples, few iterations, etc. in an interactive job (from an SSH session in the cluster with salloc), on your own computer, or in Jupyter. Once you are confident that your code works, launch an sbatch job from an SSH session in the cluster to run the code as a script on all your data. This ensures that heavy duty resources that you requested are actually put to use to run your heavy calculations and not seating idle while you are thinking, typing, etc."
  },
  {
    "objectID": "python/intro_hpc.html#logging-on-to-the-cluster",
    "href": "python/intro_hpc.html#logging-on-to-the-cluster",
    "title": "Introduction to high performance research computing in Python",
    "section": "Logging on to the cluster",
    "text": "Logging on to the cluster\nOpen a terminal emulator:\nWindows users:  launch MobaXTerm.\nMacOS users:   launch Terminal.\nLinux users:     launch xterm or the terminal emulator of your choice.\nThen access the cluster through secure shell:\n$ ssh &lt;username&gt;@&lt;hostname&gt;    # enter password"
  },
  {
    "objectID": "python/intro_hpc.html#accessing-python",
    "href": "python/intro_hpc.html#accessing-python",
    "title": "Introduction to high performance research computing in Python",
    "section": "Accessing Python",
    "text": "Accessing Python\nThis is done with the Lmod tool through the module command. You can find the full documentation here and below are the subcommands you will need:\n# get help on the module command\n$ module help\n$ module --help\n$ module -h\n\n# list modules that are already loaded\n$ module list\n\n# see which modules are available for Python\n$ module spider python\n\n# see how to load Python 3.10.2\n$ module spider python/3.10.2\n\n# load Python 3.10.2 with the required gcc module first\n# (the order is important)\n$ module load gcc/7.3.0 python/3.10.2\n\n# you can see that we now have Python 3.10.2 loaded\n$ module list"
  },
  {
    "objectID": "python/intro_hpc.html#copying-files-to-the-cluster",
    "href": "python/intro_hpc.html#copying-files-to-the-cluster",
    "title": "Introduction to high performance research computing in Python",
    "section": "Copying files to the cluster",
    "text": "Copying files to the cluster\nWe will create a python_workshop directory in ~/scratch, then copy our Python script in it.\n$ mkdir ~/scratch/python_job\nOpen a new terminal window and from your local terminal (make sure that you are not on the remote terminal by looking at the bash prompt) run:\n$ scp /local/path/to/sort.jl &lt;username&gt;@&lt;hostname&gt;:scratch/python_job\n$ scp /local/path/to/psort.jl &lt;username&gt;@&lt;hostname&gt;:scratch/python_job\n\n# enter password"
  },
  {
    "objectID": "python/intro_hpc.html#job-scripts",
    "href": "python/intro_hpc.html#job-scripts",
    "title": "Introduction to high performance research computing in Python",
    "section": "Job scripts",
    "text": "Job scripts\nWe will not run an interactive session with Python on the cluster: we already have Python scripts ready to run. All we need to do is to write job scripts to submit to Slurm, the job scheduler used by the Alliance clusters.\nWe will create 2 scripts: one to run Python on one core and one on as many cores as are available.\n\n\nYour turn:\n\nHow many processors are there on our training cluster?\n\nSave your job scripts in the files ~/scratch/python_job/job_python1c.sh and job_python2c.sh for one and two cores respectively.\nHere is what our single core Slurm script looks like:\n#!/bin/bash\n#SBATCH --job-name=python1c         # job name\n#SBATCH --time=00:01:00             # max walltime 1 min\n#SBATCH --cpus-per-task=1           # number of cores\n#SBATCH --mem=1000                  # max memory (default unit is megabytes)\n#SBATCH --output=python1c%j.out     # file name for the output\n#SBATCH --error=python1c%j.err      # file name for errors\n# %j gets replaced with the job number\n\npython sort.py\n\n\nYour turn:\n\nWrite the script for 2 cores.\n\nNow, we can submit our jobs to the cluster:\n$ cd ~/scratch/python_job\n$ sbatch job_python1c.sh\n$ sbatch job_python2c.sh\nAnd we can check their status with:\n$ sq      # This is an Alliance alias for `squeue -u $USER $@`\n\nPD stands for pending\nR stands for running\n\nmodule avail python # several versions available module load python/3.8.10 virtualenv –no-download astro # install Python tools in your $HOME/astro source astro/bin/activate pip install –no-index –upgrade pip pip install –no-index numpy jupyter pandas # all these will go into your $HOME/astro avail_wheels –name “tensorflow_gpu” –all_versions # check out the available packages pip install –no-index tensorflow_gpu==2.2.0 # if needed, install a specific version … deactivate Once created, you would use it with:\nsource ~/astro/bin/activate python … deactivate"
  },
  {
    "objectID": "python/intro_hpc.html#run-python-on-our-training-cluster",
    "href": "python/intro_hpc.html#run-python-on-our-training-cluster",
    "title": "Introduction to high performance research computing in Python",
    "section": "Run Python on our training cluster",
    "text": "Run Python on our training cluster\nThis is not the method I recommend for this workshop, but I am adding it as this is something you might want to use if you need to run heavy computations.\nFirst, you need to load the Python module.\nSee which Python modules are available:\nmodule spider python\nSee how to install one module:\n\nExample:\n\nmodule spider python/3.10.2\nLoad the required dependencies (first) and the module:\nmodule load StdEnv/2020 python/3.10.2\nYou can check that the modules were loaded with:\nmodule list\nAnd verify the Python version with:\npython --version"
  },
  {
    "objectID": "python/intro_control_flow.html",
    "href": "python/intro_control_flow.html",
    "title": "Control flow",
    "section": "",
    "text": "Control flow statements alter the linear execution of code, allowing for one or another section of code to be executed, or for one section of code to be executed multiple times.",
    "crumbs": [
      "Python",
      "<b><em>Getting started</em></b>",
      "Control flow"
    ]
  },
  {
    "objectID": "python/intro_control_flow.html#conditionals",
    "href": "python/intro_control_flow.html#conditionals",
    "title": "Control flow",
    "section": "Conditionals",
    "text": "Conditionals\nConditionals dictate the flow of information based on predicates (statements that return True or False).\n\nExample predicates:\n\n4 &lt; 3\n2 == 4\n2 != 4\n2 in range(5)\n2 not in range(5)\n3 &lt;= 4 and 4 &gt; 5\n3 &lt;= 4 and 4 &gt; 5 and 3 != 2\n3 &lt;= 4 or 4 &gt; 5\n\nIf statements\nIn the simplest case, we have:\nif &lt;predicate&gt;:\n    &lt;some action&gt;\nThis translates to:\n\nIf &lt;predicate&gt; evaluates to True, the body of the if statement gets evaluated (&lt;some action&gt; is run),\nIf &lt;predicate&gt; evaluates to False, nothing happens.\n\n\nExamples:\n\n\nx = 3\nif x &gt;= 0:\n    print(x, 'is positive')\n\n3 is positive\n\n\n\nx = -3\nif x &gt;= 0:\n    print(x, 'is positive')\n\n\nNothing gets returned since the predicate returned False.\n\n\n\nIf else statements\nLet’s add an else statement so that our code also returns something when the predicate evaluates to False:\nif &lt;predicate&gt;:\n    &lt;some action&gt;\nelse:\n    &lt;some other action&gt;\n\nExample:\n\n\nx = -3\nif x &gt;= 0:\n    print(x, 'is positive')\nelse:\n    print(x, 'is negative')\n\n-3 is negative\n\n\n\n\nIf elif else\nWe can make this even more complex with:\nif &lt;predicate1&gt;:\n    &lt;some action&gt;\nelif &lt;predicate2&gt;:\n    &lt;some other action&gt;    \nelse:\n    &lt;yet some other action&gt;\n\nExample:\n\n\nx = -3\nif x &gt; 0:\n    print(x, 'is positive')\nelif x &lt; 0:\n    print(x, 'is negative')\nelse:\n    print(x, 'is zero')\n\n-3 is negative",
    "crumbs": [
      "Python",
      "<b><em>Getting started</em></b>",
      "Control flow"
    ]
  },
  {
    "objectID": "python/intro_control_flow.html#loops",
    "href": "python/intro_control_flow.html#loops",
    "title": "Control flow",
    "section": "Loops",
    "text": "Loops\n\nFor loops\nFor loops run a set of instructions for each element of an iterable.\nAn iterable is any Python object cable of returning the items it contains one at a time.\n\nExamples of iterables:\n\nrange(5)\n'a string is an iterable'\n[2, 'word', 4.0]\nFor loops follow the syntax:\nfor &lt;iterable&gt;:\n    &lt;some action&gt;\n\nExample:\n\n\nfor i in range(5):\n    print(i)\n\n0\n1\n2\n3\n4\n\n\n\n\nYour turn:\n\nRemember that the indentation matters in Python.\nWhat do you think that this will print?\nfor i in range(5):\n    print(i)\nprint(i)\n\nStrings are iterables too, so this works:\n\nfor i in 'a string is an iterable':\n    print(i)\n\na\n \ns\nt\nr\ni\nn\ng\n \ni\ns\n \na\nn\n \ni\nt\ne\nr\na\nb\nl\ne\n\n\nTo iterate over multiple iterables at the same time, a convenient option is to use the function zip which creates an iterator of tuples:\n\nfor i, j in zip([1, 2, 3, 4], [3, 4, 5, 6]):\n    print(i + j)\n\n4\n6\n8\n10\n\n\n\n\nWhile loops\nWhile loops run as long as a predicate remains true. They follow the syntax:\nwhile &lt;predicate&gt;:\n    &lt;some action&gt;\n\nExample:\n\n\ni = 0\nwhile i &lt;= 10:\n    print(i)\n    i += 1\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10",
    "crumbs": [
      "Python",
      "<b><em>Getting started</em></b>",
      "Control flow"
    ]
  },
  {
    "objectID": "python/intro_basics.html",
    "href": "python/intro_basics.html",
    "title": "Python: the basics",
    "section": "",
    "text": "Python is a hugely popular interpreted language with a simple, easily readable syntax and a large collection of external packages.\nIt was created by Dutch programmer Guido van Rossum in the 80s, with a launch in 1989. Since the start of the PYPL PopularitY of Programming Language index (based on the number of tutorial searches on Google) in 2004, its popularity has grown steadily, reaching the number one position in 2018 where it still is (as of January 2023).",
    "crumbs": [
      "Python",
      "<b><em>Getting started</em></b>",
      "Python: the basics"
    ]
  },
  {
    "objectID": "python/intro_basics.html#about-python",
    "href": "python/intro_basics.html#about-python",
    "title": "Python: the basics",
    "section": "",
    "text": "Python is a hugely popular interpreted language with a simple, easily readable syntax and a large collection of external packages.\nIt was created by Dutch programmer Guido van Rossum in the 80s, with a launch in 1989. Since the start of the PYPL PopularitY of Programming Language index (based on the number of tutorial searches on Google) in 2004, its popularity has grown steadily, reaching the number one position in 2018 where it still is (as of January 2023).",
    "crumbs": [
      "Python",
      "<b><em>Getting started</em></b>",
      "Python: the basics"
    ]
  },
  {
    "objectID": "python/intro_basics.html#the-standard-library",
    "href": "python/intro_basics.html#the-standard-library",
    "title": "Python: the basics",
    "section": "The standard library",
    "text": "The standard library\nPython comes with a standard library. As soon as you launch the program, you can access part of the standard library such as the built-in functions and built-in constants:\n\nExample:\n\n\ntype(3)    # type is a built-in function\n\nint\n\n\nMost of the standard library however is held in several thematic modules. Each module contains additional functions, constants, and facilities. Before you can use them, you need to load them into your session.\n\nExample: the os module\nThe os module contains the function getcwd returning the path of the current working directory as a string.\nThis function cannot be used directly:\n\ngetcwd()\n\nNameError: name 'getcwd' is not defined\n\n\nIn order to access it, you have several options:\n\nLoad the module, then access the function as a method of the module:\n\n\nimport os\nos.getcwd()\n\n'/home/marie/parvus/prog/mint/python'\n\n\n\nYou can create an alias for the module:\n\nimport os as o\no.getcwd()\n\n'/home/marie/parvus/prog/mint/python'\n\n\nWhile it is a little silly for a module with such a short name, it is very convenient with modules of longer names.\n\n\nImport the function directly:\n\n\nfrom os import getcwd\ngetcwd()\n\n'/home/marie/parvus/prog/mint/python'",
    "crumbs": [
      "Python",
      "<b><em>Getting started</em></b>",
      "Python: the basics"
    ]
  },
  {
    "objectID": "python/intro_basics.html#help-and-documentation",
    "href": "python/intro_basics.html#help-and-documentation",
    "title": "Python: the basics",
    "section": "Help and documentation",
    "text": "Help and documentation\n\nModule\nYou can get help on a module thanks to the help function, but only after you have loaded that module into your session:\nimport os\nhelp(os)\nHelp on module os:\n\nNAME\n    os - OS routines for NT or Posix depending on what system we're on.\n\nMODULE REFERENCE\n    https://docs.python.org/3.10/library/os.html\n\n    The following documentation is automatically generated from the Python\n    source files.  It may be incomplete, incorrect or include features that\n    are considered implementation detail and may vary between Python\n    implementations.  When in doubt, consult the module reference at the\n    location listed above.\n    \n... \n\n\nFunctions\nYou can also access the internal Python documentation on a function with help:\n\nhelp(max)\n\nHelp on built-in function max in module builtins:\n\nmax(...)\n    max(iterable, *[, default=obj, key=func]) -&gt; value\n    max(arg1, arg2, *args, *[, key=func]) -&gt; value\n    \n    With a single iterable argument, return its biggest item. The\n    default keyword-only argument specifies an object to return if\n    the provided iterable is empty.\n    With two or more arguments, return the largest argument.\n\n\n\n\nIn Jupyter, you can also use ?max or max?.\n\nAlternatively, you can print the __doc__ method of the function:\n\nprint(max.__doc__)\n\nmax(iterable, *[, default=obj, key=func]) -&gt; value\nmax(arg1, arg2, *args, *[, key=func]) -&gt; value\n\nWith a single iterable argument, return its biggest item. The\ndefault keyword-only argument specifies an object to return if\nthe provided iterable is empty.\nWith two or more arguments, return the largest argument.\n\n\n\n\nMethods of object types\nSome methods belong to specific objects types (e.g. lists have a method called append).\nIn those cases, help(&lt;method&gt;) won’t work.\n\nExample:\n\n\nhelp(append)\n\nNameError: name 'append' is not defined\n\n\nWhat you need to run instead is help(&lt;object&gt;.&lt;method&gt;).\n\nExample:\n\n\nhelp(list.append)\n\nHelp on method_descriptor:\n\nappend(self, object, /)\n    Append object to the end of the list.",
    "crumbs": [
      "Python",
      "<b><em>Getting started</em></b>",
      "Python: the basics"
    ]
  },
  {
    "objectID": "python/intro_basics.html#syntax",
    "href": "python/intro_basics.html#syntax",
    "title": "Python: the basics",
    "section": "Syntax",
    "text": "Syntax\nCommands are usually written one per line, but you can write multiple commands on the same line with the separator ;:\n\na = 2.0; a\n\n2.0\n\n\nTabs or 4 spaces (the number of spaces can be customized in many IDEs) have a syntactic meaning in Python and are not just for human readability:\n\n# Incorrect code\nfor i in [1, 2]:\nprint(i)\n\nIndentationError: expected an indented block after 'for' statement on line 2 (1993980772.py, line 3)\n\n\n\n# Correct code\nfor i in [1, 2]:\n    print(i)\n\n1\n2\n\n\n\nIDEs and good text editors can indent the code automatically.\n\nComments (snippets of text for human consumption and ignored by Python) are marked by #:\n\n# This is a full-line comment\n\na         # This is an inline comment\n\n2.0\n\n\nPEP 8—the style guide for Python code—suggests a maximum of 72 characters per line for comments. Try to keep comments to the point and spread them over multiple lines if they are too long.",
    "crumbs": [
      "Python",
      "<b><em>Getting started</em></b>",
      "Python: the basics"
    ]
  },
  {
    "objectID": "python/intro_basics.html#creating-and-deleting-objects",
    "href": "python/intro_basics.html#creating-and-deleting-objects",
    "title": "Python: the basics",
    "section": "Creating and deleting objects",
    "text": "Creating and deleting objects\n\nAssignment\nThe assignment statement = binds a name (a reference) and a value to create an object (variable, data structure, function, or method).\n\nFor instance, we can bind the name a and the value 1 to create the variable a:\n\n\na = 1\n\nYou can define multiple objects at once (here variables), assigning them the same value:\n\na = b = 10\nprint(a, b)\n\n10 10\n\n\n… or different values:\n\na, b = 1, 2\nprint(a, b)\n\n1 2\n\n\n\n\nYour turn:\n\n\na = 1\nb = a\na = 2\n\nWhat do you think the value of b is now?\n\n\n\nChoosing names\nWhile I am using a and b a lot in this workshop (since the code has no other purpose than to demo the language itself), in your scripts you should use meaningful names (e.g. survival, age, year, species, temperature). It will make reading the code this much easier.\nMake sure not to use the names of built-in functions or built-in constants.\n\n\nDeleting objects\nDeletion of the names can be done with the del statement:\n\nvar = 3\nvar\n\n3\n\n\n\ndel var\nvar\n\nNameError: name 'var' is not defined\n\n\nThe Python garbage collector automatically removes values with no names bound to them from memory.",
    "crumbs": [
      "Python",
      "<b><em>Getting started</em></b>",
      "Python: the basics"
    ]
  },
  {
    "objectID": "python/intro_basics.html#data-types",
    "href": "python/intro_basics.html#data-types",
    "title": "Python: the basics",
    "section": "Data types",
    "text": "Data types\nPython comes with multiple built-in types.\n\nExamples (non exhaustive):\n\n\ntype(1), type(1.0), type('1'), type(3+2j), type(True)\n\n(int, float, str, complex, bool)\n\n\n\nint = integer\nfloat = floating point number\ncomplex = complex number\nstr = string\nbool = Boolean\n\nPython is dynamically-typed: names do not have types, but they are bound to typed values and they can be bound over time to values of different types.\n\nvar = 2.3\ntype1 = type(var)\nvar = \"A string.\"\ntype2 = type(var)\n\ntype1, type2\n\n(float, str)\n\n\nYou can also convert the type of some values:\n\n'4', type('4'), int('4'), type(int('4'))\n\n('4', str, 4, int)\n\n\n\nfloat(3)\n\n3.0\n\n\n\nstr(3.4)\n\n'3.4'\n\n\n\nbool(0)\n\nFalse\n\n\n\nbool(1)\n\nTrue\n\n\n\nint(True)\n\n1\n\n\n\nfloat(False)\n\n0.0\n\n\nOf course, not all conversions are possible:\n\nint('red')\n\nValueError: invalid literal for int() with base 10: 'red'\n\n\nYou might be surprised by some of the conversions:\n\nint(3.9)\n\n3\n\n\n\nbool(3.4)\n\nTrue",
    "crumbs": [
      "Python",
      "<b><em>Getting started</em></b>",
      "Python: the basics"
    ]
  },
  {
    "objectID": "python/intro_basics.html#quotes",
    "href": "python/intro_basics.html#quotes",
    "title": "Python: the basics",
    "section": "Quotes",
    "text": "Quotes\nPairs of single and double quotes are used to create strings. PEP 8 does not recommend one style over the other. It does suggest however that once you have chosen a style, you stick to it to make scripts consistent.\n\n\"This is a string.\"\n\n'This is a string.'\n\n\n\ntype(\"This is a string.\")\n\nstr\n\n\n\n'This is also a string.'\n\n'This is also a string.'\n\n\n\ntype('This is also a string.')\n\nstr\n\n\nApostrophes and textual quotes interfere with Python quotes. In these cases, use the opposite style to avoid any problem:\n\n# This doesn't work\n'This string isn't easy'\n\nSyntaxError: unterminated string literal (detected at line 2) (368933316.py, line 2)\n\n\n\n# This is good\n\"This string isn't easy\"\n\n\"This string isn't easy\"\n\n\n\n# This doesn't work\n\"He said: \"this is a problem.\"\"\n\nSyntaxError: invalid syntax (466663664.py, line 2)\n\n\n\n# This is good\n'He said: \"this is a problem.\"'\n\n'He said: \"this is a problem.\"'\n\n\nSometimes, neither option works and you have to escape some of the quotes with \\:\n\n# This doesn't work\n\"He said: \"this string isn't easy\"\"\n\nSyntaxError: unterminated string literal (detected at line 2) (392662328.py, line 2)\n\n\n\n# This doesn't work either\n'He said: \"this string isn't easy\"'\n\nSyntaxError: unterminated string literal (detected at line 2) (521375870.py, line 2)\n\n\n\n# You can use double quotes and escape double quotes in the string\n\"He said: \\\"this string isn't easy\\\"\"\n\n'He said: \"this string isn\\'t easy\"'\n\n\n\n# Or you can use single quotes and escape single quotes in the string\n'He said: \"this string isn\\'t easy\"'\n\n'He said: \"this string isn\\'t easy\"'",
    "crumbs": [
      "Python",
      "<b><em>Getting started</em></b>",
      "Python: the basics"
    ]
  },
  {
    "objectID": "python/intro_basics.html#basic-operations",
    "href": "python/intro_basics.html#basic-operations",
    "title": "Python: the basics",
    "section": "Basic operations",
    "text": "Basic operations\n\n3 + 2\n\n5\n\n\n\n3.0 - 2.0\n\n1.0\n\n\n\n10 / 2\n\n5.0\n\n\n\nNotice how the result can be of a different type\n\nVariables can be used in operations:\n\na = 3\na + 2\n\n5\n\n\na = a + 10 can be replaced by the more elegant:\n\na += 10\na\n\n13",
    "crumbs": [
      "Python",
      "<b><em>Getting started</em></b>",
      "Python: the basics"
    ]
  },
  {
    "objectID": "python/hpc_polars.html",
    "href": "python/hpc_polars.html",
    "title": "Polars DataFrames",
    "section": "",
    "text": "Polars is a modern open source and very fast DataFrame framework for Python, Rust, JS, R, and Ruby.\nIn Python—the topic of this course—Polars is much faster than pandas while remaining just as convenient.\n\nComing up in fall 2024.",
    "crumbs": [
      "Python",
      "<b><em>Faster Python</em></b>",
      "Ultrafast Polars DataFrames"
    ]
  },
  {
    "objectID": "newsletter.html",
    "href": "newsletter.html",
    "title": "Training events mailing list",
    "section": "",
    "text": "If you want to get informed about upcoming training events, please subscribe to our mailing list: \n(We will only email you about training events.)"
  },
  {
    "objectID": "julia/wb_makie.html",
    "href": "julia/wb_makie.html",
    "title": "Makie",
    "section": "",
    "text": "There are several popular data visualization libraries for the Julia programming language (e.g. Plots, Gadfly, VegaLite, Makie). They vary in their precompilation time, time to first plot, layout capabilities, ability to handle 3D data, ease of use, and syntax style. In this landscape, Makie focuses on high performance, fancy layouts, and extensibility.\nMakie comes with multiple backends. In this webinar, we will cover:\n\nGLMakie (ideal for interactive 2D and 3D plotting)\nWGLMakie (an equivalent that runs within browsers)\nCairoMakie (best for high-quality vector graphics)\n\nWe will also see how to run Makie in the Alliance clusters.\nSlides (Click and wait: this reveal.js presentation is heavy and takes some time to load.)",
    "crumbs": [
      "Julia",
      "<b><em>Webinars</em></b>",
      "Data visualization with Makie"
    ]
  },
  {
    "objectID": "julia/wb_makie.html#plotting-in-julia",
    "href": "julia/wb_makie.html#plotting-in-julia",
    "title": "Makie",
    "section": "Plotting in Julia",
    "text": "Plotting in Julia\nThere are many options to create plots in Julia. Some of the most popular ones are:\n\nPlots.jl: high-level API for working with different back-ends (GR, Pyplot, Plotly…),\nPyPlot.jl: Julia interface to Matplotlib’s matplotlib.pyplot,\nPlotlyJS.jl: Julia interface to plotly.js,\nPlotlyLight.jl: the fastest plotting option in Julia by far, but limited features,\nGadfly.jl: following the grammar of graphics popularized by Hadley Wickham in R,\nVegaLite.jl: grammar of interactive graphics,\nPGFPlotsX.jl: Julia interface to the PGFPlots LaTeX package,\nUnicodePlots.jl: plots in the terminal 🙂,\nMakie.jl: powerful plotting ecosystem: animation, 3D, GPU optimization.\n\nThis webinar focuses on Makie.jl.",
    "crumbs": [
      "Julia",
      "<b><em>Webinars</em></b>",
      "Data visualization with Makie"
    ]
  },
  {
    "objectID": "julia/wb_makie.html#the-makie-ecosystem",
    "href": "julia/wb_makie.html#the-makie-ecosystem",
    "title": "Makie",
    "section": "The Makie ecosystem",
    "text": "The Makie ecosystem\nMakie consists of a core package (Makie), with the plots functionalities.\nIn addition to this, a backend is needed to render plots into images or vector graphics. Three backends are available:\n\nCairoMakie: vector graphics or high-quality 2D plots. Creates, but does not display plots (you need an IDE that does or you can use ElectronDisplay.jl),\nGLMakie: based on OpenGL; 3D rendering and interactivity in GLFW window (no vector graphics),\nWGLMakie: web version of GLMakie (plots rendered in a browser instead of a window).",
    "crumbs": [
      "Julia",
      "<b><em>Webinars</em></b>",
      "Data visualization with Makie"
    ]
  },
  {
    "objectID": "julia/wb_makie.html#resources",
    "href": "julia/wb_makie.html#resources",
    "title": "Makie",
    "section": "Resources",
    "text": "Resources\nHere are some links and resources useful to get started with the Makie ecosystem:\n\nthe official Makie documentation,\nJulia Data Science book, chapter 5,\nthe project Beautiful Makie contains many great plot examples,\ncheatsheets:\n\nfor 2D plotting:\n\n\nFrom: Storopoli, Huijzer and Alonso (2021). Julia Data Science. https://juliadatascience.io. ISBN: 97984898\n\nfor 3D plotting:\n\n\nFrom: Storopoli, Huijzer and Alonso (2021). Julia Data Science. https://juliadatascience.io. ISBN: 97984898",
    "crumbs": [
      "Julia",
      "<b><em>Webinars</em></b>",
      "Data visualization with Makie"
    ]
  },
  {
    "objectID": "julia/wb_makie.html#troubleshooting",
    "href": "julia/wb_makie.html#troubleshooting",
    "title": "Makie",
    "section": "Troubleshooting",
    "text": "Troubleshooting\nCairoMakie and WGLMakie should install without issues. Installing GLMakie however can be challenging. This page may lead you towards a solution.",
    "crumbs": [
      "Julia",
      "<b><em>Webinars</em></b>",
      "Data visualization with Makie"
    ]
  },
  {
    "objectID": "julia/wb_makie.html#extensions",
    "href": "julia/wb_makie.html#extensions",
    "title": "Makie",
    "section": "Extensions",
    "text": "Extensions\nA number of extensions have been built on top of Makie:\n\nGeoMakie.jl add geographical plotting utilities to Makie,\nAlgebraOfGraphics.jl turns plotting into a simple algebra of building blocks,\nGraphMakie.jl to create network graphs.",
    "crumbs": [
      "Julia",
      "<b><em>Webinars</em></b>",
      "Data visualization with Makie"
    ]
  },
  {
    "objectID": "julia/wb_makie.html#fundamental-functioning",
    "href": "julia/wb_makie.html#fundamental-functioning",
    "title": "Makie",
    "section": "Fundamental functioning",
    "text": "Fundamental functioning\n\nFigure\nLoad the package (here, we are using CairoMakie):\n\nusing CairoMakie                        # no need to import Makie itself\n\nCreate a Figure (container object):\n\nfig = Figure()\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie ~/.julia/packages/Makie/QGPt0/src/scenes.jl:220\n\n\n\n\n\n\n\n\n\n\ntypeof(fig)\n\nFigure\n\n\nYou can customize a Figure:\n\nfig2 = Figure(backgroundcolor=:grey22, resolution=(300, 300))\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie ~/.julia/packages/Makie/QGPt0/src/scenes.jl:220\n\n\n\n\n\n\n\n\n\nMakie uses the Colors.jl package as a dependency. You can find a list of all named colours here.\nTo use CSS specification (e.g. hex), you need to install Colors explicitly and use its color parsing capabilities:\n\nusing Colors\nfig3 = Figure(backgroundcolor=colorant\"#adc2eb\")\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie ~/.julia/packages/Makie/QGPt0/src/scenes.jl:220\n\n\n\n\n\n\n\n\n\n\n\nAxis\nThen, you can create an Axis:\n\nax = Axis(Figure()[1, 1])\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie ~/.julia/packages/Makie/QGPt0/src/scenes.jl:220\n\n\nAxis with 0 plots:\n\n\n\ntypeof(ax)\n\nAxis\n\n\n\nAxis(fig3[1, 1])  # fig3[1, 1] sets the subplot layout: fig[row, col]\nfig3\n\n\n\n\n\n\n\n\n\nAxis(fig[2, 3])  # This is what happens if we change the layout\nfig\n\n\n\n\n\n\n\n\n\nAxis(fig3[2, 3])  # We can add another axis on fig3\nfig3\n\n\n\n\n\n\n\n\nAxis are customizable:\n\nfig4 = Figure()\nAxis(fig4[1, 1],\n     xlabel=\"x label\",\n     ylabel=\"y label\",\n     title=\"Title of the plot\")\nfig4\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie ~/.julia/packages/Makie/QGPt0/src/scenes.jl:220\n\n\n\n\n\n\n\n\n\n\n\nPlot\nFinally, you can add a plot:\n\nfig = Figure()\nax = Axis(fig[1, 1])\nx = LinRange(-10, 10, 20)\ny = x\nscatter!(ax, x, y)  # Functions with ! transform their arguments\nfig\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie ~/.julia/packages/Makie/QGPt0/src/scenes.jl:220\n\n\n\n\n\n\n\n\n\nOf course, there are many plotting functions, e.g. scatterlines!:\n\nfig = Figure()\nax = Axis(fig[1, 1])\nx = LinRange(-10, 10, 20)\ny = x\nscatterlines!(ax, x, y)  # Functions with ! transform their arguments\nfig\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie ~/.julia/packages/Makie/QGPt0/src/scenes.jl:220\n\n\n\n\n\n\n\n\n\nWe can also use lines!:\n\nfig = Figure()\nax = Axis(fig[1, 1])\nx = LinRange(-10, 10, 20)\ny = sin.(x)  # The . means that the function is broadcast to each element of x\nlines!(ax, x, y)\nfig\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie ~/.julia/packages/Makie/QGPt0/src/scenes.jl:220\n\n\n\n\n\n\n\n\n\nLet’s add points to get a smoother line:\n\nfig = Figure()\nax = Axis(fig[1, 1])\nx = LinRange(-10, 10, 1000)\ny = sin.(x)  # The . means that the function is broadcast to each element of x\nlines!(ax, x, y)\nfig\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie ~/.julia/packages/Makie/QGPt0/src/scenes.jl:220\n\n\n\n\n\n\n\n\n\nNow, you don’t have to create the Figure, Axis, and plot one at a time. You can create them at the same time with, for instance lines:\n\nx = LinRange(-10, 10, 1000)\ny = sin.(x)\nlines(x, y)  # Note the use of lines instead of lines!\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie ~/.julia/packages/Makie/QGPt0/src/scenes.jl:220\n\n\n\n\n\n\n\n\n\nOr even more simply:\n\nx = LinRange(-10, 10, 1000)\nlines(x, sin)\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie ~/.julia/packages/Makie/QGPt0/src/scenes.jl:220\n\n\n\n\n\n\n\n\n\nThis is a lot simpler, but it is important to understand the concepts of the Figure and Axis objects as you will need it to customize them:\n\nx = LinRange(-10, 10, 1000)\ny = cos.(x)\nlines(x, y;\n      figure=(; backgroundcolor=:green),\n      axis=(; title=\"Cosinus function\", xlabel=\"x label\", ylabel=\"y label\"))\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie ~/.julia/packages/Makie/QGPt0/src/scenes.jl:220\n\n\n\n\n\n\n\n\n\nWhen you create the Figure, Axis, and plot at the same time, you create a FigureAxisPlot object:\n\nx = LinRange(-10, 10, 1000)\ny = cos.(x)\nobj = lines(x, y;\n            figure=(; backgroundcolor=:green),\n            axis=(; title=\"Cosinus function\",\n                  xlabel=\"x label\",\n                  ylabel=\"y label\"));\ntypeof(obj)\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie ~/.julia/packages/Makie/QGPt0/src/scenes.jl:220\n\n\nMakie.FigureAxisPlot\n\n\n\nNote the ; in the figure and axis value. This is because these are one-element NamedTuples.\n\nThe mutating functions (with !) can be used to add plots to an existing figure, but first, you need to decompose the FigureAxisPlot object:\n\nfig, ax, plot = lines(x, sin)\nlines!(ax, x, cos)  # Remember that we are transforming the Axis object\nfig                 # Now we can plot the transformed Figure\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie ~/.julia/packages/Makie/QGPt0/src/scenes.jl:220\n\n\n\n\n\n\n\n\n\nOr we can add several plots on different Axis in the same Figure:\n\nfig, ax1, plot = lines(x, sin)\nax2 = Axis(fig[1, 2])\nlines!(ax2, x, cos)\nfig\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie ~/.julia/packages/Makie/QGPt0/src/scenes.jl:220",
    "crumbs": [
      "Julia",
      "<b><em>Webinars</em></b>",
      "Data visualization with Makie"
    ]
  },
  {
    "objectID": "julia/wb_makie.html#examples",
    "href": "julia/wb_makie.html#examples",
    "title": "Makie",
    "section": "Examples",
    "text": "Examples\n\n2D\n\nusing CairoMakie\nusing StatsBase, LinearAlgebra\nusing Interpolations, OnlineStats\nusing Distributions\nCairoMakie.activate!(type = \"png\")\n\nfunction eq_hist(matrix; nbins = 256 * 256)\n    h_eq = fit(Histogram, vec(matrix), nbins = nbins)\n    h_eq = normalize(h_eq, mode = :density)\n    cdf = cumsum(h_eq.weights)\n    cdf = cdf / cdf[end]\n    edg = h_eq.edges[1]\n    interp_linear = LinearInterpolation(edg, [cdf..., cdf[end]])\n    out = reshape(interp_linear(vec(matrix)), size(matrix))\n    return out\nend\n\nfunction getcounts!(h, fn; n = 100)\n    for _ in 1:n\n        vals = eigvals(fn())\n        x0 = real.(vals)\n        y0 = imag.(vals)\n        fit!(h, zip(x0,y0))\n    end\nend\n\nm(;a=10rand()-5, b=10rand()-5) = [0 0 0 a; -1 -1 1 0; b 0 0 0; -1 -1 -1 -1]\n\nh = HeatMap(range(-3.5,3.5,length=1200), range(-3.5,3.5, length=1200))\ngetcounts!(h, m; n=2_000_000)\n\nwith_theme(theme_black()) do\n    fig = Figure(figure_padding=0,resolution=(600,600))\n    ax = Axis(fig[1,1]; aspect = DataAspect())\n    heatmap!(ax,-3.5..3.5, -3.5..3.5, eq_hist(h.counts); colormap = :bone_1)\n    hidedecorations!(ax)\n    hidespines!(ax)\n    fig\nend\n\nPrecompiling OnlineStats\n  ✓ OnlineStatsBase\n  ✓ OnlineStats\n  2 dependencies successfully precompiled in 3 seconds. 48 already precompiled.\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie ~/.julia/packages/Makie/QGPt0/src/scenes.jl:220\n\n\n\n\n\n\n\n3D\nusing GLMakie, Random\nGLMakie.activate!()\n\nRandom.seed!(13)\nx = -6:0.5:6\ny = -6:0.5:6\nz = 6exp.( -(x.^2 .+ y' .^ 2)./4)\n\nbox = Rect3(Point3f(-0.5), Vec3f(1))\nn = 100\ng(x) = x^(1/10)\nalphas = [g(x) for x in range(0,1,length=n)]\ncmap_alpha = resample_cmap(:linear_worb_100_25_c53_n256, n, alpha = alphas)\n\nwith_theme(theme_dark()) do\n    fig, ax, = meshscatter(x, y, z;\n                           marker=box,\n                           markersize = 0.5,\n                           color = vec(z),\n                           colormap = cmap_alpha,\n                           colorrange = (0,6),\n                           axis = (;\n                                   type = Axis3,\n                                   aspect = :data,\n                                   azimuth = 7.3,\n                                   elevation = 0.189,\n            perspectiveness = 0.5),\n        figure = (;\n            resolution =(1200,800)))\n    meshscatter!(ax, x .+ 7, y, z./2;\n        markersize = 0.25,\n        color = vec(z./2),\n        colormap = cmap_alpha,\n        colorrange = (0, 6),\n        ambient = Vec3f(0.85, 0.85, 0.85),\n        backlight = 1.5f0)\n    xlims!(-5.5,10)\n    ylims!(-5.5,5.5)\n    hidedecorations!(ax; grid = false)\n    hidespines!(ax)\n    fig\nend\n\n\nFor more examples, have a look at Beautiful Makie.",
    "crumbs": [
      "Julia",
      "<b><em>Webinars</em></b>",
      "Data visualization with Makie"
    ]
  },
  {
    "objectID": "julia/wb_makie.html#compiling-sysimages",
    "href": "julia/wb_makie.html#compiling-sysimages",
    "title": "Makie",
    "section": "Compiling sysimages",
    "text": "Compiling sysimages\nWhile Makie is extremely powerful, its compilation time and its time to first plot are extremely long. For this reason, it might save you a lot of time to create a sysimage (a file containing information from a Julia session such as loaded packages, global variables, compiled code, etc.) with PackageCompiler.jl.\n\nThe upcoming Julia 1.9 will do this automatically.",
    "crumbs": [
      "Julia",
      "<b><em>Webinars</em></b>",
      "Data visualization with Makie"
    ]
  },
  {
    "objectID": "julia/wb_makie.html#using-the-alliance-clusters",
    "href": "julia/wb_makie.html#using-the-alliance-clusters",
    "title": "Makie",
    "section": "Using the Alliance clusters",
    "text": "Using the Alliance clusters\n\nCairoMakie\nCairoMakie will run without problem on the Alliance clusters. It is not designed for interactivity, so saving to file is what makes the most sense.\n\nExample:\n\nsave(\"graph.png\", fig)\n\nRemember however that CairoMakie is 2D only (for now).\n\n\n\nGLMakie\nGLMakie relies on GLFW to create windows with OpenGL. GLFW doesn’t support creating contexts without an associated window. The dependency GLFW.jl will thus not install in the clusters—even with X11 forwarding—unless you use VDI nodes, VNC, or Virtual GL.\n\n\nWGLMakie\nYou can setup a server with JSServe.jl as per the documentation. However, this method is intended for the creation of interactive widgets, e.g. for a website. While this is really cool, it isn’t optimized for performance. There might also be a way to create an SSH tunnel to your local browser, although there is no documentation on this.\nBest probably is to save to file.\n\nConclusion about the Makie ecosystem on production clusters:\n\n2D plots: use CairoMakie and save to file,\n3D plots: use WGLMakie and save to file.",
    "crumbs": [
      "Julia",
      "<b><em>Webinars</em></b>",
      "Data visualization with Makie"
    ]
  },
  {
    "objectID": "julia/wb_firstdab.html",
    "href": "julia/wb_firstdab.html",
    "title": "First dab at Julia",
    "section": "",
    "text": "Julia is fast: just-in-time (JIT) compilation and multiple dispatch bring efficiency to interactivity. People often say that using Julia feels like running R or python with a speed almost comparable to that of C.\nBut Julia also comes with parallel computing and multi-threading capabilities.\nIn this webinar, after a quickly presentation of some of the key features of Julia’s beautifully concise syntax, I will dive into using Julia for HPC.",
    "crumbs": [
      "Julia",
      "<b><em>Webinars</em></b>",
      "First dab at Julia"
    ]
  },
  {
    "objectID": "julia/intro_types.html",
    "href": "julia/intro_types.html",
    "title": "Types",
    "section": "",
    "text": "Type safety (catching errors of inadequate type) performed at compilation time.\n\nExamples: C, C++, Java, Fortran, Haskell.\n\n\n\n\nType safety performed at runtime.\n\nExamples: Python, JavaScript, PHP, Ruby, Lisp.\n\n\n\n\n\nJulia type system is dynamic (types are unknown until runtime), but types can be declared, optionally bringing the advantages of static type systems.\nThis gives users the freedom to choose between an easy and convenient language, or a clearer, faster, and more robust one (or a combination of the two).",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Types"
    ]
  },
  {
    "objectID": "julia/intro_types.html#types-systems",
    "href": "julia/intro_types.html#types-systems",
    "title": "Types",
    "section": "",
    "text": "Type safety (catching errors of inadequate type) performed at compilation time.\n\nExamples: C, C++, Java, Fortran, Haskell.\n\n\n\n\nType safety performed at runtime.\n\nExamples: Python, JavaScript, PHP, Ruby, Lisp.\n\n\n\n\n\nJulia type system is dynamic (types are unknown until runtime), but types can be declared, optionally bringing the advantages of static type systems.\nThis gives users the freedom to choose between an easy and convenient language, or a clearer, faster, and more robust one (or a combination of the two).",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Types"
    ]
  },
  {
    "objectID": "julia/intro_types.html#julia-types-a-hierarchical-tree",
    "href": "julia/intro_types.html#julia-types-a-hierarchical-tree",
    "title": "Types",
    "section": "Julia types: a hierarchical tree",
    "text": "Julia types: a hierarchical tree\nAt the bottom:  concrete types.\nAbove:     abstract types (concepts for collections of concrete types).\nAt the top:     the Any type, encompassing all types.\n\n\nFrom O’Reilly\n\nOne common type missing in this diagram is the boolean type.\nIt is a subtype of the integer type, as can be tested with the subtype operator &lt;:\n\nBool &lt;: Integer\n\ntrue\n\n\nIt can also be made obvious by the following:\n\nfalse == 0\n\ntrue\n\n\n\ntrue == 1\n\ntrue\n\n\n\na = true;\nb = false;\n3a + 2b\n\n3",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Types"
    ]
  },
  {
    "objectID": "julia/intro_types.html#optional-type-declaration",
    "href": "julia/intro_types.html#optional-type-declaration",
    "title": "Types",
    "section": "Optional type declaration",
    "text": "Optional type declaration\nDone with ::\n&lt;value&gt;::&lt;type&gt;\n\nExample:\n\n\n2::Int\n\n2",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Types"
    ]
  },
  {
    "objectID": "julia/intro_types.html#illustration-of-type-safety",
    "href": "julia/intro_types.html#illustration-of-type-safety",
    "title": "Types",
    "section": "Illustration of type safety",
    "text": "Illustration of type safety\nThis works:\n\n2::Int\n\n2\n\n\nThis doesn’t work:\n\n2.0::Int\n\nLoadError: TypeError: in typeassert, expected Int64, got a value of type Float64\n\n\nType declaration is not yet supported on global variables; this is used in local contexts such as inside a function.\n\nExample:\n\n\nfunction floatsum(a, b)\n    (a + b)::Float64\nend\n\nfloatsum (generic function with 1 method)\n\n\nThis works:\n\nfloatsum(2.3, 1.0)\n\n3.3\n\n\nThis doesn’t work:\n\nfloatsum(2, 4)\n\nLoadError: TypeError: in typeassert, expected Float64, got a value of type Int64",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Types"
    ]
  },
  {
    "objectID": "julia/intro_types.html#information-and-conversion",
    "href": "julia/intro_types.html#information-and-conversion",
    "title": "Types",
    "section": "Information and conversion",
    "text": "Information and conversion\nThe typeof function gives the type of an object:\n\ntypeof(2)\n\nInt64\n\n\n\ntypeof(2.0)\n\nFloat64\n\n\n\ntypeof(\"Hello, World!\")\n\nString\n\n\n\ntypeof(true)\n\nBool\n\n\n\ntypeof((2, 4, 1.0, \"test\"))\n\nTuple{Int64, Int64, Float64, String}\n\n\nConversion between types is possible in some cases:\n\nInt(2.0)\n\n2\n\n\n\ntypeof(Int(2.0))\n\nInt64\n\n\n\nChar(2.0)\n\n'\\x02': ASCII/Unicode U+0002 (category Cc: Other, control)\n\n\n\ntypeof(Char(2.0))\n\nChar",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Types"
    ]
  },
  {
    "objectID": "julia/intro_types.html#stylistic-convention",
    "href": "julia/intro_types.html#stylistic-convention",
    "title": "Types",
    "section": "Stylistic convention",
    "text": "Stylistic convention\nThe names of types start with a capital letter and camel case is used in multiple-word names.",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Types"
    ]
  },
  {
    "objectID": "julia/intro_resources.html",
    "href": "julia/intro_resources.html",
    "title": "Resources",
    "section": "",
    "text": "Official Julia website\nOfficial Julia manual\nAlliance wiki Julia page\nOnline training material\nThe Julia YouTube channel\nThe Julia Wikibook\nA blog aggregator for Julia",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Resources"
    ]
  },
  {
    "objectID": "julia/intro_resources.html#documentation",
    "href": "julia/intro_resources.html#documentation",
    "title": "Resources",
    "section": "",
    "text": "Official Julia website\nOfficial Julia manual\nAlliance wiki Julia page\nOnline training material\nThe Julia YouTube channel\nThe Julia Wikibook\nA blog aggregator for Julia",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Resources"
    ]
  },
  {
    "objectID": "julia/intro_resources.html#getting-help",
    "href": "julia/intro_resources.html#getting-help",
    "title": "Resources",
    "section": "Getting help",
    "text": "Getting help\n\nDiscourse forum\n[julia] tag on Stack Overflow\nSlack team (you need to agree to the community code of conduct at slackinvite.julialang.org to receive an invitation)\n#julialang hashtag on Twitter\nSubreddit\nGitter channel\n#julia IRC channel on Freenode",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Resources"
    ]
  },
  {
    "objectID": "julia/intro_packages.html",
    "href": "julia/intro_packages.html",
    "title": "Packages",
    "section": "",
    "text": "Julia comes with a collection of packages. In Linux, they are in /usr/share/julia/stdlib/vx.x.\nHere is the list:\nBase64\nCRC32c\nDates\nDelimitedFiles\nDistributed\nFileWatching\nFuture\nInteractiveUtils\nLibdl\nLibGit2\nLinearAlgebra\nLogging\nMarkdown\nMmap\nPkg\nPrintf\nProfile\nRandom\nREPL\nSerialization\nSHA\nSharedArrays\nSockets\nSparseArrays\nStatistics\nSuiteSparse\nTest\nUnicode\nUUIDs",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Packages"
    ]
  },
  {
    "objectID": "julia/intro_packages.html#standard-library",
    "href": "julia/intro_packages.html#standard-library",
    "title": "Packages",
    "section": "",
    "text": "Julia comes with a collection of packages. In Linux, they are in /usr/share/julia/stdlib/vx.x.\nHere is the list:\nBase64\nCRC32c\nDates\nDelimitedFiles\nDistributed\nFileWatching\nFuture\nInteractiveUtils\nLibdl\nLibGit2\nLinearAlgebra\nLogging\nMarkdown\nMmap\nPkg\nPrintf\nProfile\nRandom\nREPL\nSerialization\nSHA\nSharedArrays\nSockets\nSparseArrays\nStatistics\nSuiteSparse\nTest\nUnicode\nUUIDs",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Packages"
    ]
  },
  {
    "objectID": "julia/intro_packages.html#installing-additional-packages",
    "href": "julia/intro_packages.html#installing-additional-packages",
    "title": "Packages",
    "section": "Installing additional packages",
    "text": "Installing additional packages\nYou can install additional packages.\nThese go to your personal library in ~/.julia (this is also where your REPL history is saved).\nAll registered packages are on GitHub and can easily be searched here.\nThe GitHub star system allows you to easily judge the popularity of a package and to see whether it is under current development.\nIn addition to these, there are unregistered packages and you can build your own.\n\n\nYour turn:\n\nTry to find a list of popular plotting packages.\n\nYou can manage your personal library easily in package mode with the commands:\n(env) pkg&gt; add &lt;package&gt;   # install &lt;package&gt;\n(env) pkg&gt; rm &lt;package&gt;    # uninstall &lt;package&gt;\n(env) pkg&gt; up &lt;package&gt;    # upgrade &lt;package&gt;\n(env) pkg&gt; st              # st or status: list installed packages\n(env) pkg&gt; up              # up or upgrade: upgrade all packages\n\nReplace &lt;package&gt; by the name of the package (e.g. Plots ).\n\nYou can install, uninstall, or update several packages at once by listing them with a space:\n(env) pkg&gt; add &lt;package1&gt; &lt;package2&gt; &lt;package3&gt;\nAn alternative to this convenience mode is to load the package manager (package Pkg, part of stdlib) and use it as you would any other package:\nusing Pkg\n\nPkg.add(\"&lt;package&gt;\")        # install &lt;package&gt;\nPkg.rm(\"&lt;package&gt;\")         # uninstall &lt;package&gt;\nPkg.status(\"&lt;package&gt;\")     # status of &lt;package&gt;\nPkg.update(\"&lt;package&gt;\")     # update &lt;package&gt;\nPkg.update()                # status of all installed packages\nPkg.status()                # update all packages\n\nThe short forms up and st do not work in this context.\n\nTo install, uninstall, or update several packages at once in this context, you need to create an array:\nPkg.add([\"&lt;package1&gt;\", \"&lt;package2&gt;\", \"&lt;package3&gt;\"])\n\n\nYour turn:\n\nCheck your list of packages; install the packages Plots, GR, Distributions, StatsPlots, and UnicodePlot; then check that list again.\n\n\n\nYour turn:\n\nNow go explore your ~/.julia. If you don’t find it, make sure that your file explorer allows you to see hidden files.",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Packages"
    ]
  },
  {
    "objectID": "julia/intro_packages.html#loading-packages",
    "href": "julia/intro_packages.html#loading-packages",
    "title": "Packages",
    "section": "Loading packages",
    "text": "Loading packages\nWhether a package from the standard library or one you installed, before you can use a package you need to load it. This has to be done at each new Julia session so the code to load packages should be part of your scripts.\nThis is done with the using command (e.g. using Plots).",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Packages"
    ]
  },
  {
    "objectID": "julia/intro_macros.html",
    "href": "julia/intro_macros.html",
    "title": "Macros",
    "section": "",
    "text": "Julia code is itself data and can be manipulated by the language while it is running.",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Macros"
    ]
  },
  {
    "objectID": "julia/intro_macros.html#metaprogramming",
    "href": "julia/intro_macros.html#metaprogramming",
    "title": "Macros",
    "section": "Metaprogramming",
    "text": "Metaprogramming\n\nLarge influence from Lisp.\nSince Julia is entirely written in Julia, it is particularly well suited for metaprogramming.",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Macros"
    ]
  },
  {
    "objectID": "julia/intro_macros.html#parsing-and-evaluating",
    "href": "julia/intro_macros.html#parsing-and-evaluating",
    "title": "Macros",
    "section": "Parsing and evaluating",
    "text": "Parsing and evaluating\nLet’s start with something simple:\n\n2 + 3\n\n5\n\n\nHow is this run internally?\nThe string \"2 + 3\" gets parsed into an expression:\n\nMeta.parse(\"2 + 3\")\n\n:(2 + 3)\n\n\nThen that expression gets evaluated:\n\neval(Meta.parse(\"2 + 3\"))\n\n5",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Macros"
    ]
  },
  {
    "objectID": "julia/intro_macros.html#macros",
    "href": "julia/intro_macros.html#macros",
    "title": "Macros",
    "section": "Macros",
    "text": "Macros\nThey resemble functions and just like functions, they accept as input a tuple of arguments.\nBUT macros return an expression which is compiled directly rather than requiring a runtime eval call.\nSo they execute before the rest of the code is run.\nMacro’s names are preceded by @ (e.g. @time).\nJulia comes with many macros and you can create your own with:\nmacro &lt;name&gt;()\n    &lt;body&gt;\nend",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Macros"
    ]
  },
  {
    "objectID": "julia/intro_macros.html#stylistic-conventions",
    "href": "julia/intro_macros.html#stylistic-conventions",
    "title": "Macros",
    "section": "Stylistic conventions",
    "text": "Stylistic conventions\nAs with functions, Julia suggests to use lower case, without underscores, as macro names.",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Macros"
    ]
  },
  {
    "objectID": "julia/intro_functions.html",
    "href": "julia/intro_functions.html",
    "title": "Functions",
    "section": "",
    "text": "Functions are objects containing a set of instructions.\nWhen you pass a tuple of argument(s) (possibly an empty tuple) to them, you get one or more values as output.",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Functions"
    ]
  },
  {
    "objectID": "julia/intro_functions.html#operators",
    "href": "julia/intro_functions.html#operators",
    "title": "Functions",
    "section": "Operators",
    "text": "Operators\nOperators are functions and can be written in a way that shows the tuple of arguments more explicitly.\n\nFor instance, you can use the addition operator (+) in 2 ways:\n\n\n3 + 2\n+(3, 2)\n\n5\n\n\nThe multiplication operator can be omitted when this does not create any ambiguity:\n\na = 3;\n2a\n\n6\n\n\nJulia has “assignment by operation” operators:\n\na = 2;\na += 7    # this is the same as a = a + 7\n\n9\n\n\nThere is a left division operator:\n\n2\\8 == 8/2\n\ntrue\n\n\nJulia supports fraction operations:\n\n4//8\n\n1//2\n\n\n\n1//2 + 3//4\n\n5//4",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Functions"
    ]
  },
  {
    "objectID": "julia/intro_functions.html#function-definition",
    "href": "julia/intro_functions.html#function-definition",
    "title": "Functions",
    "section": "Function definition",
    "text": "Function definition\nThere are 2 ways to define a new function:\n\nLong form\nfunction &lt;name&gt;(&lt;arguments&gt;)\n    &lt;body&gt;\nend\n\nExample:\n\n\nfunction hello1()\n    println(\"Hello\")\nend\n\nhello1 (generic function with 1 method)\n\n\n\n\nAssignment form\n&lt;name&gt;(&lt;arguments&gt;) = &lt;body&gt;\n\nExample:\n\n\nhello1() = println(\"Hello\")\n\nhello1 (generic function with 1 method)\n\n\nThe function hello1 defined with this terse syntax is exactly the same as the one we defined above.\n\n\nStylistic convention\nJulia suggests to use lower case without underscores as function names when the name is readable enough.",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Functions"
    ]
  },
  {
    "objectID": "julia/intro_functions.html#calling-functions",
    "href": "julia/intro_functions.html#calling-functions",
    "title": "Functions",
    "section": "Calling functions",
    "text": "Calling functions\nSince you pass a tuple to a function when you run it, you call a function by appending parentheses to its name:\n\nhello1()\n\nHello\n\n\n\nHere, our function does not take any argument, so the tuple is empty.",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Functions"
    ]
  },
  {
    "objectID": "julia/intro_functions.html#arguments",
    "href": "julia/intro_functions.html#arguments",
    "title": "Functions",
    "section": "Arguments",
    "text": "Arguments\n\nNo argument\nOur function hello1 does not accept any argument. If we pass an argument, we get an error message:\nhello1(\"Bob\")\nLoadError: MethodError: no method matching hello1(::String)\n\n\nOne argument\nTo define a function which accepts an argument, we need to add a placeholder for it in the function definition.\n\nSo let’s try this:\n\n\nfunction hello2(name)\n    println(\"Hello name\")\nend\n\nhello2 (generic function with 1 method)\n\n\n\nhello2(\"Bob\")\n\nHello name\n\n\nMmm … not quite … this function works but does not give the result we wanted.\nHere, we need to use string interpolation:\n\nfunction hello3(name)\n    println(\"Hello $name\")\nend\n\nhello3 (generic function with 1 method)\n\n\n$name in the body of the function points to name in the tuple of argument.\nWhen we run the function, $name is replaced by the value we used in lieu of name in the function definition:\n\nhello3(\"Bob\")\n\nHello Bob\n\n\nHere is the corresponding assignment form for hello3:\n\nhello3(name) = println(\"Hello $name\")\n\nhello3 (generic function with 1 method)\n\n\n\nNote that this dollar sign is only required with strings. Here is an example with integers:\n\n\nfunction addTwo(a)\n    a + 2\nend\n\naddTwo (generic function with 1 method)\n\n\nAnd the corresponding assignment form:\n\naddTwo(a) = a + 2\n\naddTwo (generic function with 1 method)\n\n\n\naddTwo(4)\n\n6\n\n\n\n\nMultiple arguments\nNow, let’s write a function which accepts 2 arguments. For this, we put 2 placeholders in the tuple passed to the function in the function definition:\n\nfunction hello4(name1, name2)\n    println(\"Hello $name1 and $name2\")\nend\n\nhello4 (generic function with 1 method)\n\n\nThis means that this function expects a tuple of 2 values:\n\nhello4(\"Bob\", \"Pete\")\n\nHello Bob and Pete\n\n\n\n\nYour turn:\n\nSee what happens when you pass no argument, a single argument, or three arguments to this function.\n\n\n\nDefault arguments\nYou can set a default value for some or all arguments. In this case, the function will run with or without a value passed for those arguments. If no value is given, the default is used. If a value is given, it will replace the default.\n\nExample:\n\n\nfunction hello5(name=\"\")\n    println(\"Hello $name\")\nend\n\nhello5 (generic function with 2 methods)\n\n\n\nhello5()\n\nHello \n\n\n\nhello5(\"Bob\")\n\nHello Bob\n\n\n\nAnother example:\n\n\nfunction addSomethingOrTwo(a, b=2)\n    a + b\nend\n\naddSomethingOrTwo (generic function with 2 methods)\n\n\n\naddSomethingOrTwo(3)\n\n5\n\n\n\naddSomethingOrTwo(3, 4)\n\n7",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Functions"
    ]
  },
  {
    "objectID": "julia/intro_functions.html#returning-the-result",
    "href": "julia/intro_functions.html#returning-the-result",
    "title": "Functions",
    "section": "Returning the result",
    "text": "Returning the result\nIn Julia, functions return the value(s) of the last expression automatically. If you want to return something else instead, you need to use the return statement. This causes the function to exit early.\n\nLook at these 5 functions:\n\nfunction test1(x, y)\n    x + y\nend\n\nfunction test2(x, y)\n    return x + y\nend\n\nfunction test3(x, y)\n    x * y\n    x + y\nend\n\nfunction test4(x, y)\n    return x * y\n    x + y\nend\n\nfunction test5(x, y)\n    return x * y\n    return x + y\nend\n\nfunction test6(x, y)\n    x * y, x + y\nend\n\n\nYour turn:\n\nWithout running the code, try to guess the outputs of:\n\ntest1(1, 2)\ntest2(1, 2)\ntest3(1, 2)\ntest4(1, 2)\ntest5(1, 2)\ntest6(1, 2)\n\n\nYour turn:\n\nNow, run the code and draw some conclusions on the behaviour of the return statement.",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Functions"
    ]
  },
  {
    "objectID": "julia/intro_functions.html#anonymous-functions",
    "href": "julia/intro_functions.html#anonymous-functions",
    "title": "Functions",
    "section": "Anonymous functions",
    "text": "Anonymous functions\nAnonymous functions are functions which aren’t given a name:\nfunction (&lt;arguments&gt;)\n    &lt;body&gt;\nend\nIn compact form:\n&lt;arguments&gt; -&gt; &lt;body&gt;\n\nExample:\n\n\nfunction (name)\n    println(\"Hello $name\")\nend\n\n#13 (generic function with 1 method)\n\n\nCompact form:\n\nname -&gt; println(\"Hello $name\")\n\n#15 (generic function with 1 method)\n\n\n\nWhen would you want to use anonymous functions?\nThis is very useful for functional programming (when you apply a function—for instance map—to other functions to apply them in a vectorized manner which avoids repetitions).\n\nExample:\n\n\nmap(name -&gt; println(\"Hello $name\"), [\"Bob\", \"Lucie\", \"Sophie\"]);\n\nHello Bob\nHello Lucie\nHello Sophie",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Functions"
    ]
  },
  {
    "objectID": "julia/intro_functions.html#pipes",
    "href": "julia/intro_functions.html#pipes",
    "title": "Functions",
    "section": "Pipes",
    "text": "Pipes\n|&gt; is the pipe in Julia. It redirects the output of the expression on the left as the input of the expression on the right.\n\nThe following 2 expressions are equivalent:\n\nprintln(\"Hello\")\n\"Hello\" |&gt; println\n\nHere is another example:\n\n\nsqrt(2) == 2 |&gt; sqrt\n\ntrue",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Functions"
    ]
  },
  {
    "objectID": "julia/intro_functions.html#function-composition",
    "href": "julia/intro_functions.html#function-composition",
    "title": "Functions",
    "section": "Function composition",
    "text": "Function composition\nYou can pass a function inside another function:\n&lt;function2&gt;(&lt;function1&gt;(&lt;arguments&gt;))\n&lt;arguments&gt; will be passed to &lt;function1&gt; and the result will then be passed to &lt;function2&gt;.\nAn equivalent syntax is to use the composition operator ∘ (in the REPL, type \\circ then press tab):\n(&lt;function2&gt; ∘ &lt;function1&gt;)(&lt;arguments&gt;)\n\nExample:\n\n\n# sum is our first function\nsum(1:3)\n\n6\n\n\n\n# sqrt is the second function\nsqrt(sum(1:3))\n\n2.449489742783178\n\n\n\n# This is equivalent\n(sqrt ∘ sum)(1:3)\n\n2.449489742783178\n\n\n\n\nYour turn:\n\nWrite three other equivalent expressions using the pipe.\n\n\nAnother example:\n\n\nexp(+(-3, 1))\n\n(exp ∘ +)(-3, 1)\n\n0.1353352832366127\n\n\n\n\nYour turn:\n\nTry to write the same expression in another 2 different ways.",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Functions"
    ]
  },
  {
    "objectID": "julia/intro_functions.html#mutating-functions",
    "href": "julia/intro_functions.html#mutating-functions",
    "title": "Functions",
    "section": "Mutating functions",
    "text": "Mutating functions\nFunctions usually do not modify their argument(s):\n\na = [-2, 3, -5]\n\n3-element Vector{Int64}:\n -2\n  3\n -5\n\n\n\nsort(a)\n\n3-element Vector{Int64}:\n -5\n -2\n  3\n\n\n\na\n\n3-element Vector{Int64}:\n -2\n  3\n -5\n\n\nJulia has a set of functions which modify their argument(s). By convention, their names end with !\n\nThe function sort has a mutating equivalent sort!:\n\n\nsort!(a);\na\n\n3-element Vector{Int64}:\n -5\n -2\n  3\n\n\n\nIf you write functions which modify their arguments, make sure to follow this convention too.",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Functions"
    ]
  },
  {
    "objectID": "julia/intro_functions.html#broadcasting",
    "href": "julia/intro_functions.html#broadcasting",
    "title": "Functions",
    "section": "Broadcasting",
    "text": "Broadcasting\nTo apply a function to each element of a collection rather than to the collection as a whole, Julia uses broadcasting.\n\nLet’s create a collection (here a tuple):\n\n\na = (2, 3)\n\n(2, 3)\n\n\n\nIf we pass a to the string function, that function applies to the whole collection:\n\n\nstring(a)\n\n\"(2, 3)\"\n\n\n\nIn contrast, we can broadcast the function string to all elements of a:\n\n\nbroadcast(string, a)\n\n(\"2\", \"3\")\n\n\n\nAn alternative syntax is to add a period after the function name:\n\n\nstring.(a)\n\n(\"2\", \"3\")\n\n\n\nHere is another example:\n\na = [-3, 2, -5]\nabs(a)\nERROR: MethodError: no method matching abs(::Array{Int64,1})\nThis doesn’t work because the function abs only applies to single elements.\nBy broadcasting abs, you apply it to each element of a:\n\nbroadcast(abs, a)\n\n(2, 3)\n\n\nThe dot notation is equivalent:\n\nabs.(a)\n\n(2, 3)\n\n\nIt can also be applied to the pipe, to unary and binary operators, etc.\n\nExample:\n\n\na .|&gt; abs\n\n(2, 3)\n\n\n\n\nYour turn:\n\nTry to understand the difference between the following 2 expressions:\n\n\nabs.(a) == a .|&gt; abs\nabs.(a) .== a .|&gt; abs\n\n(true, true)",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Functions"
    ]
  },
  {
    "objectID": "julia/intro_functions.html#multiple-dispatch",
    "href": "julia/intro_functions.html#multiple-dispatch",
    "title": "Functions",
    "section": "Multiple dispatch",
    "text": "Multiple dispatch\nIn some programming languages, functions can be polymorphic (multiple versions exist under the same function name). The process of selecting which version to use is called dispatch.\nThere are multiple types of dispatch depending on the language:\n\nDynamic dispatch: the process of selecting one version of a function at run time.\nSingle dispatch: the choice of version is based on a single object.\n\n\nThis is typical of object-oriented languages such as Python, C++, Java, Smalltalk, etc.\n\n\nMultiple dispatch: the choice of version is based on the combination of all operands and their types.\n\n\nThis the case of Lisp and Julia. In Julia, these versions are called methods.",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Functions"
    ]
  },
  {
    "objectID": "julia/intro_functions.html#methods",
    "href": "julia/intro_functions.html#methods",
    "title": "Functions",
    "section": "Methods",
    "text": "Methods\nRunning methods(+) let’s you see that the function + has 206 methods!\nMethods can be added to existing functions.\n\n\nYour turn:\n\nRun the following and try to understand the outputs:\nabssum(x::Int64, y::Int64) = abs(x + y)\nabssum(x::Float64, y::Float64) = abs(x + y)\n\nabssum(2, 4)\nabssum(2.0, 4.0)\nabssum(2, 4.0)\nWhat could you do if you wanted the last expression to work?",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Functions"
    ]
  },
  {
    "objectID": "julia/intro_collections.html",
    "href": "julia/intro_collections.html",
    "title": "Collections",
    "section": "",
    "text": "Values can be stored in collections. This workshop introduces tuples, dictionaries, sets, and arrays in Julia.",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Collections"
    ]
  },
  {
    "objectID": "julia/intro_collections.html#tuples",
    "href": "julia/intro_collections.html#tuples",
    "title": "Collections",
    "section": "Tuples",
    "text": "Tuples\nTuples are immutable, indexable, and possibly heterogeneous collections of elements. The order of elements matters.\n\n# Possibly heterogeneous (values can be of different types)\ntypeof((2, 'a', 1.0, \"test\"))\n\nTuple{Int64, Char, Float64, String}\n\n\n\n# Indexable (note that indexing in Julia starts with 1)\nx = (2, 'a', 1.0, \"test\");\nx[3]\n\n1.0\n\n\n\n# Immutable (they cannot be modified)\n# So this returns an error\nx[3] = 8\n\nLoadError: MethodError: no method matching setindex!(::Tuple{Int64, Char, Float64, String}, ::Int64, ::Int64)\n\n\n\nNamed tuples\nTuples can have named components:\n\ntypeof((a=2, b='a', c=1.0, d=\"test\"))\n\n@NamedTuple{a::Int64, b::Char, c::Float64, d::String}\n\n\n\nx = (a=2, b='a', c=1.0, d=\"test\");\nx.c\n\n1.0",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Collections"
    ]
  },
  {
    "objectID": "julia/intro_collections.html#dictionaries",
    "href": "julia/intro_collections.html#dictionaries",
    "title": "Collections",
    "section": "Dictionaries",
    "text": "Dictionaries\nJulia also has dictionaries: associative collections of key/value pairs:\n\nx = Dict(\"Name\"=&gt;\"Roger\", \"Age\"=&gt;52, \"Index\"=&gt;0.3)\n\nDict{String, Any} with 3 entries:\n  \"Index\" =&gt; 0.3\n  \"Age\"   =&gt; 52\n  \"Name\"  =&gt; \"Roger\"\n\n\n\"Name\", \"Age\", and \"Index\" are the keys; \"Roger\", 52, and 0.3 are the values.\nThe =&gt; operator is the same as the Pair function:\n\np = \"foo\" =&gt; 7\n\n\"foo\" =&gt; 7\n\n\n\nq = Pair(\"bar\", 8)\n\n\"bar\" =&gt; 8\n\n\nDictionaries can be heterogeneous (as in this example) and the order doesn’t matter. They are also indexable:\n\nx[\"Name\"]\n\n\"Roger\"\n\n\nAnd mutable (they can be modified):\n\nx[\"Name\"] = \"Alex\";\nx\n\nDict{String, Any} with 3 entries:\n  \"Index\" =&gt; 0.3\n  \"Age\"   =&gt; 52\n  \"Name\"  =&gt; \"Alex\"",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Collections"
    ]
  },
  {
    "objectID": "julia/intro_collections.html#sets",
    "href": "julia/intro_collections.html#sets",
    "title": "Collections",
    "section": "Sets",
    "text": "Sets\nSets are collections without duplicates. The order of elements doesn’t matter.\n\nset1 = Set([9, 4, 8, 2, 7, 8])\n\nSet{Int64} with 5 elements:\n  4\n  7\n  2\n  9\n  8\n\n\n\nNotice how this is a set of 5 (and not 6) elements: the duplicated 8 didn’t matter.\n\n\nset2 = Set([10, 2, 3])\n\nSet{Int64} with 3 elements:\n  2\n  10\n  3\n\n\nYou can compare sets:\n\n# The union is the set of elements that are in one OR the other set\nunion(set1, set2)\n\nSet{Int64} with 7 elements:\n  4\n  7\n  2\n  10\n  9\n  8\n  3\n\n\n\n# The intersect is the set of elements that are in one AND the other set\nintersect(set1, set2)\n\nSet{Int64} with 1 element:\n  2\n\n\n\n# The setdiff is the set of elements that are in the first set but not in the second\n# Note that the order matters here\nsetdiff(set1, set2)\n\nSet{Int64} with 4 elements:\n  4\n  7\n  9\n  8\n\n\nSets can be heterogeneous:\n\nSet([\"test\", 9, :a])\n\nSet{Any} with 3 elements:\n  :a\n  \"test\"\n  9",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Collections"
    ]
  },
  {
    "objectID": "julia/intro_collections.html#arrays",
    "href": "julia/intro_collections.html#arrays",
    "title": "Collections",
    "section": "Arrays",
    "text": "Arrays\n\nVectors\nUnidimensional arrays in Julia are called vectors.\n\nVectors of one element\n\n[3]\n\n1-element Vector{Int64}:\n 3\n\n\n\n[3.4]\n\n1-element Vector{Float64}:\n 3.4\n\n\n\n[\"Hello, World!\"]\n\n1-element Vector{String}:\n \"Hello, World!\"\n\n\n\n\nVectors of multiple elements\n\n[3, 4]\n\n2-element Vector{Int64}:\n 3\n 4\n\n\n\n\n\nTwo dimensional arrays\n\n[3 4]\n\n1×2 Matrix{Int64}:\n 3  4\n\n\n\n[[1, 3] [1, 2]]\n\n2×2 Matrix{Int64}:\n 1  1\n 3  2\n\n\n\n\nSyntax subtleties\nThese 3 syntaxes are equivalent:\n\n[2 4 8]\n\n1×3 Matrix{Int64}:\n 2  4  8\n\n\n\nhcat(2, 4, 8)\n\n1×3 Matrix{Int64}:\n 2  4  8\n\n\n\ncat(2, 4, 8, dims=2)\n\n1×3 Matrix{Int64}:\n 2  4  8\n\n\nThese 4 syntaxes are equivalent:\n\n[2\n 4\n 8]\n\n3-element Vector{Int64}:\n 2\n 4\n 8\n\n\n\n[2; 4; 8]\n\n3-element Vector{Int64}:\n 2\n 4\n 8\n\n\n\nvcat(2, 4, 8)\n\n3-element Vector{Int64}:\n 2\n 4\n 8\n\n\n\ncat(2, 4, 8, dims=1)\n\n3-element Vector{Int64}:\n 2\n 4\n 8\n\n\nElements separated by semi-colons or end of lines get expanded vertically.\nThose separated by commas do not get expanded.\nElements separated by spaces or tabs get expanded horizontally.\n\n\nYour turn:\n\nCompare the outputs of the following:\n\n\n[1:2; 3:4]\n\n4-element Vector{Int64}:\n 1\n 2\n 3\n 4\n\n\n\n[1:2\n 3:4]\n\n4-element Vector{Int64}:\n 1\n 2\n 3\n 4\n\n\n\n[1:2, 3:4]\n\n2-element Vector{UnitRange{Int64}}:\n 1:2\n 3:4\n\n\n\n[1:2 3:4]\n\n2×2 Matrix{Int64}:\n 1  3\n 2  4\n\n\n\n\nArrays and types\nIn Julia, arrays can be heterogeneous:\n\n[3, \"hello\"]\n\n2-element Vector{Any}:\n 3\n  \"hello\"\n\n\nThis is possible because all elements of an array, no matter of what types, will always sit below the Any type in the type hierarchy.\n\n\nInitializing arrays\nBelow are examples of some of the functions initializing arrays:\n\nrand(2, 3, 4)\n\n2×3×4 Array{Float64, 3}:\n[:, :, 1] =\n 0.676981  0.00192985  0.460161\n 0.579801  0.0571031   0.19217\n\n[:, :, 2] =\n 0.709137  0.355586  0.717515\n 0.334442  0.768498  0.93754\n\n[:, :, 3] =\n 0.110039  0.468733  0.764542\n 0.708841  0.418923  0.102156\n\n[:, :, 4] =\n 0.1735    0.92587   0.822419\n 0.122246  0.749059  0.52207\n\n\n\nrand(Int64, 2, 3, 4)\n\n2×3×4 Array{Int64, 3}:\n[:, :, 1] =\n 7539227344717627596  -3475288731017273925  3435963957489459227\n 7076298999511187079  -4056353322580659761  -837262113887699001\n\n[:, :, 2] =\n -7562670463192357073  -8927025020788172752   6417337822872556077\n  3692648043801976038   3471765935100455283  -6192652305627405865\n\n[:, :, 3] =\n  7031151667336399214   5088860813990033390   5115764253454872856\n -6119814429800991191  -8609024759557032284  -1204795858698859213\n\n[:, :, 4] =\n -2961801139235269924   5392657125003334687   1122551460163567839\n  1768937120468164288  -3560940733318201168  -6602997138296377453\n\n\n\nzeros(Int64, 2, 5)\n\n2×5 Matrix{Int64}:\n 0  0  0  0  0\n 0  0  0  0  0\n\n\n\nones(2, 5)\n\n2×5 Matrix{Float64}:\n 1.0  1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0  1.0\n\n\n\nreshape([1, 2, 4, 2], (2, 2))\n\n2×2 Matrix{Int64}:\n 1  4\n 2  2\n\n\n\nfill(\"test\", (2, 2))\n\n2×2 Matrix{String}:\n \"test\"  \"test\"\n \"test\"  \"test\"\n\n\n\n\nBroadcasting\nTo apply a function to each element of a collection rather than to the collection as a whole, Julia uses broadcasting.\n\na = [-3, 2, -5]\n\n3-element Vector{Int64}:\n -3\n  2\n -5\n\n\nabs(a)\nLoadError: MethodError: no method matching abs(::Vector{Int64})\nThis doesn’t work because the function abs only applies to single elements.\nBy broadcasting abs, you apply it to each element of a:\n\nbroadcast(abs, a)\n\n3-element Vector{Int64}:\n 3\n 2\n 5\n\n\nThe dot notation is equivalent:\n\nabs.(a)\n\n3-element Vector{Int64}:\n 3\n 2\n 5\n\n\nIt can also be applied to the pipe, to unary and binary operators, etc.\n\na .|&gt; abs\n\n3-element Vector{Int64}:\n 3\n 2\n 5\n\n\n\n\nYour turn:\n\nTry to understand the difference between the following 2 expressions:\n\n\nabs.(a) == a .|&gt; abs\n\ntrue\n\n\n\nabs.(a) .== a .|&gt; abs\n\n3-element BitVector:\n 1\n 1\n 1\n\n\n\nHint: 0/1 are a short-form notations for false/true in arrays of Booleans.\n\n\n\nComprehensions\nJulia has an array comprehension syntax similar to Python’s:\n\n[ 3i + j for i=1:10, j=3 ]\n\n10-element Vector{Int64}:\n  6\n  9\n 12\n 15\n 18\n 21\n 24\n 27\n 30\n 33",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Collections"
    ]
  },
  {
    "objectID": "julia/intro_collections.html#indexing",
    "href": "julia/intro_collections.html#indexing",
    "title": "Collections",
    "section": "Indexing",
    "text": "Indexing\nAs in other mathematically oriented languages such as R, Julia starts indexing at 1.\nIndexing is done with square brackets:\n\na = [1 2; 3 4]\n\n2×2 Matrix{Int64}:\n 1  2\n 3  4\n\n\n\na[1, 1]\n\n1\n\n\n\na[1, :]\n\n2-element Vector{Int64}:\n 1\n 2\n\n\n\na[:, 1]\n\n2-element Vector{Int64}:\n 1\n 3\n\n\n\n# Here, we are indexing a tuple\n(2, 4, 1.0, \"test\")[2]\n\n4\n\n\n\n\nYour turn:\n\nIndex the element on the 3rd row and 2nd column of b:\n\nb = [\"wrong\" \"wrong\" \"wrong\"; \"wrong\" \"wrong\" \"wrong\"; \"wrong\" \"you got it\" \"wrong\"]\n\n3×3 Matrix{String}:\n \"wrong\"  \"wrong\"       \"wrong\"\n \"wrong\"  \"wrong\"       \"wrong\"\n \"wrong\"  \"you got it\"  \"wrong\"\n\n\n\n\n\nYour turn:\n\na = [1 2; 3 4]\na[1, 1]\na[1, :]\nHow can I get the second column?\nHow can I get the tuple (2, 4)? (a tuple is a list of elements)\n\nAs in Python, by default, arrays are passed by sharing:\n\na = [1, 2, 3];\na[1] = 0;\na\n\n3-element Vector{Int64}:\n 0\n 2\n 3\n\n\nThis prevents the unwanted copying of arrays.",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Collections"
    ]
  },
  {
    "objectID": "julia/index.html",
    "href": "julia/index.html",
    "title": "Julia",
    "section": "",
    "text": "Getting started with  \nAn intro course to Julia\n\n\n\n\nHigh-performance  \nAn HPC course in Julia\n\n\n\n\n\n\n60 min webinars\nVarious Julia topics",
    "crumbs": [
      "Julia",
      "<br>&nbsp;<img src=\"img/logo_julia.png\" class=\"img-fluid\" style=\"width:1.65em\" alt=\"noshadow\"><br><br>"
    ]
  },
  {
    "objectID": "julia/hpc_multithreading.html",
    "href": "julia/hpc_multithreading.html",
    "title": "Multi-threading",
    "section": "",
    "text": "Julia, which was built with efficiency in mind, aimed from the start to have parallel programming abilities. These however came gradually: first, there were coroutines, which is not parallel programming, but allows independent executions of elements of code; then there was a macro allowing for loops to run on several cores, but this would not work on nested loops and it did not integrate with the coroutines or I/O. With version 1.3 however multi-threading capabilities were born.\nWhat is great about Julia’s new task parallelism is that it is incredibly easy to use: no need to write low-level code as with MPI to set where tasks are run. Everything is automatic.",
    "crumbs": [
      "Julia",
      "<b><em>High-performance Julia</em></b>",
      "Multi-threading"
    ]
  },
  {
    "objectID": "julia/hpc_multithreading.html#launching-julia-on-multiple-threads",
    "href": "julia/hpc_multithreading.html#launching-julia-on-multiple-threads",
    "title": "Multi-threading",
    "section": "Launching Julia on multiple threads",
    "text": "Launching Julia on multiple threads\nTo use Julia with multiple threads, we need to launch julia with the JULIA_NUM_THREADS environment variable or with the flag --threads/-t:\n$ JULIA_NUM_THREADS=n julia\nor\n$ julia -t n\nFirst, we need to know how many threads we actually have on our machine.\nThere are many Linux tools for this, but here are two particularly convenient options:\n# To get the total number of available processes\n$ nproc\n\n# For more information (# of sockets, cores per socket, threads per core)\n$ lscpu | grep -E '(S|s)ocket|Thread|^CPU\\(s\\)'\nSince I have 4 available processes (2 cores with 2 threads each), I can launch Julia on 4 threads:\n$ JULIA_NUM_THREADS=4 julia\nThis can also be done from within the Juno IDE.\nTo see how many threads we are using, as well as the ID of the current thread, you can run:\nThreads.nthreads()\nThreads.threadid()",
    "crumbs": [
      "Julia",
      "<b><em>High-performance Julia</em></b>",
      "Multi-threading"
    ]
  },
  {
    "objectID": "julia/hpc_multithreading.html#for-loops-on-multiple-threads",
    "href": "julia/hpc_multithreading.html#for-loops-on-multiple-threads",
    "title": "Multi-threading",
    "section": "For loops on multiple threads",
    "text": "For loops on multiple threads\n\n\nYour turn:\n\nLaunch Julia on 1 thread and run the function below. Then run Julia on the maximum number of threads you have on your machine and run the same function.\n\nThreads.@threads for i = 1:10\n    println(\"i = $i on thread $(Threads.threadid())\")\nend\nUtilities such as htop allow you to visualize the working threads.",
    "crumbs": [
      "Julia",
      "<b><em>High-performance Julia</em></b>",
      "Multi-threading"
    ]
  },
  {
    "objectID": "julia/hpc_multithreading.html#generalization-of-multi-threading",
    "href": "julia/hpc_multithreading.html#generalization-of-multi-threading",
    "title": "Multi-threading",
    "section": "Generalization of multi-threading",
    "text": "Generalization of multi-threading\nLet’s consider the example presented in a Julia blog post in July 2019.\nBoth scripts sort a one dimensional array of 20,000,000 floats between 0 and 1, one with parallelism and one without.\nScript 1, without parallelism: sort.jl.\n# Create one dimensional array of 20,000,000 floats between 0 and 1\na = rand(20000000);\n\n# Use the MergeSort algorithm of the sort function\n# (in the standard Julia Base library)\nb = copy(a); @time sort!(b, alg = MergeSort);\n\n# Let's run the function a second time to remove the effect\n# of the initial compilation\nb = copy(a); @time sort!(b, alg = MergeSort);\nScript 2, with parallelism: psort.jl.\nimport Base.Threads.@spawn\n\n# The psort function is the same as the MergeSort algorithm\n# of the Base sort function with the addition of\n# the @spawn macro on one of the recursive calls\n\n# Sort the elements of `v` in place, from indices `lo` to `hi` inclusive\n\nfunction psort!(v, lo::Int=1, hi::Int = length(v))\n    \n    # 1 or 0 elements: nothing to do\n    if lo &gt;= hi\n        return v\n    end\n    \n    # Below some cutoff: run in serial\n    if hi - lo &lt; 100000\n        sort!(view(v, lo:hi), alg = MergeSort)\n        return v\n    end\n    \n    # Find the midpoint\n    mid = (lo + hi) &gt;&gt;&gt; 1\n    \n    # Task to sort the lower half\n    # will run in parallel with the current call sorting the upper half\n    half = @spawn psort!(v, lo, mid)\n    psort!(v, mid + 1, hi)\n    # Wait for the lower half to finish\n    wait(half)\n\n    # Workspace for merging\n    temp = v[lo:mid]\n    \n    # Merge the two sorted sub-arrays\n    i, k, j = 1, lo, mid + 1\n    @inbounds while k &lt; j &lt;= hi\n        if v[j] &lt; temp[i]\n            v[k] = v[j]\n            j += 1\n        else\n            v[k] = temp[i]\n            i += 1\n        end\n        k += 1\n    end\n    @inbounds while k &lt; j\n        v[k] = temp[i]\n        k += 1\n        i += 1\n    end\n    \n    return v\nend\n\na = rand(20000000);\n\n# Now, let's use our function\nb = copy(a); @time psort!(b);\n\n# And running it a second time to remove\n# the effect of the initial compilation\nb = copy(a); @time psort!(b);\nNow, we can test both scripts with one or multiple threads.\n\nSingle thread, non-parallel script:\n\n$ julia /path/to/sort.jl\n2.234024 seconds (111.88 k allocations: 82.489 MiB, 0.21% gc time)\n2.158333 seconds (11 allocations: 76.294 MiB, 0.51% gc time)\n\nNote the lower time for the 2nd run due to pre-compilation.\n\n\nSingle thread, parallel script:\n\n$ julia /path/to/psort.jl\n2.748138 seconds (336.77 k allocations: 703.200 MiB, 2.24% gc time)\n2.438032 seconds (3.58 k allocations: 686.932 MiB, 0.27% gc time)\n\nEven longer time: normal, there was more to run (import package, read function).\n\n\n2 threads, non-parallel script:\n\n$ JULIA_NUM_THREADS=2 julia /path/to/sort.jl\n2.233720 seconds (111.87 k allocations: 82.145 MiB, 0.21% gc time)\n2.155232 seconds (11 allocations: 76.294 MiB, 0.54% gc time)\n\nRemarkably similar to the single thread: the addition of a thread did not change anything.\n\n\n2 threads, parallel script:\n\n$ JULIA_NUM_THREADS=2 julia /path/to/psort.jl\n1.773643 seconds (336.99 k allocations: 703.171 MiB, 4.08% gc time)\n1.460539 seconds (3.79 k allocations: 686.935 MiB, 0.47% gc time)\n\n33% faster.\nNot twice as fast as one could have hoped since processes have to wait for each other. But that’s a good improvement.\n\n\n4 threads, non-parallel script:\n\n$ JULIA_NUM_THREADS=4 julia /path/to/sort.jl\n2.231717 seconds (111.87 k allocations: 82.145 MiB, 0.21% gc time)\n2.153509 seconds (11 allocations: 76.294 MiB, 0.53% gc time)\n\nAgain: same result as the single thread.\n\n\n4 threads, parallel script:\n\n$ JULIA_NUM_THREADS=4 julia /path/to/psort.jl\n1.291714 seconds (336.98 k allocations: 703.171 MiB, 3.48% gc time)\n1.194282 seconds (3.78 k allocations: 686.935 MiB, 5.19% gc time)\n\nEven though we only split our code in 2 tasks, there is still an improvement over the 2 thread run.",
    "crumbs": [
      "Julia",
      "<b><em>High-performance Julia</em></b>",
      "Multi-threading"
    ]
  },
  {
    "objectID": "julia/hpc_distributed.html",
    "href": "julia/hpc_distributed.html",
    "title": "Distributed computing",
    "section": "",
    "text": "Julia supports distributed computing thanks to the module Distributed from its standard library.\nThere are two ways to launch several Julia processes (called “workers”):\n\n\nJulia can be started with the -p flag followed by the number of workers by running (in a terminal):\njulia -p n\nThis launches n workers, available for parallel computations, in addition to the process running the interactive prompt (so there are n + 1 Julia processes in total).\nThe module Distributed is needed whenever you want to use several workers, but the -p flag loads it automatically.\n\nExample:\n\njulia -p 4\nWithin Julia, you can see how many workers are running with:\nnworkers()\nThe total number of processes can be seen with:\nnprocs()\n\n\n\nAlternatively, workers can be started from within a Julia session. In this case, you need to load the module Distributed explicitly:\nusing Distributed\nTo launch n workers:\naddprocs(n)\n\nExample:\n\naddprocs(4)",
    "crumbs": [
      "Julia",
      "<b><em>High-performance Julia</em></b>",
      "Distributed computing"
    ]
  },
  {
    "objectID": "julia/hpc_distributed.html#launching-several-julia-processes",
    "href": "julia/hpc_distributed.html#launching-several-julia-processes",
    "title": "Distributed computing",
    "section": "",
    "text": "Julia supports distributed computing thanks to the module Distributed from its standard library.\nThere are two ways to launch several Julia processes (called “workers”):\n\n\nJulia can be started with the -p flag followed by the number of workers by running (in a terminal):\njulia -p n\nThis launches n workers, available for parallel computations, in addition to the process running the interactive prompt (so there are n + 1 Julia processes in total).\nThe module Distributed is needed whenever you want to use several workers, but the -p flag loads it automatically.\n\nExample:\n\njulia -p 4\nWithin Julia, you can see how many workers are running with:\nnworkers()\nThe total number of processes can be seen with:\nnprocs()\n\n\n\nAlternatively, workers can be started from within a Julia session. In this case, you need to load the module Distributed explicitly:\nusing Distributed\nTo launch n workers:\naddprocs(n)\n\nExample:\n\naddprocs(4)",
    "crumbs": [
      "Julia",
      "<b><em>High-performance Julia</em></b>",
      "Distributed computing"
    ]
  },
  {
    "objectID": "julia/hpc_distributed.html#managing-workers",
    "href": "julia/hpc_distributed.html#managing-workers",
    "title": "Distributed computing",
    "section": "Managing workers",
    "text": "Managing workers\nTo list all the worker process identifiers:\nworkers()\n\nThe process running the Julia prompt has id 1.\n\nTo kill a worker:\nrmprocs(&lt;pid&gt;)\nwhere &lt;pid&gt; is the process identifier of the worker you want to kill (you can kill several workers by providing a list of pids).",
    "crumbs": [
      "Julia",
      "<b><em>High-performance Julia</em></b>",
      "Distributed computing"
    ]
  },
  {
    "objectID": "julia/hpc_distributed.html#using-workers",
    "href": "julia/hpc_distributed.html#using-workers",
    "title": "Distributed computing",
    "section": "Using workers",
    "text": "Using workers\nThere are a number of macros that are very convenient here:\n\nTo execute an expression on all processes, there is @everywhere\n\nFor instance, if your parallel code requires a module or an external package to run, you need to load that module or package with @everywhere:\n@everywhere using DataFrames\nIf the parallel code requires a script to run:\n@everywhere include(\"script.jl\")\nIf it requires a function that you are defining, you need to define it on all the workers:\n@everywhere function &lt;name&gt;(&lt;arguments&gt;)\n    &lt;body&gt;\nend\n\nTo assign a task to a particular worker, you use @spawnat\n\nThe first argument indicates the process id, the second argument is the expression that should be evaluated:\n@spawnat &lt;pid&gt; &lt;expression&gt;\n@spawnat returns of Future: the placeholder for a computation of unknown status and time. The function fetch waits for a Future to complete and returns the result of the computation.\n\nExample:\n\nThe function myid gives the id of the current process. As I mentioned earlier, the process running the interactive Julia prompt has the pid 1. So myid() normally returns 1.\nBut we can “spawn” myid on one of the worker, for instance the first worker (so pid 2):\n@spawnat 2 myid()\nAs you can see, we get a Future as a result. But if we pass it through fetch, we get the result of myid ran on the worker with pid 2:\nfetch(@spawnat 2 myid())\nIf you want tasks to be assigned to any worker automatically, you can pass the symbol :any to @spawnat instead of the worker id:\n@spawnat :any myid()\nTo get the result:\nfetch(@spawnat :any myid())\nIf you run this multiple times, you will see that myid is run on any of your available workers. This will however never return 1, except when you only have one running Julia process (in that case, the process running the prompt is considered a worker).",
    "crumbs": [
      "Julia",
      "<b><em>High-performance Julia</em></b>",
      "Distributed computing"
    ]
  },
  {
    "objectID": "git/ws_search.html",
    "href": "git/ws_search.html",
    "title": "Searching a version-controlled project",
    "section": "",
    "text": "What is the point of creating all these commits if you are unable to make use of them because you can’t find the information you need in them?\nIn this workshop, we will learn how to search:\n\nyour files (at any of their versions) and\nyour commit logs.\n\nBy the end of the workshop, you should be able to retrieve anything you need from your versioned project."
  },
  {
    "objectID": "git/ws_search.html#installation",
    "href": "git/ws_search.html#installation",
    "title": "Searching a version-controlled project",
    "section": "Installation",
    "text": "Installation\nMacOS & Linux users:\nInstall Git from the official website.\nWindows users:\nInstall Git for Windows. This will also install “Git Bash”, a Bash emulator."
  },
  {
    "objectID": "git/ws_search.html#using-git",
    "href": "git/ws_search.html#using-git",
    "title": "Searching a version-controlled project",
    "section": "Using Git",
    "text": "Using Git\nWe will use Git from the command line throughout this workshop.\nMacOS users:    open “Terminal”.\nWindows users:   open “Git Bash”.\nLinux users:    open the terminal emulator of your choice."
  },
  {
    "objectID": "git/ws_search.html#practice-repo",
    "href": "git/ws_search.html#practice-repo",
    "title": "Searching a version-controlled project",
    "section": "Practice repo",
    "text": "Practice repo\n\nGet a repo\nYou are welcome to use a repository of yours to follow this workshop. Alternatively, you can clone a practice repo I have on GitHub:\n\nNavigate to an appropriate location:\n\ncd /path/to/appropriate/location\n\nClone the repo:\n\n# If you have set SSH for your GitHub account\ngit clone git@github.com:prosoitos/practice_repo.git\n# If you haven't set SSH\ngit clone https://github.com/prosoitos/practice_repo.git\n\nEnter the repo:\n\ncd practice_repo"
  },
  {
    "objectID": "git/ws_search.html#searching-files",
    "href": "git/ws_search.html#searching-files",
    "title": "Searching a version-controlled project",
    "section": "Searching files",
    "text": "Searching files\nThe first thing that can happen is that you are looking for a certain pattern somewhere in your project (for instance a certain function or a certain word).\n\ngit grep\nThe main command to look through versioned files is git grep.\nYou might be familiar with the command-line utility grep which allows to search for lines matching a certain pattern in files. git grep does a similar job with these differences:\n\nit is much faster since all files under version control are already indexed by Git,\nyou can easily search any commit without having to check it out,\nit has features lacking in grep such as, for instance, pattern arithmetic or tree search using globs.\n\n\n\nLet’s try it\nBy default, git grep searches recursively through the tracked files in the working directory (that is, the current version of the tracked files).\nFirst, let’s look for the word test in the current version of the tracked files in the test repo:\n\ngit grep test\n\nintro_aliases.qmd:Now, let's build an alias for a more complex command: `git grep \"test\" $(git rev-list --all)`. This example\nintro_aliases.qmd:from the *\"Searching a Git project\"* section below will search for the string \"test\" in all previous\nintro_aliases.qmd:commits. There are two problems with this command: (1) it takes an argument (the string \"test\"), and (2) it\nintro_aliases.qmd:git search test\nintro_aliases.qmd:should search the entire current Git project history for \"test\".\nintro_branches.qmd:git branch test\nintro_branches.qmd:git switch test\nintro_branches.qmd:* test\nintro_branches.qmd:The `*` shows the branch you are currently on (i.e. the branch to which `HEAD` points to). In our example, the project has two branches and we are on the branch `test`.\nintro_branches.qmd:git diff main test\nintro_branches.qmd:When you are happy with the changes you made on your test branch, you can merge it into `main`.\nintro_branches.qmd:If you have only created new commits on the branch `test`, the merge is called a \"fast-forward merge\" because `main` and `test` have not diverged: it is simply a question of having `main` catch up to `test`.\nintro_branches.qmd:git merge test\nintro_branches.qmd:Then, usually, you delete the branch `test` as it has served its purpose:\nintro_branches.qmd:git branch -d test\nintro_branches.qmd:Alternatively, you can switch back to `test` and do the next bit of experimental work on it. This allows to keep `main` free of mishaps and bad developments.\nintro_branches.qmd:Let's go back to our situation before we created the branch `test`:\nintro_branches.qmd:This time, you create a branch called `test2`:\nintro_branches.qmd:To merge your branch `test2` into `main`, a new commit is now required. Git will create this new commit automatically. As long as there is no conflict, it is just as easy as a fast-forward merge:\nintro_branches.qmd:git merge test2\nintro_branches.qmd:After which, you can delete the (now useless) test branch (with `git branch -d test2`):\nintro_branches.qmd:&gt;&gt;&gt;&gt;&gt;&gt;&gt; test2\nintro_intro_old.qmd:The pointer `HEAD`, which normally points to the branch `main` which itself points to latest commit, can be moved around. By moving `HEAD` to any commit, you can revisit the state of your project at that particular version.\nintro_intro_old.qmd:Instead of working on your branch `main`, you create a test branch and work on it (so `HEAD` is on the branch `test` and both move along as you create commits):\nintro_intro_old.qmd:When you are happy with the changes you made on your test branch, you decide to merge `main` onto it.\nintro_intro_old.qmd:Then you do the fast-forward merge from `main` onto `test` (so `main` catches up to `test`):\nintro_intro_old.qmd:Then, usually, you delete the branch `test` as it has served its purpose (with `git branch -d test`). Alternatively, you can switch back to it and do the next bit of experimental work in it.\nintro_intro_old.qmd:This allows to keep `main` free of possible mishaps and bad developments (if you aren't happy with the work you did on your test branch, you can simply delete it and Git will clean the commits that are on it but not on `main` during the next garbage collection.\nintro_intro_old.qmd:You create a test branch and switch to it:\nintro_intro_old.qmd:To merge your main branch and your test branch, a new commit is now required (note that the command is the same as in the case of a fast-forward merge: `git merge`. Git will create the new commit automatically. As long as there is no conflict, it is just as easy as a fast-forward merge. We will talk later about resolving conflicts).\nintro_intro_old.qmd:After which, you can delete the (now useless) test branch (with `git branch -d test2`):\nintro_remotes.qmd:Click on the `Code` green drop-down button, select SSH [if you have set SSH for your GitHub account](https://docs.github.com/en/free-pro-team@latest/github/authenticating-to-github/connecting-to-github-with-ssh) or HTTPS and copy the address.\nintro_revisiting_old_commits_alternate.qmd:The pointer `HEAD`, which normally points to the branch `main` which itself points to latest commit, can be moved around. By moving `HEAD` to any commit, you can revisit the state of your project at that particular version.\nintro_undo.qmd:Here is a common scenario: you make a commit, then realize that you forgot to include some changes in that commit; or you aren't happy with the commit message; or both. You can edit your latest commit with the `--amend` flag:\nws_collab.qmd:Click on the `Code` green drop-down button, select SSH [if you have set SSH for your GitHub account](https://docs.github.com/en/free-pro-team@latest/github/authenticating-to-github/connecting-to-github-with-ssh) or HTTPS and copy the address.\n\n\nLet’s add blank lines between the results of each file for better readability:\n\ngit grep --break test\n\nintro_aliases.qmd:Now, let's build an alias for a more complex command: `git grep \"test\" $(git rev-list --all)`. This example\nintro_aliases.qmd:from the *\"Searching a Git project\"* section below will search for the string \"test\" in all previous\nintro_aliases.qmd:commits. There are two problems with this command: (1) it takes an argument (the string \"test\"), and (2) it\nintro_aliases.qmd:git search test\nintro_aliases.qmd:should search the entire current Git project history for \"test\".\n\nintro_branches.qmd:git branch test\nintro_branches.qmd:git switch test\nintro_branches.qmd:* test\nintro_branches.qmd:The `*` shows the branch you are currently on (i.e. the branch to which `HEAD` points to). In our example, the project has two branches and we are on the branch `test`.\nintro_branches.qmd:git diff main test\nintro_branches.qmd:When you are happy with the changes you made on your test branch, you can merge it into `main`.\nintro_branches.qmd:If you have only created new commits on the branch `test`, the merge is called a \"fast-forward merge\" because `main` and `test` have not diverged: it is simply a question of having `main` catch up to `test`.\nintro_branches.qmd:git merge test\nintro_branches.qmd:Then, usually, you delete the branch `test` as it has served its purpose:\nintro_branches.qmd:git branch -d test\nintro_branches.qmd:Alternatively, you can switch back to `test` and do the next bit of experimental work on it. This allows to keep `main` free of mishaps and bad developments.\nintro_branches.qmd:Let's go back to our situation before we created the branch `test`:\nintro_branches.qmd:This time, you create a branch called `test2`:\nintro_branches.qmd:To merge your branch `test2` into `main`, a new commit is now required. Git will create this new commit automatically. As long as there is no conflict, it is just as easy as a fast-forward merge:\nintro_branches.qmd:git merge test2\nintro_branches.qmd:After which, you can delete the (now useless) test branch (with `git branch -d test2`):\nintro_branches.qmd:&gt;&gt;&gt;&gt;&gt;&gt;&gt; test2\n\nintro_intro_old.qmd:The pointer `HEAD`, which normally points to the branch `main` which itself points to latest commit, can be moved around. By moving `HEAD` to any commit, you can revisit the state of your project at that particular version.\nintro_intro_old.qmd:Instead of working on your branch `main`, you create a test branch and work on it (so `HEAD` is on the branch `test` and both move along as you create commits):\nintro_intro_old.qmd:When you are happy with the changes you made on your test branch, you decide to merge `main` onto it.\nintro_intro_old.qmd:Then you do the fast-forward merge from `main` onto `test` (so `main` catches up to `test`):\nintro_intro_old.qmd:Then, usually, you delete the branch `test` as it has served its purpose (with `git branch -d test`). Alternatively, you can switch back to it and do the next bit of experimental work in it.\nintro_intro_old.qmd:This allows to keep `main` free of possible mishaps and bad developments (if you aren't happy with the work you did on your test branch, you can simply delete it and Git will clean the commits that are on it but not on `main` during the next garbage collection.\nintro_intro_old.qmd:You create a test branch and switch to it:\nintro_intro_old.qmd:To merge your main branch and your test branch, a new commit is now required (note that the command is the same as in the case of a fast-forward merge: `git merge`. Git will create the new commit automatically. As long as there is no conflict, it is just as easy as a fast-forward merge. We will talk later about resolving conflicts).\nintro_intro_old.qmd:After which, you can delete the (now useless) test branch (with `git branch -d test2`):\n\nintro_remotes.qmd:Click on the `Code` green drop-down button, select SSH [if you have set SSH for your GitHub account](https://docs.github.com/en/free-pro-team@latest/github/authenticating-to-github/connecting-to-github-with-ssh) or HTTPS and copy the address.\n\nintro_revisiting_old_commits_alternate.qmd:The pointer `HEAD`, which normally points to the branch `main` which itself points to latest commit, can be moved around. By moving `HEAD` to any commit, you can revisit the state of your project at that particular version.\n\nintro_undo.qmd:Here is a common scenario: you make a commit, then realize that you forgot to include some changes in that commit; or you aren't happy with the commit message; or both. You can edit your latest commit with the `--amend` flag:\n\nws_collab.qmd:Click on the `Code` green drop-down button, select SSH [if you have set SSH for your GitHub account](https://docs.github.com/en/free-pro-team@latest/github/authenticating-to-github/connecting-to-github-with-ssh) or HTTPS and copy the address.\n\n\nLet’s also put the file names on separate lines:\n\ngit grep --break --heading test\n\nintro_aliases.qmd\nNow, let's build an alias for a more complex command: `git grep \"test\" $(git rev-list --all)`. This example\nfrom the *\"Searching a Git project\"* section below will search for the string \"test\" in all previous\ncommits. There are two problems with this command: (1) it takes an argument (the string \"test\"), and (2) it\ngit search test\nshould search the entire current Git project history for \"test\".\n\nintro_branches.qmd\ngit branch test\ngit switch test\n* test\nThe `*` shows the branch you are currently on (i.e. the branch to which `HEAD` points to). In our example, the project has two branches and we are on the branch `test`.\ngit diff main test\nWhen you are happy with the changes you made on your test branch, you can merge it into `main`.\nIf you have only created new commits on the branch `test`, the merge is called a \"fast-forward merge\" because `main` and `test` have not diverged: it is simply a question of having `main` catch up to `test`.\ngit merge test\nThen, usually, you delete the branch `test` as it has served its purpose:\ngit branch -d test\nAlternatively, you can switch back to `test` and do the next bit of experimental work on it. This allows to keep `main` free of mishaps and bad developments.\nLet's go back to our situation before we created the branch `test`:\nThis time, you create a branch called `test2`:\nTo merge your branch `test2` into `main`, a new commit is now required. Git will create this new commit automatically. As long as there is no conflict, it is just as easy as a fast-forward merge:\ngit merge test2\nAfter which, you can delete the (now useless) test branch (with `git branch -d test2`):\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; test2\n\nintro_intro_old.qmd\nThe pointer `HEAD`, which normally points to the branch `main` which itself points to latest commit, can be moved around. By moving `HEAD` to any commit, you can revisit the state of your project at that particular version.\nInstead of working on your branch `main`, you create a test branch and work on it (so `HEAD` is on the branch `test` and both move along as you create commits):\nWhen you are happy with the changes you made on your test branch, you decide to merge `main` onto it.\nThen you do the fast-forward merge from `main` onto `test` (so `main` catches up to `test`):\nThen, usually, you delete the branch `test` as it has served its purpose (with `git branch -d test`). Alternatively, you can switch back to it and do the next bit of experimental work in it.\nThis allows to keep `main` free of possible mishaps and bad developments (if you aren't happy with the work you did on your test branch, you can simply delete it and Git will clean the commits that are on it but not on `main` during the next garbage collection.\nYou create a test branch and switch to it:\nTo merge your main branch and your test branch, a new commit is now required (note that the command is the same as in the case of a fast-forward merge: `git merge`. Git will create the new commit automatically. As long as there is no conflict, it is just as easy as a fast-forward merge. We will talk later about resolving conflicts).\nAfter which, you can delete the (now useless) test branch (with `git branch -d test2`):\n\nintro_remotes.qmd\nClick on the `Code` green drop-down button, select SSH [if you have set SSH for your GitHub account](https://docs.github.com/en/free-pro-team@latest/github/authenticating-to-github/connecting-to-github-with-ssh) or HTTPS and copy the address.\n\nintro_revisiting_old_commits_alternate.qmd\nThe pointer `HEAD`, which normally points to the branch `main` which itself points to latest commit, can be moved around. By moving `HEAD` to any commit, you can revisit the state of your project at that particular version.\n\nintro_undo.qmd\nHere is a common scenario: you make a commit, then realize that you forgot to include some changes in that commit; or you aren't happy with the commit message; or both. You can edit your latest commit with the `--amend` flag:\n\nws_collab.qmd\nClick on the `Code` green drop-down button, select SSH [if you have set SSH for your GitHub account](https://docs.github.com/en/free-pro-team@latest/github/authenticating-to-github/connecting-to-github-with-ssh) or HTTPS and copy the address.\n\n\nWe can display the line numbers for the results with the -n flag:\n\ngit grep --break --heading -n test\n\nintro_aliases.qmd\n48:Now, let's build an alias for a more complex command: `git grep \"test\" $(git rev-list --all)`. This example\n49:from the *\"Searching a Git project\"* section below will search for the string \"test\" in all previous\n50:commits. There are two problems with this command: (1) it takes an argument (the string \"test\"), and (2) it\n68:git search test\n71:should search the entire current Git project history for \"test\".\n\nintro_branches.qmd\n54:git branch test\n72:git switch test\n99:* test\n102:The `*` shows the branch you are currently on (i.e. the branch to which `HEAD` points to). In our example, the project has two branches and we are on the branch `test`.\n109:git diff main test\n116:When you are happy with the changes you made on your test branch, you can merge it into `main`.\n120:If you have only created new commits on the branch `test`, the merge is called a \"fast-forward merge\" because `main` and `test` have not diverged: it is simply a question of having `main` catch up to `test`.\n135:git merge test\n140:Then, usually, you delete the branch `test` as it has served its purpose:\n143:git branch -d test\n148:Alternatively, you can switch back to `test` and do the next bit of experimental work on it. This allows to keep `main` free of mishaps and bad developments.\n154:Let's go back to our situation before we created the branch `test`:\n158:This time, you create a branch called `test2`:\n182:To merge your branch `test2` into `main`, a new commit is now required. Git will create this new commit automatically. As long as there is no conflict, it is just as easy as a fast-forward merge:\n185:git merge test2\n190:After which, you can delete the (now useless) test branch (with `git branch -d test2`):\n215:&gt;&gt;&gt;&gt;&gt;&gt;&gt; test2\n\nintro_intro_old.qmd\n904:The pointer `HEAD`, which normally points to the branch `main` which itself points to latest commit, can be moved around. By moving `HEAD` to any commit, you can revisit the state of your project at that particular version.\n1219:Instead of working on your branch `main`, you create a test branch and work on it (so `HEAD` is on the branch `test` and both move along as you create commits):\n1227:When you are happy with the changes you made on your test branch, you decide to merge `main` onto it.\n1233:Then you do the fast-forward merge from `main` onto `test` (so `main` catches up to `test`):\n1237:Then, usually, you delete the branch `test` as it has served its purpose (with `git branch -d test`). Alternatively, you can switch back to it and do the next bit of experimental work in it.\n1238:This allows to keep `main` free of possible mishaps and bad developments (if you aren't happy with the work you did on your test branch, you can simply delete it and Git will clean the commits that are on it but not on `main` during the next garbage collection.\n1250:You create a test branch and switch to it:\n1270:To merge your main branch and your test branch, a new commit is now required (note that the command is the same as in the case of a fast-forward merge: `git merge`. Git will create the new commit automatically. As long as there is no conflict, it is just as easy as a fast-forward merge. We will talk later about resolving conflicts).\n1274:After which, you can delete the (now useless) test branch (with `git branch -d test2`):\n\nintro_remotes.qmd\n45:Click on the `Code` green drop-down button, select SSH [if you have set SSH for your GitHub account](https://docs.github.com/en/free-pro-team@latest/github/authenticating-to-github/connecting-to-github-with-ssh) or HTTPS and copy the address.\n\nintro_revisiting_old_commits_alternate.qmd\n3:The pointer `HEAD`, which normally points to the branch `main` which itself points to latest commit, can be moved around. By moving `HEAD` to any commit, you can revisit the state of your project at that particular version.\n\nintro_undo.qmd\n16:Here is a common scenario: you make a commit, then realize that you forgot to include some changes in that commit; or you aren't happy with the commit message; or both. You can edit your latest commit with the `--amend` flag:\n\nws_collab.qmd\n52:Click on the `Code` green drop-down button, select SSH [if you have set SSH for your GitHub account](https://docs.github.com/en/free-pro-team@latest/github/authenticating-to-github/connecting-to-github-with-ssh) or HTTPS and copy the address.\n\n\nNotice how the results for the file src/test_manuel.py involve functions. It would be very convenient to have the names of the functions in which test appears.\nWe can do this with the -p flag:\n\ngit grep --break --heading -p test src/test_manuel.py\n\nfatal: ambiguous argument 'src/test_manuel.py': unknown revision or path not in the working tree.\nUse '--' to separate paths from revisions, like this:\n'git &lt;command&gt; [&lt;revision&gt;...] -- [&lt;file&gt;...]'\n\n\n\nWe added the argument src/test_manuel.py to limit the search to that file.\n\nWe can now see that the word test appears in the functions test and main.\nNow, instead of printing all the matching lines, let’s print the number of matches per file:\n\ngit grep -c test\n\nintro_aliases.qmd:5\nintro_branches.qmd:17\nintro_intro_old.qmd:9\nintro_remotes.qmd:1\nintro_revisiting_old_commits_alternate.qmd:1\nintro_undo.qmd:1\nws_collab.qmd:1\n\n\n\n\nMore complex patterns\ngit grep in fact searches for regular expressions. test is a regular expression matching test, but we can look for more complex patterns.\nLet’s look for image:\n\ngit grep image\n\nintro_ignore.qmd:- Non-text files (e.g. images, office documents)\n\n\n\nNo output means that the search is not returning any result.\n\nLet’s make this search case insensitive:\n\ngit grep -i image\n\nintro_ignore.qmd:- Non-text files (e.g. images, office documents)\n\n\nWe are now getting some results as Image was present in three lines of the file src/new_file.py.\nLet’s now search for data:\n\ngit grep data\n\nintro_changes.qmd:Remember that HEAD is a pointer pointing at a branch, that a branch is itself a pointer pointing at a commit, and finally that a commit is a Git object pointing at compressed blobs containing data about your project at a certain commit. When the HEAD pointer moves around, whatever commit it points to populates the [HEAD]{.emph} tree with the corresponding data.\nintro_first_steps.qmd:Alternatively, you can download the file with this button: \nintro_first_steps.qmd:data/\nintro_first_steps.qmd:data\nintro_first_steps.qmd:data\nintro_first_steps.qmd:./data:\nintro_first_steps.qmd:dataset.csv\nintro_first_steps.qmd:├── data\nintro_first_steps.qmd:│   └── dataset.csv\nintro_first_steps.qmd:This is our very exciting data set:\nintro_first_steps.qmd:cat data/dataset.csv\nintro_first_steps.qmd:df = pd.read_csv('../data/dataset.csv')\nintro_first_steps.qmd:data\nintro_first_steps.qmd:        data/\nintro_first_steps.qmd:        data/\nintro_first_steps.qmd:Remember that each commit contains the following metadata:\nintro_first_steps.qmd:        data/\nintro_ignore.qmd:- Your initial data\nintro_ignore.qmd:Notice how `data/` is not listed in the untracked files anymore.\nintro_ignore.qmd:git commit -m \"Add .gitignore file with data and results\"\nintro_ignore.qmd:[main a1df8e5] Add .gitignore file with data and results\nintro_intro_old.qmd:mkdir chapter3/src chapter3/ms chapter3/data chapter3/results\nintro_intro_old.qmd:df &lt;- data.frame(\nintro_intro_old.qmd:data\nintro_intro_old.qmd:Each commit is identified by a unique *hash* and contains these metadata:\nintro_intro_old.qmd:@@ -7,3 +7,5 @@ df &lt;- data.frame(\nintro_intro_old.qmd:@@ -7,3 +7,5 @@ df &lt;- data.frame(\nintro_intro_old.qmd:Not everything should be under version control. For instance, you don't want to put under version control non-text files or your initial data. You also shouldn't put under version control documents that can be easily recreated such as graphs and script outputs.\nintro_intro_old.qmd:echo \"/data/\nintro_intro_old.qmd:This creates a `.gitignore` file with two entries (`/data/` and `/results/`) and from now on, any file in either of these directories will be ignored by Git.\nintro_intro_old.qmd:git commit -m \"Add .gitignore file with data and results\"\nintro_intro_old.qmd:[main a1df8e5] Add .gitignore file with data and results\nintro_intro_old.qmd:    Add .gitignore file with data and results\nintro_intro_old.qmd:a1df8e5 (HEAD -&gt; main) Add .gitignore file with data and results\nintro_intro_old.qmd:|     Add .gitignore file with data and results\nintro_intro_old.qmd:* a1df8e5 88 seconds ago  (HEAD -&gt; main)Add .gitignore file with data and results xxx@xxx\nintro_intro_old.qmd:In addition to displaying the commit metadata, this also displays the difference with the previous commit.\nintro_intro_old.qmd:    Add .gitignore file with data and results\nintro_intro_old.qmd:+/data/\nintro_intro_old.qmd:@@ -7,3 +7,5 @@ df &lt;- data.frame(\nintro_intro_old.qmd:    Add .gitignore file with data and results\nintro_intro_slides.qmd:The data is stored as blobs, doesn't create unnecessary copies (unchanged files are referenced from old blobs), and uses excellent compression\nintro_intro_slides.qmd:Each commit has a unique *hash* and contains the following metadata:\nintro_logs.qmd:    Add .gitignore file with data and results\nintro_logs.qmd:c4ab5e7 Add .gitignore file with data and results\nintro_logs.qmd:|     Add .gitignore file with data and results\nintro_logs.qmd:* c4ab5e7 34 minutes ago Add .gitignore file with data and results xxx@xxx\nintro_logs.qmd:+df = pd.read_csv('../data/dataset.csv')\nintro_logs.qmd:    Add .gitignore file with data and results\nintro_logs.qmd:+/data/\nintro_logs.qmd:In addition to displaying the commit metadata, `git show` also displays the diff of that commit with its parent commit.\nintro_remotes.qmd:## Getting data from a remote\nintro_remotes.qmd:If you collaborate on a project, you have to get the data added by your teammates to keep your local project up to date.\nintro_remotes.qmd:To download new data from a remote, you have 2 options:\nintro_remotes.qmd:*Fetching* downloads the data from a remote that you don't already have in your local version of the project:\nintro_remotes.qmd:Uploading data to the remote is called *pushing*:\nintro_undo.qmd:As you just experienced, this command leads to data loss. \\\nBinary file project.zip matches\nwb_dvc.qmd:title: Version control for data science and machine learning with DVC\n\n\nWe are getting results for the word data, but also for the pattern data in longer expressions such as train_data or dataset. If we only want results for the word data, we can use the -w flag:\n\ngit grep -w data\n\nintro_changes.qmd:Remember that HEAD is a pointer pointing at a branch, that a branch is itself a pointer pointing at a commit, and finally that a commit is a Git object pointing at compressed blobs containing data about your project at a certain commit. When the HEAD pointer moves around, whatever commit it points to populates the [HEAD]{.emph} tree with the corresponding data.\nintro_first_steps.qmd:Alternatively, you can download the file with this button: \nintro_first_steps.qmd:data/\nintro_first_steps.qmd:data\nintro_first_steps.qmd:data\nintro_first_steps.qmd:./data:\nintro_first_steps.qmd:├── data\nintro_first_steps.qmd:This is our very exciting data set:\nintro_first_steps.qmd:cat data/dataset.csv\nintro_first_steps.qmd:df = pd.read_csv('../data/dataset.csv')\nintro_first_steps.qmd:data\nintro_first_steps.qmd:        data/\nintro_first_steps.qmd:        data/\nintro_first_steps.qmd:        data/\nintro_ignore.qmd:- Your initial data\nintro_ignore.qmd:Notice how `data/` is not listed in the untracked files anymore.\nintro_ignore.qmd:git commit -m \"Add .gitignore file with data and results\"\nintro_ignore.qmd:[main a1df8e5] Add .gitignore file with data and results\nintro_intro_old.qmd:mkdir chapter3/src chapter3/ms chapter3/data chapter3/results\nintro_intro_old.qmd:df &lt;- data.frame(\nintro_intro_old.qmd:data\nintro_intro_old.qmd:@@ -7,3 +7,5 @@ df &lt;- data.frame(\nintro_intro_old.qmd:@@ -7,3 +7,5 @@ df &lt;- data.frame(\nintro_intro_old.qmd:Not everything should be under version control. For instance, you don't want to put under version control non-text files or your initial data. You also shouldn't put under version control documents that can be easily recreated such as graphs and script outputs.\nintro_intro_old.qmd:echo \"/data/\nintro_intro_old.qmd:This creates a `.gitignore` file with two entries (`/data/` and `/results/`) and from now on, any file in either of these directories will be ignored by Git.\nintro_intro_old.qmd:git commit -m \"Add .gitignore file with data and results\"\nintro_intro_old.qmd:[main a1df8e5] Add .gitignore file with data and results\nintro_intro_old.qmd:    Add .gitignore file with data and results\nintro_intro_old.qmd:a1df8e5 (HEAD -&gt; main) Add .gitignore file with data and results\nintro_intro_old.qmd:|     Add .gitignore file with data and results\nintro_intro_old.qmd:* a1df8e5 88 seconds ago  (HEAD -&gt; main)Add .gitignore file with data and results xxx@xxx\nintro_intro_old.qmd:    Add .gitignore file with data and results\nintro_intro_old.qmd:+/data/\nintro_intro_old.qmd:@@ -7,3 +7,5 @@ df &lt;- data.frame(\nintro_intro_old.qmd:    Add .gitignore file with data and results\nintro_intro_slides.qmd:The data is stored as blobs, doesn't create unnecessary copies (unchanged files are referenced from old blobs), and uses excellent compression\nintro_logs.qmd:    Add .gitignore file with data and results\nintro_logs.qmd:c4ab5e7 Add .gitignore file with data and results\nintro_logs.qmd:|     Add .gitignore file with data and results\nintro_logs.qmd:* c4ab5e7 34 minutes ago Add .gitignore file with data and results xxx@xxx\nintro_logs.qmd:+df = pd.read_csv('../data/dataset.csv')\nintro_logs.qmd:    Add .gitignore file with data and results\nintro_logs.qmd:+/data/\nintro_remotes.qmd:## Getting data from a remote\nintro_remotes.qmd:If you collaborate on a project, you have to get the data added by your teammates to keep your local project up to date.\nintro_remotes.qmd:To download new data from a remote, you have 2 options:\nintro_remotes.qmd:*Fetching* downloads the data from a remote that you don't already have in your local version of the project:\nintro_remotes.qmd:Uploading data to the remote is called *pushing*:\nintro_undo.qmd:As you just experienced, this command leads to data loss. \\\nBinary file project.zip matches\nwb_dvc.qmd:title: Version control for data science and machine learning with DVC\n\n\nNow, let’s use a more complex regular expression. We want the counts for the pattern \".*_.*\" (i.e. any name with a snail case such as train_loader):\n\ngit grep -c \".*_.*\"\n\nimg/01.png:16\nimg/02.png:32\nimg/03.png:31\nimg/04.png:26\nimg/05.png:31\nimg/06.png:32\nimg/07.png:30\nimg/08.png:34\nimg/09.png:35\nimg/10.png:41\nimg/11.png:47\nimg/12.png:40\nimg/13.png:39\nimg/14.png:32\nimg/15.png:38\nimg/16.png:43\nimg/17.png:34\nimg/18.png:35\nimg/19.png:30\nimg/20.png:33\nimg/21.png:40\nimg/22.png:41\nimg/23.png:47\nimg/24.png:64\nimg/25.png:66\nimg/26.png:50\nimg/27.png:60\nimg/28.png:57\nimg/29.png:33\nimg/30.png:39\nimg/31.png:14\nimg/32.png:16\nimg/33.png:18\nimg/34.png:16\nimg/35.png:20\nimg/36.png:18\nimg/37.png:18\nimg/51.png:55\nimg/52.png:46\nimg/53.png:55\nimg/collab.jpg:178\nimg/git_graph.png:121\nimg/gitout.png:42\nimg/logo_git.png:4\nimg/vc.jpg:259\nindex.qmd:4\nintro_documentation.qmd:1\nintro_first_steps.qmd:4\nintro_install.qmd:2\nintro_intro.qmd:1\nintro_intro_old.qmd:8\nintro_intro_slides.qmd:5\nintro_logs.qmd:1\nintro_tags.qmd:5\nintro_time_travel.qmd:1\ntop_intro.qmd:2\ntop_ws.qmd:3\nwb_dvc.qmd:1\n\n\nLet’s print the first 3 results per file:\n\ngit grep -m 3 \".*_.*\"\n\nBinary file img/01.png matches\nBinary file img/02.png matches\nBinary file img/03.png matches\nBinary file img/04.png matches\nBinary file img/05.png matches\nBinary file img/06.png matches\nBinary file img/07.png matches\nBinary file img/08.png matches\nBinary file img/09.png matches\nBinary file img/10.png matches\nBinary file img/11.png matches\nBinary file img/12.png matches\nBinary file img/13.png matches\nBinary file img/14.png matches\nBinary file img/15.png matches\nBinary file img/16.png matches\nBinary file img/17.png matches\nBinary file img/18.png matches\nBinary file img/19.png matches\nBinary file img/20.png matches\nBinary file img/21.png matches\nBinary file img/22.png matches\nBinary file img/23.png matches\nBinary file img/24.png matches\nBinary file img/25.png matches\nBinary file img/26.png matches\nBinary file img/27.png matches\nBinary file img/28.png matches\nBinary file img/29.png matches\nBinary file img/30.png matches\nBinary file img/31.png matches\nBinary file img/32.png matches\nBinary file img/33.png matches\nBinary file img/34.png matches\nBinary file img/35.png matches\nBinary file img/36.png matches\nBinary file img/37.png matches\nBinary file img/51.png matches\nBinary file img/52.png matches\nBinary file img/53.png matches\nBinary file img/collab.jpg matches\nBinary file img/git_graph.png matches\nBinary file img/gitout.png matches\nBinary file img/logo_git.png matches\nBinary file img/vc.jpg matches\nindex.qmd:  Version control & collaboration with &nbsp;[![](img/logo_git.png){width=\"1.3em\" fig-alt=\"noshadow\"}](https://git-scm.com/)\nindex.qmd:[Getting started with &nbsp;![](img/logo_git.png){width=\"1.2em\" fig-alt=\"noshadow\"}](top_intro.qmd){.card-title2 .stretched-link}\nindex.qmd:[Workshops](top_ws.qmd){.card-title2 .stretched-link}\nintro_documentation.qmd:All these methods lead to the same thing: the manual page corresponding to the command will open in a pager (usually [less](https://en.wikipedia.org/wiki/Less_(Unix))). A pager is a program which makes it easier to read documents in the command line.\nintro_first_steps.qmd:  - first_steps.html\nintro_first_steps.qmd:wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1SJV5mRGexf91lNyFwdS_JmuAXX0xS4pE' -O project.zip\nintro_first_steps.qmd:df = pd.read_csv('../data/dataset.csv')\nintro_install.qmd:Git is built for Unix-like systems (Linux and MacOS). In order to use Git from the command line on Windows, you need a Unix shell such as [Bash](https://en.wikipedia.org/wiki/Bash_(Unix_shell)). To make this very easy, Git for Windows comes with its Bash emulator.\nintro_install.qmd:git config user.email \"your_other@email\"\nintro_intro.qmd:[Slides](intro_intro_slides.html){.btn .btn-outline-primary} [(Click and wait: the presentation might take a few instants to load)]{.inlinenote}\nintro_intro_old.qmd:  - intro_old.html\nintro_intro_old.qmd:&lt;script type=\"text/javascript\" src=\"https://ssl.gstatic.com/trends_nrtr/3045_RC01/embed_loader.js\"&gt;&lt;/script&gt; &lt;script type=\"text/javascript\"&gt; trends.embed.renderExploreWidget(\"TIMESERIES\", {\"comparisonItem\":[{\"keyword\":\"/m/05vqwg\",\"geo\":\"\",\"time\":\"2004-01-01 2022-10-03\"},{\"keyword\":\"/m/08441_\",\"geo\":\"\",\"time\":\"2004-01-01 2022-10-03\"},{\"keyword\":\"/m/012ct9\",\"geo\":\"\",\"time\":\"2004-01-01 2022-10-03\"},{\"keyword\":\"/m/09d6g\",\"geo\":\"\",\"time\":\"2004-01-01 2022-10-03\"}],\"category\":0,\"property\":\"\"}, {\"exploreQuery\":\"date=all&q=%2Fm%2F05vqwg,%2Fm%2F08441_,%2Fm%2F012ct9,%2Fm%2F09d6g\",\"guestPath\":\"https://trends.google.com:443/trends/embed/\"}); &lt;/script&gt;\nintro_intro_old.qmd:git config user.email \"your_other@email\"\nintro_intro_slides.qmd:  - intro_slides.html\nintro_intro_slides.qmd:frontpic: \"img/git_graph.png\"\nintro_intro_slides.qmd:    logo: /img/logo_sfudrac.png\nintro_logs.qmd:+df = pd.read_csv('../data/dataset.csv')\nintro_tags.qmd:git tag J_Climate_2009\nintro_tags.qmd:git show J_Climate_2009\nintro_tags.qmd:git checkout J_Climate_2009\nintro_time_travel.qmd:  - time_travel.html\ntop_intro.qmd:description: An introductory course to version control with &nbsp;[![](img/logo_git.png){width=\"1.3em\" fig-alt=\"noshadow\"}](https://git-scm.com/)\ntop_intro.qmd:[[Start course ➤](intro_intro.qmd)]{.topinline}\ntop_ws.qmd:[Searching a Git project](practice_repo/ws_search.qmd){.card-title-ws .stretched-link}\ntop_ws.qmd:[Collaborating through Git](ws_collab.qmd){.card-title-ws .stretched-link}\ntop_ws.qmd:[Contributing to projects](ws_contrib.qmd){.card-title-ws .stretched-link}\nwb_dvc.qmd:[As DVC is a popular tool in machine learning, **please find this webinar [in the AI section](/ai/wb_dvc.html){.stretched-link}**.]{.btn-redirect}\n\n\nAs you can see, our results also include __init__ which is not what we were looking for. So let’s exclude __:\n\ngit grep -m 3 -e \".*_.*\" --and --not -e \"__\"\n\nBinary file img/01.png matches\nBinary file img/02.png matches\nBinary file img/03.png matches\nBinary file img/04.png matches\nBinary file img/05.png matches\nBinary file img/06.png matches\nBinary file img/07.png matches\nBinary file img/08.png matches\nBinary file img/09.png matches\nBinary file img/10.png matches\nBinary file img/11.png matches\nBinary file img/12.png matches\nBinary file img/13.png matches\nBinary file img/14.png matches\nBinary file img/15.png matches\nBinary file img/16.png matches\nBinary file img/17.png matches\nBinary file img/18.png matches\nBinary file img/19.png matches\nBinary file img/20.png matches\nBinary file img/21.png matches\nBinary file img/22.png matches\nBinary file img/23.png matches\nBinary file img/24.png matches\nBinary file img/25.png matches\nBinary file img/26.png matches\nBinary file img/27.png matches\nBinary file img/28.png matches\nBinary file img/29.png matches\nBinary file img/30.png matches\nBinary file img/31.png matches\nBinary file img/32.png matches\nBinary file img/33.png matches\nBinary file img/34.png matches\nBinary file img/35.png matches\nBinary file img/36.png matches\nBinary file img/37.png matches\nBinary file img/51.png matches\nBinary file img/52.png matches\nBinary file img/53.png matches\nBinary file img/collab.jpg matches\nBinary file img/git_graph.png matches\nBinary file img/gitout.png matches\nBinary file img/logo_git.png matches\nBinary file img/vc.jpg matches\nindex.qmd:  Version control & collaboration with &nbsp;[![](img/logo_git.png){width=\"1.3em\" fig-alt=\"noshadow\"}](https://git-scm.com/)\nindex.qmd:[Getting started with &nbsp;![](img/logo_git.png){width=\"1.2em\" fig-alt=\"noshadow\"}](top_intro.qmd){.card-title2 .stretched-link}\nindex.qmd:[Workshops](top_ws.qmd){.card-title2 .stretched-link}\nintro_documentation.qmd:All these methods lead to the same thing: the manual page corresponding to the command will open in a pager (usually [less](https://en.wikipedia.org/wiki/Less_(Unix))). A pager is a program which makes it easier to read documents in the command line.\nintro_first_steps.qmd:  - first_steps.html\nintro_first_steps.qmd:wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1SJV5mRGexf91lNyFwdS_JmuAXX0xS4pE' -O project.zip\nintro_first_steps.qmd:df = pd.read_csv('../data/dataset.csv')\nintro_install.qmd:Git is built for Unix-like systems (Linux and MacOS). In order to use Git from the command line on Windows, you need a Unix shell such as [Bash](https://en.wikipedia.org/wiki/Bash_(Unix_shell)). To make this very easy, Git for Windows comes with its Bash emulator.\nintro_install.qmd:git config user.email \"your_other@email\"\nintro_intro.qmd:[Slides](intro_intro_slides.html){.btn .btn-outline-primary} [(Click and wait: the presentation might take a few instants to load)]{.inlinenote}\nintro_intro_old.qmd:  - intro_old.html\nintro_intro_old.qmd:&lt;script type=\"text/javascript\" src=\"https://ssl.gstatic.com/trends_nrtr/3045_RC01/embed_loader.js\"&gt;&lt;/script&gt; &lt;script type=\"text/javascript\"&gt; trends.embed.renderExploreWidget(\"TIMESERIES\", {\"comparisonItem\":[{\"keyword\":\"/m/05vqwg\",\"geo\":\"\",\"time\":\"2004-01-01 2022-10-03\"},{\"keyword\":\"/m/08441_\",\"geo\":\"\",\"time\":\"2004-01-01 2022-10-03\"},{\"keyword\":\"/m/012ct9\",\"geo\":\"\",\"time\":\"2004-01-01 2022-10-03\"},{\"keyword\":\"/m/09d6g\",\"geo\":\"\",\"time\":\"2004-01-01 2022-10-03\"}],\"category\":0,\"property\":\"\"}, {\"exploreQuery\":\"date=all&q=%2Fm%2F05vqwg,%2Fm%2F08441_,%2Fm%2F012ct9,%2Fm%2F09d6g\",\"guestPath\":\"https://trends.google.com:443/trends/embed/\"}); &lt;/script&gt;\nintro_intro_old.qmd:git config user.email \"your_other@email\"\nintro_intro_slides.qmd:  - intro_slides.html\nintro_intro_slides.qmd:frontpic: \"img/git_graph.png\"\nintro_intro_slides.qmd:    logo: /img/logo_sfudrac.png\nintro_logs.qmd:+df = pd.read_csv('../data/dataset.csv')\nintro_tags.qmd:git tag J_Climate_2009\nintro_tags.qmd:git show J_Climate_2009\nintro_tags.qmd:git checkout J_Climate_2009\nintro_time_travel.qmd:  - time_travel.html\ntop_intro.qmd:description: An introductory course to version control with &nbsp;[![](img/logo_git.png){width=\"1.3em\" fig-alt=\"noshadow\"}](https://git-scm.com/)\ntop_intro.qmd:[[Start course ➤](intro_intro.qmd)]{.topinline}\ntop_ws.qmd:[Searching a Git project](practice_repo/ws_search.qmd){.card-title-ws .stretched-link}\ntop_ws.qmd:[Collaborating through Git](ws_collab.qmd){.card-title-ws .stretched-link}\ntop_ws.qmd:[Contributing to projects](ws_contrib.qmd){.card-title-ws .stretched-link}\nwb_dvc.qmd:[As DVC is a popular tool in machine learning, **please find this webinar [in the AI section](/ai/wb_dvc.html){.stretched-link}**.]{.btn-redirect}\n\n\n\nFor simple searches, you don’t have to use the -e flag before the pattern you are searching for. Here however, our command has gotten complex enough that we have to use it before each pattern.\n\nLet’s make sure this worked as expected:\n\ngit grep -c \".*_.*\"\necho \"---\"\ngit grep -c \"__\"\necho \"---\"\ngit grep -ce \".*_.*\" --and --not -e \"__\"\n\nimg/01.png:16\nimg/02.png:32\nimg/03.png:31\nimg/04.png:26\nimg/05.png:31\nimg/06.png:32\nimg/07.png:30\nimg/08.png:34\nimg/09.png:35\nimg/10.png:41\nimg/11.png:47\nimg/12.png:40\nimg/13.png:39\nimg/14.png:32\nimg/15.png:38\nimg/16.png:43\nimg/17.png:34\nimg/18.png:35\nimg/19.png:30\nimg/20.png:33\nimg/21.png:40\nimg/22.png:41\nimg/23.png:47\nimg/24.png:64\nimg/25.png:66\nimg/26.png:50\nimg/27.png:60\nimg/28.png:57\nimg/29.png:33\nimg/30.png:39\nimg/31.png:14\nimg/32.png:16\nimg/33.png:18\nimg/34.png:16\nimg/35.png:20\nimg/36.png:18\nimg/37.png:18\nimg/51.png:55\nimg/52.png:46\nimg/53.png:55\nimg/collab.jpg:178\nimg/git_graph.png:121\nimg/gitout.png:42\nimg/logo_git.png:4\nimg/vc.jpg:259\nindex.qmd:4\nintro_documentation.qmd:1\nintro_first_steps.qmd:4\nintro_install.qmd:2\nintro_intro.qmd:1\nintro_intro_old.qmd:8\nintro_intro_slides.qmd:5\nintro_logs.qmd:1\nintro_tags.qmd:5\nintro_time_travel.qmd:1\ntop_intro.qmd:2\ntop_ws.qmd:3\nwb_dvc.qmd:1\n---\nimg/01.png:1\nimg/02.png:2\nimg/03.png:2\nimg/04.png:1\nimg/05.png:3\nimg/06.png:3\nimg/07.png:1\nimg/08.png:1\nimg/09.png:1\nimg/10.png:1\nimg/11.png:1\nimg/12.png:1\nimg/13.png:2\nimg/14.png:2\nimg/15.png:3\nimg/16.png:1\nimg/17.png:1\nimg/18.png:2\nimg/19.png:1\nimg/20.png:1\nimg/21.png:1\nimg/22.png:2\nimg/23.png:4\nimg/24.png:2\nimg/25.png:1\nimg/26.png:1\nimg/27.png:2\nimg/28.png:3\nimg/29.png:2\nimg/30.png:1\nimg/31.png:1\nimg/51.png:1\nimg/52.png:2\nimg/53.png:1\nimg/collab.jpg:1\nimg/git_graph.png:2\nimg/gitout.png:1\n---\nimg/01.png:15\nimg/02.png:30\nimg/03.png:29\nimg/04.png:25\nimg/05.png:28\nimg/06.png:29\nimg/07.png:29\nimg/08.png:33\nimg/09.png:34\nimg/10.png:40\nimg/11.png:46\nimg/12.png:39\nimg/13.png:37\nimg/14.png:30\nimg/15.png:35\nimg/16.png:42\nimg/17.png:33\nimg/18.png:33\nimg/19.png:29\nimg/20.png:32\nimg/21.png:39\nimg/22.png:39\nimg/23.png:43\nimg/24.png:62\nimg/25.png:65\nimg/26.png:49\nimg/27.png:58\nimg/28.png:54\nimg/29.png:31\nimg/30.png:38\nimg/31.png:13\nimg/32.png:16\nimg/33.png:18\nimg/34.png:16\nimg/35.png:20\nimg/36.png:18\nimg/37.png:18\nimg/51.png:54\nimg/52.png:44\nimg/53.png:54\nimg/collab.jpg:177\nimg/git_graph.png:119\nimg/gitout.png:41\nimg/logo_git.png:4\nimg/vc.jpg:259\nindex.qmd:4\nintro_documentation.qmd:1\nintro_first_steps.qmd:4\nintro_install.qmd:2\nintro_intro.qmd:1\nintro_intro_old.qmd:8\nintro_intro_slides.qmd:5\nintro_logs.qmd:1\nintro_tags.qmd:5\nintro_time_travel.qmd:1\ntop_intro.qmd:2\ntop_ws.qmd:3\nwb_dvc.qmd:1\n\n\nThere were 2 lines matching __ in src/test_manuel.py and we have indeed excluded them from our search.\nExtended regular expressions are also covered with the flag -E.\n\n\nSearching other trees\nSo far, we have searched the current version of tracked files, but we can just as easily search files at any commit.\nLet’s search for test in the tracked files 20 commits ago:\n\ngit grep test HEAD~20\n\nHEAD~20:intro_aliases.qmd:Now, let's build an alias for a more complex command: `git grep \"test\" $(git rev-list --all)`. This example\nHEAD~20:intro_aliases.qmd:from the *\"Searching a Git project\"* section below will search for the string \"test\" in all previous\nHEAD~20:intro_aliases.qmd:commits. There are two problems with this command: (1) it takes an argument (the string \"test\"), and (2) it\nHEAD~20:intro_aliases.qmd:git search test\nHEAD~20:intro_aliases.qmd:should search the entire current Git project history for \"test\".\nHEAD~20:intro_branches.qmd:git branch test\nHEAD~20:intro_branches.qmd:git switch test\nHEAD~20:intro_branches.qmd:* test\nHEAD~20:intro_branches.qmd:The `*` shows the branch you are currently on (i.e. the branch to which `HEAD` points to). In our example, the project has two branches and we are on the branch `test`.\nHEAD~20:intro_branches.qmd:git diff main test\nHEAD~20:intro_branches.qmd:When you are happy with the changes you made on your test branch, you can merge it into `main`.\nHEAD~20:intro_branches.qmd:If you have only created new commits on the branch `test`, the merge is called a \"fast-forward merge\" because `main` and `test` have not diverged: it is simply a question of having `main` catch up to `test`.\nHEAD~20:intro_branches.qmd:git merge test\nHEAD~20:intro_branches.qmd:Then, usually, you delete the branch `test` as it has served its purpose:\nHEAD~20:intro_branches.qmd:git branch -d test\nHEAD~20:intro_branches.qmd:Alternatively, you can switch back to `test` and do the next bit of experimental work on it. This allows to keep `main` free of mishaps and bad developments.\nHEAD~20:intro_branches.qmd:Let's go back to our situation before we created the branch `test`:\nHEAD~20:intro_branches.qmd:This time, you create a branch called `test2`:\nHEAD~20:intro_branches.qmd:To merge your branch `test2` into `main`, a new commit is now required. Git will create this new commit automatically. As long as there is no conflict, it is just as easy as a fast-forward merge:\nHEAD~20:intro_branches.qmd:git merge test2\nHEAD~20:intro_branches.qmd:After which, you can delete the (now useless) test branch (with `git branch -d test2`):\nHEAD~20:intro_branches.qmd:&gt;&gt;&gt;&gt;&gt;&gt;&gt; test2\nHEAD~20:intro_intro_old.qmd:The pointer `HEAD`, which normally points to the branch `main` which itself points to latest commit, can be moved around. By moving `HEAD` to any commit, you can revisit the state of your project at that particular version.\nHEAD~20:intro_intro_old.qmd:Instead of working on your branch `main`, you create a test branch and work on it (so `HEAD` is on the branch `test` and both move along as you create commits):\nHEAD~20:intro_intro_old.qmd:When you are happy with the changes you made on your test branch, you decide to merge `main` onto it.\nHEAD~20:intro_intro_old.qmd:Then you do the fast-forward merge from `main` onto `test` (so `main` catches up to `test`):\nHEAD~20:intro_intro_old.qmd:Then, usually, you delete the branch `test` as it has served its purpose (with `git branch -d test`). Alternatively, you can switch back to it and do the next bit of experimental work in it.\nHEAD~20:intro_intro_old.qmd:This allows to keep `main` free of possible mishaps and bad developments (if you aren't happy with the work you did on your test branch, you can simply delete it and Git will clean the commits that are on it but not on `main` during the next garbage collection.\nHEAD~20:intro_intro_old.qmd:You create a test branch and switch to it:\nHEAD~20:intro_intro_old.qmd:To merge your main branch and your test branch, a new commit is now required (note that the command is the same as in the case of a fast-forward merge: `git merge`. Git will create the new commit automatically. As long as there is no conflict, it is just as easy as a fast-forward merge. We will talk later about resolving conflicts).\nHEAD~20:intro_intro_old.qmd:After which, you can delete the (now useless) test branch (with `git branch -d test2`):\nHEAD~20:intro_remotes.qmd:Click on the `Code` green drop-down button, select SSH [if you have set SSH for your GitHub account](https://docs.github.com/en/free-pro-team@latest/github/authenticating-to-github/connecting-to-github-with-ssh) or HTTPS and copy the address.\nHEAD~20:intro_revisiting_old_commits_alternate.qmd:The pointer `HEAD`, which normally points to the branch `main` which itself points to latest commit, can be moved around. By moving `HEAD` to any commit, you can revisit the state of your project at that particular version.\nHEAD~20:intro_undo.qmd:Here is a common scenario: you make a commit, then realize that you forgot to include some changes in that commit; or you aren't happy with the commit message; or both. You can edit your latest commit with the `--amend` flag:\nHEAD~20:ws_collab.qmd:Click on the `Code` green drop-down button, select SSH [if you have set SSH for your GitHub account](https://docs.github.com/en/free-pro-team@latest/github/authenticating-to-github/connecting-to-github-with-ssh) or HTTPS and copy the address.\n\n\n\nAs you can see, the file src/test_manuel.py is not in the results. Either it didn’t exist or it didn’t have the word test at that commit.\n\nIf you want to search tracked files AND untracked files, you need to use the --untracked flag.\nLet’s create a new (thus untracked) file with some content including the word test:\n\necho \"This is a test\" &gt; newfile\n\nNow compare the following:\n\ngit grep -c test\n\nintro_aliases.qmd:5\nintro_branches.qmd:17\nintro_intro_old.qmd:9\nintro_remotes.qmd:1\nintro_revisiting_old_commits_alternate.qmd:1\nintro_undo.qmd:1\nws_collab.qmd:1\n\n\nwith:\n\ngit grep -c --untracked test\n\nindex.html:1\nintro_aliases.html:4\nintro_aliases.qmd:5\nintro_branches.html:18\nintro_branches.qmd:17\nintro_changes.html:1\nintro_documentation.html:1\nintro_first_steps.html:1\nintro_ignore.html:1\nintro_install.html:1\nintro_intro.html:1\nintro_intro_old.qmd:9\nintro_intro_slides.html:18\nintro_logs.html:1\nintro_remotes.html:2\nintro_remotes.qmd:1\nintro_reset.html:1\nintro_resources.html:1\nintro_revisiting_old_commits_alternate.html:2\nintro_revisiting_old_commits_alternate.qmd:1\nintro_stash.html:1\nintro_tags.html:1\nintro_three_trees.html:1\nintro_time_travel.html:1\nintro_tools.html:1\nintro_undo.html:2\nintro_undo.qmd:1\nnewfile:1\ntop_intro.html:1\ntop_ws.html:1\nwb_dvc.html:1\nws_collab.html:2\nws_collab.qmd:1\nws_contrib.html:1\nws_search.rmarkdown:41\n\n\n\nThis last result also returned our untracked file newfile.\n\nIf you want to search untracked and ignored files (meaning all your files), use the flags --untracked --no-exclude-standard.\nLet’s see what the .gitignore file contains:\n\ncat .gitignore\n\ncat: .gitignore: No such file or directory\n\n\nThe directory data is in .gitignore. This means that it is not under version control and it thus doesn’t exist in our repo (since we cloned our repo, we only have the version-controlled files). Let’s create it:\nmkdir data\nNow, let’s create a file in it that contains test:\n\necho \"And another test\" &gt; data/file\n\nbash: line 1: data/file: No such file or directory\n\n\nWe can rerun our previous two searches to verify that files excluded from version control are not searched:\n\ngit grep -c test\n\nintro_aliases.qmd:5\nintro_branches.qmd:17\nintro_intro_old.qmd:9\nintro_remotes.qmd:1\nintro_revisiting_old_commits_alternate.qmd:1\nintro_undo.qmd:1\nws_collab.qmd:1\n\n\n\ngit grep -c --untracked test\n\nindex.html:1\nintro_aliases.html:4\nintro_aliases.qmd:5\nintro_branches.html:18\nintro_branches.qmd:17\nintro_changes.html:1\nintro_documentation.html:1\nintro_first_steps.html:1\nintro_ignore.html:1\nintro_install.html:1\nintro_intro.html:1\nintro_intro_old.qmd:9\nintro_intro_slides.html:18\nintro_logs.html:1\nintro_remotes.html:2\nintro_remotes.qmd:1\nintro_reset.html:1\nintro_resources.html:1\nintro_revisiting_old_commits_alternate.html:2\nintro_revisiting_old_commits_alternate.qmd:1\nintro_stash.html:1\nintro_tags.html:1\nintro_three_trees.html:1\nintro_time_travel.html:1\nintro_tools.html:1\nintro_undo.html:2\nintro_undo.qmd:1\nnewfile:1\ntop_intro.html:1\ntop_ws.html:1\nwb_dvc.html:1\nws_collab.html:2\nws_collab.qmd:1\nws_contrib.html:1\nws_search.rmarkdown:41\n\n\nAnd now, let’s try:\n\ngit grep -c --untracked --no-exclude-standard test\n\nindex.html:1\nintro_aliases.html:4\nintro_aliases.qmd:5\nintro_branches.html:18\nintro_branches.qmd:17\nintro_changes.html:1\nintro_documentation.html:1\nintro_first_steps.html:1\nintro_ignore.html:1\nintro_install.html:1\nintro_intro.html:1\nintro_intro_old.qmd:9\nintro_intro_slides.html:18\nintro_logs.html:1\nintro_remotes.html:2\nintro_remotes.qmd:1\nintro_reset.html:1\nintro_resources.html:1\nintro_revisiting_old_commits_alternate.html:2\nintro_revisiting_old_commits_alternate.qmd:1\nintro_stash.html:1\nintro_tags.html:1\nintro_three_trees.html:1\nintro_time_travel.html:1\nintro_tools.html:1\nintro_undo.html:2\nintro_undo.qmd:1\nnewfile:1\ntop_intro.html:1\ntop_ws.html:1\nwb_dvc.html:1\nws_collab.html:2\nws_collab.qmd:1\nws_contrib.html:1\nws_search.rmarkdown:41\n\n\n\ndata/file, despite being excluded from version control, is also searched.\n\n\n\nSearching all commits\nWe saw that git grep &lt;pattern&gt; &lt;commit&gt; can search a pattern in any commit. Now, what if we all to search all commits for a pattern?\nFor this, we pass the expression $(git rev-list --all) in lieu of &lt;commit&gt;.\ngit rev-list --all creates a list of all the commits in a way that can be used as an argument to other functions. The $() allows to run the expression inside it and pass the result as and argument.\nTo search for test in all the commits, we thus run:\ngit grep \"test\" $(git rev-list --all)\nI am not running this command has it has a huge output. Instead, I will limit the search to the last two commits:\n\ngit grep \"test\" $(git rev-list --all -2)\n\nf1802fb9273fdbaad5fa0f1381ff8b18a84a15ce:intro_aliases.qmd:Now, let's build an alias for a more complex command: `git grep \"test\" $(git rev-list --all)`. This example\nf1802fb9273fdbaad5fa0f1381ff8b18a84a15ce:intro_aliases.qmd:from the *\"Searching a Git project\"* section below will search for the string \"test\" in all previous\nf1802fb9273fdbaad5fa0f1381ff8b18a84a15ce:intro_aliases.qmd:commits. There are two problems with this command: (1) it takes an argument (the string \"test\"), and (2) it\nf1802fb9273fdbaad5fa0f1381ff8b18a84a15ce:intro_aliases.qmd:git search test\nf1802fb9273fdbaad5fa0f1381ff8b18a84a15ce:intro_aliases.qmd:should search the entire current Git project history for \"test\".\nf1802fb9273fdbaad5fa0f1381ff8b18a84a15ce:intro_branches.qmd:git branch test\nf1802fb9273fdbaad5fa0f1381ff8b18a84a15ce:intro_branches.qmd:git switch test\nf1802fb9273fdbaad5fa0f1381ff8b18a84a15ce:intro_branches.qmd:* test\nf1802fb9273fdbaad5fa0f1381ff8b18a84a15ce:intro_branches.qmd:The `*` shows the branch you are currently on (i.e. the branch to which `HEAD` points to). In our example, the project has two branches and we are on the branch `test`.\nf1802fb9273fdbaad5fa0f1381ff8b18a84a15ce:intro_branches.qmd:git diff main test\nf1802fb9273fdbaad5fa0f1381ff8b18a84a15ce:intro_branches.qmd:When you are happy with the changes you made on your test branch, you can merge it into `main`.\nf1802fb9273fdbaad5fa0f1381ff8b18a84a15ce:intro_branches.qmd:If you have only created new commits on the branch `test`, the merge is called a \"fast-forward merge\" because `main` and `test` have not diverged: it is simply a question of having `main` catch up to `test`.\nf1802fb9273fdbaad5fa0f1381ff8b18a84a15ce:intro_branches.qmd:git merge test\nf1802fb9273fdbaad5fa0f1381ff8b18a84a15ce:intro_branches.qmd:Then, usually, you delete the branch `test` as it has served its purpose:\nf1802fb9273fdbaad5fa0f1381ff8b18a84a15ce:intro_branches.qmd:git branch -d test\nf1802fb9273fdbaad5fa0f1381ff8b18a84a15ce:intro_branches.qmd:Alternatively, you can switch back to `test` and do the next bit of experimental work on it. This allows to keep `main` free of mishaps and bad developments.\nf1802fb9273fdbaad5fa0f1381ff8b18a84a15ce:intro_branches.qmd:Let's go back to our situation before we created the branch `test`:\nf1802fb9273fdbaad5fa0f1381ff8b18a84a15ce:intro_branches.qmd:This time, you create a branch called `test2`:\nf1802fb9273fdbaad5fa0f1381ff8b18a84a15ce:intro_branches.qmd:To merge your branch `test2` into `main`, a new commit is now required. Git will create this new commit automatically. As long as there is no conflict, it is just as easy as a fast-forward merge:\nf1802fb9273fdbaad5fa0f1381ff8b18a84a15ce:intro_branches.qmd:git merge test2\nf1802fb9273fdbaad5fa0f1381ff8b18a84a15ce:intro_branches.qmd:After which, you can delete the (now useless) test branch (with `git branch -d test2`):\nf1802fb9273fdbaad5fa0f1381ff8b18a84a15ce:intro_branches.qmd:&gt;&gt;&gt;&gt;&gt;&gt;&gt; test2\nf1802fb9273fdbaad5fa0f1381ff8b18a84a15ce:intro_intro_old.qmd:The pointer `HEAD`, which normally points to the branch `main` which itself points to latest commit, can be moved around. By moving `HEAD` to any commit, you can revisit the state of your project at that particular version.\nf1802fb9273fdbaad5fa0f1381ff8b18a84a15ce:intro_intro_old.qmd:Instead of working on your branch `main`, you create a test branch and work on it (so `HEAD` is on the branch `test` and both move along as you create commits):\nf1802fb9273fdbaad5fa0f1381ff8b18a84a15ce:intro_intro_old.qmd:When you are happy with the changes you made on your test branch, you decide to merge `main` onto it.\nf1802fb9273fdbaad5fa0f1381ff8b18a84a15ce:intro_intro_old.qmd:Then you do the fast-forward merge from `main` onto `test` (so `main` catches up to `test`):\nf1802fb9273fdbaad5fa0f1381ff8b18a84a15ce:intro_intro_old.qmd:Then, usually, you delete the branch `test` as it has served its purpose (with `git branch -d test`). Alternatively, you can switch back to it and do the next bit of experimental work in it.\nf1802fb9273fdbaad5fa0f1381ff8b18a84a15ce:intro_intro_old.qmd:This allows to keep `main` free of possible mishaps and bad developments (if you aren't happy with the work you did on your test branch, you can simply delete it and Git will clean the commits that are on it but not on `main` during the next garbage collection.\nf1802fb9273fdbaad5fa0f1381ff8b18a84a15ce:intro_intro_old.qmd:You create a test branch and switch to it:\nf1802fb9273fdbaad5fa0f1381ff8b18a84a15ce:intro_intro_old.qmd:To merge your main branch and your test branch, a new commit is now required (note that the command is the same as in the case of a fast-forward merge: `git merge`. Git will create the new commit automatically. As long as there is no conflict, it is just as easy as a fast-forward merge. We will talk later about resolving conflicts).\nf1802fb9273fdbaad5fa0f1381ff8b18a84a15ce:intro_intro_old.qmd:After which, you can delete the (now useless) test branch (with `git branch -d test2`):\nf1802fb9273fdbaad5fa0f1381ff8b18a84a15ce:intro_remotes.qmd:Click on the `Code` green drop-down button, select SSH [if you have set SSH for your GitHub account](https://docs.github.com/en/free-pro-team@latest/github/authenticating-to-github/connecting-to-github-with-ssh) or HTTPS and copy the address.\nf1802fb9273fdbaad5fa0f1381ff8b18a84a15ce:intro_revisiting_old_commits_alternate.qmd:The pointer `HEAD`, which normally points to the branch `main` which itself points to latest commit, can be moved around. By moving `HEAD` to any commit, you can revisit the state of your project at that particular version.\nf1802fb9273fdbaad5fa0f1381ff8b18a84a15ce:intro_undo.qmd:Here is a common scenario: you make a commit, then realize that you forgot to include some changes in that commit; or you aren't happy with the commit message; or both. You can edit your latest commit with the `--amend` flag:\nf1802fb9273fdbaad5fa0f1381ff8b18a84a15ce:ws_collab.qmd:Click on the `Code` green drop-down button, select SSH [if you have set SSH for your GitHub account](https://docs.github.com/en/free-pro-team@latest/github/authenticating-to-github/connecting-to-github-with-ssh) or HTTPS and copy the address.\n397ef976e18724c06713ffbf7ebe205b7016a35f:intro_aliases.qmd:Now, let's build an alias for a more complex command: `git grep \"test\" $(git rev-list --all)`. This example\n397ef976e18724c06713ffbf7ebe205b7016a35f:intro_aliases.qmd:from the *\"Searching a Git project\"* section below will search for the string \"test\" in all previous\n397ef976e18724c06713ffbf7ebe205b7016a35f:intro_aliases.qmd:commits. There are two problems with this command: (1) it takes an argument (the string \"test\"), and (2) it\n397ef976e18724c06713ffbf7ebe205b7016a35f:intro_aliases.qmd:git search test\n397ef976e18724c06713ffbf7ebe205b7016a35f:intro_aliases.qmd:should search the entire current Git project history for \"test\".\n397ef976e18724c06713ffbf7ebe205b7016a35f:intro_branches.qmd:git branch test\n397ef976e18724c06713ffbf7ebe205b7016a35f:intro_branches.qmd:git switch test\n397ef976e18724c06713ffbf7ebe205b7016a35f:intro_branches.qmd:* test\n397ef976e18724c06713ffbf7ebe205b7016a35f:intro_branches.qmd:The `*` shows the branch you are currently on (i.e. the branch to which `HEAD` points to). In our example, the project has two branches and we are on the branch `test`.\n397ef976e18724c06713ffbf7ebe205b7016a35f:intro_branches.qmd:git diff main test\n397ef976e18724c06713ffbf7ebe205b7016a35f:intro_branches.qmd:When you are happy with the changes you made on your test branch, you can merge it into `main`.\n397ef976e18724c06713ffbf7ebe205b7016a35f:intro_branches.qmd:If you have only created new commits on the branch `test`, the merge is called a \"fast-forward merge\" because `main` and `test` have not diverged: it is simply a question of having `main` catch up to `test`.\n397ef976e18724c06713ffbf7ebe205b7016a35f:intro_branches.qmd:git merge test\n397ef976e18724c06713ffbf7ebe205b7016a35f:intro_branches.qmd:Then, usually, you delete the branch `test` as it has served its purpose:\n397ef976e18724c06713ffbf7ebe205b7016a35f:intro_branches.qmd:git branch -d test\n397ef976e18724c06713ffbf7ebe205b7016a35f:intro_branches.qmd:Alternatively, you can switch back to `test` and do the next bit of experimental work on it. This allows to keep `main` free of mishaps and bad developments.\n397ef976e18724c06713ffbf7ebe205b7016a35f:intro_branches.qmd:Let's go back to our situation before we created the branch `test`:\n397ef976e18724c06713ffbf7ebe205b7016a35f:intro_branches.qmd:This time, you create a branch called `test2`:\n397ef976e18724c06713ffbf7ebe205b7016a35f:intro_branches.qmd:To merge your branch `test2` into `main`, a new commit is now required. Git will create this new commit automatically. As long as there is no conflict, it is just as easy as a fast-forward merge:\n397ef976e18724c06713ffbf7ebe205b7016a35f:intro_branches.qmd:git merge test2\n397ef976e18724c06713ffbf7ebe205b7016a35f:intro_branches.qmd:After which, you can delete the (now useless) test branch (with `git branch -d test2`):\n397ef976e18724c06713ffbf7ebe205b7016a35f:intro_branches.qmd:&gt;&gt;&gt;&gt;&gt;&gt;&gt; test2\n397ef976e18724c06713ffbf7ebe205b7016a35f:intro_intro_old.qmd:The pointer `HEAD`, which normally points to the branch `main` which itself points to latest commit, can be moved around. By moving `HEAD` to any commit, you can revisit the state of your project at that particular version.\n397ef976e18724c06713ffbf7ebe205b7016a35f:intro_intro_old.qmd:Instead of working on your branch `main`, you create a test branch and work on it (so `HEAD` is on the branch `test` and both move along as you create commits):\n397ef976e18724c06713ffbf7ebe205b7016a35f:intro_intro_old.qmd:When you are happy with the changes you made on your test branch, you decide to merge `main` onto it.\n397ef976e18724c06713ffbf7ebe205b7016a35f:intro_intro_old.qmd:Then you do the fast-forward merge from `main` onto `test` (so `main` catches up to `test`):\n397ef976e18724c06713ffbf7ebe205b7016a35f:intro_intro_old.qmd:Then, usually, you delete the branch `test` as it has served its purpose (with `git branch -d test`). Alternatively, you can switch back to it and do the next bit of experimental work in it.\n397ef976e18724c06713ffbf7ebe205b7016a35f:intro_intro_old.qmd:This allows to keep `main` free of possible mishaps and bad developments (if you aren't happy with the work you did on your test branch, you can simply delete it and Git will clean the commits that are on it but not on `main` during the next garbage collection.\n397ef976e18724c06713ffbf7ebe205b7016a35f:intro_intro_old.qmd:You create a test branch and switch to it:\n397ef976e18724c06713ffbf7ebe205b7016a35f:intro_intro_old.qmd:To merge your main branch and your test branch, a new commit is now required (note that the command is the same as in the case of a fast-forward merge: `git merge`. Git will create the new commit automatically. As long as there is no conflict, it is just as easy as a fast-forward merge. We will talk later about resolving conflicts).\n397ef976e18724c06713ffbf7ebe205b7016a35f:intro_intro_old.qmd:After which, you can delete the (now useless) test branch (with `git branch -d test2`):\n397ef976e18724c06713ffbf7ebe205b7016a35f:intro_remotes.qmd:Click on the `Code` green drop-down button, select SSH [if you have set SSH for your GitHub account](https://docs.github.com/en/free-pro-team@latest/github/authenticating-to-github/connecting-to-github-with-ssh) or HTTPS and copy the address.\n397ef976e18724c06713ffbf7ebe205b7016a35f:intro_revisiting_old_commits_alternate.qmd:The pointer `HEAD`, which normally points to the branch `main` which itself points to latest commit, can be moved around. By moving `HEAD` to any commit, you can revisit the state of your project at that particular version.\n397ef976e18724c06713ffbf7ebe205b7016a35f:intro_undo.qmd:Here is a common scenario: you make a commit, then realize that you forgot to include some changes in that commit; or you aren't happy with the commit message; or both. You can edit your latest commit with the `--amend` flag:\n397ef976e18724c06713ffbf7ebe205b7016a35f:ws_collab.qmd:Click on the `Code` green drop-down button, select SSH [if you have set SSH for your GitHub account](https://docs.github.com/en/free-pro-team@latest/github/authenticating-to-github/connecting-to-github-with-ssh) or HTTPS and copy the address.\n\n\n\nIn combination with the fuzzy finder tool fzf, this can make finding a particular commit extremely easy.\nFor instance, the code below allows you to dynamically search in the result through incremental completion:\ngit grep \"test\" $(git rev-list --all) | fzf --cycle -i -e\nOr even better, you can automatically copy the short form of the hash of the selected commit to clipboard so that you can use it with git show, git checkout, etc.:\ngit grep \"test\" $(git rev-list --all) |\n    fzf --cycle -i -e |\n    cut -c 1-7 |\n    xclip -r -selection clipboard\n\nHere, I am using xclip to copy to the clipboard as I am on Linux. Depending on your OS you might need to use a different tool.\n\nOf course, you can create a function in your .bashrc file with such code so that you wouldn’t have to type it each time:\ngrep_all_commits () {\n    git grep \"$1\" $(git rev-list --all) |\n        fzf --cycle -i -e |\n        cut -c 1-7 |\n        xclip -r -selection clipboard\n}\nAlternatively, you can pass the result directly into whatever git command you want to use that commit for.\nHere is an example with git show:\ngit grep \"test\" $(git rev-list --all) |\n    fzf --cycle -i -e |\n    cut -c 1-7 |\n    git show\nAnd if you wanted to get really fancy, you could go with:\ngit grep \"test\" $(git rev-list --all) |\n    fzf --cycle -i -e --no-multi \\\n        --ansi --preview=\"$_viewGitLogLine\" \\\n        --header \"enter: view, C-c: copy hash\" \\\n        --bind \"enter:execute:$_viewGitLogLine | less -R\" \\\n        --bind \"ctrl-c:execute:$_gitLogLineToHash |\n        xclip -r -selection clipboard\"\nWrapped in a function:\ngrep_all_commits_preview () {\n    git grep \"$1\" $(git rev-list --all) |\n        fzf --cycle -i -e --no-multi \\\n            --ansi --preview=\"$_viewGitLogLine\" \\\n            --header \"enter: view, C-c: copy hash\" \\\n            --bind \"enter:execute:$_viewGitLogLine |\n              less -R\" \\\n            --bind \"ctrl-c:execute:$_gitLogLineToHash |\n        xclip -r -selection clipboard\"\n}\nThis last function allows you to search through all the results in an incremental fashion while displaying a preview of the selected diff (the changes made at that particular commit). If you want to see more of the diff than the preview displays, press &lt;enter&gt; (then q to quit the pager), if you want to copy the hash of a commit, press C-c (Control + c).\nWith this function, you can now instantly get a preview of the changes made to any line containing an expression for any file, at any commit, and copy the hash of the selected commit. This is really powerful.\n\n\n\nAliases\nIf you don’t want to type a series of flags all the time, you can configure aliases for Git. For instance, Alex Razoumov uses the alias git search for git grep --break --heading -n -i.\nLet’s add to it the -p flag. Here is how you would set this alias:\ngit config --global alias.search 'grep --break --heading -n -i -p'\n\nThis setting gets added to your main Git configuration file (on Linux, by default, at ~/.gitconfig).\n\nFrom there on, you can use your alias with:\n\ngit search test\n\ngit: 'search' is not a git command. See 'git --help'."
  },
  {
    "objectID": "git/ws_search.html#searching-logs",
    "href": "git/ws_search.html#searching-logs",
    "title": "Searching a version-controlled project",
    "section": "Searching logs",
    "text": "Searching logs\nThe second thing that can happen is that you are looking for some pattern in your version control logs.\n\ngit log\ngit log allows to get information on commit logs.\nBy default, it outputs all the commits of the current branch.\nLet’s show the logs of the last 3 commits:\n\ngit log -3\n\ncommit f1802fb9273fdbaad5fa0f1381ff8b18a84a15ce\nAuthor: Marie-Helene Burle &lt;marie.burle@westdri.ca&gt;\nDate:   Wed Apr 24 12:26:52 2024 -0700\n\n    update site\n\ncommit 397ef976e18724c06713ffbf7ebe205b7016a35f\nAuthor: Marie-Helene Burle &lt;marie.burle@westdri.ca&gt;\nDate:   Wed Apr 24 12:26:43 2024 -0700\n\n    styles: improve callouts\n\ncommit e24aebab9ca82555effde05942503ba677df36e3\nAuthor: Marie-Helene Burle &lt;marie.burle@westdri.ca&gt;\nDate:   Wed Apr 24 12:25:09 2024 -0700\n\n    minor edits numpy\n\n\nThe output can be customized thanks to a plethora of options.\nFor instance, here are the logs of the last 15 commits, in a graph, with one line per commit:\n\ngit log --graph --oneline -n 15\n\n* f1802fb9 update site\n* 397ef976 styles: improve callouts\n* e24aebab minor edits numpy\n* 212577bc minor edit benchmark\n* 84337761 update site\n* b44b9816 big improvements benchmark\n* 45cd8e29 update site\n* 4d9e0e27 correct static function explanation + make graph bigger\n* 949277e2 jx numpy: change order headers\n* e0ccfb29 update site\n* e79d100d edit jax parallel\n* 5e7e0a0b update site\n* 7b202e6f small edits to jax parallel\n* 66f50cb4 add jax parallel to navbar\n* c9d7da6e update site\n\n\nBut git log has also flags that allow to search for patterns.\n\n\nSearching commit messages\nOne of the reasons it is so important to write informative commit messages is that they are key to finding commits later on.\nTo look for a pattern in all your commit messages, use git log --grep=&lt;pattern&gt;.\nLet’s look for test in the commit messages and limit the output to 3 commits:\n\ngit log --grep=test -3\n\ncommit 6f07fd90be8045378b482d5ca0175446b42797c8\nAuthor: Marie-Helene Burle &lt;marie.burle@westdri.ca&gt;\nDate:   Tue Feb 27 20:32:43 2024 -0800\n\n    add test csv data file into the site\n\ncommit 7167606e3188e9497768761963af0c4bdc7aad90\nAuthor: Marie-Helene Burle &lt;marie.burle@westdri.ca&gt;\nDate:   Tue Feb 27 20:16:20 2024 -0800\n\n    add test csv data file\n\ncommit 87f11c6715a5da31888dc6b92645156e6738d207\nAuthor: Marie-Helene Burle &lt;marie.burle@westdri.ca&gt;\nDate:   Mon Dec 18 14:06:51 2023 -0800\n\n    test blockquote media for phones\n\n\nFor a more compact output:\n\ngit log --grep=\"test\" -3 --oneline\n\n6f07fd90 add test csv data file into the site\n7167606e add test csv data file\n87f11c67 test blockquote media for phones\n\n\n\nHere too you can use this in combination to fzf with for instance:\ngit log --grep=\"test\" | fzf --cycle -i -e\nOr:\ngit log --grep=\"test\" --oneline |\n    fzf --cycle -i -e --no-multi \\\n        --ansi --preview=\"$_viewGitLogLine\" \\\n        --header \"enter: view, C-c: copy hash\" \\\n        --bind \"enter:execute:$_viewGitLogLine | less -R\" \\\n        --bind \"ctrl-c:execute:$_gitLogLineToHash |\n        xclip -r -selection clipboard\"\n\n\n\nChanges made to a pattern\nRemember that test was present in the file src/test_manuel.py. If we want to see when the pattern was first created and then each time it was modified, we use the -L flag in this fashion:\ngit log -L :&lt;pattern&gt;:file\nIn our case:\n\ngit log -L :test:src/test_manuel.py\n\nfatal: There is no path git/src/test_manuel.py in the commit\n\n\nThis is very useful if you want to see, for instance, changes made to a function in a script.\n\n\nChanges in number of occurrences of a pattern\nNow, if we want to list all commits that created a change in the number of occurrences of test in our project, we run:\n\ngit log -S test --oneline\n\n84337761 update site\n45cd8e29 update site\n93bb9017 add jax benchmark section\n112a5403 update site\n1c85d9fe edits to install\n5a3afe3a jax parallel: remove asynchronous dispatch moved to benchmarking section\n7da77a9a update site\nf0f936cc finish jax webinar slides presentation and embed resources\na847af69 update site\ncd5ac347 update site after embedding resources JAX webinar slides\n1639488f update site\ne5b6373a update site\nfc1e5925 improve jax jit\nc8da0b3d update site\nd395fe8b add intro to ipython + better formatting with tabset (rather than columns)\nb1e330ba add JAX section on jobs\n67a51585 add JAX section on installation\n361c27bf update title and abstract jax webinar\neba13650 update title and abstract jax course\nf84c38bc update site\n8877228e update site\ncd4d8869 update site\n4a6b96cb update site\nb3e68d9b update site\nf9abd0f1 update site\nf162c6a9 update site\nb0f28e7b update site\ncc696002 update site\n7f4d8801 update site\n373d2f49 add content in intro ml nlp slides\n93503a04 update site after full render\n0fa44f29 update site\ned3df92b update site\n8976cb6a update site\n7b1d418e update site\nc813fca7 update site\n4c2cca59 update site\n4ec83107 update\n289d7eee update\n2e9865d5 update\ndb5eebf7 update\nc59f0926 update\n84016b9c update\n62af7b93 update\n88bcbaba update\n428ee1db update\n97cf5731 update\n27bb78e3 add qmd files from molecules\n8e70f8b6 update site\ne395a204 update\nc0a93fbd add prefix (intro_, ws_, wb_) to bash, git, and tools sections\n63ba55fc update\ndbded239 update\n1ca5c158 update\n79ebb58b minor fixes dvc slides\nac8afe70 update\n525ca03c dvc slides before another big change\nd287f33c update\na6dc00eb update\na4e14d34 update\n91d6403f update\na38386a2 update\n413a323d update\ne126a321 update after render\nfee353f5 update\ne934a832 update site\n7e6fe93b update site after render\n10277778 update\ne2640dc6 update\n69ab00ba update site\n0cd6525f update site\n484054e8 update site\n7d4a19bb finish draft stateless\nf4bae193 update site\nc2aadc75 minor edits\ne9f983a8 update site\n8aa9f4e5 finish quick draft of parallel section\n719274ed update site\nb648de49 combine datasets loading from 3 options in one section\nba698653 update site\n169845ab version with: loading datasets with Hugging Face\n5376cca0 update site\n7aaf321f add state draft\n4d98a5b0 update site after full render\n4fbc9736 update site\nfbad50d0 update site\n7c19d98d update site\n92dad162 update site\n4a5ef1ea update site\nf763bc1a update site\n3b955ea5 update site\n27e1a097 update site\n3158ff17 update site\n7c132d5e update site\n141204ed add flax abstract\n52152ef3 update JAX abstract after removing dl part with flax\n8f250d57 update site\n255b3205 update freeze\nb61b6df9 update site\n99a77db3 finish jit draft\n57d56299 update site\nd7e0bcb9 finish jx numpy section\nd344ad8a update site\n858e5422 JAX: big revamp course structure\n62c400b2 update site\n1b2e846f jx principles: add async dispatch\n8c8116e0 update site\n09080851 update site\n8f32ea61 update site\ncd78f159 udpate site\n4a003c52 update site\n4b3bec75 update site\ne8e73865 update site\nc545c684 update site\n0b628db2 finish hpc data partition chapter\n1a5c59f7 save a version of hpc_partition before modifying it\n08552b77 update site\nc7fccd5b update site\nd85a0541 edit running htop on local machine\nb5fa9d35 update site\nacb1a4ff rename and heavily modify the foreach chapter\n4536fc63 add pics\n7c318e74 f4 and f5: do testing on the cluster and remove quarto comment\n2aa72b9b update site\n22192dbf add example releasing memory\n08d8f896 add jx profiling page\na841844f improve hpc optimization\nbde35ec8 improve intro indexing\nf1780432 jx why: replace gtrend embedded by img (keeps breaking), improve graph color, add abstract, add a bit of content\n78c3b7b9 add colors to jax intro diagrams\nfb5662b6 add a number of jx early drafts\nc54f6ed7 add new jx sections and update site using a virtual env for Python\ndbcfca2a add prefix for various sections in julia and r, prevent old and bk files from being executed/rendered, prevent webscraping files from being created\n43b79abd update site\n64ddfd7d small edits to hpc r before course\n9204ff32 add jax top intro\n09f64aee add info on profiling\n0cce7374 move jax below PyTorch as it is a more advanced course\nf33135e1 update gcc and r module versions\n96c23b77 update site\n279f6937 update site after full render\n084d5221 embed resources\nae9b67be update site\n4e9c611e more improvements and little tweaks frameworks slides. Add more info\nd58ae469 update site\n445bb296 improvements frameworks slides\n21882057 update site\nfd17244c update formatting framework slides\n185e051a update site\nbead0b06 many tweaks of formatting\nc68c4611 first very rough draft of frameworks slides\n09f1fe51 replace mermaid diagram in file system exercise with a graphviz one\nc6d4a350 filesystem: add exercise with a diagram\n2798c6a1 minor fix\n7a08552d update skl workflow\nac76eea3 update site\n8a504230 add sklearn workflow\nea800fde add an sklearn serie\nc7ac7301 update site\nccf1a85c added content to aliases.qmd\nf17c8c95 added aliases.qmd\ndc7aae4e update site\n74a4e08b finish logs\n0ffe3e1a add logs draft\n23827256 add project.zip\nc8afe95a add downloadthis extension\nfe3f956d add abstract to documentation section\nbb145734 edits intro slides\n40571641 embed resources intro slides\nffa2c749 total revamp of git intro with simply link to slides\n28032012 update ml course\n043c6cf4 minor edits r course\n4888db47 edit sections on how to run r\ne16c9467 update site\n44ceba8e update site\n0ece9c2e update site\n51b8534e replace old webscraping (Python) workshop to new version from DHSI\nfecc161b add webscraping old to gitignore\n4bb8faf0 little improvements web scraping R\nb784f17d update site\n91fa0247 update site\n3bbe8ac5 add (bad) intro blurb to Python course\n3a3361af update site\nf589a660 rename the ext section into talks\n73dfeda0 improvements collections section\n6617a5bc update site\n622631fc fix and improve pandas section\n3eaacd19 update site\nf8c47d0a add index for new big section (talks)\ne29ae75f update site\n9e6a1b8a edit scripts\n0f60d0db finish redirections\n583f27c1 add filesystem section\na7c08558 gis slides: fix typo\nfec32a19 makie: add content in html below video\n954eb10d makie slides: minor improvements\n90adee86 more info in workflow section\n2250d59a minor edits workflow\n39f737d4 add workflow section\n2cd8c6f3 update navbar by moving data, model, and training in a single section\n5f33a182 make backup of autograd in autograd_old and start to make new version of autograd (not complete)\nfc853e13 some edits to training, but still not complete\n01f0f732 finish tensor section\n24b059f9 finalize parallel loops\ne5401892 again many changes to parallel loops before changing yet again\n1428b5fa many changes to parallel loops before making yet many new alternative changes\n6abe68e0 move copy on modify from basics to indexing and make it better\nfd1d1408 update site\n0649af93 move concepts to reading and create a new intro section with slides\nba7938d1 update site\n2525e9e8 finish function section\nb71c7614 finish control flow section\n7f226584 finalize plotting section\n302627a3 add plotting section\na477bd4d add publishing page with links to quarto workshop and webinar\na30a0759 add data structures section\nc5a92ed6 add blurb basics\n68489a39 basics: change title + move a lot of content into section specific pages\n58e586ec packages: add blurb\n848f4362 update navbar\nb7284048 minor edits bash intro\n075dd527 update site\na519d857 create wildcards section\n42f51d4a rename file from search to find\n7df872bf add videos: 4 workshops for HSS series + staff to staff webinar + regular webinar\nb2e565c7 update site\n590bb505 quarto: add installation links\neca80cdb make slides less wordy based on the s2s webinar given on quarto\n43d1ea7f update site\nf424bba7 add slides for quarto staff to staff webinar\n1f33a777 update site\n99c7e56f add new minor optimization\ndd71a56a turn the parallel loop lesson from the webinar version to the workshop version using batch jobs\n07871c1f add 2nd optimization by louis\n55e7c61a update site\nabb1ced7 update site\nf758b8c1 update site\n61f3b8ec add function suggested by workshop participant\n649f8258 update site\nbafb7696 remove profiler from performance section\n176e5ba4 finish optimization section\n3d42df1c add section on memory\n1ba9cc4a important commit: remove \"avoid type conversions\" in R hpc optimizations section as this doesn't change the timing consistently\nb64856dd first draft optimizations\na563fa97 re-render site\n95086e88 update site\n473d722b many edits r resource page\ne0ab5c78 tweak all heading levels\n934f9a86 update site\n37cef298 remove front page for workshops and webinars + add logo image for front page for intro and hpc courses + re-shuffle a few sections + move most ml topics into a course + minor edits (abstract, etc.)\nfc82cf5d update site\n3bcf2df8 rename first git section of git course to match structure of other topics\n628525d5 fix how to download bash data\n952d027c create front pages for Bash and cards on main topic page\nc6ea3e02 add buttons to r main page\nf3e70692 several improvements to web scraping\nc5258d5f split parallel r section into 3 section and add improvements and edits\n046b607d minor fixes hss slides\ncda4fc9d add missing image and very minor edits ml hss slides\nb9f8b5f5 embed resources intro hss ml slides\n898cdd93 update site\n4c07296e add intro ml for hss slides\n38a825da many formatting edits all reveal.js presentations\n8688a717 replace workshop by webinar in all webinars\n20224c1e update site\n50fcdc72 finish script section (shortened. Need to add more content)\n5ef06d8d finish function section\nbf2653fa move control flow, script, and search to molecules folder\nbad3da30 transfer: add globus and abstract\ne7412c33 finish redirections and move it to the molecules repo to run code\n23bed49d add html_children\n9ff42fbe add html section\n9c2f06ef add delay at each iteration to reduce risk of being blocked\n246c64d1 web scraping: minor edits and improvements\n0d08979d add explanations and comments web scraping\n4599ed35 disable cache for webscraping as it conflicts with rvest\n2703353f minor edit nav titles\n5cd69306 change rstudio server time to 1.5h and remove jh option image as it is not the right one\n8159f9b2 minor edits: add some explanations, improve code a bit\nb78b4b6d add first decent draft webscraping with more or less all code and some explanations\n1c251034 add alliance wiki page for r in intro hss resources\ncf851391 first draft bash redirections and pipes\n03b9e332 first draft bash script\n20a392b6 add a little content to intro r\n5c3cfb03 add 2 new sections (not covered by alex)\nfb69b01d add draft content to intro hss r\n6c6d0cf3 add bash empty chapters for online course\nb900e688 intro hpc slides by re-embeding resources\n66bca7e2 fix typo git search\nd72b1706 finish hpc r slides\n1f14f6be open link to hpc slides\n95f20745 finish control flow chapter\n12d248d8 add many little things in list and make a correction for strings\nc9d2e1dd update site\na65bd054 remove out of the package section everything that can live elsewhere\n173d128f move content jh instructions to a new chapter on running Python\nb259e525 remove alex acknowledgement\nb4b87e69 minor edit git front page\nbb2c6fe2 add acknowledgement of alex content\ndd132ab7 supress redundancy between basic and functions\n6f2f0ae4 finish list section (Python)\ndb33cd1f finish basic chapter (Python)\n7ac57447 first draft collections\n04db2fcc edit basics\nfd677ff1 add first draft Python intro hpc copied from Julia. Still needs lots of work\n7464d141 add first draft Python functions (not far from ready)\n1ccb1569 add first draft web scraping in Python\n663e401f yml: uncomment Python tab and add first two Python workshops\n4450be7c add alias\n479f1aa6 many small edits search + add fzf example for searching the logs\n2556ab4e update site\nc53e0f51 add a big info block with more fancy searches using fzf\n1752db62 first final version of search workshop\nfab10006 minor updates hpc r slides\n534a0da6 first draft hpc in r slides\n2f647bd8 remove unnecessary jupyter: python3 in 2 ml revealjs\n919d2061 git lessons: adjust img new names + fixes, corrections, improvements\n7a3d5b13 rename all git diagram img in some sensible order\n16ca4f6a finish branches and add it to the nav\n2725f415 improve front page image\n115aa01f edits many julia files: remove unnecessary jupyter: julia-1.8, small additions, small fixes, small improvements\n9db0e744 add preliminary draft of branches (git) and commented out nav entry\n4ca06a6a add distributed (julia)\na181d66f add symlink for search.qmd which is in the nested git repo\nbdb30cb3 edit control flow\nda7332f8 add remotes\n774a9d45 rename git main workshop\n8f8eb031 add tags and its img\nc1d8385f add multithreading\na31c5aad edits, additions, fixes\n2ba97085 remove from basics elements moved to other files\n8472020c remove from intro hpc everything that is intro julia (move it in various other sections)\na8f09800 turn arrays to collections and add content\n0cc0c59a add julia functions\n1d65319d add julia control flow\n5ec2bccf add julia basics\n22494671 add julia types\n4b09f00e add julia performance\n55813908 add non interactive julia\nf9dd47ca add julia arrays\nea059d51 edit paths and remove shadow img three trees\n38e370fe add undo\n29eb1ac1 add tools\n81fa8648 add stashing\n73ee1b14 add ignore\nd8970df8 add three trees of git\n8148a866 move all top levels to h2 instead of h1 following chat with quarto developpers\n4637b686 julia intro: change header levels + reformat all code blocks\nc072c6c9 add packages\n1601205a add r resources\n3e81b34a add contrib workshop\n5cc2efa0 move section about collaboration through git to new workshop\n96108ed4 uncomment grid section with wider body width\ncddb972c add ml hpc.qmd\n6fa885a4 add quarto link to about\n08e488af make all page start with h1 instead of h2 + add author where missing + move intro to def block\nf09e60bf update site\n9f97ec82 add 5 new ml workshops\n50a465a0 add note about revealjs presentations slow to load in all links\n2a6a3faf improvement to mnist: small additions + run code\ne2056339 add choosing frameworks\n2e281131 add concept workshop in ml\n68183ee7 add autograd to workshops\n8e6c61c4 add mnist to workshops\ne8b21668 add intro scripting workshop\n6e5faa1f add julia intro hpc workshop\n2ab3deb6 add julia covid plotting\n7e19653b add torchtensor slides\n515a4ddb fix R logo (not good on light bg) and add logos for all other sections\n6a781ea0 rename ml intro hss\n06496189 finish formatting upscaling slides\n44bc8530 reduce high res pics upscaling because GitHub's limit is reach with revealjs with self embedded option\n7989e1a5 add gis mapping slides\n0454b612 add upscaling slides\nd9e4e106 turn link to slides into buttons\n82356532 add _site to vc to solve publish issue on GitHub\neb7f3b17 delete publish.yml for GitHub actions\n82781458 update freeze\nf7f08ca0 update freeze\n74174941 update freeze\nccc30a73 update freeze\n9ef79da4 update freeze\nf23a4123 remove in code lengthy comment and add note instead\nb75bf0d5 add custom title-slide.html with partial template for revealjs title slide\n16ebe722 re-add makie slides *after* having rebuilt the site with freeze true\n348b7acb remove makie slides\nf131770c makie webinar: re-add slides\n36539180 remove makie slides for now for gh actions to build\necda9e1d update freeze with julia makie slides\nff2ec56b add makie slides\n6d406585 front page: switch buttons to cards and readjust content accordingly\nfc203393 update freeze\n4ceac573 add outputs of quarto demos so as not to have to run them all the time (annoying with latex). works with blocking rendering of that dir in yml\n0bbc3117 update freeze with computations from r the basics\n71c4ab68 add r the basics from autumn school 22 to r workshops\nb2e73700 add all quarto example files\n358bfb88 add quarto webinar\n818cb8c7 first commit with _freeze (for the quarto examples)\n8b21abf0 add all ml webinars\nfeafdc57 front page: finalize title and add aside about main site\n13dad000 big changes to about page\n3dd050c0 add publish.yml file for GitHub actions\nb6fc959e add 2022_git_sfu.qmd\n\n\nThis can be useful to identify the commit you need."
  },
  {
    "objectID": "git/ws_search.html#tldr",
    "href": "git/ws_search.html#tldr",
    "title": "Searching a version-controlled project",
    "section": "TL;DR",
    "text": "TL;DR\nHere are the search functions you are the most likely to use:\n\nSearch for a pattern in the current version of your tracked files:\n\ngit grep &lt;pattern&gt;\n\nSearch for a pattern in your files at a certain commit:\n\ngit grep &lt;pattern&gt; &lt;commit&gt;\n\nSearch for a pattern in your files in all the commits:\n\ngit grep &lt;pattern&gt; $(git rev-list --all)\n\nSearch for a pattern in your commit messages:\n\ngit log --grep=&lt;pattern&gt;\nNow you should be able to find pretty much anything in your projects and their histories."
  },
  {
    "objectID": "git/ws_collab.html",
    "href": "git/ws_collab.html",
    "title": "Collaborating through Git & GitHub",
    "section": "",
    "text": "Using Internet hosting services such as GitHub, Git is a powerful collaboration tool.\nIn this workshop, we will cover the three classic collaboration situations and see how a collaborative workflow works.",
    "crumbs": [
      "Git",
      "<b><em>Workshops</em></b>",
      "Collaborating through Git"
    ]
  },
  {
    "objectID": "git/ws_collab.html#three-situations",
    "href": "git/ws_collab.html#three-situations",
    "title": "Collaborating through Git & GitHub",
    "section": "Three situations",
    "text": "Three situations\nWhen you collaborate on a project through Git and a remote such as GitHub, there are three situations:\n\nyou create a project on your machine and want others to contribute to it (1),\nyou want to contribute to a project started by others and\n\nyou have write access to it (2),\nyou do not have write access to it (3).\n\n\n\n(1) You start the project\nIn this first situation, you are the author of a project (you have a project under version control on your own machine) and you want to initiate a collaboration with others on it using GitHub as a remote.\n\nCreate a remote on GitHub\nYou need to create a remote on GitHub.\n\nCreate a free GitHub account\nIf you don’t already have one, sign up for a free GitHub account.\n\nTo avoid having to type your password all the time, you should set up SSH for your account.\n\n\n\nCreate an empty repository on GitHub\n\nGo to the GitHub website, login, and go to your home page.\nLook for the Repositories tab & click the green New button.\nEnter the name you want for your repo, without spaces.\nMake the repository public or private.\n\n\n\nLink empty repository to your repo\nClick on the Code green drop-down button, select SSH if you have set SSH for your GitHub account or HTTPS and copy the address.\nIn the command line, cd inside your project, and add the remote:\ngit remote add &lt;remote-name&gt; &lt;remote-address&gt;\nremote-name is a convenience name to identify that remote. You can choose any name, but since Git automatically call the remote origin when you clone a repo, it is common practice to use origin as the name for the first remote.\n\nExample (using an SSH address):\n\ngit remote add origin git@github.com:&lt;user&gt;/&lt;repo&gt;.git\n\nExample (using an HTTPS address):\n\ngit remote add origin https://github.com/&lt;user&gt;/&lt;repo&gt;.git\nIf you don’t want to grant others write access to the project, and you only accept contributions through pull requests, you are set.\nIf you want to grant your collaborators write access to the project however, you need to add them to it.\n\n\n\nInvite collaborators\n\nGo to your GitHub project page.\nClick on the Settings tab.\nClick on the Manage access section on the left-hand side (you will be prompted for your GitHub password).\nClick on the Invite a collaborator green button.\nInvite your collaborators with one of their GitHub user name, their email address, or their full name.\n\n\n\n\n(2) Write access to project\nIn this second situation, someone else started a project and they are inviting you to collaborate to it, giving you write access to the project.\nIn this case, you need to clone the project: cd to the location where you want your local copy, then:\ngit clone &lt;remote-address&gt; &lt;local-name&gt;\nThis sets the project as a remote to your new local copy and that remote is automatically called origin.\nWithout &lt;local-name&gt;, the repo will have the name of the last part of the remote address.\n\n\n(3) No write access to project\nIn this third situation, someone else started a project and you want to collaborate to it, but you do not have write access to it.\nIn this case, you will have to submit pull requests.\nHere is the workflow for a pull request (PR):\n\nFork the project on GitHub.\nClone your fork on your machine.\nAdd the initial project as a second remote & call it upstream.\nPull from upstream to update your local project.\nCreate & checkout a new branch.\nMake & commit your changes on that branch.\nPush that branch to your fork (i.e. origin — remember that you do not have write access to upstream).\nGo to the original project GitHub’s page & open a pull request.",
    "crumbs": [
      "Git",
      "<b><em>Workshops</em></b>",
      "Collaborating through Git"
    ]
  },
  {
    "objectID": "git/ws_collab.html#collaborative-workflow",
    "href": "git/ws_collab.html#collaborative-workflow",
    "title": "Collaborating through Git & GitHub",
    "section": "Collaborative workflow",
    "text": "Collaborative workflow\n\nPulling and pushing\nWhen you collaborate with others using GitHub (or other remote), you and others will work simultaneously on some project. How does this work?\nTo upload your changes to the remote on GitHub you push to it with git push.\nIf one of your collaborators has made changes to the remote (pushing from their own local version of the project), you won’t be able to push. Instead, you will get the following message:\nTo xxx.git\n ! [rejected]        main -&gt; main (fetch first)\nerror: failed to push some refs to 'xxx.git'\nhint: Updates were rejected because the remote contains work that you do\nhint: not have locally. This is usually caused by another repository pushing\nhint: to the same ref. You may want to first integrate the remote changes\nhint: (e.g., 'git pull ...') before pushing again.\nhint: See the 'Note about fast-forwards' in 'git push --help' for details.\nThe solution?\nYou first have to download (git pull) their work onto your machine, merge it with yours (which will happen automatically if there are no conflicts), before you can push your work to GitHub.\nNow… what if there are conflicts?\n\n\nResolving conflicts\n\n\nFrom crystallize.com\n\nGit works line by line. As long as your collaborators and you aren’t working on the same line(s) of the same file(s) at the same time, there will not be any problem. If however you modified one or more of the same line(s) of the same file(s), Git will not be able to decide which version should be kept. When you git pull their work on your machine, the automatic merging will get interrupted and Git will ask you to resolve the conflict(s) before the merge can resume. It will conveniently tell you which file(s) contain the conflict(s).\nThere are fancy tools to resolve conflicts, but you can do it in any text editor: simply open the file(s) listed by Git as having conflicts and look for the following markers:\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\nThis is your version.\n=======\nThis is the alternative version of the same section of the file.\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; alternative version\nThese markers are added by Git to signal the areas of conflict. It is up to you to choose between the two versions (or create a third one) and remove the conflict markers. After that, you can stage the file(s) which contained the conflicts to finish the merge (and then you can commit).",
    "crumbs": [
      "Git",
      "<b><em>Workshops</em></b>",
      "Collaborating through Git"
    ]
  },
  {
    "objectID": "git/top_ws.html",
    "href": "git/top_ws.html",
    "title": "Git workshops",
    "section": "",
    "text": "Searching a Git project\n\n\n\n\nCollaborating through Git\n\n\n\n\nContributing to projects",
    "crumbs": [
      "Git",
      "<b><em>Workshops</em></b>"
    ]
  },
  {
    "objectID": "git/practice_repo/ws_search.html",
    "href": "git/practice_repo/ws_search.html",
    "title": "Searching a version-controlled project",
    "section": "",
    "text": "What is the point of creating all these commits if you are unable to make use of them because you can’t find the information you need in them?\nIn this workshop, we will learn how to search:\n\nyour files (at any of their versions) and\nyour commit logs.\n\nBy the end of the workshop, you should be able to retrieve anything you need from your versioned project.",
    "crumbs": [
      "Git",
      "<b><em>Workshops</em></b>",
      "Searching a Git project"
    ]
  },
  {
    "objectID": "git/practice_repo/ws_search.html#installation",
    "href": "git/practice_repo/ws_search.html#installation",
    "title": "Searching a version-controlled project",
    "section": "Installation",
    "text": "Installation\nMacOS & Linux users:\nInstall Git from the official website.\nWindows users:\nInstall Git for Windows. This will also install “Git Bash”, a Bash emulator.",
    "crumbs": [
      "Git",
      "<b><em>Workshops</em></b>",
      "Searching a Git project"
    ]
  },
  {
    "objectID": "git/practice_repo/ws_search.html#using-git",
    "href": "git/practice_repo/ws_search.html#using-git",
    "title": "Searching a version-controlled project",
    "section": "Using Git",
    "text": "Using Git\nWe will use Git from the command line throughout this workshop.\nMacOS users:    open “Terminal”.\nWindows users:   open “Git Bash”.\nLinux users:    open the terminal emulator of your choice.",
    "crumbs": [
      "Git",
      "<b><em>Workshops</em></b>",
      "Searching a Git project"
    ]
  },
  {
    "objectID": "git/practice_repo/ws_search.html#practice-repo",
    "href": "git/practice_repo/ws_search.html#practice-repo",
    "title": "Searching a version-controlled project",
    "section": "Practice repo",
    "text": "Practice repo\n\nGet a repo\nYou are welcome to use a repository of yours to follow this workshop. Alternatively, you can clone a practice repo I have on GitHub:\n\nNavigate to an appropriate location:\n\ncd /path/to/appropriate/location\n\nClone the repo:\n\n# If you have set SSH for your GitHub account\ngit clone git@github.com:prosoitos/practice_repo.git\n# If you haven't set SSH\ngit clone https://github.com/prosoitos/practice_repo.git\n\nEnter the repo:\n\ncd practice_repo",
    "crumbs": [
      "Git",
      "<b><em>Workshops</em></b>",
      "Searching a Git project"
    ]
  },
  {
    "objectID": "git/practice_repo/ws_search.html#searching-files",
    "href": "git/practice_repo/ws_search.html#searching-files",
    "title": "Searching a version-controlled project",
    "section": "Searching files",
    "text": "Searching files\nThe first thing that can happen is that you are looking for a certain pattern somewhere in your project (for instance a certain function or a certain word).\n\ngit grep\nThe main command to look through versioned files is git grep.\nYou might be familiar with the command-line utility grep which allows to search for lines matching a certain pattern in files. git grep does a similar job with these differences:\n\nit is much faster since all files under version control are already indexed by Git,\nyou can easily search any commit without having to check it out,\nit has features lacking in grep such as, for instance, pattern arithmetic or tree search using globs.\n\n\n\nLet’s try it\nBy default, git grep searches recursively through the tracked files in the working directory (that is, the current version of the tracked files).\nFirst, let’s look for the word test in the current version of the tracked files in the test repo:\n\ngit grep test\n\nadrian.txt:Adrian's test text file.\nformerlyadrian.txt:Adrian's test text file.\nms/protocol.md:This is my test.\nms/smabraha.txt:This is a test file that I wanted to make, then push it somehow\nredone17.txt:this is a test file from redone17\nsrc/test_manuel.py:def test(model, device, test_loader):\nsrc/test_manuel.py:    test_loss = 0\nsrc/test_manuel.py:        for data, target in test_loader:\nsrc/test_manuel.py:            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\nsrc/test_manuel.py:    test_loss /= len(test_loader.dataset)\nsrc/test_manuel.py:        test_loss, correct, len(test_loader.dataset),\nsrc/test_manuel.py:        100. * correct / len(test_loader.dataset)))\nsrc/test_manuel.py:    test_data = datasets.MNIST(\nsrc/test_manuel.py:    test_loader = torch.utils.data.DataLoader(test_data, batch_size=100)\nsrc/test_manuel.py:        test(model, device, test_loader)\ntestAV1.txt:This is a test\ntext-collab.txt:This is the collaboration testing\n\n\nLet’s add blank lines between the results of each file for better readability:\n\ngit grep --break test\n\nadrian.txt:Adrian's test text file.\n\nformerlyadrian.txt:Adrian's test text file.\n\nms/protocol.md:This is my test.\n\nms/smabraha.txt:This is a test file that I wanted to make, then push it somehow\n\nredone17.txt:this is a test file from redone17\n\nsrc/test_manuel.py:def test(model, device, test_loader):\nsrc/test_manuel.py:    test_loss = 0\nsrc/test_manuel.py:        for data, target in test_loader:\nsrc/test_manuel.py:            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\nsrc/test_manuel.py:    test_loss /= len(test_loader.dataset)\nsrc/test_manuel.py:        test_loss, correct, len(test_loader.dataset),\nsrc/test_manuel.py:        100. * correct / len(test_loader.dataset)))\nsrc/test_manuel.py:    test_data = datasets.MNIST(\nsrc/test_manuel.py:    test_loader = torch.utils.data.DataLoader(test_data, batch_size=100)\nsrc/test_manuel.py:        test(model, device, test_loader)\n\ntestAV1.txt:This is a test\n\ntext-collab.txt:This is the collaboration testing\n\n\nLet’s also put the file names on separate lines:\n\ngit grep --break --heading test\n\nadrian.txt\nAdrian's test text file.\n\nformerlyadrian.txt\nAdrian's test text file.\n\nms/protocol.md\nThis is my test.\n\nms/smabraha.txt\nThis is a test file that I wanted to make, then push it somehow\n\nredone17.txt\nthis is a test file from redone17\n\nsrc/test_manuel.py\ndef test(model, device, test_loader):\n    test_loss = 0\n        for data, target in test_loader:\n            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n    test_loss /= len(test_loader.dataset)\n        test_loss, correct, len(test_loader.dataset),\n        100. * correct / len(test_loader.dataset)))\n    test_data = datasets.MNIST(\n    test_loader = torch.utils.data.DataLoader(test_data, batch_size=100)\n        test(model, device, test_loader)\n\ntestAV1.txt\nThis is a test\n\ntext-collab.txt\nThis is the collaboration testing\n\n\nWe can display the line numbers for the results with the -n flag:\n\ngit grep --break --heading -n test\n\nadrian.txt\n1:Adrian's test text file.\n\nformerlyadrian.txt\n1:Adrian's test text file.\n\nms/protocol.md\n9:This is my test.\n\nms/smabraha.txt\n1:This is a test file that I wanted to make, then push it somehow\n\nredone17.txt\n1:this is a test file from redone17\n\nsrc/test_manuel.py\n50:def test(model, device, test_loader):\n52:    test_loss = 0\n55:        for data, target in test_loader:\n58:            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n62:    test_loss /= len(test_loader.dataset)\n65:        test_loss, correct, len(test_loader.dataset),\n66:        100. * correct / len(test_loader.dataset)))\n84:    test_data = datasets.MNIST(\n90:    test_loader = torch.utils.data.DataLoader(test_data, batch_size=100)\n97:        test(model, device, test_loader)\n\ntestAV1.txt\n1:This is a test\n\ntext-collab.txt\n1:This is the collaboration testing\n\n\nNotice how the results for the file src/test_manuel.py involve functions. It would be very convenient to have the names of the functions in which test appears.\nWe can do this with the -p flag:\n\ngit grep --break --heading -p test src/test_manuel.py\n\nsrc/test_manuel.py\ndef train(model, device, train_loader, optimizer, epoch):\ndef test(model, device, test_loader):\n    test_loss = 0\n        for data, target in test_loader:\n            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n    test_loss /= len(test_loader.dataset)\n        test_loss, correct, len(test_loader.dataset),\n        100. * correct / len(test_loader.dataset)))\ndef main():\n    test_data = datasets.MNIST(\n    test_loader = torch.utils.data.DataLoader(test_data, batch_size=100)\n        test(model, device, test_loader)\n\n\n\nWe added the argument src/test_manuel.py to limit the search to that file.\n\nWe can now see that the word test appears in the functions test and main.\nNow, instead of printing all the matching lines, let’s print the number of matches per file:\n\ngit grep -c test\n\nadrian.txt:1\nformerlyadrian.txt:1\nms/protocol.md:1\nms/smabraha.txt:1\nredone17.txt:1\nsrc/test_manuel.py:10\ntestAV1.txt:1\ntext-collab.txt:1\n\n\n\n\nMore complex patterns\ngit grep in fact searches for regular expressions. test is a regular expression matching test, but we can look for more complex patterns.\nLet’s look for image:\n\ngit grep image\n\n\nNo output means that the search is not returning any result.\n\nLet’s make this search case insensitive:\n\ngit grep -i image\n\nsrc/new_file.py:from PIL import Image\nsrc/new_file.py:berlin1_lr = Image.open(\"/home/marie/parvus/pwg/wtm/slides/static/img/upscaling/lr/berlin_1945_1.jpg\")\nsrc/new_file.py:berlin1_hr = Image.open(\"/home/marie/parvus/pwg/wtm/slides/static/img/upscaling/hr/berlin_1945_1.png\")\n\n\nWe are now getting some results as Image was present in three lines of the file src/new_file.py.\nLet’s now search for data:\n\ngit grep data\n\n.gitignore:data/\nms/protocol.md:Collected and analyzed amazing data\nsrc/new_file.py:from datasets import load_dataset\nsrc/new_file.py:set5 = load_dataset('eugenesiow/Set5', 'bicubic_x4', split='validation')\nsrc/test_manuel.py:from torchvision import datasets, transforms\nsrc/test_manuel.py:    for batch_idx, (data, target) in enumerate(train_loader):\nsrc/test_manuel.py:        data, target = data.to(device), target.to(device)\nsrc/test_manuel.py:        output = model(data)\nsrc/test_manuel.py:                epoch, batch_idx * len(data), len(train_loader.dataset),\nsrc/test_manuel.py:        for data, target in test_loader:\nsrc/test_manuel.py:            data, target = data.to(device), target.to(device)\nsrc/test_manuel.py:            output = model(data)\nsrc/test_manuel.py:    test_loss /= len(test_loader.dataset)\nsrc/test_manuel.py:        test_loss, correct, len(test_loader.dataset),\nsrc/test_manuel.py:        100. * correct / len(test_loader.dataset)))\nsrc/test_manuel.py:    train_data = datasets.MNIST(\nsrc/test_manuel.py:        '~/parvus/pwg/wtm/tml/data',\nsrc/test_manuel.py:        # '~/projects/def-sponsor00/data',\nsrc/test_manuel.py:    test_data = datasets.MNIST(\nsrc/test_manuel.py:        '~/parvus/pwg/wtm/tml/data',\nsrc/test_manuel.py:        # '~/projects/def-sponsor00/data',\nsrc/test_manuel.py:    train_loader = torch.utils.data.DataLoader(train_data, batch_size=50)\nsrc/test_manuel.py:    test_loader = torch.utils.data.DataLoader(test_data, batch_size=100)\n\n\nWe are getting results for the word data, but also for the pattern data in longer expressions such as train_data or dataset. If we only want results for the word data, we can use the -w flag:\n\ngit grep -w data\n\n.gitignore:data/\nms/protocol.md:Collected and analyzed amazing data\nsrc/test_manuel.py:    for batch_idx, (data, target) in enumerate(train_loader):\nsrc/test_manuel.py:        data, target = data.to(device), target.to(device)\nsrc/test_manuel.py:        output = model(data)\nsrc/test_manuel.py:                epoch, batch_idx * len(data), len(train_loader.dataset),\nsrc/test_manuel.py:        for data, target in test_loader:\nsrc/test_manuel.py:            data, target = data.to(device), target.to(device)\nsrc/test_manuel.py:            output = model(data)\nsrc/test_manuel.py:        '~/parvus/pwg/wtm/tml/data',\nsrc/test_manuel.py:        # '~/projects/def-sponsor00/data',\nsrc/test_manuel.py:        '~/parvus/pwg/wtm/tml/data',\nsrc/test_manuel.py:        # '~/projects/def-sponsor00/data',\nsrc/test_manuel.py:    train_loader = torch.utils.data.DataLoader(train_data, batch_size=50)\nsrc/test_manuel.py:    test_loader = torch.utils.data.DataLoader(test_data, batch_size=100)\n\n\nNow, let’s use a more complex regular expression. We want the counts for the pattern \".*_.*\" (i.e. any name with a snail case such as train_loader):\n\ngit grep -c \".*_.*\"\n\n.gitignore:4\nsrc/new_file.py:22\nsrc/test_manuel.py:29\n\n\nLet’s print the first 3 results per file:\n\ngit grep -m 3 \".*_.*\"\n\n.gitignore:hidden_file\n.gitignore:search_cache/\n.gitignore:ws_search_cache/html\nsrc/new_file.py:from datasets import load_dataset\nsrc/new_file.py:set5 = load_dataset('eugenesiow/Set5', 'bicubic_x4', split='validation')\nsrc/new_file.py:set5.column_names\nsrc/test_manuel.py:from torch.optim.lr_scheduler import StepLR\nsrc/test_manuel.py:    def __init__(self):\nsrc/test_manuel.py:        super(Net, self).__init__()\n\n\nAs you can see, our results also include __init__ which is not what we were looking for. So let’s exclude __:\n\ngit grep -m 3 -e \".*_.*\" --and --not -e \"__\"\n\n.gitignore:hidden_file\n.gitignore:search_cache/\n.gitignore:ws_search_cache/html\nsrc/new_file.py:from datasets import load_dataset\nsrc/new_file.py:set5 = load_dataset('eugenesiow/Set5', 'bicubic_x4', split='validation')\nsrc/new_file.py:set5.column_names\nsrc/test_manuel.py:from torch.optim.lr_scheduler import StepLR\nsrc/test_manuel.py:        x = F.max_pool2d(x, 2)\nsrc/test_manuel.py:        output = F.log_softmax(x, dim=1)\n\n\n\nFor simple searches, you don’t have to use the -e flag before the pattern you are searching for. Here however, our command has gotten complex enough that we have to use it before each pattern.\n\nLet’s make sure this worked as expected:\n\ngit grep -c \".*_.*\"\necho \"---\"\ngit grep -c \"__\"\necho \"---\"\ngit grep -ce \".*_.*\" --and --not -e \"__\"\n\n.gitignore:4\nsrc/new_file.py:22\nsrc/test_manuel.py:29\n---\nsrc/test_manuel.py:2\n---\n.gitignore:4\nsrc/new_file.py:22\nsrc/test_manuel.py:27\n\n\nThere were 2 lines matching __ in src/test_manuel.py and we have indeed excluded them from our search.\nExtended regular expressions are also covered with the flag -E.\n\n\nSearching other trees\nSo far, we have searched the current version of tracked files, but we can just as easily search files at any commit.\nLet’s search for test in the tracked files 20 commits ago:\n\ngit grep test HEAD~20\n\nHEAD~20:adrian.txt:Adrian's test text file.\nHEAD~20:formerlyadrian.txt:Adrian's test text file.\nHEAD~20:ms/protocol.md:This is my test.\nHEAD~20:ms/smabraha.txt:This is a test file that I wanted to make, then push it somehow\nHEAD~20:redone17.txt:this is a test file from redone17\nHEAD~20:testAV1.txt:This is a test\nHEAD~20:text-collab.txt:This is the collaboration testing\n\n\n\nAs you can see, the file src/test_manuel.py is not in the results. Either it didn’t exist or it didn’t have the word test at that commit.\n\nIf you want to search tracked files AND untracked files, you need to use the --untracked flag.\nLet’s create a new (thus untracked) file with some content including the word test:\n\necho \"This is a test\" &gt; newfile\n\nNow compare the following:\n\ngit grep -c test\n\nadrian.txt:1\nformerlyadrian.txt:1\nms/protocol.md:1\nms/smabraha.txt:1\nredone17.txt:1\nsrc/test_manuel.py:10\ntestAV1.txt:1\ntext-collab.txt:1\n\n\nwith:\n\ngit grep -c --untracked test\n\nadrian.txt:1\nformerlyadrian.txt:1\nms/protocol.md:1\nms/smabraha.txt:1\nredone17.txt:1\nsrc/test_manuel.py:10\ntestAV1.txt:1\ntext-collab.txt:1\nws_search.rmarkdown:41\n\n\n\nThis last result also returned our untracked file newfile.\n\nIf you want to search untracked and ignored files (meaning all your files), use the flags --untracked --no-exclude-standard.\nLet’s see what the .gitignore file contains:\n\ncat .gitignore\n\ndata/\noutput/\nhidden_file\nsearch_cache/\nsearch.qmd\nnewfile\nimg\nws_search_cache/html\nws_search.qmd\n\n\nThe directory data is in .gitignore. This means that it is not under version control and it thus doesn’t exist in our repo (since we cloned our repo, we only have the version-controlled files). Let’s create it:\nmkdir data\nNow, let’s create a file in it that contains test:\n\necho \"And another test\" &gt; data/file\n\nWe can rerun our previous two searches to verify that files excluded from version control are not searched:\n\ngit grep -c test\n\nadrian.txt:1\nformerlyadrian.txt:1\nms/protocol.md:1\nms/smabraha.txt:1\nredone17.txt:1\nsrc/test_manuel.py:10\ntestAV1.txt:1\ntext-collab.txt:1\n\n\n\ngit grep -c --untracked test\n\nadrian.txt:1\nformerlyadrian.txt:1\nms/protocol.md:1\nms/smabraha.txt:1\nredone17.txt:1\nsrc/test_manuel.py:10\ntestAV1.txt:1\ntext-collab.txt:1\nws_search.rmarkdown:41\n\n\nAnd now, let’s try:\n\ngit grep -c --untracked --no-exclude-standard test\n\nadrian.txt:1\ndata/file:1\nformerlyadrian.txt:1\nms/protocol.md:1\nms/smabraha.txt:1\nnewfile:1\nredone17.txt:1\nsrc/test_manuel.py:10\ntestAV1.txt:1\ntext-collab.txt:1\nws_search.qmd:41\nws_search.rmarkdown:41\n\n\n\ndata/file, despite being excluded from version control, is also searched.\n\n\n\nSearching all commits\nWe saw that git grep &lt;pattern&gt; &lt;commit&gt; can search a pattern in any commit. Now, what if we all to search all commits for a pattern?\nFor this, we pass the expression $(git rev-list --all) in lieu of &lt;commit&gt;.\ngit rev-list --all creates a list of all the commits in a way that can be used as an argument to other functions. The $() allows to run the expression inside it and pass the result as and argument.\nTo search for test in all the commits, we thus run:\ngit grep \"test\" $(git rev-list --all)\nI am not running this command has it has a huge output. Instead, I will limit the search to the last two commits:\n\ngit grep \"test\" $(git rev-list --all -2)\n\n388fdc13de66537cac2169253cb385dfd409e710:adrian.txt:Adrian's test text file.\n388fdc13de66537cac2169253cb385dfd409e710:formerlyadrian.txt:Adrian's test text file.\n388fdc13de66537cac2169253cb385dfd409e710:ms/protocol.md:This is my test.\n388fdc13de66537cac2169253cb385dfd409e710:ms/smabraha.txt:This is a test file that I wanted to make, then push it somehow\n388fdc13de66537cac2169253cb385dfd409e710:redone17.txt:this is a test file from redone17\n388fdc13de66537cac2169253cb385dfd409e710:src/test_manuel.py:def test(model, device, test_loader):\n388fdc13de66537cac2169253cb385dfd409e710:src/test_manuel.py:    test_loss = 0\n388fdc13de66537cac2169253cb385dfd409e710:src/test_manuel.py:        for data, target in test_loader:\n388fdc13de66537cac2169253cb385dfd409e710:src/test_manuel.py:            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n388fdc13de66537cac2169253cb385dfd409e710:src/test_manuel.py:    test_loss /= len(test_loader.dataset)\n388fdc13de66537cac2169253cb385dfd409e710:src/test_manuel.py:        test_loss, correct, len(test_loader.dataset),\n388fdc13de66537cac2169253cb385dfd409e710:src/test_manuel.py:        100. * correct / len(test_loader.dataset)))\n388fdc13de66537cac2169253cb385dfd409e710:src/test_manuel.py:    test_data = datasets.MNIST(\n388fdc13de66537cac2169253cb385dfd409e710:src/test_manuel.py:    test_loader = torch.utils.data.DataLoader(test_data, batch_size=100)\n388fdc13de66537cac2169253cb385dfd409e710:src/test_manuel.py:        test(model, device, test_loader)\n388fdc13de66537cac2169253cb385dfd409e710:testAV1.txt:This is a test\n388fdc13de66537cac2169253cb385dfd409e710:text-collab.txt:This is the collaboration testing\n423f454765d45e21e0ae401da0b3dec2d84113ce:adrian.txt:Adrian's test text file.\n423f454765d45e21e0ae401da0b3dec2d84113ce:formerlyadrian.txt:Adrian's test text file.\n423f454765d45e21e0ae401da0b3dec2d84113ce:ms/protocol.md:This is my test.\n423f454765d45e21e0ae401da0b3dec2d84113ce:ms/smabraha.txt:This is a test file that I wanted to make, then push it somehow\n423f454765d45e21e0ae401da0b3dec2d84113ce:redone17.txt:this is a test file from redone17\n423f454765d45e21e0ae401da0b3dec2d84113ce:src/test_manuel.py:def test(model, device, test_loader):\n423f454765d45e21e0ae401da0b3dec2d84113ce:src/test_manuel.py:    test_loss = 0\n423f454765d45e21e0ae401da0b3dec2d84113ce:src/test_manuel.py:        for data, target in test_loader:\n423f454765d45e21e0ae401da0b3dec2d84113ce:src/test_manuel.py:            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n423f454765d45e21e0ae401da0b3dec2d84113ce:src/test_manuel.py:    test_loss /= len(test_loader.dataset)\n423f454765d45e21e0ae401da0b3dec2d84113ce:src/test_manuel.py:        test_loss, correct, len(test_loader.dataset),\n423f454765d45e21e0ae401da0b3dec2d84113ce:src/test_manuel.py:        100. * correct / len(test_loader.dataset)))\n423f454765d45e21e0ae401da0b3dec2d84113ce:src/test_manuel.py:    test_data = datasets.MNIST(\n423f454765d45e21e0ae401da0b3dec2d84113ce:src/test_manuel.py:    test_loader = torch.utils.data.DataLoader(test_data, batch_size=100)\n423f454765d45e21e0ae401da0b3dec2d84113ce:src/test_manuel.py:        test(model, device, test_loader)\n423f454765d45e21e0ae401da0b3dec2d84113ce:testAV1.txt:This is a test\n423f454765d45e21e0ae401da0b3dec2d84113ce:text-collab.txt:This is the collaboration testing\n\n\n\nIn combination with the fuzzy finder tool fzf, this can make finding a particular commit extremely easy.\nFor instance, the code below allows you to dynamically search in the result through incremental completion:\ngit grep \"test\" $(git rev-list --all) | fzf --cycle -i -e\nOr even better, you can automatically copy the short form of the hash of the selected commit to clipboard so that you can use it with git show, git checkout, etc.:\ngit grep \"test\" $(git rev-list --all) |\n    fzf --cycle -i -e |\n    cut -c 1-7 |\n    xclip -r -selection clipboard\n\nHere, I am using xclip to copy to the clipboard as I am on Linux. Depending on your OS you might need to use a different tool.\n\nOf course, you can create a function in your .bashrc file with such code so that you wouldn’t have to type it each time:\ngrep_all_commits () {\n    git grep \"$1\" $(git rev-list --all) |\n        fzf --cycle -i -e |\n        cut -c 1-7 |\n        xclip -r -selection clipboard\n}\nAlternatively, you can pass the result directly into whatever git command you want to use that commit for.\nHere is an example with git show:\ngit grep \"test\" $(git rev-list --all) |\n    fzf --cycle -i -e |\n    cut -c 1-7 |\n    git show\nAnd if you wanted to get really fancy, you could go with:\ngit grep \"test\" $(git rev-list --all) |\n    fzf --cycle -i -e --no-multi \\\n        --ansi --preview=\"$_viewGitLogLine\" \\\n        --header \"enter: view, C-c: copy hash\" \\\n        --bind \"enter:execute:$_viewGitLogLine | less -R\" \\\n        --bind \"ctrl-c:execute:$_gitLogLineToHash |\n        xclip -r -selection clipboard\"\nWrapped in a function:\ngrep_all_commits_preview () {\n    git grep \"$1\" $(git rev-list --all) |\n        fzf --cycle -i -e --no-multi \\\n            --ansi --preview=\"$_viewGitLogLine\" \\\n            --header \"enter: view, C-c: copy hash\" \\\n            --bind \"enter:execute:$_viewGitLogLine |\n              less -R\" \\\n            --bind \"ctrl-c:execute:$_gitLogLineToHash |\n        xclip -r -selection clipboard\"\n}\nThis last function allows you to search through all the results in an incremental fashion while displaying a preview of the selected diff (the changes made at that particular commit). If you want to see more of the diff than the preview displays, press &lt;enter&gt; (then q to quit the pager), if you want to copy the hash of a commit, press C-c (Control + c).\nWith this function, you can now instantly get a preview of the changes made to any line containing an expression for any file, at any commit, and copy the hash of the selected commit. This is really powerful.\n\n\n\nAliases\nIf you don’t want to type a series of flags all the time, you can configure aliases for Git. For instance, Alex Razoumov uses the alias git search for git grep --break --heading -n -i.\nLet’s add to it the -p flag. Here is how you would set this alias:\ngit config --global alias.search 'grep --break --heading -n -i -p'\n\nThis setting gets added to your main Git configuration file (on Linux, by default, at ~/.gitconfig).\n\nFrom there on, you can use your alias with:\n\ngit search test\n\ngit: 'search' is not a git command. See 'git --help'.",
    "crumbs": [
      "Git",
      "<b><em>Workshops</em></b>",
      "Searching a Git project"
    ]
  },
  {
    "objectID": "git/practice_repo/ws_search.html#searching-logs",
    "href": "git/practice_repo/ws_search.html#searching-logs",
    "title": "Searching a version-controlled project",
    "section": "Searching logs",
    "text": "Searching logs\nThe second thing that can happen is that you are looking for some pattern in your version control logs.\n\ngit log\ngit log allows to get information on commit logs.\nBy default, it outputs all the commits of the current branch.\nLet’s show the logs of the last 3 commits:\n\ngit log -3\n\ncommit 388fdc13de66537cac2169253cb385dfd409e710\nAuthor: Marie-Helene Burle &lt;marie.burle@westdri.ca&gt;\nDate:   Thu Dec 14 20:55:30 2023 -0800\n\n    update gitignore\n\ncommit 423f454765d45e21e0ae401da0b3dec2d84113ce\nMerge: 7342af5 818c32a\nAuthor: Marie-Helene Burle &lt;marie.burle@westdri.ca&gt;\nDate:   Tue Dec 12 17:30:07 2023 -0800\n\n    Merge branch 'main' of github.com:prosoitos/practice_repo\n\ncommit 7342af5dfff53dc51dfaf99da1e29448fd253e03\nAuthor: Marie-Helene Burle &lt;marie.burle@westdri.ca&gt;\nDate:   Tue Dec 12 17:29:59 2023 -0800\n\n    update gitignore\n\n\nThe output can be customized thanks to a plethora of options.\nFor instance, here are the logs of the last 15 commits, in a graph, with one line per commit:\n\ngit log --graph --oneline -n 15\n\n* 388fdc1 update gitignore\n*   423f454 Merge branch 'main' of github.com:prosoitos/practice_repo\n|\\  \n| * 818c32a Delete ml_models directory\n| * b3c2414 Created using Colaboratory\n* | 7342af5 update gitignore\n|/  \n* e3cfb2e Update gitignore with Quarto files\n* 15fdec6 Update README.org\n* 15d4ee9 change values training\n* 06efa34 add lots of code\n* 1457143 remove stupid line\n* 711e1dc add real py content to test_manual.py\n* 90016aa adding new python file\n*   2c0f612 Merge branch 'main' of github.com:prosoitos/git_workshop_collab\n|\\  \n| *   6f7d03d Merge branch 'main' of https://github.com/prosoitos/git_workshop_collab into main\n| |\\  \n| * \\   3c53269 Merge branch 'main' of https://github.com/prosoitos/git_workshop_collab into main\n| |\\ \\  \n\n\nBut git log has also flags that allow to search for patterns.\n\n\nSearching commit messages\nOne of the reasons it is so important to write informative commit messages is that they are key to finding commits later on.\nTo look for a pattern in all your commit messages, use git log --grep=&lt;pattern&gt;.\nLet’s look for test in the commit messages and limit the output to 3 commits:\n\ngit log --grep=test -3\n\ncommit 711e1dc53011e5071b17dc7c35b516f6e066f396\nAuthor: Marie-Helene Burle &lt;marie.burle@westgrid.ca&gt;\nDate:   Tue Mar 15 11:52:48 2022 -0700\n\n    add real py content to test_manual.py\n\ncommit a55ca0d60d82578c94bd49fc4ca987727b851216\nAuthor: Manuelhrokr &lt;zl.manuel@protonmail.com&gt;\nDate:   Thu Feb 17 15:19:42 2022 -0700\n\n    new comment add just as test\n\ncommit ea74e46f487fba09c31524a110fdf060796e3cf8\nAuthor: mpkin &lt;mikin@physics.ubc.ca&gt;\nDate:   Thu Sep 23 14:51:24 2021 -0700\n\n    Add test_mk.txt\n\n\nFor a more compact output:\n\ngit log --grep=\"test\" -3 --oneline\n\n711e1dc add real py content to test_manual.py\na55ca0d new comment add just as test\nea74e46 Add test_mk.txt\n\n\n\nHere too you can use this in combination to fzf with for instance:\ngit log --grep=\"test\" | fzf --cycle -i -e\nOr:\ngit log --grep=\"test\" --oneline |\n    fzf --cycle -i -e --no-multi \\\n        --ansi --preview=\"$_viewGitLogLine\" \\\n        --header \"enter: view, C-c: copy hash\" \\\n        --bind \"enter:execute:$_viewGitLogLine | less -R\" \\\n        --bind \"ctrl-c:execute:$_gitLogLineToHash |\n        xclip -r -selection clipboard\"\n\n\n\nChanges made to a pattern\nRemember that test was present in the file src/test_manuel.py. If we want to see when the pattern was first created and then each time it was modified, we use the -L flag in this fashion:\ngit log -L :&lt;pattern&gt;:file\nIn our case:\n\ngit log -L :test:src/test_manuel.py\n\ncommit 711e1dc53011e5071b17dc7c35b516f6e066f396\nAuthor: Marie-Helene Burle &lt;marie.burle@westgrid.ca&gt;\nDate:   Tue Mar 15 11:52:48 2022 -0700\n\n    add real py content to test_manual.py\n\ndiff --git a/src/test_manuel.py b/src/test_manuel.py\n--- a/src/test_manuel.py\n+++ b/src/test_manuel.py\n@@ -1,1 +50,19 @@\n-test\n+def test(model, device, test_loader):\n+    model.eval()\n+    test_loss = 0\n+    correct = 0\n+    with torch.no_grad():\n+        for data, target in test_loader:\n+            data, target = data.to(device), target.to(device)\n+            output = model(data)\n+            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n+            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n+            correct += pred.eq(target.view_as(pred)).sum().item()\n+\n+    test_loss /= len(test_loader.dataset)\n+\n+    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n+        test_loss, correct, len(test_loader.dataset),\n+        100. * correct / len(test_loader.dataset)))\n+\n+\n\ncommit 90016aa3ed3a6cf71e206392bbf10adfe1a14c17\nAuthor: Manuelhrokr &lt;zl.manuel@protonmail.com&gt;\nDate:   Thu Feb 17 15:33:03 2022 -0700\n\n    adding new python file\n\ndiff --git a/src/test_manuel.py b/src/test_manuel.py\n--- /dev/null\n+++ b/src/test_manuel.py\n@@ -0,0 +1,1 @@\n+test\n\n\nThis is very useful if you want to see, for instance, changes made to a function in a script.\n\n\nChanges in number of occurrences of a pattern\nNow, if we want to list all commits that created a change in the number of occurrences of test in our project, we run:\n\ngit log -S test --oneline\n\n818c32a Delete ml_models directory\nb3c2414 Created using Colaboratory\n711e1dc add real py content to test_manual.py\n90016aa adding new python file\n652faa5 delete my file\nb684eac Deleted file\n6717236 For collab\nca1845d delete alex.txt\n6b56198 editing adrians text file\n01a7358 test dtrad\ne44a454 Create testAV1.txt\n5ee88e6 For collab\ncf3d4ea Collab-test\n13faa1e test, test\n0366115 Adrian's test file\n9ebd3ce This is my test\n6dfefa8 create redone17.txt\ne43163c added alex.txt\n\n\nThis can be useful to identify the commit you need.",
    "crumbs": [
      "Git",
      "<b><em>Workshops</em></b>",
      "Searching a Git project"
    ]
  },
  {
    "objectID": "git/practice_repo/ws_search.html#tldr",
    "href": "git/practice_repo/ws_search.html#tldr",
    "title": "Searching a version-controlled project",
    "section": "TL;DR",
    "text": "TL;DR\nHere are the search functions you are the most likely to use:\n\nSearch for a pattern in the current version of your tracked files:\n\ngit grep &lt;pattern&gt;\n\nSearch for a pattern in your files at a certain commit:\n\ngit grep &lt;pattern&gt; &lt;commit&gt;\n\nSearch for a pattern in your files in all the commits:\n\ngit grep &lt;pattern&gt; $(git rev-list --all)\n\nSearch for a pattern in your commit messages:\n\ngit log --grep=&lt;pattern&gt;\nNow you should be able to find pretty much anything in your projects and their histories.",
    "crumbs": [
      "Git",
      "<b><em>Workshops</em></b>",
      "Searching a Git project"
    ]
  },
  {
    "objectID": "git/intro_tools.html",
    "href": "git/intro_tools.html",
    "title": "Tools for a friendlier Git",
    "section": "",
    "text": "Two great open-source tools to work with Git in a nice visual manner while remaining in the command line.",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Tools for a friendlier Git"
    ]
  },
  {
    "objectID": "git/intro_tools.html#fzf",
    "href": "git/intro_tools.html#fzf",
    "title": "Tools for a friendlier Git",
    "section": "fzf",
    "text": "fzf\nfzf is a fantastic multi-platform command line fuzzy finder with a huge versatility.\n\nIn this video, I demo quickly how it can be used with Git:",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Tools for a friendlier Git"
    ]
  },
  {
    "objectID": "git/intro_tools.html#lazygit",
    "href": "git/intro_tools.html#lazygit",
    "title": "Tools for a friendlier Git",
    "section": "lazygit",
    "text": "lazygit\nlazygit is an excellent multi-platform user interface for Git which works in the command line.",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Tools for a friendlier Git"
    ]
  },
  {
    "objectID": "git/intro_three_trees.html",
    "href": "git/intro_three_trees.html",
    "title": "The three trees of Git",
    "section": "",
    "text": "One useful mental representation of the functioning of Git is to imagine three file trees."
  },
  {
    "objectID": "git/intro_three_trees.html#the-three-trees-of-git",
    "href": "git/intro_three_trees.html#the-three-trees-of-git",
    "title": "The three trees of Git",
    "section": "The three trees of Git",
    "text": "The three trees of Git\n\nWorking directory\nLet’s imagine that you are starting to work on a project.\nFirst, you create a directory.\nIn it, you create several sub-directories.\nIn those, you create a number of files.\nYou can open these files, read them, edit them, etc. This is something you are very familiar with.\nIn the Git world, this is the working directory or working tree of the project.\nThat is: an uncompressed version of your files that you can access and edit.\nYou can think of it as a sandbox because this is where you can experiment with the project. This is where the project gets developed.\nNow, Git has two other important pieces in its architecture.\n\n\nIndex\nIf you want the project history to be useful to future you, it has to be nice and tidy. You don’t want to record snapshots haphazardly or you will never be able to find anything back.\nBefore you record a snapshot, you carefully select the elements of the project as it is now that would be useful to write to the project history together. The index or staging area is what allows to do that: it contains the suggested future snapshot.\n\n\nHEAD\nFinally, the last tree in Git architecture is one snapshot in the project history that serves as a reference version of the project: if you want to see what you have been experimenting on in your “sandbox”, you need to compare the state of the working directory with some snapshot.\nRemember that HEAD is a pointer pointing at a branch, that a branch is itself a pointer pointing at a commit, and finally that a commit is a Git object pointing at a snapshot. When the HEAD pointer moves around, whatever snapshot it points to populates the HEAD tree.\nAs we saw earlier, when you create a commit, HEAD automatically points to the new commit. So the HEAD tree is often filled with the last snapshot you created. But—as we will see later—we can move the HEAD pointer around through other ways. So the HEAD tree can be populated by any snapshot in your project history."
  },
  {
    "objectID": "git/intro_three_trees.html#status-of-the-three-trees",
    "href": "git/intro_three_trees.html#status-of-the-three-trees",
    "title": "The three trees of Git",
    "section": "Status of the three trees",
    "text": "Status of the three trees\nTo display the status of these trees, you run:\ngit status"
  },
  {
    "objectID": "git/intro_three_trees.html#three-trees-in-action",
    "href": "git/intro_three_trees.html#three-trees-in-action",
    "title": "The three trees of Git",
    "section": "Three trees in action",
    "text": "Three trees in action\n\nClean working tree\nWe say that the working tree is “clean” when all changes tracked by Git were staged and committed:\n\nHere is an example for a project with a single file called File at version v1.\n\n\n\n\nMaking changes to the working tree\nWhen you edit files in your project, you make changes in the working directory or working tree.\n\nFor instance, you make changes to File. Let’s say that it is now at version v2:\n\n\nThe other two trees remain at version v1.\nIf you run git status, this is what you get:\nOn branch main\nChanges not staged for commit:\n  (use \"git add &lt;file&gt;...\" to update what will be committed)\n  (use \"git restore &lt;file&gt;...\" to discard changes in working directory)\n        modified:   File\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\n\n\nStaging changes\nYou stage that file (meaning that you will include the changes of that file in the next commit) with:\ngit add File\nAfter which, your Git trees look like this:\n\nNow, the index also has File at version v2 and git status returns:\nOn branch main\nChanges to be committed:\n  (use \"git restore --staged &lt;file&gt;...\" to unstage)\n        modified:   File\n\n\nCommitting changes\nFinally, you create a snapshot and the commit pointing to it—recording the staged changes to history—with:\ngit commit -m \"Added File\"\n-m is a flag that allows to provide the commit message directly in the command line. If you don’t use it, Git will open a text editor so that you can type the message. Without a message, there can be no commit.\nNow your trees look like this:\n\nOur working tree is clean again and git status returns:\nOn branch main\nnothing to commit, working tree clean\nThis means that there are no uncommitted changes in the working tree or the staging area: all the changes have been written to history.\n\nYou don’t have to stage all the changes in the working directory before making a commit; that is actually the whole point of the staging area.\nThis means that the working directory is not necessarily clean after you have created a new commit."
  },
  {
    "objectID": "git/intro_stash.html",
    "href": "git/intro_stash.html",
    "title": "Stashing",
    "section": "",
    "text": "Stashing is a way to put changes aside for some time. Changes can be reapplied later (and/or applied on other branches).\nWhy would you want to do that? And how?",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Stashing"
    ]
  },
  {
    "objectID": "git/intro_stash.html#what-are-git-stashes",
    "href": "git/intro_stash.html#what-are-git-stashes",
    "title": "Stashing",
    "section": "What are Git stashes?",
    "text": "What are Git stashes?\nHaving a clean working tree is necessary for many Git operations and recommended for others. If you have changes from an unfinished piece of work getting in the way, you could do an ugly commit to get rid of them. But if you care about having an organized history with meaningful commits (or if you don’t want others to see your messy drafts), stashing is a much better alternative.",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Stashing"
    ]
  },
  {
    "objectID": "git/intro_stash.html#creating-a-stash",
    "href": "git/intro_stash.html#creating-a-stash",
    "title": "Stashing",
    "section": "Creating a stash",
    "text": "Creating a stash\nYou can stash the changes in your modified files with:\ngit stash\nTo also include new (untracked) files, you have to use the -u flag:\ngit stash -u",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Stashing"
    ]
  },
  {
    "objectID": "git/intro_stash.html#listing-stashes",
    "href": "git/intro_stash.html#listing-stashes",
    "title": "Stashing",
    "section": "Listing stashes",
    "text": "Listing stashes\nOf course, you don’t want to lose or forget about your stashes.\nYou can list them with:\ngit stash list\nStashes are also shown when you run git log (with any of its variations) with the --all flag.\n\nExample:\n\ngit log --graph --oneline --all",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Stashing"
    ]
  },
  {
    "objectID": "git/intro_stash.html#re-applying-changes-from-a-stash",
    "href": "git/intro_stash.html#re-applying-changes-from-a-stash",
    "title": "Stashing",
    "section": "Re-applying changes from a stash",
    "text": "Re-applying changes from a stash\nTo re-apply the changes (or apply them on another branch), you run:\ngit stash apply\nIf you had staged changes, you can also restore the state of the index by running instead:\ngit stash apply --index\nIf you created multiple stashes, this will apply the last one you created. If this is not what you want, you have to specify which stash you want to use with the reflog syntax:\n\nstash@{0} is the last stash (so you can omit it)\n\nstash@{1} is the one before it\n\nstash@{2} the one before that\n\netc.\n\n\nTo apply the stash before last:\n\ngit stash apply stash@{1}",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Stashing"
    ]
  },
  {
    "objectID": "git/intro_stash.html#deleting-a-stash",
    "href": "git/intro_stash.html#deleting-a-stash",
    "title": "Stashing",
    "section": "Deleting a stash",
    "text": "Deleting a stash\nYou delete the last (or the only) stash with:\ngit stash drop\nHere again, if you want to delete another stash, specify it with its reflog index.\n\nTo delete the antepenultimate stash:\n\ngit stash drop stash@{2}\nYou can apply and delete a stash at the same time with:\ngit stash pop\nThis is convenient, but less flexible.",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Stashing"
    ]
  },
  {
    "objectID": "git/intro_resources.html",
    "href": "git/intro_resources.html",
    "title": "Resources",
    "section": "",
    "text": "Official Git manual\nOpen source Pro Git book",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Resources"
    ]
  },
  {
    "objectID": "git/intro_resources.html#online-documentation",
    "href": "git/intro_resources.html#online-documentation",
    "title": "Resources",
    "section": "",
    "text": "Official Git manual\nOpen source Pro Git book",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Resources"
    ]
  },
  {
    "objectID": "git/intro_resources.html#courses-workshops",
    "href": "git/intro_resources.html#courses-workshops",
    "title": "Resources",
    "section": "Courses & workshops",
    "text": "Courses & workshops\n\nWestern Canada Research Computing Git workshops\nWestGrid Summer School 2020 Git course\nWestGrid Autumn School 2020 Git course\nSoftware Carpentry Git lesson",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Resources"
    ]
  },
  {
    "objectID": "git/intro_resources.html#q-a",
    "href": "git/intro_resources.html#q-a",
    "title": "Resources",
    "section": "Q & A",
    "text": "Q & A\n\nStack Overflow [git] tag",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Resources"
    ]
  },
  {
    "objectID": "git/intro_resources.html#troubleshooting",
    "href": "git/intro_resources.html#troubleshooting",
    "title": "Resources",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n\n“Listen” to Git!\nGit is extremely verbose: by default, it will return lots of information. Read it!\nThese messages may feel overwhelming at first, but:\n\nthey will make more and more sense as you gain expertise,\nthey often give you clues as to what the problem is,\neven if you don’t understand them, you can use them as Google search terms.\n\n\n\n(Re-read) the doc\nAs I have no memory, I need to check the man pages all the time. That’s ok! It is quick and easy.\nFor more detailed information and examples, I really like the Official Git manual.\n\n\nSearch online\n\nGoogle\nStack Overflow [git] tag\n\n\n\nDon’t panic\nBe analytical. It is easy to panic and feel lost if something doesn’t work as expected. Take a breath and start with the basis:\n\nmake sure you are in the repo (pwd) and the files are where you think they are (ls -a),\ninspect the repository (git status, git diff, git log). Make sure not to overlook what Git is “telling” you there.",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Resources"
    ]
  },
  {
    "objectID": "git/intro_remotes.html",
    "href": "git/intro_remotes.html",
    "title": "Remotes",
    "section": "",
    "text": "Remotes are copies of a project and its history.\nThey can be located anywhere, including on external drive or on the same machine as the project, although they are often on a different machine to serve as backup, or on a network (e.g. internet) to serve as a syncing hub for collaborations.\nPopular online Git repository managers & hosting services:\n\nGitHub\nGitLab\nBitbucket\n\nLet’s see how to create and manage remotes.",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Remotes"
    ]
  },
  {
    "objectID": "git/intro_remotes.html#creating-a-remote-on-github",
    "href": "git/intro_remotes.html#creating-a-remote-on-github",
    "title": "Remotes",
    "section": "Creating a remote on GitHub",
    "text": "Creating a remote on GitHub\n\nCreate a free GitHub account\nIf you don’t already have one, sign up for a free GitHub account.\n\nTo avoid having to type your password all the time, you should set up SSH for your account.\n\n\n\nCreate an empty repository on GitHub\n\nGo to the GitHub website, login, and go to your home page.\nLook for the Repositories tab & click the green New button.\nEnter the name you want for your repo, without spaces.\nMake the repository public or private.\n\n\n\nLink empty repository to your repo\nClick on the Code green drop-down button, select SSH if you have set SSH for your GitHub account or HTTPS and copy the address.\nIn the command line, cd inside your project, and add the remote:\ngit remote add &lt;remote-name&gt; &lt;remote-address&gt;\nremote-name is a convenience name to identify that remote. You can choose any name, but since Git automatically call the remote origin when you clone a repo, it is common practice to use origin as the name for the first remote.\n\nExample (using an SSH address):\n\ngit remote add origin git@github.com:&lt;user&gt;/&lt;repo&gt;.git\n\nExample (using an HTTPS address):\n\ngit remote add origin https://github.com/&lt;user&gt;/&lt;repo&gt;.git",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Remotes"
    ]
  },
  {
    "objectID": "git/intro_remotes.html#getting-information-on-remotes",
    "href": "git/intro_remotes.html#getting-information-on-remotes",
    "title": "Remotes",
    "section": "Getting information on remotes",
    "text": "Getting information on remotes\nList remotes:\ngit remote\nList remotes with their addresses:\ngit remote -v\nGet more information on a remote:\ngit remote show &lt;remote-name&gt;\n\nExample:\n\ngit remote show origin",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Remotes"
    ]
  },
  {
    "objectID": "git/intro_remotes.html#managing-remotes",
    "href": "git/intro_remotes.html#managing-remotes",
    "title": "Remotes",
    "section": "Managing remotes",
    "text": "Managing remotes\nRename a remote:\ngit remote rename &lt;old-remote-name&gt; &lt;new-remote-name&gt;\nDelete a remote:\ngit remote remove &lt;remote-name&gt;\nChange the address of a remote:\ngit remote set-url &lt;remote-name&gt; &lt;new-url&gt; [&lt;old-url&gt;]",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Remotes"
    ]
  },
  {
    "objectID": "git/intro_remotes.html#getting-data-from-a-remote",
    "href": "git/intro_remotes.html#getting-data-from-a-remote",
    "title": "Remotes",
    "section": "Getting data from a remote",
    "text": "Getting data from a remote\nIf you collaborate on a project, you have to get the data added by your teammates to keep your local project up to date.\nTo download new data from a remote, you have 2 options:\n\ngit fetch\ngit pull\n\n\nFetching changes\nFetching downloads the data from a remote that you don’t already have in your local version of the project:\ngit fetch &lt;remote-name&gt;\nThe branches on the remote are now accessible locally as &lt;remote-name&gt;/&lt;branch&gt;. You can inspect them or you can merge them into your local branches.\n\nExample:\n\ngit fetch origin\n\n\nPulling changes\nPulling fetches the changes & merges them onto your local branches:\ngit pull &lt;remote-name&gt; &lt;branch&gt;\n\nExample:\n\ngit pull origin main\nIf your branch is already tracking a remote branch, you can omit the arguments:\ngit pull",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Remotes"
    ]
  },
  {
    "objectID": "git/intro_remotes.html#pushing-to-a-remote",
    "href": "git/intro_remotes.html#pushing-to-a-remote",
    "title": "Remotes",
    "section": "Pushing to a remote",
    "text": "Pushing to a remote\nUploading data to the remote is called pushing:\ngit push &lt;remote-name&gt; &lt;branch-name&gt;\n\nExample:\n\ngit push origin main\nYou can set an upstream branch to track a local branch with the -u flag:\ngit push -u &lt;remote-name&gt; &lt;branch-name&gt;\n\nExample:\n\ngit push -u origin main\nFrom now on, all you have to run when you are on main is:\ngit push\n \n\nby jscript",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Remotes"
    ]
  },
  {
    "objectID": "git/intro_install.html",
    "href": "git/intro_install.html",
    "title": "Installation and setup",
    "section": "",
    "text": "In this section, we will install and configure Git.",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Installation and setup"
    ]
  },
  {
    "objectID": "git/intro_install.html#installing-git",
    "href": "git/intro_install.html#installing-git",
    "title": "Installation and setup",
    "section": "Installing Git",
    "text": "Installing Git\n\nMacOS/Linux users\nInstall Git from the official website.\n\n\nWindows users\nInstall Git for Windows. This will also install Git Bash, a Bash emulator.\n\nGit is built for Unix-like systems (Linux and MacOS). In order to use Git from the command line on Windows, you need a Unix shell such as Bash. To make this very easy, Git for Windows comes with its Bash emulator.",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Installation and setup"
    ]
  },
  {
    "objectID": "git/intro_install.html#using-git",
    "href": "git/intro_install.html#using-git",
    "title": "Installation and setup",
    "section": "Using Git",
    "text": "Using Git\nWe will use Git from the command line throughout this workshop.\nMacOS users:    open Terminal.\nWindows users:   open Git Bash.\nLinux users:    open the terminal emulator of your choice.",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Installation and setup"
    ]
  },
  {
    "objectID": "git/intro_install.html#configuring-git",
    "href": "git/intro_install.html#configuring-git",
    "title": "Installation and setup",
    "section": "Configuring Git",
    "text": "Configuring Git\nBefore you can use Git, you need to set some basic configuration. You will do this in the terminal you just opened.\n\nList settings\ngit config --list\n\n\nUser identity\ngit config --global user.name \"&lt;Your Name&gt;\"\ngit config --global user.email \"&lt;your@email&gt;\"\n\nExample:\n\ngit config --global user.name \"John Doe\"\ngit config --global user.email \"john.doe@gmail.com\"\n\nIt is recommended to use your real name and real email address: when you will collaborate on projects, you will probably want this information to be attached to your commits rather than a weird pseudo.\n\n\n\nText editor\ngit config --global core.editor \"&lt;text-editor&gt;\"\n\nExample for nano:\n\ngit config --global core.editor \"nano\"\n\n\nLine ending\n\nmacOS, Linux, or WSL\ngit config --global core.autocrlf input\n\n\nWindows\ngit config --global core.autocrlf true\n\n\n\nProject-specific configuration\nYou can also set project-specific configurations (e.g. maybe you want to use a different email address for a certain project).\nIn that case, navigate to your project and run the command without the --global flag.\n\nExample:\n\ncd /path/to/project\ngit config user.email \"your_other@email\"",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Installation and setup"
    ]
  },
  {
    "objectID": "git/intro_first_steps.html",
    "href": "git/intro_first_steps.html",
    "title": "First steps",
    "section": "",
    "text": "In this section, we will initialize our first Git repository, learn to explore it, and create a few commits.",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "First steps"
    ]
  },
  {
    "objectID": "git/intro_first_steps.html#get-a-mock-project",
    "href": "git/intro_first_steps.html#get-a-mock-project",
    "title": "First steps",
    "section": "Get a mock project",
    "text": "Get a mock project\nLet’s download a mock project with a couple of files to practice with.\n\n1. Download the zip file\nNavigate to a suitable location (e.g. cd ~), then download the file with:\nwget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1SJV5mRGexf91lNyFwdS_JmuAXX0xS4pE' -O project.zip\n\nAlternatively, you can download the file with this button:  Download the data \nNote that if you do this, you will have to know how to navigate to your Downloads directory to find the file.\n\n\n\n2. Unzip the file\nUnzip the file with:\nunzip project.zip\nYou should now have a project directory with a number of subdirectories and files. This is the project we will use today.\n\n\n3. Enter in the project root\ncd in the root of the project:\ncd project",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "First steps"
    ]
  },
  {
    "objectID": "git/intro_first_steps.html#inspect-the-project",
    "href": "git/intro_first_steps.html#inspect-the-project",
    "title": "First steps",
    "section": "Inspect the project",
    "text": "Inspect the project\nFirst, let’s have a look at our (very small) mock project.\nLet’s list the content of project:\nls -F\ndata/\nms/\nresults/\nsrc/\nThere are 4 subdirectories.\nls -a     # Show hidden files\n.\n..\ndata\nms\nresults\nsrc\nAs you can see, there are no hidden files.\nls -R\n.:\ndata\nms\nresults\nsrc\n\n./data:\ndataset.csv\n\n./ms:\nproposal.md\n\n./results:\n\n./src:\nscript.py\nNow we can see the content of each subdirectory.\nYou probably don’t have the tree command on your machine, so this won’t work for you, but don’t worry about it: this is only to show you the same result in a more readable format:\ntree\n.\n├── data\n│   └── dataset.csv\n├── ms\n│   └── proposal.md\n├── results\n└── src\n    └── script.py\n\n5 directories, 3 files\nLet’s look at the content of the files.\nThis is our very exciting data set:\ncat data/dataset.csv\nvar1,var2,var3,\n1,2,1,\n0,1,0,\n3,3,3\nAnd a no less exciting manuscript (the proposal for our project):\ncat ms/proposal.md\n# Summary\n\nThis is the summary for our proposal.\n\n# Funding\n\nInformation on funding for the project.\n\n# Methods\n\nHere we have some methods using our Python scripts.\n\n# Expected results\n\nWe hope to achieve a lot.\n\n# Conclusion\n\nThis is truly a great proposal.\nAnd finally, a Python script:\ncat src/script.py\nimport pandas as pd\nfrom matplotlib import pyplot as plt\n\ndf = pd.read_csv('../data/dataset.csv')\n\ndf.plot()\nplt.savefig('../results/plot.png', dpi=300)",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "First steps"
    ]
  },
  {
    "objectID": "git/intro_first_steps.html#initializing-a-git-repository",
    "href": "git/intro_first_steps.html#initializing-a-git-repository",
    "title": "First steps",
    "section": "Initializing a Git repository",
    "text": "Initializing a Git repository\nOur project is just a bunch of files and subdirectories. To turn it into a Git repository, we run:\ngit init\nInitialized empty Git repository in project/.git/\n\nMake sure to be at the root of the project (here, inside the project directory) before initializing the repository.\n\n\nGit is verbose: you will often get useful feed-back after running commands. Read them!\n\nWhen you run this command, Git creates a .git repository. This is where it will store all its files (all those blob objects, pointers, and other files).\nYou can see that this repository was created by running:\nls -a\n.\n..\n.git\ndata\nms\nresults\nsrc\n\nIf you run git init in the wrong location, you can easily fix this by deleting the .git directory that you created (e.g. rm -r .git).\n\n\nGit commands\nAs you might have already noticed, Git commands start with git.\nA typical command is of the form:\ngit &lt;command&gt; [flags] [arguments]\n\nExample of a command we used to configure Git:\n\ngit config --global \"Your Name\"\n\nExample of a much simpler command with no flag nor argument:\n\ngit init",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "First steps"
    ]
  },
  {
    "objectID": "git/intro_first_steps.html#status-of-the-repository",
    "href": "git/intro_first_steps.html#status-of-the-repository",
    "title": "First steps",
    "section": "Status of the repository",
    "text": "Status of the repository\nOne command you will run often when working with Git is git status. It gives you information on new changes to your project:\ngit status\nOn branch main\n\nNo commits yet\n\nUntracked files:\n  (use \"git add &lt;file&gt;...\" to include in what will be committed)\n        data/\n        ms/\n        src/\n\nnothing added to commit but untracked files present (use \"git add\" to track)\nThere is a lot of information here:\n\nWe are on branch main. That is the default name for the branch that gets created automatically as soon as we initialize a Git repository.\nThere are no commits yet (normal: we just initialized a new repository).\nThere are untracked files in our repo.\n\nIt is time to create a first commit…",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "First steps"
    ]
  },
  {
    "objectID": "git/intro_first_steps.html#creating-commits",
    "href": "git/intro_first_steps.html#creating-commits",
    "title": "First steps",
    "section": "Creating commits",
    "text": "Creating commits\nRemember that commits are those “snapshots” of your project at certain moments in time. You should create a new commit whenever you think that your project is at a point to which you might want to go back to later. There is no rule about when or how often to create commits: it is really up to you.\nBefore we create our first commit, we need to decide what file(s) we want to add to this commit.\nWe could add everything.\nWe can also be more selective and add files one by one.\nWe can even add only sections of files.\nHow do we tell Git what to add to the next commit?\n\nThe staging area\nGit has a staging area: a way to select what to add to the next commit. Files (or sections of files) get added to the staging area with git add.\nTo add all the files at once, we run, from the root of the project:\ngit add .\nThe . represents the current directory. Because Git adds files recursively, this will add all new files.\nTo add a particular file, we add its path as an argument to git add.\n\nExample:\n\ngit add ms/proposal.md\nTo add all the files in a directory, we add the path of that directory as an argument to git add.\n\nExample:\n\ngit add ms\n\nSince there is a single file in the ms directory, both commands will in our case lead to the same result.\n\nLet’s run that last command:\ngit add ms\nIt looks like nothing happened. Did it work? How can I know?\nAnswer: by running git status again. That’s the command to go to whenever you need to get some update on the status of the repo:\ngit status\nOn branch main\n\nNo commits yet\n\nChanges to be committed:\n  (use \"git rm --cached &lt;file&gt;...\" to unstage)\n        new file:   ms/proposal.md\n\nUntracked files:\n  (use \"git add &lt;file&gt;...\" to include in what will be committed)\n        data/\n        src/\nIt worked! We can see that the content of the ms subdirectory is ready to be committed. It is in the staging area.\n\n\nYour turn:\n\nWhat if we change our mind as to the composition of our first commit and we want to create our first commit with the Python script instead? To do that, we need to unstage proposal.md first.\nHow can we do that? Give it a try.\nThen, how can we stage the Python script?\n\n\n\nCommitting\nOnce we are happy with the content of the future commit, it is time to create it. The command for this is git commit.\nRemember that each commit contains the following metadata:\n\nauthor,\ndate and time,\nthe hash of parent commit(s),\na message.\n\nThe first three can be set by Git automatically (you have configured Git so it knows who the author is).\nGit has no way to come up with the forth one however. You need to create it yourself.\nIf you run git commit, Git will open your text editor so that you can type the message. If you want to enter the message directly from the command line, you use the -m flag followed by the quoted message:\ngit commit -m \"Initial commit\"\n[main (root-commit) 61abf96] Initial commit\n 1 file changed, 19 insertions(+)\n create mode 100644 ms/proposal.md\nWe now have a first commit. Its hash starts (in my case—yours will be different of course) with 61abf96.\n\nGood commit messages\n\n\nFrom xkcd.com\n\n\nUse the present tense.\nThe first line is a summary of the commit and is less than 50 characters long.\nLeave a blank line below.\nThen add the body of your commit message with more details.\n\n\nExample of a good commit message:\n\ngit commit -m \"Reduce boundary conditions by a factor of 0.3\n\nUpdate boundaries\nRerun model and update table\nRephrase method section in ms\"\nFuture you will thank you! (And so will your collaborators).\n\nIf we run git status once more, we now get:\nOn branch main\nUntracked files:\n  (use \"git add &lt;file&gt;...\" to include in what will be committed)\n        data/\n        src/\n\nnothing added to commit but untracked files present (use \"git add\" to track)\nThe directory ms has disappeared from the list of untracked files: its content has now been committed to history.\n\n\nUnderstanding the staging area\nNew Git users are often confused about the two-step commit process (first, you stage with git add, then you commit with git commit). This intermediate step seems, at first, totally unnecessary. In fact, it is very useful: without it, commits would always include all new changes made to a project and they would thus be very messy. The staging area allows to prepare (“stage”) the next commit. This way, you only commit what you want when you want.\n\nLet’s go over a simple example:\n\nWe don’t always work linearly. Maybe you are working on a section of your manuscript when you realize by chance that there is a mistake in your script. You fix that mistake. On your next commit, it might make little sense to commit together that fix and your manuscript changes since they are not related. If your commits are random bag of changes, it will be very hard for future you to navigate your project history.\nIt is a lot better to only stage your script fix, commit it, then only stage your manuscript update, and commit this in a different commit.\nThe staging area allows you to pick and chose the changes from one or various files that constitute some coherent change to the project and that make sense to commit together.",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "First steps"
    ]
  },
  {
    "objectID": "git/intro_changes.html",
    "href": "git/intro_changes.html",
    "title": "Inspecting changes",
    "section": "",
    "text": "While git status gives you information on the files that were changed since the last commit, it doesn’t provide any information on what those changes are.\nIn this section, we will see how we can get information on the changes to the files contents.",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Inspecting changes"
    ]
  },
  {
    "objectID": "git/intro_changes.html#the-three-trees-of-git",
    "href": "git/intro_changes.html#the-three-trees-of-git",
    "title": "Inspecting changes",
    "section": "The three trees of Git",
    "text": "The three trees of Git\nBefore we can jump into this section, we need to understand a bit more how Git works.\nOne useful mental representation is to imagine three file trees:\n\n\nThe working tree\nLet’s imagine that you are starting to work on a project.\nFirst, you create a directory.\nIn it, you create several sub-directories.\nIn those, you create a number of files.\nYou can open these files, read them, edit them, etc. This is something you are very familiar with.\nIn the Git world, this is the working directory or working tree of the project.\nThat is: an uncompressed version of your files that you can access and edit.\nYou can think of it as a sandbox because this is where you can experiment with the project. This is where the project gets developed.\nNow, Git has two other important pieces in its architecture.\n\n\nThe index\nIf you want the project history to be useful to future you, it has to be nice and tidy. You don’t want to record snapshots haphazardly or you will never be able to find anything back.\nBefore you record a snapshot, you carefully select the elements of the project as it is now that would be useful to write to the project history together. The index or staging area is what allows to do that: it contains the suggested future commit.\n\n\nHEAD\nFinally, the last tree in Git architecture is one snapshot in the project history that serves as a reference version of the project: if you want to see what you have been experimenting on in your “sandbox”, you need to compare the state of the working directory with some snapshot.\nRemember that HEAD is a pointer pointing at a branch, that a branch is itself a pointer pointing at a commit, and finally that a commit is a Git object pointing at compressed blobs containing data about your project at a certain commit. When the HEAD pointer moves around, whatever commit it points to populates the HEAD tree with the corresponding data.\nAs we saw earlier, when you create a commit, HEAD automatically points to the new commit. So the HEAD tree is often filled with the last snapshot you created. But—as we will see later—we can move the HEAD pointer around through other ways. So the HEAD tree can be populated by any snapshot in your project history.",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Inspecting changes"
    ]
  },
  {
    "objectID": "git/intro_changes.html#git-diff",
    "href": "git/intro_changes.html#git-diff",
    "title": "Inspecting changes",
    "section": "git diff",
    "text": "git diff\nThe command git diff prints the differences between any two Git objects. In particular, it allows to compare any two of the three trees with each other.\n\nDiff between working directory & index\n\ngit diff displays the differences between the working directory (the uncompressed files) and the index (the staging area):\ngit diff\nRight now, git diff does not return anything because our working tree and staging area are at the same point.\nLet’s make some changes in our proposal:\nnano ms/proposal.md\nNow, if we run git diff again, we get:\ndiff --git a/ms/proposal.md b/ms/proposal.md\nindex 0fb5cc8..4fdc1b0 100644\n--- a/ms/proposal.md\n+++ b/ms/proposal.md\n@@ -16,4 +16,4 @@ We hope to achieve a lot.\n\n # Conclusion\n\n-This is truly a great proposal.\n+This is truly a great proposal, but it needs a little more work.\n\n\nDiff between index & last commit\n\nTo see what would be committed if you ran git commit (that is the differences between the index and the last commit), you need to run instead:\ngit diff --cached\nWe aren’t getting any output because we haven’t staged anything that isn’t in our last commit. Let’s stage our changes and try again:\ngit add .\ngit diff --cached\ndiff --git a/ms/proposal.md b/ms/proposal.md\nindex 0fb5cc8..4fdc1b0 100644\n--- a/ms/proposal.md\n+++ b/ms/proposal.md\n@@ -16,4 +16,4 @@ We hope to achieve a lot.\n\n # Conclusion\n\n-This is truly a great proposal.\n+This is truly a great proposal, but it needs a little more work.\nThe changes are now between the staging area and HEAD.\n\nIf we run git diff now, we aren’t getting any output because the staging area has caught up with the working tree: they are now the same, so no differences.\n\ngit diff --cached is very convenient to check what will enter in the next commit.\n\n\nDiff between working directory & last commit\n\nFinally, to see the differences between your working directory and HEAD, you run:\ngit diff HEAD\nThis will be the sum of the previous two.\nRight now, git diff --cached and git diff HEAD print the same result, but if we make new changes, they will become different since git diff HEAD will reflect all the changes between the working tree and the last commit, while git diff --cached will only contain the differences between the staging area and the last commit.\n\n\nYour turn:\n\nModify one of the tracked files to visualize this.",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Inspecting changes"
    ]
  },
  {
    "objectID": "git/intro_aliases.html",
    "href": "git/intro_aliases.html",
    "title": "Aliases",
    "section": "",
    "text": "Sometimes, you want to replace longer Git commands with a shorter version. For example, wouldn’t it be nice if instead of typing git status you could type git st? It turns out this is easy to do via Git aliases – all you need to do is run the following command:\ngit config --global alias.st 'status'\nThis will create a Git alias (new Git command) st that will run status underneath.\nOf course, this becomes very useful with longer commands, e.g.\ngit config --global alias.last 'diff HEAD~1 HEAD'\ngit last\nwill show the file changes in the last commit, i.e. underneath git last will run git diff HEAD~1 HEAD.\nAll these definitions are stored in your ~/.gitconfig file, and you can always check them with (pay attention to the alias section in the output):\ngit config --list\nConsider the following useful aliases:\ngit config --global alias.list 'ls-tree --full-tree -r HEAD'\ngit config --global alias.one \"log --graph --date-order --date=short --pretty=format:'%C(cyan)%h %C(yellow)%ar %C(auto)%s%+b %C(green)%ae'\"\nNow try running:\ngit list   # list all files inside the repository\ngit one    # very nicely formatted version of git log\nNow, let’s build an alias for a more complex command: git grep \"test\" $(git rev-list --all). This example from the “Searching a Git project” section below will search for the string “test” in all previous commits. There are two problems with this command: (1) it takes an argument (the string “test”), and (2) it uses an output from another Unix command (git rev-list --all) as its input.\nIt turns out you can still automate this with a Git alias stored as a bash script! You need to place an executable bash script into a directory that is listed in your $PATH environment variable. For example, let’s assume that $HOME/bin is listed in your $PATH. Doing the following\ncat &lt;&lt; EOF &gt; $HOME/bin/git-search\n#!/bin/bash\ngit grep \\${1} \\$(git rev-list --all)\nEOF\nchmod u+x $HOME/bin/git-search\nwill place the script into the file $HOME/bin/git-search and will make it executable. Now, running\ngit search test\nshould search the entire current Git project history for “test”.",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Aliases"
    ]
  },
  {
    "objectID": "emacs/wb_emacs_new_tools_slides.html#lexical-binding",
    "href": "emacs/wb_emacs_new_tools_slides.html#lexical-binding",
    "title": "Modern Emacs",
    "section": "Lexical binding",
    "text": "Lexical binding\nIntroduced in version 24\nLexical binding can be used instead of dynamic binding for Emacs Lisp code\nSet as a file local variable\n\n\nDynamic binding\nName resolution depends on program state (runtime context), determined at run time\nGlobal environment for all variables\nMakes modifying behaviour easy\n\n\n\nLexical binding\nName resolution depends on lexical context (static context), determined at compile time\nLocal environments of functions and let, defconst, defvar, etc. expressions\nMakes compiler optimization much easier → faster Elisp code = faster Emacs"
  },
  {
    "objectID": "emacs/wb_emacs_new_tools_slides.html#jit-native-compilation",
    "href": "emacs/wb_emacs_new_tools_slides.html#jit-native-compilation",
    "title": "Modern Emacs",
    "section": "JIT native compilation",
    "text": "JIT native compilation\nIntroduced in version 28\nRequires libgccjit\nBuild Emacs --with-native-compilation\nPackages can also be compiled natively (automatic with straight)\n\nFaster startup\nSpeedup of 2.5 to 5 compared to corresponding byte-compiled code"
  },
  {
    "objectID": "emacs/wb_emacs_new_tools_slides.html#lazy-loading",
    "href": "emacs/wb_emacs_new_tools_slides.html#lazy-loading",
    "title": "Modern Emacs",
    "section": "Lazy loading",
    "text": "Lazy loading\nBuilt-in since version 29\nFine-tuned loading of packages with use-package\nIntegrates nicely with straight\n\nFaster startup time\nMore organized init file\nEasier to reload configurations for single package"
  },
  {
    "objectID": "emacs/wb_emacs_new_tools_slides.html#accurate-syntax-tree",
    "href": "emacs/wb_emacs_new_tools_slides.html#accurate-syntax-tree",
    "title": "Modern Emacs",
    "section": "Accurate syntax tree",
    "text": "Accurate syntax tree\nBuilt-in since version 29\nTree-sitter for Emacs\nCode is parsed accurately instead of using regexp\n\nPerfect syntax highlighting, indentation, and navigation\nFaster\n\nSimplest setup with treesit-auto:\n(use-package treesit-auto\n  :config\n  (treesit-auto-add-to-auto-mode-alist 'all))"
  },
  {
    "objectID": "emacs/wb_emacs_new_tools_slides.html#history-of-code-completion-in-emacs-ido",
    "href": "emacs/wb_emacs_new_tools_slides.html#history-of-code-completion-in-emacs-ido",
    "title": "Modern Emacs",
    "section": "History of code completion in Emacs: IDO",
    "text": "History of code completion in Emacs: IDO\n\n\n\nFrom Xah Emacs Blog"
  },
  {
    "objectID": "emacs/wb_emacs_new_tools_slides.html#history-of-code-completion-in-emacs-ido-vertical",
    "href": "emacs/wb_emacs_new_tools_slides.html#history-of-code-completion-in-emacs-ido-vertical",
    "title": "Modern Emacs",
    "section": "History of code completion in Emacs: IDO vertical",
    "text": "History of code completion in Emacs: IDO vertical\n\n\n\nFrom oremacs"
  },
  {
    "objectID": "emacs/wb_emacs_new_tools_slides.html#history-of-code-completion-in-emacs-helm",
    "href": "emacs/wb_emacs_new_tools_slides.html#history-of-code-completion-in-emacs-helm",
    "title": "Modern Emacs",
    "section": "History of code completion in Emacs: HELM",
    "text": "History of code completion in Emacs: HELM\n\n\n\nFrom oracleyue"
  },
  {
    "objectID": "emacs/wb_emacs_new_tools_slides.html#history-of-code-completion-in-emacs-ivy",
    "href": "emacs/wb_emacs_new_tools_slides.html#history-of-code-completion-in-emacs-ivy",
    "title": "Modern Emacs",
    "section": "History of code completion in Emacs: Ivy",
    "text": "History of code completion in Emacs: Ivy\nWith optional Counsel & Swiper\n\n\n\nFrom abo-abo/swiper"
  },
  {
    "objectID": "emacs/wb_emacs_new_tools_slides.html#new-framework",
    "href": "emacs/wb_emacs_new_tools_slides.html#new-framework",
    "title": "Modern Emacs",
    "section": "New framework",
    "text": "New framework\nExternal packages\nUse default Emacs functions (less code)\nFaster, flexible, customizable with discrete units"
  },
  {
    "objectID": "emacs/wb_emacs_new_tools_slides.html#packages",
    "href": "emacs/wb_emacs_new_tools_slides.html#packages",
    "title": "Modern Emacs",
    "section": "Packages",
    "text": "Packages\n\n\nMinibuffer\n\nvertico    frontend completion UI\norderless  backend completion style\nconsult   backend completion functions\nmarginalia  annotations\nembark     actions on completion buffer\n\n\nIn buffer\n\ncorfu    frontend completion UI\norderless  backend completion style\ncape    backend completion functions\neglot     backend LSP client"
  },
  {
    "objectID": "emacs/wb_emacs_new_tools_slides.html#compared-to-previous-frameworks",
    "href": "emacs/wb_emacs_new_tools_slides.html#compared-to-previous-frameworks",
    "title": "Modern Emacs",
    "section": "Compared to previous frameworks",
    "text": "Compared to previous frameworks\n\nIntegrates beautifully with internal Emacs functions\nEasy jump back & forth between buffer and completion buffer\nMuch faster than HELM\nLightning fast previews with auto-closing buffers\nEasy customization"
  },
  {
    "objectID": "emacs/wb_emacs_new_tools_slides.html#example-configuration",
    "href": "emacs/wb_emacs_new_tools_slides.html#example-configuration",
    "title": "Modern Emacs",
    "section": "Example configuration",
    "text": "Example configuration\nVertico (frontend for completion in minibuffer)\n(use-package vertico\n  :init\n  (vertico-mode 1)\n  (vertico-multiform-mode 1)\n  :config\n  (setq vertico-multiform-commands\n    '((consult-line buffer)\n      (consult-line-thing-at-point buffer)\n      (consult-recent-file buffer)\n      (consult-mode-command buffer)\n      (consult-complex-command buffer)\n      (embark-bindings buffer)\n      (consult-locate buffer)\n      (consult-project-buffer buffer)\n      (consult-ripgrep buffer)\n      (consult-fd buffer)))\n  :bind (:map vertico-map\n          (\"C-k\" . kill-whole-line)\n          (\"C-u\" . kill-whole-line)\n          (\"C-o\" . vertico-next-group)\n          (\"&lt;tab&gt;\" . minibuffer-complete)\n          (\"M-&lt;return&gt;\" . minibuffer-force-complete-and-exit)))\n\n;; save search history\n(use-package savehist\n  :init\n  (savehist-mode 1))"
  },
  {
    "objectID": "emacs/wb_emacs_new_tools_slides.html#language-server-protocol-client",
    "href": "emacs/wb_emacs_new_tools_slides.html#language-server-protocol-client",
    "title": "Modern Emacs",
    "section": "Language Server Protocol client",
    "text": "Language Server Protocol client\nBuilt-in since version 29\nEglot (Emacs Polyglot) allows to connect to a programming language server\nExample: Julia\nNeed to install an LSP for Julia:\n(straight-use-package 'eglot-jl)\nThen run eglot-jl-init\nNow eglot in a Julia buffer connects to the server\n\nSimilarly, you can install an LSP for R or Python or any language and use Eglot with R, Python, or whatever language"
  },
  {
    "objectID": "emacs/wb_emacs_new_tools_slides.html#to-all-emacs-developers-and-maintainers",
    "href": "emacs/wb_emacs_new_tools_slides.html#to-all-emacs-developers-and-maintainers",
    "title": "Modern Emacs",
    "section": "❤ to all Emacs developers and maintainers",
    "text": "❤ to all Emacs developers and maintainers\nIn particular,\ndevelopers, maintainers, and contributors to Emacs core,\ndevelopers and maintainers to some of the mentioned packages:\nDaniel Mendler\nOmar Antolín Camarena\nJoão Távora\nRobb Enzmann\nJohn Wiegley\nAdam B\nand all their contributors\n\n\n\n\n Back to webinar page"
  },
  {
    "objectID": "emacs/wb_emacs_ide_slides.html#helm",
    "href": "emacs/wb_emacs_ide_slides.html#helm",
    "title": "Emacs as a programming IDE",
    "section": "Helm",
    "text": "Helm\nSearching in buffer\n\nNavigating open buffers and recent files\n\n\nNavigating file sections\n\n\nSelecting from kill ring\n\n\nMoving in mark ring\n\n\nLooking at active modes"
  },
  {
    "objectID": "emacs/wb_emacs_ide_slides.html#completion",
    "href": "emacs/wb_emacs_ide_slides.html#completion",
    "title": "Emacs as a programming IDE",
    "section": "Completion",
    "text": "Completion\ncompany-mode\n\nyasnippet\n\n\nDynamic abbrev expansion"
  },
  {
    "objectID": "emacs/wb_emacs_ide_slides.html#undoingredoing-with-undo-tree",
    "href": "emacs/wb_emacs_ide_slides.html#undoingredoing-with-undo-tree",
    "title": "Emacs as a programming IDE",
    "section": "Undoing/redoing with undo-tree",
    "text": "Undoing/redoing with undo-tree"
  },
  {
    "objectID": "emacs/top_wb.html",
    "href": "emacs/top_wb.html",
    "title": "Emacs webinars",
    "section": "",
    "text": "Emacs as a programming IDE\n\n\n\n\nModern Emacs\n\n\n\n\nEmacs Polymode",
    "crumbs": [
      "Emacs",
      "<b><em>Webinars</em></b>"
    ]
  },
  {
    "objectID": "emacs/intro_packages.html",
    "href": "emacs/intro_packages.html",
    "title": "Packages",
    "section": "",
    "text": "Package management system\nElpa\nstraight\nuse-package"
  },
  {
    "objectID": "emacs/intro_concepts.html",
    "href": "emacs/intro_concepts.html",
    "title": "Concepts",
    "section": "",
    "text": "Frame Buffer Minibuffer Mode line\nMajor mode Minor mode"
  },
  {
    "objectID": "emacs/index.html",
    "href": "emacs/index.html",
    "title": "Emacs",
    "section": "",
    "text": "Getting started with  \nAn intro course to Emacs\n\n\n\n\n60 min webinars\nVarious Emacs topics",
    "crumbs": [
      "Emacs",
      "<br>&nbsp;<img src=\"img/logo_emacs.png\" class=\"img-fluid\" style=\"width:1.75em\" alt=\"noshadow\"><br><br>"
    ]
  },
  {
    "objectID": "calendar.html",
    "href": "calendar.html",
    "title": "Upcoming training events",
    "section": "",
    "text": "Our training events also get posted on our main site."
  },
  {
    "objectID": "bash/ws_scripting.html#background",
    "href": "bash/ws_scripting.html#background",
    "title": "Automation & scripting in bash for beginners",
    "section": "Background",
    "text": "Background\n\nWhat are Unix shells?\nA Unix shell is a command line interpreter: the user enters commands as text, either interactively in the command line or in a script, and the shell passes them to the operating system.\n\nBash\nBash (Bourne Again SHell), released in 1989, is part of the GNU Project and is the default Unix shell on many systems (MacOS recently changed its default to zsh).\n\n\nOther shells\nPrior to Bash, the default was the Bourne shell (sh).\nA new and popular shell (backward compatible with Bash) is zsh. It extends Bash’s capabilities.\nAnother shell in the same family is the KornShell (ksh).\nAll these shells are quite similar. The C shell (csh) however was modeled on the C programming language.\nBash is the most common shell and the one which makes the most sense to learn as a first Unix shell.\n\n\n\nWhy use a shell?\nWhile automating GUI operations is really difficult, it is easy to rerun a script (a file with a number of commands). Unix shells thus allow the creation of reproducible workflows and the automation of repetitive tasks.\nThey are powerful to launch tools, modify files, search text, or combine commands.\nThey also allow to work on remote machines and HPC systems.",
    "crumbs": [
      "Bash",
      "<b><em>Workshops</em></b>",
      "Scripting for beginners"
    ]
  },
  {
    "objectID": "bash/ws_scripting.html#how-we-will-use-bash-today",
    "href": "bash/ws_scripting.html#how-we-will-use-bash-today",
    "title": "Automation & scripting in bash for beginners",
    "section": "How we will use Bash today",
    "text": "How we will use Bash today\nBash is a Unix shell. You thus need a Unix or Unix-like operating system.\nWe will connect to a remote HPC system via SSH (secure shell). HPC systems always run Linux.\nThose on Linux or MacOS can alternatively use Bash directly on their machine. On MacOS, the default is now zsh (you can see that by typing echo $SHELL in Terminal), but zsh is fully compatible with Bash commands, so it is totally fine to use it instead. If you really want to use Bash, simply launch it by typing in Terminal: bash.\n\nConnecting to a remote HPC system via SSH\n\nUsernames and password\nWe will give you a link to an etherpad during the workshop. Add your name next to a free username to claim it.\nWe will also give you the password for our training cluster. When prompted, enter it.\n\nNote that you will not see any character as you type the password: this is called blind typing and is a Linux safety feature. Type slowly and make sure not to make typos. It can be unsettling at first not to get any feed-back while typing.\n\n\n\nLinux and MacOS users\nLinux users: open the terminal emulator of your choice.\nMacOS users: open “Terminal”.\nThen type:\nssh userxx@bashworkshop.c3.ca  # Replace userxx by your username (e.g. user09)\n\n\nWindows users\nWe suggest using the free version of MobaXterm.\nMobaXterm comes with a terminal emulator and a GUI interface for SSH sessions.\nOpen MobaXterm, click on “Session”, then “SSH”, and fill in the Remote host name and your username. Here is a live demo.",
    "crumbs": [
      "Bash",
      "<b><em>Workshops</em></b>",
      "Scripting for beginners"
    ]
  },
  {
    "objectID": "bash/ws_scripting.html#bash-the-basics",
    "href": "bash/ws_scripting.html#bash-the-basics",
    "title": "Automation & scripting in bash for beginners",
    "section": "Bash: the basics",
    "text": "Bash: the basics\n\nThe prompt\nIn command-line interfaces, a command prompt is a sequence of characters indicating that the interpreter is ready to accept input. It can also provide some information (e.g. time, error types, username and hostname, etc.)\nThe Bash prompt is customizable. By default, it often gives the username and the hostname, and it typically ends with $.\n\n\nHelp on commands\nMan pages:\nman &lt;command&gt;\n\nMan pages open in a pager (usually less).\nNavigate up/down with the space bar and the b key.\nQuit the pager with the q key.\n\nHelp pages:\n&lt;command&gt; --help\nInspect commands:\ncommand -V &lt;command&gt;\n\n\nExamples of commands\n\nPrint working directory: pwd\nChange directory: cd\nPrint: echo\nPrint content of a file: cat\nList: ls\nCopy: cp\nMove or rename: mv\nCreate a new directory: mkdir\nCreate a new file: touch\n\n\n\nKeybindings\nClear the terminal (command clear) with C-l (this means: press the Ctrl and L keys at the same time).\nNavigate command history with C-p and C-n (or up and down arrows).\nYou can auto-complete commands by pressing the tab key.",
    "crumbs": [
      "Bash",
      "<b><em>Workshops</em></b>",
      "Scripting for beginners"
    ]
  },
  {
    "objectID": "bash/ws_scripting.html#bash-scripting-the-basics",
    "href": "bash/ws_scripting.html#bash-scripting-the-basics",
    "title": "Automation & scripting in bash for beginners",
    "section": "Bash scripting: the basics",
    "text": "Bash scripting: the basics\nInstead of typing commands one at a time directly in a terminal, you can write them down, one per line, in a text file called a script.\nThey will be run in the order in which they are written when you execute the script.\nThis is a great way to automate tasks: to rerun this sequence of commands, you simply have to rerun the script.\n\nFile name\nShell scripts, including Bash scripts, are usually given the extension sh (e.g. my_script.sh).\nYou can store scripts anywhere, but a common practice is to store them in a ~/bin directory.\n\n\nSyntax\n\nShebang\nScripts can be written for any interpreter (e.g. Bash, Python, R, etc.) The way to tell the system which one to use is to use a shebang (#!) followed by the path of the interpreter on the first line of the script.\nTo use Bash, start your scripts with:\n#!/bin/bash\nYou may also encounter this notation:\n#!/usr/bin/env bash\nIf you are curious, you can read the answers to this Stack Overflow question for the differences between the two.\n\n\nComments\nAnything to the left of # is ignored by the interpreter and is for human consumption only.\n# You can write full-line comments\n\npwd       # You can also write comments after commands\n\n\n\nExecuting scripts\nThere are two ways to execute a script:\nbash my_script.sh\n./my_script.sh  # The dot represents the current directory\nIn the latter case, you need to make sure that your script is executable by first running:\nchmod u+x my_script.sh  # This makes the script executable by the user (i.e. you)\n\n\nOur first script\nOpen a text editor (e.g. nano) and type:\n#!/bin/bash\n\necho \"This is our first script.\"\nSave and close the file.\n\n\nYour turn:\n\nNow run the script with one, then the other method.\nWhat does this script do?",
    "crumbs": [
      "Bash",
      "<b><em>Workshops</em></b>",
      "Scripting for beginners"
    ]
  },
  {
    "objectID": "bash/ws_scripting.html#variables",
    "href": "bash/ws_scripting.html#variables",
    "title": "Automation & scripting in bash for beginners",
    "section": "Variables",
    "text": "Variables\n\nDeclaring variables\nYou can declare a variable (i.e. a name that holds a value) with the = sign.\n!! Make sure not to put spaces around the equal sign.\nvariable=Test\n\n\nQuotes\nLet’s experiment with quotes:\n\nvariable=This string is the value of the variable\necho $variable\n\nbash: line 1: string: command not found\n\n\nOops…\n\nvariable=\"This string is the value of the variable\"\necho $variable\n\nThis string is the value of the variable\n\n\n\nvariable='This string is the value of the variable'\necho $variable\n\nThis string is the value of the variable\n\n\n\nvariable='This string's the value of the variable'\necho $variable\n\nbash: -c: line 1: unexpected EOF while looking for matching `''\n\n\nOops…\nOne solution to this is to use double quotes:\n\nvariable=\"This string's the value of the variable\"\necho $variable\n\nThis string's the value of the variable\n\n\nAlternatively, single quotes can be escaped:\n\nvariable='This string'\"'\"'s the value of the variable'\necho $variable\n\nThis string's the value of the variable\n\n\n\nAdmittedly, this last one is a little crazy. It is the way to escape single quotes in single-quoted strings.\nThe first ' ends the first string, both \" create a double-quoted string with ' (escaped) in it, then the last ' starts the second string.\nEscaping double quotes is a lot easier and simply requires \\\".\n\n\n\nExpanding a variable’s value\nTo expand a variable (to access its value), you need to prepend its name with $:\n\nvariable=Test\necho variable\n\nvariable\n\n\nMmmm… not really want we want!\n\nvariable=Test\necho $variable\n\nTest\n\n\n\nvariable=Test; echo \"$variable\"\n\nTest\n\n\n!! Single quotes don’t expand variables.\n\nvariable=Test; echo '$variable'\n\n$variable\n\n\n\n\nPassing variables to a Bash script\nCreate a script called name.sh with the following content:\n#!/bin/bash\n\necho \"My name is $1.\"  # $1 refers to the first variable passed to the script\nYou can now pass a variable to this script with:\nbash name.sh Marie\nMy name is Marie.\nYou can pass several variables to a script. Copy name.sh to name2.sh and edit name2.sh to look like the following:\n#!/bin/bash\n\necho \"My name is $1 and I am $2 years old.\"\nbash name2.sh Marie 43\nMy name is Marie and I am 43 years old.\nYou can also pass any number of variables to a script:\n#!/bin/bash\n\necho $@\nbash script.sh argument1 argument2 argument3 argument4\nargument1 argument2 argument3 argument4\n\n\nBrace expansion\n\necho {1..5}\n\n1 2 3 4 5\n\n\n\necho {01..10}\n\n01 02 03 04 05 06 07 08 09 10\n\n\n\necho {1..5}.txt\n\n1.txt 2.txt 3.txt 4.txt 5.txt\n\n\n\necho {r..v}\n\nr s t u v\n\n\n\necho {file1,file2}.sh\n\nfile1.sh file2.sh\n\n\n!! Make sure not to add a space after the comma.\ntouch {file1,file2}.sh\ntouch file{3..6}.sh\n\necho {list,of,strings}\n\nlist of strings\n\n\n\n\nWildcards\nWildcards are really powerful to apply a command to all the elements having a common pattern.\nFor instance, we can delete all the files we created earlier (file1.sh, file2.sh, etc.) with a single command:\nrm file*.sh\n!! Be very careful that rm is irreversible. Deleted files do not go to the trash: they are gone.",
    "crumbs": [
      "Bash",
      "<b><em>Workshops</em></b>",
      "Scripting for beginners"
    ]
  },
  {
    "objectID": "bash/ws_scripting.html#loops",
    "href": "bash/ws_scripting.html#loops",
    "title": "Automation & scripting in bash for beginners",
    "section": "Loops",
    "text": "Loops\nTo apply a set of commands to all the elements of a list, you can use for loops. The general structure is as follows:\nfor &lt;iterable&gt; in &lt;list&gt;\ndo\n    &lt;statement1&gt;\n    &lt;statement2&gt;\n    ...\ndone\nLet’s create the script names.sh:\n#!/bin/bash\n\nfor name in $@\ndo\n    echo $name\ndone\nNow let’s run it with a list of arguments:\nbash names.sh Patrick Paul Marie Alex\nPatrick\nPaul\nMarie\nAlex\n\n\nYour turn:\n\nCompare the outputs of the following 2 scripts:\n\nscript1.sh:\n\n#!/bin/bash\n\necho $@\n\nscript2.sh:\n\n#!/bin/bash\n\nfor i in $@\ndo\n    echo $i\ndone\nHow do you explain the difference between running:\nbash script1.sh arg1 arg2 arg3\nand running:\nbash script2.sh arg1 arg2 arg3",
    "crumbs": [
      "Bash",
      "<b><em>Workshops</em></b>",
      "Scripting for beginners"
    ]
  },
  {
    "objectID": "bash/ws_scripting.html#lets-put-it-all-together-to-automate-some-task",
    "href": "bash/ws_scripting.html#lets-put-it-all-together-to-automate-some-task",
    "title": "Automation & scripting in bash for beginners",
    "section": "Let’s put it all together to automate some task",
    "text": "Let’s put it all together to automate some task\nThis is a rather silly example, but bear with me and let’s imagine that it actually makes sense (of course, you don’t write that many thesis chapters so you would probably never automate these tasks…)\nSo… let’s imagine that each time you write a thesis chapter, you do the same things:\n\nyou create a directory with the name of the chapter,\nyou create a number of subdirectories (for your source code, your manuscript, your data, and your results),\nyou create a Python script in the source code directory,\nyou create a markdown document in your manuscript directory,\nyou put the whole thing under version control with Git,\nyou create a .gitignore file in which you put the data subdirectory.\n\n\n\nYour turn:\n\nWrite a script that would do all this, then test the script.\nGive it a try on your own before looking at the solution below…\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nHere is what the script looks like (let’s call it chapter.sh):\n#!/bin/bash\n\nmkdir $1\ncd $1\nmkdir src data results ms\ntouch src/$1.py ms/$1.md\ngit init\necho data/ &gt; .gitignore\nYou then run the script:\nbash chapter.sh chapter1\nYou can verify that all the files and directories got created with:\ntree chapter1\nchapter1/\n├── data\n├── ms\n│   └── chapter1.md\n├── results\n└── src\n    └── chapter1.py\nand:\nls -aF chapter1\n./  ../  data/  .git/  .gitignore  ms/  results/  src/\nYou can also verify the content of your .gitignore file with:\ncat chapter1/.gitignore\ndata/",
    "crumbs": [
      "Bash",
      "<b><em>Workshops</em></b>",
      "Scripting for beginners"
    ]
  },
  {
    "objectID": "bash/ws_scripting.html#resources",
    "href": "bash/ws_scripting.html#resources",
    "title": "Automation & scripting in bash for beginners",
    "section": "Resources",
    "text": "Resources\nOne very useful (although very dense) resource is the Bash manual.\nYou can also get information on Bash from within Bash with:\ninfo bash\nand:\nman bash\nThere are also countless resources online and don’t forget to Google anything you don’t know how to do: you will almost certainly find the answer on StackOverflow or some Stack Exchange site.",
    "crumbs": [
      "Bash",
      "<b><em>Workshops</em></b>",
      "Scripting for beginners"
    ]
  },
  {
    "objectID": "bash/wb_tools1.html",
    "href": "bash/wb_tools1.html",
    "title": "Fun tools to simplify your life in the shell",
    "section": "",
    "text": "Working in the command-line has many advantages and it is often necessary, but can it be fun?\nIn this webinar, aimed at any command-line user, I intend to demonstrate that yes, it can! by introducing three free and open source utilities which make navigating your system and your outputs a lot easier:\n\nfzf is a simple, yet extremely powerful interactive fuzzy finder allowing for incremental completion and narrowing selection of any command line output. I will show you how to build simple shell functions which harvest its power to instantly refresh your memory on your custom keybindings or aliases, navigate your command history, find and kill processes, and explore and checkout your git commits. After this, you will be able to use fzf for any number of other applications in your work in the command-line.\nautojump lets you jump anywhere you want in your directories in just a few keystrokes (no more of this painful navigation writing down long paths).\nWith the ranger file manager, you can browse (with preview!), open, copy, move, delete, etc. your files and directories in a friendly way from the command line. Added bonus: you can use fzf and autojump within ranger!\n\nWarning: too much fun in the command-line can lead to addiction and geek behaviours. Use in moderation.",
    "crumbs": [
      "Bash",
      "<b><em>Webinars</em></b>",
      "Fun tools for the command line"
    ]
  },
  {
    "objectID": "bash/top_wb.html",
    "href": "bash/top_wb.html",
    "title": "Bash webinars",
    "section": "",
    "text": "Fun tools for the command line\n\n\n\n\nMore fun tools for the CLI",
    "crumbs": [
      "Bash",
      "<b><em>Webinars</em></b>"
    ]
  },
  {
    "objectID": "bash/molecules/intro_wildcards.html",
    "href": "bash/molecules/intro_wildcards.html",
    "title": "Wildcards",
    "section": "",
    "text": "Wildcards are a convenient way to select items matching patterns.\n\n\n\n\n\n\n\nData for this section\n\n\n\n\n\nFor this section, we will play with files created by The Carpentries.\nYou can download them into a zip file called bash.zip with:\nwget https://bit.ly/bashfile -O bash.zip\nYou can then unzip that file with:\nunzip bash.zip\nYou should now have a data-shell directory with a molecules subdirectory.\ncd into it:\ncd data-shell/molecules\n\n\n\nLet’s list the files in the molecules directory:\nls\ncubane.pdb  ethane.pdb  methane.pdb  octane.pdb  pentane.pdb  propane.pdb\nYou could do the same with:\nls *\nThe star expands to all files/directories matching any pattern. It is a wildcard.\nOf course, you can match more interesting patterns.\nFor instance, to list all files starting with the letter o, we can run:\nls o*\noctane.pdb\nTo list all files containing the letter o anywhere in their name, you can use:\nls *o*\noctane.pdb  propane.pdb\nThis saves a lot of typing and is a powerful way to apply a command to a subset of files/directories.\n\n\nYour turn:\n\nWildcards are often used to select all files with a certain extension.\nLet’s create 3 new files:\ntouch file1.txt file2.txt file3.md\nHow would you list all files with the .txt extension and only those?",
    "crumbs": [
      "Bash",
      "<b><em>Getting started</em></b>",
      "Wildcards"
    ]
  },
  {
    "objectID": "bash/molecules/intro_redirections.html",
    "href": "bash/molecules/intro_redirections.html",
    "title": "Redirections & pipes",
    "section": "",
    "text": "By default, commands that produce an output print it to the terminal. This output can however be redirected to be printed elsewhere (e.g. to a file) or to be passed as the argument of another command.",
    "crumbs": [
      "Bash",
      "<b><em>Getting started</em></b>",
      "Redirections & pipes"
    ]
  },
  {
    "objectID": "bash/molecules/intro_redirections.html#redirections",
    "href": "bash/molecules/intro_redirections.html#redirections",
    "title": "Redirections & pipes",
    "section": "Redirections",
    "text": "Redirections\nBy default, commands that produce an output print it to standard output—that is, the terminal. This is what we have been doing so far.\nThe output can however be redirected with the &gt; sign. For instance, it can be redirected to a file, which is very handy if you want to save the result.\n\nExample:\n\nLet’s print the number of lines in each .pdb file in the molecules directory:\n\nwc -l *.pdb\n\n  20 gas_cubane.pdb\n  12 gas_ethane.pdb\n   9 gas_methane.pdb\n  30 gas_octane.pdb\n  21 gas_pentane.pdb\n  15 gas_propane.pdb\n 107 total\n\n\n\n\nYour turn:\n\n\nWhat does the wc command do?\nWhat does the -l flag for this command do?\nHow did you find out?\n\n\nTo save this result into a file called lengths.txt, we run:\n\nwc -l *.pdb &gt; lengths.txt\n\n\nNote that &gt; always creates a new file. If a file called lengths.txt already exists, it will be overwritten. Be careful not to lose data this way!\nIf you don’t want to lose the content of the old file, you can append the output to the existing file with &gt;&gt; (&gt;&gt; will create a file lengths.txt if it doesn’t exist yet, but if it exists, it will append the new content below the old one).\n\n\n\nYour turn:\n\nHow can you make sure that you did create a file called lengths.txt?\n\nLet’s print its content to the terminal:\n\ncat lengths.txt\n\n  20 gas_cubane.pdb\n  12 gas_ethane.pdb\n   9 gas_methane.pdb\n  30 gas_octane.pdb\n  21 gas_pentane.pdb\n  15 gas_propane.pdb\n 107 total\n\n\nAs you can see, it contains the output of the command wc -l *.pdb.\nOf course, we can print the content of the file with modification. For instance, we can sort it:\n\nsort -n lengths.txt\n\n   9 gas_methane.pdb\n  12 gas_ethane.pdb\n  15 gas_propane.pdb\n  20 gas_cubane.pdb\n  21 gas_pentane.pdb\n  30 gas_octane.pdb\n 107 total\n\n\nAnd we can redirect this new output to a new file:\n\nsort -n lengths.txt &gt; sorted.txt\n\nInstead of printing an entire file to the terminal, you can print only part of it.\nLet’s print the first line of the new file sorted.txt:\n\nhead -1 sorted.txt\n\n   9 gas_methane.pdb",
    "crumbs": [
      "Bash",
      "<b><em>Getting started</em></b>",
      "Redirections & pipes"
    ]
  },
  {
    "objectID": "bash/molecules/intro_redirections.html#pipes",
    "href": "bash/molecules/intro_redirections.html#pipes",
    "title": "Redirections & pipes",
    "section": "Pipes",
    "text": "Pipes\nAnother form of redirection is the Bash pipe. Instead of redirecting the output to a different stream for printing, the output is passed as an argument to another command. This is very convenient because it allows to chain multiple commands without having to create files or variables to save intermediate results.\nFor instance, we could run the three commands we ran previously at once, without the creation of the two intermediate files:\n\nwc -l *.pdb | sort -n | head -1\n\n   9 gas_methane.pdb\n\n\nIn each case, the output of the command on the left-hand side (LHS) is passed as the input of the command on the right-hand side (RHS).\n\n\nYour turn:\n\nIn a directory we want to find the 3 files that have the least number of lines. Which command would work for this?\n\nwc -l * &gt; sort -n &gt; head -3\nwc -l * | sort -n | head 1-3\nwc -l * | sort -n | head -3\nwc -l * | head -3 | sort -n\n\n\nHere is a video of a previous version of this workshop.",
    "crumbs": [
      "Bash",
      "<b><em>Getting started</em></b>",
      "Redirections & pipes"
    ]
  },
  {
    "objectID": "bash/molecules/intro_control_flow.html#execution-conditional-on-success",
    "href": "bash/molecules/intro_control_flow.html#execution-conditional-on-success",
    "title": "Control flow",
    "section": "Execution conditional on success",
    "text": "Execution conditional on success\nCommands can be limited to running only if the previous commands ran successfully thanks to &&.\n\nExample:\n\nLook at the following commands:\nunzip bash.zip\nrm bash.zip\nThis is equivalent to:\nunzip bash.zip;\nrm bash.zip\nand to:\nunzip bash.zip; rm bash.zip\nThis is what we did to get the data for the past few sessions.\nIn both cases, both commands will try to run. Now, if for some reason, the unzipping fails, we have deleted the zip file and we have to re-download it. Not a big deal here, but in some situations, executing a command if the one before fails can be a real bummer.\nTo prevent this, we can use the double-ampersand (&&) operator:\nunzip bash.zip &&\nrm bash.zip\nThis is equivalent to:\nunzip bash.zip && rm bash.zip\nIf the unzipping works (if it returns a zero exit status), then the Zip file gets deleted. If however, the unzipping fails (if it returns a non-zero exit status), the script aborts and we haven’t lost our Zip file.",
    "crumbs": [
      "Bash",
      "<b><em>Getting started</em></b>",
      "Control flow"
    ]
  },
  {
    "objectID": "bash/molecules/intro_control_flow.html#repeated-execution",
    "href": "bash/molecules/intro_control_flow.html#repeated-execution",
    "title": "Control flow",
    "section": "Repeated execution",
    "text": "Repeated execution\nSections of scripts can be repeated for each element of a list thanks to for loops.\n\nSyntax\nThe general structure of a for loop is as follows:\nfor &lt;iterable&gt; in &lt;list&gt;\ndo\n    &lt;command1&gt;\n    &lt;command2&gt;\n    ...\ndone\n\n\nExample\nThe molecules directory contains the following .pdb files:\nls *.pdb\ncubane.pdb  ethane.pdb  methane.pdb  octane.pdb  pentane.pdb  propane.pdb\nWe want to rename these files by prepending “gas_” to their current names.\nWildcards don’t work here:\n\nmv *.pdb gas_*.pdb\n\nmv: target 'gas_propane.pdb': Not a directory\n\n\nThe solution is to use a for loop:\nfor file in *.pdb\ndo\n    mv $file gas_$file\ndone\nThis can also be written as a one-liner, although it is harder to read:\nfor file in *.pdb; do mv $file gas_$file; done\n\n\nYour turn:\n\nUsing what we learnt in the string manipulation section, how could you remove the gas_ prefix to all these files?\n\n\n\nCollections\nFor loops run a set of commands for each item of a collection. How do you create those collections?\n\nListing items one by one\nThe least efficient method is to list all the items one by one:\n\nExample:\n\nfor i in file1 file2 file3\ndo\n    echo $i\ndone\nfile1\nfile2\nfile3\n\n\nWildcards\nAs we have already seen, wildcards are very useful to build for loops.\n\n\nBrace expansion\nCollections can also be created with brace expansion.\n\nExamples:\n\n\necho {1,2,5}\n\n1 2 5\n\n\n\nMake sure not to add a space after the commas.\n\n\necho {list,of,strings}\n\nlist of strings\n\n\n\necho {file1,file2}.sh\n\nfile1.sh file2.sh\n\n\n\nls -l {ethane,methane,pentane}.pdb\n\nls: cannot access 'ethane.pdb': No such file or directory\nls: cannot access 'methane.pdb': No such file or directory\nls: cannot access 'pentane.pdb': No such file or directory\n\n\n\necho {1..5}\n\n1 2 3 4 5\n\n\n\necho {01..10}\n\n01 02 03 04 05 06 07 08 09 10\n\n\n\necho {r..v}\n\nr s t u v\n\n\n\necho {v..r}\n\nv u t s r\n\n\n\necho {a..e}{1..3}\n\na1 a2 a3 b1 b2 b3 c1 c2 c3 d1 d2 d3 e1 e2 e3\n\n\n\necho {a..c}{a..c}\n\naa ab ac ba bb bc ca cb cc\n\n\n\necho {1..5}.txt\n\n1.txt 2.txt 3.txt 4.txt 5.txt\n\n\n\necho file{3..6}.sh\n\nfile3.sh file4.sh file5.sh file6.sh\n\n\nBrace expansion can be used to create lists iterated over in loops, but also to apply commands to files or directories.\n\n\nSequences\nCollections can also be sequences:\n\nseq 1 2 10\n\n1\n3\n5\n7\n9\n\n\n\nHere, 1 is the start of the sequence, 10 is the end, and 2 is the step.\n\nSuch a sequence could be used in a loop this way:\n\nfor i in $(seq 1 2 10)\ndo\n    echo file$i.txt\ndone\n\nfile1.txt\nfile3.txt\nfile5.txt\nfile7.txt\nfile9.txt\n\n\n\n\n\nYour turn:\n\nIn a directory the command ls returns:\nfructose.dat  glucose.dat  sucrose.dat  maltose.txt\nWhat would be the output of the following loop?\nfor datafile in *.dat\ndo\n  cat $datafile &gt;&gt; sugar.dat\ndone\n\nAll of the text from fructose.dat, glucose.dat and sucrose.dat would be concatenated and saved to a file called sugar.dat.\nThe text from sucrose.dat will be saved to a file called sugar.dat.\nAll of the text from fructose.dat, glucose.dat, sucrose.dat, and maltose.txt would be concatenated and saved to a file called sugar.dat.\nAll of the text from fructose.dat, glucose.dat and sucrose.dat will be printed to the screen and saved into a file called sugar.dat.",
    "crumbs": [
      "Bash",
      "<b><em>Getting started</em></b>",
      "Control flow"
    ]
  },
  {
    "objectID": "bash/molecules/intro_control_flow.html#conditionally-repeated-execution",
    "href": "bash/molecules/intro_control_flow.html#conditionally-repeated-execution",
    "title": "Control flow",
    "section": "Conditionally repeated execution",
    "text": "Conditionally repeated execution\nSections of scripts can be repeated as long as a condition returns True thanks to while loops.\n\nSyntax\nThe syntax of a while loop in Bash is:\nwhile predicate\ndo\n    command1\n    command2\n    ...\ndone\nThe set of commands in the body of the while loop are executed as long as the predicate returns true.\nBe careful that while loop can lead to infinite loops. Such loops need to be manually interrupted (by pressing &lt;Ctrl+C&gt;).\n\nExample of infinite loop:\n\nwhile true\ndo\n    echo \"Press &lt;Ctrl+C&gt; to stop\"\n    sleep 1\ndone\nHere is a video of a previous version of this workshop.",
    "crumbs": [
      "Bash",
      "<b><em>Getting started</em></b>",
      "Control flow"
    ]
  },
  {
    "objectID": "bash/molecules/intro_control_flow.html#conditional-execution",
    "href": "bash/molecules/intro_control_flow.html#conditional-execution",
    "title": "Control flow",
    "section": "Conditional execution",
    "text": "Conditional execution\nSections of scripts can be executed (or not) based on conditions thanks to if statements.\n\nSyntax\nif [ predicate1 ]\nthen\n    command1\n    command2\n    ...\nelif [ predicate2 ]\nthen\n    command3\n    command4\n    ...\nelse\n    command5\n    command6\n    ...\nfi\n\n\nExample\nLet’s create a file called check.sh with the following if statement:\nfor f in $@\ndo\n    if [ -e $f ]      # Make sure to have spaces around each bracket\n    then\n        echo $f exists\n    else\n        echo $f does not exist\n    fi\ndone\nNow, let’s make it executable:\nchmod u+x check.sh\nAnd let’s run this:\n./check.sh file1 file2 check.sh file3\n\n\nPredicates\nHere are a few predicates:\n[ $var == 'text' ] checks whether var is equal to 'text'.\n[ $var == number ] checks whether var is equal to number.\n[ -e file ] checks whether file exists.\n[ -d name ] checks whether name is a directory.\n[ -f name ] checks whether name is a file.",
    "crumbs": [
      "Bash",
      "<b><em>Getting started</em></b>",
      "Control flow"
    ]
  },
  {
    "objectID": "bash/intro_variables.html#declaring-variables",
    "href": "bash/intro_variables.html#declaring-variables",
    "title": "Variables",
    "section": "Declaring variables",
    "text": "Declaring variables\nYou declare a variable (i.e. a name that holds a value) with the = sign:\nvar=value\n\nMake sure not to put spaces around the equal sign.\n\n\nExample:\n\n\nvar=5\n\nYou can delete a variable with the unset command:\n\nunset var",
    "crumbs": [
      "Bash",
      "<b><em>Getting started</em></b>",
      "Variables"
    ]
  },
  {
    "objectID": "bash/intro_variables.html#expanding-variables",
    "href": "bash/intro_variables.html#expanding-variables",
    "title": "Variables",
    "section": "Expanding variables",
    "text": "Expanding variables\nTo expand a variable (to access its value), you need to prepend its name with $.\n\nThis is not what we want:\n\n\nvar=value\necho var\n\nvar\n\n\n\nThis however works:\n\n\nvar=value\necho $var\n\nvalue",
    "crumbs": [
      "Bash",
      "<b><em>Getting started</em></b>",
      "Variables"
    ]
  },
  {
    "objectID": "bash/intro_variables.html#quotes",
    "href": "bash/intro_variables.html#quotes",
    "title": "Variables",
    "section": "Quotes",
    "text": "Quotes\n\nWhen declaring\nQuotes are necessary for values containing special characters such as spaces.\n\nThis doesn’t work:\n\n\nvar=string with spaces\necho $var\n\nbash: line 1: with: command not found\n\n\n\nThis works:\n\n\nvar=\"string with spaces\"\necho $var\n\nstring with spaces\n\n\n\nThis also works:\n\n\nvar='string with spaces'\necho $var\n\nstring with spaces\n\n\nWhen declaring variables, single and double quotes are equivalent. Which one should you use then? Use the one that is most convenient.\n\nThis is not good:\n\n\nvar='that's a string with spaces'\necho $var\n\nbash: -c: line 1: unexpected EOF while looking for matching `''\n\n\n\nThis works well:\n\n\nvar=\"that's a string with spaces\"\necho $var\n\nthat's a string with spaces\n\n\n\nAlternatively, single quotes can be escaped, but it is a little crazy: the first ' ends the first string, then the apostrophe needs to be escaped (\\'), finally, the third ' starts the second string.\n\nvar='that'\\''s a string with spaces'\necho $var\n\nthat's a string with spaces\n\n\n\n\nConversely, this is not good:\n\n\nvar=\"he said: \"string with spaces\"\"\necho $var\n\nbash: line 1: with: command not found\n\n\n\nWhile this works:\n\n\nvar='he said: \"string with spaces\"'\necho $var\n\nhe said: \"string with spaces\"\n\n\n\nDouble quotes as well can be escaped (simply by prepending them with \\):\n\nvar=\"he said: \\\"string with spaces\\\"\"\necho $var\n\nhe said: \"string with spaces\"\n\n\n\n\n\nWhen expanding\nWhile not necessary in many situations, it is safer to expand variables in double quotes, in case the expansion leads to problematic special characters. In the example above, this was not problematic and using $var or \"$var\" both work.\nIn the following example however, it is problematic:\nvar=\"string with spaces\"\ntouch $var\nThis creates 3 files called string, with, and spaces. Probably not what you wanted…\nThe following creates a single file called string with spaces:\nvar=\"string with spaces\"\ntouch \"$var\"\n\nTo be safe, it is thus a good habit to quote expanded variables.\n\nIt is important to note however that single quotes don’t expand variables (only double quotes do).\nThe following would thus create a file called $var:\nvar=\"string with spaces\"\ntouch '$var'",
    "crumbs": [
      "Bash",
      "<b><em>Getting started</em></b>",
      "Variables"
    ]
  },
  {
    "objectID": "bash/intro_variables.html#exporting-variables",
    "href": "bash/intro_variables.html#exporting-variables",
    "title": "Variables",
    "section": "Exporting variables",
    "text": "Exporting variables\nUsing export ensures that all inherited processes of the current shell also have access to this variable:\n\nExample:\n\nvar=3\nzsh           # Launch Zsh (another shell)\necho $var\nThis returns nothing: var is not defined in the Zsh process.\nexport var=3\nzsh\necho $var\nThis returns 3: var got exported into the Zsh process.",
    "crumbs": [
      "Bash",
      "<b><em>Getting started</em></b>",
      "Variables"
    ]
  },
  {
    "objectID": "bash/intro_variables.html#string-manipulation",
    "href": "bash/intro_variables.html#string-manipulation",
    "title": "Variables",
    "section": "String manipulation",
    "text": "String manipulation\n\nGetting a subset\n\nvar=\"hello\"\necho ${var:2}      # Print from character 2\necho ${var:2:1}    # Print 1 character from character 2\n\nllo\nl\n\n\n\nBash indexes from 0.\n\n\n\nSearch and replace\n\nvar=\"hello\"\necho ${var/l/L}    # Replace the first match of l by L\necho ${var//l/L}   # Replace all matches of l by L\n\nheLlo\nheLLo\n\n\n\n\nString concatenation\nIf you want to concatenate the expanded variable with another string, you need to use curly braces or quotes.\n\nThis does not return anything because there is no variable called varshine:\n\n\nvar=sun\necho $varshine\n\n\nThese two syntaxes do work:\n\n\nvar=sun\necho ${var}shine\necho \"$var\"shine\n\nsunshine\nsunshine",
    "crumbs": [
      "Bash",
      "<b><em>Getting started</em></b>",
      "Variables"
    ]
  },
  {
    "objectID": "bash/intro_variables.html#environment-variables",
    "href": "bash/intro_variables.html#environment-variables",
    "title": "Variables",
    "section": "Environment variables",
    "text": "Environment variables\nEnvironment variables help control the behaviour of processes on a machine. You can think of them as customizations of your system.\nMany are set automatically.\n\nExample:\n\necho $HOME\n/home/user09\nThere are many other environment variables (e.g. PATH, PWD, PS1). To see the list, you can run printenv or env.\nIf you want to add new environment variables, you can add them to your ~/.bashrc file which is sourced each time you start a new shell.\nHere is a video of a previous version of this workshop.",
    "crumbs": [
      "Bash",
      "<b><em>Getting started</em></b>",
      "Variables"
    ]
  },
  {
    "objectID": "bash/intro_special_parameters.html",
    "href": "bash/intro_special_parameters.html",
    "title": "Special parameters",
    "section": "",
    "text": "A number of special parameters, all starting with $, get expanded by Bash.\n\n\n$1, $2, $3, … are positional special characters\n$@ is an array-like construct referring of all positional parameters\n$# expands to the number of arguments\n$$ pid of the current shell\n$! expands to the PID of the most recent background command\n$0 expands to the name of the current shell or script",
    "crumbs": [
      "Bash",
      "<b><em>Getting started</em></b>",
      "Special parameters"
    ]
  },
  {
    "objectID": "bash/intro_resources.html",
    "href": "bash/intro_resources.html",
    "title": "Resources",
    "section": "",
    "text": "This section lists a few useful Bash resources.\n\nOne very useful (although very dense) resource is the Bash manual. This is the absolute reference.\nThere are countless sites with Bash courses and guides. Here are a few:\n\nBash Guide for Beginners\nLearn X in Y minutes\nLinux Bash Shell Scripting Tutorial\n\nYou can also get information on Bash from within Bash with:\ninfo bash\nand:\nman bash\nThere are also countless resources online and don’t forget to Google anything you don’t know how to do: you will almost certainly find the answer on StackOverflow or some Stack Exchange site.",
    "crumbs": [
      "Bash",
      "<b><em>Getting started</em></b>",
      "Resources"
    ]
  },
  {
    "objectID": "bash/intro_quotes.html",
    "href": "bash/intro_quotes.html",
    "title": "Quotes",
    "section": "",
    "text": "Let’s experiment with quotes:\n\nvariable=This string is the value of the variable\necho $variable\n\nbash: line 1: string: command not found\n\n\nOops…\n\nvariable=\"This string is the value of the variable\"\necho $variable\n\nThis string is the value of the variable\n\n\n\nvariable='This string is the value of the variable'\necho $variable\n\nThis string is the value of the variable\n\n\n\nvariable='This string's the value of the variable'\necho $variable\n\nbash: -c: line 1: unexpected EOF while looking for matching `''\n\n\nOops…\nOne solution to this is to use double quotes:\n\nvariable=\"This string's the value of the variable\"\necho $variable\n\nThis string's the value of the variable\n\n\nAlternatively, single quotes can be escaped:\n\nvariable='This string'\"'\"'s the value of the variable'\necho $variable\n\nThis string's the value of the variable\n\n\n\nAdmittedly, this last one is a little crazy. It is the way to escape single quotes in single-quoted strings.\nThe first ' ends the first string, both \" create a double-quoted string with ' (escaped) in it, then the last ' starts the second string.\nEscaping double quotes is a lot easier and simply requires \\\"."
  },
  {
    "objectID": "bash/intro_intro.html",
    "href": "bash/intro_intro.html",
    "title": "Introduction",
    "section": "",
    "text": "This section explains what Bash is, why it is useful to know how to use it, and how we are going to run it in this course.",
    "crumbs": [
      "Bash",
      "<b><em>Getting started</em></b>",
      "Introduction"
    ]
  },
  {
    "objectID": "bash/intro_intro.html#unix-shells",
    "href": "bash/intro_intro.html#unix-shells",
    "title": "Introduction",
    "section": "Unix shells",
    "text": "Unix shells\nUnix shells are command line interpreters for Unix-like operating systems1: the user enters commands as text—interactively in a terminal or in scripts—and the shell passes them to the operating system.\n1 Unix-like systems include Linux, macOS, and a few others.It is thus a way to give instructions to the machine through text instead of using a graphical user interface (GUI).\n\nTypes of Unix shells\nBash (Bourne Again SHell)—released in 1989—is part of the GNU Project and is the default Unix shell on many systems (although macOS recently changed its default to Zsh).\nPrior to Bash, the default was the Bourne shell (sh).\nA new and popular shell (backward compatible with Bash) is Zsh (zsh). It extends Bash’s capabilities.\nAnother shell in the same family is the KornShell (ksh).\nBash is the most common shell and the one which makes the most sense to learn as a first Unix shell. It is also the one used by default on the Alliance clusters.\n\n\nWhy use a shell?\nWhile automating GUI operations is really difficult, it is easy to rerun a script (a file with a number of commands). Unix shells thus allow the creation of reproducible workflows and the automation of repetitive tasks.\n\nImagine you had 1000 files in a directory and you wanted to rename them all.\nUsing Windows Explorer or MacOS Finder, you could right click on every file one by one to rename it, but it would take you hours. Using Bash, this is done by a very simple command and takes an instant.\n\nShells are powerful to launch tools, modify files, search text, or combine commands.\n\nBecause shells are powerful, you can easily make consequential mistakes (e.g. deleting a lot of files). For this reason, it is a good idea to make backups of your data (it is a very good idea to make frequent backups of your data anyway!).\n\nShells also allow to work on remote machines and HPC clusters.",
    "crumbs": [
      "Bash",
      "<b><em>Getting started</em></b>",
      "Introduction"
    ]
  },
  {
    "objectID": "bash/intro_intro.html#running-bash",
    "href": "bash/intro_intro.html#running-bash",
    "title": "Introduction",
    "section": "Running Bash",
    "text": "Running Bash\nSince Bash is a Unix shell, you need a Unix or Unix-like operating system. This means that people on Linux or MacOS can use Bash directly on their machine.\nFor Windows users, there are various options:\n\nusing Windows Subsystem for Linux (WSL),\nusing a Bash emulator (e.g. Git BASH), but those only have a subset of the usual Bash utilities,\nusing a Unix-like environment for Windows (e.g. Cygwin),\nusing a Unix Virtual machine,\naccessing a remote Unix machine.\n\n\nHow we will use Bash today\nToday, we will connect to a remote HPC cluster (supercomputer) via SSH (secure shell). HPC systems always run Linux.\nThose on Linux or MacOS can alternatively use Bash directly on their machine.\n\nOn MacOS, the default is now Zsh (you can see that by launching the application called “Terminal” and typing echo $SHELL followed by the &lt;enter&gt; key), but Zsh is fully compatible with Bash commands, so it is totally fine to use it instead. If you really want to use Bash, simply launch it by typing in “Terminal”: bash, then pressing the &lt;enter&gt; key.",
    "crumbs": [
      "Bash",
      "<b><em>Getting started</em></b>",
      "Introduction"
    ]
  },
  {
    "objectID": "bash/intro_intro.html#connecting-to-a-remote-hpc-system-via-ssh",
    "href": "bash/intro_intro.html#connecting-to-a-remote-hpc-system-via-ssh",
    "title": "Introduction",
    "section": "Connecting to a remote HPC system via SSH",
    "text": "Connecting to a remote HPC system via SSH\n\nStep one: usernames and password\nWe will give you a link to an etherpad during the workshop. Add your name next to a free username to claim it.\nWe will also give you the password for our training cluster. When prompted, enter it.\n\nNote that you will not see any character as you type the password: this is called blind typing and is a Linux safety feature. Type slowly and make sure not to make typos. It can be unsettling at first not to get any feed-back while typing.\n\n\n\nStep two: logging in\n\n •  Linux and MacOS users\nLinux users:   open the terminal emulator of your choice.\nMacOS users:   open “Terminal”.\nThen type:\nssh userxx@hostname\n\n\nReplace userxx by your username (e.g. user09)\nReplace hostname by the hostname we will give you the day of the workshop.\n\n\n\n\n •  Windows users\nWe suggest using the free version of MobaXterm. MobaXterm comes with a terminal emulator and a GUI interface for SSH sessions.\nOpen MobaXterm, click on “Session”, then “SSH”, and fill in the Remote host name and your username.\n\nHere is a live demo.",
    "crumbs": [
      "Bash",
      "<b><em>Getting started</em></b>",
      "Introduction"
    ]
  },
  {
    "objectID": "bash/intro_find.html#data-for-this-section",
    "href": "bash/intro_find.html#data-for-this-section",
    "title": "Finding files",
    "section": "Data for this section",
    "text": "Data for this section\nFor this section, we will play with files created by The Carpentries.\nYou can download them into a zip file called bash.zip with:\nwget http://bit.ly/bashfile -O bash.zip\nYou can then unzip that file with:\nunzip bash.zip\nFinally, you can delete the zip file:\nrm bash.zip\nYou should now have a data-shell directory with a molecules subdirectory.\ncd into it:\ncd data-shell/molecules"
  },
  {
    "objectID": "bash/intro_find.html#command-find",
    "href": "bash/intro_find.html#command-find",
    "title": "Finding files",
    "section": "Command find",
    "text": "Command find\nSearch for files inside the current working directory:\nfind . -type f\n./methane.pdb\n./pentane.pdb\n./sorted.txt\n./propane.pdb\n./lengths.txt\n./cubane.pdb\n./ethane.pdb\n./octane.pdb\nfind . -type d will instead search for directories inside the current working directory.\nHere are other examples:\nfind . -maxdepth 1 -type f     # depth 1 is the current directory\nfind . -mindepth 2 -type f     # current directory and one level down\nfind . -name haiku.txt      # finds specific file\nls data       # shows one.txt two.txt\nfind . -name *.txt      # still finds one file -- why? answer: expands *.txt to haiku.txt\nfind . -name '*.txt'    # finds all three files -- good!\nLet’s wrap the last command into $()—called command substitution—as if it were a variable:\necho $(find . -name '*.txt')   # will print ./data/one.txt ./data/two.txt ./haiku.txt\nls -l $(find . -name '*.txt')   # will expand to ls -l ./data/one.txt ./data/two.txt ./haiku.txt\nwc -l $(find . -name '*.txt')   # will expand to wc -l ./data/one.txt ./data/two.txt ./haiku.txt\ngrep elegant $(find . -name '*.txt')   # will look for 'elegant' inside all *.txt files\n\n\nYour turn:\n\ngrep’s -v flag inverts pattern matching, so that only lines that do not match the pattern are printed.\nGiven that, which of the following commands will find all files in /data whose names end in ose.dat (e.g. sucrose.dat or maltose.dat), but do not contain the word temp?\n\nfind /data -name '*.dat' | grep ose | grep -v temp\nfind /data -name ose.dat | grep -v temp\ngrep -v temp $(find /data -name '*ose.dat')\nNone of the above\n\n\nHere is a video of a previous version of this workshop."
  },
  {
    "objectID": "bash/intro_find.html#running-a-command-on-the-results-of-find",
    "href": "bash/intro_find.html#running-a-command-on-the-results-of-find",
    "title": "Finding files",
    "section": "Running a command on the results of find",
    "text": "Running a command on the results of find\nLet’s say that you want to run a command on each of the files in the output of find. You can always do something using command substitution like this:\nfor f in $(find . -name \"*.txt\")\ndo\n    command on $f\ndone\nAlternatively, you can make it a one-liner:\nfind . -name \"*.txt\" -exec command {} \\;\nAnother—perhaps more elegant—one-line alternative is to use xargs. In its simplest usage, xargs command lets you construct a list of arguments:\nfind . -name \"*.txt\"                   # returns multiple lines\nfind . -name \"*.txt\" | xargs           # use those lines to construct a list\nfind . -name \"*.txt\" | xargs command   # pass this list as arguments to `command`\ncommand $(find . -name \"*.txt\")        # command substitution, achieving the same result (this is riskier!)\ncommand `(find . -name \"*.txt\")`       # alternative syntax for command substitution\nIn these examples, xargs achieves the same result as command substitution, but it is safer in terms of memory usage and the length of lists you can pass.\nWhen would you need to use this? A good example is with the command grep. grep takes a search stream (and not a list of files) as its standard input:\ncat filename | grep pattern\nTo pass a list of files to grep, you can use xargs that takes that list from its standard input and converts it into a list of arguments that is then passed to grep:\nfind . -name \"*.txt\" | xargs grep pattern   # search for `pattern` inside all those files (`grep` does not take a list of files as standard input)\n\n\nHere is a video of a previous version of this workshop."
  },
  {
    "objectID": "bash/intro_control_flow.html#execution-conditional-on-success",
    "href": "bash/intro_control_flow.html#execution-conditional-on-success",
    "title": "Control flow",
    "section": "Execution conditional on success",
    "text": "Execution conditional on success\nCommands can be limited to running only if the previous commands ran successfully thanks to &&.\n\nExample:\n\nLook at the following commands:\nunzip bash.zip\nrm bash.zip\nThis is equivalent to:\nunzip bash.zip;\nrm bash.zip\nand to:\nunzip bash.zip; rm bash.zip\nThis is what we did to get the data for the past few sessions.\nIn both cases, both commands will try to run. Now, if for some reason, the unzipping fails, we have deleted the zip file and we have to re-download it. Not a big deal here, but in some situations, executing a command if the one before fails can be a real bummer.\nTo prevent this, we can use the double-ampersand (&&) operator:\nunzip bash.zip &&\nrm bash.zip\nThis is equivalent to:\nunzip bash.zip && rm bash.zip\nIf the unzipping works (if it returns a zero exit status), then the Zip file gets deleted. If however, the unzipping fails (if it returns a non-zero exit status), the script aborts and we haven’t lost our Zip file."
  },
  {
    "objectID": "bash/intro_control_flow.html#repeated-execution",
    "href": "bash/intro_control_flow.html#repeated-execution",
    "title": "Control flow",
    "section": "Repeated execution",
    "text": "Repeated execution\nSections of scripts can be repeated for each element of a list thanks to for loops.\n\nSyntax\nThe general structure of a for loop is as follows:\nfor &lt;iterable&gt; in &lt;list&gt;\ndo\n    &lt;command1&gt;\n    &lt;command2&gt;\n    ...\ndone\n\n\nExample\nThe molecules directory contains the following .pdb files:\nls *.pdb\ncubane.pdb  ethane.pdb  methane.pdb  octane.pdb  pentane.pdb  propane.pdb\nWe want to rename these files by prepending “gas_” to their current names.\nWildcards don’t work here:\n\nmv *.pdb gas_*.pdb\n\nmv: cannot stat '*.pdb': No such file or directory\n\n\nThe solution is to use a for loop:\nfor file in *.pdb\ndo\n    mv $file gas_$file\ndone\nThis can also be written as a one-liner, although it is harder to read:\nfor file in *.pdb; do mv $file gas_$file; done\n\n\nYour turn:\n\nUsing what we learnt in the string manipulation section, how could you remove the gas_ prefix to all these files?\n\n\n\nCollections\nFor loops run a set of commands for each item of a collection. How do you create those collections?\n\nListing items one by one\nThe least efficient method is to list all the items one by one:\n\nExample:\n\nfor i in file1 file2 file3\ndo\n    echo $i\ndone\nfile1\nfile2\nfile3\n\n\nWildcards\nAs we have already seen, wildcards are very useful to build for loops.\n\n\nBrace expansion\nCollections can also be created with brace expansion.\n\nExamples:\n\n\necho {1,2,5}\n\n1 2 5\n\n\n\nMake sure not to add a space after the commas.\n\n\necho {list,of,strings}\n\nlist of strings\n\n\n\necho {file1,file2}.sh\n\nfile1.sh file2.sh\n\n\n\nls -l {ethane,methane,pentane}.pdb\n\nls: cannot access 'ethane.pdb': No such file or directory\nls: cannot access 'methane.pdb': No such file or directory\nls: cannot access 'pentane.pdb': No such file or directory\n\n\n\necho {1..5}\n\n1 2 3 4 5\n\n\n\necho {01..10}\n\n01 02 03 04 05 06 07 08 09 10\n\n\n\necho {r..v}\n\nr s t u v\n\n\n\necho {v..r}\n\nv u t s r\n\n\n\necho {a..e}{1..3}\n\na1 a2 a3 b1 b2 b3 c1 c2 c3 d1 d2 d3 e1 e2 e3\n\n\n\necho {a..c}{a..c}\n\naa ab ac ba bb bc ca cb cc\n\n\n\necho {1..5}.txt\n\n1.txt 2.txt 3.txt 4.txt 5.txt\n\n\n\necho file{3..6}.sh\n\nfile3.sh file4.sh file5.sh file6.sh\n\n\nBrace expansion can be used to create lists iterated over in loops, but also to apply commands to files or directories.\n\n\nSequences\nCollections can also be sequences:\n\nseq 1 2 10\n\n1\n3\n5\n7\n9\n\n\n\nHere, 1 is the start of the sequence, 10 is the end, and 2 is the step.\n\nSuch a sequence could be used in a loop this way:\n\nfor i in $(seq 1 2 10)\ndo\n    echo file$i.txt\ndone\n\nfile1.txt\nfile3.txt\nfile5.txt\nfile7.txt\nfile9.txt\n\n\n\n\n\nYour turn:\n\nIn a directory the command ls returns:\nfructose.dat  glucose.dat  sucrose.dat  maltose.txt\nWhat would be the output of the following loop?\nfor datafile in *.dat\ndo\n  cat $datafile &gt;&gt; sugar.dat\ndone\n\nAll of the text from fructose.dat, glucose.dat and sucrose.dat would be concatenated and saved to a file called sugar.dat.\nThe text from sucrose.dat will be saved to a file called sugar.dat.\nAll of the text from fructose.dat, glucose.dat, sucrose.dat, and maltose.txt would be concatenated and saved to a file called sugar.dat.\nAll of the text from fructose.dat, glucose.dat and sucrose.dat will be printed to the screen and saved into a file called sugar.dat."
  },
  {
    "objectID": "bash/intro_control_flow.html#conditionally-repeated-execution",
    "href": "bash/intro_control_flow.html#conditionally-repeated-execution",
    "title": "Control flow",
    "section": "Conditionally repeated execution",
    "text": "Conditionally repeated execution\nSections of scripts can be repeated as long as a condition returns True thanks to while loops.\n\nSyntax\nThe syntax of a while loop in Bash is:\nwhile predicate\ndo\n    command1\n    command2\n    ...\ndone\nThe set of commands in the body of the while loop are executed as long as the predicate returns true.\nBe careful that while loop can lead to infinite loops. Such loops need to be manually interrupted (by pressing &lt;Ctrl+C&gt;).\n\nExample of infinite loop:\n\nwhile true\ndo\n    echo \"Press &lt;Ctrl+C&gt; to stop\"\n    sleep 1\ndone\nHere is a video of a previous version of this workshop."
  },
  {
    "objectID": "bash/intro_control_flow.html#conditional-execution",
    "href": "bash/intro_control_flow.html#conditional-execution",
    "title": "Control flow",
    "section": "Conditional execution",
    "text": "Conditional execution\nSections of scripts can be executed (or not) based on conditions thanks to if statements.\n\nSyntax\nif [ predicate1 ]\nthen\n    command1\n    command2\n    ...\nelif [ predicate2 ]\nthen\n    command3\n    command4\n    ...\nelse\n    command5\n    command6\n    ...\nfi\n\n\nExample\nLet’s create a file called check.sh with the following if statement:\nfor f in $@\ndo\n    if [ -e $f ]      # Make sure to have spaces around each bracket\n    then\n        echo $f exists\n    else\n        echo $f does not exist\n    fi\ndone\nNow, let’s make it executable:\nchmod u+x check.sh\nAnd let’s run this:\n./check.sh file1 file2 check.sh file3\n\n\nPredicates\nHere are a few predicates:\n[ $var == 'text' ] checks whether var is equal to 'text'.\n[ $var == number ] checks whether var is equal to number.\n[ -e file ] checks whether file exists.\n[ -d name ] checks whether name is a directory.\n[ -f name ] checks whether name is a file."
  },
  {
    "objectID": "bash/index.html",
    "href": "bash/index.html",
    "title": "Bash",
    "section": "",
    "text": "Getting started with  \nAn intro course to the Unix shell\n\n\n\n\nWorkshops\nVarious Bash topics\n\n\n\n\n\n\n60 min webinars\nVarious Bash topics",
    "crumbs": [
      "Bash",
      "<br>&nbsp;<img src=\"img/logo_bash.png\" class=\"img-fluid\" style=\"width:1.4em\" alt=\"noshadow\"><br><br>"
    ]
  },
  {
    "objectID": "ai/ws_hss_intro.html",
    "href": "ai/ws_hss_intro.html",
    "title": "Introduction to machine learning for the humanities",
    "section": "",
    "text": "We hear about it all the time, but what really is machine learning? And what about deep learning? Neural networks?? How can any of this help me with my work? And how? Which tools do I need to make use of the transformative advances happening in that field??\nThis workshop will answer these questions in a non-technical manner to give you a high level overview of a discipline that has become crucial in all fields of research.\n\nSlides (Click and wait: the presentation might take a few instants to load)",
    "crumbs": [
      "AI",
      "<b><em>Workshops</em></b>",
      "Intro ML for the humanities"
    ]
  },
  {
    "objectID": "ai/ws_dl_nlp_llm.html",
    "href": "ai/ws_dl_nlp_llm.html",
    "title": "A quick introduction to deep learning, NLP, and LLMs",
    "section": "",
    "text": "◦ How does deep learning really work?\n◦ What exactly are those large language models everybody talks about?\n◦ How can I build a neural network?\n◦ What is NLP?\nThis presentation will answer these questions in a non-technical manner to give you a high-level understanding of a discipline that has become crucial in all fields of research.\n\nSlides (Click and wait: the presentation might take a few instants to load)",
    "crumbs": [
      "AI",
      "<b><em>Workshops</em></b>",
      "Quick intro to DL, NLP, and LLMs"
    ]
  },
  {
    "objectID": "ai/wb_frameworks.html",
    "href": "ai/wb_frameworks.html",
    "title": "A map of current machine learning frameworks",
    "section": "",
    "text": "We are in a period of active development of new deep learning techniques, adding to the already mature area of traditional machine learning. This is leading to a vast and ever evolving field of implementations which can be disorienting.\nIn this webinar, I will guide you through a map of the current frameworks, organizing them based on their domain (machine learning vs deep learning) and the languages required to use them. I will also talk about the various automatic differentiation options available.\nTo narrow such a large topic, I am limiting the map to frameworks that can be used from Python, Julia, and R.\n\nSlides (Click and wait: this reveal.js presentation is heavy and takes some time to load.)",
    "crumbs": [
      "AI",
      "<b><em>Webinars</em></b>",
      "Map of current frameworks"
    ]
  },
  {
    "objectID": "ai/wb_dvc_slides.html#on-version-control",
    "href": "ai/wb_dvc_slides.html#on-version-control",
    "title": "Version control for data science & machine learning with DVC",
    "section": "On version control",
    "text": "On version control\nI won’t introduce here the benefits of using a good version control system such as Git\n\n\n\nOn the benefits of VCS"
  },
  {
    "objectID": "ai/wb_dvc_slides.html#extending-git-for-data",
    "href": "ai/wb_dvc_slides.html#extending-git-for-data",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Extending Git for data",
    "text": "Extending Git for data\nWhile Git is a wonderful tool for text files versioning (code, writings in markup formats), it isn’t a tool to manage changes to datasets\nSeveral open source tools—each with a different structure and functioning—extend Git capabilities to track data: Git LFS, git-annex, lakeFS, Dolt, DataLad"
  },
  {
    "objectID": "ai/wb_dvc_slides.html#extending-git-for-models-and-experiments",
    "href": "ai/wb_dvc_slides.html#extending-git-for-models-and-experiments",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Extending Git for models and experiments",
    "text": "Extending Git for models and experiments\nReproducible research and collaboration on data science and machine learning projects involve more than datasets management:\nExperiments and the models they produce also need to be tracked"
  },
  {
    "objectID": "ai/wb_dvc_slides.html#many-moving-parts",
    "href": "ai/wb_dvc_slides.html#many-moving-parts",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Many moving parts",
    "text": "Many moving parts\n\n*hp = hyperparameter\n\n\n\n\n\n\n\n\n\n\ndata1\n\ndata1\n\n\n\nmodel1\n\nmodel1\n\n\n\ndata1-&gt;model1\n\n\n\n\n\nmodel2\n\nmodel2\n\n\n\ndata1-&gt;model2\n\n\n\n\n\nmodel3\n\nmodel3\n\n\n\ndata1-&gt;model3\n\n\n\n\n\ndata2\n\ndata2\n\n\n\ndata2-&gt;model1\n\n\n\n\n\ndata2-&gt;model2\n\n\n\n\n\ndata2-&gt;model3\n\n\n\n\n\ndata3\n\ndata3\n\n\n\ndata3-&gt;model1\n\n\n\n\n\ndata3-&gt;model2\n\n\n\n\n\ndata3-&gt;model3\n\n\n\n\n\nhp1\n\nhp1\n\n\n\nhp1-&gt;model1\n\n\n\n\n\nhp1-&gt;model2\n\n\n\n\n\nhp1-&gt;model3\n\n\n\n\n\nhp2\n\nhp2\n\n\n\nhp2-&gt;model1\n\n\n\n\n\nhp2-&gt;model2\n\n\n\n\n\nhp2-&gt;model3\n\n\n\n\n\nhp3\n\nhp3\n\n\n\nhp3-&gt;model1\n\n\n\n\n\nhp3-&gt;model2\n\n\n\n\n\nhp3-&gt;model3\n\n\n\n\n\nperformance\n\nperformance1 ... performance27\n\n\n\nmodel1-&gt;performance\n\n\n\n\n\nmodel1-&gt;performance\n\n\n\n\n\nmodel1-&gt;performance\n\n\n\n\n\nmodel1-&gt;performance\n\n\n\n\n\nmodel1-&gt;performance\n\n\n\n\n\nmodel1-&gt;performance\n\n\n\n\n\nmodel1-&gt;performance\n\n\n\n\n\nmodel1-&gt;performance\n\n\n\n\n\nmodel1-&gt;performance\n\n\n\n\n\nmodel2-&gt;performance\n\n\n\n\n\nmodel2-&gt;performance\n\n\n\n\n\nmodel2-&gt;performance\n\n\n\n\n\nmodel2-&gt;performance\n\n\n\n\n\nmodel2-&gt;performance\n\n\n\n\n\nmodel2-&gt;performance\n\n\n\n\n\nmodel2-&gt;performance\n\n\n\n\n\nmodel2-&gt;performance\n\n\n\n\n\nmodel2-&gt;performance\n\n\n\n\n\nmodel3-&gt;performance\n\n\n\n\n\nmodel3-&gt;performance\n\n\n\n\n\nmodel3-&gt;performance\n\n\n\n\n\nmodel3-&gt;performance\n\n\n\n\n\nmodel3-&gt;performance\n\n\n\n\n\nmodel3-&gt;performance\n\n\n\n\n\nmodel3-&gt;performance\n\n\n\n\n\nmodel3-&gt;performance\n\n\n\n\n\nmodel3-&gt;performance\n\n\n\n\n\n\n\n\n\n\n\nHow did we get performance17 again? 🤯"
  },
  {
    "objectID": "ai/wb_dvc_slides.html#dvc-principles",
    "href": "ai/wb_dvc_slides.html#dvc-principles",
    "title": "Version control for data science & machine learning with DVC",
    "section": "DVC principles",
    "text": "DVC principles\nLarge files (datasets, models…) are kept outside Git\nEach large file or directory put under DVC tracking has an associated .dvc file\nGit only tracks the .dvc files (metadata)\n\nWorkflows can be tracked for collaboration and reproducibility\n\n\nDVC functions as a Makefile and allows to only rerun what is necessary"
  },
  {
    "objectID": "ai/wb_dvc_slides.html#installation",
    "href": "ai/wb_dvc_slides.html#installation",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Installation",
    "text": "Installation\nFor Linux (other OSes, refer to the doc):\n\npip:\npip install dvc\nconda\npipx (if you want dvc available everywhere without having to activate virtual envs):\npipx install dvc\n\n\nOptional dependencies [s3], [gdrive], etc. for remote storage"
  },
  {
    "objectID": "ai/wb_dvc_slides.html#how-to-run",
    "href": "ai/wb_dvc_slides.html#how-to-run",
    "title": "Version control for data science & machine learning with DVC",
    "section": "How to run",
    "text": "How to run\n\nTerminal\ndvc ...\nVS Code extension\nPython library if installed via pip or conda\nimport dvc.api\n\n\nIn this webinar, I will use DVC through the command line"
  },
  {
    "objectID": "ai/wb_dvc_slides.html#acknowledgements",
    "href": "ai/wb_dvc_slides.html#acknowledgements",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nCode and data for this webinar modified from:\n\nReal Python\nDataLad handbook\nDVC documentation"
  },
  {
    "objectID": "ai/wb_dvc_slides.html#the-project",
    "href": "ai/wb_dvc_slides.html#the-project",
    "title": "Version control for data science & machine learning with DVC",
    "section": "The project",
    "text": "The project\ntree -L 3\n├── LICENSE\n├── data\n│   ├── prepared\n│   └── raw\n│       ├── train\n│       └── val\n├── metrics\n├── model\n├── requirements.txt\n└── src\n    ├── evaluate.py\n    ├── prepare.py\n    └── train.py"
  },
  {
    "objectID": "ai/wb_dvc_slides.html#initialize-git-repo",
    "href": "ai/wb_dvc_slides.html#initialize-git-repo",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Initialize Git repo",
    "text": "Initialize Git repo\ngit init\nInitialized empty Git repository in dvc/.git/\n\nThis creates the .git directory\n\n\ngit status\nOn branch main\n\nNo commits yet\n\nUntracked files:\n    LICENSE\n    data/\n    requirements.txt\n    src/"
  },
  {
    "objectID": "ai/wb_dvc_slides.html#initialize-dvc-project",
    "href": "ai/wb_dvc_slides.html#initialize-dvc-project",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Initialize DVC project",
    "text": "Initialize DVC project\ndvc init\nInitialized DVC repository.\n\nYou can now commit the changes to git.\n\nYou will also see a note about usage analytics collection and info on how to opt out\n\n\nA .dvc directory and a .dvcignore file got created"
  },
  {
    "objectID": "ai/wb_dvc_slides.html#commit-dvc-system-files",
    "href": "ai/wb_dvc_slides.html#commit-dvc-system-files",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Commit DVC system files",
    "text": "Commit DVC system files\n\nDVC automatically staged its system file for us:\ngit status\nOn branch main\n\nNo commits yet\n\nChanges to be committed:\n    new file:   .dvc/.gitignore\n    new file:   .dvc/config\n    new file:   .dvcignore\n\nUntracked files:\n    LICENSE\n    data/\n    requirements.txt\n    src/\n\nSo we can directly commit:\ngit commit -m \"Initialize DVC\""
  },
  {
    "objectID": "ai/wb_dvc_slides.html#prepare-repo",
    "href": "ai/wb_dvc_slides.html#prepare-repo",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Prepare repo",
    "text": "Prepare repo\nLet’s work in a virtual environment:\n# Create venv and add to .gitignore\npython -m venv venv && echo venv &gt; .gitignore\n\n# Activate venv\nsource venv/bin/activate\n\n# Update pip\npython -m pip install --upgrade pip\n\n# Install packages needed\npython -m pip install -r requirements.txt"
  },
  {
    "objectID": "ai/wb_dvc_slides.html#clean-working-tree",
    "href": "ai/wb_dvc_slides.html#clean-working-tree",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Clean working tree",
    "text": "Clean working tree\ngit add .gitignore LICENSE requirements.txt\ngit commit -m \"Add general files\"\ngit add src\ngit commit -m \"Add scripts\"\ngit status\nOn branch main\nUntracked files:\n    data/\n\n\nNow, it is time to deal with the data"
  },
  {
    "objectID": "ai/wb_dvc_slides.html#put-data-under-dvc-tracking",
    "href": "ai/wb_dvc_slides.html#put-data-under-dvc-tracking",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Put data under DVC tracking",
    "text": "Put data under DVC tracking\nWe are still not tracking any data:\ndvc status\nThere are no data or pipelines tracked in this project yet.\nYou can choose what to track as a unit (i.e. each picture individually, the whole data directory as a unit)\nLet’s break it down by set:\ndvc add data/raw/train\ndvc add data/raw/val"
  },
  {
    "objectID": "ai/wb_dvc_slides.html#section",
    "href": "ai/wb_dvc_slides.html#section",
    "title": "Version control for data science & machine learning with DVC",
    "section": "",
    "text": "This adds data to .dvc/cache/files and created 3 files in data/raw:\n\n.gitignore\ntrain.dvc\nval.dvc\n\nThe .gitignore tells Git not to track the data:\ncat data/raw/.gitignore\n/train\n/val\nThe .dvc files contain the metadata for the cached directories"
  },
  {
    "objectID": "ai/wb_dvc_slides.html#tracked-data",
    "href": "ai/wb_dvc_slides.html#tracked-data",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Tracked data",
    "text": "Tracked data\nWe are all good:\ndvc status\nData and pipelines are up to date."
  },
  {
    "objectID": "ai/wb_dvc_slides.html#data-deduplication",
    "href": "ai/wb_dvc_slides.html#data-deduplication",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Data (de)duplication",
    "text": "Data (de)duplication\nLink between checked-out version of a file/directory and the cache:\n\nCache ⟷ working directory\n\n\n\n\n\n\n\n\nDuplication\nEditable\n\n\n\n\nReflinks*\nOnly when needed\nYes\n\n\nHardlinks/Symlinks\nNo\nNo\n\n\nCopies\nYes\nYes\n\n\n\n*Reflinks only available for a few file systems (Btrfs, XFS, OCFS2, or APFS)"
  },
  {
    "objectID": "ai/wb_dvc_slides.html#commit-the-metafiles",
    "href": "ai/wb_dvc_slides.html#commit-the-metafiles",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Commit the metafiles",
    "text": "Commit the metafiles\nThe metafiles should be put under Git version control\n\nYou can configure DVC to automatically stage its newly created system files:\ndvc config [--system] [--global] core.autostage true\n\nYou can then commit directly:\ngit commit -m \"Initial version of data\"\ngit status\nOn branch main\nnothing to commit, working tree clean"
  },
  {
    "objectID": "ai/wb_dvc_slides.html#track-changes-to-the-data",
    "href": "ai/wb_dvc_slides.html#track-changes-to-the-data",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Track changes to the data",
    "text": "Track changes to the data\nLet’s make some change to the data:\nrm data/raw/val/n03445777/ILSVRC2012_val*\n\nRemember that Git is not tracking the data:\ngit status\nOn branch main\nnothing to commit, working tree clean\n\n\nBut DVC is:\ndvc status\ndata/raw/val.dvc:\n    changed outs:\n            modified:           data/raw/val"
  },
  {
    "objectID": "ai/wb_dvc_slides.html#add-changes-to-dvc",
    "href": "ai/wb_dvc_slides.html#add-changes-to-dvc",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Add changes to DVC",
    "text": "Add changes to DVC\ndvc add data/raw/val\ndvc status\nData and pipelines are up to date.\n\nNow we need to commit the changes to the .dvc file to Git:\ngit status\nOn branch main\nChanges to be committed:\n    modified:   data/raw/val.dvc\n\nStaging happened automatically because I have set the autostage option to true on my system\n\ngit commit -m \"Delete data/raw/val/n03445777/ILSVRC2012_val*\""
  },
  {
    "objectID": "ai/wb_dvc_slides.html#check-out-older-versions",
    "href": "ai/wb_dvc_slides.html#check-out-older-versions",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Check out older versions",
    "text": "Check out older versions\nWhat if we want to go back to the 1st version of our data?\nFor this, we first use Git to checkout the proper commit, then run dvc checkout to have the data catch up to the .dvc file\nTo avoid forgetting to run the commands that will make DVC catch up to Git, we can automate this process by installing Git hooks:\ndvc install"
  },
  {
    "objectID": "ai/wb_dvc_slides.html#git-workflows",
    "href": "ai/wb_dvc_slides.html#git-workflows",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Git workflows",
    "text": "Git workflows\ngit checkout is ok to have a look, but a detached HEAD is not a good place to create new commits\nLet’s create a new branch and switch to it:\ngit switch -c alternative\nSwitched to a new branch 'alternative'\nGoing back and forth between both versions of our data is now as simple as switching branch:\ngit switch main\ngit switch alternative"
  },
  {
    "objectID": "ai/wb_dvc_slides.html#classic-workflow",
    "href": "ai/wb_dvc_slides.html#classic-workflow",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Classic workflow",
    "text": "Classic workflow\nThe Git project (including .dvc files) go to a Git remote (GitHub/GitLab/Bitbucket/server)\nThe data go to a DVC remote (AWS/Azure/Google Drive/server/etc.)"
  },
  {
    "objectID": "ai/wb_dvc_slides.html#dvc-remotes",
    "href": "ai/wb_dvc_slides.html#dvc-remotes",
    "title": "Version control for data science & machine learning with DVC",
    "section": "DVC remotes",
    "text": "DVC remotes\nDVC can use many cloud storage or remote machines/server via SSH, WebDAV, etc.\nLet’s create a local remote here:\n# Create a directory outside the project\nmkdir ../remote\n\n# Setup default (-d) remote\ndvc remote add -d local_remote ../remote\nSetting 'local_remote' as a default remote.\ncat .dvc/config\n[core]\n    remote = local_remote\n['remote \"local_remote\"']\n    url = ../../remote"
  },
  {
    "objectID": "ai/wb_dvc_slides.html#commit-remote-config",
    "href": "ai/wb_dvc_slides.html#commit-remote-config",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Commit remote config",
    "text": "Commit remote config\nThe new remote configuration should be committed:\ngit status\nOn branch alternative\n\nChanges not staged for commit:\n    modified:   .dvc/config\ngit add .\ngit commit -m \"Config remote\""
  },
  {
    "objectID": "ai/wb_dvc_slides.html#push-to-remotes",
    "href": "ai/wb_dvc_slides.html#push-to-remotes",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Push to remotes",
    "text": "Push to remotes\nLet’s push the data from the cache (.dvc/cache) to the remote:\ndvc push\n2702 files pushed\n\nWith Git hooks installed, dvc push is automatically run after git push\n(But the data is pushed to the DVC remote while the files tracked by Git get pushed to the Git remote)\n\nBy default, the entire data cache gets pushed to the remote, but there are many options\n\nExample: only push data corresponding to a certain .dvc files\ndvc push data/raw/val.dvc"
  },
  {
    "objectID": "ai/wb_dvc_slides.html#pull-from-remotes",
    "href": "ai/wb_dvc_slides.html#pull-from-remotes",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Pull from remotes",
    "text": "Pull from remotes\ndvc fetch downloads data from the remote into the cache. To have it update the working directory, follow by dvc checkout\nYou can do these 2 commands at the same time with dvc pull"
  },
  {
    "objectID": "ai/wb_dvc_slides.html#dvc-pipelines",
    "href": "ai/wb_dvc_slides.html#dvc-pipelines",
    "title": "Version control for data science & machine learning with DVC",
    "section": "DVC pipelines",
    "text": "DVC pipelines\nDVC pipelines create reproducible workflows and are functionally similar to Makefiles\nEach step in a pipeline is created with dvc stage add and add an entry to a dvc.yaml file\n\ndvc stage add options:\n-n: name of stage\n-d: dependency\n-o: output\n\n\nEach stage contains:\n\ncmd: the command executed\ndeps: the dependencies\nouts: the outputs\n\nThe file is then used to visualize the pipeline and run it"
  },
  {
    "objectID": "ai/wb_dvc_slides.html#example",
    "href": "ai/wb_dvc_slides.html#example",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Example",
    "text": "Example\nLet’s create a pipeline to run a classifier on our data\nThe pipeline contains 3 steps:\n\nprepare\ntrain\nevaluate"
  },
  {
    "objectID": "ai/wb_dvc_slides.html#create-a-pipeline",
    "href": "ai/wb_dvc_slides.html#create-a-pipeline",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Create a pipeline",
    "text": "Create a pipeline\n1st stage (data preparation):\ndvc stage add -n prepare -d src/prepare.py -d data/raw \\\n    -o data/prepared/train.csv -o data/prepared/test.csv \\\n    python src/prepare.py\nAdded stage 'prepare' in 'dvc.yaml'\n\n2nd stage (training)\ndvc stage add -n train -d src/train.py -d data/prepared/train.csv \\\n    -o model/model.joblib \\\n    python src/train.py\nAdded stage `train` in 'dvc.yaml'\n\n\n3rd stage (evaluation)\ndvc stage add -n evaluate -d src/evaluate.py -d model/model.joblib \\\n    -M metrics/accuracy.json \\\n    python src/evaluate.py\nAdded stage `evaluate` in 'dvc.yaml'"
  },
  {
    "objectID": "ai/wb_dvc_slides.html#commit-pipeline",
    "href": "ai/wb_dvc_slides.html#commit-pipeline",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Commit pipeline",
    "text": "Commit pipeline\ngit commit -m \"Define pipeline\"\nprepare:\n    changed deps:\n            modified:           data/raw\n            modified:           src/prepare.py\n    changed outs:\n            deleted:            data/prepared/test.csv\n            deleted:            data/prepared/train.csv\ntrain:\n    changed deps:\n            deleted:            data/prepared/train.csv\n            modified:           src/train.py\n    changed outs:\n            deleted:            model/model.joblib\nevaluate:\n    changed deps:\n            deleted:            model/model.joblib\n            modified:           src/evaluate.py\n    changed outs:\n            deleted:            metrics/accuracy.json\n[main 4aa331b] Define pipeline\n 3 files changed, 27 insertions(+)\n create mode 100644 data/prepared/.gitignore\n create mode 100644 dvc.yaml\n create mode 100644 model/.gitignore"
  },
  {
    "objectID": "ai/wb_dvc_slides.html#visualize-pipeline-in-a-dag",
    "href": "ai/wb_dvc_slides.html#visualize-pipeline-in-a-dag",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Visualize pipeline in a DAG",
    "text": "Visualize pipeline in a DAG\ndvc dag\n+--------------------+         +------------------+\n| data/raw/train.dvc |         | data/raw/val.dvc |\n+--------------------+         +------------------+\n                  ***           ***\n                     **       **\n                       **   **\n                    +---------+\n                    | prepare |\n                    +---------+\n                          *\n                          *\n                          *\n                      +-------+\n                      | train |\n                      +-------+\n                          *\n                          *\n                          *\n                    +----------+\n                    | evaluate |\n                    +----------+"
  },
  {
    "objectID": "ai/wb_dvc_slides.html#run-pipeline",
    "href": "ai/wb_dvc_slides.html#run-pipeline",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Run pipeline",
    "text": "Run pipeline\ndvc repro\n'data/raw/train.dvc' didn't change, skipping\n'data/raw/val.dvc' didn't change, skipping\nRunning stage 'prepare':\n&gt; python src/prepare.py\nGenerating lock file 'dvc.lock'\nUpdating lock file 'dvc.lock'\n\nRunning stage 'train':\n&gt; python src/train.py\nUpdating lock file 'dvc.lock'\n\nRunning stage 'evaluate':\n&gt; python src/evaluate.py\nUpdating lock file 'dvc.lock'\nUse `dvc push` to send your updates to remote storage."
  },
  {
    "objectID": "ai/wb_dvc_slides.html#dvc-repro-breakdown",
    "href": "ai/wb_dvc_slides.html#dvc-repro-breakdown",
    "title": "Version control for data science & machine learning with DVC",
    "section": "dvc repro breakdown",
    "text": "dvc repro breakdown\n\ndvc repro runs the dvc.yaml file in a Makefile fashion\nFirst, it looks at the dependencies: the data didn’t change\nThen it ran the commands to produce the outputs (since it is our first run, we had no outputs)\nWhen the 1st stage is run, a dvc.lock is created with information on that part of the run\nWhen the 2nd and 3rd stages are run, dvc.lock is updated\nAt the end of the run dvc.lock contains all the info about the run we just did (version of the data used, etc.)\nA new directory called runs is created in .dvc/cache with cached data for this run"
  },
  {
    "objectID": "ai/wb_dvc_slides.html#results-of-the-run",
    "href": "ai/wb_dvc_slides.html#results-of-the-run",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Results of the run",
    "text": "Results of the run\n\nThe prepared data was created in data/prepared (with a .gitignore to exclude it from Git—you don’t want to track results in Git, but the scripts that can reproduce them)\nA model was saved in model (with another .gitignore file)\nThe accuracy of this run was created in metrics"
  },
  {
    "objectID": "ai/wb_dvc_slides.html#clean-working-tree-1",
    "href": "ai/wb_dvc_slides.html#clean-working-tree-1",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Clean working tree",
    "text": "Clean working tree\nNow, we definitely want to create a commit with the dvc.lock\nWe could add the metrics resulting from this run in the same commit:\ngit add metrics\ngit commit -m \"First pipeline run and results\"\n\nOur working tree is now clean and our data/pipeline up to date:\ngit status\nOn branch alternative\nnothing to commit, working tree clean\ndvc status\nData and pipelines are up to date."
  },
  {
    "objectID": "ai/wb_dvc_slides.html#modify-pipeline",
    "href": "ai/wb_dvc_slides.html#modify-pipeline",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Modify pipeline",
    "text": "Modify pipeline\nFrom now on, if we edit one of the scripts, or one of the dependencies, dvc status will tell us what changed and dvc repro will only rerun the parts of the pipeline to update the result, pretty much as a Makefile would"
  },
  {
    "objectID": "ai/wb_dvc_slides.html#going-further-next-time",
    "href": "ai/wb_dvc_slides.html#going-further-next-time",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Going further … next time",
    "text": "Going further … next time\n DVC is a sophisticated tool with many additional features:\n\nCreation of data registries\nDVCLive\nA Python library to log experiment metrics\nVisualize the performance logs as plots\nContinuous integration\nWith the sister project CML (Continuous Machine Learning)\n\n\n\n\n\n Back to webinar page"
  },
  {
    "objectID": "ai/wb_copilot_slides.html#codex",
    "href": "ai/wb_copilot_slides.html#codex",
    "title": "AI-powered programming with",
    "section": "Codex",
    "text": "Codex\nOpenAI Codex—based on GPT-3—is the model behind GitHub Copilot\nAll the big corporate companies are rushing to launch a growing number of similar (and not free, not open source) productivity products (e.g. tabnine, Microsoft Visual Studio IntelliCode, Amazon CodeWhisperer)\nThese products generate code in a narrow context (auto-completion or transformation of natural language to code or vise-versa)"
  },
  {
    "objectID": "ai/wb_copilot_slides.html#alphacode-2",
    "href": "ai/wb_copilot_slides.html#alphacode-2",
    "title": "AI-powered programming with",
    "section": "AlphaCode 2",
    "text": "AlphaCode 2\nGoogle DeepMind AlphaCode 2—based on Gemini—stands out as a totally different (and for now totally unavailable) product generating code at the level of competitive programming (reaching the 85th percentile)\nThink of it as code evolution by “natural” selection:\n\na very large number of code samples are generated (think “mutations”)\na filtering and scoring system selects for the best candidates (that’s the selection part)\n\nAlphaCode 2 is able to solve much more open-ended problems"
  },
  {
    "objectID": "ai/wb_copilot_slides.html#what-about-foss",
    "href": "ai/wb_copilot_slides.html#what-about-foss",
    "title": "AI-powered programming with",
    "section": "What about FOSS?",
    "text": "What about FOSS?\nFree\nThese models are large and most convenient to run on servers\n → Price of cloud service\nSome self-hosted options exist. A very promising one is Tabby. Not practical for everyone\nOpen source\nWhile these models feed from open source code, they are themselves not open source 🙁\nThe open source community is trying to provide open source alternatives (e.g. Tabby). Despite the much more limited resources, the performance of some of these alternatives is very good"
  },
  {
    "objectID": "ai/wb_copilot_slides.html#what-is-github-copilot",
    "href": "ai/wb_copilot_slides.html#what-is-github-copilot",
    "title": "AI-powered programming with",
    "section": "What is GitHub Copilot?",
    "text": "What is GitHub Copilot?\n\n → Cloud-hosted AI programming assistant\n\n\nDeveloped by GitHub (Microsoft)\nRunning Codex, a model by OpenAI derived from the LLM GPT-3 and trained on open source code"
  },
  {
    "objectID": "ai/wb_copilot_slides.html#access",
    "href": "ai/wb_copilot_slides.html#access",
    "title": "AI-powered programming with",
    "section": "Access",
    "text": "Access\nIndividual or organization GitHub accounts\nRequires subscription\nStudents, teachers, and maintainers of popular open source projects can apply for free access"
  },
  {
    "objectID": "ai/wb_copilot_slides.html#safety",
    "href": "ai/wb_copilot_slides.html#safety",
    "title": "AI-powered programming with",
    "section": "Safety",
    "text": "Safety\nFilters are in place for offensive words, but…\nGenerated code comes with no guaranty of safety or quality\nA lawsuit is open against GitHub Copilot for licenses violation"
  },
  {
    "objectID": "ai/wb_copilot_slides.html#supported-languages",
    "href": "ai/wb_copilot_slides.html#supported-languages",
    "title": "AI-powered programming with",
    "section": "Supported languages",
    "text": "Supported languages\nAny language used in public repos\nQuality of suggestions is higher for languages with lots of data"
  },
  {
    "objectID": "ai/wb_copilot_slides.html#how-to-use-it",
    "href": "ai/wb_copilot_slides.html#how-to-use-it",
    "title": "AI-powered programming with",
    "section": "How to use it?",
    "text": "How to use it?\n\nStart typing code and get autocomplete suggestions\n\n\nWrite comments describing what the code should do and get code generation based on context\n\n\nIt is easy to:\n  → accept suggestions word by word\n  → line by line\n  → for entire functions\n  → cycle through different suggestions"
  },
  {
    "objectID": "ai/wb_copilot_slides.html#interface",
    "href": "ai/wb_copilot_slides.html#interface",
    "title": "AI-powered programming with",
    "section": "Interface",
    "text": "Interface\nExtensions to text editors:\n  → Visual Studio Code/Visual Studio\n  → Vim/Neovim/Emacs\n  → JetBrains IDEs\n  → Azure Data Studio"
  },
  {
    "objectID": "ai/wb_copilot_slides.html#get-a-subscription",
    "href": "ai/wb_copilot_slides.html#get-a-subscription",
    "title": "AI-powered programming with",
    "section": "Get a subscription",
    "text": "Get a subscription\nGo to your GitHub account page\n  → Settings\n  → Copilot\n  → Enable\nProvide free access or payment method\nSet settings"
  },
  {
    "objectID": "ai/wb_copilot_slides.html#vs-code",
    "href": "ai/wb_copilot_slides.html#vs-code",
    "title": "AI-powered programming with",
    "section": "VS Code",
    "text": "VS Code\n\nInstall the GitHub Copilot extension\n\n\nNext suggestion:       Alt+]\nPrevious suggestion:      Alt+[\nReject suggestion:       Esc\nAccept suggestion:      Tab\nAccept next suggested word: Ctrl+→  (Command+→  for macOS)\nSet your own key binding for editor.action.inlineSuggest.acceptNextLine to accept next suggested line\nOpen new tab with options:  Ctrl+Enter\nYou can also hover over suggestions"
  },
  {
    "objectID": "ai/wb_copilot_slides.html#vimneovim",
    "href": "ai/wb_copilot_slides.html#vimneovim",
    "title": "AI-powered programming with",
    "section": "Vim/Neovim",
    "text": "Vim/Neovim\nInstall Node.js\nClone https://github.com/github/copilot.vim\nConfigure:\n:Copilot setup\nEnable:\n:Copilot enable\nGet help:\n:help copilot"
  },
  {
    "objectID": "ai/wb_copilot_slides.html#emacs",
    "href": "ai/wb_copilot_slides.html#emacs",
    "title": "AI-powered programming with",
    "section": "Emacs",
    "text": "Emacs\nInstall Node.js\nAssuming straight is installed:\n(straight-use-package 'editorconfig)                   ; Copilot dependency\n\n(use-package copilot\n    :straight (:host github\n                     :repo \"copilot-emacs/copilot.el\"\n                     :files (\"dist\" \"*.el\"))\n    :hook (prog-mode . copilot-mode)                   ; Settings up to you\n    :bind ((\"C-8\" . copilot-complete)\n           :map copilot-completion-map\n           (\"C-j\" . copilot-accept-completion)\n           (\"C-f\" . copilot-accept-completion-by-word)\n           (\"C-t\" . copilot-accept-completion-by-line)\n           (\"C-n\" . copilot-next-completion)\n           (\"C-p\" . copilot-previous-completion)))\nLogin to your GitHub account (only needs to be done once): M-x copilot-login"
  },
  {
    "objectID": "ai/wb_copilot_slides.html#what-is-copilot-in-the-cli",
    "href": "ai/wb_copilot_slides.html#what-is-copilot-in-the-cli",
    "title": "AI-powered programming with",
    "section": "What is Copilot in the CLI?",
    "text": "What is Copilot in the CLI?\nIn beta\nAn extension to GitHub CLI (GitHub operations from the CLI)\n → Generate commands from natural language\n → Generate natural language explanations from commands\nTrained on data up to 2021\nLower performance for natural languages ≠ English\nBe very careful: the command line is powerful and you can delete your data or mess up your system if you don’t know what you are doing. Check commands carefully!"
  },
  {
    "objectID": "ai/wb_copilot_slides.html#setup-1",
    "href": "ai/wb_copilot_slides.html#setup-1",
    "title": "AI-powered programming with",
    "section": "Setup",
    "text": "Setup\n\nInstall GitHub CLI\nConnect to your GitHub account:\ngh auth login\n\n\n Install Copilot in the CLI:\ngh extension install github/gh-copilot\n\nUpdate with: gh extension upgrade gh-copilot"
  },
  {
    "objectID": "ai/wb_copilot_slides.html#usage",
    "href": "ai/wb_copilot_slides.html#usage",
    "title": "AI-powered programming with",
    "section": "Usage",
    "text": "Usage\nGet code explanations:\ngh copilot explain\n Get code from natural language:\ngh copilot suggest"
  },
  {
    "objectID": "ai/wb_copilot_slides.html#resources",
    "href": "ai/wb_copilot_slides.html#resources",
    "title": "AI-powered programming with",
    "section": "Resources",
    "text": "Resources\nGitHub support portal\nGitHub Copilot documentation\nStack Overflow [github-copilot] tag\ncopilot.el (unofficial Emacs plug-in)\n\n\n\n\n Back to webinar page"
  },
  {
    "objectID": "ai/top_ws.html",
    "href": "ai/top_ws.html",
    "title": "AI workshops",
    "section": "",
    "text": "Audio DataLoader\n\n\n\n\nFinding pre-trained models\n\n\n\n\nIntro ML for the humanities",
    "crumbs": [
      "AI",
      "<b><em>Workshops</em></b>"
    ]
  },
  {
    "objectID": "ai/top_pt.html",
    "href": "ai/top_pt.html",
    "title": "Getting started with PyTorch",
    "section": "",
    "text": "This introductory course to deep learning and neural networks with the PyTorch framework does not assume any prior knowledge in machine learning. Some basic Python is useful, but not strictly necessary.\n\n Start course ➤",
    "crumbs": [
      "AI",
      "<b><em>PyTorch</em></b>"
    ]
  },
  {
    "objectID": "ai/top_fl.html",
    "href": "ai/top_fl.html",
    "title": "JAX neural networks with Flax",
    "section": "",
    "text": "JAX is a very fast open source Python library for function transformations (including differentiation) and array computations on accelerators (GPUs/TPUs). It provides a structural framework on which domain specific libraries can build.\nIn the field of deep learning, the most popular of these libraries is Flax: Flax makes full use of JAX’s power and adds the tools to build and train neural networks.\nThis introduction to Flax covers the construction of networks, the handling of model states, and training optimization techniques. It assumes basic understanding of the functioning of JAX.\n\n Start course ➤",
    "crumbs": [
      "AI",
      "<b><em>Flax</em></b>"
    ]
  },
  {
    "objectID": "ai/sk_intro.html",
    "href": "ai/sk_intro.html",
    "title": "Introduction",
    "section": "",
    "text": "What is scikit-learn and how does it fit in the wide landscape of machine learning frameworks?",
    "crumbs": [
      "AI",
      "<b><em>Scikit-learn</em></b>"
    ]
  },
  {
    "objectID": "ai/sk_intro.html#what-is-machine-learning",
    "href": "ai/sk_intro.html#what-is-machine-learning",
    "title": "Introduction",
    "section": "What is machine learning?",
    "text": "What is machine learning?\nMaybe we should start with some definitions:\n\nArtificial intelligence (AI) can be defined as any human-made system mimicking animal intelligence. This is a large and very diverse field.\nMachine learning (ML) is a subfield of AI that can be defined as computer programs whose performance at a task improves with experience. This learning can be split into three categories:\n\nSupervised learning\nRegression is a form of supervised learning with continuous outputs. Classification is supervised learning with discrete outputs.\nSupervised learning uses training data in the form of example input/output pairs.\nThe goal is to find the relationship between inputs and outputs.\nUnsupervised learning\nClustering, social network analysis, market segmentation, dimensionality reduction (e.g. PCA), anomaly detection … are all forms of unsupervised learning.\nUnsupervised learning uses unlabelled data and the goal is to find patterns within the data.\nReinforcement learning\nThe algorithm explores by performing random actions and these actions are rewarded or punished (bonus points or penalties).\nThis is how algorithms learn to play games.\n\nDeep learning (DL) is itself a subfield of machine learning using deep artificial neural networks.",
    "crumbs": [
      "AI",
      "<b><em>Scikit-learn</em></b>"
    ]
  },
  {
    "objectID": "ai/sk_intro.html#what-is-scikit-learn",
    "href": "ai/sk_intro.html#what-is-scikit-learn",
    "title": "Introduction",
    "section": "What is scikit-learn?",
    "text": "What is scikit-learn?\nScikit-learn (also called sklearn) is a free and open source set of libraries for Python built on top of SciPy (thus using NumPy’s ndarray as the main data structure) used for machine learning.\nSklearn is a rich toolkit characterized by a clean, consistent, and very simple API.\nIt is easy to learn and very well documented. You can think of it as a vast collection of routines that are easy to apply and require little computing power.\n\n\nfrom scikit-learn.org",
    "crumbs": [
      "AI",
      "<b><em>Scikit-learn</em></b>"
    ]
  },
  {
    "objectID": "ai/sk_intro.html#scikit-learn-in-the-ml-landscape",
    "href": "ai/sk_intro.html#scikit-learn-in-the-ml-landscape",
    "title": "Introduction",
    "section": "Scikit-learn in the ML landscape",
    "text": "Scikit-learn in the ML landscape\nThere are many machine learning frameworks. While sklearn has many advantages (ease of use, wide range of tools), it has clear limitations: there is no GPU support and deep learning and reinforcement learning are out of its scope (the only neural network implemented is a multilayer perceptron (an historical, very basic, and terribly inefficient neural network).\nIf you want to access the power and flexibility that deep neural networks provide, you should turn towards tools such as PyTorch (wonderful tool both in academia, research, and industry) or the higher-level Keras (easy API, easy to learn, but less flexible).",
    "crumbs": [
      "AI",
      "<b><em>Scikit-learn</em></b>"
    ]
  },
  {
    "objectID": "ai/pt/ws_audio_dataloader.html",
    "href": "ai/pt/ws_audio_dataloader.html",
    "title": "Creating an audio DataLoader",
    "section": "",
    "text": "import torch\nimport torchaudio\nimport matplotlib.pyplot as plt",
    "crumbs": [
      "AI",
      "<b><em>Workshops</em></b>",
      "Audio DataLoader"
    ]
  },
  {
    "objectID": "ai/pt/ws_audio_dataloader.html#load-packages",
    "href": "ai/pt/ws_audio_dataloader.html#load-packages",
    "title": "Creating an audio DataLoader",
    "section": "",
    "text": "import torch\nimport torchaudio\nimport matplotlib.pyplot as plt",
    "crumbs": [
      "AI",
      "<b><em>Workshops</em></b>",
      "Audio DataLoader"
    ]
  },
  {
    "objectID": "ai/pt/ws_audio_dataloader.html#download-and-unzip-data",
    "href": "ai/pt/ws_audio_dataloader.html#download-and-unzip-data",
    "title": "Creating an audio DataLoader",
    "section": "Download and unzip data",
    "text": "Download and unzip data\nPyTorch comes with many classic datasets.\n\nExamples:\n\nlist of available datasets for vision\nlist of audio datasets\nlist of texts datasets\n\n\nThis is convenient to develop and test your model, or to compare its performance with existing models using these datasets.\nHere, we will use the YESNO dataset which can be accessed through the torchaudio.datasets.YESNO class:\nhelp(torchaudio.datasets.YESNO)\nHelp on class YESNO in module torchaudio.datasets.yesno:\n\nclass YESNO(torch.utils.data.dataset.Dataset)\n\n |  YESNO(root: Union[str, pathlib.Path], url: str =\n |    'http://www.openslr.org/resources/1/waves_yesno.tar.gz', \n |    folder_in_archive: str = 'waves_yesno', \n |    download: bool = False) -&gt; None\n |  \n |  Args:\n |    root (str or Path): Path to the directory where the dataset is found \n |      or downloaded.\n |    url (str, optional): The URL to download the dataset from.\n |      (default: \"http://www.openslr.org/resources/1/waves_yesno.tar.gz\")\n |    folder_in_archive (str, optional):\n |      The top-level directory of the dataset. (default: \"waves_yesno\")\n |    download (bool, optional):\n |      Whether to download the dataset if it is not found at root path. \n |      (default: False).\nThe root argument sets the location of the downloaded data.\n\nWhere to store this data in the cluster\nWe will all use the same data. It would make little sense to all download it in our home directory.\n\nIn the Alliance clusters, a good place to store data shared amongst members of a project is in the /project file system.\nYou usually belong to /project/def-&lt;group&gt;, where &lt;group&gt; is the name of your PI. You can access it from your home directory through the symbolic link ~/projects/def-&lt;group&gt;.\n\nIn our training cluster, we are all part of the group def-sponsor00, accessible through /project/def-sponsor00 (or the hyperlink ~/projects/def-sponsor00).\nWe will thus use ~/projects/def-sponsor00/data as the root argument for torchaudio.datasets.yesno):\nyesno_data = torchaudio.datasets.YESNO(\n    '~/projects/def-sponsor00/data/',\n    download=True)",
    "crumbs": [
      "AI",
      "<b><em>Workshops</em></b>",
      "Audio DataLoader"
    ]
  },
  {
    "objectID": "ai/pt/ws_audio_dataloader.html#explore-the-data",
    "href": "ai/pt/ws_audio_dataloader.html#explore-the-data",
    "title": "Creating an audio DataLoader",
    "section": "Explore the data",
    "text": "Explore the data\nA data point in YESNO is a tuple of waveform, sample_rate, and labels (the labels are 1 for “yes” and 0 for “no”).\nLet’s have a look at the first data point:\n\nyesno_data[0]\n\n(tensor([[ 3.0518e-05,  6.1035e-05,  3.0518e-05,  ..., -1.8616e-03,\n          -2.2583e-03, -1.3733e-03]]),\n 8000,\n [0, 0, 0, 0, 1, 1, 1, 1])\n\n\nOr, more nicely:\n\nwaveform, sample_rate, labels = yesno_data[0]\nprint(\"Waveform: {}\\nSample rate: {}\\nLabels: {}\".format(waveform, sample_rate, labels))\n\nWaveform: tensor([[ 3.0518e-05,  6.1035e-05,  3.0518e-05,  ..., -1.8616e-03,\n         -2.2583e-03, -1.3733e-03]])\nSample rate: 8000\nLabels: [0, 0, 0, 0, 1, 1, 1, 1]\n\n\nYou can also plot the data. For this, we will use pyplot from matplotlib.\nLet’s look at the waveform:\n\nplt.figure()\nplt.plot(waveform.t().numpy())",
    "crumbs": [
      "AI",
      "<b><em>Workshops</em></b>",
      "Audio DataLoader"
    ]
  },
  {
    "objectID": "ai/pt/ws_audio_dataloader.html#split-the-data-into-a-training-set-and-a-testing-set",
    "href": "ai/pt/ws_audio_dataloader.html#split-the-data-into-a-training-set-and-a-testing-set",
    "title": "Creating an audio DataLoader",
    "section": "Split the data into a training set and a testing set",
    "text": "Split the data into a training set and a testing set\n\ntrain_size = int(0.8 * len(yesno_data))\ntest_size = len(yesno_data) - train_size\ntrain_dataset, test_dataset = torch.utils.data.random_split(yesno_data, [train_size, test_size])",
    "crumbs": [
      "AI",
      "<b><em>Workshops</em></b>",
      "Audio DataLoader"
    ]
  },
  {
    "objectID": "ai/pt/ws_audio_dataloader.html#create-training-and-testing-dataloaders",
    "href": "ai/pt/ws_audio_dataloader.html#create-training-and-testing-dataloaders",
    "title": "Creating an audio DataLoader",
    "section": "Create training and testing DataLoaders",
    "text": "Create training and testing DataLoaders\nDataLoaders are Python iterables created by the torch.utils.data.DataLoader class from a dataset and a sampler.\nWe already have a dataset (yesno_data). Now we need a sampler (or sampling strategy) to draw samples from it. The sampling strategy contains the batch size, whether the data get shuffled prior to sampling, the number of workers used if the data is loaded in parallel, etc.\nTo create a training DataLoader with shuffled data and batch size of 1 (the default), we run:\n\ntrain_loader = torch.utils.data.DataLoader(train_dataset, shuffle=True)\n\ndata_loader is an iterable of 0.8*60=48 elements (80% of the 60 samples in the YESNO dataset):\n\nlen(train_loader)\n\n48\n\n\nWe do the same to create the testing DataLoader:\n\ntest_loader = torch.utils.data.DataLoader(test_dataset, shuffle=True)",
    "crumbs": [
      "AI",
      "<b><em>Workshops</em></b>",
      "Audio DataLoader"
    ]
  },
  {
    "objectID": "ai/pt/wb_upscaling.html",
    "href": "ai/pt/wb_upscaling.html",
    "title": "Image upscaling",
    "section": "",
    "text": "Super-resolution—the process of (re)creating high resolution images from low resolution ones—is an old field, but deep neural networks have seen a sudden surge of new and very impressive methods over the past 10 years, from SRCCN to SRGAN to Transformers.\nIn this webinar, I will give a quick overview of these methods and show how the latest state-of-the-art model—SwinIR—performs on a few test images. We will use PyTorch as our framework.\n\nSlides (Click and wait: this reveal.js presentation is heavy and takes some time to load.)",
    "crumbs": [
      "AI",
      "<b><em>Webinars</em></b>",
      "Image upscaling"
    ]
  },
  {
    "objectID": "ai/pt/wb_torchtensors.html",
    "href": "ai/pt/wb_torchtensors.html",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "",
    "text": "Before information can be fed to artificial neural networks (ANNs), it needs to be converted to a form ANNs can process: floating point numbers. Indeed, you don’t pass a sentence or an image through an ANN; you input numbers representing a sequence of words or pixel values.\nAll these floating point numbers need to be stored in a data structure. The most suited structure is multidimensional (to hold several layers of information) and since all data is of the same type, it is an array.\nPython already has several multidimensional array structures—the most popular of which being NumPy’s ndarray—but the particularities of deep learning call for special characteristics: ability to run operations on GPUs and/or in a distributed fashion, as well as the ability to keep track of computation graphs for automatic differentiation.\nThe PyTorch tensor is a Python data structure with these characteristics that can also easily be converted to/from NumPy’s ndarray and integrates well with other Python libraries such as Pandas.\nIn this webinar, suitable for users of all levels, we will have a deep look at this data structure and go much beyond a basic introduction.\nIn particular, we will:\n\nsee how tensors are stored in memory,\nlook at the metadata which allows this efficient memory storage,\ncover the basics of working with tensors (indexing, vectorized operations…),\nmove tensors to/from GPUs,\nconvert tensors to/from NumPy ndarrays,\nsee how tensors work in distributed frameworks,\nsee how linear algebra can be done with PyTorch tensors.\n\n\nSlides (Click and wait: this reveal.js presentation may take a little time to load.)",
    "crumbs": [
      "AI",
      "<b><em>Webinars</em></b>",
      "PyTorch tensors"
    ]
  },
  {
    "objectID": "ai/pt/pt_workflow.html",
    "href": "ai/pt/pt_workflow.html",
    "title": "Overall workflow",
    "section": "",
    "text": "This classic PyTorch tutorial goes over the entire workflow to create and train a simple image classifier.\nLet’s go over it step by step.",
    "crumbs": [
      "AI",
      "<b><em>PyTorch</em></b>",
      "Workflow"
    ]
  },
  {
    "objectID": "ai/pt/pt_workflow.html#the-data",
    "href": "ai/pt/pt_workflow.html#the-data",
    "title": "Overall workflow",
    "section": "The data",
    "text": "The data\nCIFAR-10 from the Canadian Institute for Advanced Research is a classic dataset of 60,000 color images falling into 10 classes (6,000 images in each class):\n\nairplane\nautomobile\nbird\ncat\ndeer\ndog\nfrog\nhorse\nship\ntruck\n\nThe images are of size 32x32 pixels (tiny!), which makes it very lightweight, quick to load and easy to play with.\n\nCreate a DataLoader\nA DataLoader is an iterable feeding data to a model at each iteration. The data loader transforms the data to the proper format, sets the batch size, whether the data is shuffled or not, and how the I/O is parallelized. You can create DataLoaders with the torch.utils.data.DataLoader class.\nLet’s create 2 DataLoaders: one for the train set and one for the test set.\n\nLoad packages\n\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\n\n\n\nCreate a transform object\nThe CIFAR-10 images in the TorchVision library are Image objects (from the PIL.Image module of the pillow package).\nWe need to normalize them and turn them into tensors:\n\ntransform = transforms.Compose(\n    [transforms.ToTensor(),\n     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n\n\n\nChoose a batch size\nRemember that the data move forward through the network (forward pass), outputting some estimates which are used to calculate some loss (or error) value. Then we get gradients through automatic differentiation and the model parameters are adjusted a little through gradient descent.\nYou do not have to have the entire training set go through this process each time: you can use batches.\nThe batch size is the number of items from the data that are processed before the model is updated. There is no hard rule to set good batch sizes and sizes tend to be picked through trial and error.\nHere are some rules to chose a batch size:\n\nmake sure that the batch fits in the CPU or GPU,\nsmall batches give faster results (each training iteration is very fast), but give less accuracy,\nlarge batches lead to slower training, but better accuracy.\n\nLet’s set the batch size to 4:\n\nbatch_size = 4\n\n\n\nPut it together into DataLoaders\n\ntrainset = torchvision.datasets.CIFAR10(root='./data',\n                                        train=True,\n                                        download=True,\n                                        transform=transform)\n\ntrainloader = torch.utils.data.DataLoader(trainset,\n                                          batch_size=batch_size,\n                                          shuffle=True,\n                                          num_workers=2)\n\ntestset = torchvision.datasets.CIFAR10(root='./data',\n                                       train=False,\n                                       download=True,\n                                       transform=transform)\n\ntestloader = torch.utils.data.DataLoader(testset,\n                                         batch_size=batch_size,\n                                         shuffle=False,\n                                         num_workers=2)\n\nWe will also need the classes:\n\nclasses = ('plane', 'car', 'bird', 'cat', 'deer',\n           'dog', 'frog', 'horse', 'ship', 'truck')\n\n\n\n\nVisualize a sample of the data\nThough not necessary, it can be useful to have a look at the data:\n\n# Load the packages for this\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Define a function to display an image\ndef imshow(img):\n    img = img / 2 + 0.5     # unnormalize\n    npimg = img.numpy()\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.show()\n\n# Get a batch of random training images\ndataiter = iter(trainloader)\nimages, labels = next(dataiter)\n\n# Display the images\nimshow(torchvision.utils.make_grid(images))\n\n# Print the labels\nprint(' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))\n\n\n\n\n\n\n\n\nfrog  deer  cat   plane",
    "crumbs": [
      "AI",
      "<b><em>PyTorch</em></b>",
      "Workflow"
    ]
  },
  {
    "objectID": "ai/pt/pt_workflow.html#the-model",
    "href": "ai/pt/pt_workflow.html#the-model",
    "title": "Overall workflow",
    "section": "The model",
    "text": "The model\n\nArchitecture\nFirst, we need to define the architecture of the network. There are many types of architectures. For images, CNN are well suited.\nIn Python, you can define a subclass of an existing class with:\nclass YourSubclass(BaseClass):\n    &lt;definition of your subclass&gt;        \nThe subclass is derived from the base class and inherits its properties. PyTorch contains the class torch.nn.Module which is used as the base class when defining a neural network.\n\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Net(nn.Module):\n    # Define the architecture of the network\n    def __init__(self):\n        super().__init__()\n        # 3 input image channel (3 colour channels)\n        # 6 output channels,\n        # 5x5 square convolution kernel\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        # Max pooling over a (2, 2) window\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        # 5*5 from image dimension\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n        self.fc2 = nn.Linear(120, 84)\n        # 10 is the size of the output layer\n        # since there are 10 classes\n        self.fc3 = nn.Linear(84, 10)\n\n    # Set the flow of data through the network for the forward pass\n    # x represents the data\n    def forward(self, x):\n        # F.relu is the rectified-linear activation function\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        # flatten all dimensions except the batch dimension\n        x = torch.flatten(x, 1)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\nLet’s create an instance of Net and print its structure:\n\nnet = Net()\nprint(net)\n\nNet(\n  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n  (fc1): Linear(in_features=400, out_features=120, bias=True)\n  (fc2): Linear(in_features=120, out_features=84, bias=True)\n  (fc3): Linear(in_features=84, out_features=10, bias=True)\n)\n\n\n\n\nLoss function and optimizer\nWe need to chose a loss function that will be used to calculate the gradients through backpropagation as well as an optimizer to do the gradient descent.\nSGD with momentum has proved a very efficient optimizing technique and is widely used.\n\nimport torch.optim as optim\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)",
    "crumbs": [
      "AI",
      "<b><em>PyTorch</em></b>",
      "Workflow"
    ]
  },
  {
    "objectID": "ai/pt/pt_workflow.html#training",
    "href": "ai/pt/pt_workflow.html#training",
    "title": "Overall workflow",
    "section": "Training",
    "text": "Training\nWe can now train the model:\n\nfor epoch in range(2):  # loop over the dataset twice\n\n    running_loss = 0.0\n\n    for i, data in enumerate(trainloader, 0):\n        # get the inputs; data is a list of [inputs, labels]\n        inputs, labels = data\n\n        # zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward + backward + optimize\n        outputs = net(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        # print statistics\n        running_loss += loss.item()\n        if i % 2000 == 1999:    # print every 2000 mini-batches\n            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n            running_loss = 0.0\n\nprint('Finished Training')\n\n[1,  2000] loss: 2.224\n[1,  4000] loss: 1.891\n[1,  6000] loss: 1.712\n[1,  8000] loss: 1.598\n[1, 10000] loss: 1.555\n[1, 12000] loss: 1.502\n[2,  2000] loss: 1.425\n[2,  4000] loss: 1.399\n[2,  6000] loss: 1.377\n[2,  8000] loss: 1.325\n[2, 10000] loss: 1.336\n[2, 12000] loss: 1.296\nFinished Training",
    "crumbs": [
      "AI",
      "<b><em>PyTorch</em></b>",
      "Workflow"
    ]
  },
  {
    "objectID": "ai/pt/pt_workflow.html#testing",
    "href": "ai/pt/pt_workflow.html#testing",
    "title": "Overall workflow",
    "section": "Testing",
    "text": "Testing\n\nLittle test on one batch for fun\nLet’s now test our model on one batch of testing data.\nFirst, let’s get a batch of random testing data:\n\ndataiter = iter(testloader)\nimages, labels = next(dataiter)\n\nLet’s display them and print their true labels:\n\nimshow(torchvision.utils.make_grid(images))\nprint('Real: ', ' '.join(f'{classes[labels[j]]:5s}' for j in range(4)))\n\n\n\n\n\n\n\n\nReal:  cat   ship  ship  plane\n\n\nNow, let’s run the same batch of testing images through our model:\n\noutputs = net(images)\n\nLet’s get the best predictions for these:\n\n_, predicted = torch.max(outputs, 1)\n\nprint('Predicted: ', ' '.join(f'{classes[predicted[j]]:5s}'\n                              for j in range(4)))\n\nPredicted:  cat   car   car   ship \n\n\n\n\nMore serious testing\nThis was fun, but of course, with a sample of one, we can’t say anything about how good our model is. We need to test it on many more images from the test set.\nLet’s use the entire test set:\n\ncorrect = 0\ntotal = 0\n# since we're not training, we don't need to calculate the gradients for our outputs\nwith torch.no_grad():\n    for data in testloader:\n        images, labels = data\n        # calculate outputs by running images through the network\n        outputs = net(images)\n        # the class with the highest energy is what we choose as prediction\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\nprint(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n\nAccuracy of the network on the 10000 test images: 51 %\n\n\n\n\nPer class testing\nWe could see whether the model seem to perform better for some classes than others:\n\n# prepare to count predictions for each class\ncorrect_pred = {classname: 0 for classname in classes}\ntotal_pred = {classname: 0 for classname in classes}\n\n# again no gradients needed\nwith torch.no_grad():\n    for data in testloader:\n        images, labels = data\n        outputs = net(images)\n        _, predictions = torch.max(outputs, 1)\n        # collect the correct predictions for each class\n        for label, prediction in zip(labels, predictions):\n            if label == prediction:\n                correct_pred[classes[label]] += 1\n            total_pred[classes[label]] += 1\n\n\n# print accuracy for each class\nfor classname, correct_count in correct_pred.items():\n    accuracy = 100 * float(correct_count) / total_pred[classname]\n    print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')\n\nAccuracy for class: plane is 42.2 %\nAccuracy for class: car   is 57.9 %\nAccuracy for class: bird  is 23.9 %\nAccuracy for class: cat   is 14.9 %\nAccuracy for class: deer  is 49.6 %\nAccuracy for class: dog   is 58.6 %\nAccuracy for class: frog  is 69.6 %\nAccuracy for class: horse is 68.2 %\nAccuracy for class: ship  is 63.6 %\nAccuracy for class: truck is 70.9 %",
    "crumbs": [
      "AI",
      "<b><em>PyTorch</em></b>",
      "Workflow"
    ]
  },
  {
    "objectID": "ai/pt/pt_tensors.html",
    "href": "ai/pt/pt_tensors.html",
    "title": "PyTorch tensors",
    "section": "",
    "text": "Before information can be processed by algorithms, it needs to be converted to floating point numbers. Indeed, you don’t pass a sentence or an image through a model; instead you input numbers representing a sequence of words or pixel values.\nAll these floating point numbers need to be stored in a data structure. The most suited structure is multidimensional (to hold several layers of information) and homogeneous—all data of the same type—for efficiency.\nPython already has several multidimensional array structures (e.g. NumPy’s ndarray) but the particularities of deep learning call for special characteristics such as the ability to run operations on GPUs and/or in a distributed fashion, the ability to keep track of computation graphs for automatic differentiation, and different defaults (lower precision for improved training performance).\nThe PyTorch tensor is a Python data structure with these characteristics that can easily be converted to/from NumPy’s ndarray and integrates well with other Python libraries such as Pandas.\nIn this section, we will explore the basics of PyTorch tensors.",
    "crumbs": [
      "AI",
      "<b><em>PyTorch</em></b>",
      "PyTorch tensors"
    ]
  },
  {
    "objectID": "ai/pt/pt_tensors.html#importing-pytorch",
    "href": "ai/pt/pt_tensors.html#importing-pytorch",
    "title": "PyTorch tensors",
    "section": "Importing PyTorch",
    "text": "Importing PyTorch\nFirst of all, we need to import the torch library:\n\nimport torch\n\nWe can check its version with:\n\ntorch.__version__\n\n'2.3.0+cu121'",
    "crumbs": [
      "AI",
      "<b><em>PyTorch</em></b>",
      "PyTorch tensors"
    ]
  },
  {
    "objectID": "ai/pt/pt_tensors.html#creating-tensors",
    "href": "ai/pt/pt_tensors.html#creating-tensors",
    "title": "PyTorch tensors",
    "section": "Creating tensors",
    "text": "Creating tensors\nThere are many ways to create tensors:\n\ntorch.tensor:   Input individual values\ntorch.arange:   1D tensor with a sequence of integers\ntorch.linspace:  1D linear scale tensor\ntorch.logspace:  1D log scale tensor\ntorch.rand:     Random numbers from a uniform distribution on [0, 1)\ntorch.randn:     Numbers from the standard normal distribution\ntorch.randperm:   Random permutation of integers\ntorch.empty:     Uninitialized tensor\ntorch.zeros:     Tensor filled with 0\ntorch.ones:     Tensor filled with 1\ntorch.eye:       Identity matrix\n\n\nFrom input values\n\nt = torch.tensor(3)\n\n\n\nYour turn:\n\nWithout using the shape descriptor, try to get the shape of the following tensors:\ntorch.tensor([0.9704, 0.1339, 0.4841])\n\ntorch.tensor([[0.9524, 0.0354],\n        [0.9833, 0.2562],\n        [0.0607, 0.6420]])\n\ntorch.tensor([[[0.4604, 0.2699],\n         [0.8360, 0.0317],\n         [0.3289, 0.1171]]])\n\ntorch.tensor([[[[0.0730, 0.8737],\n          [0.2305, 0.4719],\n          [0.0796, 0.2745]]],\n\n        [[[0.1534, 0.9442],\n          [0.3287, 0.9040],\n          [0.0948, 0.1480]]]])\n\nLet’s create a random tensor with a single element:\n\nt = torch.rand(1)\nt\n\ntensor([0.0664])\n\n\nWe can extract the value from a tensor with one element:\n\nt.item()\n\n0.06640791893005371\n\n\nAll these tensors have a single element, but an increasing number of dimensions:\n\ntorch.rand(1)\n\ntensor([0.4521])\n\n\n\ntorch.rand(1, 1)\n\ntensor([[0.4222]])\n\n\n\ntorch.rand(1, 1, 1)\n\ntensor([[[0.8421]]])\n\n\n\ntorch.rand(1, 1, 1, 1)\n\ntensor([[[[0.5642]]]])\n\n\n\nYou can tell the number of dimensions of a tensor easily by counting the number of opening square brackets.\n\n\ntorch.rand(1, 1, 1, 1).dim()\n\n4\n\n\nTensors can have multiple elements in one dimension:\n\ntorch.rand(6)\n\ntensor([0.4880, 0.4136, 0.6164, 0.7771, 0.3317, 0.9322])\n\n\n\ntorch.rand(6).dim()\n\n1\n\n\nAnd multiple elements in multiple dimensions:\n\ntorch.rand(2, 3, 4, 5)\n\ntensor([[[[1.9265e-01, 7.1588e-01, 5.4991e-02, 8.2984e-02, 4.7106e-01],\n          [1.4702e-01, 7.0770e-01, 3.7774e-01, 1.9632e-01, 3.7828e-01],\n          [5.3180e-01, 5.0883e-01, 8.8231e-01, 6.6615e-01, 8.9560e-01],\n          [2.1757e-01, 5.9166e-01, 9.3296e-01, 4.9402e-01, 7.4369e-01]],\n\n         [[5.6226e-01, 7.9807e-01, 8.5299e-01, 3.0352e-02, 5.7470e-01],\n          [6.9126e-01, 1.4833e-03, 1.0773e-01, 2.4625e-01, 3.3941e-01],\n          [1.1600e-01, 9.9698e-01, 4.1395e-01, 8.2424e-01, 5.0606e-01],\n          [9.3411e-01, 4.9257e-01, 7.2200e-01, 3.5606e-01, 6.8473e-01]],\n\n         [[6.3870e-01, 8.4146e-01, 1.4000e-02, 4.7660e-01, 2.5765e-01],\n          [3.9077e-01, 7.6622e-02, 5.0639e-01, 3.7614e-02, 3.4253e-02],\n          [2.3641e-01, 6.4974e-01, 7.0924e-01, 7.3478e-01, 6.9183e-01],\n          [5.5115e-01, 5.7502e-01, 8.1053e-01, 6.5448e-01, 7.6442e-01]]],\n\n\n        [[[6.6645e-01, 5.6170e-01, 5.5790e-01, 5.9724e-01, 6.7921e-01],\n          [5.9885e-01, 6.0820e-01, 5.0443e-02, 1.2864e-01, 3.9098e-01],\n          [8.1274e-01, 7.8897e-01, 4.7621e-01, 8.8376e-02, 2.0044e-01],\n          [5.5256e-01, 2.6450e-01, 1.5427e-01, 2.6887e-01, 2.2558e-01]],\n\n         [[4.1520e-01, 9.7462e-01, 7.5100e-01, 9.9890e-01, 6.8974e-01],\n          [2.3860e-01, 6.1438e-01, 3.9230e-01, 7.8527e-01, 5.9984e-01],\n          [5.7508e-01, 7.9849e-02, 8.4372e-01, 1.5977e-01, 1.0906e-01],\n          [1.7758e-01, 8.3926e-01, 9.9416e-01, 8.6307e-01, 8.6240e-01]],\n\n         [[4.6696e-01, 8.9729e-01, 9.9784e-01, 8.6357e-01, 2.0131e-01],\n          [3.9958e-01, 5.5251e-01, 5.1938e-01, 5.3351e-01, 2.3864e-01],\n          [9.4331e-01, 8.3029e-05, 6.8900e-01, 5.0304e-01, 1.3088e-01],\n          [6.5368e-01, 9.8662e-01, 7.8843e-01, 4.3189e-01, 9.8437e-01]]]])\n\n\n\ntorch.rand(2, 3, 4, 5).dim()\n\n4\n\n\n\ntorch.rand(2, 3, 4, 5).numel()\n\n120\n\n\n\ntorch.ones(2, 4)\n\ntensor([[1., 1., 1., 1.],\n        [1., 1., 1., 1.]])\n\n\n\nt = torch.rand(2, 3)\ntorch.zeros_like(t)             # Matches the size of t\n\ntensor([[0., 0., 0.],\n        [0., 0., 0.]])\n\n\n\ntorch.ones_like(t)\n\ntensor([[1., 1., 1.],\n        [1., 1., 1.]])\n\n\n\ntorch.randn_like(t)\n\ntensor([[-1.6889, -1.4382,  1.1412],\n        [ 1.3235, -1.4399, -0.5927]])\n\n\n\ntorch.arange(2, 10, 3)    # From 2 to 10 in increments of 3\n\ntensor([2, 5, 8])\n\n\n\ntorch.linspace(2, 10, 3)  # 3 elements from 2 to 10 on the linear scale\n\ntensor([ 2.,  6., 10.])\n\n\n\ntorch.logspace(2, 10, 3)  # Same on the log scale\n\ntensor([1.0000e+02, 1.0000e+06, 1.0000e+10])\n\n\n\ntorch.randperm(3)\n\ntensor([0, 1, 2])\n\n\n\ntorch.eye(3)\n\ntensor([[1., 0., 0.],\n        [0., 1., 0.],\n        [0., 0., 1.]])",
    "crumbs": [
      "AI",
      "<b><em>PyTorch</em></b>",
      "PyTorch tensors"
    ]
  },
  {
    "objectID": "ai/pt/pt_tensors.html#conversion-tofrom-numpy",
    "href": "ai/pt/pt_tensors.html#conversion-tofrom-numpy",
    "title": "PyTorch tensors",
    "section": "Conversion to/from NumPy",
    "text": "Conversion to/from NumPy\nPyTorch tensors can be converted to NumPy ndarrays and vice-versa in a very efficient manner as both objects share the same memory.\n\nFrom PyTorch tensor to NumPy ndarray\n\nt = torch.rand(2, 3)\nt\n\ntensor([[0.7550, 0.4205, 0.3024],\n        [0.9266, 0.8816, 0.9083]])\n\n\n\nt_np = t.numpy()\nt_np\n\narray([[0.7550101 , 0.42052966, 0.30236405],\n       [0.92664444, 0.88160074, 0.90829116]], dtype=float32)\n\n\n\n\nFrom NumPy ndarray to PyTorch tensor\n\nimport numpy as np\na = np.random.rand(2, 3)\na\n\narray([[0.08169611, 0.69920343, 0.83400031],\n       [0.40020636, 0.99345611, 0.94510268]])\n\n\n\na_pt = torch.from_numpy(a)\na_pt\n\ntensor([[0.0817, 0.6992, 0.8340],\n        [0.4002, 0.9935, 0.9451]], dtype=torch.float64)\n\n\n\nNote the different default data types.",
    "crumbs": [
      "AI",
      "<b><em>PyTorch</em></b>",
      "PyTorch tensors"
    ]
  },
  {
    "objectID": "ai/pt/pt_tensors.html#indexing-tensors",
    "href": "ai/pt/pt_tensors.html#indexing-tensors",
    "title": "PyTorch tensors",
    "section": "Indexing tensors",
    "text": "Indexing tensors\n\nt = torch.rand(3, 4)\nt\n\ntensor([[0.3933, 0.6787, 0.4420, 0.1485],\n        [0.1954, 0.8715, 0.7792, 0.6891],\n        [0.0908, 0.3443, 0.7069, 0.0127]])\n\n\n\nt[:, 2]\n\ntensor([0.4420, 0.7792, 0.7069])\n\n\n\nt[1, :]\n\ntensor([0.1954, 0.8715, 0.7792, 0.6891])\n\n\n\nt[2, 3]\n\ntensor(0.0127)\n\n\n\nA word of caution about indexing\nWhile indexing elements of a tensor to extract some of the data as a final step of some computation is fine, you should not use indexing to run operations on tensor elements in a loop as this would be extremely inefficient.\nInstead, you want to use vectorized operations.",
    "crumbs": [
      "AI",
      "<b><em>PyTorch</em></b>",
      "PyTorch tensors"
    ]
  },
  {
    "objectID": "ai/pt/pt_tensors.html#vectorized-operations",
    "href": "ai/pt/pt_tensors.html#vectorized-operations",
    "title": "PyTorch tensors",
    "section": "Vectorized operations",
    "text": "Vectorized operations\nSince PyTorch tensors are homogeneous (i.e. made of a single data type), as with NumPy’s ndarrays, operations are vectorized and thus fast.\nNumPy is mostly written in C, PyTorch in C++. With either library, when you run vectorized operations on arrays/tensors, you don’t use raw Python (slow) but compiled C/C++ code (much faster).\nHere is an excellent post explaining Python vectorization & why it makes such a big difference.",
    "crumbs": [
      "AI",
      "<b><em>PyTorch</em></b>",
      "PyTorch tensors"
    ]
  },
  {
    "objectID": "ai/pt/pt_tensors.html#data-types",
    "href": "ai/pt/pt_tensors.html#data-types",
    "title": "PyTorch tensors",
    "section": "Data types",
    "text": "Data types\n\nDefault data type\nSince PyTorch tensors were built with efficiency in mind for neural networks, the default data type is 32-bit floating points.\nThis is sufficient for accuracy and much faster than 64-bit floating points.\n\nBy contrast, NumPy ndarrays use 64-bit as their default.\n\n\nt = torch.rand(2, 4)\nt.dtype\n\ntorch.float32\n\n\n\n\nSetting data type at creation\nThe type can be set with the dtype argument:\n\nt = torch.rand(2, 4, dtype=torch.float64)\nt\n\ntensor([[0.7931, 0.0869, 0.0231, 0.6726],\n        [0.1689, 0.2116, 0.7150, 0.2311]], dtype=torch.float64)\n\n\n\nPrinted tensors display attributes with values ≠ default values.\n\n\nt.dtype\n\ntorch.float64\n\n\n\n\nChanging data type\n\nt = torch.rand(2, 4)\nt.dtype\n\ntorch.float32\n\n\n\nt2 = t.type(torch.float64)\nt2.dtype\n\ntorch.float64\n\n\n\n\nList of data types\n\n\n\n\n\n\n\ndtype\nDescription\n\n\n\n\ntorch.float16 / torch.half\n16-bit / half-precision floating-point\n\n\ntorch.float32 / torch.float\n32-bit / single-precision floating-point\n\n\ntorch.float64 / torch.double\n64-bit / double-precision floating-point\n\n\ntorch.uint8\nunsigned 8-bit integers\n\n\ntorch.int8\nsigned 8-bit integers\n\n\ntorch.int16 / torch.short\nsigned 16-bit integers\n\n\ntorch.int32 / torch.int\nsigned 32-bit integers\n\n\ntorch.int64 / torch.long\nsigned 64-bit integers\n\n\ntorch.bool\nboolean",
    "crumbs": [
      "AI",
      "<b><em>PyTorch</em></b>",
      "PyTorch tensors"
    ]
  },
  {
    "objectID": "ai/pt/pt_tensors.html#simple-operations",
    "href": "ai/pt/pt_tensors.html#simple-operations",
    "title": "PyTorch tensors",
    "section": "Simple operations",
    "text": "Simple operations\n\nt1 = torch.tensor([[1, 2], [3, 4]])\nt1\n\ntensor([[1, 2],\n        [3, 4]])\n\n\n\nt2 = torch.tensor([[1, 1], [0, 0]])\nt2\n\ntensor([[1, 1],\n        [0, 0]])\n\n\nOperation performed between elements at corresponding locations:\n\nt1 + t2\n\ntensor([[2, 3],\n        [3, 4]])\n\n\nOperation applied to each element of the tensor:\n\nt1 + 1\n\ntensor([[2, 3],\n        [4, 5]])\n\n\n\nReduction\n\nt = torch.ones(2, 3, 4);\nt\n\ntensor([[[1., 1., 1., 1.],\n         [1., 1., 1., 1.],\n         [1., 1., 1., 1.]],\n\n        [[1., 1., 1., 1.],\n         [1., 1., 1., 1.],\n         [1., 1., 1., 1.]]])\n\n\n\nt.sum()   # Reduction over all entries\n\ntensor(24.)\n\n\n\nOther reduction functions (e.g. mean) behave the same way.\n\nReduction over a specific dimension:\n\nt.sum(0)\n\ntensor([[2., 2., 2., 2.],\n        [2., 2., 2., 2.],\n        [2., 2., 2., 2.]])\n\n\n\nt.sum(1)\n\ntensor([[3., 3., 3., 3.],\n        [3., 3., 3., 3.]])\n\n\n\nt.sum(2)\n\ntensor([[4., 4., 4.],\n        [4., 4., 4.]])\n\n\nReduction over multiple dimensions:\n\nt.sum((0, 1))\n\ntensor([6., 6., 6., 6.])\n\n\n\nt.sum((0, 2))\n\ntensor([8., 8., 8.])\n\n\n\nt.sum((1, 2))\n\ntensor([12., 12.])\n\n\n\n\nIn-place operations\nWith operators post-fixed with _:\n\nt1 = torch.tensor([1, 2])\nt1\n\ntensor([1, 2])\n\n\n\nt2 = torch.tensor([1, 1])\nt2\n\ntensor([1, 1])\n\n\n\nt1.add_(t2)\nt1\n\ntensor([2, 3])\n\n\n\nt1.zero_()\nt1\n\ntensor([0, 0])\n\n\n\nWhile reassignments will use new addresses in memory, in-place operations will use the same addresses.\n\n\n\nTensor views\nt = torch.tensor([[1, 2, 3], [4, 5, 6]]); print(t)\nt.size()\nt.view(6)\nt.view(3, 2)\nt.view(3, -1) # Same: with -1, the size is inferred from other dimensions\n\nNote the difference\n\nt1 = torch.tensor([[1, 2, 3], [4, 5, 6]])\nt1\n\ntensor([[1, 2, 3],\n        [4, 5, 6]])\n\n\n\nt2 = t1.t()\nt2\n\ntensor([[1, 4],\n        [2, 5],\n        [3, 6]])\n\n\n\nt3 = t1.view(3, 2)\nt3\n\ntensor([[1, 2],\n        [3, 4],\n        [5, 6]])\n\n\n\n\n\nLogical operations\n\nt1 = torch.randperm(5)\nt1\n\ntensor([1, 0, 2, 4, 3])\n\n\n\nt2 = torch.randperm(5)\nt2\n\ntensor([4, 1, 2, 0, 3])\n\n\nTest each element:\n\nt1 &gt; 3\n\ntensor([False, False, False,  True, False])\n\n\nTest corresponding pairs of elements:\n\nt1 &lt; t2\n\ntensor([ True,  True, False, False, False])",
    "crumbs": [
      "AI",
      "<b><em>PyTorch</em></b>",
      "PyTorch tensors"
    ]
  },
  {
    "objectID": "ai/pt/pt_tensors.html#device-attribute",
    "href": "ai/pt/pt_tensors.html#device-attribute",
    "title": "PyTorch tensors",
    "section": "Device attribute",
    "text": "Device attribute\nTensor data can be placed in the memory of various processor types:\n\nthe RAM of CPU,\nthe RAM of a GPU with CUDA support,\nthe RAM of a GPU with AMD’s ROCm support,\nthe RAM of an XLA device (e.g. Cloud TPU) with the torch_xla package.\n\nThe values for the device attributes are:\n\nCPU:  'cpu',\nGPU (CUDA & AMD’s ROCm):  'cuda',\nXLA:  xm.xla_device().\n\nThis last option requires to load the torch_xla package first:\nimport torch_xla\nimport torch_xla.core.xla_model as xm\n\nCreating a tensor on a specific device\nBy default, tensors are created on the CPU.\nYou can create a tensor on an accelerator by specifying the device attribute (our current training cluster does not have GPUs, so don’t run this on it):\nt_gpu = torch.rand(2, device='cuda')\n\n\nCopying a tensor to a specific device\nYou can also make copies of a tensor on other devices:\n# Make a copy of t on the GPU\nt_gpu = t.to(device='cuda')\nt_gpu = t.cuda()             # Alternative syntax\n\n# Make a copy of t_gpu on the CPU\nt = t_gpu.to(device='cpu')\nt = t_gpu.cpu()              # Alternative syntax\n\n\nMultiple GPUs\nIf you have multiple GPUs, you can optionally specify which one a tensor should be created on or copied to:\nt1 = torch.rand(2, device='cuda:0')  # Create a tensor on 1st GPU\nt2 = t1.to(device='cuda:0')          # Make a copy of t1 on 1st GPU\nt3 = t1.to(device='cuda:1')          # Make a copy of t1 on 2nd GPU\nOr the equivalent short forms:\nt2 = t1.cuda(0)\nt3 = t1.cuda(1)",
    "crumbs": [
      "AI",
      "<b><em>PyTorch</em></b>",
      "PyTorch tensors"
    ]
  },
  {
    "objectID": "ai/pt/pt_resources.html",
    "href": "ai/pt/pt_resources.html",
    "title": "Resources",
    "section": "",
    "text": "This section contains a list of general machine learning resources, resources specific to PyTorch, as well as resources for Python and fastai.",
    "crumbs": [
      "AI",
      "<b><em>PyTorch</em></b>",
      "Resources"
    ]
  },
  {
    "objectID": "ai/pt/pt_resources.html#machine-learning",
    "href": "ai/pt/pt_resources.html#machine-learning",
    "title": "Resources",
    "section": "Machine learning",
    "text": "Machine learning\nAlliance wiki ML page\n\nOpen-access preprints\nArxiv Sanity Preserver by Andrej Karpathy\nML papers in the computer science category on arXiv\nML papers in the stats category on arXiv\nDistill ML research online journal\n\n\nAdvice and sources\nAdvice and sources from ML research student\n\n\nGetting help\nStack Overflow [machine-learning] tag\nStack Overflow [deep-learning] tag\nStack Overflow [supervised-learning] tag\nStack Overflow [unsupervised-learning] tag\nStack Overflow [semisupervised-learning] tag\nStack Overflow [reinforcement-learning] tag\nStack Overflow [transfer-learning] tag\nStack Overflow [machine-learning-model] tag\nStack Overflow [learning-rate] tag\nStack Overflow [bayesian-deep-learning] tag\n\n\nFree introductory courses\ndeeplearning.ai\nfast.ai\nGoogle\n\n\nLists of open datasets\nbenchmarks.ai\nAIBench\nkaggle\nWikipedia",
    "crumbs": [
      "AI",
      "<b><em>PyTorch</em></b>",
      "Resources"
    ]
  },
  {
    "objectID": "ai/pt/pt_resources.html#pytorch",
    "href": "ai/pt/pt_resources.html#pytorch",
    "title": "Resources",
    "section": "PyTorch",
    "text": "PyTorch\nAlliance wiki PyTorch page\n\n\nDocumentation\nPyTorch website\nPyTorch documentation\nPyTorch tutorials\nPyTorch online courses\nPyTorch examples\n\n\nGetting help\nPyTorch Discourse forum\nStack Overflow [pytorch] tag\nStack Overflow [pytorch-dataloader] tag\nStack Overflow [pytorch-ignite] tag\n\n\nPre-trained models\nPyTorch Hub",
    "crumbs": [
      "AI",
      "<b><em>PyTorch</em></b>",
      "Resources"
    ]
  },
  {
    "objectID": "ai/pt/pt_resources.html#python",
    "href": "ai/pt/pt_resources.html#python",
    "title": "Resources",
    "section": "Python",
    "text": "Python\nAlliance wiki Python page\n\nIDE\nProject Jupyter\nList of IDEs with description\nComparison of IDEs\nEmacs Python IDE\n\n\nShell\nIPython\nbpython\nptpython\n\n\nGetting help\nStack Overflow [python] tag",
    "crumbs": [
      "AI",
      "<b><em>PyTorch</em></b>",
      "Resources"
    ]
  },
  {
    "objectID": "ai/pt/pt_resources.html#fastai",
    "href": "ai/pt/pt_resources.html#fastai",
    "title": "Resources",
    "section": "fastai",
    "text": "fastai\n\nDocumentation\nManual\nTutorials\nPeer-reviewed paper\n\n\nBook\nPaperback version\nFree MOOC version of part 1 of the book\nJupyter notebooks version of the book\n\n\nGetting help\nDiscourse forum",
    "crumbs": [
      "AI",
      "<b><em>PyTorch</em></b>",
      "Resources"
    ]
  },
  {
    "objectID": "ai/pt/pt_nn_slides.html#fully-connected-neural-networks",
    "href": "ai/pt/pt_nn_slides.html#fully-connected-neural-networks",
    "title": "NN vs biological neurons Types of NN",
    "section": "Fully connected neural networks",
    "text": "Fully connected neural networks\n\n\n\n\n\nFrom Glosser.ca, Wikipedia\n\n\n\nEach neuron receives inputs from every neuron of the previous layer and passes its output to every neuron of the next layer."
  },
  {
    "objectID": "ai/pt/pt_nn_slides.html#convolutional-neural-networks",
    "href": "ai/pt/pt_nn_slides.html#convolutional-neural-networks",
    "title": "NN vs biological neurons Types of NN",
    "section": "Convolutional neural networks",
    "text": "Convolutional neural networks\n\n\nFrom Programming Journeys by Rensu Theart\n\nConvolutional neural networks (CNN) are used for spatially structured data (e.g. images).\nImages have huge input sizes and would require a very large number of neurons in a fully connected neural net. In convolutional layers, neurons receive input from a subarea (called local receptive field) of the previous layer. This greatly reduces the number of parameters. Optionally, pooling (combining the outputs of neurons in a subarea) reduces the data dimensions."
  },
  {
    "objectID": "ai/pt/pt_nn_slides.html#recurrent-neural-networks",
    "href": "ai/pt/pt_nn_slides.html#recurrent-neural-networks",
    "title": "NN vs biological neurons Types of NN",
    "section": "Recurrent neural networks",
    "text": "Recurrent neural networks\n\n\nFrom fdeloche, Wikipedia\n\nRecurrent neural networks (RNN) such as Long Short-Term Memory (LSTM) are used for chain structured data (e.g. text).\nThey are not feedforward networks (i.e. networks for which the information moves only in the forward direction without any loop)."
  },
  {
    "objectID": "ai/pt/pt_nn_slides.html#transformers",
    "href": "ai/pt/pt_nn_slides.html#transformers",
    "title": "NN vs biological neurons Types of NN",
    "section": "Transformers",
    "text": "Transformers\nA combination of two RNNs (the encoder and the decoder) is used in sequence to sequence models for translation or picture captioning.\nIn 2014 the concept of attention (giving added weight to important words) was developed, greatly improving the ability of such models to process a lot of data.\nThe problem with recurrence is that it is not easily to parallelize (and thus to run fast on GPUs).\nIn 2017, a new model—the transformer—was proposed: by using only attention mechanisms and no recurrence, the transformer achieves better results in an easily parallelizable fashion.\nWith the addition of transfer learning, powerful transformers emerged in the field of NLP (e.g. Bidirectional Encoder Representations from Transformers (BERT) from Google and Generative Pre-trained Transformer-3 (GPT-3) from OpenAI)."
  },
  {
    "objectID": "ai/pt/pt_model.html",
    "href": "ai/pt/pt_model.html",
    "title": "Building a model",
    "section": "",
    "text": "Key to creating neural networks in PyTorch is the torch.nn package which contains the nn.Module and a forward method which returns an output from some input.\nLet’s build a neural network to classify the MNIST.\n\nFirst, we need to define the architecture of the network. There are many types of architectures. For images, CNN are well suited.\nIn Python, you can define a subclass of an existing class with:\nclass YourSubclass(BaseClass):\n    &lt;definition of your subclass&gt;        \nThe subclass is derived from the base class and inherits its properties. PyTorch contains the class torch.nn.Module which is used as the base class when defining a neural network.\n\n# Load packages\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Net(nn.Module):\n    # Define the architecture of the network\n    def __init__(self):\n        super(Net, self).__init__()\n        # 1 input image channel, 6 output channels,\n        # 5x5 square convolution kernel\n        self.conv1 = nn.Conv2d(1, 6, 5)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        # an affine operation: y = Wx + b\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 5*5 from image dimension\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    # Set the flow of data through the network for the forward pass\n    # x represents the data\n    def forward(self, x):\n        # Max pooling over a (2, 2) window\n        # F.relu is the rectified-linear activation function\n        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n        # If the size is a square, you can specify with a single number\n        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n        # flatten all dimensions except the batch dimension\n        x = torch.flatten(x, 1)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\nLet’s create an instance of Net and print its structure:\n\nnet = Net()\nprint(net)\n\nNet(\n  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n  (fc1): Linear(in_features=400, out_features=120, bias=True)\n  (fc2): Linear(in_features=120, out_features=84, bias=True)\n  (fc3): Linear(in_features=84, out_features=10, bias=True)\n)\n\n\n\nparams = list(net.parameters())\nprint(len(params))\nprint(params[0].size())  # conv1's .weight\n\n10\ntorch.Size([6, 1, 5, 5])"
  },
  {
    "objectID": "ai/pt/pt_intro_slides.html#ml-allows-to-achieve-previously-impossible-tasks",
    "href": "ai/pt/pt_intro_slides.html#ml-allows-to-achieve-previously-impossible-tasks",
    "title": "Introduction to machine learning",
    "section": "ML allows to achieve previously impossible tasks",
    "text": "ML allows to achieve previously impossible tasks\n\nLet’s take the example of image recognition:\n\nIn typical computing, a programmer writes code that gives a computer detailed instructions of what to do\nCoding all the possible ways—pixel by pixel—that an image can represent, say, a dog is an impossibly large task: there are many breeds of dogs, the image can be a picture, a blurred picture, a drawing, a cartoon, the dog can be in all sorts of positions, wearing clothes, etc\nThere just aren’t enough resources to make the traditional programming approach able to create a computer program that can identify a dog in images\nBy feeding a very large number of dog images to a neural network however, we can train that network to recognize dogs in images that it has never seen (without explicitly programming how it does this!)"
  },
  {
    "objectID": "ai/pt/pt_intro_slides.html#old-concept-new-computing-power",
    "href": "ai/pt/pt_intro_slides.html#old-concept-new-computing-power",
    "title": "Introduction to machine learning",
    "section": "Old concept … new computing power",
    "text": "Old concept … new computing power\nThe concept is everything but new: Arthur Samuel came up with it in 1949 and built a self-learning Checkers-playing program in 1959\n\n\nMachine learning consists of feeding vast amounts of data to algorithms to strengthen pathways, so the excitement for the approach became somewhat dormant due to the lack of computing power and the lack of training data at the time\nThe advent of powerful computers, GPUs, and massive amounts of data have brought the old concept to the forefront\n\n\n\n\n\n\nFrom xkcd.com"
  },
  {
    "objectID": "ai/pt/pt_intro_slides.html#supervised-learning",
    "href": "ai/pt/pt_intro_slides.html#supervised-learning",
    "title": "Introduction to machine learning",
    "section": "Supervised learning",
    "text": "Supervised learning\n\nRegression is a form of supervised learning with continuous outputs\nClassification is supervised learning with discrete outputs\n\nSupervised learning uses training data in the form of example input/output pairs\nGoal\nFind the relationship between inputs and outputs"
  },
  {
    "objectID": "ai/pt/pt_intro_slides.html#unsupervised-learning",
    "href": "ai/pt/pt_intro_slides.html#unsupervised-learning",
    "title": "Introduction to machine learning",
    "section": "Unsupervised learning",
    "text": "Unsupervised learning\nClustering, social network analysis, market segmentation, PCA … are all forms of unsupervised learning\nUnsupervised learning uses unlabelled data\nGoal\nFind structure within the data"
  },
  {
    "objectID": "ai/pt/pt_intro_slides.html#reinforcement-learning",
    "href": "ai/pt/pt_intro_slides.html#reinforcement-learning",
    "title": "Introduction to machine learning",
    "section": "Reinforcement learning",
    "text": "Reinforcement learning\nThe algorithm explores by performing random actions and these actions are rewarded or punished (bonus points or penalties)\nThis is how algorithms learn to play games"
  },
  {
    "objectID": "ai/pt/pt_intro_slides.html#decide-on-an-architecture",
    "href": "ai/pt/pt_intro_slides.html#decide-on-an-architecture",
    "title": "Introduction to machine learning",
    "section": "Decide on an architecture",
    "text": "Decide on an architecture\n\nThe architecture won’t change during training\nThe type of architecture you choose (e.g. CNN, Transformer) depends on the type of data you have (e.g. vision, textual). The depth and breadth of your network depend on the amount of data and computing resource you have"
  },
  {
    "objectID": "ai/pt/pt_intro_slides.html#set-some-initial-parameters",
    "href": "ai/pt/pt_intro_slides.html#set-some-initial-parameters",
    "title": "Introduction to machine learning",
    "section": "Set some initial parameters",
    "text": "Set some initial parameters\n\nYou can initialize them randomly or get much better ones through transfer learning\nWhile the parameters are also part of the model, those will change during training"
  },
  {
    "objectID": "ai/pt/pt_intro_slides.html#get-some-labelled-data",
    "href": "ai/pt/pt_intro_slides.html#get-some-labelled-data",
    "title": "Introduction to machine learning",
    "section": "Get some labelled data",
    "text": "Get some labelled data\n\nWhen we say that we need a lot of data for machine learning, we mean “lots of labelled data” as this is what gets used for training models"
  },
  {
    "objectID": "ai/pt/pt_intro_slides.html#make-sure-to-keep-some-data-for-testing",
    "href": "ai/pt/pt_intro_slides.html#make-sure-to-keep-some-data-for-testing",
    "title": "Introduction to machine learning",
    "section": "Make sure to keep some data for testing",
    "text": "Make sure to keep some data for testing\n\nThose data won’t be used for training the model. Often people keep around 20% of their data for testing"
  },
  {
    "objectID": "ai/pt/pt_intro_slides.html#pass-data-and-parameters-through-the-architecture",
    "href": "ai/pt/pt_intro_slides.html#pass-data-and-parameters-through-the-architecture",
    "title": "Introduction to machine learning",
    "section": "Pass data and parameters through the architecture",
    "text": "Pass data and parameters through the architecture\n\nThe train data are the inputs and the process of calculating the outputs is the forward pass"
  },
  {
    "objectID": "ai/pt/pt_intro_slides.html#the-outputs-of-the-model-are-predictions",
    "href": "ai/pt/pt_intro_slides.html#the-outputs-of-the-model-are-predictions",
    "title": "Introduction to machine learning",
    "section": "The outputs of the model are predictions",
    "text": "The outputs of the model are predictions"
  },
  {
    "objectID": "ai/pt/pt_intro_slides.html#compare-those-predictions-to-the-train-labels",
    "href": "ai/pt/pt_intro_slides.html#compare-those-predictions-to-the-train-labels",
    "title": "Introduction to machine learning",
    "section": "Compare those predictions to the train labels",
    "text": "Compare those predictions to the train labels\n\nSince our data was labelled, we know what the true outputs are"
  },
  {
    "objectID": "ai/pt/pt_intro_slides.html#calculate-train-loss",
    "href": "ai/pt/pt_intro_slides.html#calculate-train-loss",
    "title": "Introduction to machine learning",
    "section": "Calculate train loss",
    "text": "Calculate train loss\n\nThe deviation of our predictions from the true outputs gives us a measure of training loss"
  },
  {
    "objectID": "ai/pt/pt_intro_slides.html#adjust-parameters",
    "href": "ai/pt/pt_intro_slides.html#adjust-parameters",
    "title": "Introduction to machine learning",
    "section": "Adjust parameters",
    "text": "Adjust parameters\n\nThe parameters get automatically adjusted to reduce the training loss through the mechanism of backpropagation. This is the actual training part\nThis process is repeated many times. Training models is pretty much a giant for loop"
  },
  {
    "objectID": "ai/pt/pt_intro_slides.html#from-model-to-program",
    "href": "ai/pt/pt_intro_slides.html#from-model-to-program",
    "title": "Introduction to machine learning",
    "section": "From model to program",
    "text": "From model to program\n\nRemember that the model architecture is fixed, but that the parameters change at each iteration of the training process"
  },
  {
    "objectID": "ai/pt/pt_intro_slides.html#evaluate-the-model",
    "href": "ai/pt/pt_intro_slides.html#evaluate-the-model",
    "title": "Introduction to machine learning",
    "section": "Evaluate the model",
    "text": "Evaluate the model\n\nWe can now use the testing set (which was never used to train the model) to evaluate our model: if we pass the test inputs through our program, we get some predictions that we can compare to the test labels (which are the true outputs)\nThis gives us the test loss: a measure of how well our model performs"
  },
  {
    "objectID": "ai/pt/pt_intro_slides.html#use-the-model",
    "href": "ai/pt/pt_intro_slides.html#use-the-model",
    "title": "Introduction to machine learning",
    "section": "Use the model",
    "text": "Use the model\n\nNow that we have a program, we can use it on unlabelled inputs to get what people ultimately want: unknown outputs\nThis is when we put our model to actual use to solve some problem\n\n\n\n\n Back to the course"
  },
  {
    "objectID": "ai/pt/pt_hpc.html",
    "href": "ai/pt/pt_hpc.html",
    "title": "Deep learning on production clusters",
    "section": "",
    "text": "This section is a summary of relevant information while using Python in an HPC context for deep learning.",
    "crumbs": [
      "AI",
      "<b><em>PyTorch</em></b>",
      "DL on production clusters"
    ]
  },
  {
    "objectID": "ai/pt/pt_hpc.html#run-code-in-a-job",
    "href": "ai/pt/pt_hpc.html#run-code-in-a-job",
    "title": "Deep learning on production clusters",
    "section": "Run code in a job",
    "text": "Run code in a job\nWhen you ssh into one of the Alliance clusters, you log into the login node.\nEverybody using a cluster uses that node to enter the cluster. Do not run anything computationally intensive on this node or you would make the entire cluster very slow for everyone. To run your code, you need to start an interactive job or submit a batch job to Slurm (the job scheduler used by the Alliance clusters).",
    "crumbs": [
      "AI",
      "<b><em>PyTorch</em></b>",
      "DL on production clusters"
    ]
  },
  {
    "objectID": "ai/pt/pt_hpc.html#plots",
    "href": "ai/pt/pt_hpc.html#plots",
    "title": "Deep learning on production clusters",
    "section": "Plots",
    "text": "Plots\nDo not run code that displays plots on screen. Instead, have them written to files.",
    "crumbs": [
      "AI",
      "<b><em>PyTorch</em></b>",
      "DL on production clusters"
    ]
  },
  {
    "objectID": "ai/pt/pt_hpc.html#data",
    "href": "ai/pt/pt_hpc.html#data",
    "title": "Deep learning on production clusters",
    "section": "Data",
    "text": "Data\n\nCopy files to/from the cluster\n\nFew files\nIf you need to copy files to or from the cluster, you can use scp from your local machine.\n\nCopy file from your computer to the cluster\n[local]$ scp &lt;/local/path/to/file&gt; &lt;user&gt;@&lt;hostname&gt;:&lt;path/in/cluster&gt;\n\nExpressions between the &lt; and &gt; signs need to be replaced by the relevant information (without those signs).\n\n\n\nCopy file from the cluster to your computer\n[local]$ scp &lt;user&gt;@&lt;hostname&gt;:&lt;cluster/path/to/file&gt; &lt;/local/path&gt;\n\n\n\nLarge amount of data\nUse Globus for large data transfers.\n\nThe Alliance is starting to store classic ML datasets on its clusters. So if your research uses a common dataset, it may be worth inquiring whether it might be available before downloading a copy.\n\n\n\n\nLarge collections of files\nThe Alliance clusters are optimized for very large files and are slowed by large collections of small files. Datasets with many small files need to be turned into single-file archives with tar. Failing to do so will affect performance not just for you, but for all users of the cluster.\n$ tar cf &lt;data&gt;.tar &lt;path/to/dataset/directory&gt;/*\n\n\nIf you want to also compress the files, replace tar cf with tar czf\nAs a modern alternative to tar, you can use Dar",
    "crumbs": [
      "AI",
      "<b><em>PyTorch</em></b>",
      "DL on production clusters"
    ]
  },
  {
    "objectID": "ai/pt/pt_hpc.html#interactive-jobs",
    "href": "ai/pt/pt_hpc.html#interactive-jobs",
    "title": "Deep learning on production clusters",
    "section": "Interactive jobs",
    "text": "Interactive jobs\nInteractive jobs are useful for code testing and development. They are not however the most efficient way to run code, so you should limit their use to testing and development.\nYou start an interactive job with:\n$ salloc --account=def-&lt;account&gt; --cpus-per-task=&lt;n&gt; --gres=gpu:&lt;n&gt; --mem=&lt;mem&gt; --time=&lt;time&gt;\nOur training cluster does not have GPUs, so for this workshop, do not use the --gres=gpu:&lt;n&gt; option.\nFor the workshop, you also don’t have to worry about the --account=def-&lt;account&gt; option (or, if you want, you can use --account=def-sponsor00).\nOur training cluster has a total of 60 CPUs on 5 compute nodes. Since there are many of you in this workshop, please be very mindful when running interactive jobs: if you request a lot of CPUs for a long time, the other workshop attendees won’t be able to use the cluster anymore until your interactive job requested time ends (even if you aren’t running any code).\nHere are my suggestions so that we don’t run into this problem:\n\nOnly start interactive jobs when you need to understand what Python is doing at every step, or to test, explore, and develop code (so where an interactive Python shell is really beneficial). Once you have a model, submit a batch job to Slurm instead\nWhen running interactive jobs on this training cluster, only request 1 CPU (so --cpus-per-task=1)\nOnly request the time that you will really use (e.g. for the lesson on Python tensors, maybe 30 min to 1 hour seems reasonable)\nIf you don’t need your job allocation anymore before it runs out, you can relinquish it with Ctrl+d\n\n\nBe aware that, on Cedar, you are not allowed to submit jobs from ~/home. Instead, you have to submit jobs from ~/scratch or ~/project.",
    "crumbs": [
      "AI",
      "<b><em>PyTorch</em></b>",
      "DL on production clusters"
    ]
  },
  {
    "objectID": "ai/pt/pt_hpc.html#batch-jobs",
    "href": "ai/pt/pt_hpc.html#batch-jobs",
    "title": "Deep learning on production clusters",
    "section": "Batch jobs",
    "text": "Batch jobs\nAs soon as you have a working Python script, you want to submit a batch job instead of running an interactive job. To do that, you need to write an sbatch script.\n\nJob script\n\nHere is an example script:\n\n#!/bin/bash\n#SBATCH --job-name=&lt;name&gt;*            # job name\n#SBATCH --account=def-&lt;account&gt;\n#SBATCH --time=&lt;time&gt;                 # max walltime in D-HH:MM or HH:MM:SS\n#SBATCH --cpus-per-task=&lt;number&gt;      # number of cores\n#SBATCH --gres=gpu:&lt;type&gt;:&lt;number&gt;    # type and number of GPU(s) per node\n#SBATCH --mem=&lt;mem&gt;                   # max memory (default unit is MB) per node\n#SBATCH --output=%x_%j.out*           # file name for the output\n#SBATCH --error=%x_%j.err*            # file name for errors\n#SBATCH --mail-user=&lt;email_address&gt;*\n#SBATCH --mail-type=ALL*\n\n# Load modules\n# (Do not use this in our workshop since we aren't using GPUs)\n# (Note: loading the Python module is not necessary\n# when you activate a Python virtual environment)\n# module load cudacore/.10.1.243 cuda/10 cudnn/7.6.5\n\n# Create a variable with the directory for your ML project\nSOURCEDIR=~/&lt;path/project/dir&gt;\n\n# Activate your Python virtual environment\nsource ~/env/bin/activate\n\n# Transfer and extract data to a compute node\nmkdir $SLURM_TMPDIR/data\ntar xf ~/projects/def-&lt;user&gt;/&lt;data&gt;.tar -C $SLURM_TMPDIR/data\n\n# Run your Python script on the data\npython $SOURCEDIR/&lt;script&gt;.py $SLURM_TMPDIR/data\n\n\n%x will get replaced by the script name and %j by the job number\nIf you compressed your data with tar czf, you need to extract it with tar xzf\nSBATCH options marked with a * are optional\nThere are various other options for email notifications\n\n\nYou may wonder why we transferred data to a compute node. This makes any I/O operation involving your data a lot faster, so it will speed up your code. Here is how this works:\nFirst, we create a temporary data directory in $SLURM_TMPDIR:\n$ mkdir $SLURM_TMPDIR/data\n\nThe variable $SLURM_TMPDIR is created by Slurm on the compute node where a job is running. Its path is /localscratch/&lt;user&gt;.&lt;jobid&gt;.0. Anything in it gets deleted when the job is done.\n\nThen we extract the data into it:\n$ tar xf ~/projects/def-&lt;user&gt;/&lt;data&gt;.tar -C $SLURM_TMPDIR/data\nIf your data is not in a tar file, you can simply copy it to the compute node running your job:\n$ cp -r ~/projects/def-&lt;user&gt;/&lt;data&gt; $SLURM_TMPDIR/data\n\n\nJob handling\n\nSubmit a job\n$ cd &lt;/dir/containing/job&gt;\n$ sbatch &lt;jobscript&gt;.sh\n\n\nCheck the status of your job(s)\n$ sq\n\nPD = pending\nR = running\nCG = completing (Slurm is doing the closing processes)\nNo information = your job has finished running\n\n\n\nCancel a job\n$ scancel &lt;jobid&gt;\n\n\nDisplay efficiency measures of a completed job\n$ seff &lt;jobid&gt;",
    "crumbs": [
      "AI",
      "<b><em>PyTorch</em></b>",
      "DL on production clusters"
    ]
  },
  {
    "objectID": "ai/pt/pt_hpc.html#gpus",
    "href": "ai/pt/pt_hpc.html#gpus",
    "title": "Deep learning on production clusters",
    "section": "GPU(s)",
    "text": "GPU(s)\n\nGPU types\nSeveral Alliance clusters have GPUs. Their numbers and types differ:\n From the Alliance Wiki\nThe default is 12G P100, but you can request another type with SBATCH --gres=gpu:&lt;type&gt;:&lt;number&gt; (example: --gres=gpu:p100l:1 to request a 16G P100 on Cedar). Please refer to the Alliance Wiki for more details.\n\n\nNumber of GPU(s)\nTry running your model on a single GPU first.\nIt is very likely that you do not need more than one GPU. Asking for more than you need will greatly increase your waiting time until your job is run. The lesson on distributed computing with PyTorch gives a few information as to when you might benefit from using several GPUs and provides some links to more resources. We will also offer workshops on distributed ML in the future. In any event, you should test your model before asking for several GPUs.\n\n\nCPU/GPU ratio\nHere are the Alliance recommendations:\nBéluga:\nNo more than 10 CPU per GPU.\nCedar:\nP100 GPU: no more than 6 CPU per GPU.\nV100 GPU: no more than 8 CPU per GPU.\nGraham:\nNo more than 16 CPU per GPU.",
    "crumbs": [
      "AI",
      "<b><em>PyTorch</em></b>",
      "DL on production clusters"
    ]
  },
  {
    "objectID": "ai/pt/pt_hpc.html#code-testing",
    "href": "ai/pt/pt_hpc.html#code-testing",
    "title": "Deep learning on production clusters",
    "section": "Code testing",
    "text": "Code testing\nIt might be wise to test your code in an interactive job before submitting a really big batch job to Slurm.\n\nActivate your Python virtual environment\n$ source ~/env/bin/activate\n\n\nStart an interactive job\n\nExample:\n\n$ salloc --account=def-&lt;account&gt; --gres=gpu:1 --cpus-per-task=6 --mem=32000 --time=0:30:0\n\n\nPrepare the data\nCreate a temporary data directory in $SLURM_TMPDIR:\n(env) $ mkdir $SLURM_TMPDIR/data\n\nThe variable $SLURM_TMPDIR is created by Slurm on the compute node where a job is running. Its path is /localscratch/&lt;user&gt;.&lt;jobid&gt;.0. Anything in it gets deleted when the job is done.\n\nExtract the data into it:\n(env) $ tar xf ~/projects/def-&lt;user&gt;/&lt;data&gt;.tar -C $SLURM_TMPDIR/data\n\n\nTry to run your code\nPlay in Python to test your code:\n(env) $ python\n&gt;&gt;&gt; import torch\n&gt;&gt;&gt; ...\nTo exit the virtual environment, run:\n(env) $ deactivate",
    "crumbs": [
      "AI",
      "<b><em>PyTorch</em></b>",
      "DL on production clusters"
    ]
  },
  {
    "objectID": "ai/pt/pt_hpc.html#checkpoints",
    "href": "ai/pt/pt_hpc.html#checkpoints",
    "title": "Deep learning on production clusters",
    "section": "Checkpoints",
    "text": "Checkpoints\nLong jobs should have a checkpoint at least every 24 hours. This ensures that an outage won’t lead to days of computation lost and it will help get the job started by the scheduler sooner.\nFor instance, you might want to have checkpoints every n epochs (choose n so that n epochs take less than 24 hours to run).\nIn PyTorch, you can create dictionaries with all the information necessary and save them as .tar files with torch.save(). You can then load them back with torch.load().\nThe information you want to save in each checkpoint includes the model’s state_dict, the optimizer’s state_dict, the epoch at which you stopped, the latest training loss, and anything else needed to restart training where you left off.\n\nFor example, saving a checkpoint during training could look something like this:\n\ntorch.save({\n    'epoch': &lt;last epoch run&gt;,\n    'model_state_dict': net.state_dict(),\n    'optimizer_state_dict': optimizer.state_dict(),\n    'loss': &lt;latest loss&gt;,\n}, &lt;path/to/checkpoint-file.tar&gt;)\n\nTo restart, initialize the model and optimizer, load the dictionary, and resume training:\n\n# Initialize the model and optimizer\nmodel = &lt;your model&gt;\noptimizer = &lt;your optimizer&gt;\n\n# Load the dictionary\ncheckpoint = torch.load(&lt;path/to/checkpoint-file.tar&gt;)\nmodel.load_state_dict(checkpoint['model_state_dict'])\noptimizer.load_state_dict(checkpoint['optimizer_state_dict'])\nepoch = checkpoint['epoch']\nloss = checkpoint['loss']\n\n# Resume training\nmodel.train()",
    "crumbs": [
      "AI",
      "<b><em>PyTorch</em></b>",
      "DL on production clusters"
    ]
  },
  {
    "objectID": "ai/pt/pt_hpc.html#tensorboard-on-the-cluster",
    "href": "ai/pt/pt_hpc.html#tensorboard-on-the-cluster",
    "title": "Deep learning on production clusters",
    "section": "TensorBoard on the cluster",
    "text": "TensorBoard on the cluster\nTensorBoard allows to visually track your model metrics (e.g. loss, accuracy, model graph, etc.). It requires a lot of processing power however, so if you want to use it on an Alliance cluster, do not run it from the login node. Instead, run it as part of your job. This section guides you through the whole workflow.\n\nLaunch TensorBoard\nFirst, you need to launch TensorBoard in the background (with a trailing &) before running your Python script. To do so, ad to your sbatch script:\ntensorboard --logdir=/tmp/&lt;your log dir&gt; --host 0.0.0.0 &\n\nExample:\n\n#!/bin/bash\n#SBATCH ...\n...\n\ntensorboard --logdir=/tmp/&lt;your log dir&gt; --host 0.0.0.0 &\npython $SOURCEDIR/&lt;script&gt;.py $SLURM_TMPDIR/data\n\n\nCreate a connection between the compute node and your computer\nOnce the job is running, you need to create a connection between the compute node running TensorBoard and your computer.\nFirst, you need to find the hostname of the compute node running the Tensorboard server. This is the value under NODELIST for your job when you run:\n$ sq\nThen, from your computer, enter this ssh command:\n[local]$ ssh -N -f -L localhost:6006:&lt;node hostname&gt;:6006 &lt;user&gt;@&lt;cluster&gt;.computecanada.ca\n\nReplace &lt;node hostname&gt; by the compute node hostname you just identified, &lt;user&gt; by your user name, and &lt;cluster&gt; by the name of the Alliance cluster hostname—e.g. beluga, cedar, graham.\n\n\n\nAccess TensorBoard\nYou can now open a browser (on your computer) and go to http://localhost:6006 to monitor your model running on a compute node in the cluster!",
    "crumbs": [
      "AI",
      "<b><em>PyTorch</em></b>",
      "DL on production clusters"
    ]
  },
  {
    "objectID": "ai/pt/pt_hpc.html#running-several-similar-jobs",
    "href": "ai/pt/pt_hpc.html#running-several-similar-jobs",
    "title": "Deep learning on production clusters",
    "section": "Running several similar jobs",
    "text": "Running several similar jobs\nA number of ML tasks (e.g. hyperparameter optimization) require running several instances of similar jobs. Grouping them into a single job with GLOST or GNU Parallel reduces the stress on the scheduler.",
    "crumbs": [
      "AI",
      "<b><em>PyTorch</em></b>",
      "DL on production clusters"
    ]
  },
  {
    "objectID": "ai/pt/pt_data.html",
    "href": "ai/pt/pt_data.html",
    "title": "Loading data",
    "section": "",
    "text": "import torch\nimport torchvision\nimport torchvision.transforms as transforms\ntransform = transforms.Compose(\n    [transforms.ToTensor(),\n     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n\nbatch_size = 4\n\ntrainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n                                        download=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n                                          shuffle=True, num_workers=2)\n\ntestset = torchvision.datasets.CIFAR10(root='./data', train=False,\n                                       download=True, transform=transform)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n                                         shuffle=False, num_workers=2)\n\nclasses = ('plane', 'car', 'bird', 'cat',\n           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n\nFiles already downloaded and verified\nFiles already downloaded and verified\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# functions to show an image\n\n\ndef imshow(img):\n    img = img / 2 + 0.5     # unnormalize\n    npimg = img.numpy()\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.show()\n\n\n# get some random training images\ndataiter = iter(trainloader)\nimages, labels = next(dataiter)\n\n# show images\nimshow(torchvision.utils.make_grid(images))\n# print labels\nprint(' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))\n\n\n\n\n\n\n\n\ncat   truck cat   deer"
  },
  {
    "objectID": "ai/pt/pt_data.html#creating-a-dataloader",
    "href": "ai/pt/pt_data.html#creating-a-dataloader",
    "title": "Loading data",
    "section": "Creating a DataLoader",
    "text": "Creating a DataLoader\nA DataLoader is an iterable feeding data to a model. When we train a model, we run it for each element of the DataLoader in a for loop:\nfor i in data_loader:\n    &lt;some model&gt;"
  },
  {
    "objectID": "ai/pt/pt_choosing_frameworks.html",
    "href": "ai/pt/pt_choosing_frameworks.html",
    "title": "Which framework to choose?",
    "section": "",
    "text": "With the growing popularity of machine learning, many frameworks have appeared in various languages. One of the questions you might be facing is: which tool should I choose?\nThe main focus here is on the downsides of proprietary tools.",
    "crumbs": [
      "AI",
      "<b><em>PyTorch</em></b>",
      "Which framework to choose?"
    ]
  },
  {
    "objectID": "ai/pt/pt_choosing_frameworks.html#points-worth-considering",
    "href": "ai/pt/pt_choosing_frameworks.html#points-worth-considering",
    "title": "Which framework to choose?",
    "section": "Points worth considering",
    "text": "Points worth considering\nThere are a several points you might want to consider in making that choice. For instance, what tools do people use in your field? (what tools are used in the papers you read?) What tools are your colleagues and collaborators using?\nLooking a bit further into the future, whether you are considering staying in academia or working in industry could also influence your choice. If the former, paying attention to the literature is key, if the latter, it may be good to have a look at the trends in job postings.\nSome frameworks offer collections of already-made toolkits. They are thus easy to start using and do not require a lot of programming experience. On the other hand, they may feel like black boxes and they are not very customizable. Scikit-learn and Keras—usually run on top of TensorFlow—fall in that category. Lower level tools allow you full control and tuning of your models, but can come with a steeper learning curve.\nPyTorch, developed by Facebook’s AI Research lab, has seen a huge increase in popularity in research in recent years due to its highly pythonic syntax, very convenient tensors, just-in-time (JIT) compilation, dynamic computation graphs, and because it is free and open-source.\nSeveral libraries are now adding a higher level on top of PyTorch: fastai, which we will use in this course, PyTorch Lightning, and PyTorch Ignite. fastai, in addition to the convenience of being able to write a model in a few lines of code, allows to dive as low as you choose into the PyTorch code, thus making it unconstrained by the optional ease of use. It also adds countless functionality. The downside of using this added layer is that it can make it less straightforward to install on a machine or to tweak and customize.\nThe most popular machine learning library currently remains TensorFlow, developed by the Google Brain Team. While it has a Python API, its syntax can be more obscure.\nJulia’s syntax is well suited for the implementation of mathematical models, GPU kernels can be written directly in Julia, and Julia’s speed is attractive in computation hungry fields. So Julia has also seen the development of many ML packages such as Flux or Knet. The user base of Julia remains quite small however.\nMy main motivation in writing this section however is to raise awareness about one question that should really be considered: whether the tool you decide to learn and use in your research is open-source or proprietary.",
    "crumbs": [
      "AI",
      "<b><em>PyTorch</em></b>",
      "Which framework to choose?"
    ]
  },
  {
    "objectID": "ai/pt/pt_choosing_frameworks.html#proprietary-tools-a-word-of-caution",
    "href": "ai/pt/pt_choosing_frameworks.html#proprietary-tools-a-word-of-caution",
    "title": "Which framework to choose?",
    "section": "Proprietary tools: a word of caution",
    "text": "Proprietary tools: a word of caution\nAs a student, it is tempting to have the following perspective:\n\nMy university pays for this very expensive license. I have free access to this very expensive tool. It would be foolish not to make use of it while I can!\n\nWhen there are no equivalent or better open-source tools, that might be true. But when superior open-source tools exist, these university licenses are more of a trap than a gift.\nHere are some of the reasons you should be wary of proprietary tools:\n\nResearchers who do not have access to the tool cannot reproduce your methods\nLarge Canadian universities may offer a license for the tool, but grad students in other countries, independent researchers, researchers in small organizations, etc. may not have access to a free license (or tens of thousands of dollars to pay for it).\n\n\nOnce you graduate, you may not have access to the tool anymore\nOnce you leave your Canadian institution, you may become one of those researchers who do not have access to that tool. This means that you will not be able to re-run your thesis analyses, re-use your methods, and apply the skills you learnt. The time you spent learning that expensive tool you could play with for free may feel a lot less like a gift then.\n\n\nYour university may stop paying for a license\nAs commercial tools fall behind successful open-source ones, some universities may decide to stop purchasing a license. It happened during my years at SFU with an expensive and clunky citation manager which, after having been promoted for years by the library through countless workshops, got abandoned in favour of a much better free and open-source one.\n\n\nYou may get locked-in\nProprietary tools often come with proprietary formats and, depending on the tool, it may be painful (or impossible) to convert your work to another format. When that happens, you are locked-in.\n\n\nProprietary tools are often black boxes\nIt is often impossible to see the source code of proprietary software.\n\n\nLong-term access\nIt is often very difficult to have access to old versions of proprietary tools (and this can be necessary to reproduce old studies). When companies disappear, the tools they produced usually disappear with them. open-source tools, particularly those who have their code under version control in repositories such as GitHub, remain fully accessible (including all stages of development), and if they get abandoned, their maintenance can be taken over or restarted by others.\n\n\nThe licenses you have access to may be limiting and a cause of headache\nFor instance, the Alliance does not have an unlimited number of MATLAB licenses. Since these licenses come with complex rules (one license needed for each node, additional licenses for each toolbox, additional licenses for newer tools, etc.), it can quickly become a nightmare to navigate through it all. You may want to have a look at some of the comments in this thread.\n\n\nProprietary tools fall behind popular open-source tools\nEven large teams of software engineers cannot compete against an active community of researchers developing open-source tools. When open-source tools become really popular, the number of users contributing to their development vastly outnumbers what any company can provide. The testing, licensing, and production of proprietary tools are also too slow to keep up with quickly evolving fields of research. (Of course, open-source tools which do not take off and remain absolutely obscure do not see the benefit of a vast community.)\n\n\nProprietary tools often fail to address specialized edge cases needed in research\nIt is not commercially sound to develop cutting edge capabilities so specialized in a narrow subfield that they can only target a minuscule number of customers. But this is often what research needs. With open-source tools, researchers can develop the capabilities that fit their very specific needs. So while commercial tools are good and reliable for large audiences, they are often not the best in research. This explains the success of R over tools such as SASS or Stata in the past decade.",
    "crumbs": [
      "AI",
      "<b><em>PyTorch</em></b>",
      "Which framework to choose?"
    ]
  },
  {
    "objectID": "ai/pt/pt_choosing_frameworks.html#conclusion",
    "href": "ai/pt/pt_choosing_frameworks.html#conclusion",
    "title": "Which framework to choose?",
    "section": "Conclusion",
    "text": "Conclusion\nAll that said, sometimes you don’t have a choice over the tool to use for your research as this may be dictated by the culture in your field or by your supervisor. But if you are free to choose and if superior or equal open-source alternatives exist and are popular, do not fall in the trap of thinking that because your university and the Alliance pay for a license, you should make use of it. It may be free for you—for now—but it can have hidden costs.",
    "crumbs": [
      "AI",
      "<b><em>PyTorch</em></b>",
      "Which framework to choose?"
    ]
  },
  {
    "objectID": "ai/pt/pt_autograd.html",
    "href": "ai/pt/pt_autograd.html",
    "title": "Automatic differentiation",
    "section": "",
    "text": "PyTorch has automatic differentiation capabilities—meaning that it can track all the operations performed on tensors during the forward pass and compute all the gradients automatically for the backpropagation—thanks to its package torch.autograd.\nLet’s have a look at this.",
    "crumbs": [
      "AI",
      "<b><em>PyTorch</em></b>",
      "Automatic differentiation"
    ]
  },
  {
    "objectID": "ai/pt/pt_autograd.html#some-definitions",
    "href": "ai/pt/pt_autograd.html#some-definitions",
    "title": "Automatic differentiation",
    "section": "Some definitions",
    "text": "Some definitions\nDerivative of a function:\nRate of change of a function with a single variable w.r.t. its variable.\nPartial derivative:\nRate of change of a function with multiple variables w.r.t. one variable while other variables are considered as constants.\nGradient:\nVector of partial derivatives of function with several variables.\nDifferentiation:\nCalculation of the derivatives of a function.\nChain rule:\nFormula to calculate the derivatives of composite functions.\nAutomatic differentiation:\nAutomatic computation of partial derivatives by algorithms.",
    "crumbs": [
      "AI",
      "<b><em>PyTorch</em></b>",
      "Automatic differentiation"
    ]
  },
  {
    "objectID": "ai/pt/pt_autograd.html#backpropagation",
    "href": "ai/pt/pt_autograd.html#backpropagation",
    "title": "Automatic differentiation",
    "section": "Backpropagation",
    "text": "Backpropagation\nFirst, we need to talk about backpropagation: the backward pass following each forward pass and which adjusts the model’s parameters to minimize the output of the loss function.\nThe last 2 videos of 3Blue1Brown neural network series explains backpropagation and its manual calculation very well.\n\nWhat is backpropagation? (14 min)\n\n\nThere is one minor terminological error in this video: they call the use of mini-batches stochastic gradient descent. In fact, this is called mini-batch gradient descent. Stochastic gradient descent uses a single example at each iteration.\n\n\n\nHow does backpropagation work? (10 min)",
    "crumbs": [
      "AI",
      "<b><em>PyTorch</em></b>",
      "Automatic differentiation"
    ]
  },
  {
    "objectID": "ai/pt/pt_autograd.html#automatic-differentiation",
    "href": "ai/pt/pt_autograd.html#automatic-differentiation",
    "title": "Automatic differentiation",
    "section": "Automatic differentiation",
    "text": "Automatic differentiation\nIf we had to do all this manually, it would be absolute hell. Thankfully, many tools—including PyTorch—can do this automatically.\n\nTracking computations\nFor the automation of the calculation of all those derivatives through chain rules, PyTorch needs to track computations during the forward pass.\nPyTorch does not however track all the computations on all the tensors (this would be extremely memory intensive!). To start tracking computations on a vector, set the requires_grad attribute to True:\n\nimport torch\n\nx = torch.ones(2, 4, requires_grad=True)\nx\n\ntensor([[1., 1., 1., 1.],\n        [1., 1., 1., 1.]], requires_grad=True)\n\n\n\nThe grad_fun attribute\nWhenever a tensor is created by an operation involving a tracked tensor, it has a grad_fun attribute:\n\ny = x + 1\ny\n\ntensor([[2., 2., 2., 2.],\n        [2., 2., 2., 2.]], grad_fn=&lt;AddBackward0&gt;)\n\n\n\ny.grad_fn\n\n&lt;AddBackward0 at 0x70340dfaea40&gt;\n\n\n\n\nJudicious tracking\nYou don’t want to track more than is necessary. There are multiple ways to avoid tracking what you don’t want.\nYou can stop tracking computations on a tensor with the method detach:\n\nx\n\ntensor([[1., 1., 1., 1.],\n        [1., 1., 1., 1.]], requires_grad=True)\n\n\n\nx.detach_()\n\ntensor([[1., 1., 1., 1.],\n        [1., 1., 1., 1.]])\n\n\nYou can change its requires_grad flag:\n\nx = torch.zeros(2, 3, requires_grad=True)\nx\n\ntensor([[0., 0., 0.],\n        [0., 0., 0.]], requires_grad=True)\n\n\n\nx.requires_grad_(False)\n\ntensor([[0., 0., 0.],\n        [0., 0., 0.]])\n\n\nAlternatively, you can wrap any code you don’t want to track under with torch.no_grad():\n\nx = torch.ones(2, 4, requires_grad=True)\n\nwith torch.no_grad():\n    y = x + 1\n\ny\n\ntensor([[2., 2., 2., 2.],\n        [2., 2., 2., 2.]])\n\n\n\nCompare this with what we just did above.\n\n\n\n\nCalculating gradients\nLet’s calculate gradients manually, then use autograd, in a very simple case: imagine that \\(x\\), \\(y\\), and \\(z\\) are tensors containing the parameters of a model and that the error \\(e\\) could be calculated with the equation:\n\\[e=2x^4-y^3+3z^2\\]\n\nManual derivative calculation\nLet’s see how we would do this manually.\nFirst, we need the model parameters tensors:\n\nx = torch.tensor([1., 2.])\ny = torch.tensor([3., 4.])\nz = torch.tensor([5., 6.])\n\nWe calculate \\(e\\) following the above equation:\n\ne = 2*x**4 - y**3 + 3*z**2\n\nThe gradients of the error \\(e\\) w.r.t. the parameters \\(x\\), \\(y\\), and \\(z\\) are:\n\\[\\frac{de}{dx}=8x^3\\] \\[\\frac{de}{dy}=-3y^2\\] \\[\\frac{de}{dz}=6z\\]\nWe can calculate them with:\n\ngradient_x = 8*x**3\ngradient_x\n\ntensor([ 8., 64.])\n\n\n\ngradient_y = -3*y**2\ngradient_y\n\ntensor([-27., -48.])\n\n\n\ngradient_z = 6*z\ngradient_z\n\ntensor([30., 36.])\n\n\n\n\nAutomatic derivative calculation\nFor this method, we need to define our model parameters with requires_grad set to True:\n\nx = torch.tensor([1., 2.], requires_grad=True)\ny = torch.tensor([3., 4.], requires_grad=True)\nz = torch.tensor([5., 6.], requires_grad=True)\n\n\\(e\\) is calculated in the same fashion (except that here, all the computations on \\(x\\), \\(y\\), and \\(z\\) are tracked):\n\ne = 2*x**4 - y**3 + 3*z**2\n\nThe backward propagation is done automatically with:\n\ne.backward(torch.tensor([1., 1.]))\n\nAnd we have our 3 partial derivatives:\n\nprint(x.grad)\nprint(y.grad)\nprint(z.grad)\n\ntensor([ 8., 64.])\ntensor([-27., -48.])\ntensor([30., 36.])\n\n\n\n\nComparison\nThe result is the same, as can be tested with:\n\n8*x**3 == x.grad\n\ntensor([True, True])\n\n\n\n-3*y**2 == y.grad\n\ntensor([True, True])\n\n\n\n6*z == z.grad\n\ntensor([True, True])\n\n\nOf course, calculating the gradients manually here was extremely easy, but imagine how tedious and lengthy it would be to write the chain rules to calculate the gradients of all the composite functions in a neural network manually…",
    "crumbs": [
      "AI",
      "<b><em>PyTorch</em></b>",
      "Automatic differentiation"
    ]
  },
  {
    "objectID": "ai/jx/wb_jax.html",
    "href": "ai/jx/wb_jax.html",
    "title": "Accelerated array computing and flexible differentiation with JAX",
    "section": "",
    "text": "JAX is an open source Python library for high-performance array computing and flexible automatic differentiation.\nHigh-performance computing is achieved by asynchronous dispatch, just-in-time compilation, the XLA compiler for linear algebra, and full compatibility with accelerators (GPUs and TPUs).\nAutomatic differentiation uses Autograd and works with complex control flows (conditions, recursions), second and third-order derivatives, forward and reverse modes. This makes JAX ideal for machine learning and neural network libraries such as Flax are built on it.\nThis webinar will give an overview of JAX’s principles and functioning.\n\nSlides (Click and wait: this reveal.js presentation may take a little time to load.)",
    "crumbs": [
      "AI",
      "<b><em>Webinars</em></b>",
      "Accelerated array & AD with JAX"
    ]
  },
  {
    "objectID": "ai/jx/jx_resources.html",
    "href": "ai/jx/jx_resources.html",
    "title": "Resources",
    "section": "",
    "text": "Here is a list of resources to get started with JAX.",
    "crumbs": [
      "AI",
      "<b><em>JAX</em></b>",
      "Resources"
    ]
  },
  {
    "objectID": "ai/jx/jx_resources.html#official-documentation",
    "href": "ai/jx/jx_resources.html#official-documentation",
    "title": "Resources",
    "section": "Official documentation",
    "text": "Official documentation\n\nJAX GitHub repo\nOfficial documentation\nJAX API",
    "crumbs": [
      "AI",
      "<b><em>JAX</em></b>",
      "Resources"
    ]
  },
  {
    "objectID": "ai/jx/jx_resources.html#other-resources",
    "href": "ai/jx/jx_resources.html#other-resources",
    "title": "Resources",
    "section": "Other resources",
    "text": "Other resources\nAwesome JAX is a great list of resources on JAX.",
    "crumbs": [
      "AI",
      "<b><em>JAX</em></b>",
      "Resources"
    ]
  },
  {
    "objectID": "ai/jx/jx_resources.html#qa",
    "href": "ai/jx/jx_resources.html#qa",
    "title": "Resources",
    "section": "Q&A",
    "text": "Q&A\n\nJAX GitHub Discussions\nStack Overflow [jax] tag",
    "crumbs": [
      "AI",
      "<b><em>JAX</em></b>",
      "Resources"
    ]
  },
  {
    "objectID": "ai/jx/jx_resources.html#alliance-wiki",
    "href": "ai/jx/jx_resources.html#alliance-wiki",
    "title": "Resources",
    "section": "Alliance wiki",
    "text": "Alliance wiki\nThere is currently no page on JAX, but there is a page on Flax.",
    "crumbs": [
      "AI",
      "<b><em>JAX</em></b>",
      "Resources"
    ]
  },
  {
    "objectID": "ai/jx/jx_parallel.html",
    "href": "ai/jx/jx_parallel.html",
    "title": "Parallel computing",
    "section": "",
    "text": "JAX is designed for DNN and linear algebra at scale. Processing vast amounts of data in parallel is crucial to its goal. Two of JAX’s transformations allow to turn linear code into parallel code very easily.",
    "crumbs": [
      "AI",
      "<b><em>JAX</em></b>",
      "Parallel computing"
    ]
  },
  {
    "objectID": "ai/jx/jx_parallel.html#vectorization",
    "href": "ai/jx/jx_parallel.html#vectorization",
    "title": "Parallel computing",
    "section": "Vectorization",
    "text": "Vectorization\nRemember how a number of transformations are applied to jaxprs. We already saw two of JAX’s main transformations: JIT compilation with jax.jit and automatic differentiation with jax.grad. Vectorization with jax.vmap is another one.\nIt automates the vectorization of complex functions (operations on arrays are naturally executed in a vectorized fashion—as is the case in R, in NumPy, etc.—but more complex functions are not).\nHere is an example from JAX 101 commonly encountered in deep learning:\nimport jax\nimport jax.numpy as jnp\n\nx = jnp.arange(5)\nw = jnp.array([2., 3., 4.])\n\ndef convolve(x, w):\n    output = []\n    for i in range(1, len(x)-1):\n        output.append(jnp.dot(x[i-1:i+2], w))\n    return jnp.array(output)\n\nconvolve(x, w)\nArray([11., 20., 29.], dtype=float32)\n\nSee this great post for explanations of convolutions.\n\nYou will probably want to apply the function convolve() to a batch of weights w and vectors x.\nxs = jnp.stack([x, x, x])\nws = jnp.stack([w, w, w])\nWe apply the jax.vmap() transformation to the convolve() function and pass the batches to it:\nvconvolve = jax.vmap(convolve)\nvconvolve(xs, ws)\nArray([[11., 20., 29.],\n       [11., 20., 29.],\n       [11., 20., 29.]], dtype=float32)\n\nAs we already saw, transformations can be composed:\nvconvolve_jit = jax.jit(vconvolve)\nvconvolve_jit(xs, ws)\nArray([[11., 20., 29.],\n       [11., 20., 29.],\n       [11., 20., 29.]], dtype=float32)",
    "crumbs": [
      "AI",
      "<b><em>JAX</em></b>",
      "Parallel computing"
    ]
  },
  {
    "objectID": "ai/jx/jx_parallel.html#parallel-runs-across-devices",
    "href": "ai/jx/jx_parallel.html#parallel-runs-across-devices",
    "title": "Parallel computing",
    "section": "Parallel runs across devices",
    "text": "Parallel runs across devices\nThe jax.pmap transformation does the same thing but each computation runs on a different device (e.g. a different GPU) on the same node, allowing to scale things up further:\njax.pmap(convolve)(xs, ws)\njax.pmap automatically JIT compiles the code, so it is unnecessary to pass this to jax.jit.\n\nJAX is also capable of running distributed arrays across multiple devices through sharding.\nJAX does not have the ability to scale things up to the level of multi-node clusters, but the mpi4jax extension provides multi-host communication for distributed parallelism.",
    "crumbs": [
      "AI",
      "<b><em>JAX</em></b>",
      "Parallel computing"
    ]
  },
  {
    "objectID": "ai/jx/jx_optimizations.html",
    "href": "ai/jx/jx_optimizations.html",
    "title": "Pushing optimizations further",
    "section": "",
    "text": "JAX feels lower level than other libraries (more constraints, more performance). This can be pushed further for additional speedups (but with additional code complexity).",
    "crumbs": [
      "AI",
      "<b><em>JAX</em></b>",
      "Pushing optimizations further"
    ]
  },
  {
    "objectID": "ai/jx/jx_optimizations.html#the-lax-api",
    "href": "ai/jx/jx_optimizations.html#the-lax-api",
    "title": "Pushing optimizations further",
    "section": "The lax API",
    "text": "The lax API\njax.numpy is a high-level NumPy-like API wrapped around jax.lax. jax.lax is a more efficient lower-level API itself wrapped around XLA. It is more powerful, but even stricter and requires many more lines of code.",
    "crumbs": [
      "AI",
      "<b><em>JAX</em></b>",
      "Pushing optimizations further"
    ]
  },
  {
    "objectID": "ai/jx/jx_optimizations.html#pallas-extension-to-write-gpu-and-tpu-kernels",
    "href": "ai/jx/jx_optimizations.html#pallas-extension-to-write-gpu-and-tpu-kernels",
    "title": "Pushing optimizations further",
    "section": "Pallas: extension to write GPU and TPU kernels",
    "text": "Pallas: extension to write GPU and TPU kernels\nWith the success of Triton, JAX built the Pallas extension that allows JAX users to write GPU kernels.\nIt also allows to write kernels for the TPU with moisaic.\n\n\n\n\n\n\n\n\n\ntracer\n\nTracing\n\n\n\njaxpr\n\nJaxprs\n(JAX expressions)\nintermediate\nrepresentation\n(IR)\n\n\n\ntracer-&gt;jaxpr\n\n\n\n\n\njit\n\n Just-in-time \n(JIT)\ncompilation\n\n\n\nhlo\n\nHigh-level\noptimized (HLO)\nprogram\n\n\n\njit-&gt;hlo\n\n\n\n\n\ntriton\n\nTriton\n\n\n\nGPU\n\nGPU\n\n\n\ntriton-&gt;GPU\n\n\n\n\n\nmosaic\n\nMosaic\n\n\n\nTPU\n\nTPU\n\n\n\nmosaic-&gt;TPU\n\n\n\n\n\ntransform\n\nVectorization\nParallelization\n   Differentiation  \n\n\n\npy\n\nPure Python\nfunctions\n\n\n\npy-&gt;tracer\n\n\n\n\njaxpr-&gt;jit\n\n\n\n\njaxpr-&gt;transform\n\n\n\n\n\n\nhlo-&gt;triton\n\n\n\n\nhlo-&gt;mosaic",
    "crumbs": [
      "AI",
      "<b><em>JAX</em></b>",
      "Pushing optimizations further"
    ]
  },
  {
    "objectID": "ai/jx/jx_map.html",
    "href": "ai/jx/jx_map.html",
    "title": "How does it work?",
    "section": "",
    "text": "Before using JAX, it is critical to understand its functioning: JAX architecture is at the core of its efficiency and flexibility, but also the cause of a number of constraints.",
    "crumbs": [
      "AI",
      "<b><em>JAX</em></b>",
      "How does it work?"
    ]
  },
  {
    "objectID": "ai/jx/jx_map.html#map",
    "href": "ai/jx/jx_map.html#map",
    "title": "How does it work?",
    "section": "Map",
    "text": "Map\nHere is a schematic of JAX’s functioning:\n\n\n\n\n\n\n\n\n\ntracer\n\nTracing\n\n\n\njaxpr\n\nJaxprs\n(JAX expressions)\nintermediate\nrepresentation\n(IR)\n\n\n\ntracer-&gt;jaxpr\n\n\n\n\n\njit\n\n Just-in-time \n(JIT)\ncompilation\n\n\n\nhlo\n\nHigh-level\noptimized (HLO)\nprogram\n\n\n\njit-&gt;hlo\n\n\n\n\n\nxla\n\nAccelerated\n Linear Algebra \n(XLA)\n\n\n\nCPU\n\nCPU\n\n\n\nxla-&gt;CPU\n\n\n\n\n\nGPU\n\nGPU\n\n\n\nxla-&gt;GPU\n\n\n\n\n\nTPU\n\nTPU\n\n\n\nxla-&gt;TPU\n\n\n\n\n\ntransform\n\n Transformations \n\n\n\npy\n\nPure Python\nfunctions\n\n\n\npy-&gt;tracer\n\n\n\n\njaxpr-&gt;jit\n\n\n\n\njaxpr-&gt;transform\n\n\n\n\n\n\nhlo-&gt;xla",
    "crumbs": [
      "AI",
      "<b><em>JAX</em></b>",
      "How does it work?"
    ]
  },
  {
    "objectID": "ai/jx/jx_map.html#tracing",
    "href": "ai/jx/jx_map.html#tracing",
    "title": "How does it work?",
    "section": "Tracing",
    "text": "Tracing\nTracing happens during the first call of a function. Tracer objects are wrapped around each argument and record all operations performed on them, creating a Jaxpr (JAX expression). It is this intermediate representation—rather than the Python code—that JAX then uses.\nThe tracer objects used to create the Jaxpr contain information about the shape and dtype of the initial Python arguments, but not their values. This means that new inputs with the same shape and dtype will use the cached compiled program directly, skipping the Python code entirely. Inputs with new shape and/or dtype will trigger tracing again (so the Python function gets executed again).\nFunction side-effects are not recorded by the tracers, which means that they are not part of the Jaxprs. They will be executed once (during tracing), but are thereafter absent from the cached compiled program.\nFunctions which use values outside of their arguments (e.g. values from the global environment) will not update the cache if such values change.\nFor these reasons, only functionally pure functions (functions without side effects and which do not rely on values outside their arguments) should be used with JAX.",
    "crumbs": [
      "AI",
      "<b><em>JAX</em></b>",
      "How does it work?"
    ]
  },
  {
    "objectID": "ai/jx/jx_map.html#transformations",
    "href": "ai/jx/jx_map.html#transformations",
    "title": "How does it work?",
    "section": "Transformations",
    "text": "Transformations\nJAX is essentially a functional programming framework. Transformations are higher-order functions transforming Jaxprs.\nTransformations are composable and include:\n\njax.grad(): creates a function that evaluates the gradient of the input function,\njax.vmap(): implementation of automatic vectorization,\njax.pmap(): implementation of data parallelism across processing units,\n\nand finally, once other necessary transformations have been performed:\n\njax.jit(): just-in-time compilation for the XLA.",
    "crumbs": [
      "AI",
      "<b><em>JAX</em></b>",
      "How does it work?"
    ]
  },
  {
    "objectID": "ai/jx/jx_map.html#xla",
    "href": "ai/jx/jx_map.html#xla",
    "title": "How does it work?",
    "section": "XLA",
    "text": "XLA\nThe XLA (Accelerated Linear Algebra) compiler takes JIT-compiled JAX programs and optimizes them for the available hardware (CPUs, GPUs, or TPUs).",
    "crumbs": [
      "AI",
      "<b><em>JAX</em></b>",
      "How does it work?"
    ]
  },
  {
    "objectID": "ai/jx/jx_jobs.html",
    "href": "ai/jx/jx_jobs.html",
    "title": "Running jobs",
    "section": "",
    "text": "This section is a quick review on how to submit jobs to the scheduler (Slurm) on the Alliance clusters. For more detailed information, please visit our wiki.\n\nThere are two types of jobs that can be launched on an Alliance cluster: interactive jobs and batch jobs.\n\nDon’t run computations on the login node: those are very small nodes not designed to handle anything heavy.\n\n\nInteractive jobs\nTo run Python interactively, you should launch an salloc session.\n\nExample:\n\nsalloc --time=xxx --mem-per-cpu=xxx --cpus-per-task=xxx\nThis takes you to a compute node where you can now launch Python (or even better IPython) to run computations:\nipython\n\nNote that while interactive jobs are great for code development, they are not resource efficient: all the resources that you requested are blocked for you while your job is running, whether you are making use of them (running heavy computations) or not (thinking, typing code, running computations that use only a fraction of the requested resources).\nBest to use this on sample data using few resources.\n\n\n\nScripts\nOnce you have a working and tested program, you should run a batch job on the resources you need to get your results. To run a Python script called &lt;your_script&gt;.py, you first need to write a job script:\n\nExample:\n\n\n&lt;your_job&gt;.sh\n\n#!/bin/bash\n#SBATCH --account=def-&lt;your_account&gt;\n#SBATCH --time=xxx\n#SBATCH --mem-per-cpu=xxx\n#SBATCH --cpus-per-task=xxx\n#SBATCH --job-name=\"&lt;your_job&gt;\"\n\nsource ~/env/bin/activate\npython &lt;your_script&gt;.py\n\n\nThen launch your job with:\nsbatch &lt;your_job&gt;.sh\nYou can monitor your job with sq (an alias for squeue -u $USER $@).\n\nBatch jobs are the best approach to run heavy computations requiring a lot of hardware.\nIt will save you lots of waiting time (Alliance clusters) or money (commercial clusters).",
    "crumbs": [
      "AI",
      "<b><em>JAX</em></b>",
      "Running jobs"
    ]
  },
  {
    "objectID": "ai/jx/jx_install.html",
    "href": "ai/jx/jx_install.html",
    "title": "Installing JAX",
    "section": "",
    "text": "In this section, we will access a virtual training cluster through SSH and make JAX accessible.\nWe will also cover how to install JAX in the Alliance production clusters.\nUnless you aren’t planning to use accelerators, JAX relies on GPUs/TPUs dependencies determined by your OS and hardware (e.g. CUDA and CUDNN). Making sure that the dependencies are installed, compatible, and working with JAX can be finicky, so it is a lot easier to install JAX from pip wheels.",
    "crumbs": [
      "AI",
      "<b><em>JAX</em></b>",
      "Installation"
    ]
  },
  {
    "objectID": "ai/jx/jx_install.html#on-your-computer",
    "href": "ai/jx/jx_install.html#on-your-computer",
    "title": "Installing JAX",
    "section": "On your computer",
    "text": "On your computer\nOn your personal computer, use the wheel installation command from the official JAX site corresponding to your system.\n\nOn Windows, GPUs are only supported via Windows Subsystem for Linux 2.\n\nBecause JAX is designed for large array computations and machine learning, you will most likely want to use it on supercomputers. In this course, we will thus use a virtual Alliance cluster.",
    "crumbs": [
      "AI",
      "<b><em>JAX</em></b>",
      "Installation"
    ]
  },
  {
    "objectID": "ai/jx/jx_install.html#on-an-alliance-cluster",
    "href": "ai/jx/jx_install.html#on-an-alliance-cluster",
    "title": "Installing JAX",
    "section": "On an Alliance cluster",
    "text": "On an Alliance cluster\n\nLogging in through SSH\n\nOpen a terminal emulator\nWindows users:  Install the free version of MobaXTerm and launch it.\nMacOS users:   Launch Terminal.\nLinux users:     Open the terminal emulator of your choice.\n\n\nAccess the cluster through secure shell\n\nWindows users\nFollow the first 18% of this demo.\nFor “Remote host”, use the hostname we gave you.\nSelect the box “Specify username” and provide your username.\n\nNote that the password is entered through blind typing, meaning that you will not see anything happening as you type it. This is a Linux feature. While it is a little disturbing at first, do know that it is working. Make sure to type it slowly to avoid typos, then press the “enter” key on your keyboard.\n\n\n\nMacOS and Linux users\nIn the terminal, run:\nssh &lt;username&gt;@&lt;hostname&gt;\n\nReplace the username and hostname by their values. For instance:\nssh user21@somecluster.c3.ca\n\nYou will be asked a question, answer “Yes”.\nWhen prompted, type the password.\n\nNote that the password is entered through blind typing, meaning that you will not see anything happening as you type it. This is a Linux feature. While it is a little disturbing at first, do know that it is working. Make sure to type it slowly to avoid typos, then press the “enter” key on your keyboard.\n\n\n\n\nTroubleshooting\nProblems logging in are almost always due to typos. If you cannot log in, retry slowly, entering your password carefully.\n\n\n\nInstall JAX\n\nWe already created a Python virtual environment and installed JAX to save time. The instructions for today thus differ from what you would normally do, but I include the normal instructions in a separate tab for your future reference.\n\n\nTodayProduction cluster\n\n\nI already created a virtual Python environment under /project and installed JAX in it to save time and space. All you have to do is activate it:\nsource /project/60055/env/bin/activate\n\n\nLook for available Python modules:\nmodule spider python\nLoad the version of your choice:\nmodule load python/3.11.5\nCreate a Python virtual environment:\npython -m venv ~/env\nActivate it:\nsource ~/env/bin/activate\nUpdate pip from wheel:\npython -m pip install --upgrade pip --no-index\n\nWhenever a Python wheel for a package is available on the Alliance clusters, you should use it instead of downloading the package from PyPI. To do this, simply add the --no-index flag to the install command.\nYou can see whether a wheel is available with avail_wheels &lt;package&gt; or look at the list of available wheels.\nAdvantages of wheels:\n\ncompiled for the clusters hardware,\nensures no missing or conflicting dependencies,\nmuch faster installation.\n\n\nInstall JAX from wheel:\npython -m pip install jax --no-index\n\nDon’t forget the --no-index flag here: the wheel will save you from having to deal with the CUDA and CUDNN dependencies, making your life a lot easier.",
    "crumbs": [
      "AI",
      "<b><em>JAX</em></b>",
      "Installation"
    ]
  },
  {
    "objectID": "ai/jx/jx_ad.html",
    "href": "ai/jx/jx_ad.html",
    "title": "Automatic differentiation",
    "section": "",
    "text": "One of the transformations that can be applied to array computations is the calculation of gradients which is crucial to the backpropagation through deep neural networks.\nConsidering the function f:\nWe can create a new function dfdx that computes the gradient of f w.r.t. x:\ndfdx returns the derivatives:",
    "crumbs": [
      "AI",
      "<b><em>JAX</em></b>",
      "Automatic differentiation"
    ]
  },
  {
    "objectID": "ai/jx/jx_ad.html#composing-transformations",
    "href": "ai/jx/jx_ad.html#composing-transformations",
    "title": "Automatic differentiation",
    "section": "Composing transformations",
    "text": "Composing transformations\nTransformations can be composed:\nprint(jit(grad(f))(1.))\n4.0\nprint(grad(jit(f))(1.))\n4.0",
    "crumbs": [
      "AI",
      "<b><em>JAX</em></b>",
      "Automatic differentiation"
    ]
  },
  {
    "objectID": "ai/jx/jx_ad.html#forward-and-reverse-modes",
    "href": "ai/jx/jx_ad.html#forward-and-reverse-modes",
    "title": "Automatic differentiation",
    "section": "Forward and reverse modes",
    "text": "Forward and reverse modes\nJAX offers other autodiff methods:\n\nreverse-mode vector-Jacobian products: jax.vjp,\nforward-mode Jacobian-vector products: jax.jvp.",
    "crumbs": [
      "AI",
      "<b><em>JAX</em></b>",
      "Automatic differentiation"
    ]
  },
  {
    "objectID": "ai/jx/jx_ad.html#higher-order-differentiation",
    "href": "ai/jx/jx_ad.html#higher-order-differentiation",
    "title": "Automatic differentiation",
    "section": "Higher-order differentiation",
    "text": "Higher-order differentiation\nWith a single variable, the grad function calls can be nested:\nd2fdx = grad(dfdx)   # function to compute 2nd order derivatives\nd3fdx = grad(d2fdx)  # function to compute 3rd order derivatives\n...\nWith several variables, you have to use the functions:\n\njax.jacfwd for forward-mode,\njax.jacrev for reverse-mode.",
    "crumbs": [
      "AI",
      "<b><em>JAX</em></b>",
      "Automatic differentiation"
    ]
  },
  {
    "objectID": "ai/jx/fl_training.html#optimization-function",
    "href": "ai/jx/fl_training.html#optimization-function",
    "title": "Training",
    "section": "Optimization function",
    "text": "Optimization function",
    "crumbs": [
      "AI",
      "<b><em>Flax</em></b>",
      "Training"
    ]
  },
  {
    "objectID": "ai/jx/fl_resources.html",
    "href": "ai/jx/fl_resources.html",
    "title": "Resources",
    "section": "",
    "text": "Here is a list of resources to get started with Flax."
  },
  {
    "objectID": "ai/jx/fl_resources.html#official-documentation",
    "href": "ai/jx/fl_resources.html#official-documentation",
    "title": "Resources",
    "section": "Official documentation",
    "text": "Official documentation\n\nFlax GitHub repo\nOfficial documentation\nFlax API"
  },
  {
    "objectID": "ai/jx/fl_resources.html#other-resources",
    "href": "ai/jx/fl_resources.html#other-resources",
    "title": "Resources",
    "section": "Other resources",
    "text": "Other resources\nHere is a list of projects using Flax."
  },
  {
    "objectID": "ai/jx/fl_resources.html#qa",
    "href": "ai/jx/fl_resources.html#qa",
    "title": "Resources",
    "section": "Q&A",
    "text": "Q&A\n\nFlax GitHub Discussions\nStack Overflow [flax] tag"
  },
  {
    "objectID": "ai/jx/fl_resources.html#alliance",
    "href": "ai/jx/fl_resources.html#alliance",
    "title": "Resources",
    "section": "Alliance",
    "text": "Alliance\nThe Alliance wiki has a page on Flax."
  },
  {
    "objectID": "ai/jx/fl_optimization.html#data-padding",
    "href": "ai/jx/fl_optimization.html#data-padding",
    "title": "Optimizations",
    "section": "Data padding",
    "text": "Data padding\n\nPrevent recompilation for the last batch that is smaller (different shape)."
  },
  {
    "objectID": "ai/jx/fl_optimization.html#learning-rate-scheduling",
    "href": "ai/jx/fl_optimization.html#learning-rate-scheduling",
    "title": "Optimizations",
    "section": "Learning rate scheduling",
    "text": "Learning rate scheduling"
  },
  {
    "objectID": "ai/jx/fl_linen.html",
    "href": "ai/jx/fl_linen.html",
    "title": "The Linen API",
    "section": "",
    "text": "In Flax, the base class for neural networks is the flax.linen.Module. Linen is a new API replacing the initial flax.nn API and taking full advantage of JAX transformations while automating the initialization and handling of the parameters.\nLinen is imported this way:\nfrom flax import linen as nn\nTo define a model, you create a subclass. The syntax closely resembles that of PyTorch torch.nn:\nclass Net(nn.Module):\n    ...\nBut unlike in PyTorch, the parameters are passed through the model in the form of Pytrees (nested containers such as dictionaries, lists, and tuples)."
  },
  {
    "objectID": "ai/jx/fl_linen.html#the-linen-api",
    "href": "ai/jx/fl_linen.html#the-linen-api",
    "title": "The Linen API",
    "section": "",
    "text": "In Flax, the base class for neural networks is the flax.linen.Module. Linen is a new API replacing the initial flax.nn API and taking full advantage of JAX transformations while automating the initialization and handling of the parameters.\nLinen is imported this way:\nfrom flax import linen as nn\nTo define a model, you create a subclass. The syntax closely resembles that of PyTorch torch.nn:\nclass Net(nn.Module):\n    ...\nBut unlike in PyTorch, the parameters are passed through the model in the form of Pytrees (nested containers such as dictionaries, lists, and tuples)."
  },
  {
    "objectID": "ai/jx/fl_data.html",
    "href": "ai/jx/fl_data.html",
    "title": "Loading data",
    "section": "",
    "text": "Flax does not implement methods to load datasets since Hugging Face, PyTorch, and TensorFlow already provide great APIs for this.",
    "crumbs": [
      "AI",
      "<b><em>Flax</em></b>",
      "Loading data"
    ]
  },
  {
    "objectID": "ai/jx/fl_data.html#hugging-face-datasets",
    "href": "ai/jx/fl_data.html#hugging-face-datasets",
    "title": "Loading data",
    "section": "Hugging Face datasets",
    "text": "Hugging Face datasets\nThe Datasets library from 🤗 is a lightweight, framework-agnostic, and easy to use API to download datasets from the Hugging Face Hub. It uses Apache Arrow’s efficient caching system, allowing large datasets to be used on machines with small memory (Lhoest et al. 2021).\n\nSearch dataset\nGo to the Hugging Face Hub and search through thousands of open source datasets provided by the community.\n\n\nInspect dataset\nYou can get information on a dataset before downloading it.\nLoad the dataset builder for the dataset you are interested in:\n\nfrom datasets import load_dataset_builder\nds_builder = load_dataset_builder(\"mnist\")\n\nModuleNotFoundError: No module named 'datasets'\n\n\nGet a description of the dataset:\n\nds_builder.info.description\n\nNameError: name 'ds_builder' is not defined\n\n\nGet information on the features:\n\nds_builder.info.features\n\nNameError: name 'ds_builder' is not defined\n\n\n\n\nDownload dataset and load in session\nfrom datasets import load_dataset\n\ndef get_dataset_hf():\n    mnist = load_dataset(\"mnist\")\n\n    ds = {}\n\n    for split in ['train', 'test']:\n        ds[split] = {\n            'image': np.array([np.array(im) for im in mnist[split]['image']]),\n            'label': np.array(mnist[split]['label'])\n        }\n\n        ds[split]['image'] = jnp.float32(ds[split]['image']) / 255\n        ds[split]['label'] = jnp.int16(ds[split]['label'])\n\n        ds[split]['image'] = jnp.expand_dims(ds[split]['image'], 3)\n\n    return ds['train'], ds['test']",
    "crumbs": [
      "AI",
      "<b><em>Flax</em></b>",
      "Loading data"
    ]
  },
  {
    "objectID": "ai/jx/fl_data.html#pytorch-torchvision-datasets",
    "href": "ai/jx/fl_data.html#pytorch-torchvision-datasets",
    "title": "Loading data",
    "section": "PyTorch Torchvision datasets",
    "text": "PyTorch Torchvision datasets\nfrom torchvision import datasets\n\ndef get_dataset_torch():\n    mnist = {\n        'train': datasets.MNIST('./data', train=True, download=True),\n        'test': datasets.MNIST('./data', train=False, download=True)\n    }\n\n    ds = {}\n\n    for split in ['train', 'test']:\n        ds[split] = {\n            'image': mnist[split].data.numpy(),\n            'label': mnist[split].targets.numpy()\n        }\n\n        ds[split]['image'] = jnp.float32(ds[split]['image']) / 255\n        ds[split]['label'] = jnp.int16(ds[split]['label'])\n\n        ds[split]['image'] = jnp.expand_dims(ds[split]['image'], 3)\n\n    return ds['train'], ds['test']",
    "crumbs": [
      "AI",
      "<b><em>Flax</em></b>",
      "Loading data"
    ]
  },
  {
    "objectID": "ai/jx/fl_data.html#tensorflow-datasets",
    "href": "ai/jx/fl_data.html#tensorflow-datasets",
    "title": "Loading data",
    "section": "TensorFlow datasets",
    "text": "TensorFlow datasets\n\n\nimport tensorflow_datasets as tfds\n\ndef get_dataset_tf(epochs, batch_size):\n    mnist = tfds.builder('mnist')\n    mnist.download_and_prepare()\n\n    ds = {}\n\n    for set in ['train', 'test']:\n        ds[set] = tfds.as_numpy(mnist.as_dataset(split=set, batch_size=-1))\n\n        # cast to jnp and rescale pixel values\n        ds[set]['image'] = jnp.float32(ds[set]['image']) / 255\n        ds[set]['label'] = jnp.int16(ds[set]['label'])\n\n    return ds['train'], ds['test']",
    "crumbs": [
      "AI",
      "<b><em>Flax</em></b>",
      "Loading data"
    ]
  },
  {
    "objectID": "ai/index.html",
    "href": "ai/index.html",
    "title": "AI",
    "section": "",
    "text": "Getting started with  \nAn intro course to DL with PyTorch\n\n\n\n\nFast computing with \nAn intro course to JAX\n\n\n\n\n\n\nJAX NN with \nA DL course with Flax\n\n\n\n\nA brief overview of  \nTraditional ML with scikit-learn\n\n\n\n\n\n\nWorkshops\nVarious DL topics\n\n\n\n\n60 min webinars\nVarious DL topics",
    "crumbs": [
      "AI",
      "<br>&nbsp;<em><b>AI</b></em><br><br>"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About this site",
    "section": "",
    "text": "This site contains Marie-Hélène Burle’s latest content.\nHer older training material can be found on the archived sites:"
  },
  {
    "objectID": "about.html#main-training-website",
    "href": "about.html#main-training-website",
    "title": "About this site",
    "section": "Main training website",
    "text": "Main training website\nThis is the mint (“mint is not training”) website.\nTo view all our training material, please visit our main training website."
  },
  {
    "objectID": "about.html#other-websites",
    "href": "about.html#other-websites",
    "title": "About this site",
    "section": "Other websites",
    "text": "Other websites\nIn addition, here are a few of our websites for various training events:\n\nAutumn School 2022\nTraining Modules 2022\nTraining Modules 2021\nSummer School 2020\nCoding Fundamentals for Humanists 2022 for the Digital Humanities Summer Institute\nCoding Fundamentals for Humanists 2021 for the Digital Humanities Summer Institute\nHSS Winter Series 2023\nHSS Winter Series 2022"
  },
  {
    "objectID": "ai/jx/fl_jaxdl.html",
    "href": "ai/jx/fl_jaxdl.html",
    "title": "Deep learning with JAX",
    "section": "",
    "text": "JAX is perfect for developing deep learning models:\n\nit deals with multi-dimensional arrays,\nit is extremely fast,\nit is optimized for accelerators,\nand it is capable of flexible automatic differentiation.\n\nJAX is however not a DL library. While it is possible to create neural networks directly in JAX, it makes more sense to use libraries built on JAX that provide the toolkit necessary to build and train neural networks.",
    "crumbs": [
      "AI",
      "<b><em>Flax</em></b>",
      "Deep learning with JAX"
    ]
  },
  {
    "objectID": "ai/jx/fl_jaxdl.html#deep-learning-workflow",
    "href": "ai/jx/fl_jaxdl.html#deep-learning-workflow",
    "title": "Deep learning with JAX",
    "section": "Deep learning workflow",
    "text": "Deep learning workflow\nTraining a neural network from scratch requires a number of steps:\n\n\n\n\n\n\n\n\n\nLoad\\ndataset\n\nLoad\ndataset\n\n\n\nDefine\\narchitecture\n\nDefine\narchitecture\n\n\n\nLoad\\ndataset-&gt;Define\\narchitecture\n\n\n\n\n\nTrain\n\nTrain\n\n\n\nDefine\\narchitecture-&gt;Train\n\n\n\n\n\nTest\n\nTest\n\n\n\nTrain-&gt;Test\n\n\n\n\n\nSave\\nmodel\n\nSave\nmodel\n\n\n\nTest-&gt;Save\\nmodel\n\n\n\n\n\n\n\n\n\n\n Pretrained models can also be used for feature extraction or transfer learning.",
    "crumbs": [
      "AI",
      "<b><em>Flax</em></b>",
      "Deep learning with JAX"
    ]
  },
  {
    "objectID": "ai/jx/fl_jaxdl.html#deep-learning-ecosystem-for-jax",
    "href": "ai/jx/fl_jaxdl.html#deep-learning-ecosystem-for-jax",
    "title": "Deep learning with JAX",
    "section": "Deep learning ecosystem for JAX",
    "text": "Deep learning ecosystem for JAX\nHere is a classic ecosystem of libraries for deep learning with JAX:\n\nLoad datasets\nThere are already good tools to load datasets (e.g. PyTorch, TensorFlow, Hugging Face), so JAX did not worry about creating its own implementation.\nDefine network architecture\nNeural networks can be build in JAX from scratch, but a number of packages built on JAX provide the necessary toolkit. Flax is the option recommended by the JAX developers and the one we will use in this course.\nTrain\nTraining a model requires optimization functions. These can be implemented in JAX from scratch but the library Optax provides the core components. Orbax provide checkpointing utilities.\nTest\nTesting a model is easy to do directly in JAX.\nSave model\nFlax provides methods to save a model.\n\n To sum up, here is an ecosystem of libraries to use JAX for neural networks:\n\n\n\n\n\n\n\n\n\nLoad\\ndataset\nLoad\ndataset\n\n\n\nDefine\\narchitecture\nDefine\narchitecture\n\n\n\n\nPyTorch\n\nPyTorch\n\n\n\n\nTrain\nTrain\n\n\n\n\nflax1\n\nFlax\n\n\n\n\nTest\nTest\n\n\n\n\nSave\\nmodel\nSave\nmodel\n\n\n\n\nTensorFlow\n\nTensorFlow\n\n\n\n\nPyTorch-&gt;flax1\n\n\n\n\n\n🤗\n\n🤗\n\n\n\n\nTensorFlow-&gt;flax1\n\n\n\n\n\n🤗-&gt;flax1\n\n\n\n\n\njax1\n\nJAX\nOptax \nOrbax\n\n\n\nflax1-&gt;jax1\n\n\n\n\n\nFlax\n\nFlax\n\n\n\nJAX\n\nJAX\n\n\n\njax1-&gt;JAX\n\n\n\n\n\nJAX-&gt;Flax\n\n\n\n\n\n\n\n\n\n\n When working from pretrained models, Hugging Face also provides a great API to download from thousands of pretrained models.",
    "crumbs": [
      "AI",
      "<b><em>Flax</em></b>",
      "Deep learning with JAX"
    ]
  },
  {
    "objectID": "ai/jx/fl_regularization.html",
    "href": "ai/jx/fl_regularization.html",
    "title": "Regularization",
    "section": "",
    "text": "Regularization"
  },
  {
    "objectID": "ai/jx/fl_regularization.html#batch-normalization",
    "href": "ai/jx/fl_regularization.html#batch-normalization",
    "title": "Regularization",
    "section": "Batch normalization",
    "text": "Batch normalization\nBatch normalization"
  },
  {
    "objectID": "ai/jx/fl_regularization.html#dropout",
    "href": "ai/jx/fl_regularization.html#dropout",
    "title": "Regularization",
    "section": "Dropout",
    "text": "Dropout\nDropout"
  },
  {
    "objectID": "ai/jx/fl_state.html",
    "href": "ai/jx/fl_state.html",
    "title": "Flax’s handling of model states",
    "section": "",
    "text": "Deep learning models can be split into two categories depending on the framework used to train them: stateful and stateless models. Flax—being built on top of JAX—falls in the latter category.\nIn this section, we will see what all of this means and how Flax handles model states.",
    "crumbs": [
      "AI",
      "<b><em>Flax</em></b>",
      "Flax's handling of model states"
    ]
  },
  {
    "objectID": "ai/jx/fl_state.html#dealing-with-state-in-jax",
    "href": "ai/jx/fl_state.html#dealing-with-state-in-jax",
    "title": "Flax’s handling of model states",
    "section": "Dealing with state in JAX",
    "text": "Dealing with state in JAX\nJAX JIT compilation requires that functions be without side effects since side effects are only executed once, during tracing.\nUpdating model parameters and optimizer state thus cannot be done as a side-effect. The state cannot be part of the model instance—it needs to be explicit, that is, separated from the model. During instantiation, no memory is allocated for the parameters. During the forward pass, the parameters will be part of the inputs, along with the data. The model is thus stateless and the constrains of pure functional programming are met (inputs lead to outputs without external influence or side effects).\nLet’s see why a stateful approach doesn’t work with JAX1: instead of defining a neural network class, we will define a very simple Counter class, following the PyTorch approach, that just adds 1. This allows us to see right away what is going on.\n1 Modified from JAX’s documentation.import jax\nimport jax.numpy as jnp\n\nclass Counter:\n    def __init__(self):\n        self.n = 0\n      \n    def count(self) -&gt; int:\n        \"\"\"Adds one to the counter and returns the new value.\"\"\"\n        self.n += 1\n        return self.n\n  \n    def reset(self):\n        \"\"\"Resets the counter to zero.\"\"\"\n        self.n = 0\nNow we can create an instance called counter of the Counter class.\ncounter = Counter()\nWe can use the counter:\nfor _ in range(3):\n    print(counter.count())\n1\n2\n3\nNow, let’s try with a JIT compiled version of count():\ncount_jit = jax.jit(counter.count)\n\ncounter.reset()\n\nfor _ in range(3):\n    print(count_jit())\n1\n1\n1\nThis is because count is not a functionally pure function. The tracing happens for the first run of the function (first iteration of the loop). Thereafter, the compiled version will rerun without taking into account the modifications of the attributes of counter.\nFor this to work, we need to initialize an explicit state and pass it as an argument to the count function:\nState = int\n\nclass Counter:\n    def count(self, n: State) -&gt; tuple[int, State]:\n        return n+1, n+1\n    \n    def reset(self) -&gt; State:\n        return 0\n\ncounter = Counter()\nstate = counter.reset()\n\nfor _ in range(3):\n    value, state = counter.count(state)\n    print(value)\n1\n2\n3\ncount_jit = jax.jit(counter.count)\n\nstate = counter.reset()\n\nfor _ in range(3):\n    value, state = count_jit(state)\n    print(value)\n1\n2\n3\n\nAs explained in JAX’s documentation, we turned a function of the type:\nclass StatefulClass\n  state: State\n  def stateful_method(*args, **kwargs) -&gt; Output:\nInto:\nclass StatelessClass\n  def stateless_method(state: State, *args, **kwargs) -&gt; (Output, State):",
    "crumbs": [
      "AI",
      "<b><em>Flax</em></b>",
      "Flax's handling of model states"
    ]
  },
  {
    "objectID": "ai/jx/fl_state.html#stateful-vs-stateless-models",
    "href": "ai/jx/fl_state.html#stateful-vs-stateless-models",
    "title": "Flax’s handling of model states",
    "section": "Stateful vs stateless models",
    "text": "Stateful vs stateless models\n\nStateful models\nIn frameworks such as PyTorch or the Julia package Flux, model parameters and optimizer state are stored within the model instance. Instantiating a PyTorch model allocates memory for the model parameters. The model can then be described as stateful.\n\n\nStateless models\nFrameworks based on JAX such as Flax but also the Julia package Lux (a modern rewrite of Flux with explicit model parameters and a philosophy similar to JAX’s) are stateless: they follow a functional programming approach in which the parameters are separate from the model and are passed as inputs to the forward pass along with the data.\n\n\nExample: PyTorch vs Flax\nFlax, being built on JAX, it requires functionally pure functions and thus stateless models.\nHere is a comparison of the approach taken by PyTorch (stateful) vs Flax (stateless) to define and initialize a model (simplified model and workflow to show the principle):\n\nPyTorchFlax\n\n\nThis is how PyTorch works:\nimport torch\nimport torch.nn as nn\n\n# we create a subclass of torch.nn.Module\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.dense1 = nn.Linear(4, 144)\n        self.dense2 = nn.Linear(144, 4)\n\n    def forward(self, x):\n        x = self.dense1(x)\n        x = F.relu(x)\n        x = self.dense2(x)\n        return x\n\n# Create model instance\nmodel = Net()\n\n# Random data and labels\ndata = torch.empty((4, 12, 12, 1))\nlabels = torch.randn((4, 12, 12, 1))\nDuring the forward pass, only the inputs are passed through the model, but of course the outputs depend on the inputs and on the state of the model.\n\n\nHere is the Flax approach:\nimport jax\nimport jax.numpy as jnp\nfrom jax import random\nfrom flax import linen as nn\nimport optax\n\nSetup syntaxCompact syntax\n\n\nFlax provides a setup syntax of model definition which will look more familiar to PyTorch users:\n# Create a subclass of torch.nn.Module\nclass Net(nn.Module):\n  def setup(self):\n    self.dense1 = nn.Dense(12)\n    self.dense2 = nn.Dense(1)\n\n  def __call__(self, x):\n    x = self.dense1(x)\n    x = nn.relu(x)\n    x = self.dense2(x)\n    return x\n\n\nFlax comes with a compact syntax of model definition which is equivalent to the setup syntax in all respect except style:\n# Create a subclass of torch.nn.Module\nclass Net(nn.Module):\n  @nn.compact\n  def __call__(self, x):\n    x = nn.Dense(12, name=\"dense1\")(x)\n    x = nn.relu(x)\n    x = nn.Dense(12, name=\"dense2\")(x)\n    return x\n\n\n\nThe parameters are not part of the model. You initialize them afterwards and create a parameter object:\n# Create model instance\nmodel = Net()\n\n# Random data and labels\nkey, subkey1, subkey2 = random.split(random.key(13), 3)\ndata = jnp.empty((4, 12, 12, 1))\nlabels = random.normal(subkey1, (4, 12, 12, 1))\n\n# Initialize model parameters\nparams = model.init(subkey2, data)\n\n\n\nSimilarly, here are the stateful and stateless approaches to train the model:\n\nPyTorchFlax\n\n\n# Forward pass\nlogits = model(data)\n\nloss = nn.CrossEntropyLoss(logits, labels)\n\n# Calculate gradients\nloss.backward()\n\n# Optimze parameters\noptimizer.step()\n\n\n# Forward pass\ndef loss_func(params, data):\n    logits = model.apply(params, data)\n    loss = optax.softmax_cross_entropy(logits, labels).mean()\n    return loss\n\n# Calculate gradients\ngrads = jax.grad(loss_func)(params)\n\n# Optimze parameters\nparams = state.apply_gradients(grads)\nThe parameters are passed as inputs, along with the data, during the forward pass.",
    "crumbs": [
      "AI",
      "<b><em>Flax</em></b>",
      "Flax's handling of model states"
    ]
  },
  {
    "objectID": "ai/jx/fl_state.html#flax-training-state",
    "href": "ai/jx/fl_state.html#flax-training-state",
    "title": "Flax’s handling of model states",
    "section": "Flax training state",
    "text": "Flax training state\nThe demo above is freed from a lot of model complexity and is not realistic.",
    "crumbs": [
      "AI",
      "<b><em>Flax</em></b>",
      "Flax's handling of model states"
    ]
  },
  {
    "objectID": "ai/jx/jx_accelerator.html",
    "href": "ai/jx/jx_accelerator.html",
    "title": "Accelerators",
    "section": "",
    "text": "One of the efficiencies of JAX is its use of accelerators. In this section, we can see how easily this is done.",
    "crumbs": [
      "AI",
      "<b><em>JAX</em></b>",
      "Accelerators"
    ]
  },
  {
    "objectID": "ai/jx/jx_accelerator.html#auto-detection",
    "href": "ai/jx/jx_accelerator.html#auto-detection",
    "title": "Accelerators",
    "section": "Auto-detection",
    "text": "Auto-detection\nOne of the convenience of the XLA used by JAX (and TensorFlow) is that the same code runs on any device without modification.\n\nThis is in contrast with PyTorch where tensors are created on the CPU by default and can be moved to the GPU using the .to method.\nOr tensors need to be created explicitly on a device (e.g. for GPU, x = torch.ones(2, 4, device='cuda')).\nAlternatively, the code can be made more robust with the creation of a device handle which will allow it to run without modification on CPU or GPU:\nimport torch\n\ndevice = (\n    \"cuda\"\n    if torch.cuda.is_available()\n    else \"mps\"\n    if torch.backends.mps.is_available()\n    else \"cpu\"\n)\n\nx = torch.ones(2, 4, device=device)\n\nMPS = Apple Metal Performance Shaders (GPU on MacOS).\n\nAnd there are methods to run PyTorch on TPU with the torch_xla package, with tricks of scalability.",
    "crumbs": [
      "AI",
      "<b><em>JAX</em></b>",
      "Accelerators"
    ]
  },
  {
    "objectID": "ai/jx/jx_accelerator.html#interactive-job-with-a-gpu",
    "href": "ai/jx/jx_accelerator.html#interactive-job-with-a-gpu",
    "title": "Accelerators",
    "section": "Interactive job with a GPU",
    "text": "Interactive job with a GPU\nThe Alliance wiki documents how to use GPUs on Alliance clusters.\nFor now, let’s relinquish our current interactive job. It is important not to run nested jobs by running salloc inside a running job.\nKill the current job by running (from Bash, not from ipython):\nexit\nThen, start an interactive job with a GPU:\nsalloc --time=1:0:0 --gpus-per-node=1 --mem=22000M\nReload the ipython module:\nmodule load ipython-kernel/3.11\nRe-activate the virtual python environment:\nsource /project/60055/env/bin/activate\nFinally, relaunch IPython:\nipython",
    "crumbs": [
      "AI",
      "<b><em>JAX</em></b>",
      "Accelerators"
    ]
  },
  {
    "objectID": "ai/jx/jx_accelerator.html#effect-on-timing",
    "href": "ai/jx/jx_accelerator.html#effect-on-timing",
    "title": "Accelerators",
    "section": "Effect on timing",
    "text": "Effect on timing\nHere is an example of the difference that a GPU makes compared to CPUs for a simple computation.\nThe following times are on my laptop which has one dedicated GPU.\nFirst, let’s set things up:\nimport jax. numpy as jnp\nfrom jax import random, device_put\nimport numpy as np\n\nseed = 0\nkey = random.PRNGKey(seed)\nkey, subkey = random.split(key)\n\nsize = 3000\nNow, let’s time a dot product of two arrays using NumPy (which only uses CPUs):\nx_np = np.random.normal(size=(size, size)).astype(np.float32)\n%timeit np.dot(x_np, x_np.T)\n58.6 ms ± 2.67 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\nWe can use the NumPy ndarrays in a JAX dot product function:\n%timeit jnp.dot(x_np, x_np.T).block_until_ready()\n31.1 ms ± 1.82 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\nRemember that whenever you benchmark JAX computations, you need to use the block_until_ready method, due to asynchronous dispatch, to ensure that you are timing the computation and not the creation of a future.\n\nIf you want to use NumPy ndarrays in JAX and you have accelerators available, a much better approach is to transfer them to the accelerators with the device_put method:\nx_to_gpu = device_put(x_np)\n%timeit jnp.dot(x_to_gpu, x_to_gpu.T).block_until_ready()\n13.2 ms ± 27.9 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\nThis is much faster and similar to the full JAX code would be:\nx_jx = random.normal(key, (size, size), dtype=jnp.float32)\n\n%timeit jnp.dot(x_jx, x_jx.T).block_until_ready()\n13.3 ms ± 33.3 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)",
    "crumbs": [
      "AI",
      "<b><em>JAX</em></b>",
      "Accelerators"
    ]
  },
  {
    "objectID": "ai/jx/jx_benchmark.html",
    "href": "ai/jx/jx_benchmark.html",
    "title": "Benchmarking JAX code",
    "section": "",
    "text": "You have to be careful when benchmarking JAX code to actually measure the computation time and not the dispatch time.",
    "crumbs": [
      "AI",
      "<b><em>JAX</em></b>",
      "Benchmarking JAX code"
    ]
  },
  {
    "objectID": "ai/jx/jx_benchmark.html#asynchronous-dispatch",
    "href": "ai/jx/jx_benchmark.html#asynchronous-dispatch",
    "title": "Benchmarking JAX code",
    "section": "Asynchronous dispatch",
    "text": "Asynchronous dispatch\nOne of the efficiencies of JAX is its use of asynchronous execution.\nLet’s consider the code:\nimport jax.numpy as jnp\nfrom jax import random\n\nkey = random.PRNGKey(11)\nkey, subkey1, subkey2 = random.split(key, 3)\n\nx = random.normal(subkey1, (1000, 1000))\ny = random.normal(subkey2, (1000, 1000))\n\nz = jnp.dot(x, y)\nInstead of having to wait for the computation to complete before control returns to Python, this computation is dispatched to an accelerator and a future is created. This future is a jax.Array and can be passed to further computations immediately.\nOf course, if you print the result or convert it to a NumPy ndarray, JAX forces Python to wait for the result of the computation.",
    "crumbs": [
      "AI",
      "<b><em>JAX</em></b>",
      "Benchmarking JAX code"
    ]
  },
  {
    "objectID": "ai/jx/jx_benchmark.html#consequence-for-benchmarking",
    "href": "ai/jx/jx_benchmark.html#consequence-for-benchmarking",
    "title": "Benchmarking JAX code",
    "section": "Consequence for benchmarking",
    "text": "Consequence for benchmarking\nTiming jnp.dot(x, y) would not give us the time it takes for the computation to take place, but the time it takes to dispatch the computation.\nOn my laptop which has one dedicated GPU I get:\n%timeit jnp.dot(x, y)\n496 µs ± 948 ns per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n\n%timeit is an IPython built-in magic command. In Python, you would have to use the timeit module.\n\nTo get a proper timing, we need to make sure that the future is resolved using the block_until_ready method:\nOn the same machine:\n%timeit jnp.dot(x, y).block_until_ready()\n598 µs ± 10.4 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\nThe difference here is not huge because the GPU executes the matrix multiplication rapidly. Nevertheless, this is the true timing. If you benchmark your JAX code, make sure to do it this way.\n\nIf you are running small computations such as this one without accelerator, the computation will be dispatched on the thread running the Python process because the overhead of the asynchronous execution would be larger than the speedup you would gain from it. This means that, if you are running the above code on CPUs, you should get the same time with and without block_until_ready.\nNevertheless, because it is difficult to predict when the dispatch will be asynchronous, you should always use block_until_ready in your benchmarks.",
    "crumbs": [
      "AI",
      "<b><em>JAX</em></b>",
      "Benchmarking JAX code"
    ]
  },
  {
    "objectID": "ai/jx/jx_jit.html",
    "href": "ai/jx/jx_jit.html",
    "title": "JIT compilation",
    "section": "",
    "text": "JIT compilation is a key component to JAX efficiency. For the most part, it is very easy to use, but there are subtleties to be aware of.",
    "crumbs": [
      "AI",
      "<b><em>JAX</em></b>",
      "JIT compilation"
    ]
  },
  {
    "objectID": "ai/jx/jx_jit.html#jit",
    "href": "ai/jx/jx_jit.html#jit",
    "title": "JIT compilation",
    "section": "JIT",
    "text": "JIT\nJAX functions are already compiled and optimized, but user functions can also be optimized for the XLA by JIT compilation which will combine computations.\nRemember the map of JAX functioning:\n\n\n\n\n\n\n\n\n\ntracer\n\nTracing\n\n\n\njaxpr\n\nJaxpr\n(JAX expression)\nintermediate\nrepresentation\n(IR)\n\n\n\ntracer-&gt;jaxpr\n\n\n\n\n\njit\n\nJIT\n compilation \n\n\n\nhlo\n\nHigh-level\noptimized (HLO)\nprogram\n\n\n\njit-&gt;hlo\n\n\n\n\n\nxla\n\nAccelerated\n Linear Algebra \n(XLA)\n\n\n\nCPU\n\nCPU\n\n\n\nxla-&gt;CPU\n\n\n\n\n\nGPU\n\nGPU\n\n\n\nxla-&gt;GPU\n\n\n\n\n\nTPU\n\nTPU\n\n\n\nxla-&gt;TPU\n\n\n\n\n\ntransform\n\n Transformations \n\n\n\npy\n\nPure Python\nfunctions\n\n\n\npy-&gt;tracer\n\n\n\n\njaxpr-&gt;jit\n\n\n\n\njaxpr-&gt;transform\n\n\n\n\n\n\nhlo-&gt;xla\n\n\n\n\n\n\n\n\n\nThis is done by the jax.jit() function or the equivalent decorator @jit.\nLet’s consider this code:\nimport jax.numpy as jnp\nfrom jax import jit\nfrom jax import random\n\nkey = random.PRNGKey(8)\nkey, subkey1, subkey2 = random.split(key, 3)\n\na = random.normal(subkey1, (500, 500))\nb = random.normal(subkey2, (500, 500))\n\ndef sum_squared_error(a, b):\n    return jnp.sum((a-b)**2)\nOur function can simply be used as:\nprint(sum_squared_error(a, b))\nOur code will run faster if we create a JIT compiled version and use that instead (we will see how to benchmark JAX code later in the course. There are some subtleties for that too, so for now, just believe that it is faster. You will be able to test it later):\nsum_squared_error_jit = jit(sum_squared_error)\nprint(sum_squared_error_jit(a, b))\n502084.75\nAlternatively, this can be written as:\nprint(jit(sum_squared_error)(a, b))\n502084.75\nOr as:\n@jit\ndef sum_squared_error(a, b):\n    return jnp.sum((a - b) ** 2)\n\nprint(sum_squared_error(a, b))\n502084.75\n\nUnderstanding jaxprs\nLet’s have a look at the jaxpr of a jit-compiled function.\nThis is what the jaxpr of the non-jit-compiled function looks like:\nimport jax\n\ndef sum_squared_error(a, b):\n    return jnp.sum((a - b) ** 2)\n\njax.make_jaxpr(sum_squared_error)(x, y)\n{ lambda ; a:f32[3] b:f32[3]. let\n    c:f32[3] = sub a b\n    d:f32[3] = integer_pow[y=2] c\n    e:f32[] = reduce_sum[axes=(0,)] d\n  in (e,) }\nThe jaxpr of the jit-compiled function looks like this:\n@jit\ndef sum_squared_error(a, b):\n    return jnp.sum((a - b) ** 2)\n\njax.make_jaxpr(sum_squared_error)(x, y)\n{ lambda ; a:f32[3] b:f32[3]. let\n    c:f32[] = pjit[\n      name=sum_squared_error\n      jaxpr={ lambda ; d:f32[3] e:f32[3]. let\n          f:f32[3] = sub d e\n          g:f32[3] = integer_pow[y=2] f\n          h:f32[] = reduce_sum[axes=(0,)] g\n        in (h,) }\n    ] a b\n  in (c,) }",
    "crumbs": [
      "AI",
      "<b><em>JAX</em></b>",
      "JIT compilation"
    ]
  },
  {
    "objectID": "ai/jx/jx_jit.html#jit-constraints",
    "href": "ai/jx/jx_jit.html#jit-constraints",
    "title": "JIT compilation",
    "section": "JIT constraints",
    "text": "JIT constraints\nUsing jit in the example above was very easy. There are situations however in which tracing will fail.\n\nControl flow\nOne example can arise with control flow:\n@jit\ndef cond_func(x):\n    if x &lt; 0.0:\n        return x ** 2.0\n    else:\n        return x ** 3.0\n\nprint(cond_func(1.0))\njax.errors.TracerBoolConversionError: Attempted boolean conversion of traced array with shape bool[]..\nThe error occurred while tracing the function cond_func at jx_jit.qmd:85 for jit. This concrete value was not available in Python because it depends on the value of the argument x.\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.TracerBoolConversionError\nJIT compilation uses tracing of the code based on shape and dtype so that the same compiled code can be reused for new values with the same characteristics. The tracer objects are not real values but abstract representation that are more general. In control flow situations such as the one we have here, an abstract general value does not work as it wouldn’t know which branch to take.\n\nStatic variables\nOne solution is to tell jit() to exclude the problematic arguments (in our example the argument: x) from tracing (i.e. to consider them as static). Of course, those elements will not be optimized, but the rest of the code will, so it is a lot better than not JIT compiling the function at all.\nYou can either use the static_argnums parameter which takes an integer or a collection of integers to specify the position of the arguments to treat as static:\ndef cond_func(x):\n    if x &lt; 0.0:\n        return x ** 2.0\n    else:\n        return x ** 3.0\n\ncond_func_jit = jit(cond_func, static_argnums=(0,))\n\nprint(cond_func_jit(2.0))\nprint(cond_func_jit(-2.0))\n8.0\n4.0\nOr you can use static_argnames which accepts argument names:\ndef cond_func(x):\n    if x &lt; 0.0:\n        return x ** 2.0\n    else:\n        return x ** 3.0\n\ncond_func_jit_alt = jit(cond_func, static_argnames='x')\n\nprint(cond_func_jit_alt(2.0))\nprint(cond_func_jit_alt(-2.0))\n8.0\n4.0\nYou cannot use the @jit decorator when you need to pass arguments to the jit function, but you can still use a decorator:\nfrom functools import partial\n\n@partial(jit, static_argnums=(0,))\ndef cond_func(x):\n    if x &lt; 0.0:\n        return x ** 2.0\n    else:\n        return x ** 3.0\nor:\n@partial(jit, static_argnames=['x'])\ndef cond_func(x):\n    if x &lt; 0.0:\n        return x ** 2.0\n    else:\n        return x ** 3.0\n\n\nControl flow primitives\nIf you don’t want the code to recompile for each new value, another solution, is to use one of the structured control flow primitives:\nfrom jax import lax\n\nlax.cond(False, lambda x: x ** 2.0, lambda x: x ** 3.0, jnp.array([2.]))\nArray([8.], dtype=float32)\nlax.cond(True, lambda x: x ** 2.0, lambda x: x ** 3.0, jnp.array([-2.]))\nArray([4.], dtype=float32)\nThere are other control flow primitives:\n\nlax.while_loop\nlax.fori_loop\nlax.scan\n\nand other pseudo dynamic control flow functions:\n\nlax.select (NumPy API jnp.where and jnp.select)\nlax.switch (NumPy API jnp.piecewise)\n\n\n\n\nStatic operations\nSimilarly, you will need to mark problematic operations as static so that they don’t get traced during JIT compilation:\n@jit\ndef f(x):\n    return x.reshape(jnp.array(x.shape).prod())\n\nx = jnp.ones((2, 3))\nprint(f(x))\nTypeError: Shapes must be 1D sequences of concrete values of integer type, got [Traced&lt;ShapedArray(int32[])&gt;with&lt;DynamicJaxprTrace(level=1/0)&gt;].\nIf using `jit`, try using `static_argnums` or applying `jit` to smaller subfunctions.\nThe problem here is that the shape of the argument to jnp.reshape is traced while it needs to be static.\nOne solution is to use the NumPy version of prod which will not create a traced result:\nimport numpy as np\n\n@jit\ndef f(x):\n    return x.reshape(np.prod(x.shape))\n\nprint(f(x))\n[1. 1. 1. 1. 1. 1.]",
    "crumbs": [
      "AI",
      "<b><em>JAX</em></b>",
      "JIT compilation"
    ]
  },
  {
    "objectID": "ai/jx/jx_libraries.html",
    "href": "ai/jx/jx_libraries.html",
    "title": "Libraries on top of JAX",
    "section": "",
    "text": "JAX is an efficient and flexible framework for function transformations (including automatic differentiation) built to run on accelerators. Its goal is not to develop specialized applications, but to focus on the chore of the language.\nWhile it is possible to use JAX directly in application (e.g. to build a NN from scratch), it makes sense to use specialized libraries that are built on top of JAX, that make use of its characteristics, and provide functions for specialized applications.\nHere are a few important such libraries.",
    "crumbs": [
      "AI",
      "<b><em>JAX</em></b>",
      "Libraries on top of JAX"
    ]
  },
  {
    "objectID": "ai/jx/jx_libraries.html#neural-networks",
    "href": "ai/jx/jx_libraries.html#neural-networks",
    "title": "Libraries on top of JAX",
    "section": "Neural networks",
    "text": "Neural networks\nFlax is an NN library initially developed by Google Brain and now by Google DeepMind. It is the deep learning library officially recommended by the JAX developers.\nHaiku was the initial library developed by Google DeepMind. While it is still maintained, development has been stopped in favour of Flax.\nEquinox is another DL library, relying on models as pytrees. While its syntax is a lot more user-friendly and familiar to PyTorch users, it has limitations.\n\nIt is worth noting that PyTorch is attempting to incorporate JAX’s ideas with a new library under development, functorch.\n\nOptax is a gradient manipulation and optimization library developed by Google DeepMind.",
    "crumbs": [
      "AI",
      "<b><em>JAX</em></b>",
      "Libraries on top of JAX"
    ]
  },
  {
    "objectID": "ai/jx/jx_libraries.html#probabilistic-state-space-models",
    "href": "ai/jx/jx_libraries.html#probabilistic-state-space-models",
    "title": "Libraries on top of JAX",
    "section": "Probabilistic state space models",
    "text": "Probabilistic state space models\nDynamax provides state and parameter estimation for, among others:\n\nhidden markov models,\nlinear gaussian state space models,\nnonlinear gaussian state space models,\ngeneralized gaussian state space models.",
    "crumbs": [
      "AI",
      "<b><em>JAX</em></b>",
      "Libraries on top of JAX"
    ]
  },
  {
    "objectID": "ai/jx/jx_numpy.html",
    "href": "ai/jx/jx_numpy.html",
    "title": "Relation to NumPy",
    "section": "",
    "text": "NumPy is a popular Python scientific API at the core of many libraries. JAX uses a NumPy-inspired API. There are however important differences that we will explore in this section.",
    "crumbs": [
      "AI",
      "<b><em>JAX</em></b>",
      "Relation to NumPy"
    ]
  },
  {
    "objectID": "ai/jx/jx_numpy.html#a-numpy-inspired-api",
    "href": "ai/jx/jx_numpy.html#a-numpy-inspired-api",
    "title": "Relation to NumPy",
    "section": "A NumPy-inspired API",
    "text": "A NumPy-inspired API\nNumPy being so popular, JAX comes with a convenient high-level wrapper to NumPy: jax.numpy.\n\nBeing familiar with NumPy is thus an advantage to get started with JAX. The NumPy quickstart is a useful resource.\n\n\nFor a more efficient usage, JAX also comes with a lower-level API: jax.lax.\n\n\nNumPyJAX NumPy\n\n\n\nimport numpy as np\n\n\nprint(np.array([(1, 2, 3), (4, 5, 6)]))\n\n[[1 2 3]\n [4 5 6]]\n\n\n\nprint(np.zeros((2, 3)))\n\n[[0. 0. 0.]\n [0. 0. 0.]]\n\n\n\nprint(np.ones((2, 3, 2)))\n\n[[[1. 1.]\n  [1. 1.]\n  [1. 1.]]\n\n [[1. 1.]\n  [1. 1.]\n  [1. 1.]]]\n\n\n\nprint(np.arange(24).reshape(2, 3, 4))\n\n[[[ 0  1  2  3]\n  [ 4  5  6  7]\n  [ 8  9 10 11]]\n\n [[12 13 14 15]\n  [16 17 18 19]\n  [20 21 22 23]]]\n\n\n\nprint(np.linspace(0, 2, 9))\n\n[0.   0.25 0.5  0.75 1.   1.25 1.5  1.75 2.  ]\n\n\n\nprint(np.linspace(0, 2, 9)[::-1])\n\n[2.   1.75 1.5  1.25 1.   0.75 0.5  0.25 0.  ]\n\n\n\n\nimport jax.numpy as jnp\nprint(jnp.array([(1, 2, 3), (4, 5, 6)]))\n[[1 2 3]\n [4 5 6]]\nprint(jnp.zeros((2, 3)))\n[[0. 0. 0.]\n [0. 0. 0.]]\nprint(jnp.ones((2, 3, 2)))\n[[[1. 1.]\n  [1. 1.]\n  [1. 1.]]\n\n [[1. 1.]\n  [1. 1.]\n  [1. 1.]]]\nprint(jnp.arange(24).reshape(2, 3, 4))\n[[[ 0  1  2  3]\n  [ 4  5  6  7]\n  [ 8  9 10 11]]\n\n [[12 13 14 15]\n  [16 17 18 19]\n  [20 21 22 23]]]\nprint(jnp.linspace(0, 2, 9))\n[0.   0.25 0.5  0.75 1.   1.25 1.5  1.75 2.  ]\nprint(jnp.linspace(0, 2, 9)[::-1])\n[2.   1.75 1.5  1.25 1.   0.75 0.5  0.25 0.  ]\n\n\n\nDespite the similarities, there are important differences between JAX and NumPy.",
    "crumbs": [
      "AI",
      "<b><em>JAX</em></b>",
      "Relation to NumPy"
    ]
  },
  {
    "objectID": "ai/jx/jx_numpy.html#differences-with-numpy",
    "href": "ai/jx/jx_numpy.html#differences-with-numpy",
    "title": "Relation to NumPy",
    "section": "Differences with NumPy",
    "text": "Differences with NumPy\n\nDifferent types\ntype(np.zeros((2, 3))) == type(jnp.zeros((2, 3)))\nFalse\n\nNumpyJAX NumPy\n\n\n\ntype(np.zeros((2, 3)))\n\nnumpy.ndarray\n\n\n\n\ntype(jnp.zeros((2, 3)))\njaxlib.xla_extension.ArrayImpl\n\n\n\n\n\nDifferent default data types\n\nNumpyJAX NumPy\n\n\n\nnp.zeros((2, 3)).dtype\n\ndtype('float64')\n\n\n\n\njnp.zeros((2, 3)).dtype\ndtype('float32')\n\n\n\n\nLower numerical precision improves speed and reduces memory usage at no cost while training neural networks and is thus a net benefit. Having been built with deep learning in mind, JAX defaults align with that of other DL libraries (e.g. PyTorch, TensorFlow).\n\n\n\nImmutable arrays\n\nNumpyJAX NumPy\n\n\nIn NumPy, you can modify ndarrays:\n\na = np.arange(5)\na[0] = 9\nprint(a)\n\n[9 1 2 3 4]\n\n\n\n\nJAX arrays are immutable:\na = jnp.arange(5)\na[0] = 9\nTypeError: '&lt;class 'jaxlib.xla_extension.ArrayImpl'&gt;' object does not support item assignment. JAX arrays are immutable. Instead of ``x[idx] = y``, use ``x = x.at[idx].set(y)`` or another .at[] method: https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.ndarray.at.html\nInstead, you need to create a copy of the array with the mutation. This is done with:\nb = a.at[0].set(9)\nprint(b)\n[9 1 2 3 4]\nOf course, you can overwrite a:\na = a.at[0].set(9)\n\n\n\n\n\nPseudorandom number generation\nProgramming languages usually come with automated pseudorandom number generator (PRNG) based on nondeterministic data from the operating system. They are extremely convenient, but slow, based on repeats, and problematic in parallel executions.\nJAX relies on an explicitly set random state called a key.\nfrom jax import random\n\nkey = random.PRNGKey(18)\nprint(key)\n[ 0 18]\nEach time you call a random function, you need a subkey split from your key. Keys should only ever be used once in your code. The key is what makes your code reproducible, but you don’t want to reuse it within your code as it would create spurious correlations.\nHere is the workflow:\n\nyou split your key into a new key and one or multiple subkeys,\nyou discard the old key (because it was used to do the split—so its entropy budget, so to speak, has been used),\nyou use the subkey(s) to run your random function(s) and keep the new key for a future split.\n\n\nSubkeys are of the same nature as keys. This is just a terminology.\n\nTo make sure not to reuse the old key, you can overwrite it by the new one:\nkey, subkey = random.split(key)\nprint(key)\n[4197003906 1654466292]\n\nThat’s the value of our new key for future splits.\n\nprint(subkey)\n[1685972163 1654824463]\n\nThis is the value of the subkey that we can use to call a random function.\n\nLet’s use that subkey now:\nprint(random.normal(subkey))\n1.1437175\n\nTo split your key into more subkeys, pass an argument to random.split:\nkey, subkey1, subkey2, subkey3 = random.split(key, 4)\n\n\n\nStrict input control\n\nNumpyJAX NumPy\n\n\nNumPy’s fundamental object is the ndarray, but NumPy is very tolerant as to the type of input.\n\nnp.sum([1.0, 2.0])  # here we are using a list\n\n3.0\n\n\n\nnp.sum((1.0, 2.0))  # here is a tuple\n\n3.0\n\n\n\n\nTo avoid inefficiencies, JAX will only accept arrays.\njnp.sum([1.0, 2.0])\nTypeError: sum requires ndarray or scalar arguments, got &lt;class 'list'&gt; at position 0.\njnp.sum((1.0, 2.0))\nTypeError: sum requires ndarray or scalar arguments, got &lt;class 'tuple'&gt; at position 0.\n\n\n\n\n\nOut of bounds indexing\n\nNumpyJAX NumPy\n\n\nNumPy will warn you with an error message if you try to index out of bounds:\n\nprint(np.arange(5)[10])\n\nIndexError: index 10 is out of bounds for axis 0 with size 5\n\n\n\n\nBe aware that JAX will not raise an error. Instead, it will silently return the closest boundary:\nprint(jnp.arange(5)[10])\n4\n\n\n\n\n\nFunctionally pure functions\nMore importantly, only functionally pure functions—that is, functions for which the outputs are only based on the inputs and which have no side effects—can be used with JAX.\n\nOutputs only based on inputs\nConsider the function:\ndef f(x):\n    return a + x\nwhich uses the variable a from the global environment.\nThis function is not functionally pure because the outputs (the results of the function) do not solely depend on the arguments (the values given to x) passed to it. They also depend on the value of a.\nRemember how tracing works: new inputs with the same shape and dtype use the cached compiled program directly. If the value of a changes in the global environment, a new tracing is not triggered and the cached compiled program uses the old value of a (the one that was used during tracing).\nIt is only if the code is run on an input x with a different shape and/or dtype that tracing happens again and that the new value for a takes effect.\n\nTo demo this, we need to use JIT compilation that we will explain in a later section.\n\nfrom jax import jit\n\na = jnp.ones(3)\nprint(a)\n[1. 1. 1.]\ndef f(x):\n    return a + x\n\nprint(jit(f)(jnp.ones(3)))\n[2. 2. 2.]\n\nAll good here because this is the first run (tracing).\n\nNow, let’s change the value of a to an array of zeros:\na = jnp.zeros(3)\nprint(a)\n[0. 0. 0.]\nAnd rerun the same code:\nprint(jit(f)(jnp.ones(3)))\n[2. 2. 2.]\nWe should have an array of ones, but we get the same result we got earlier. Why? because we are running a cached program with the value that a had during tracing.\nThe new value for a will only take effect if we re-trigger tracing by changing the shape and/or dtype of x:\na = jnp.zeros(4)\nprint(a)\n[0. 0. 0. 0.]\nprint(jit(f)(jnp.ones(4)))\n[1. 1. 1. 1.]\nPassing an argument of a different shape to f forced recompilation. Using a different data type (e.g. with jnp.arange(3)) would have done the same.\n\n\nNo side effects\nA function is said to have a side effect if it changes something outside of its local environment (if it does anything beside returning an output).\nExamples of side effects include:\n\nprinting to standard output/shell,\nreading from file/writing to file,\nmodifying a global variable.\n\nIn JAX, the side effects will happen during the first run (tracing), but will not happen on subsequent runs. You thus cannot rely on side effects in your code.\ndef f(a, b):\n    print(\"Calculating sum\")\n    return a + b\n\nprint(jit(f)(jnp.arange(3), jnp.arange(3)))\nCalculating sum\n[0 2 4]\n\nPrinting (the side effect) happened here because this is the first run.\n\nLet’s rerun the function:\nprint(jit(f)(jnp.arange(3), jnp.arange(3)))\n[0 2 4]\nThis time, no printing.\n\nUnderstanding jaxprs\nJaxprs are created by tracers wrapping the Python code during compilation (the first run). They contain information on the shape and data type of arrays as well as the operations performed on these arrays. Jaxprs do not however contain information on values: this allows the compiled program to be general enough to be rerun with any new arrays of the same shape and data type without having to rerun the slow Python code and recompile.\nJaxprs also do not contain any information on elements that are not part of the inputs such as external variables, nor do they contain information on side effects.\nJaxprs can be visualized with the jax.make_jaxpr function:\nimport jax\n\nx = jnp.array([1., 4., 3.])\ny = jnp.array([8., 1., 2.])\n\ndef f(x, y):\n    return 2 * x**2 + y\n\njax.make_jaxpr(f)(x, y) \n{ lambda ; a:f32[3] b:f32[3]. let\n    c:f32[3] = integer_pow[y=2] a\n    d:f32[3] = mul 2.0 c\n    e:f32[3] = add d b\n  in (e,) }\nLet’s add a print function to f:\ndef f(x, y):\n    print(\"This is a function with side-effect\")\n    return 2 * x**2 + y\n\njax.make_jaxpr(f)(x, y)\n{ lambda ; a:f32[3] b:f32[3]. let\n    c:f32[3] = integer_pow[y=2] a\n    d:f32[3] = mul 2.0 c\n    e:f32[3] = add d b\n  in (e,) }\nThe jaxpr is exactly the same. This is why printing will happen during tracing (when the Python code is run), but not afterwards (when the compiled code using the jaxpr is run).",
    "crumbs": [
      "AI",
      "<b><em>JAX</em></b>",
      "Relation to NumPy"
    ]
  },
  {
    "objectID": "ai/jx/jx_numpy.html#why-the-constraints",
    "href": "ai/jx/jx_numpy.html#why-the-constraints",
    "title": "Relation to NumPy",
    "section": "Why the constraints?",
    "text": "Why the constraints?\nThe more constraints you add to a programming language, the more optimization you can get from the compiler. Speed comes at the cost of convenience.\nFor instance, consider a Python list. It is an extremely convenient and flexible object: heterogeneous, mutable… You can do anything with it. But computations on lists are extremely slow.\nNumPy’s ndarrays are more constrained (homogeneous), but the type constraint permits the creation of a much faster language (NumPy is written in C and Fortran as well as Python) with vectorization, optimizations, and a greatly improved performance.\nJAX takes it further: by using an intermediate representation and very strict constraints on type, pure functional programming, etc., yet more optimizations can be achieved and you can optimize your own functions with JIT compilation and the XLA. Ultimately, this is what makes JAX so fast.",
    "crumbs": [
      "AI",
      "<b><em>JAX</em></b>",
      "Relation to NumPy"
    ]
  },
  {
    "objectID": "ai/jx/jx_pallas.html",
    "href": "ai/jx/jx_pallas.html",
    "title": "Pallas",
    "section": "",
    "text": "tracer\n\nTracing\n\n\n\njaxpr\n\nJaxprs\n(JAX expressions)\nintermediate\nrepresentation\n(IR)\n\n\n\ntracer-&gt;jaxpr\n\n\n\n\n\njit\n\n Just-in-time \n(JIT)\ncompilation\n\n\n\nhlo\n\nHigh-level\noptimized (HLO)\nprogram\n\n\n\njit-&gt;hlo\n\n\n\n\n\ntriton\n\nTriton\n\n\n\nGPU\n\nGPU\n\n\n\ntriton-&gt;GPU\n\n\n\n\n\nmosaic\n\nMosaic\n\n\n\nTPU\n\nTPU\n\n\n\nmosaic-&gt;TPU\n\n\n\n\n\ntransform\n\nVectorization\nParallelization\n   Differentiation  \n\n\n\npy\n\nPure Python\nfunctions\n\n\n\npy-&gt;tracer\n\n\n\n\njaxpr-&gt;jit\n\n\n\n\njaxpr-&gt;transform\n\n\n\n\n\n\nhlo-&gt;triton\n\n\n\n\nhlo-&gt;mosaic"
  },
  {
    "objectID": "ai/jx/jx_pytree.html",
    "href": "ai/jx/jx_pytree.html",
    "title": "Pytrees",
    "section": "",
    "text": "It is convenient to store data, model parameters, gradients, etc. in container structures such as lists or dicts. JAX has a container-like structure, the pytree that is flexible, can be nested, and is supported by many JAX functions, making for convenient workflows.\nThis section introduces pytrees and their functioning.",
    "crumbs": [
      "AI",
      "<b><em>JAX</em></b>",
      "Pytrees"
    ]
  },
  {
    "objectID": "ai/jx/jx_pytree.html#a-tree-like-structure",
    "href": "ai/jx/jx_pytree.html#a-tree-like-structure",
    "title": "Pytrees",
    "section": "A tree-like structure",
    "text": "A tree-like structure\nThe pytree container registry contains, by default, lists, tuples, and dicts. It can be extended to other containers.\nObjects in the pytree container registry are pytrees. Other objects are leaf pytrees (so pytrees are recursive).\nPytrees are great for holding data and parameters, keeping everything organized, even for complex models. The leaves are usually made of arrays. Many JAX functions can be applied to pytrees.\n\nExamples of pytrees:\n\n(1, 2, 3),\n[1, 1., \"string\", True],\njnp.arange(2),\n{'key1': 3.4, 'key2': 6.},\n[3., (1, 2), {'key1': 'val1', 'key2': 'val2', 'key3': 'val3'}],\n(3, 2, (6, 0), 2, ()),\njnp.zeros(3)\n\n\n\n\n\n\nCluster setup\n\n\n\n\n\nLet’s kill our previous interactive job with a GPU:\nexit\nThen start an interactive job with a CPU:\nsalloc --time=2:0:0 --mem-per-cpu=5500M\nLoad the ipython module:\nmodule load ipython-kernel/3.11\nActivate the virtual python environment:\nsource /project/60055/env/bin/activate\nLaunch IPython:\nipython",
    "crumbs": [
      "AI",
      "<b><em>JAX</em></b>",
      "Pytrees"
    ]
  },
  {
    "objectID": "ai/jx/jx_pytree.html#extracting-leaves",
    "href": "ai/jx/jx_pytree.html#extracting-leaves",
    "title": "Pytrees",
    "section": "Extracting leaves",
    "text": "Extracting leaves\nTrees can be flattened and their leaves extracted into a list with jax.tree.leaves:\njax.tree.leaves([3., (1, 2), {'key1': 'val1', 'key2': 'val2', 'key3': 'val3'}])\n[3.0, 1, 2, 'val1', 'val2', 'val3']\nLet’s create a list of pytrees and extract their leaves to look at more examples:\nimport jax\nimport jax.numpy as jnp\n\nlist_trees = [\n    (1, 2, 3),\n    [1, 1., \"string\", True],\n    jnp.arange(2),\n    {'key1': 3.4, 'key2': 6.},\n    [3., (1, 2), {'key1': 'val1', 'key2': 'val2', 'key3': 'val3'}],\n    (3, 2, (6, 0, 9), 2, ()),\n    jnp.zeros(3)\n    ]\n\nfor pytree in list_trees:\n  leaves = jax.tree.leaves(pytree)\n  print(f\"{len(leaves)} leaves: {leaves}\")\n3 leaves: [1, 2, 3]\n4 leaves: [1, 1.0, 'string', True]\n1 leaves: [Array([0, 1], dtype=int32)]\n2 leaves: [3.4, 6.0]\n6 leaves: [3.0, 1, 2, 'val1', 'val2', 'val3']\n5 leaves: [3, 2, 6, 0, 9, 2]\n1 leaves: [Array([0., 0., 0.], dtype=float32)]\n\nBe careful that leaves are not the same as container elements:\n\nwhile an array contains many elements, it is a single leaf,\nwhile a nested list or tuple represent a single element of the parent container, all the elements of nested tuples and lists are leaves,\nan empty tuple or list is a pytree without children and is not counted as a leaf.\n\nContrast this with the length (i.e. the number of elements of containers):\nfor pytree in list_trees:\n  print(f\"{len(pytree)} elements\")\n3 elements\n4 elements\n2 elements\n2 elements\n3 elements\n5 elements\n3 elements",
    "crumbs": [
      "AI",
      "<b><em>JAX</em></b>",
      "Pytrees"
    ]
  },
  {
    "objectID": "ai/jx/jx_pytree.html#structure-of-pytrees",
    "href": "ai/jx/jx_pytree.html#structure-of-pytrees",
    "title": "Pytrees",
    "section": "Structure of pytrees",
    "text": "Structure of pytrees\nAs we just saw, JAX can extract the leaves of pytrees. This is useful to run functions on them. But JAX also records their structure and is able to recreate them. The structure can be obtained with jax.tree.structure:\njax.tree.structure([3., (1, 2), {'key1': 'val1', 'key2': 'val2', 'key3': 'val3'}])\nPyTreeDef([*, (*, *), {'key1': *, 'key2': *, 'key3': *}])\nSo each pytree can be turned into a tuple of the list of its leaves and its structure and that tuple can be turned back into the pytree.\njax.tree.flatten([3., (1, 2), {'key1': 'val1', 'key2': 'val2', 'key3': 'val3'}])\n([3.0, 1, 2, 'val1', 'val2', 'val3'],\n PyTreeDef([*, (*, *), {'key1': *, 'key2': *, 'key3': *}]))\nvalues, structure = jax.tree.flatten(\n    [3., (1, 2), {'key1': 'val1', 'key2': 'val2', 'key3': 'val3'}]\n)\njax.tree.unflatten(structure, values)\n[3.0, (1, 2), {'key1': 'val1', 'key2': 'val2', 'key3': 'val3'}]\nThe path to each leaf can be obtained with jax.tree_util.tree_flatten_with_path:\njax.tree_util.tree_flatten_with_path(\n    [3., (1, 2), {'key1': 'val1', 'key2': 'val2', 'key3': 'val3'}]\n)\n([((SequenceKey(idx=0),), 3.0),\n  ((SequenceKey(idx=1), SequenceKey(idx=0)), 1),\n  ((SequenceKey(idx=1), SequenceKey(idx=1)), 2),\n  ((SequenceKey(idx=2), DictKey(key='key1')), 'val1'),\n  ((SequenceKey(idx=2), DictKey(key='key2')), 'val2'),\n  ((SequenceKey(idx=2), DictKey(key='key3')), 'val3')],\n PyTreeDef([*, (*, *), {'key1': *, 'key2': *, 'key3': *}]))",
    "crumbs": [
      "AI",
      "<b><em>JAX</em></b>",
      "Pytrees"
    ]
  },
  {
    "objectID": "ai/jx/jx_pytree.html#pytree-operations",
    "href": "ai/jx/jx_pytree.html#pytree-operations",
    "title": "Pytrees",
    "section": "Pytree operations",
    "text": "Pytree operations\nJAX can run operations on pytrees. Let’s create a few pytrees to play with:\ntree1 = {'key1': 1., 'key2': 2., 'key3': 3.}\ntree2 = {'key1': 4., 'key2': 5., 'key3': 6.}\ntree3 = {'key1': 7., 'key2': 8., 'key3': 9.}\njax.tree.map allows to apply functions to each leaf of a tree:\njax.tree.map(lambda x: 3 * x, tree1)\n{'key1': 3.0, 'key2': 6.0, 'key3': 9.0}\nAs long as pytrees share the same structure (including the same dicts keys), operations combining multiple pytrees also work:\njax.tree.map(lambda x, y, z: x * y + z, tree1, tree2, tree3)\n{'key1': 11.0, 'key2': 18.0, 'key3': 27.0}\nHere are a few more examples:\ntree4 = [[1, 1, 1], (2, 2, 2, 2), 3]\ntree5 = [[0, 5, 1], (2, 2, 2, 2), 3]\ntree6 = [[0, 5, 1, 2], (2, 2, 2), 3]\njax.tree.map(lambda x, y: x + y, tree4, tree5)\n[[1, 6, 2], (4, 4, 4, 4), 6]\nThis won’t work though as the structures are different:\njax.tree.map(lambda x, y: x + y, tree5, tree6)\nValueError: Tuple arity mismatch: 3 != 4; tuple: (2, 2, 2).",
    "crumbs": [
      "AI",
      "<b><em>JAX</em></b>",
      "Pytrees"
    ]
  },
  {
    "objectID": "ai/jx/jx_pytree.html#pytree-transposition",
    "href": "ai/jx/jx_pytree.html#pytree-transposition",
    "title": "Pytrees",
    "section": "Pytree transposition",
    "text": "Pytree transposition\nA list of pytrees can be transposed into a pytree of lists.\nLet’s create a list with a few of our previous pytrees:\ntrees = [tree1, tree2, tree3]\nprint(trees)\n[{'key1': 1.0, 'key2': 2.0, 'key3': 3.0}, {'key1': 1.0, 'key2': 2.0, 'key3': 3.0}, {'key1': 1.0, 'key2': 2.0, 'key3': 3.0}]\nHere is how to transpose this list of pytrees:\njax.tree.map(lambda *x: list(x), *trees)\n{'key1': [1.0, 1.0, 1.0], 'key2': [2.0, 2.0, 2.0], 'key3': [3.0, 3.0, 3.0]}",
    "crumbs": [
      "AI",
      "<b><em>JAX</em></b>",
      "Pytrees"
    ]
  },
  {
    "objectID": "ai/jx/jx_pytree.html#pytrees-in-nn",
    "href": "ai/jx/jx_pytree.html#pytrees-in-nn",
    "title": "Pytrees",
    "section": "Pytrees in NN",
    "text": "Pytrees in NN\nPytrees are very useful when using JAX for deep learning. Our course on DL with Flax will show this, but below is a basic example modified from the JAX documentation.\nimport jax\nimport jax.numpy as jnp\nfrom jax import random\nThe parameters of a multi-layer perceptron can be initialized with:\ndef init_params(layer_width):\n  params = []\n  key = random.PRNGKey(11)\n  key, subkey = random.split(key)\n  for n_in, n_out in zip(layer_width[:-1], layer_width[1:]):\n    params.append(\n        dict(weights=random.normal(subkey, (n_in, n_out)) * jnp.sqrt(2/n_in),\n             biases=jnp.ones(n_out)\n            )\n    )\n  return params\n\nparams = init_params([1, 128, 128, 1])\nparams is a pytree:\njax.tree.map(lambda x: x.shape, params)\n[{'biases': (128,), 'weights': (1, 128)},\n {'biases': (128,), 'weights': (128, 128)},\n {'biases': (1,), 'weights': (128, 1)}]\nTo train our MLP, we need to define a function for the forward pass:\n@jax.jit\ndef forward(params, x):\n  *hidden, last = params\n  for layer in hidden:\n    x = jax.nn.relu(x @ layer['weights'] + layer['biases'])\n  return x @ last['weights'] + last['biases']\nAnd a loss function:\n@jax.jit\ndef loss_fn(params, x, y):\n  return jnp.mean((forward(params, x) - y) ** 2)\nThen we choose a learning rate and define a function for the backpropagation:\nlr = 0.0001\n\n@jax.jit\ndef update(params, x, y):\n  grads = jax.grad(loss_fn)(params, x, y)\n  return jax.tree.map(\n      lambda p, g: p - lr * g, params, grads\n  )\n\nBecause jax.grad can accept pytrees, we can create a new pytree grads by passing the params pytree to it.\nThe gradient descent can be applied using both pytrees thanks to jax.tree.map.\n\nThen of course we could train our model:\nkey = random.PRNGKey(3)\nkey, subkey = random.split(key)\n\nx = random.normal(subkey, (128, 1))\ny = x ** 2\n\nfor _ in range(1000):\n  params = update(params, x, y)",
    "crumbs": [
      "AI",
      "<b><em>JAX</em></b>",
      "Pytrees"
    ]
  },
  {
    "objectID": "ai/jx/jx_why.html",
    "href": "ai/jx/jx_why.html",
    "title": "Why JAX?",
    "section": "",
    "text": "There are many excellent and popular deep learning frameworks already (e.g. PyTorch). So why did Google—already behind the successful TensorFlow project—start developing JAX?\nIn this section, we will look at the advantages brought by JAX—namely speed and flexible automatic differentiation.",
    "crumbs": [
      "AI",
      "<b><em>JAX</em></b>",
      "Why JAX?"
    ]
  },
  {
    "objectID": "ai/jx/jx_why.html#what-is-jax",
    "href": "ai/jx/jx_why.html#what-is-jax",
    "title": "Why JAX?",
    "section": "What is JAX?",
    "text": "What is JAX?\nJAX is a library for Python developed by Google. Its key data structure is the array. It can perform composition, transformation, and differentiation of numerical programs as well as compilation for CPUs, GPUs, and TPUs.\nIt comes with a NumPy-like API as well as a lower-level API called lax. While the NumPy-like API looks familiar to NumPy users, JAX requires strict functional programming (i.e. functions should only depend on their inputs and should only return outputs).",
    "crumbs": [
      "AI",
      "<b><em>JAX</em></b>",
      "Why JAX?"
    ]
  },
  {
    "objectID": "ai/jx/jx_why.html#a-relatively-new-project",
    "href": "ai/jx/jx_why.html#a-relatively-new-project",
    "title": "Why JAX?",
    "section": "A relatively new project",
    "text": "A relatively new project\nIt is clear that JAX is not a widely adopted project yet.\n\nTrends of Google searches\n\n\n\nAs of October 16, 2023.\n\n\n\n\n\nTrends of Stack Overflow tags\n\n\n\nAs of October 16, 2023.\n\n\n\nSo why JAX?",
    "crumbs": [
      "AI",
      "<b><em>JAX</em></b>",
      "Why JAX?"
    ]
  },
  {
    "objectID": "ai/jx/jx_why.html#jax-is-fast",
    "href": "ai/jx/jx_why.html#jax-is-fast",
    "title": "Why JAX?",
    "section": "JAX is fast",
    "text": "JAX is fast\nJAX was built with performance in mind. Its speed relies on design decisions at all levels.\n\nDefault data type\nLike PyTorch—a popular deep learning library—JAX uses float32 as its default data type. This level of precision is perfectly suitable for deep learning and increases efficiency (by contrast, NumPy defaults to float64).\nJIT compilation\nJIT compilation combines computations, avoids the allocation of memory to temporary objects, and more generally optimizes code for the XLA.\nAccelerators\nThe same code can run on CPUs or on accelerators (GPUs and TPUs).\nXLA optimization\nXLA (Accelerated Linear Algebra) is a domain-specific compiler for linear algebra that takes JIT-compiled JAX programs and optimizes them for the available hardware (CPUs, GPUs, or TPUs).\nAsynchronous dispatch\nComputations are executed on the accelerators asynchronously.\nVectorization, data parallelism, and sharding\nAll levels of shared and distributed memory parallelism are supported in JAX.",
    "crumbs": [
      "AI",
      "<b><em>JAX</em></b>",
      "Why JAX?"
    ]
  },
  {
    "objectID": "ai/jx/jx_why.html#flexible-differentiation",
    "href": "ai/jx/jx_why.html#flexible-differentiation",
    "title": "Why JAX?",
    "section": "Flexible differentiation",
    "text": "Flexible differentiation\nAutomatic differentiation (autodiff or AD) is the evaluation by computer programs of the partial derivatives of functions. It is a key part of deep learning since training a model mostly consists of updating its weights and biases to decrease some loss function and this is done thanks to various gradient-based optimizations.\nSeveral implementations have been developed by different teams over time. This post by Chris Rackauckas summarizes the trade-offs of the various strategies.\nRemoving Julia (which by the way has a lot to offer in the field of AD) and PyTorch’s stale attempt at JIT compilation, Chris Rackauckas’ post can be summarized this way:\n\n\n\n\n\n\n\n\n\n01\n\n\nAutodiff method\n\n\n\n1\nStatic graph\nand XLA\n\n\n\n\n02\n\n\nFramework\n\n\n\n\n2\nDynamic graph\n\n\n\n1-&gt;2\n\n\n\n\n\na\n\nTensorFlow\n\n\n\n\n4\nDynamic graph\nand XLA\n\n\n\n2-&gt;4\n\n\n\n\n\nb\n\nPyTorch\n\n\n\n\n5\nPseudo-dynamic\nand XLA\n\n\n\n4-&gt;5\n\n\n\n\n\nd\n\nTensorFlow2\n\n\n\n\ne\n\nJAX\n\n\n\n\n\n03\n\n\nAdvantage\n\n\n\n\n\n7\nMostly\noptimized AD\n\n\n\n\n\n8\nConvenient\n\n\n\n\n\n9\nConvenient\n\n\n\n\n10\nConvenient and\nmostly optimized AD\n\n\n\n\n\n04\n\n\nDisadvantage\n\n\n\n\n\nA\nManual writing of IR\n\n\n\n\n\nB\nLimited AD optimization\n\n\n\n\n\nD\nDisappointing speed\n\n\n\n\nE\nPure functions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTensorFlow’s initial approach with static computational graphs in a domain-specific language—while efficient thanks to the intermediate representation (IR) and XLA—was inconvenient, limited, and hard to debug. Mostly, users had to write the IR themselves.\nPyTorch came with dynamic graphs—an approach so much more convenient that it marked the beginning of the decline of TensorFlow. The operations are stored during the forward pass which allows for easy automatic differentiation. However this “per value” AD does not allow for a lot of optimizations.\nTensorFlow2 tried to bring dynamic graphs, but it was a poor match for the XLA.\nThis leaves room for new strategies. Julia offers several promising approaches, but implementations are not straightforward and projects are not always mature. It is an exciting avenue for developers, not necessarily an easy one for end users.\nJAX is another attempt at bringing both optimization and flexibility to autodiff. With Google behind it, it is a new but fast growing project.",
    "crumbs": [
      "AI",
      "<b><em>JAX</em></b>",
      "Why JAX?"
    ]
  },
  {
    "objectID": "ai/jx/wb_jax_slides.html#what-is-jax",
    "href": "ai/jx/wb_jax_slides.html#what-is-jax",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "What is JAX?",
    "text": "What is JAX?\n\nLibrary for Python developed by Google\n\n\nKey data structure: Array\n\n\nComposition, transformation, and differentiation of numerical programs\n\n\nCompilation for CPUs, GPUs, and TPUs\n\n\nNumPy-like and lower-level APIs\n\n\nRequires strict functional programming"
  },
  {
    "objectID": "ai/jx/wb_jax_slides.html#why-jax",
    "href": "ai/jx/wb_jax_slides.html#why-jax",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "Why JAX?",
    "text": "Why JAX?\n\n\n\n\n\n\n\n\n\n01\n\n\nAutodiff method\n\n\n\n1\nStatic graph\nand XLA\n\n\n\n\n02\n\n\nFramework\n\n\n\n\n2\nDynamic graph\n\n\n\n1-&gt;2\n\n\n\n\n\na\n\nTensorFlow\n\n\n\n\n4\nDynamic graph\nand XLA\n\n\n\n2-&gt;4\n\n\n\n\n\nb\n\nPyTorch\n\n\n\n\n5\nPseudo-dynamic\nand XLA\n\n\n\n4-&gt;5\n\n\n\n\n\nd\n\nTensorFlow2\n\n\n\n\ne\n\nJAX\n\n\n\n\n\n03\n\n\nAdvantage\n\n\n\n\n\n7\nMostly\noptimized AD\n\n\n\n\n\n8\nConvenient\n\n\n\n\n\n9\nConvenient\n\n\n\n\n10\nConvenient and\nmostly optimized AD\n\n\n\n\n\n04\n\n\nDisadvantage\n\n\n\n\n\nA\nManual writing of IR\n\n\n\n\n\nB\nLimited AD optimization\n\n\n\n\n\nD\nDisappointing speed\n\n\n\n\nE\nPure functions\n\n\n\n\n\n\n\n\n\n\n\n\n\n  Summarized from a blog post by Chris Rackauckas"
  },
  {
    "objectID": "ai/jx/wb_jax_slides.html#installation",
    "href": "ai/jx/wb_jax_slides.html#installation",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "Installation",
    "text": "Installation\n Install from pip wheels:\n\nPersonal computer: use wheels installation commands from official site\nAlliance clusters: python -m pip install jax --no-index \n\n\nWindows: GPU support only via WSL"
  },
  {
    "objectID": "ai/jx/wb_jax_slides.html#the-numpy-api",
    "href": "ai/jx/wb_jax_slides.html#the-numpy-api",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "The NumPy API",
    "text": "The NumPy API\n\nNumPyJAX NumPy\n\n\n\nimport numpy as np\n\nprint(np.array([(1, 2, 3), (4, 5, 6)]))\n\n[[1 2 3]\n [4 5 6]]\n\n\n\nprint(np.arange(5))\n\n[0 1 2 3 4]\n\n\n\nprint(np.zeros(2))\n\n[0. 0.]\n\n\n\nprint(np.linspace(0, 2, 9))\n\n[0.   0.25 0.5  0.75 1.   1.25 1.5  1.75 2.  ]\n\n\n\n\nimport jax.numpy as jnp\n\nprint(jnp.array([(1, 2, 3), (4, 5, 6)]))\n[[1 2 3]\n [4 5 6]]\nprint(jnp.arange(5))\n[0 1 2 3 4]\nprint(jnp.zeros(2))\n[0. 0.]\nprint(jnp.linspace(0, 2, 9))\n[0.   0.25 0.5  0.75 1.   1.25 1.5  1.75 2.  ]"
  },
  {
    "objectID": "ai/jx/wb_jax_slides.html#different-types",
    "href": "ai/jx/wb_jax_slides.html#different-types",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "Different types",
    "text": "Different types\n\nNumpyJAX NumPy\n\n\n\ntype(np.zeros((2, 3)))\n\nnumpy.ndarray\n\n\n\n\ntype(jnp.zeros((2, 3)))\njaxlib.xla_extension.ArrayImpl"
  },
  {
    "objectID": "ai/jx/wb_jax_slides.html#different-default-data-types",
    "href": "ai/jx/wb_jax_slides.html#different-default-data-types",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "Different default data types",
    "text": "Different default data types\n\nNumpyJAX NumPy\n\n\n\nnp.zeros((2, 3)).dtype\n\ndtype('float64')\n\n\n\n\njnp.zeros((2, 3)).dtype\ndtype('float32')\n\nStandard for DL and libraries built for accelerators\nFloat64 are very slow on GPUs and not supported on TPUs"
  },
  {
    "objectID": "ai/jx/wb_jax_slides.html#immutable-arrays",
    "href": "ai/jx/wb_jax_slides.html#immutable-arrays",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "Immutable arrays",
    "text": "Immutable arrays\n\nNumpyJAX NumPy\n\n\n\na = np.arange(5)\na[0] = 9\nprint(a)\n\n[9 1 2 3 4]\n\n\n\n\na = jnp.arange(5)\na[0] = 9\nTypeError: '&lt;class 'jaxlib.xla_extension.ArrayImpl'&gt;' object does not support item assignment. JAX arrays are immutable.\nb = a.at[0].set(9)\nprint(b)\n[9 1 2 3 4]"
  },
  {
    "objectID": "ai/jx/wb_jax_slides.html#strict-input-control",
    "href": "ai/jx/wb_jax_slides.html#strict-input-control",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "Strict input control",
    "text": "Strict input control\n\nNumpyJAX NumPy\n\n\nNumPy is easy-going:\n\nnp.sum([1.0, 2.0])  # argument is a list\n\n3.0\n\n\n\nnp.sum((1.0, 2.0))  # argument is a tuple\n\n3.0\n\n\n\n\nTo avoid inefficiencies, JAX will only accept arrays:\njnp.sum([1.0, 2.0])\nTypeError: sum requires ndarray or scalar arguments, got &lt;class 'list'&gt;\njnp.sum((1.0, 2.0))\nTypeError: sum requires ndarray or scalar arguments, got &lt;class 'tuple'&gt;"
  },
  {
    "objectID": "ai/jx/wb_jax_slides.html#out-of-bounds-indexing",
    "href": "ai/jx/wb_jax_slides.html#out-of-bounds-indexing",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "Out of bounds indexing",
    "text": "Out of bounds indexing\n\nNumpyJAX NumPy\n\n\nNumPy will error if you index out of bounds:\n\nprint(np.arange(5)[10])\n\nIndexError: index 10 is out of bounds for axis 0 with size 5\n\n\n\n\nJAX will silently return the closest boundary:\nprint(jnp.arange(5)[10])\n4"
  },
  {
    "objectID": "ai/jx/wb_jax_slides.html#prng",
    "href": "ai/jx/wb_jax_slides.html#prng",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "PRNG",
    "text": "PRNG\nTraditional pseudorandom number generators are based on nondeterministic state of OS\nSlow and problematic for parallel executions\nJAX relies on explicitly-set random state called a key:\nfrom jax import random\n\ninitial_key = random.PRNGKey(18)\nprint(initial_key)\n[ 0 18]"
  },
  {
    "objectID": "ai/jx/wb_jax_slides.html#prng-1",
    "href": "ai/jx/wb_jax_slides.html#prng-1",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "PRNG",
    "text": "PRNG\nEach key can only be used for one random function, but it can be split into new keys:\nnew_key1, new_key2 = random.split(initial_key)\n\ninitial_key can’t be used anymore now\n\nprint(new_key1)\n[4197003906 1654466292]\nprint(new_key2)\n[1685972163 1654824463]\nWe need to keep one key to split whenever we need and we can use the other one"
  },
  {
    "objectID": "ai/jx/wb_jax_slides.html#prng-2",
    "href": "ai/jx/wb_jax_slides.html#prng-2",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "PRNG",
    "text": "PRNG\nTo make sure we don’t reuse a key by accident, it is best to overwrite the initial key with one of the new ones\nHere are easier names:\nkey = random.PRNGKey(18)\nkey, subkey = random.split(key)\nWe can now use subkey to generate a random array:\nx = random.normal(subkey, (3, 2))"
  },
  {
    "objectID": "ai/jx/wb_jax_slides.html#benchmarking",
    "href": "ai/jx/wb_jax_slides.html#benchmarking",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "Benchmarking",
    "text": "Benchmarking\nJAX uses asynchronous dispatch\nInstead of waiting for a computation to complete before control returns to Python, the computation is dispatched to an accelerator and a future is created\nTo get proper timings, we need to make sure the future is resolved by using the block_until_ready() method"
  },
  {
    "objectID": "ai/jx/wb_jax_slides.html#jit-syntax",
    "href": "ai/jx/wb_jax_slides.html#jit-syntax",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "JIT syntax",
    "text": "JIT syntax\nfrom jax import jit\n\nkey = random.PRNGKey(8)\nkey, subkey1, subkey2 = random.split(key, 3)\n\na = random.normal(subkey1, (500, 500))\nb = random.normal(subkey2, (500, 500))\n\ndef sum_squared_error(a, b):\n    return jnp.sum((a-b)**2)\nOur function could simply be used as:\nsse = sum_squared_error(a, b)"
  },
  {
    "objectID": "ai/jx/wb_jax_slides.html#jit-syntax-1",
    "href": "ai/jx/wb_jax_slides.html#jit-syntax-1",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "JIT syntax",
    "text": "JIT syntax\nOur code will run faster if we create a JIT compiled version and use that instead:\nsum_squared_error_jit = jit(sum_squared_error)\n\nsse = sum_squared_error_jit(a, b)\nAlternatively, this can be written as:\nsse = jit(sum_squared_error)(a, b)\nOr with the @jit decorator:\n@jit\ndef sum_squared_error(a, b):\n    return jnp.sum((a - b) ** 2)\n\nsse = sum_squared_error(a, b)"
  },
  {
    "objectID": "ai/jx/wb_jax_slides.html#static-vs-traced-variables",
    "href": "ai/jx/wb_jax_slides.html#static-vs-traced-variables",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "Static vs traced variables",
    "text": "Static vs traced variables\n@jit\ndef cond_func(x):\n    if x &lt; 0.0:\n        return x ** 2.0\n    else:\n        return x ** 3.0\n\nprint(cond_func(1.0))\njax.errors.TracerBoolConversionError: Attempted boolean conversion of traced array with shape bool[]\nJIT compilation uses tracing of the code based on shape and dtype so that the same compiled code can be reused for new values with the same characteristics\nTracer objects are not real values but abstract representation that are more general\nHere, an abstract general value does not work as it wouldn’t know which branch to take"
  },
  {
    "objectID": "ai/jx/wb_jax_slides.html#static-vs-traced-variables-1",
    "href": "ai/jx/wb_jax_slides.html#static-vs-traced-variables-1",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "Static vs traced variables",
    "text": "Static vs traced variables\nOne solution is to tell jit() to exclude the problematic arguments from tracing\nwith arguments positions:\ndef cond_func(x):\n    if x &lt; 0.0:\n        return x ** 2.0\n    else:\n        return x ** 3.0\n\ncond_func_jit = jit(cond_func, static_argnums=(0,))\n\nprint(cond_func_jit(2.0))\nprint(cond_func_jit(-2.0))\n8.0\n4.0"
  },
  {
    "objectID": "ai/jx/wb_jax_slides.html#static-vs-traced-variables-2",
    "href": "ai/jx/wb_jax_slides.html#static-vs-traced-variables-2",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "Static vs traced variables",
    "text": "Static vs traced variables\nOne solution is to tell jit() to exclude the problematic arguments from tracing\nwith arguments names:\ndef cond_func(x):\n    if x &lt; 0.0:\n        return x ** 2.0\n    else:\n        return x ** 3.0\n\ncond_func_jit_alt = jit(cond_func, static_argnames=\"x\")\n\nprint(cond_func_jit_alt(2.0))\nprint(cond_func_jit_alt(-2.0))\n8.0\n4.0"
  },
  {
    "objectID": "ai/jx/wb_jax_slides.html#control-flow-primitives",
    "href": "ai/jx/wb_jax_slides.html#control-flow-primitives",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "Control flow primitives",
    "text": "Control flow primitives\nAnother solution, is to use one of the structured control flow primitives:\nfrom jax import lax\n\nlax.cond(False, lambda x: x ** 2.0, lambda x: x ** 3.0, jnp.array([2.]))\nArray([8.], dtype=float32)\nlax.cond(True, lambda x: x ** 2.0, lambda x: x ** 3.0, jnp.array([-2.]))\nArray([4.], dtype=float32)"
  },
  {
    "objectID": "ai/jx/wb_jax_slides.html#control-flow-primitives-1",
    "href": "ai/jx/wb_jax_slides.html#control-flow-primitives-1",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "Control flow primitives",
    "text": "Control flow primitives\nOther control flow primitives:\n\nlax.while_loop\nlax.fori_loop\nlax.scan\n\nOther pseudo dynamic control flow functions:\n\nlax.select (NumPy API jnp.where and jnp.select)\nlax.switch (NumPy API jnp.piecewise)"
  },
  {
    "objectID": "ai/jx/wb_jax_slides.html#static-vs-traced-operations",
    "href": "ai/jx/wb_jax_slides.html#static-vs-traced-operations",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "Static vs traced operations",
    "text": "Static vs traced operations\nSimilarly, you can mark problematic operations as static so that they don’t get traced during JIT compilation:\n@jit\ndef f(x):\n    return x.reshape(jnp.array(x.shape).prod())\n\nx = jnp.ones((2, 3))\nprint(f(x))\nTypeError: Shapes must be 1D sequences of concrete values of integer type, got [Traced&lt;ShapedArray(int32[])&gt;with&lt;DynamicJaxprTrace(level=1/0)&gt;]"
  },
  {
    "objectID": "ai/jx/wb_jax_slides.html#static-vs-traced-operations-1",
    "href": "ai/jx/wb_jax_slides.html#static-vs-traced-operations-1",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "Static vs traced operations",
    "text": "Static vs traced operations\nThe problem here is that the shape of the argument to prod() depends on the value of x which is unknown at compilation time\nOne solution is to use the NumPy version of prod():\nimport numpy as np\n\n@jit\ndef f(x):\n    return x.reshape((np.prod(x.shape)))\n\nprint(f(x))\n[1. 1. 1. 1. 1. 1.]"
  },
  {
    "objectID": "ai/jx/wb_jax_slides.html#jaxprs",
    "href": "ai/jx/wb_jax_slides.html#jaxprs",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "Jaxprs",
    "text": "Jaxprs\nimport jax\n\nx = jnp.array([1., 4., 3.])\ny = jnp.array([8., 1., 2.])\n\ndef f(x, y):\n    return 2 * x**2 + y\n\njax.make_jaxpr(f)(x, y) \n{ lambda ; a:f32[3] b:f32[3]. let\n    c:f32[3] = integer_pow[y=2] a\n    d:f32[3] = mul 2.0 c\n    e:f32[3] = add d b\n  in (e,) }"
  },
  {
    "objectID": "ai/jx/wb_jax_slides.html#outputs-only-based-on-inputs",
    "href": "ai/jx/wb_jax_slides.html#outputs-only-based-on-inputs",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "Outputs only based on inputs",
    "text": "Outputs only based on inputs\ndef f(x):\n    return a + x\nf uses the variable a from the global environment\nThe output does not solely depend on the inputs: not a pure function"
  },
  {
    "objectID": "ai/jx/wb_jax_slides.html#outputs-only-based-on-inputs-1",
    "href": "ai/jx/wb_jax_slides.html#outputs-only-based-on-inputs-1",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "Outputs only based on inputs",
    "text": "Outputs only based on inputs\na = jnp.ones(3)\nprint(a)\n[1. 1. 1.]\ndef f(x):\n    return a + x\n\nprint(jit(f)(jnp.ones(3)))\n[2. 2. 2.]\n\nThings seem ok here because this is the first run (tracing)"
  },
  {
    "objectID": "ai/jx/wb_jax_slides.html#outputs-only-based-on-inputs-2",
    "href": "ai/jx/wb_jax_slides.html#outputs-only-based-on-inputs-2",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "Outputs only based on inputs",
    "text": "Outputs only based on inputs\nNow, let’s change the value of a to an array of zeros:\na = jnp.zeros(3)\nprint(a)\n[0. 0. 0.]\nAnd rerun the same code:\nprint(jit(f)(jnp.ones(3)))\n[2. 2. 2.]\n\nOur cached compiled program is run and we get a wrong result"
  },
  {
    "objectID": "ai/jx/wb_jax_slides.html#outputs-only-based-on-inputs-3",
    "href": "ai/jx/wb_jax_slides.html#outputs-only-based-on-inputs-3",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "Outputs only based on inputs",
    "text": "Outputs only based on inputs\nThe new value for a will only take effect if we re-trigger tracing by changing the shape and/or dtype of x:\na = jnp.zeros(4)\nprint(a)\n[0. 0. 0. 0.]\nprint(jit(f)(jnp.ones(4)))\n[1. 1. 1. 1.]\nPassing to f() an argument of a different shape forced retracing"
  },
  {
    "objectID": "ai/jx/wb_jax_slides.html#no-side-effects",
    "href": "ai/jx/wb_jax_slides.html#no-side-effects",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "No side effects",
    "text": "No side effects\nSide effects: anything beside returned output\nExamples:\n\nPrinting to standard output\nReading from file/writing to file\nModifying a global variable"
  },
  {
    "objectID": "ai/jx/wb_jax_slides.html#no-side-effects-1",
    "href": "ai/jx/wb_jax_slides.html#no-side-effects-1",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "No side effects",
    "text": "No side effects\nThe side effects will happen during tracing, but not on subsequent runs. You cannot rely on side effects in your code\ndef f(a, b):\n    print(\"Calculating sum\")\n    return a + b\n\nprint(jit(f)(jnp.arange(3), jnp.arange(3)))\nCalculating sum\n[0 2 4]\n\nPrinting happened here because this is the first run"
  },
  {
    "objectID": "ai/jx/wb_jax_slides.html#no-side-effects-2",
    "href": "ai/jx/wb_jax_slides.html#no-side-effects-2",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "No side effects",
    "text": "No side effects\nLet’s rerun the function:\nprint(jit(f)(jnp.arange(3), jnp.arange(3)))\n[0 2 4]\nThis time, no printing"
  },
  {
    "objectID": "ai/jx/wb_jax_slides.html#automatic-differentiation",
    "href": "ai/jx/wb_jax_slides.html#automatic-differentiation",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "Automatic differentiation",
    "text": "Automatic differentiation\nConsidering the function f:\nf = lambda x: x**3 + 2*x**2 - 3*x + 8\nWe can create a new function dfdx that computes the gradient of f w.r.t. x:\nfrom jax import grad\n\ndfdx = grad(f)\ndfdx returns the derivatives\nprint(dfdx(1.))\n4.0"
  },
  {
    "objectID": "ai/jx/wb_jax_slides.html#composing-transformations",
    "href": "ai/jx/wb_jax_slides.html#composing-transformations",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "Composing transformations",
    "text": "Composing transformations\nTransformations can be composed:\nprint(jit(grad(f))(1.))\n4.0\nprint(grad(jit(f))(1.))\n4.0"
  },
  {
    "objectID": "ai/jx/wb_jax_slides.html#forward-and-reverse-modes",
    "href": "ai/jx/wb_jax_slides.html#forward-and-reverse-modes",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "Forward and reverse modes",
    "text": "Forward and reverse modes\nOther autodiff methods:\n\nReverse-mode vector-Jacobian products: jax.vjp\nForward-mode Jacobian-vector products: jax.jvp"
  },
  {
    "objectID": "ai/jx/wb_jax_slides.html#higher-order-differentiation",
    "href": "ai/jx/wb_jax_slides.html#higher-order-differentiation",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "Higher-order differentiation",
    "text": "Higher-order differentiation\n With a single variable, the grad function calls can be nested:\nd2fdx = grad(dfdx)   # function to compute 2nd order derivatives\nd3fdx = grad(d2fdx)  # function to compute 3rd order derivatives\n...\n With several variables:\n\njax.jacfwd for forward-mode\njax.jacrev for reverse-mode"
  },
  {
    "objectID": "ai/jx/wb_jax_slides.html#pytrees",
    "href": "ai/jx/wb_jax_slides.html#pytrees",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "Pytrees",
    "text": "Pytrees\nJAX has a nested container structure: pytree extremely useful for DNN"
  },
  {
    "objectID": "ai/jx/wb_jax_slides.html#vectorization-and-parallelization",
    "href": "ai/jx/wb_jax_slides.html#vectorization-and-parallelization",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "Vectorization and parallelization",
    "text": "Vectorization and parallelization\nOther transformations for parallel run of computations across batches of arrays:\n\nAutomatic vectorization with jax.vmap\nParallelization across devices with jax.pmap"
  },
  {
    "objectID": "ai/jx/wb_jax_slides.html#lax-api",
    "href": "ai/jx/wb_jax_slides.html#lax-api",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "Lax API",
    "text": "Lax API\njax.numpy is a high-level NumPy-like API wrapped around jax.lax\njax.lax is a more efficient lower-level API itself wrapped around XLA"
  },
  {
    "objectID": "ai/jx/wb_jax_slides.html#pallas-extension-to-write-gpu-and-tpu-kernels",
    "href": "ai/jx/wb_jax_slides.html#pallas-extension-to-write-gpu-and-tpu-kernels",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "Pallas: extension to write GPU and TPU kernels",
    "text": "Pallas: extension to write GPU and TPU kernels\n\n\n\n\n\n\n\n\n\ntracer\n\nTracing\n\n\n\njaxpr\n\nJaxprs\n(JAX expressions)\nintermediate\nrepresentation\n(IR)\n\n\n\ntracer-&gt;jaxpr\n\n\n\n\n\njit\n\n Just-in-time \n(JIT)\ncompilation\n\n\n\nhlo\n\nHigh-level\noptimized (HLO)\nprogram\n\n\n\njit-&gt;hlo\n\n\n\n\n\ntriton\n\nTriton\n\n\n\nGPU\n\nGPU\n\n\n\ntriton-&gt;GPU\n\n\n\n\n\nmosaic\n\nMosaic\n\n\n\nTPU\n\nTPU\n\n\n\nmosaic-&gt;TPU\n\n\n\n\n\ntransform\n\nVectorization\nParallelization\n   Differentiation  \n\n\n\npy\n\nPure Python\nfunctions\n\n\n\npy-&gt;tracer\n\n\n\n\njaxpr-&gt;jit\n\n\n\n\njaxpr-&gt;transform\n\n\n\n\n\n\nhlo-&gt;triton\n\n\n\n\nhlo-&gt;mosaic\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to webinar page"
  },
  {
    "objectID": "ai/pt/pt_checkpoints.html",
    "href": "ai/pt/pt_checkpoints.html",
    "title": "Saving/loading models and checkpointing",
    "section": "",
    "text": "After you have trained your model, obviously you will want to save it to use it thereafter. You will then need to load it in any session in which you want to use it.\nIn addition to saving or loading a fully trained model, it is important to know how to create regular checkpoints: training ML models takes a long time and a cluster crash or countless other issues might interrupt the training process. You don’t want to have to restart from scratch if that happens.",
    "crumbs": [
      "AI",
      "<b><em>PyTorch</em></b>",
      "Creating checkpoints"
    ]
  },
  {
    "objectID": "ai/pt/pt_checkpoints.html#savingloading-models",
    "href": "ai/pt/pt_checkpoints.html#savingloading-models",
    "title": "Saving/loading models and checkpointing",
    "section": "Saving/loading models",
    "text": "Saving/loading models\n\nSaving models\nYou can save a model by serializing its internal state dictionary. The state dictionary is a Python dictionary that contains the learnable parameters of your model (weights and biases).\ntorch.save(model.state_dict(), \"model.pt\")\n\n\nLoading models\nTo recreate your model, you first need to recreate its structure:\nmodel = TheModelClass(*args, **kwargs)\nThen you can load the state dictionary containing the parameters values into it:\nmodel.load_state_dict(torch.load(\"model.pt\"))\nAssuming you want to use your model for inference, you also must run:\nmodel.eval()\n\nIf instead you want to do more training on your model, you would of course run model.train() instead.",
    "crumbs": [
      "AI",
      "<b><em>PyTorch</em></b>",
      "Creating checkpoints"
    ]
  },
  {
    "objectID": "ai/pt/pt_checkpoints.html#checkpointing",
    "href": "ai/pt/pt_checkpoints.html#checkpointing",
    "title": "Saving/loading models and checkpointing",
    "section": "Checkpointing",
    "text": "Checkpointing\n\nCreating a checkpoint\ntorch.save({\n    'epoch': epoch,\n    'model_state_dict': model.state_dict(),\n    'optimizer_state_dict': optimizer.state_dict(),\n    'loss': loss,\n    ...\n}, \"model.pt\")\n\n\nResuming training from a checkpoint\nRecreate the state of your model from the checkpoint:\nmodel = TheModelClass(*args, **kwargs)\noptimizer = TheOptimizerClass(*args, **kwargs)\n\ncheckpoint = torch.load(\"model.pt\")\nmodel.load_state_dict(checkpoint['model_state_dict'])\noptimizer.load_state_dict(checkpoint['optimizer_state_dict'])\nepoch = checkpoint['epoch']\nloss = checkpoint['loss']\nResume training:\nmodel.train()",
    "crumbs": [
      "AI",
      "<b><em>PyTorch</em></b>",
      "Creating checkpoints"
    ]
  },
  {
    "objectID": "ai/pt/pt_concepts.html",
    "href": "ai/pt/pt_concepts.html",
    "title": "Concepts:",
    "section": "",
    "text": "Artificial intelligence is a vast field: any system mimicking animal intelligence falls in its scope.\nMachine learning (ML) is a subfield of artificial intelligence that can be defined as computer programs whose performance at a task improves with experience.\nSince this experience comes in the form of data, ML consists of feeding vast amounts of data to algorithms to strengthen pathways.\n\n\nFrom xkcd.com\n\n\n\nCoding all the possible ways—pixel by pixel—that a picture can represent a certain object is an impossibly large task. By feeding examples of images of that object to a neural network however, we can train it to recognize that object in images that it has never seen (without explicitly programming how it does this!).\n\n\nFrom xkcd.com"
  },
  {
    "objectID": "ai/pt/pt_concepts.html#what-is-machine-learning",
    "href": "ai/pt/pt_concepts.html#what-is-machine-learning",
    "title": "Concepts:",
    "section": "",
    "text": "Artificial intelligence is a vast field: any system mimicking animal intelligence falls in its scope.\nMachine learning (ML) is a subfield of artificial intelligence that can be defined as computer programs whose performance at a task improves with experience.\nSince this experience comes in the form of data, ML consists of feeding vast amounts of data to algorithms to strengthen pathways.\n\n\nFrom xkcd.com\n\n\n\nCoding all the possible ways—pixel by pixel—that a picture can represent a certain object is an impossibly large task. By feeding examples of images of that object to a neural network however, we can train it to recognize that object in images that it has never seen (without explicitly programming how it does this!).\n\n\nFrom xkcd.com"
  },
  {
    "objectID": "ai/pt/pt_concepts.html#types-of-learning",
    "href": "ai/pt/pt_concepts.html#types-of-learning",
    "title": "Concepts:",
    "section": "Types of learning",
    "text": "Types of learning\nThere are now more types of learning than those presented here. But these initial types are interesting because they will already be familiar to you.\n\nSupervised learning\nYou have been doing supervised machine learning for years without looking at it in the framework of machine learning:\n\nRegression is a form of supervised learning with continuous outputs\nClassification is supervised learning with discrete outputs\n\nSupervised learning uses training data in the form of example input/output \\((x_i, y_i)\\) pairs.\nGoal:\nIf \\(X\\) is the space of inputs and \\(Y\\) the space of outputs, the goal is to find a function \\(h\\) so that\nfor each \\(x_i \\in X\\):\n\n\\(h_\\theta(x_i)\\) is a predictor for the corresponding value \\(y_i\\)\n\n(\\(\\theta\\) represents the set of parameters of \\(h_\\theta\\)).\n\n→ i.e. we want to find the relationship between inputs and outputs.\n\n\nUnsupervised learning\nHere too, you are familiar with some forms of unsupervised learning that you weren’t thinking about in such terms:\nClustering, social network analysis, market segmentation, PCA … are all forms of unsupervised learning.\nUnsupervised learning uses unlabelled data (training set of \\(x_i\\)).\nGoal:\nFind structure within the data.\n\n\nFrom xkcd.com"
  },
  {
    "objectID": "ai/pt/pt_concepts.html#artificial-neural-networks",
    "href": "ai/pt/pt_concepts.html#artificial-neural-networks",
    "title": "Concepts:",
    "section": "Artificial neural networks",
    "text": "Artificial neural networks\nArtificial neural networks (ANN) are one of the machine learning models (other models include decision trees or Bayesian networks). Their potential and popularity has truly exploded in recent years and this is what we will focus on in this course.\nArtificial neural networks are a series of layered units mimicking the concept of biological neurons: inputs are received by every unit of a layer, computed, then transmitted to units of the next layer. In the process of learning, experience strengthens some connections between units and weakens others.\nIn biological networks, the information consists of action potentials (neuron membrane rapid depolarizations) propagating through the network. In artificial ones, the information consists of tensors (multidimensional arrays) of weights and biases: each unit passes a weighted sum of an input tensor with an additional—possibly weighted—bias through an activation function before passing on the output tensor to the next layer of units.\n\n\nThe bias allows to shift the output of the activation function to the right or to the left (i.e. it creates an offset).\n\nSchematic of a biological neuron:\n\n\nFrom Dhp1080, Wikipedia\n\nSchematic of an artificial neuron:\n\n\nModified from O.C. Akgun & J. Mei 2019\n\nWhile biological neurons are connected in extremely intricate patterns, artificial ones follow a layered structure. Another difference in complexity is in the number of units: the human brain has 65–90 billion neurons. ANN have much fewer units.\nNeurons in mouse cortex:\n\n\nNeurons are in green, the dark branches are blood vessels. Image by Na Ji, UC Berkeley\n\nNeural network with 2 hidden layers:\n\n\nFrom The Maverick Meerkat\n\nThe information in biological neurons is an all-or-nothing electrochemical pulse or action potential. Greater stimuli don’t produce stronger signals but increase firing frequency. In contrast, artificial neurons pass the computation of their inputs through an activation function and the output can take any of the values possible with that function.\nThreshold potential in biological neurons:\n\n\nModified from Blacktc, Wikimedia\n\nSome of the most common activation functions in artificial neurons:\n\n\nFrom Diganta Misra 2019\n\nWhich activation function to use depends on the type of problem and the available computing budget. Some early functions have fallen out of use while new ones have emerged (e.g. sigmoid got replaced by ReLU which is easier to train).\n\nLearning\nThe process of learning in biological NN happens through neuron death or growth and through the creation or loss of synaptic connections between neurons. In ANN, learning happens through optimization algorithms such as gradient descent which minimize cross entropy loss functions by adjusting the weights and biases connecting each layer of neurons over many iterations (cross entropy is the difference between the predicted and the real distributions).\n\n\nFrom xkcd.com\n\n\n\nGradient descent\nThere are several gradient descent methods:\nBatch gradient descent uses all examples in each iteration and is thus slow for large datasets (the parameters are adjusted only after all the samples have been processed).\nMini-batch gradient descent is an intermediate approach: it uses mini-batch sized examples in each iteration. This allows a vectorized approach (and hence parallelization).\nThe Adam optimization algorithm is a popular variation of mini-batch gradient descent.\nStochastic gradient descent uses one example in each iteration. It is thus much faster than batch gradient descent (the parameters are adjusted after each example). But it does not allow any vectorization.\n\n\nFrom Imad Dabbura\n\n\n\n3Blue1Brown by Grant Sanderson videos\n3Blue1Brown by Grant Sanderson has a series of 4 videos on neural networks which is easy to watch, fun, and does an excellent job at introducing the functioning of a simple neural network.\n\nWhat are NN? (19 min)\n\nWatch this video beyond the acknowledgement as the function ReLU (a really important function in modern neural networks) is introduced at the very end.\n\n\n\nAs you develop your own ML models, if you find that your mathematical background is shaky, 3blue1brown also has an excellent series of videos on linear algebra and an equally great one on calculus.\n\n\n\nHow do NN learn? (21 min)\n\n\n\nWhat is backpropagation? (14 min)\n\n\nThere is one minor terminological error in this video: they call the use of mini-batches stochastic gradient descent. In fact, this is called mini-batch gradient descent. Stochastic gradient descent uses a single example at each iteration.\n\n\n\nHow does backpropagation work? (10 min)\n\n\n\n\nTypes of ANN\n\nFully connected neural networks\n\n\nFrom Glosser.ca, Wikipedia\n\nEach neuron receives inputs from every neuron of the previous layer and passes its output to every neuron of the next layer.\n\n\nConvolutional neural networks\n\n\nFrom Programming Journeys by Rensu Theart\n\nConvolutional neural networks (CNN) are used for spatially structured data (e.g. in image recognition).\nImages have huge input sizes and would require a very large number of neurons in a fully connected neural net. In convolutional layers, neurons receive input from a subarea (called local receptive field) of the previous layer. This greatly reduces the number of parameters.\nOptionally, pooling (combining the outputs of neurons in a subarea) reduces the data dimensions. The stride then dictates how the subarea is moved across the image. Max-pooling is one of the forms of pooling which uses the maximum for each subarea.\n\n\nRecurrent neural networks\n\n\nFrom fdeloche, Wikipedia\n\nRecurrent neural networks (RNN) such as Long Short-Term Memory (LSTM) are used for chain structured data (e.g. in speech recognition).\nThey are not feedforward networks (i.e. networks for which the information moves only in the forward direction without any loop).\n\n\nTransformers\nA combination of two RNNs or sets of RNNs (the encoder and the decoder) is used in sequence to sequence models for translation or picture captioning. Such models were slow to process a lot of data.\nIn 2014 and 2015, the concept of attention (giving added weight to important words) was developed, greatly improving the ability of such models to process a lot of data.\nThis blog post by Jay Alammar—a blogger whose high-quality posts have been referenced in MIT and Stanford courses—explains this in a high-level visual fashion.\nThe problem with recurrence is that it is not easily to parallelize (and thus to run fast on GPUs).\nIn 2017, a new model—the transformer—was proposed: by using only attention mechanisms and no recurrence, the transformer achieves better results in an easily parallelizable fashion.\nJay Alammar has also a blog post on the transformer. The post includes a 30 min video.\nWith the addition of transfer learning, powerful transformers emerged in the field of NLP (Natural Language Processing). Examples include BERT (Bidirectional Encoder Representations from Transformers) from Google and GPT-3 (Generative Pre-trained Transformer-3) from OpenAI.\nJay Alammar has yet another great blog post on these advanced NLP models.\n\n\n\nDeep learning\nThe first layer of a neural net is the input layer. The last one is the output layer. All the layers in-between are called hidden layers. Shallow neural networks have only one hidden layer and deep networks have two or more hidden layers. When an ANN is deep, we talk about Deep Learning (DL).\n\n\nFrom xkcd.com"
  },
  {
    "objectID": "ai/pt/pt_high_level_frameworks.html",
    "href": "ai/pt/pt_high_level_frameworks.html",
    "title": "High-level frameworks for PyTorch",
    "section": "",
    "text": "Several popular higher-level frameworks are built on top of PyTorch and make the code easier to write and run:\nThe following tag trends on Stack Overflow might give an idea of the popularity of these frameworks over time (catalyst doesn’t have any Stack Overflow tag):\nIf this data is to be believed, ignite never really took off (it also has a lower number of stars on GitHub), fast-ai was extremely popular when it came out, but its usage is going down, and PyTorch-lightning is currently the most popular.",
    "crumbs": [
      "AI",
      "<b><em>PyTorch</em></b>",
      "High-level frameworks"
    ]
  },
  {
    "objectID": "ai/pt/pt_high_level_frameworks.html#should-you-use-one-and-which-one",
    "href": "ai/pt/pt_high_level_frameworks.html#should-you-use-one-and-which-one",
    "title": "High-level frameworks for PyTorch",
    "section": "Should you use one (and which one)?",
    "text": "Should you use one (and which one)?\nLearning raw PyTorch is probably the best option for research. PyTorch is stable and here to stay. Higher-level frameworks may rise and drop in popularity and today’s popular one may see little usage tomorrow.\nRaw PyTorch is also the most flexible, the closest to the actual computations happening in your model, and probably the easiest to debug.\nDepending on your deep learning trajectory, you might find some of these tools useful though:\n\nIf you work in industry, you might want or need to get results quickly\nSome operations (e.g. parallel execution on multiple GPUs) can be tricky in raw PyTorch, while being extremely streamlined when using e.g. PyTorch-lightning\nEven in research, it might make sense to spend more time thinking about the structure of your model and the functioning of a network instead of getting bogged down in writing code\n\n\nBefore moving to any of these tools, it is probably a good idea to get a good knowledge of raw PyTorch: use these tools to simplify your workflow, not cloud your understanding of what your code is doing.",
    "crumbs": [
      "AI",
      "<b><em>PyTorch</em></b>",
      "High-level frameworks"
    ]
  },
  {
    "objectID": "ai/pt/pt_intro.html",
    "href": "ai/pt/pt_intro.html",
    "title": "Introduction to machine learning",
    "section": "",
    "text": "This presentation gives a high-level overview of machine learning.\nI will define concepts, present the different types of learning, and explain the basic functioning of neural networks.\n\nSlides (Click and wait: the presentation might take a few instants to load)",
    "crumbs": [
      "AI",
      "<b><em>PyTorch</em></b>",
      "Introduction"
    ]
  },
  {
    "objectID": "ai/pt/pt_mnist.html",
    "href": "ai/pt/pt_mnist.html",
    "title": "Example: classifying the MNIST dataset",
    "section": "",
    "text": "Here is an example you can try on your own after the workshop: the classification of the MNIST dataset—a classic of machine learning.",
    "crumbs": [
      "AI",
      "<b><em>PyTorch</em></b>",
      "Example: the MNIST"
    ]
  },
  {
    "objectID": "ai/pt/pt_mnist.html#the-mnist-dataset",
    "href": "ai/pt/pt_mnist.html#the-mnist-dataset",
    "title": "Example: classifying the MNIST dataset",
    "section": "The MNIST dataset",
    "text": "The MNIST dataset\nThe MNIST is a classic dataset commonly used for testing machine learning systems. It consists of pairs of images of handwritten digits and their corresponding labels.\nThe images are composed of 28x28 pixels of greyscale RGB codes ranging from 0 to 255 and the labels are the digits from 0 to 9 that each image represents.\nThere are 60,000 training pairs and 10,000 testing pairs.\nThe goal is to build a neural network which can learn from the training set to properly identify the handwritten digits and which will perform well when presented with the testing set that it has never seen. This is a typical case of supervised learning.\n\nNow, let’s explore the MNIST with PyTorch.",
    "crumbs": [
      "AI",
      "<b><em>PyTorch</em></b>",
      "Example: the MNIST"
    ]
  },
  {
    "objectID": "ai/pt/pt_mnist.html#download-unzip-and-transform-the-data",
    "href": "ai/pt/pt_mnist.html#download-unzip-and-transform-the-data",
    "title": "Example: classifying the MNIST dataset",
    "section": "Download, unzip, and transform the data",
    "text": "Download, unzip, and transform the data\n\nWhere to store the data in the cluster\nWe will all use the same data. It would make little sense to all download it in our home directory.\nOn the Alliance clusters, a good place to store data shared amongst members of a project is in the /project file system.\nYou usually belong to /project/def-&lt;group&gt;, where &lt;group&gt; is the name of your PI. You can access it from your home directory through the symbolic link ~/projects/def-&lt;group&gt;.\nIn our training cluster, we are all part of the group def-sponsor00, accessible through /project/def-sponsor00 (or the hyperlink ~/projects/def-sponsor00).\nWe will thus all access the MNIST data in ~/projects/def-sponsor00/data.\n\n\nHow to obtain the data?\nThe dataset can be downloaded directly from the MNIST website, but the PyTorch package TorchVision has tools to download and transform several classic vision datasets, including the MNIST.\nhelp(torchvision.datasets.MNIST)\nHelp on class MNIST in module torchvision.datasets.mnist:\n\nclass MNIST(torchvision.datasets.vision.VisionDataset)\n\n |  MNIST(root: str, train: bool = True, \n |    transform: Optional[Callable] = None,\n |    target_transform: Optional[Callable] = None, \n |    download: bool = False) -&gt; None\n |   \n |  Args:\n |    root (string): Root directory of dataset where \n |      MNIST/raw/train-images-idx3-ubyte and \n |      MNIST/raw/t10k-images-idx3-ubyte exists.\n |    train (bool, optional): If True, creates dataset from \n |      train-images-idx3-ubyte, otherwise from t10k-images-idx3-ubyte.\n |    download (bool, optional): If True, downloads the dataset from the \n |      internet and puts it in root directory. If dataset is already \n |      downloaded, it is not downloaded again.\n |    transform (callable, optional): A function/transform that takes in \n |      an PIL image and returns a transformed version.\n |      E.g, transforms.RandomCrop\n |    target_transform (callable, optional): A function/transform that \n |      takes in the target and transforms it.\nNote that here too, the root argument sets the location of the downloaded data and we will use /project/def-sponsor00/data/.\n\n\nPrepare the data\nFirst, let’s load the needed libraries:\n\nimport torch\nfrom torchvision import datasets, transforms\nfrom matplotlib import pyplot as plt\n\nThe MNIST dataset already consists of a training and a testing sets, so we don’t have to split the data manually. Instead, we can directly create 2 different objects with the same function (train=True selects the train set and train=False selects the test set).\nWe will transform the raw data to tensors and normalize them using the mean and standard deviation of the MNIST training set: 0.1307 and 0.3081 respectively (even though the mean and standard deviation of the test data are slightly different, it is important to normalize the test data with the values of the training data to apply the same treatment to both sets).\nSo we first need to define a transformation:\n\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.1307,), (0.3081,))\n])\n\n\n\nWe can now create our data objects\n\nTraining data\n\nRemember that train=True selects the training set of the MNIST.\n\n\ntrain_data = datasets.MNIST(\n    '~/projects/def-sponsor00/data',\n    train=True, download=True, transform=transform)\n\n\n\nTest data\n\ntrain=False selects the test set.\n\n\ntest_data = datasets.MNIST(\n    '~/projects/def-sponsor00/data',\n    train=False, transform=transform)",
    "crumbs": [
      "AI",
      "<b><em>PyTorch</em></b>",
      "Example: the MNIST"
    ]
  },
  {
    "objectID": "ai/pt/pt_mnist.html#exploring-the-data",
    "href": "ai/pt/pt_mnist.html#exploring-the-data",
    "title": "Example: classifying the MNIST dataset",
    "section": "Exploring the data",
    "text": "Exploring the data\n\nData inspection\nFirst, let’s check the size of train_data:\n\nprint(len(train_data))\n\n60000\n\n\nThat makes sense since the MNIST’s training set has 60,000 pairs. train_data has 60,000 elements and we should expect each element to be of size 2 since it is a pair. Let’s double-check with the first element:\n\nprint(len(train_data[0]))\n\n2\n\n\nSo far, so good. We can print that first pair:\n\nprint(train_data[0])\n\n(tensor([[[-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.3860, -0.1951,\n          -0.1951, -0.1951,  1.1795,  1.3068,  1.8032, -0.0933,  1.6887,\n           2.8215,  2.7197,  1.1923, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.0424,  0.0340,  0.7722,  1.5359,  1.7396,  2.7960,\n           2.7960,  2.7960,  2.7960,  2.7960,  2.4396,  1.7650,  2.7960,\n           2.6560,  2.0578,  0.3904, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n           0.1995,  2.6051,  2.7960,  2.7960,  2.7960,  2.7960,  2.7960,\n           2.7960,  2.7960,  2.7960,  2.7706,  0.7595,  0.6195,  0.6195,\n           0.2886,  0.0722, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.1951,  2.3633,  2.7960,  2.7960,  2.7960,  2.7960,  2.7960,\n           2.0960,  1.8923,  2.7197,  2.6433, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242,  0.5940,  1.5614,  0.9377,  2.7960,  2.7960,  2.1851,\n          -0.2842, -0.4242,  0.1231,  1.5359, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.2460, -0.4115,  1.5359,  2.7960,  0.7213,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242,  1.3450,  2.7960,  1.9942,\n          -0.3988, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.2842,  1.9942,  2.7960,\n           0.4668, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.0213,  2.6433,\n           2.4396,  1.6123,  0.9504, -0.4115, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.6068,\n           2.6306,  2.7960,  2.7960,  1.0904, -0.1060, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n           0.1486,  1.9432,  2.7960,  2.7960,  1.4850, -0.0806, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.2206,  0.7595,  2.7833,  2.7960,  1.9560, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242,  2.7451,  2.7960,  2.7451,  0.3904,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n           0.1613,  1.2305,  1.9051,  2.7960,  2.7960,  2.2105, -0.3988,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.0722,  1.4596,\n           2.4906,  2.7960,  2.7960,  2.7960,  2.7578,  1.8923, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.1187,  1.0268,  2.3887,  2.7960,\n           2.7960,  2.7960,  2.7960,  2.1342,  0.5686, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.1315,  0.4159,  2.2869,  2.7960,  2.7960,  2.7960,\n           2.7960,  2.0960,  0.6068, -0.3988, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.1951,\n           1.7523,  2.3633,  2.7960,  2.7960,  2.7960,  2.7960,  2.0578,\n           0.5940, -0.3097, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242,  0.2758,  1.7650,  2.4524,\n           2.7960,  2.7960,  2.7960,  2.7960,  2.6815,  1.2686, -0.2842,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242,  1.3068,  2.7960,  2.7960,\n           2.7960,  2.2742,  1.2941,  1.2559, -0.2206, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242]]]), 5)\n\n\nAnd you can see that it is a tuple with:\n\nprint(type(train_data[0]))\n\n&lt;class 'tuple'&gt;\n\n\nWhat is that tuple made of?\n\nprint(type(train_data[0][0]))\nprint(type(train_data[0][1]))\n\n&lt;class 'torch.Tensor'&gt;\n&lt;class 'int'&gt;\n\n\nIt is made of the tensor for the first image (remember that we transformed the images into tensors when we created the objects train_data and test_data) and the integer of the first label (which you can see is 5 when you print train_data[0][1]).\nSo since train_data[0][0] is the tensor representing the image of the first pair, let’s check its size:\n\nprint(train_data[0][0].size())\n\ntorch.Size([1, 28, 28])\n\n\nThat makes sense: a color image would have 3 layers of RGB values (so the size in the first dimension would be 3), but because the MNIST has black and white images, there is a single layer of values—the values of each pixel on a gray scale—so the first dimension has a size of 1. The 2nd and 3rd dimensions correspond to the width and length of the image in pixels, hence 28 and 28.\n\n\nYour turn:\n\nRun the following:\nprint(train_data[0][0][0])\nprint(train_data[0][0][0][0])\nprint(train_data[0][0][0][0][0])\nAnd think about what each of them represents.\nThen explore the test_data object.\n\n\n\nPlotting an image from the data\nFor this, we will use pyplot from matplotlib.\nFirst, we select the image of the first pair and we resize it from 3 to 2 dimensions by removing its dimension of size 1 with torch.squeeze:\nimg = torch.squeeze(train_data[0][0])\nThen, we plot it with pyplot, but since we are in a cluster, instead of showing it to screen with plt.show(), we save it to file:\nplt.imshow(img, cmap='gray')\nThis is what that first image looks like:\n\nAnd indeed, it matches the first label we explored earlier (train_data[0][1]).\n\n\nPlotting an image with its pixel values\nWe can plot it with more details by showing the value of each pixel in the image. One little twist is that we need to pick a threshold value below which we print the pixel values in white otherwise they would not be visible (black on near black background). We also round the pixel values to one decimal digit so as not to clutter the result.\nimgplot = plt.figure(figsize = (12, 12))\nsub = imgplot.add_subplot(111)\nsub.imshow(img, cmap='gray')\nwidth, height = img.shape\nthresh = img.max() / 2.5\nfor x in range(width):\n    for y in range(height):\n        val = round(img[x][y].item(), 1)\n        sub.annotate(str(val), xy=(y, x),\n                     horizontalalignment='center',\n                     verticalalignment='center',\n                     color='white' if img[x][y].item() &lt; thresh else 'black')",
    "crumbs": [
      "AI",
      "<b><em>PyTorch</em></b>",
      "Example: the MNIST"
    ]
  },
  {
    "objectID": "ai/pt/pt_mnist.html#batch-processing",
    "href": "ai/pt/pt_mnist.html#batch-processing",
    "title": "Example: classifying the MNIST dataset",
    "section": "Batch processing",
    "text": "Batch processing\nPyTorch provides the torch.utils.data.DataLoader class which combines a dataset and an optional sampler and provides an iterable (while training or testing our neural network, we will iterate over that object). It allows, among many other things, to set the batch size and shuffle the data.\nSo our last step in preparing the data is to pass it through DataLoader.\n\nCreate DataLoaders\n\nTraining data\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=20, shuffle=True)\n\n\nTest data\ntest_loader = torch.utils.data.DataLoader(\n    test_data, batch_size=20, shuffle=False)\n\n\n\nPlot a full batch of images with their labels\nNow that we have passed our data through DataLoader, it is easy to select one batch from it. Let’s plot an entire batch of images with their labels.\nFirst, we need to get one batch of training images and their labels:\ndataiter = iter(train_loader)\nbatchimg, batchlabel = dataiter.next()\nThen, we can plot them:\nbatchplot = plt.figure(figsize=(20, 5))\nfor i in torch.arange(20):\n    sub = batchplot.add_subplot(2, 10, i+1, xticks=[], yticks=[])\n    sub.imshow(torch.squeeze(batchimg[i]), cmap='gray')\n    sub.set_title(str(batchlabel[i].item()), fontsize=25)",
    "crumbs": [
      "AI",
      "<b><em>PyTorch</em></b>",
      "Example: the MNIST"
    ]
  },
  {
    "objectID": "ai/pt/pt_mnist.html#time-to-build-a-nn-to-classify-the-mnist",
    "href": "ai/pt/pt_mnist.html#time-to-build-a-nn-to-classify-the-mnist",
    "title": "Example: classifying the MNIST dataset",
    "section": "Time to build a NN to classify the MNIST",
    "text": "Time to build a NN to classify the MNIST\nLet’s build a multi-layer perceptron (MLP): the simplest neural network. It is a feed-forward (i.e. no loop), fully-connected (i.e. each neuron of one layer is connected to all the neurons of the adjacent layers) neural network with a single hidden layer.\n\n\nLoad packages\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torchvision import datasets, transforms\nfrom torch.optim.lr_scheduler import StepLR\nThe torch.nn.functional module contains all the functions of the torch.nn package.\nThese functions include loss functions, activation functions, pooling functions…\n\n\nCreate a SummaryWriter instance for TensorBoard\nwriter = SummaryWriter()\n\n\nDefine the architecture of the network\n# To build a model, create a subclass of torch.nn.Module:\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.fc1 = nn.Linear(784, 128)\n        self.fc2 = nn.Linear(128, 10)\n\n    # Method for the forward pass:\n    def forward(self, x):\n        x = torch.flatten(x, 1)\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.fc2(x)\n        output = F.log_softmax(x, dim=1)\n        return output\nPython’s class inheritance gives our subclass all the functionality of torch.nn.Module while allowing us to customize it.\n\n\nDefine a training function\ndef train(model, device, train_loader, optimizer, epoch):\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n        optimizer.zero_grad()  # reset the gradients to 0\n        output = model(data)\n        loss = F.nll_loss(output, target)  # negative log likelihood\n        writer.add_scalar(\"Loss/train\", loss, epoch)\n        loss.backward()\n        optimizer.step()\n        if batch_idx % 10 == 0:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(data), len(train_loader.dataset),\n                100. * batch_idx / len(train_loader), loss.item()))\n\n\nDefine a testing function\ndef test(model, device, test_loader):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            # Sum up batch loss:\n            test_loss += F.nll_loss(output, target, reduction='sum').item()\n            # Get the index of the max log-probability:\n            pred = output.argmax(dim=1, keepdim=True)\n            correct += pred.eq(target.view_as(pred)).sum().item()\n\n    test_loss /= len(test_loader.dataset)\n\n    # Print a summary\n    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n        test_loss, correct, len(test_loader.dataset),\n        100. * correct / len(test_loader.dataset)))\n\n\nDefine a function main() which runs our network\ndef main():\n    epochs = 1\n    torch.manual_seed(1)\n    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n\n    transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,))\n    ])\n\n    train_data = datasets.MNIST(\n        '~/projects/def-sponsor00/data',\n        train=True, download=True, transform=transform)\n\n    test_data = datasets.MNIST(\n        '~/projects/def-sponsor00/data',\n        train=False, transform=transform)\n\n    train_loader = torch.utils.data.DataLoader(train_data, batch_size=64)\n    test_loader = torch.utils.data.DataLoader(test_data, batch_size=1000)\n    model = Net().to(device)  # create instance of our network and send it to device\n    optimizer = optim.Adadelta(model.parameters(), lr=1.0)\n    scheduler = StepLR(optimizer, step_size=1, gamma=0.7)\n\n    for epoch in range(1, epochs + 1):\n        train(model, device, train_loader, optimizer, epoch)\n        test(model, device, test_loader)\n        scheduler.step()\n\n\nRun the network\nmain()\n\n\nWrite pending events to disk and close the TensorBoard\nwriter.flush()\nwriter.close()\nThe code is working. Time to actually train our model!\nJupyter is a fantastic tool. It has a major downside however: when you launch a Jupyter server, you are running a job on a compute node. If you want to play for 8 hours in Jupyter, you are requesting an 8 hour job. Now, most of the time you spend on Jupyter is spent typing, running bits and pieces of code, or doing nothing at all. If you ask for GPUs, many CPUs, and lots of RAM, all of it will remain idle almost all of the time. It is a really suboptimal use of the Alliance resources.\nIn addition, if you ask for lots of resources for a long time, you will have to wait a long time in the queue before they get allocated to you.\nLastly, you will go through your allocation quickly.\nA much better strategy is to develop and test your code (with very little data, few epochs, etc.) in an interactive job (with salloc) or in Jupyter, then, launch an sbatch job to actually train your model. This ensures that heavy duty resources such as GPU(s) are only allocated to you when you are actually needing and using them.",
    "crumbs": [
      "AI",
      "<b><em>PyTorch</em></b>",
      "Example: the MNIST"
    ]
  },
  {
    "objectID": "ai/pt/pt_mnist.html#lets-train-and-test-our-model",
    "href": "ai/pt/pt_mnist.html#lets-train-and-test-our-model",
    "title": "Example: classifying the MNIST dataset",
    "section": "Let’s train and test our model",
    "text": "Let’s train and test our model\n\nLog in a cluster\nOpen a terminal and SSH to a cluster.\n\n\nLoad necessary modules\nFirst, we need to load the Python and CUDA modules. This is done with the Lmod tool through the module command. Here are some key Lmod commands:\n# Get help on the module command\n$ module help\n\n# List modules that are already loaded\n$ module list\n\n# See which modules are available for a tool\n$ module avail &lt;tool&gt;\n\n# Load a module\n$ module load &lt;module&gt;[/&lt;version&gt;]\nHere are the modules we need:\n$ module load nixpkgs/16.09 gcc/7.3.0 cuda/10.0.130 cudnn/7.6 python/3.8.2\n\n\nInstall Python packages\nYou also need the Python packages matplotlib, torch, torchvision, and tensorboard.\nOn the Alliance clusters, you need to create a virtual environment in which you install packages with pip, then activate that virtual environment.\n\nDo not use Anaconda.\nWhile Anaconda is a great tool on personal computers, it is not an appropriate tool when working on the Alliance clusters: binaries are unoptimized for those clusters and library paths are inconsistent with their architecture. Anaconda installs packages in $HOME where it creates a very large number of small files. It can also create conflicts by modifying .bashrc.\n\nCreate a virtual environment:\n$ virtualenv --no-download ~/env\nActivate the virtual environment:\n$ source ~/env/bin/activate\nUpdate pip:\n(env) $ pip install --no-index --upgrade pip\nInstall the packages you need in the virtual environment:\n(env) $ pip install --no-cache-dir --no-index matplotlib torch torchvision tensorboard\nIf you want to exit the virtual environment, you can press Ctrl-D or run:\n(env) $ deactivate\n\n\nWrite a Python script\nCreate a directory for this project and cd into it:\nmkdir mnist\ncd mnist\nStart a Python script with the text editor of your choice:\nnano nn.py\nIn it, copy-paste the code we played with in Jupyter, but this time have it run for 10 epochs:\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torchvision import datasets, transforms\nfrom torch.optim.lr_scheduler import StepLR\n\nwriter = SummaryWriter()\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.fc1 = nn.Linear(784, 128)\n        self.fc2 = nn.Linear(128, 10)\n\n    def forward(self, x):\n        x = torch.flatten(x, 1)\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.fc2(x)\n        output = F.log_softmax(x, dim=1)\n        return output\n\ndef train(model, device, train_loader, optimizer, epoch):\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = F.nll_loss(output, target)\n        writer.add_scalar(\"Loss/train\", loss, epoch)\n        loss.backward()\n        optimizer.step()\n        if batch_idx % 10 == 0:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(data), len(train_loader.dataset),\n                100. * batch_idx / len(train_loader), loss.item()))\n\ndef test(model, device, test_loader):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            test_loss += F.nll_loss(output, target, reduction='sum').item()\n            pred = output.argmax(dim=1, keepdim=True)\n            correct += pred.eq(target.view_as(pred)).sum().item()\n\n    test_loss /= len(test_loader.dataset)\n\n    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n        test_loss, correct, len(test_loader.dataset),\n        100. * correct / len(test_loader.dataset)))\n\ndef main():\n    epochs = 10  # don't forget to change the number of epochs\n    torch.manual_seed(1)\n    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n\n    transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,))\n    ])\n\n    train_data = datasets.MNIST(\n        '~/projects/def-sponsor00/data',\n        train=True, download=True, transform=transform)\n\n    test_data = datasets.MNIST(\n        '~/projects/def-sponsor00/data',\n        train=False, transform=transform)\n\n    train_loader = torch.utils.data.DataLoader(train_data, batch_size=64)\n    test_loader = torch.utils.data.DataLoader(test_data, batch_size=1000)\n    model = Net().to(device)\n    optimizer = optim.Adadelta(model.parameters(), lr=1.0)\n    scheduler = StepLR(optimizer, step_size=1, gamma=0.7)\n\n    for epoch in range(1, epochs + 1):\n        train(model, device, train_loader, optimizer, epoch)\n        test(model, device, test_loader)\n        scheduler.step()\n\nmain()\n\nwriter.flush()\nwriter.close()\n\n\nWrite a Slurm script\nWrite a shell script with the text editor of your choice:\nnano nn.sh\nThis is what you want in that script:\n#!/bin/bash\n#SBATCH --time=5:0\n#SBATCH --cpus-per-task=1\n#SBATCH --gres=gpu:1\n#SBATCH --mem=4G\n#SBATCH --output=%x_%j.out\n#SBATCH --error=%x_%j.err\n\npython ~/mnist/nn.py\n\n--time accepts these formats: “min”, “min:s”, “h:min:s”, “d-h”, “d-h:min” & “d-h:min:s”\n%x will get replaced by the script name & %j by the job number\n\n\n\nSubmit a job\nFinally, you need to submit your job to Slurm:\n$ sbatch ~/mnist/nn.sh\nYou can check the status of your job with:\n$ sq\n\nPD = pending\nR = running\nCG = completing (Slurm is doing the closing processes)\nNo information = your job has finished running\n\nYou can cancel it with:\n$ scancel &lt;jobid&gt;\nOnce your job has finished running, you can display efficiency measures with:\n$ seff &lt;jobid&gt;",
    "crumbs": [
      "AI",
      "<b><em>PyTorch</em></b>",
      "Example: the MNIST"
    ]
  },
  {
    "objectID": "ai/pt/pt_mnist.html#lets-explore-our-models-metrics-with-tensorboard",
    "href": "ai/pt/pt_mnist.html#lets-explore-our-models-metrics-with-tensorboard",
    "title": "Example: classifying the MNIST dataset",
    "section": "Let’s explore our model’s metrics with TensorBoard",
    "text": "Let’s explore our model’s metrics with TensorBoard\nTensorBoard is a web visualization toolkit developed by TensorFlow which can be used with PyTorch.\nBecause we have sent our model’s metrics logs to TensorBoard as part of our code, a directory called runs with those logs was created in our ~/mnist directory.\n\nLaunch TensorBoard\nTensorBoard requires too much processing power to be run on the login node. When you run long jobs, the best strategy is to launch it in the background as part of the job. This allows you to monitor your model as it is running (and cancel it if things don’t look right).\n\nExample:\n\n#!/bin/bash\n#SBATCH ...\n#SBATCH ...\n\ntensorboard --logdir=runs --host 0.0.0.0 &\npython ~/mnist/nn.py\nBecause we only have 1 GPU and are taking turns running our jobs, we need to keep our jobs very short here. So we will launch a separate job for TensorBoard. This time, we will launch an interactive job:\nsalloc --time=1:0:0 --mem=2000M\nTo launch TensorBoard, we need to activate our Python virtual environment (TensorBoard was installed by pip):\nsource ~/projects/def-sponsor00/env/bin/activate\nThen we can launch TensorBoard in the background:\ntensorboard --logdir=~/mnist/runs --host 0.0.0.0 &\nNow, we need to create a connection with SSH tunnelling between your computer and the compute note running your TensorBoard job.\n\n\nConnect to TensorBoard from your computer\nFrom a new terminal on your computer, run:\nssh -NfL localhost:6006:&lt;hostname&gt;:6006 userxxx@uu.c3.ca\n\nReplace &lt;hostname&gt; by the name of the compute node running your salloc job. You can find it by looking at your prompt (your prompt shows &lt;username&gt;@&lt;hostname&gt;).\nReplace &lt;userxxx&gt; by your user name.\n\nNow, you can open a browser on your computer and access TensorBoard at http://localhost:6006.",
    "crumbs": [
      "AI",
      "<b><em>PyTorch</em></b>",
      "Example: the MNIST"
    ]
  },
  {
    "objectID": "ai/pt/pt_nn.html",
    "href": "ai/pt/pt_nn.html",
    "title": "Introduction to neural networks",
    "section": "",
    "text": "3Blue1Brown by Grant Sanderson has a series of 4 videos on neural networks which is easy to watch, fun, and does an excellent job at explaining the functioning of a simple neural network. In this section, we will watch the first 2 videos as an introduction to what neural networks are and how they learn.",
    "crumbs": [
      "AI",
      "<b><em>PyTorch</em></b>",
      "Introduction to NN"
    ]
  },
  {
    "objectID": "ai/pt/pt_nn.html#what-are-nn-19-min",
    "href": "ai/pt/pt_nn.html#what-are-nn-19-min",
    "title": "Introduction to neural networks",
    "section": "What are NN? (19 min)",
    "text": "What are NN? (19 min)\n\n\nAs you develop your own ML models, if you find that your mathematical background is shaky, 3blue1brown also has an excellent series of videos on linear algebra and an equally great one on calculus.",
    "crumbs": [
      "AI",
      "<b><em>PyTorch</em></b>",
      "Introduction to NN"
    ]
  },
  {
    "objectID": "ai/pt/pt_nn.html#how-do-nn-learn-21-min",
    "href": "ai/pt/pt_nn.html#how-do-nn-learn-21-min",
    "title": "Introduction to neural networks",
    "section": "How do NN learn? (21 min)",
    "text": "How do NN learn? (21 min)",
    "crumbs": [
      "AI",
      "<b><em>PyTorch</em></b>",
      "Introduction to NN"
    ]
  },
  {
    "objectID": "ai/pt/pt_nn.html#nn-vs-biological-neurons-and-types-of-nn",
    "href": "ai/pt/pt_nn.html#nn-vs-biological-neurons-and-types-of-nn",
    "title": "Introduction to neural networks",
    "section": "NN vs biological neurons and types of NN",
    "text": "NN vs biological neurons and types of NN\nSlides (Click and wait: the presentation might take a few instants to load)",
    "crumbs": [
      "AI",
      "<b><em>PyTorch</em></b>",
      "Introduction to NN"
    ]
  },
  {
    "objectID": "ai/pt/pt_pytorch.html",
    "href": "ai/pt/pt_pytorch.html",
    "title": "The PyTorch API",
    "section": "",
    "text": "PyTorch is a free and open-source machine learning and scientific computing framework based on Torch. While Torch uses a scripting language based on Lua, PyTorch has a Python and a C++ interface.\nCreated by Meta AI (formerly Facebook, Inc.) in 2017, it is now a project of The Linux Foundation.\nPyTorch is widely used in academia and research. Part of its popularity stems from the fact that the Python interface is truly pythonic in nature, making it easier to learn than other popular frameworks such as TensorFlow.\nThe PyTorch API is vast and complex. This section links to the key components to get you started.",
    "crumbs": [
      "AI",
      "<b><em>PyTorch</em></b>",
      "The PyTorch API"
    ]
  },
  {
    "objectID": "ai/pt/pt_pytorch.html#domain-specific-libraries",
    "href": "ai/pt/pt_pytorch.html#domain-specific-libraries",
    "title": "The PyTorch API",
    "section": "Domain-specific libraries",
    "text": "Domain-specific libraries\nPyTorch is a large framework with domain-specific libraries:\n\nTorchVision for computer vision,\nTorchText for natural languages,\nTorchAudio for audio and signal processing.\n\nThese libraries contain standard datasets and utilities specific to the data in those fields.",
    "crumbs": [
      "AI",
      "<b><em>PyTorch</em></b>",
      "The PyTorch API"
    ]
  },
  {
    "objectID": "ai/pt/pt_pytorch.html#loading-data",
    "href": "ai/pt/pt_pytorch.html#loading-data",
    "title": "The PyTorch API",
    "section": "Loading data",
    "text": "Loading data\ntorch.utils.data contains everything you need create data loaders (iterables that present the data to a model).",
    "crumbs": [
      "AI",
      "<b><em>PyTorch</em></b>",
      "The PyTorch API"
    ]
  },
  {
    "objectID": "ai/pt/pt_pytorch.html#building-models",
    "href": "ai/pt/pt_pytorch.html#building-models",
    "title": "The PyTorch API",
    "section": "Building models",
    "text": "Building models\ntorch.nn contains the elements you need to build your model architecture and chose a loss function.",
    "crumbs": [
      "AI",
      "<b><em>PyTorch</em></b>",
      "The PyTorch API"
    ]
  },
  {
    "objectID": "ai/pt/pt_pytorch.html#training",
    "href": "ai/pt/pt_pytorch.html#training",
    "title": "The PyTorch API",
    "section": "Training",
    "text": "Training\nTraining a model consists of optimizing the model parameters.\ntorch.autograd contains the tools for automatic differentiation (to compute the gradients, that is the tensors containing the partial derivatives of the error with respect to the parameters of the functions in the model) and torch.optim contains optimization algorithms that can be used for gradient descent.",
    "crumbs": [
      "AI",
      "<b><em>PyTorch</em></b>",
      "The PyTorch API"
    ]
  },
  {
    "objectID": "ai/pt/pt_supervised_learning.html",
    "href": "ai/pt/pt_supervised_learning.html",
    "title": "Understanding supervised learning",
    "section": "",
    "text": "In supervised learning, neural networks learn by adjusting their parameters automatically in an iterative manner. This is derived from Arthur Samuel’s concept.\nIt is important to get a good understanding of this process, so let’s go over it step by step."
  },
  {
    "objectID": "ai/pt/pt_supervised_learning.html#decide-on-an-architecture",
    "href": "ai/pt/pt_supervised_learning.html#decide-on-an-architecture",
    "title": "Understanding supervised learning",
    "section": "Decide on an architecture",
    "text": "Decide on an architecture\n\nThe architecture won’t change during training. This is set. The type of architecture you choose (e.g. CNN, Transformer, etc.) depends on the type of data you have (e.g. vision, textual, etc.). The depth and breadth of your network depend on the amount of data and computing resource you have."
  },
  {
    "objectID": "ai/pt/pt_supervised_learning.html#set-some-initial-parameters",
    "href": "ai/pt/pt_supervised_learning.html#set-some-initial-parameters",
    "title": "Understanding supervised learning",
    "section": "Set some initial parameters",
    "text": "Set some initial parameters\n\nYou can initialize them randomly or get much better ones through transfer learning.\nWhile the parameters are also part of the model, those will change during training."
  },
  {
    "objectID": "ai/pt/pt_supervised_learning.html#get-some-labelled-data",
    "href": "ai/pt/pt_supervised_learning.html#get-some-labelled-data",
    "title": "Understanding supervised learning",
    "section": "Get some labelled data",
    "text": "Get some labelled data\n\nWhen we say that we need a lot of data for machine learning, we mean “lots of labelled data” as this is what gets used for training models."
  },
  {
    "objectID": "ai/pt/pt_supervised_learning.html#make-sure-to-keep-some-data-for-testing",
    "href": "ai/pt/pt_supervised_learning.html#make-sure-to-keep-some-data-for-testing",
    "title": "Understanding supervised learning",
    "section": "Make sure to keep some data for testing",
    "text": "Make sure to keep some data for testing\n\nThose data won’t be used for training the model. Often people keep around 20% of their data for testing."
  },
  {
    "objectID": "ai/pt/pt_supervised_learning.html#pass-data-and-parameters-through-the-architecture",
    "href": "ai/pt/pt_supervised_learning.html#pass-data-and-parameters-through-the-architecture",
    "title": "Understanding supervised learning",
    "section": "Pass data and parameters through the architecture",
    "text": "Pass data and parameters through the architecture\n\nThe train data are the inputs and the process of calculating the outputs is the forward pass."
  },
  {
    "objectID": "ai/pt/pt_supervised_learning.html#the-outputs-of-the-model-are-predictions",
    "href": "ai/pt/pt_supervised_learning.html#the-outputs-of-the-model-are-predictions",
    "title": "Understanding supervised learning",
    "section": "The outputs of the model are predictions",
    "text": "The outputs of the model are predictions"
  },
  {
    "objectID": "ai/pt/pt_supervised_learning.html#compare-those-predictions-to-the-train-labels",
    "href": "ai/pt/pt_supervised_learning.html#compare-those-predictions-to-the-train-labels",
    "title": "Understanding supervised learning",
    "section": "Compare those predictions to the train labels",
    "text": "Compare those predictions to the train labels\n\nSince our data was labelled, we know what the true outputs are."
  },
  {
    "objectID": "ai/pt/pt_supervised_learning.html#calculate-train-loss",
    "href": "ai/pt/pt_supervised_learning.html#calculate-train-loss",
    "title": "Understanding supervised learning",
    "section": "Calculate train loss",
    "text": "Calculate train loss\n\nThe deviation of our predictions from the true outputs gives us a measure of training loss."
  },
  {
    "objectID": "ai/pt/pt_supervised_learning.html#adjust-parameters",
    "href": "ai/pt/pt_supervised_learning.html#adjust-parameters",
    "title": "Understanding supervised learning",
    "section": "Adjust parameters",
    "text": "Adjust parameters\n\nThe parameters get automatically adjusted to reduce the training loss through the mechanism of backpropagation.\nThis is the actual training part.\nThis process is repeated many times. Training models is pretty much a giant for loop."
  },
  {
    "objectID": "ai/pt/pt_supervised_learning.html#from-model-to-program",
    "href": "ai/pt/pt_supervised_learning.html#from-model-to-program",
    "title": "Understanding supervised learning",
    "section": "From model to program",
    "text": "From model to program\n\nRemember that the model architecture is fixed, but that the parameters change at each iteration of the training process.\nWhile the labelled data are key to training, what we are really interested in is the combination of architecture + final parameters.\n\nWhen the training is over, the parameters become fixed. Which means that our model now behaves like a classic program."
  },
  {
    "objectID": "ai/pt/pt_supervised_learning.html#evaluate-the-model",
    "href": "ai/pt/pt_supervised_learning.html#evaluate-the-model",
    "title": "Understanding supervised learning",
    "section": "Evaluate the model",
    "text": "Evaluate the model\n\nWe can now use the testing set (which was never used to train the model) to evaluate our model: if we pass the test inputs through our program, we get some predictions that we can compare to the test labels (which are the true outputs).\nThis gives us the test loss: a measure of how well our model performs."
  },
  {
    "objectID": "ai/pt/pt_supervised_learning.html#use-the-model",
    "href": "ai/pt/pt_supervised_learning.html#use-the-model",
    "title": "Understanding supervised learning",
    "section": "Use the model",
    "text": "Use the model\n\nNow that we have a program, we can use it on unlabelled inputs to get what people ultimately want: unknown outputs. This is when we put our model to actual use to solve some problem."
  },
  {
    "objectID": "ai/pt/pt_training.html",
    "href": "ai/pt/pt_training.html",
    "title": "Training",
    "section": "",
    "text": "After you have created the data loaders and defined your model, it is time to improve the weights and biases through training."
  },
  {
    "objectID": "ai/pt/pt_training.html#chose-hyperparameters",
    "href": "ai/pt/pt_training.html#chose-hyperparameters",
    "title": "Training",
    "section": "Chose hyperparameters",
    "text": "Chose hyperparameters\nWhile the learning parameters of a model (weights and biases) are the values that get adjusted through training (and they will become part of the final program, along with the model architecture, once training is over), hyperparameters control the training process.\nThey include:\n\nthe batch size: number of samples passed through the model before the parameters are updated,\nthe number of epochs: number iterations,\nthe learning rate (lr): size of the incremental changes to model parameters at each iteration. Smaller values yield slow learning speed, while large values may miss minima.\n\nLet’s define them here:\nlearning_rate = 1e-3\nbatch_size = 64\nepochs = 5"
  },
  {
    "objectID": "ai/pt/pt_training.html#define-the-loss-function",
    "href": "ai/pt/pt_training.html#define-the-loss-function",
    "title": "Training",
    "section": "Define the loss function",
    "text": "Define the loss function\nTo assess the predicted outputs of our model against the true values from the labels, we also need a loss function (e.g. mean square error for regressions: nn.MSELoss or negative log likelihood for classification: nn.NLLLoss)\nThe machine learning literature is rich in information about various loss functions.\nHere is an example with nn.CrossEntropyLoss which combines nn.LogSoftmax and nn.NLLLoss:\nloss_fn = nn.CrossEntropyLoss()"
  },
  {
    "objectID": "ai/pt/pt_training.html#initialize-the-optimizer",
    "href": "ai/pt/pt_training.html#initialize-the-optimizer",
    "title": "Training",
    "section": "Initialize the optimizer",
    "text": "Initialize the optimizer\nThe optimization algorithm determines how the model parameters get adjusted at each iteration.\nThere are many optimizers and you need to search in the literature which one performs best for your time of model and data.\nBelow is an example with stochastic gradient descent:\noptimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n\nlr is the learning rate.\nmomentum is a method increasing convergence rate and reducing oscillation for SDG."
  },
  {
    "objectID": "ai/pt/pt_training.html#define-the-train-and-test-loops",
    "href": "ai/pt/pt_training.html#define-the-train-and-test-loops",
    "title": "Training",
    "section": "Define the train and test loops",
    "text": "Define the train and test loops\nFinally, we need to define the train and test loops.\nThe train loop:\n\ngets a batch of training data from the DataLoader,\nresets the gradients of model parameters with optimizer.zero_grad(),\ncalculates predictions from the model for an input batch,\ncalculates the loss for that set of predictions vs. the labels on the dataset,\ncalculates the backward gradients over the learning parameters (that’s the backpropagation) with loss.backward(),\nadjusts the parameters by the gradients collected in the backward pass with optimizer.step().\n\nThe test loop evaluates the model’s performance against the test data.\ndef train(dataloader, model, loss_fn, optimizer):\n    size = len(dataloader.dataset)\n    for batch, (X, y) in enumerate(dataloader):\n        # Compute prediction and loss\n        pred = model(X)\n        loss = loss_fn(pred, y)\n\n        # Backpropagation\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if batch % 100 == 0:\n            loss, current = loss.item(), batch * len(X)\n            print(f\"loss: {loss:&gt;7f}  [{current:&gt;5d}/{size:&gt;5d}]\")\n\n\ndef test(dataloader, model, loss_fn):\n    size = len(dataloader.dataset)\n    num_batches = len(dataloader)\n    test_loss, correct = 0, 0\n\n    with torch.no_grad():\n        for X, y in dataloader:\n            pred = model(X)\n            test_loss += loss_fn(pred, y).item()\n            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n\n    test_loss /= num_batches\n    correct /= size\n    print(f\"Test Error: \\n Accuracy: {(100*correct):&gt;0.1f}%, Avg loss: {test_loss:&gt;8f} \\n\")"
  },
  {
    "objectID": "ai/pt/pt_training.html#train",
    "href": "ai/pt/pt_training.html#train",
    "title": "Training",
    "section": "Train",
    "text": "Train\nTo train our model, we run the loop over the epochs:\nfor t in range(epochs):\n    print(f\"Epoch {t+1}\\n-------------------------------\")\n    train(train_dataloader, model, loss_fn, optimizer)\n    test(test_dataloader, model, loss_fn)\nprint(\"Training completed\")"
  },
  {
    "objectID": "ai/pt/wb_fastai.html",
    "href": "ai/pt/wb_fastai.html",
    "title": "fastai",
    "section": "",
    "text": "fastai is a deep learning library which provides practitioners with high-level components that can quickly and easily provide state-of-the-art results in standard deep learning domains. This webinar will take a closer look at the features and functionality of fastai.",
    "crumbs": [
      "AI",
      "<b><em>Webinars</em></b>",
      "fastai"
    ]
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#acknowledgements",
    "href": "ai/pt/wb_torchtensors_slides.html#acknowledgements",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nMany drawings in this webinar come from the book:\n\nThe section on storage is also highly inspired by it"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#using-tensors-locally",
    "href": "ai/pt/wb_torchtensors_slides.html#using-tensors-locally",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Using tensors locally",
    "text": "Using tensors locally\nYou need to have Python and PyTorch installed\nAdditionally, you might want to use an IDE such as elpy if you are an Emacs user, JupyterLab, etc.\n\nNote that PyTorch does not yet support Python 3.10 except in some Linux distributions or on systems where a wheel has been built For the time being, you might have to use it with Python 3.9"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#using-tensors-on-cc-clusters",
    "href": "ai/pt/wb_torchtensors_slides.html#using-tensors-on-cc-clusters",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Using tensors on CC clusters",
    "text": "Using tensors on CC clusters\n(In the terminal)\nList available wheels and compatible Python versions:\navail_wheels \"torch*\"\nList available Python versions:\nmodule avail python\nGet setup:\nmodule load python/3.9.6             # Load a sensible Python version\nvirtualenv --no-download env         # Create a virtual env\nsource env/bin/activate              # Activate the virtual env\npip install --no-index --upgrade pip # Update pip\npip install --no-index torch         # Install PyTorch\nYou can then launch jobs with sbatch or salloc\nLeave the virtual env with the command: deactivate"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#outline",
    "href": "ai/pt/wb_torchtensors_slides.html#outline",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Outline",
    "text": "Outline\n\nWhat is a PyTorch tensor?\nMemory storage\nData type (dtype)\nBasic operations\nWorking with NumPy\nLinear algebra\nHarvesting the power of GPUs\nDistributed operations"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#outline-1",
    "href": "ai/pt/wb_torchtensors_slides.html#outline-1",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Outline",
    "text": "Outline\n\nWhat is a PyTorch tensor?\nMemory storage\nData type (dtype)\nBasic operations\nWorking with NumPy\nLinear algebra\nHarvesting the power of GPUs\nDistributed operations"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#ann-do-not-process-information-directly",
    "href": "ai/pt/wb_torchtensors_slides.html#ann-do-not-process-information-directly",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "ANN do not process information directly",
    "text": "ANN do not process information directly\n\n\nModified from Stevens, E., Antiga, L., & Viehmann, T. (2020). Deep learning with PyTorch. Manning Publications"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#it-needs-to-be-converted-to-numbers",
    "href": "ai/pt/wb_torchtensors_slides.html#it-needs-to-be-converted-to-numbers",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "It needs to be converted to numbers",
    "text": "It needs to be converted to numbers\n\n\nModified from Stevens, E., Antiga, L., & Viehmann, T. (2020). Deep learning with PyTorch. Manning Publications"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#these-numbers-must-be-stored-in-a-data-structure",
    "href": "ai/pt/wb_torchtensors_slides.html#these-numbers-must-be-stored-in-a-data-structure",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "These numbers must be stored in a data structure",
    "text": "These numbers must be stored in a data structure\n\nPyTorch tensors are Python objects holding multidimensional arrays\n\n\n\nStevens, E., Antiga, L., & Viehmann, T. (2020). Deep learning with PyTorch. Manning Publications"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#why-a-new-object-when-numpy-already-exists",
    "href": "ai/pt/wb_torchtensors_slides.html#why-a-new-object-when-numpy-already-exists",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Why a new object when NumPy already exists?",
    "text": "Why a new object when NumPy already exists?\n\n\nCan run on accelerators (GPUs, TPUs…)\nKeep track of computation graphs, allowing automatic differentiation\nFuture plan for sharded tensors to run distributed computations"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#what-is-a-pytorch-tensor-2",
    "href": "ai/pt/wb_torchtensors_slides.html#what-is-a-pytorch-tensor-2",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "What is a PyTorch tensor?",
    "text": "What is a PyTorch tensor?\nPyTorch is foremost a deep learning library\nIn deep learning, the information contained in objects of interest (e.g. images, texts, sounds) is converted to floating-point numbers (e.g. pixel values, token values, frequencies)\nAs this information is complex, multiple dimensions are required (e.g. two dimensions for the width and height of an image, plus one dimension for the RGB colour channels)\nAdditionally, items are grouped into batches to be processed together, adding yet another dimension\nMultidimensional arrays are thus particularly well suited for deep learning"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#what-is-a-pytorch-tensor-3",
    "href": "ai/pt/wb_torchtensors_slides.html#what-is-a-pytorch-tensor-3",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "What is a PyTorch tensor?",
    "text": "What is a PyTorch tensor?\nArtificial neurons perform basic computations on these tensors\nTheir number however is huge and computing efficiency is paramount\nGPUs/TPUs are particularly well suited to perform many simple operations in parallel\nThe very popular NumPy library has, at its core, a mature multidimensional array object well integrated into the scientific Python ecosystem\nBut the PyTorch tensor has additional efficiency characteristics ideal for machine learning and it can be converted to/from NumPy’s ndarray if needed"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#outline-2",
    "href": "ai/pt/wb_torchtensors_slides.html#outline-2",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Outline",
    "text": "Outline\n\nWhat is a PyTorch tensor?\nMemory storage\nData type (dtype)\nBasic operations\nWorking with NumPy\nLinear algebra\nHarvesting the power of GPUs\nDistributed operations"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#efficient-memory-storage",
    "href": "ai/pt/wb_torchtensors_slides.html#efficient-memory-storage",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Efficient memory storage",
    "text": "Efficient memory storage\nIn Python, collections (lists, tuples) are groupings of boxed Python objects\nPyTorch tensors and NumPy ndarrays are made of unboxed C numeric types\n\n\nStevens, E., Antiga, L., & Viehmann, T. (2020). Deep learning with PyTorch. Manning Publications"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#efficient-memory-storage-1",
    "href": "ai/pt/wb_torchtensors_slides.html#efficient-memory-storage-1",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Efficient memory storage",
    "text": "Efficient memory storage\nThey are usually contiguous memory blocks, but the main difference is that they are unboxed: floats will thus take 4 (32-bit) or 8 (64-bit) bytes each\nBoxed values take up more memory (memory for the pointer + memory for the primitive)\n\n\nStevens, E., Antiga, L., & Viehmann, T. (2020). Deep learning with PyTorch. Manning Publications"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#implementation",
    "href": "ai/pt/wb_torchtensors_slides.html#implementation",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Implementation",
    "text": "Implementation\nUnder the hood, the values of a PyTorch tensor are stored as a torch.Storage instance which is a one-dimensional array\n\nimport torch\nt = torch.arange(10.).view(2, 5); print(t) # Functions explained later\ntensor([[ 0.,  1.,  2., 3.,  4.],\n        [ 5.,  6.,  7.,  8.,  9.]])"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#implementation-1",
    "href": "ai/pt/wb_torchtensors_slides.html#implementation-1",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Implementation",
    "text": "Implementation\nstorage = t.storage(); print(storage)\n 0.0\n 1.0\n 2.0\n 3.0\n 4.0\n 5.0\n 6.0\n 7.0\n 8.0\n 9.0\n[torch.FloatStorage of size 10]"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#implementation-2",
    "href": "ai/pt/wb_torchtensors_slides.html#implementation-2",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Implementation",
    "text": "Implementation\nThe storage can be indexed\nstorage[3]\n3.0"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#implementation-3",
    "href": "ai/pt/wb_torchtensors_slides.html#implementation-3",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Implementation",
    "text": "Implementation\nstorage[3] = 10.0; print(storage)\n 0.0\n 1.0\n 2.0\n 10.0\n 4.0\n 5.0\n 6.0\n 7.0\n 8.0\n 9.0\n[torch.FloatStorage of size 10]"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#implementation-4",
    "href": "ai/pt/wb_torchtensors_slides.html#implementation-4",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Implementation",
    "text": "Implementation\nTo view a multidimensional array from storage, we need metadata:\n\nthe size (shape in NumPy) sets the number of elements in each dimension\nthe offset indicates where the first element of the tensor is in the storage\nthe stride establishes the increment between each element"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#storage-metadata",
    "href": "ai/pt/wb_torchtensors_slides.html#storage-metadata",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Storage metadata",
    "text": "Storage metadata\n\n\nStevens, E., Antiga, L., & Viehmann, T. (2020). Deep learning with PyTorch. Manning Publications"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#storage-metadata-1",
    "href": "ai/pt/wb_torchtensors_slides.html#storage-metadata-1",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Storage metadata",
    "text": "Storage metadata\nt.size()\nt.storage_offset()\nt.stride()\ntorch.Size([2, 5])\n0\n(5, 1)"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#storage-metadata-2",
    "href": "ai/pt/wb_torchtensors_slides.html#storage-metadata-2",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Storage metadata",
    "text": "Storage metadata"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#sharing-storage",
    "href": "ai/pt/wb_torchtensors_slides.html#sharing-storage",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Sharing storage",
    "text": "Sharing storage\nMultiple tensors can use the same storage, saving a lot of memory since the metadata is a lot lighter than a whole new array\n\n\nStevens, E., Antiga, L., & Viehmann, T. (2020). Deep learning with PyTorch. Manning Publications"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#transposing-in-2-dimensions",
    "href": "ai/pt/wb_torchtensors_slides.html#transposing-in-2-dimensions",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Transposing in 2 dimensions",
    "text": "Transposing in 2 dimensions\nt = torch.tensor([[3, 1, 2], [4, 1, 7]]); print(t)\nt.size()\nt.t()\nt.t().size()\ntensor([[3, 1, 2],\n        [4, 1, 7]])\ntorch.Size([2, 3])\ntensor([[3, 4],\n        [1, 1],\n        [2, 7]])\ntorch.Size([3, 2])"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#transposing-in-2-dimensions-1",
    "href": "ai/pt/wb_torchtensors_slides.html#transposing-in-2-dimensions-1",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Transposing in 2 dimensions",
    "text": "Transposing in 2 dimensions\n= flipping the stride elements around\n\n\nStevens, E., Antiga, L., & Viehmann, T. (2020). Deep learning with PyTorch. Manning Publications"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#transposing-in-higher-dimensions",
    "href": "ai/pt/wb_torchtensors_slides.html#transposing-in-higher-dimensions",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Transposing in higher dimensions",
    "text": "Transposing in higher dimensions\ntorch.t() is a shorthand for torch.transpose(0, 1):\ntorch.equal(t.t(), t.transpose(0, 1))\nTrue\nWhile torch.t() only works for 2D tensors, torch.transpose() can be used to transpose 2 dimensions in tensors of any number of dimensions"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#transposing-in-higher-dimensions-1",
    "href": "ai/pt/wb_torchtensors_slides.html#transposing-in-higher-dimensions-1",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Transposing in higher dimensions",
    "text": "Transposing in higher dimensions\nt = torch.zeros(1, 2, 3); print(t)\n\nt.size()\nt.stride()\ntensor([[[0., 0., 0.],\n         [0., 0., 0.]]])\n\ntorch.Size([1, 2, 3])\n(6, 3, 1)"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#transposing-in-higher-dimensions-2",
    "href": "ai/pt/wb_torchtensors_slides.html#transposing-in-higher-dimensions-2",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Transposing in higher dimensions",
    "text": "Transposing in higher dimensions\nt.transpose(0, 1)\n\nt.transpose(0, 1).size()\nt.transpose(0, 1).stride()\ntensor([[[0., 0., 0.]],\n        [[0., 0., 0.]]])\n\ntorch.Size([2, 1, 3])\n(3, 6, 1)  # Notice how transposing flipped 2 elements of the stride"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#transposing-in-higher-dimensions-3",
    "href": "ai/pt/wb_torchtensors_slides.html#transposing-in-higher-dimensions-3",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Transposing in higher dimensions",
    "text": "Transposing in higher dimensions\nt.transpose(0, 2)\n\nt.transpose(0, 2).size()\nt.transpose(0, 2).stride()\ntensor([[[0.],\n         [0.]],\n        [[0.],\n         [0.]],\n        [[0.],\n         [0.]]])\n\ntorch.Size([3, 2, 1])\n(1, 3, 6)"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#transposing-in-higher-dimensions-4",
    "href": "ai/pt/wb_torchtensors_slides.html#transposing-in-higher-dimensions-4",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Transposing in higher dimensions",
    "text": "Transposing in higher dimensions\nt.transpose(1, 2)\n\nt.transpose(1, 2).size()\nt.transpose(1, 2).stride()\ntensor([[[0., 0.],\n         [0., 0.],\n         [0., 0.]]])\n\ntorch.Size([1, 3, 2])\n(6, 1, 3)"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#outline-3",
    "href": "ai/pt/wb_torchtensors_slides.html#outline-3",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Outline",
    "text": "Outline\n\nWhat is a PyTorch tensor?\nMemory storage\nData type (dtype)\nBasic operations\nWorking with NumPy\nLinear algebra\nHarvesting the power of GPUs\nDistributed operations"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#default-dtype",
    "href": "ai/pt/wb_torchtensors_slides.html#default-dtype",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Default dtype",
    "text": "Default dtype\nSince PyTorch tensors were built with utmost efficiency in mind for neural networks, the default data type is 32-bit floating points\nThis is sufficient for accuracy and much faster than 64-bit floating points\n\nNote that, by contrast, NumPy ndarrays use 64-bit as their default"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#list-of-pytorch-tensor-dtypes",
    "href": "ai/pt/wb_torchtensors_slides.html#list-of-pytorch-tensor-dtypes",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "List of PyTorch tensor dtypes",
    "text": "List of PyTorch tensor dtypes\n\n\n\ntorch.float16 / torch.half\n\n\n  \n\n\n16-bit / half-precision floating-point\n\n\n\n\ntorch.float32 / torch.float\n\n\n\n\n32-bit / single-precision floating-point\n\n\n\n\ntorch.float64 / torch.double\n\n\n\n\n64-bit / double-precision floating-point\n\n\n\n\n\n\n\n\n\n\ntorch.uint8\n\n\n\n\nunsigned 8-bit integers\n\n\n\n\ntorch.int8\n\n\n\n\nsigned 8-bit integers\n\n\n\n\ntorch.int16 / torch.short\n\n\n\n\nsigned 16-bit integers\n\n\n\n\ntorch.int32 / torch.int\n\n\n\n\nsigned 32-bit integers\n\n\n\n\ntorch.int64 / torch.long\n\n\n\n\nsigned 64-bit integers\n\n\n\n\n\n\n\n\n\n\n\n\ntorch.bool\n\n\n\n\nboolean"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#checking-and-changing-dtype",
    "href": "ai/pt/wb_torchtensors_slides.html#checking-and-changing-dtype",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Checking and changing dtype",
    "text": "Checking and changing dtype\nt = torch.rand(2, 3)\nprint(t)\n\n# Remember that the default dtype for PyTorch tensors is float32\nt.dtype\n\n# If dtype ≠ default, it is printed\nt2 = t.type(torch.float64)\nprint(t2)\n\nt2.dtype\ntensor([[0.8130, 0.3757, 0.7682],\n        [0.3482, 0.0516, 0.3772]])\n\ntorch.float32\n\ntensor([[0.8130, 0.3757, 0.7682],\n        [0.3482, 0.0516, 0.3772]], dtype=torch.float64)\n\ntorch.float64"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#outline-4",
    "href": "ai/pt/wb_torchtensors_slides.html#outline-4",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Outline",
    "text": "Outline\n\nWhat is a PyTorch tensor?\nMemory storage\nData type (dtype)\nBasic operations\nWorking with NumPy\nLinear algebra\nHarvesting the power of GPUs\nDistributed operations"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#creating-tensors",
    "href": "ai/pt/wb_torchtensors_slides.html#creating-tensors",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Creating tensors",
    "text": "Creating tensors\n\ntorch.tensor:    Input individual values\ntorch.arange:    Similar to range but creates a 1D tensor\ntorch.linspace:   1D linear scale tensor\ntorch.logspace:   1D log scale tensor\ntorch.rand:     Random numbers from a uniform distribution on [0, 1)\ntorch.randn:    Numbers from the standard normal distribution\ntorch.randperm:   Random permutation of integers\ntorch.empty:    Uninitialized tensor\ntorch.zeros:    Tensor filled with 0\ntorch.ones:     Tensor filled with 1\ntorch.eye:      Identity matrix"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#creating-tensors-1",
    "href": "ai/pt/wb_torchtensors_slides.html#creating-tensors-1",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Creating tensors",
    "text": "Creating tensors\ntorch.manual_seed(0)  # If you want to reproduce the result\ntorch.rand(1)\n\ntorch.manual_seed(0)  # Run before each operation to get the same result\ntorch.rand(1).item()  # Extract the value from a tensor\ntensor([0.4963])\n\n0.49625658988952637"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#creating-tensors-2",
    "href": "ai/pt/wb_torchtensors_slides.html#creating-tensors-2",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Creating tensors",
    "text": "Creating tensors\ntorch.rand(1)\ntorch.rand(1, 1)\ntorch.rand(1, 1, 1)\ntorch.rand(1, 1, 1, 1)\ntensor([0.6984])\ntensor([[0.5675]])\ntensor([[[0.8352]]])\ntensor([[[[0.2056]]]])"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#creating-tensors-3",
    "href": "ai/pt/wb_torchtensors_slides.html#creating-tensors-3",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Creating tensors",
    "text": "Creating tensors\ntorch.rand(2)\ntorch.rand(2, 2, 2, 2)\ntensor([0.5932, 0.1123])\ntensor([[[[0.1147, 0.3168],\n          [0.6965, 0.9143]],\n         [[0.9351, 0.9412],\n          [0.5995, 0.0652]]],\n        [[[0.5460, 0.1872],\n          [0.0340, 0.9442]],\n         [[0.8802, 0.0012],\n          [0.5936, 0.4158]]]])"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#creating-tensors-4",
    "href": "ai/pt/wb_torchtensors_slides.html#creating-tensors-4",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Creating tensors",
    "text": "Creating tensors\ntorch.rand(2)\ntorch.rand(3)\ntorch.rand(1, 1)\ntorch.rand(1, 1, 1)\ntorch.rand(2, 6)\ntensor([0.7682, 0.0885])\ntensor([0.1320, 0.3074, 0.6341])\ntensor([[0.4901]])\ntensor([[[0.8964]]])\ntensor([[0.4556, 0.6323, 0.3489, 0.4017, 0.0223, 0.1689],\n        [0.2939, 0.5185, 0.6977, 0.8000, 0.1610, 0.2823]])"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#creating-tensors-5",
    "href": "ai/pt/wb_torchtensors_slides.html#creating-tensors-5",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Creating tensors",
    "text": "Creating tensors\ntorch.rand(2, 4, dtype=torch.float64)  # You can set dtype\ntorch.ones(2, 1, 4, 5)\ntensor([[0.6650, 0.7849, 0.2104, 0.6767],\n        [0.1097, 0.5238, 0.2260, 0.5582]], dtype=torch.float64)\ntensor([[[[1., 1., 1., 1., 1.],\n          [1., 1., 1., 1., 1.],\n          [1., 1., 1., 1., 1.],\n          [1., 1., 1., 1., 1.]]],\n        [[[1., 1., 1., 1., 1.],\n          [1., 1., 1., 1., 1.],\n          [1., 1., 1., 1., 1.],\n          [1., 1., 1., 1., 1.]]]])"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#creating-tensors-6",
    "href": "ai/pt/wb_torchtensors_slides.html#creating-tensors-6",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Creating tensors",
    "text": "Creating tensors\nt = torch.rand(2, 3); print(t)\ntorch.zeros_like(t)             # Matches the size of t\ntorch.ones_like(t)\ntorch.randn_like(t)\ntensor([[0.4051, 0.6394, 0.0871],\n        [0.4509, 0.5255, 0.5057]])\ntensor([[0., 0., 0.],\n        [0., 0., 0.]])\ntensor([[1., 1., 1.],\n        [1., 1., 1.]])\ntensor([[-0.3088, -0.0104,  1.0461],\n        [ 0.9233,  0.0236, -2.1217]])"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#creating-tensors-7",
    "href": "ai/pt/wb_torchtensors_slides.html#creating-tensors-7",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Creating tensors",
    "text": "Creating tensors\ntorch.arange(2, 10, 4)    # From 2 to 10 in increments of 4\ntorch.linspace(2, 10, 4)  # 4 elements from 2 to 10 on the linear scale\ntorch.logspace(2, 10, 4)  # Same on the log scale\ntorch.randperm(4)\ntorch.eye(3)\ntensor([2, 6])\ntensor([2.0000,  4.6667,  7.3333, 10.0000])\ntensor([1.0000e+02, 4.6416e+04, 2.1544e+07, 1.0000e+10])\ntensor([1, 3, 2, 0])\ntensor([[1., 0., 0.],\n        [0., 1., 0.],\n        [0., 0., 1.]])"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#tensor-information",
    "href": "ai/pt/wb_torchtensors_slides.html#tensor-information",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Tensor information",
    "text": "Tensor information\nt = torch.rand(2, 3); print(t)\nt.size()\nt.dim()\nt.numel()\ntensor([[0.5885, 0.7005, 0.1048],\n        [0.1115, 0.7526, 0.0658]])\ntorch.Size([2, 3])\n2\n6"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#tensor-indexing",
    "href": "ai/pt/wb_torchtensors_slides.html#tensor-indexing",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Tensor indexing",
    "text": "Tensor indexing\nx = torch.rand(3, 4)\nx[:]                 # With a range, the comma is implicit: same as x[:, ]\nx[:, 2]\nx[1, :]\nx[2, 3]\ntensor([[0.6575, 0.4017, 0.7391, 0.6268],\n        [0.2835, 0.0993, 0.7707, 0.1996],\n        [0.4447, 0.5684, 0.2090, 0.7724]])\ntensor([0.7391, 0.7707, 0.2090])\ntensor([0.2835, 0.0993, 0.7707, 0.1996])\ntensor(0.7724)"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#tensor-indexing-1",
    "href": "ai/pt/wb_torchtensors_slides.html#tensor-indexing-1",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Tensor indexing",
    "text": "Tensor indexing\nx[-1:]        # Last element (implicit comma, so all columns)\n\n# No range, no implicit comma\n# Indexing from a list of tensors, so the result is a one dimensional tensor\n# (Each dimension is a list of tensors of the previous dimension)\nx[-1]\n\nx[-1].size()  # Same number of dimensions than x (2 dimensions)\n\nx[-1:].size() # We dropped one dimension\ntensor([[0.8168, 0.0879, 0.2642, 0.3777]])\n\ntensor([0.8168, 0.0879, 0.2642, 0.3777])\n\ntorch.Size([4])\n\ntorch.Size([1, 4])"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#tensor-indexing-2",
    "href": "ai/pt/wb_torchtensors_slides.html#tensor-indexing-2",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Tensor indexing",
    "text": "Tensor indexing\nx[0:1]     # Python ranges are inclusive to the left, not the right\nx[:-1]     # From start to one before last (and implicit comma)\nx[0:3:2]   # From 0th (included) to 3rd (excluded) in increment of 2\ntensor([[0.5873, 0.0225, 0.7234, 0.4538]])\ntensor([[0.5873, 0.0225, 0.7234, 0.4538],\n        [0.9525, 0.0111, 0.6421, 0.4647]])\ntensor([[0.5873, 0.0225, 0.7234, 0.4538],\n        [0.8168, 0.0879, 0.2642, 0.3777]])"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#tensor-indexing-3",
    "href": "ai/pt/wb_torchtensors_slides.html#tensor-indexing-3",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Tensor indexing",
    "text": "Tensor indexing\nx[None]          # Adds a dimension of size one as the 1st dimension\nx.size()\nx[None].size()\ntensor([[[0.5873, 0.0225, 0.7234, 0.4538],\n         [0.9525, 0.0111, 0.6421, 0.4647],\n         [0.8168, 0.0879, 0.2642, 0.3777]]])\ntorch.Size([3, 4])\ntorch.Size([1, 3, 4])"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#a-word-of-caution-about-indexing",
    "href": "ai/pt/wb_torchtensors_slides.html#a-word-of-caution-about-indexing",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "A word of caution about indexing",
    "text": "A word of caution about indexing\nWhile indexing elements of a tensor to extract some of the data as a final step of some computation is fine, you should not use indexing to run operations on tensor elements in a loop as this would be extremely inefficient\nInstead, you want to use vectorized operations"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#vectorized-operations",
    "href": "ai/pt/wb_torchtensors_slides.html#vectorized-operations",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Vectorized operations",
    "text": "Vectorized operations\nSince PyTorch tensors are homogeneous (i.e. made of a single data type), as with NumPy’s ndarrays, operations are vectorized and thus staggeringly fast\nNumPy is mostly written in C and PyTorch in C++. With either library, when you run vectorized operations on arrays/tensors, you don’t use raw Python (slow) but compiled C/C++ code (much faster)\nHere is an excellent post explaining Python vectorization and why it makes such a big difference"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#vectorized-operations-comparison",
    "href": "ai/pt/wb_torchtensors_slides.html#vectorized-operations-comparison",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Vectorized operations: comparison",
    "text": "Vectorized operations: comparison\nRaw Python method\n# Create tensor. We use float64 here to avoid truncation errors\nt = torch.rand(10**6, dtype=torch.float64)\n\n# Initialize sum\nsum = 0\n\n# Run loop\nfor i in range(len(t)): sum += t[i]\n\n# Print result\nprint(sum)\nVectorized function\nt.sum()"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#vectorized-operations-comparison-1",
    "href": "ai/pt/wb_torchtensors_slides.html#vectorized-operations-comparison-1",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Vectorized operations: comparison",
    "text": "Vectorized operations: comparison\nBoth methods give the same result\n\nThis is why we used float64:\nWhile the accuracy remains excellent with float32 if we use the PyTorch function torch.sum(), the raw Python loop gives a fairly inaccurate result\n\ntensor(500023.0789, dtype=torch.float64)\ntensor(500023.0789, dtype=torch.float64)"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#vectorized-operations-timing",
    "href": "ai/pt/wb_torchtensors_slides.html#vectorized-operations-timing",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Vectorized operations: timing",
    "text": "Vectorized operations: timing\nLet’s compare the timing with PyTorch built-in benchmark utility\n# Load utility\nimport torch.utils.benchmark as benchmark\n\n# Create a function for our loop\ndef sum_loop(t, sum):\n    for i in range(len(t)): sum += t[i]"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#vectorized-operations-timing-1",
    "href": "ai/pt/wb_torchtensors_slides.html#vectorized-operations-timing-1",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Vectorized operations: timing",
    "text": "Vectorized operations: timing\nNow we can create the timers\nt0 = benchmark.Timer(\n    stmt='sum_loop(t, sum)',\n    setup='from __main__ import sum_loop',\n    globals={'t': t, 'sum': sum})\n\nt1 = benchmark.Timer(\n    stmt='t.sum()',\n    globals={'t': t})"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#vectorized-operations-timing-2",
    "href": "ai/pt/wb_torchtensors_slides.html#vectorized-operations-timing-2",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Vectorized operations: timing",
    "text": "Vectorized operations: timing\nLet’s time 100 runs to have a reliable benchmark\nprint(t0.timeit(100))\nprint(t1.timeit(100))\n\nI ran the code on my laptop with a dedicated GPU and 32GB RAM"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#vectorized-operations-timing-3",
    "href": "ai/pt/wb_torchtensors_slides.html#vectorized-operations-timing-3",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Vectorized operations: timing",
    "text": "Vectorized operations: timing\nTiming of raw Python loop\nsum_loop(t, sum)\nsetup: from __main__ import sum_loop\n  1.37 s\n  1 measurement, 100 runs , 1 thread\nTiming of vectorized function\nt.sum()\n  191.26 us\n  1 measurement, 100 runs , 1 thread"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#vectorized-operations-timing-4",
    "href": "ai/pt/wb_torchtensors_slides.html#vectorized-operations-timing-4",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Vectorized operations: timing",
    "text": "Vectorized operations: timing\nSpeedup:\n1.37/(191.26 * 10**-6) = 7163\n\nThe vectorized function runs more than 7,000 times faster!!!"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#even-more-important-on-gpus",
    "href": "ai/pt/wb_torchtensors_slides.html#even-more-important-on-gpus",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Even more important on GPUs",
    "text": "Even more important on GPUs\nWe will talk about GPUs in detail later\nTiming of raw Python loop on GPU (actually slower on GPU!)\nsum_loop(t, sum)\nsetup: from __main__ import sum_loop\n  4.54 s\n  1 measurement, 100 runs , 1 thread\nTiming of vectorized function on GPU (here we do get a speedup)\nt.sum()\n  50.62 us\n  1 measurement, 100 runs , 1 thread"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#even-more-important-on-gpus-1",
    "href": "ai/pt/wb_torchtensors_slides.html#even-more-important-on-gpus-1",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Even more important on GPUs",
    "text": "Even more important on GPUs\nSpeedup:\n4.54/(50.62 * 10**-6) = 89688\n\nOn GPUs, it is even more important not to index repeatedly from a tensor\n\n\nOn GPUs, the vectorized function runs almost 90,000 times faster!!!"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#simple-mathematical-operations",
    "href": "ai/pt/wb_torchtensors_slides.html#simple-mathematical-operations",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Simple mathematical operations",
    "text": "Simple mathematical operations\nt1 = torch.arange(1, 5).view(2, 2); print(t1)\nt2 = torch.tensor([[1, 1], [0, 0]]); print(t2)\n\nt1 + t2 # Operation performed between elements at corresponding locations\nt1 + 1  # Operation applied to each element of the tensor\ntensor([[1, 2],\n        [3, 4]])\ntensor([[1, 1],\n        [0, 0]])\n\ntensor([[2, 3],\n        [3, 4]])\ntensor([[2, 3],\n        [4, 5]])"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#reduction",
    "href": "ai/pt/wb_torchtensors_slides.html#reduction",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Reduction",
    "text": "Reduction\nt = torch.ones(2, 3, 4); print(t)\nt.sum()   # Reduction over all entries\ntensor([[[1., 1., 1., 1.],\n         [1., 1., 1., 1.],\n         [1., 1., 1., 1.]],\n        [[1., 1., 1., 1.],\n         [1., 1., 1., 1.],\n         [1., 1., 1., 1.]]])\ntensor(24.)\n\nOther reduction functions (e.g. mean) behave the same way"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#reduction-1",
    "href": "ai/pt/wb_torchtensors_slides.html#reduction-1",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Reduction",
    "text": "Reduction\n# Reduction over a specific dimension\nt.sum(0)\nt.sum(1)\nt.sum(2)\ntensor([[2., 2., 2., 2.],\n        [2., 2., 2., 2.],\n        [2., 2., 2., 2.]])\ntensor([[3., 3., 3., 3.],\n        [3., 3., 3., 3.]])\ntensor([[4., 4., 4.],\n        [4., 4., 4.]])"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#reduction-2",
    "href": "ai/pt/wb_torchtensors_slides.html#reduction-2",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Reduction",
    "text": "Reduction\n# Reduction over multiple dimensions\nt.sum((0, 1))\nt.sum((0, 2))\nt.sum((1, 2))\ntensor([6., 6., 6., 6.])\ntensor([8., 8., 8.])\ntensor([12., 12.])"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#in-place-operations",
    "href": "ai/pt/wb_torchtensors_slides.html#in-place-operations",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "In-place operations",
    "text": "In-place operations\nWith operators post-fixed with _:\nt1 = torch.tensor([1, 2]); print(t1)\nt2 = torch.tensor([1, 1]); print(t2)\nt1.add_(t2); print(t1)\nt1.zero_(); print(t1)\ntensor([1, 2])\ntensor([1, 1])\ntensor([2, 3])\ntensor([0, 0])"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#in-place-operations-vs-reassignments",
    "href": "ai/pt/wb_torchtensors_slides.html#in-place-operations-vs-reassignments",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "In-place operations vs reassignments",
    "text": "In-place operations vs reassignments\nt1 = torch.ones(1); t1, hex(id(t1))\nt1.add_(1); t1, hex(id(t1))        # In-place operation: same address\nt1 = t1.add(1); t1, hex(id(t1))    # Reassignment: new address in memory\nt1 = t1 + 1; t1, hex(id(t1))       # Reassignment: new address in memory\n(tensor([1.]), '0x7fc61accc3b0')\n(tensor([2.]), '0x7fc61accc3b0')\n(tensor([3.]), '0x7fc61accc5e0')\n(tensor([4.]), '0x7fc61accc6d0')"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#tensor-views",
    "href": "ai/pt/wb_torchtensors_slides.html#tensor-views",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Tensor views",
    "text": "Tensor views\nt = torch.tensor([[1, 2, 3], [4, 5, 6]]); print(t)\nt.size()\nt.view(6)\nt.view(3, 2)\nt.view(3, -1) # Same: with -1, the size is inferred from other dimensions\ntensor([[1, 2, 3],\n        [4, 5, 6]])\ntorch.Size([2, 3])\ntensor([1, 2, 3, 4, 5, 6])\ntensor([[1, 2],\n        [3, 4],\n        [5, 6]])"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#note-the-difference",
    "href": "ai/pt/wb_torchtensors_slides.html#note-the-difference",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Note the difference",
    "text": "Note the difference\nt1 = torch.tensor([[1, 2, 3], [4, 5, 6]]); print(t1)\nt2 = t1.t(); print(t2)\nt3 = t1.view(3, 2); print(t3)\ntensor([[1, 2, 3],\n        [4, 5, 6]])\ntensor([[1, 4],\n        [2, 5],\n        [3, 6]])\ntensor([[1, 2],\n        [3, 4],\n        [5, 6]])"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#logical-operations",
    "href": "ai/pt/wb_torchtensors_slides.html#logical-operations",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Logical operations",
    "text": "Logical operations\nt1 = torch.randperm(5); print(t1)\nt2 = torch.randperm(5); print(t2)\nt1 &gt; 3                            # Test each element\nt1 &lt; t2                           # Test corresponding pairs of elements\ntensor([4, 1, 0, 2, 3])\ntensor([0, 4, 2, 1, 3])\ntensor([ True, False, False, False, False])\ntensor([False,  True,  True, False, False])"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#outline-5",
    "href": "ai/pt/wb_torchtensors_slides.html#outline-5",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Outline",
    "text": "Outline\n\nWhat is a PyTorch tensor?\nMemory storage\nData type (dtype)\nBasic operations\nWorking with NumPy\nLinear algebra\nHarvesting the power of GPUs\nDistributed operations"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#conversion-without-copy",
    "href": "ai/pt/wb_torchtensors_slides.html#conversion-without-copy",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Conversion without copy",
    "text": "Conversion without copy\nPyTorch tensors can be converted to NumPy ndarrays and vice-versa in a very efficient manner as both objects share the same memory\nt = torch.rand(2, 3); print(t)     # PyTorch Tensor\nt_np = t.numpy(); print(t_np)      # NumPy ndarray\ntensor([[0.8434, 0.0876, 0.7507],\n        [0.1457, 0.3638, 0.0563]])   \n\n[[0.84344184 0.08764815 0.7506627 ]\n [0.14567494 0.36384273 0.05629885]]"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#mind-the-different-defaults",
    "href": "ai/pt/wb_torchtensors_slides.html#mind-the-different-defaults",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Mind the different defaults",
    "text": "Mind the different defaults\nt_np.dtype\ndtype('float32')\n\nRemember that PyTorch tensors use 32-bit floating points by default\n(because this is what you want in neural networks)\n\n\nBut NumPy defaults to 64-bit\nDepending on your workflow, you might have to change dtype"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#from-numpy-to-pytorch",
    "href": "ai/pt/wb_torchtensors_slides.html#from-numpy-to-pytorch",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "From NumPy to PyTorch",
    "text": "From NumPy to PyTorch\nimport numpy as np\na = np.random.rand(2, 3); print(a)\na_pt = torch.from_numpy(a); print(a_pt)    # From ndarray to tensor\n[[0.55892276 0.06026952 0.72496545]\n [0.65659463 0.27697739 0.29141587]]\n\ntensor([[0.5589, 0.0603, 0.7250],\n        [0.6566, 0.2770, 0.2914]], dtype=torch.float64)\n\nHere again, you might have to change dtype"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#notes-about-conversion-without-copy",
    "href": "ai/pt/wb_torchtensors_slides.html#notes-about-conversion-without-copy",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Notes about conversion without copy",
    "text": "Notes about conversion without copy\nt and t_np are objects of different Python types, so, as far as Python is concerned,\nthey have different addresses\nid(t) == id(t_np)\nFalse"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#notes-about-conversion-without-copy-1",
    "href": "ai/pt/wb_torchtensors_slides.html#notes-about-conversion-without-copy-1",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Notes about conversion without copy",
    "text": "Notes about conversion without copy\nHowever—that’s quite confusing—they share an underlying C array in memory and modifying one in-place also modifies the other\nt.zero_()\nprint(t_np)\ntensor([[0., 0., 0.],\n        [0., 0., 0.]])\n\n[[0. 0. 0.]\n [0. 0. 0.]]"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#notes-about-conversion-without-copy-2",
    "href": "ai/pt/wb_torchtensors_slides.html#notes-about-conversion-without-copy-2",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Notes about conversion without copy",
    "text": "Notes about conversion without copy\nLastly, as NumPy only works on CPU, to convert a PyTorch tensor allocated to the GPU, the content will have to be copied to the CPU first"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#outline-6",
    "href": "ai/pt/wb_torchtensors_slides.html#outline-6",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Outline",
    "text": "Outline\n\nWhat is a PyTorch tensor?\nMemory storage\nData type (dtype)\nBasic operations\nWorking with NumPy\nLinear algebra\nHarvesting the power of GPUs\nDistributed operations"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#torch.linalg-module",
    "href": "ai/pt/wb_torchtensors_slides.html#torch.linalg-module",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "torch.linalg module",
    "text": "torch.linalg module\nAll functions from numpy.linalg implemented (with accelerator and automatic differentiation support) + additional functions\n\nRequires torch &gt;= 1.9\nLinear algebra support was less developed before the introduction of this module"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#system-of-linear-equations-solver",
    "href": "ai/pt/wb_torchtensors_slides.html#system-of-linear-equations-solver",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "System of linear equations solver",
    "text": "System of linear equations solver\nLet’s have a look at an extremely basic example:\n2x + 3y - z = 5\nx - 2y + 8z = 21\n6x + y - 3z = -1\nWe are looking for the values of x, y, and z that would satisfy this system"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#system-of-linear-equations-solver-1",
    "href": "ai/pt/wb_torchtensors_slides.html#system-of-linear-equations-solver-1",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "System of linear equations solver",
    "text": "System of linear equations solver\nWe create a 2D tensor A of size (3, 3) with the coefficients of the equations\nand a 1D tensor b of size 3 with the right hand sides values of the equations\nA = torch.tensor([[2., 3., -1.], [1., -2., 8.], [6., 1., -3.]]); print(A)\nb = torch.tensor([5., 21., -1.]); print(b)\ntensor([[ 2.,  3., -1.],\n        [ 1., -2.,  8.],\n        [ 6.,  1., -3.]])\ntensor([ 5., 21., -1.])"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#system-of-linear-equations-solver-2",
    "href": "ai/pt/wb_torchtensors_slides.html#system-of-linear-equations-solver-2",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "System of linear equations solver",
    "text": "System of linear equations solver\nSolving this system is as simple as running the torch.linalg.solve function:\nx = torch.linalg.solve(A, b); print(x)\ntensor([1., 2., 3.])\nOur solution is:\nx = 1\ny = 2\nz = 3"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#verify-our-result",
    "href": "ai/pt/wb_torchtensors_slides.html#verify-our-result",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Verify our result",
    "text": "Verify our result\ntorch.allclose(A @ x, b)\nTrue"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#system-of-linear-equations-solver-3",
    "href": "ai/pt/wb_torchtensors_slides.html#system-of-linear-equations-solver-3",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "System of linear equations solver",
    "text": "System of linear equations solver\nHere is another simple example:\n# Create a square normal random matrix\nA = torch.randn(4, 4); print(A)\n# Create a tensor of right hand side values\nb = torch.randn(4); print(b)\n\n# Solve the system\nx = torch.linalg.solve(A, b); print(x)\n\n# Verify\ntorch.allclose(A @ x, b)"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#system-of-linear-equations-solver-4",
    "href": "ai/pt/wb_torchtensors_slides.html#system-of-linear-equations-solver-4",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "System of linear equations solver",
    "text": "System of linear equations solver\n(Results)\nA (coefficients):\ntensor([[ 1.5091,  2.0820,  1.7067,  2.3804],\n        [-1.1256, -0.3170, -1.0925, -0.0852],\n        [ 0.3276, -0.7607, -1.5991,  0.0185],\n        [-0.7504,  0.1854,  0.6211,  0.6382]])\nb (right hand side values):\ntensor([-1.0886, -0.2666,  0.1894, -0.2190])\nx (our solution):\ntensor([ 0.1992, -0.7011,  0.2541, -0.1526])\nVerification:\nTrue"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#with-2-multidimensional-tensors",
    "href": "ai/pt/wb_torchtensors_slides.html#with-2-multidimensional-tensors",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "With 2 multidimensional tensors",
    "text": "With 2 multidimensional tensors\nA = torch.randn(2, 3, 3)              # Must be batches of square matrices\nB = torch.randn(2, 3, 5)              # Dimensions must be compatible\nX = torch.linalg.solve(A, B); print(X)\ntorch.allclose(A @ X, B)\ntensor([[[-0.0545, -0.1012,  0.7863, -0.0806, -0.0191],\n         [-0.9846, -0.0137, -1.7521, -0.4579, -0.8178],\n         [-1.9142, -0.6225, -1.9239, -0.6972,  0.7011]],\n        [[ 3.2094,  0.3432, -1.6604, -0.7885,  0.0088],\n         [ 7.9852,  1.4605, -1.7037, -0.7713,  2.7319],\n         [-4.1979,  0.0849,  1.0864,  0.3098, -1.0347]]])\nTrue"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#matrix-inversions",
    "href": "ai/pt/wb_torchtensors_slides.html#matrix-inversions",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Matrix inversions",
    "text": "Matrix inversions\n\n\nIt is faster and more numerically stable to solve a system of linear equations directly than to compute the inverse matrix first\n\n\n\nLimit matrix inversions to situations where it is truly necessary"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#matrix-inversions-1",
    "href": "ai/pt/wb_torchtensors_slides.html#matrix-inversions-1",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Matrix inversions",
    "text": "Matrix inversions\nA = torch.rand(2, 3, 3)      # Batch of square matrices\nA_inv = torch.linalg.inv(A)  # Batch of inverse matrices\nA @ A_inv                    # Batch of identity matrices\ntensor([[[ 1.0000e+00, -6.0486e-07,  1.3859e-06],\n         [ 5.5627e-08,  1.0000e+00,  1.0795e-06],\n         [-1.4133e-07,  7.9992e-08,  1.0000e+00]],\n        [[ 1.0000e+00,  4.3329e-08, -3.6741e-09],\n         [-7.4627e-08,  1.0000e+00,  1.4579e-07],\n         [-6.3580e-08,  8.2354e-08,  1.0000e+00]]])"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#other-linear-algebra-functions",
    "href": "ai/pt/wb_torchtensors_slides.html#other-linear-algebra-functions",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Other linear algebra functions",
    "text": "Other linear algebra functions\ntorch.linalg contains many more functions:\n\ntorch.tensordot which generalizes matrix products\ntorch.linalg.tensorsolve which computes the solution X to the system torch.tensordot(A, X) = B\ntorch.linalg.eigvals which computes the eigenvalues of a square matrix\n…"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#outline-7",
    "href": "ai/pt/wb_torchtensors_slides.html#outline-7",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Outline",
    "text": "Outline\n\nWhat is a PyTorch tensor?\nMemory storage\nData type (dtype)\nBasic operations\nWorking with NumPy\nLinear algebra\nHarvesting the power of GPUs\nDistributed operations"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#device-attribute",
    "href": "ai/pt/wb_torchtensors_slides.html#device-attribute",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Device attribute",
    "text": "Device attribute\nTensor data can be placed in the memory of various processor types:\n\nthe RAM of CPU\nthe RAM of a GPU with CUDA support\nthe RAM of a GPU with AMD’s ROCm support\nthe RAM of an XLA device (e.g. Cloud TPU) with the torch_xla package"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#device-attribute-1",
    "href": "ai/pt/wb_torchtensors_slides.html#device-attribute-1",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Device attribute",
    "text": "Device attribute\nThe values for the device attributes are:\n\nCPU:  'cpu'\nGPU (CUDA and AMD’s ROCm):  'cuda'\nXLA:  xm.xla_device()\n\nThis last option requires to load the torch_xla package first:\nimport torch_xla\nimport torch_xla.core.xla_model as xm"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#creating-a-tensor-on-a-specific-device",
    "href": "ai/pt/wb_torchtensors_slides.html#creating-a-tensor-on-a-specific-device",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Creating a tensor on a specific device",
    "text": "Creating a tensor on a specific device\nBy default, tensors are created on the CPU\nt1 = torch.rand(2); print(t1)\ntensor([0.1606, 0.9771])  # Implicit: device='cpu'\n\nPrinted tensors only display attributes with values ≠ default values"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#creating-a-tensor-on-a-specific-device-1",
    "href": "ai/pt/wb_torchtensors_slides.html#creating-a-tensor-on-a-specific-device-1",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Creating a tensor on a specific device",
    "text": "Creating a tensor on a specific device\nYou can create a tensor on an accelerator by specifying the device attribute\nt2_gpu = torch.rand(2, device='cuda'); print(t2_gpu)\ntensor([0.0664, 0.7829], device='cuda:0')  # :0 means the 1st GPU"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#copying-a-tensor-to-a-specific-device",
    "href": "ai/pt/wb_torchtensors_slides.html#copying-a-tensor-to-a-specific-device",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Copying a tensor to a specific device",
    "text": "Copying a tensor to a specific device\nYou can also make copies of a tensor on other devices\n# Make a copy of t1 on the GPU\nt1_gpu = t1.to(device='cuda'); print(t1_gpu)\nt1_gpu = t1.cuda()  # Same as above written differently\n\n# Make a copy of t2_gpu on the CPU\nt2 = t2_gpu.to(device='cpu'); print(t2)\nt2 = t2_gpu.cpu()   # For the altenative form\ntensor([0.1606, 0.9771], device='cuda:0')\ntensor([0.0664, 0.7829]) # Implicit: device='cpu'"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#multiple-gpus",
    "href": "ai/pt/wb_torchtensors_slides.html#multiple-gpus",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Multiple GPUs",
    "text": "Multiple GPUs\nIf you have multiple GPUs, you can optionally specify which one a tensor should be created on or copied to\nt3_gpu = torch.rand(2, device='cuda:0')  # Create a tensor on 1st GPU\nt4_gpu = t1.to(device='cuda:0')          # Make a copy of t1 on 1st GPU\nt5_gpu = t1.to(device='cuda:1')          # Make a copy of t1 on 2nd GPU\n\nOr the equivalent short forms for the last two:\nt4_gpu = t1.cuda(0)\nt5_gpu = t1.cuda(1)"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#timing",
    "href": "ai/pt/wb_torchtensors_slides.html#timing",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Timing",
    "text": "Timing\nLet’s compare the timing of some matrix multiplications on CPU and GPU with PyTorch built-in benchmark utility\n# Load utility\nimport torch.utils.benchmark as benchmark\n# Define tensors on the CPU\nA = torch.randn(500, 500)\nB = torch.randn(500, 500)\n# Define tensors on the GPU\nA_gpu = torch.randn(500, 500, device='cuda')\nB_gpu = torch.randn(500, 500, device='cuda')\n\nI ran the code on my laptop with a dedicated GPU and 32GB RAM"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#timing-1",
    "href": "ai/pt/wb_torchtensors_slides.html#timing-1",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Timing",
    "text": "Timing\nLet’s time 100 runs to have a reliable benchmark\nt0 = benchmark.Timer(\n    stmt='A @ B',\n    globals={'A': A, 'B': B})\n\nt1 = benchmark.Timer(\n    stmt='A_gpu @ B_gpu',\n    globals={'A_gpu': A_gpu, 'B_gpu': B_gpu})\n\nprint(t0.timeit(100))\nprint(t1.timeit(100))"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#timing-2",
    "href": "ai/pt/wb_torchtensors_slides.html#timing-2",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Timing",
    "text": "Timing\nA @ B\n  2.29 ms\n  1 measurement, 100 runs , 1 thread\n\nA_gpu @ B_gpu\n  108.02 us\n  1 measurement, 100 runs , 1 thread\nSpeedup:\n(2.29 * 10**-3)/(108.02 * 10**-6) = 21\nThis computation was 21 times faster on my GPU than on CPU"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#timing-3",
    "href": "ai/pt/wb_torchtensors_slides.html#timing-3",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Timing",
    "text": "Timing\nBy replacing 500 with 5000, we get:\nA @ B\n  2.21 s\n  1 measurement, 100 runs , 1 thread\n\nA_gpu @ B_gpu\n  57.88 ms\n  1 measurement, 100 runs , 1 thread\nSpeedup:\n2.21/(57.88 * 10**-3) = 38\nThe larger the computation, the greater the benefit: now 38 times faster"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#outline-8",
    "href": "ai/pt/wb_torchtensors_slides.html#outline-8",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Outline",
    "text": "Outline\n\nWhat is a PyTorch tensor?\nMemory storage\nData type (dtype)\nBasic operations\nWorking with NumPy\nLinear algebra\nHarvesting the power of GPUs\nDistributed operations"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#parallel-tensor-operations",
    "href": "ai/pt/wb_torchtensors_slides.html#parallel-tensor-operations",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Parallel tensor operations",
    "text": "Parallel tensor operations\nPyTorch already allows for distributed training of ML models\nThe implementation of distributed tensor operations—for instance for linear algebra—is in the work through the use of a ShardedTensor primitive that can be sharded across nodes\nSee also this issue for more comments about upcoming developments on (among other things) tensor sharding\n\n\n\n\n Back to webinar page"
  },
  {
    "objectID": "ai/pt/wb_upscaling_slides.html#can-be-broken-down-into-2-main-periods",
    "href": "ai/pt/wb_upscaling_slides.html#can-be-broken-down-into-2-main-periods",
    "title": "Super-resolution with PyTorch",
    "section": "Can be broken down into 2 main periods:",
    "text": "Can be broken down into 2 main periods:\n\nA rather slow history with various interpolation algorithms of increasing complexity before deep neural networks\nAn incredibly fast evolution since the advent of deep learning (DL)"
  },
  {
    "objectID": "ai/pt/wb_upscaling_slides.html#sr-history-pre-dl",
    "href": "ai/pt/wb_upscaling_slides.html#sr-history-pre-dl",
    "title": "Super-resolution with PyTorch",
    "section": "SR history Pre-DL",
    "text": "SR history Pre-DL\nPixel-wise interpolation prior to DL\nVarious methods ranging from simple (e.g. nearest-neighbour, bicubic) to complex (e.g. Gaussian process regression, iterative FIR Wiener filter) algorithms"
  },
  {
    "objectID": "ai/pt/wb_upscaling_slides.html#sr-history-pre-dl-1",
    "href": "ai/pt/wb_upscaling_slides.html#sr-history-pre-dl-1",
    "title": "Super-resolution with PyTorch",
    "section": "SR history Pre-DL",
    "text": "SR history Pre-DL\nNearest-neighbour interpolation\nSimplest method of interpolation\nSimply uses the value of the nearest pixel\nBicubic interpolation\nConsists of determining the 16 coefficients \\(a_{ij}\\) in:\n\\[p(x, y) = \\sum_{i=0}^3\\sum_{i=0}^3 a\\_{ij} x^i y^j\\]"
  },
  {
    "objectID": "ai/pt/wb_upscaling_slides.html#sr-history-with-dl",
    "href": "ai/pt/wb_upscaling_slides.html#sr-history-with-dl",
    "title": "Super-resolution with PyTorch",
    "section": "SR history with DL",
    "text": "SR history with DL\nDeep learning has seen a fast evolution marked by the successive emergence of various frameworks and architectures over the past 10 years\nSome key network architectures and frameworks:\n\nCNN\nGAN\nTransformers\n\nThese have all been applied to SR"
  },
  {
    "objectID": "ai/pt/wb_upscaling_slides.html#srcnn",
    "href": "ai/pt/wb_upscaling_slides.html#srcnn",
    "title": "Super-resolution with PyTorch",
    "section": "SRCNN",
    "text": "SRCNN\n\n\nDong, C., Loy, C. C., He, K., & Tang, X. (2015). Image super-resolution using deep convolutional networks. IEEE transactions on pattern analysis and machine intelligence, 38(2), 295-307\n\n\nGiven a low-resolution image Y, the first convolutional layer of the SRCNN extracts a set of feature maps. The second layer maps these feature maps nonlinearly to high-resolution patch representations. The last layer combines the predictions within a spatial neighbourhood to produce the final high-resolution image F(Y)"
  },
  {
    "objectID": "ai/pt/wb_upscaling_slides.html#srcnn-1",
    "href": "ai/pt/wb_upscaling_slides.html#srcnn-1",
    "title": "Super-resolution with PyTorch",
    "section": "SRCNN",
    "text": "SRCNN\nCan use sparse-coding-based methods\n\n\nDong, C., Loy, C. C., He, K., & Tang, X. (2015). Image super-resolution using deep convolutional networks. IEEE transactions on pattern analysis and machine intelligence, 38(2), 295-307"
  },
  {
    "objectID": "ai/pt/wb_upscaling_slides.html#srgan",
    "href": "ai/pt/wb_upscaling_slides.html#srgan",
    "title": "Super-resolution with PyTorch",
    "section": "SRGAN",
    "text": "SRGAN\nDo not provide the best PSNR, but can give more realistic results by providing more texture (less smoothing)"
  },
  {
    "objectID": "ai/pt/wb_upscaling_slides.html#gan",
    "href": "ai/pt/wb_upscaling_slides.html#gan",
    "title": "Super-resolution with PyTorch",
    "section": "GAN",
    "text": "GAN\n\n\nStevens E., Antiga L., & Viehmann T. (2020). Deep Learning with PyTorch"
  },
  {
    "objectID": "ai/pt/wb_upscaling_slides.html#srgan-1",
    "href": "ai/pt/wb_upscaling_slides.html#srgan-1",
    "title": "Super-resolution with PyTorch",
    "section": "SRGAN",
    "text": "SRGAN\n\n\nLedig, C., Theis, L., Huszár, F., Caballero, J., Cunningham, A., Acosta, A., … & Shi, W. (2017). Photo-realistic single image super-resolution using a generative adversarial network. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 4681-4690)"
  },
  {
    "objectID": "ai/pt/wb_upscaling_slides.html#srgan-2",
    "href": "ai/pt/wb_upscaling_slides.html#srgan-2",
    "title": "Super-resolution with PyTorch",
    "section": "SRGAN",
    "text": "SRGAN\nFollowed by the ESRGAN and many other flavours of SRGANs"
  },
  {
    "objectID": "ai/pt/wb_upscaling_slides.html#attention",
    "href": "ai/pt/wb_upscaling_slides.html#attention",
    "title": "Super-resolution with PyTorch",
    "section": "Attention",
    "text": "Attention\n\nMnih, V., Heess, N., & Graves, A. (2014). Recurrent models of visual attention. In Advances in neural information processing systems (pp. 2204-2212)\n\n(cited 2769 times)\n\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., … & Polosukhin, I. (2017). Attention is all you need. In Advances in neural information processing systems (pp. 5998-6008)\n\n(cited 30999 times…)"
  },
  {
    "objectID": "ai/pt/wb_upscaling_slides.html#transformers",
    "href": "ai/pt/wb_upscaling_slides.html#transformers",
    "title": "Super-resolution with PyTorch",
    "section": "Transformers",
    "text": "Transformers\n\n\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., … & Polosukhin, I. (2017). Attention is all you need. In Advances in neural information processing systems (pp. 5998-6008)"
  },
  {
    "objectID": "ai/pt/wb_upscaling_slides.html#transformers-1",
    "href": "ai/pt/wb_upscaling_slides.html#transformers-1",
    "title": "Super-resolution with PyTorch",
    "section": "Transformers",
    "text": "Transformers\nInitially used for NLP to replace RNN as they allow parallelization Now entering the domain of vision and others Very performant with relatively few parameters"
  },
  {
    "objectID": "ai/pt/wb_upscaling_slides.html#swin-transformer",
    "href": "ai/pt/wb_upscaling_slides.html#swin-transformer",
    "title": "Super-resolution with PyTorch",
    "section": "Swin Transformer",
    "text": "Swin Transformer\nThe Swin Transformer improved the use of transformers to the vision domain\nSwin = Shifted WINdows"
  },
  {
    "objectID": "ai/pt/wb_upscaling_slides.html#swin-transformer-1",
    "href": "ai/pt/wb_upscaling_slides.html#swin-transformer-1",
    "title": "Super-resolution with PyTorch",
    "section": "Swin Transformer",
    "text": "Swin Transformer\nSwin transformer (left) vs transformer as initially applied to vision (right):\n\n\nLiu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., … & Guo, B. (2021). Swin transformer: Hierarchical vision transformer using shifted windows. arXiv preprint arXiv:2103.14030"
  },
  {
    "objectID": "ai/pt/wb_upscaling_slides.html#swinir-1",
    "href": "ai/pt/wb_upscaling_slides.html#swinir-1",
    "title": "Super-resolution with PyTorch",
    "section": "SwinIR",
    "text": "SwinIR\n\n\nLiang, J., Cao, J., Sun, G., Zhang, K., Van Gool, L., & Timofte, R. (2021). SwinIR: Image restoration using swin transformer. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 1833-1844)"
  },
  {
    "objectID": "ai/pt/wb_upscaling_slides.html#training-sets-used",
    "href": "ai/pt/wb_upscaling_slides.html#training-sets-used",
    "title": "Super-resolution with PyTorch",
    "section": "Training sets used",
    "text": "Training sets used\nDIV2K, Flickr2K, and other datasets"
  },
  {
    "objectID": "ai/pt/wb_upscaling_slides.html#models-assessment",
    "href": "ai/pt/wb_upscaling_slides.html#models-assessment",
    "title": "Super-resolution with PyTorch",
    "section": "Models assessment",
    "text": "Models assessment\n3 metrics commonly used:\nPeak sign-to-noise ratio (PSNR) measured in dB\n\\(\\frac{\\text{Maximum possible power of signal}}{\\text{Power of noise (calculated as the mean squared error)}}\\)\nCalculated at the pixel level\nStructural similarity index measure (SSIM)\nPrediction of perceived image quality based on a “perfect” reference image\nMean opinion score (MOS)\nMean of subjective quality ratings"
  },
  {
    "objectID": "ai/pt/wb_upscaling_slides.html#models-assessment-1",
    "href": "ai/pt/wb_upscaling_slides.html#models-assessment-1",
    "title": "Super-resolution with PyTorch",
    "section": "Models assessment",
    "text": "Models assessment\nPeak sign-to-noise ratio (PSNR) measured in dB\n\\[PSNR = 10\\,\\cdot\\,log_{10}\\,\\left(\\frac{MAX_I^2}{MSE}\\right)\\]\nStructural similarity index measure (SSIM)\n\\[SSIM(x,y) = \\frac{(2\\mu_x\\mu_y + c_1) + (2 \\sigma _{xy} + c_2)}\n    {(\\mu_x^2 + \\mu_y^2+c_1) (\\sigma_x^2 + \\sigma_y^2+c_2)}\\]\nMean opinion score (MOS)\n\\[MOS = \\frac{\\sum_{n=1}^N R\\_n}{N}\\]"
  },
  {
    "objectID": "ai/pt/wb_upscaling_slides.html#metrics-implementation",
    "href": "ai/pt/wb_upscaling_slides.html#metrics-implementation",
    "title": "Super-resolution with PyTorch",
    "section": "Metrics implementation",
    "text": "Metrics implementation\n\nImplement them yourself (using torch.log10, etc.)\nUse some library that implements them (e.g. kornia)\nUse code of open source project with good implementation (e.g. SwinIR)\nUse some higher level library that provides them (e.g. ignite)"
  },
  {
    "objectID": "ai/pt/wb_upscaling_slides.html#metrics-implementation-1",
    "href": "ai/pt/wb_upscaling_slides.html#metrics-implementation-1",
    "title": "Super-resolution with PyTorch",
    "section": "Metrics implementation",
    "text": "Metrics implementation\n\nImplement them yourself (using torch.log10, etc.)\nUse some library that implements them (e.g. kornia)\nUse code of open source project with good implementation (e.g. SwinIR)\nUse some higher level library that provides them (e.g. ignite)"
  },
  {
    "objectID": "ai/pt/wb_upscaling_slides.html#metrics-implementation-2",
    "href": "ai/pt/wb_upscaling_slides.html#metrics-implementation-2",
    "title": "Super-resolution with PyTorch",
    "section": "Metrics implementation",
    "text": "Metrics implementation\nimport kornia\n\npsnr_value = kornia.metrics.psnr(input, target, max_val)\nssim_value = kornia.metrics.ssim(img1, img2, window_size, max_val=1.0, eps=1e-12)\nSee the Kornia documentation for more info on kornia.metrics.psnr & kornia.metrics.ssim"
  },
  {
    "objectID": "ai/pt/wb_upscaling_slides.html#benchmark-datasets",
    "href": "ai/pt/wb_upscaling_slides.html#benchmark-datasets",
    "title": "Super-resolution with PyTorch",
    "section": "Benchmark datasets",
    "text": "Benchmark datasets\nSet5\n\nSet14\n\nBSD100 (Berkeley Segmentation Dataset)"
  },
  {
    "objectID": "ai/pt/wb_upscaling_slides.html#benchmark-datasets-1",
    "href": "ai/pt/wb_upscaling_slides.html#benchmark-datasets-1",
    "title": "Super-resolution with PyTorch",
    "section": "Benchmark datasets",
    "text": "Benchmark datasets\nSet5\n\nSet14\n\nBSD100 (Berkeley Segmentation Dataset)"
  },
  {
    "objectID": "ai/pt/wb_upscaling_slides.html#the-set5-dataset",
    "href": "ai/pt/wb_upscaling_slides.html#the-set5-dataset",
    "title": "Super-resolution with PyTorch",
    "section": "The Set5 dataset",
    "text": "The Set5 dataset\nA dataset consisting of 5 images which has been used for at least 18 years to assess SR methods"
  },
  {
    "objectID": "ai/pt/wb_upscaling_slides.html#how-to-get-the-dataset",
    "href": "ai/pt/wb_upscaling_slides.html#how-to-get-the-dataset",
    "title": "Super-resolution with PyTorch",
    "section": "How to get the dataset?",
    "text": "How to get the dataset?\nFrom the HuggingFace Datasets Hub with the HuggingFace datasets package:\nfrom datasets import load_dataset\n\nset5 = load_dataset('eugenesiow/Set5', 'bicubic_x4', split='validation')"
  },
  {
    "objectID": "ai/pt/wb_upscaling_slides.html#dataset-exploration",
    "href": "ai/pt/wb_upscaling_slides.html#dataset-exploration",
    "title": "Super-resolution with PyTorch",
    "section": "Dataset exploration",
    "text": "Dataset exploration\nprint(set5)\nlen(set5)\nset5[0]\nset5.shape\nset5.column_names\nset5.features\nset5.set_format('torch', columns=['hr', 'lr'])\nset5.format"
  },
  {
    "objectID": "ai/pt/wb_upscaling_slides.html#benchmarks",
    "href": "ai/pt/wb_upscaling_slides.html#benchmarks",
    "title": "Super-resolution with PyTorch",
    "section": "Benchmarks",
    "text": "Benchmarks\nA 2012 review of interpolation methods for SR gives the metrics for a series of interpolation methods (using other datasets)"
  },
  {
    "objectID": "ai/pt/wb_upscaling_slides.html#interpolation-methods",
    "href": "ai/pt/wb_upscaling_slides.html#interpolation-methods",
    "title": "Super-resolution with PyTorch",
    "section": "Interpolation methods",
    "text": "Interpolation methods"
  },
  {
    "objectID": "ai/pt/wb_upscaling_slides.html#dl-methods",
    "href": "ai/pt/wb_upscaling_slides.html#dl-methods",
    "title": "Super-resolution with PyTorch",
    "section": "DL methods",
    "text": "DL methods\nThe Papers with Code website lists available benchmarks on Set5"
  },
  {
    "objectID": "ai/pt/wb_upscaling_slides.html#lets-use-swinir",
    "href": "ai/pt/wb_upscaling_slides.html#lets-use-swinir",
    "title": "Super-resolution with PyTorch",
    "section": "Let’s use SwinIR",
    "text": "Let’s use SwinIR\n# Get the model\ngit clone git@github.com:JingyunLiang/SwinIR.git\ncd SwinIR\n\n# Copy our test images in the repo\ncp -r &lt;some/path&gt;/my_tests /testsets/my_tests\n\n# Run the model on our images\npython main_test_swinir.py --tile 400 --task real_sr --scale 4 --large_model --model_path model_zoo/swinir/003_realSR_BSRGAN_DFOWMFC_s64w8_SwinIR-L_x4_GAN.pth --folder_lq testsets/my_tests\nRan in 9 min on my machine with one GPU and 32GB of RAM"
  },
  {
    "objectID": "ai/pt/wb_upscaling_slides.html#results",
    "href": "ai/pt/wb_upscaling_slides.html#results",
    "title": "Super-resolution with PyTorch",
    "section": "Results",
    "text": "Results"
  },
  {
    "objectID": "ai/pt/wb_upscaling_slides.html#results-1",
    "href": "ai/pt/wb_upscaling_slides.html#results-1",
    "title": "Super-resolution with PyTorch",
    "section": "Results",
    "text": "Results"
  },
  {
    "objectID": "ai/pt/wb_upscaling_slides.html#results-2",
    "href": "ai/pt/wb_upscaling_slides.html#results-2",
    "title": "Super-resolution with PyTorch",
    "section": "Results",
    "text": "Results"
  },
  {
    "objectID": "ai/pt/wb_upscaling_slides.html#results-3",
    "href": "ai/pt/wb_upscaling_slides.html#results-3",
    "title": "Super-resolution with PyTorch",
    "section": "Results",
    "text": "Results"
  },
  {
    "objectID": "ai/pt/wb_upscaling_slides.html#results-4",
    "href": "ai/pt/wb_upscaling_slides.html#results-4",
    "title": "Super-resolution with PyTorch",
    "section": "Results",
    "text": "Results"
  },
  {
    "objectID": "ai/pt/wb_upscaling_slides.html#results-5",
    "href": "ai/pt/wb_upscaling_slides.html#results-5",
    "title": "Super-resolution with PyTorch",
    "section": "Results",
    "text": "Results"
  },
  {
    "objectID": "ai/pt/wb_upscaling_slides.html#results-6",
    "href": "ai/pt/wb_upscaling_slides.html#results-6",
    "title": "Super-resolution with PyTorch",
    "section": "Results",
    "text": "Results"
  },
  {
    "objectID": "ai/pt/wb_upscaling_slides.html#results-7",
    "href": "ai/pt/wb_upscaling_slides.html#results-7",
    "title": "Super-resolution with PyTorch",
    "section": "Results",
    "text": "Results"
  },
  {
    "objectID": "ai/pt/wb_upscaling_slides.html#results-8",
    "href": "ai/pt/wb_upscaling_slides.html#results-8",
    "title": "Super-resolution with PyTorch",
    "section": "Results",
    "text": "Results"
  },
  {
    "objectID": "ai/pt/wb_upscaling_slides.html#results-9",
    "href": "ai/pt/wb_upscaling_slides.html#results-9",
    "title": "Super-resolution with PyTorch",
    "section": "Results",
    "text": "Results"
  },
  {
    "objectID": "ai/pt/wb_upscaling_slides.html#metrics",
    "href": "ai/pt/wb_upscaling_slides.html#metrics",
    "title": "Super-resolution with PyTorch",
    "section": "Metrics",
    "text": "Metrics\nWe could use the PSNR and SSIM implementations from SwinIR, but let’s try the Kornia functions we mentioned earlier:\n\nkornia.metrics.psnr\nkornia.metrics.ssim"
  },
  {
    "objectID": "ai/pt/wb_upscaling_slides.html#metrics-1",
    "href": "ai/pt/wb_upscaling_slides.html#metrics-1",
    "title": "Super-resolution with PyTorch",
    "section": "Metrics",
    "text": "Metrics\nLet’s load the libraries we need:\nimport kornia\nfrom PIL import Image\nimport torch\nfrom torchvision import transforms"
  },
  {
    "objectID": "ai/pt/wb_upscaling_slides.html#metrics-2",
    "href": "ai/pt/wb_upscaling_slides.html#metrics-2",
    "title": "Super-resolution with PyTorch",
    "section": "Metrics",
    "text": "Metrics\nThen, we load one pair images (LR and HR):\nberlin1_lr = Image.open(\"&lt;some/path&gt;/lr/berlin_1945_1.jpg\")\nberlin1_hr = Image.open(\"&lt;some/path&gt;/hr/berlin_1945_1.png\")\n We can display these images with:\nberlin1_lr.show()\nberlin1_hr.show()"
  },
  {
    "objectID": "ai/pt/wb_upscaling_slides.html#metrics-3",
    "href": "ai/pt/wb_upscaling_slides.html#metrics-3",
    "title": "Super-resolution with PyTorch",
    "section": "Metrics",
    "text": "Metrics\nNow, we need to resize them so that they have identical dimensions and turn them into tensors:\npreprocess = transforms.Compose([\n        transforms.Resize(256),\n        transforms.ToTensor()\n        ])\n\nberlin1_lr_t = preprocess(berlin1_lr)\nberlin1_hr_t = preprocess(berlin1_hr)"
  },
  {
    "objectID": "ai/pt/wb_upscaling_slides.html#metrics-4",
    "href": "ai/pt/wb_upscaling_slides.html#metrics-4",
    "title": "Super-resolution with PyTorch",
    "section": "Metrics",
    "text": "Metrics\nberlin1_lr_t.shape\nberlin1_hr_t.shape\ntorch.Size([3, 267, 256])\ntorch.Size([3, 267, 256])\nWe now have tensors with 3 dimensions:\n\nthe channels (RGB)\nthe height of the image (in pixels)\nthe width of the image (in pixels)"
  },
  {
    "objectID": "ai/pt/wb_upscaling_slides.html#metrics-5",
    "href": "ai/pt/wb_upscaling_slides.html#metrics-5",
    "title": "Super-resolution with PyTorch",
    "section": "Metrics",
    "text": "Metrics\nAs data processing is done in batch in ML, we need to add a 4th dimension: the batch size\n(It will be equal to 1 since we have a batch size of a single image)\nbatch_berlin1_lr_t = torch.unsqueeze(berlin1_lr_t, 0)\nbatch_berlin1_hr_t = torch.unsqueeze(berlin1_hr_t, 0)"
  },
  {
    "objectID": "ai/pt/wb_upscaling_slides.html#metrics-6",
    "href": "ai/pt/wb_upscaling_slides.html#metrics-6",
    "title": "Super-resolution with PyTorch",
    "section": "Metrics",
    "text": "Metrics\nOur new tensors are now ready:\nbatch_berlin1_lr_t.shape\nbatch_berlin1_hr_t.shape\ntorch.Size([1, 3, 267, 256])\ntorch.Size([1, 3, 267, 256])"
  },
  {
    "objectID": "ai/pt/wb_upscaling_slides.html#psnr",
    "href": "ai/pt/wb_upscaling_slides.html#psnr",
    "title": "Super-resolution with PyTorch",
    "section": "PSNR",
    "text": "PSNR\npsnr_value = kornia.metrics.psnr(batch_berlin1_lr_t, batch_berlin1_hr_t, max_val=1.0)\npsnr_value.item()\n33.379642486572266"
  },
  {
    "objectID": "ai/pt/wb_upscaling_slides.html#ssim",
    "href": "ai/pt/wb_upscaling_slides.html#ssim",
    "title": "Super-resolution with PyTorch",
    "section": "SSIM",
    "text": "SSIM\nssim_map = kornia.metrics.ssim(\n    batch_berlin1_lr_t, batch_berlin1_hr_t, window_size=5, max_val=1.0, eps=1e-12)\n\nssim_map.mean().item()\n0.9868119359016418\n\n\n\n\n Back to webinar page"
  },
  {
    "objectID": "ai/pt/ws_pretrained_models.html",
    "href": "ai/pt/ws_pretrained_models.html",
    "title": "Finding pretrained models for transfer learning",
    "section": "",
    "text": "Training models from scratch requires way too much data, time, and computing power (or money) to be a practical option. This is why transfer learning has become such a common practice: by starting with models trained on related problems, you are saving time and achieving good results with little data.\nNow, where do you find such models?\nIn this workshop, we will see how to use pre-trained models included in PyTorch libraries, have a look at some of the most popular pre-trained models repositories, and learn how to search models in the literature and on GitHub.",
    "crumbs": [
      "AI",
      "<b><em>Workshops</em></b>",
      "Finding pre-trained models"
    ]
  },
  {
    "objectID": "ai/pt/ws_pretrained_models.html#what-are-pre-trained-models",
    "href": "ai/pt/ws_pretrained_models.html#what-are-pre-trained-models",
    "title": "Finding pretrained models for transfer learning",
    "section": "What are pre-trained models?",
    "text": "What are pre-trained models?\n\nTransfer learning\nIf you build models from scratch, expect their performance to be mediocre. Totally naive models with random weights and biases usually need to be trained for a long time on very large datasets, using vast amounts of computing resources, before they produce competitive results. You may not even have enough data to train a model from scratch.\nInstead of starting from zero however, you can use a model that has been trained on a similar task. For instance, if your goal is to create a model able to identify bird species from pictures, you could look for a model developed for image recognition tasks trained on a classic dataset such as ImageNet. Classic such models include AlexNet (2012) and ResNet (2015). These models will already have features that are useful to you and you will get better performance with less training time and fewer data. This is called transfer learning.\n\n\nHow transfer learning works\nTypically, you remove the last layer (for instance, with AlexNet, you would remove the classification layer), replace it with a layer suitable to your task, then, optionally, you can fine tune the model.\nFine tuning a model consists of freezing the first layers (fixing their weights and biases) while retraining the model with data specific to the new task. This will only train the last few layers, greatly reducing the size of the model actually being trained and taking advantage of the early features from the source model.\nI will talk about transfer learning in another workshop, but today, we are focusing on finding a suitable pre-trained model.\nNote that the most powerful recent transformers such as GPT-3 and 4 and their competitors perform well in different tasks without the need for re-training.\n\n\nHow to find a pre-trained model\nKey to transfer learning is the search for an appropriate source model. The great news is that the world of machine learning research is incredibly open: many teams make their papers and models available online. But you need a way to navigate this abundance of resource.\nThings you should probably care about when looking for a pre-trained model include:\n\nHow pertinent is the model relative to your task?\nDoes the model have an open license?\nIs the performance good?\nIs the model size suitable for the resources I have?",
    "crumbs": [
      "AI",
      "<b><em>Workshops</em></b>",
      "Finding pre-trained models"
    ]
  },
  {
    "objectID": "ai/pt/ws_pretrained_models.html#models-in-pytorch-libraries",
    "href": "ai/pt/ws_pretrained_models.html#models-in-pytorch-libraries",
    "title": "Finding pretrained models for transfer learning",
    "section": "Models in PyTorch libraries",
    "text": "Models in PyTorch libraries\nThe PyTorch ecosystem contains domain specific libraries (e.g. torchvision, torchtext, torchaudio). Among many domain specific utilities, these libraries contain many pretrained models in vision, text, and audio.\nThese models benefit from optimum convenience since they are entirely integrated into PyTorch.\n\nLoading ResNet-18 is as simple as:\n\nimport torchvision\nmodel = torchvision.models.resnet18()\n\nInitializing a pretrained ResNet-50 model with the best currently available weights is as simple as:\n\nfrom torchvision.models import resnet50, ResNet50_Weights\nmodel = resnet50(weights=ResNet50_Weights.DEFAULT)",
    "crumbs": [
      "AI",
      "<b><em>Workshops</em></b>",
      "Finding pre-trained models"
    ]
  },
  {
    "objectID": "ai/pt/ws_pretrained_models.html#pytorch-hub",
    "href": "ai/pt/ws_pretrained_models.html#pytorch-hub",
    "title": "Finding pretrained models for transfer learning",
    "section": "PyTorch Hub",
    "text": "PyTorch Hub\nPyTorch Hub is a repository of pretrained models.\n\nLoading ResNet-18 from the hub is done with:\n\nimport torch\nmodel = torch.hub.load('pytorch/vision', 'resnet18', pretrained=True)\n\n\nYour turn:\n\nLook for a small image classification model in the PyTorch Hub.",
    "crumbs": [
      "AI",
      "<b><em>Workshops</em></b>",
      "Finding pre-trained models"
    ]
  },
  {
    "objectID": "ai/pt/ws_pretrained_models.html#hugging-face",
    "href": "ai/pt/ws_pretrained_models.html#hugging-face",
    "title": "Finding pretrained models for transfer learning",
    "section": "Hugging Face",
    "text": "Hugging Face\nHugging Face, launched in 2016, provides a Model Hub. Let’s explore it together.\n\nNote that Hugging Face also has a Dataset Hub.\n\n\n\nYour turn:\n\nFind a pre-trained model for image classification in PyTorch, trained on ImageNet, with an open license, and less than 100MB in size.\n\n\ntimm\nFor computer vision specifically, the timm (PyTorch Image Models) library contains more than 700 pretrained models, as well as scripts, utilities, optimizers, data-loaders, etc. The repo can be found here.\nYou can load models from the Hugging Face Hub with:\nimport timm\nmodel = timm.create_model('hf_hub:author/model', pretrained=True)",
    "crumbs": [
      "AI",
      "<b><em>Workshops</em></b>",
      "Finding pre-trained models"
    ]
  },
  {
    "objectID": "ai/pt/ws_pretrained_models.html#github",
    "href": "ai/pt/ws_pretrained_models.html#github",
    "title": "Finding pretrained models for transfer learning",
    "section": "GitHub",
    "text": "GitHub\nA large number of open source models are hosted on GitHub and the platform can be searched directly for specific models.\n\n\nYour turn:\n\nDo a search on GitHub, trying to find pre-trained models in PyTorch for image classification.",
    "crumbs": [
      "AI",
      "<b><em>Workshops</em></b>",
      "Finding pre-trained models"
    ]
  },
  {
    "objectID": "ai/pt/ws_pretrained_models.html#literature",
    "href": "ai/pt/ws_pretrained_models.html#literature",
    "title": "Finding pretrained models for transfer learning",
    "section": "Literature",
    "text": "Literature\nWhile a less direct way to find pre-trained models, the literature is invaluable to (try to) keep up with what people are doing in the field.\nPapers With Code gathers machine learning papers with open source code.\narXiv is an open-source repository of scientific preprints created by Paul Ginsparg from Cornell University in 1991. It contains a huge number of e-prints on machine learning in the computer science and the statistics fields. arxiv-sanity, created by Andrej Karpathy, tracks arXiv machine learning papers and is easier to browse.",
    "crumbs": [
      "AI",
      "<b><em>Workshops</em></b>",
      "Finding pre-trained models"
    ]
  },
  {
    "objectID": "ai/sk_workflow.html",
    "href": "ai/sk_workflow.html",
    "title": "Sklearn workflow",
    "section": "",
    "text": "Scikit-learn has a very clean and consistent API, making it very easy to use: a similar workflow can be applied to most techniques. Let’s go over two examples.\nThis code was modified from Matthew Greenberg.",
    "crumbs": [
      "AI",
      "<b><em>Scikit-learn</em></b>",
      "Sklearn workflow"
    ]
  },
  {
    "objectID": "ai/sk_workflow.html#load-packages",
    "href": "ai/sk_workflow.html#load-packages",
    "title": "Sklearn workflow",
    "section": "Load packages",
    "text": "Load packages\n\nfrom sklearn.datasets import fetch_california_housing, load_breast_cancer\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import (\n    mean_squared_error,\n    mean_absolute_percentage_error,\n    accuracy_score\n)\n\nimport pandas as pd\n\nimport matplotlib\nfrom matplotlib import pyplot as plt\n\nimport numpy as np\n\nfrom collections import Counter",
    "crumbs": [
      "AI",
      "<b><em>Scikit-learn</em></b>",
      "Sklearn workflow"
    ]
  },
  {
    "objectID": "ai/sk_workflow.html#example-1-california-housing-dataset",
    "href": "ai/sk_workflow.html#example-1-california-housing-dataset",
    "title": "Sklearn workflow",
    "section": "Example 1: California housing dataset",
    "text": "Example 1: California housing dataset\n\nLoad and explore the data\n\ncal_housing = fetch_california_housing()\ntype(cal_housing)\n\nsklearn.utils._bunch.Bunch\n\n\nLet’s look at the attributes of cal_housing:\n\ndir(cal_housing)\n\n['DESCR', 'data', 'feature_names', 'frame', 'target', 'target_names']\n\n\n\ncal_housing.feature_names\n\n['MedInc',\n 'HouseAge',\n 'AveRooms',\n 'AveBedrms',\n 'Population',\n 'AveOccup',\n 'Latitude',\n 'Longitude']\n\n\n\nprint(cal_housing.DESCR)\n\n.. _california_housing_dataset:\n\nCalifornia Housing dataset\n--------------------------\n\n**Data Set Characteristics:**\n\n:Number of Instances: 20640\n\n:Number of Attributes: 8 numeric, predictive attributes and the target\n\n:Attribute Information:\n    - MedInc        median income in block group\n    - HouseAge      median house age in block group\n    - AveRooms      average number of rooms per household\n    - AveBedrms     average number of bedrooms per household\n    - Population    block group population\n    - AveOccup      average number of household members\n    - Latitude      block group latitude\n    - Longitude     block group longitude\n\n:Missing Attribute Values: None\n\nThis dataset was obtained from the StatLib repository.\nhttps://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html\n\nThe target variable is the median house value for California districts,\nexpressed in hundreds of thousands of dollars ($100,000).\n\nThis dataset was derived from the 1990 U.S. census, using one row per census\nblock group. A block group is the smallest geographical unit for which the U.S.\nCensus Bureau publishes sample data (a block group typically has a population\nof 600 to 3,000 people).\n\nA household is a group of people residing within a home. Since the average\nnumber of rooms and bedrooms in this dataset are provided per household, these\ncolumns may take surprisingly large values for block groups with few households\nand many empty houses, such as vacation resorts.\n\nIt can be downloaded/loaded using the\n:func:`sklearn.datasets.fetch_california_housing` function.\n\n.. topic:: References\n\n    - Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\n      Statistics and Probability Letters, 33 (1997) 291-297\n\n\n\n\nX = cal_housing.data\ny = cal_housing.target\n\n\nThis can also be obtained with X, y = fetch_california_housing(return_X_y=True).\n\nLet’s have a look at the shape of X and y:\n\nX.shape\n\n(20640, 8)\n\n\n\ny.shape\n\n(20640,)\n\n\nWhile not at all necessary, we can turn this bunch object into a more familiar data frame to explore the data further:\n\ncal_housing_df = pd.DataFrame(cal_housing.data, columns=cal_housing.feature_names)\n\n\ncal_housing_df.head()\n\n\n\n\n\n\n\n\n\nMedInc\nHouseAge\nAveRooms\nAveBedrms\nPopulation\nAveOccup\nLatitude\nLongitude\n\n\n\n\n0\n8.3252\n41.0\n6.984127\n1.023810\n322.0\n2.555556\n37.88\n-122.23\n\n\n1\n8.3014\n21.0\n6.238137\n0.971880\n2401.0\n2.109842\n37.86\n-122.22\n\n\n2\n7.2574\n52.0\n8.288136\n1.073446\n496.0\n2.802260\n37.85\n-122.24\n\n\n3\n5.6431\n52.0\n5.817352\n1.073059\n558.0\n2.547945\n37.85\n-122.25\n\n\n4\n3.8462\n52.0\n6.281853\n1.081081\n565.0\n2.181467\n37.85\n-122.25\n\n\n\n\n\n\n\n\n\ncal_housing_df.tail()\n\n\n\n\n\n\n\n\n\nMedInc\nHouseAge\nAveRooms\nAveBedrms\nPopulation\nAveOccup\nLatitude\nLongitude\n\n\n\n\n20635\n1.5603\n25.0\n5.045455\n1.133333\n845.0\n2.560606\n39.48\n-121.09\n\n\n20636\n2.5568\n18.0\n6.114035\n1.315789\n356.0\n3.122807\n39.49\n-121.21\n\n\n20637\n1.7000\n17.0\n5.205543\n1.120092\n1007.0\n2.325635\n39.43\n-121.22\n\n\n20638\n1.8672\n18.0\n5.329513\n1.171920\n741.0\n2.123209\n39.43\n-121.32\n\n\n20639\n2.3886\n16.0\n5.254717\n1.162264\n1387.0\n2.616981\n39.37\n-121.24\n\n\n\n\n\n\n\n\n\ncal_housing_df.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 20640 entries, 0 to 20639\nData columns (total 8 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   MedInc      20640 non-null  float64\n 1   HouseAge    20640 non-null  float64\n 2   AveRooms    20640 non-null  float64\n 3   AveBedrms   20640 non-null  float64\n 4   Population  20640 non-null  float64\n 5   AveOccup    20640 non-null  float64\n 6   Latitude    20640 non-null  float64\n 7   Longitude   20640 non-null  float64\ndtypes: float64(8)\nmemory usage: 1.3 MB\n\n\n\ncal_housing_df.describe() \n\n\n\n\n\n\n\n\n\nMedInc\nHouseAge\nAveRooms\nAveBedrms\nPopulation\nAveOccup\nLatitude\nLongitude\n\n\n\n\ncount\n20640.000000\n20640.000000\n20640.000000\n20640.000000\n20640.000000\n20640.000000\n20640.000000\n20640.000000\n\n\nmean\n3.870671\n28.639486\n5.429000\n1.096675\n1425.476744\n3.070655\n35.631861\n-119.569704\n\n\nstd\n1.899822\n12.585558\n2.474173\n0.473911\n1132.462122\n10.386050\n2.135952\n2.003532\n\n\nmin\n0.499900\n1.000000\n0.846154\n0.333333\n3.000000\n0.692308\n32.540000\n-124.350000\n\n\n25%\n2.563400\n18.000000\n4.440716\n1.006079\n787.000000\n2.429741\n33.930000\n-121.800000\n\n\n50%\n3.534800\n29.000000\n5.229129\n1.048780\n1166.000000\n2.818116\n34.260000\n-118.490000\n\n\n75%\n4.743250\n37.000000\n6.052381\n1.099526\n1725.000000\n3.282261\n37.710000\n-118.010000\n\n\nmax\n15.000100\n52.000000\n141.909091\n34.066667\n35682.000000\n1243.333333\n41.950000\n-114.310000\n\n\n\n\n\n\n\n\nWe can even plot it:\n\nplt.hist(y)\n\n(array([ 877., 3612., 4099., 3771., 2799., 1769., 1239.,  752.,  479.,\n        1243.]),\n array([0.14999 , 0.634992, 1.119994, 1.604996, 2.089998, 2.575   ,\n        3.060002, 3.545004, 4.030006, 4.515008, 5.00001 ]),\n &lt;BarContainer object of 10 artists&gt;)\n\n\n\n\n\n\n\n\n\n\n\nCreate and fit a model\nLet’s start with a very simple model: linear regression.\n\nmodel = LinearRegression().fit(X, y)\n\n\nThis is equivalent to:\nmodel = LinearRegression()\nmodel.fit(X, y)\nFirst, we create an instance of the class LinearRegression, then we call .fit() on it to fit the model.\n\n\nmodel.coef_\n\narray([ 4.36693293e-01,  9.43577803e-03, -1.07322041e-01,  6.45065694e-01,\n       -3.97638942e-06, -3.78654265e-03, -4.21314378e-01, -4.34513755e-01])\n\n\n\nTrailing underscores indicate that an attribute is estimated. .coef_ here is an estimated value.\n\n\nmodel.coef_.shape\n\n(8,)\n\n\n\nmodel.intercept_\n\n-36.94192020718425\n\n\nWe can now get our predictions:\n\ny_hat = model.predict(X)\n\nAnd calculate some measures of error:\n\nSum of squared errors\n\n\nnp.sum((y - y_hat) ** 2)\n\n10821.985154850292\n\n\n\nMean squared error\n\n\nmean_squared_error(y, y_hat)\n\n0.5243209861846072\n\n\n\nMSE could also be calculated with np.mean((y - y_hat)**2).\n\n\nmean_absolute_percentage_error(y, y_hat)\n\n0.31715404597233504\n\n\nIndex of minimum value:\n\nmodel.coef_.argmin()\n\n7\n\n\nIndex of maximum value:\n\nmodel.coef_.argmax()\n\n3\n\n\n\nXX = np.concatenate([np.ones((len(X), 1)), X], axis=1)\n\nbeta = np.linalg.lstsq(XX, y, rcond=None)[0]\nintercept_, *coef_ = beta\n\nintercept_, model.intercept_\n\n(-36.94192020718431, -36.94192020718425)\n\n\n\nnp.allclose(coef_, model.coef_)\n\nTrue\n\n\n\nThis means that the two arrays are equal element-wise, within a certain tolerance.\n\n\nX_test = np.random.normal(size=(10, X.shape[1]))\nX_test.shape\n\n(10, 8)\n\n\n\ny_test = X_test @ coef_ + intercept_\ny_test\n\narray([-37.55052184, -35.60907306, -38.12522405, -37.84858902,\n       -38.47751771, -38.6977647 , -35.36190087, -36.25309428,\n       -38.06223057, -37.91025597])\n\n\n\nmodel.predict(X_test)\n\narray([-37.55052184, -35.60907306, -38.12522405, -37.84858902,\n       -38.47751771, -38.6977647 , -35.36190087, -36.25309428,\n       -38.06223057, -37.91025597])\n\n\nOf course, instead of LinearRegression(), we could have used another model such as a random forest regressor (a meta estimator that fits a number of classifying decision trees on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting) for instance:\n\nmodel = RandomForestRegressor().fit(X, y).predict(X_test)\nmodel\n\narray([1.5015501, 1.46458  , 1.5050501, 1.39368  , 1.5015501, 1.5015501,\n       1.10905  , 1.37466  , 1.3070705, 1.6489316])\n\n\n\nWhich is equivalent to:\nmodel = RandomForestRegressor()\nmodel.fit(X, y).predict(X_test)",
    "crumbs": [
      "AI",
      "<b><em>Scikit-learn</em></b>",
      "Sklearn workflow"
    ]
  },
  {
    "objectID": "ai/sk_workflow.html#example-2-breast-cancer",
    "href": "ai/sk_workflow.html#example-2-breast-cancer",
    "title": "Sklearn workflow",
    "section": "Example 2: breast cancer",
    "text": "Example 2: breast cancer\n\nLoad and explore the data\n\nb_cancer = load_breast_cancer()\n\nLet’s print the description of this dataset:\n\nprint(b_cancer.DESCR)\n\n.. _breast_cancer_dataset:\n\nBreast cancer wisconsin (diagnostic) dataset\n--------------------------------------------\n\n**Data Set Characteristics:**\n\n:Number of Instances: 569\n\n:Number of Attributes: 30 numeric, predictive attributes and the class\n\n:Attribute Information:\n    - radius (mean of distances from center to points on the perimeter)\n    - texture (standard deviation of gray-scale values)\n    - perimeter\n    - area\n    - smoothness (local variation in radius lengths)\n    - compactness (perimeter^2 / area - 1.0)\n    - concavity (severity of concave portions of the contour)\n    - concave points (number of concave portions of the contour)\n    - symmetry\n    - fractal dimension (\"coastline approximation\" - 1)\n\n    The mean, standard error, and \"worst\" or largest (mean of the three\n    worst/largest values) of these features were computed for each image,\n    resulting in 30 features.  For instance, field 0 is Mean Radius, field\n    10 is Radius SE, field 20 is Worst Radius.\n\n    - class:\n            - WDBC-Malignant\n            - WDBC-Benign\n\n:Summary Statistics:\n\n===================================== ====== ======\n                                        Min    Max\n===================================== ====== ======\nradius (mean):                        6.981  28.11\ntexture (mean):                       9.71   39.28\nperimeter (mean):                     43.79  188.5\narea (mean):                          143.5  2501.0\nsmoothness (mean):                    0.053  0.163\ncompactness (mean):                   0.019  0.345\nconcavity (mean):                     0.0    0.427\nconcave points (mean):                0.0    0.201\nsymmetry (mean):                      0.106  0.304\nfractal dimension (mean):             0.05   0.097\nradius (standard error):              0.112  2.873\ntexture (standard error):             0.36   4.885\nperimeter (standard error):           0.757  21.98\narea (standard error):                6.802  542.2\nsmoothness (standard error):          0.002  0.031\ncompactness (standard error):         0.002  0.135\nconcavity (standard error):           0.0    0.396\nconcave points (standard error):      0.0    0.053\nsymmetry (standard error):            0.008  0.079\nfractal dimension (standard error):   0.001  0.03\nradius (worst):                       7.93   36.04\ntexture (worst):                      12.02  49.54\nperimeter (worst):                    50.41  251.2\narea (worst):                         185.2  4254.0\nsmoothness (worst):                   0.071  0.223\ncompactness (worst):                  0.027  1.058\nconcavity (worst):                    0.0    1.252\nconcave points (worst):               0.0    0.291\nsymmetry (worst):                     0.156  0.664\nfractal dimension (worst):            0.055  0.208\n===================================== ====== ======\n\n:Missing Attribute Values: None\n\n:Class Distribution: 212 - Malignant, 357 - Benign\n\n:Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\n\n:Donor: Nick Street\n\n:Date: November, 1995\n\nThis is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\nhttps://goo.gl/U2Uwz2\n\nFeatures are computed from a digitized image of a fine needle\naspirate (FNA) of a breast mass.  They describe\ncharacteristics of the cell nuclei present in the image.\n\nSeparating plane described above was obtained using\nMultisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\nConstruction Via Linear Programming.\" Proceedings of the 4th\nMidwest Artificial Intelligence and Cognitive Science Society,\npp. 97-101, 1992], a classification method which uses linear\nprogramming to construct a decision tree.  Relevant features\nwere selected using an exhaustive search in the space of 1-4\nfeatures and 1-3 separating planes.\n\nThe actual linear program used to obtain the separating plane\nin the 3-dimensional space is that described in:\n[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\nProgramming Discrimination of Two Linearly Inseparable Sets\",\nOptimization Methods and Software 1, 1992, 23-34].\n\nThis database is also available through the UW CS ftp server:\n\nftp ftp.cs.wisc.edu\ncd math-prog/cpo-dataset/machine-learn/WDBC/\n\n|details-start|\n**References**\n|details-split|\n\n- W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction\n  for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on\n  Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\n  San Jose, CA, 1993.\n- O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and\n  prognosis via linear programming. Operations Research, 43(4), pages 570-577,\n  July-August 1995.\n- W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\n  to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994)\n  163-171.\n\n|details-end|\n\n\n\n\nb_cancer.feature_names\n\narray(['mean radius', 'mean texture', 'mean perimeter', 'mean area',\n       'mean smoothness', 'mean compactness', 'mean concavity',\n       'mean concave points', 'mean symmetry', 'mean fractal dimension',\n       'radius error', 'texture error', 'perimeter error', 'area error',\n       'smoothness error', 'compactness error', 'concavity error',\n       'concave points error', 'symmetry error',\n       'fractal dimension error', 'worst radius', 'worst texture',\n       'worst perimeter', 'worst area', 'worst smoothness',\n       'worst compactness', 'worst concavity', 'worst concave points',\n       'worst symmetry', 'worst fractal dimension'], dtype='&lt;U23')\n\n\n\nb_cancer.target_names\n\narray(['malignant', 'benign'], dtype='&lt;U9')\n\n\n\nX = b_cancer.data\ny = b_cancer.target\n\n\nHere again, we could have used instead X, y = load_breast_cancer(return_X_y=True).\n\n\nX.shape\n\n(569, 30)\n\n\n\ny.shape\n\n(569,)\n\n\n\nset(y)\n\n{0, 1}\n\n\n\nCounter(y)\n\nCounter({1: 357, 0: 212})\n\n\n\n\nCreate and fit a first model\n\nmodel = LogisticRegression(max_iter=10000)\ny_hat = model.fit(X, y).predict(X)\n\nGet some measure of accuracy:\n\naccuracy_score(y, y_hat)\n\n0.9578207381370826\n\n\n\nThis can also be obtained with:\nnp.mean(y_hat == y)\n\n\ndef sigmoid(x):\n  return 1/(1 + np.exp(-x))\n\nx = np.linspace(-10, 10, 100)\nplt.plot(x, sigmoid(x), lw=3)\nplt.title(\"The Sigmoid Function $\\\\sigma(x)$\")\n\nText(0.5, 1.0, 'The Sigmoid Function $\\\\sigma(x)$')\n\n\n\n\n\n\n\n\n\n\ny_pred = 1*(sigmoid(X @ model.coef_.squeeze() + model.intercept_) &gt; 0.5)\nassert np.all(y_pred == model.predict(X))\n\nnp.allclose(\n    model.predict_proba(X)[:, 1],\n    sigmoid(X @ model.coef_.squeeze() + model.intercept_)\n)\n\nTrue\n\n\n\ndef make_spirals(k=20, s=1.0, n=2000):\n    X = np.zeros((n, 2))\n    y = np.round(np.random.uniform(size=n)).astype(int)\n    r = np.random.uniform(size=n)*k*np.pi\n    rr = r**0.5\n    theta = rr + np.random.normal(loc=0, scale=s, size=n)\n    theta[y == 1] = theta[y == 1] + np.pi\n    X[:,0] = rr*np.cos(theta)\n    X[:,1] = rr*np.sin(theta)\n    return X, y\n\nX, y = make_spirals()\ncmap = matplotlib.colormaps[\"viridis\"]\n\na = cmap(0)\na = [*a[:3], 0.3]\nb = cmap(0.99)\nb = [*b[:3], 0.3]\n\nplt.figure(figsize=(7,7))\nax = plt.gca()\nax.set_aspect(\"equal\")\nax.plot(X[y == 0, 0], X[y == 0, 1], 'o', color=a, ms=8, label=\"$y=0$\")\nax.plot(X[y == 1, 0], X[y == 1, 1], 'o', color=b, ms=8, label=\"$y=1$\")\nplt.title(\"Spirals\")\nplt.legend()\n\n\n\n\n\n\n\n\n\n\nCreate and fit a second model\nHere, we use a logistic regression:\n\nmodel = LogisticRegression()\ny_hat = model.fit(X, y).predict(X)\naccuracy_score(y, y_hat)\n\n0.566\n\n\n\nu = np.linspace(-8, 8, 100)\nv = np.linspace(-8, 8, 100)\nU, V = np.meshgrid(u, v)\nUV = np.array([U.ravel(), V.ravel()]).T\nU.shape, V.shape, UV.shape\n\n((100, 100), (100, 100), (10000, 2))\n\n\n\nnp.ravel returns a contiguous flattened array.\n\n\nW = model.predict(UV).reshape(U.shape)\nW.shape\n\n(100, 100)\n\n\n\nplt.pcolormesh(U, V, W)\n\n\n\n\n\n\n\n\n\n\nCreate and fit a third model\nLet’s use a k-nearest neighbours classifier this time:\n\nmodel = KNeighborsClassifier(n_neighbors=5)\ny_hat = model.fit(X, y).predict(X)\naccuracy_score(y, y_hat)\n\n0.886\n\n\n\nu = np.linspace(-8, 8, 100)\nv = np.linspace(-8, 8, 100)\nU, V = np.meshgrid(u, v)\nUV = np.array([U.ravel(), V.ravel()]).T\nU.shape, V.shape, UV.shape\n\n((100, 100), (100, 100), (10000, 2))\n\n\n\nW = model.predict(UV).reshape(U.shape)\nW.shape\n\n(100, 100)\n\n\n\nplt.pcolormesh(U, V, W)\n\n\n\n\n\n\n\n\nWe can iterate over various values of k to see how the accuracy and pseudocolor plot evolve:\n\nfig, axes = plt.subplots(2, 4, figsize=(9.8, 5))\nfig.suptitle(\"Decision Regions\")\n\nu = np.linspace(-8, 8, 100)\nv = np.linspace(-8, 8, 100)\nU, V = np.meshgrid(u, v)\nUV = np.array([U.ravel(), V.ravel()]).T\n\nks = np.arange(1, 16, 2)\n\nfor k, ax in zip(ks, axes.ravel()):\n  model = KNeighborsClassifier(n_neighbors=k)\n  model.fit(X, y)\n  acc = accuracy_score(y, model.predict(X))\n  W = model.predict(UV).reshape(U.shape)\n  ax.imshow(W, origin=\"lower\", cmap=cmap)\n  ax.set_axis_off()\n  ax.set_title(f\"$k$={k}, acc={acc:.2f}\")",
    "crumbs": [
      "AI",
      "<b><em>Scikit-learn</em></b>",
      "Sklearn workflow"
    ]
  },
  {
    "objectID": "ai/top_jx.html",
    "href": "ai/top_jx.html",
    "title": "JAX: a framework for high-performance array computing and differentiation",
    "section": "",
    "text": "JAX is an open source Python library for high-performance array computing and flexible automatic differentiation.\nHigh-performance computing is achieved by asynchronous dispatch, just-in-time compilation, the XLA compiler for linear algebra, and full compatibility with accelerators (GPUs and TPUs).\nAutomatic differentiation uses Autograd and works with complex control flows (conditions, recursions), second and third-order derivatives, forward and reverse modes. This makes JAX ideal for machine learning and neural network libraries such as Flax are built on it.\nThis course will teach you the basics of JAX and is a prerequisite for the following machine learning course on Flax.\nYou do not need to install anything on your machine for this course as we will provide access to a temporary remote cluster.\n\n Start course ➤",
    "crumbs": [
      "AI",
      "<b><em>JAX</em></b>"
    ]
  },
  {
    "objectID": "ai/top_wb.html",
    "href": "ai/top_wb.html",
    "title": "AI webinars",
    "section": "",
    "text": "Map of current frameworks\n\n\n\n\nData version control\n\n\n\n\nFast computing with JAX\n\n\n\n\nDL in Julia with Flux\n\n\n\n\n\n\nPyTorch tensors\n\n\n\n\nImage upscaling\n\n\n\n\nfastai\n\n\n\n\nGitHub Copilot",
    "crumbs": [
      "AI",
      "<b><em>Webinars</em></b>"
    ]
  },
  {
    "objectID": "ai/wb_copilot.html",
    "href": "ai/wb_copilot.html",
    "title": "AI-powered programming with Copilot",
    "section": "",
    "text": "The recent advances in generative AI have brought about a number of code generators and code-completion assistants. This webinar will give an overview of the state of the field, briefly explain the functioning of various types of tools, then focus on GitHub Copilot.\nCopilot is developed by GitHub and OpenAI. It is a cloud-based service requiring a subscription, but students and teachers can apply for free access. It can be used directly in the command line or as an extension to text editors such as VS Code, Emacs, or Neovim.\nI will demo Copilot’s main features:\n\nprovide live code-completion,\nturn comments into code,\ntranslate from one programming language to another.\n\n\nSlides (Click and wait: this reveal.js presentation may take a little time to load.)",
    "crumbs": [
      "AI",
      "<b><em>Webinars</em></b>",
      "AI-powered coding with Copilot"
    ]
  },
  {
    "objectID": "ai/wb_dvc.html",
    "href": "ai/wb_dvc.html",
    "title": "Version control for data science and machine learning with DVC",
    "section": "",
    "text": "Data version control (DVC) is an open source tool that brings all the versioning and collaboration capabilities you use on your code with Git to your data and machine learning workflow.\nIf you use datasets in your work, it makes it easy to track their evolution.\nIf you are in the field of machine learning, it additionally allows you to track your models, manage your pipelines from parameters to metrics, collaborate on your experiments, and integrate with the continuous integration tool for machine learning projects CML.\nThis webinar will show you how to get started with DVC, first in the simple case where you just want to put your data under version control, then in the more complex situation where you want to manage your machine learning workflow in a more organized and reproducible fashion.\n\nSlides (Click and wait: this reveal.js presentation may take a little time to load.)",
    "crumbs": [
      "AI",
      "<b><em>Webinars</em></b>",
      "Data version control"
    ]
  },
  {
    "objectID": "ai/wb_flux.html",
    "href": "ai/wb_flux.html",
    "title": "Machine learning in Julia with Flux",
    "section": "",
    "text": "This webinar, aimed at users with no experience in machine learning, is an introduction to the basic concepts of neural networks, followed by a simple example—the classic classification of the MNIST database of handwritten digits—using the Julia package Flux.",
    "crumbs": [
      "AI",
      "<b><em>Webinars</em></b>",
      "DL in Julia with Flux"
    ]
  },
  {
    "objectID": "ai/ws_dl_nlp_llm_slides.html#ml-allows-to-achieve-previously-impossible-tasks",
    "href": "ai/ws_dl_nlp_llm_slides.html#ml-allows-to-achieve-previously-impossible-tasks",
    "title": "A quick introduction to deep learning, NLP, and LLMs ",
    "section": "ML allows to achieve previously impossible tasks",
    "text": "ML allows to achieve previously impossible tasks\n\nLet’s take the example of image recognition:\n\nIn typical computing, a programmer writes code that gives a computer detailed instructions of what to do\nCoding all the possible ways—pixel by pixel—that an image can represent, say, a dog is an impossibly large task: there are many breeds of dogs, the image can be a picture, a blurred picture, a drawing, a cartoon, the dog can be in all sorts of positions, wearing clothes, etc.\nThere just aren’t enough resources to make the traditional programming approach able to create a computer program that can identify a dog in images\nBy feeding a very large number of dog images to a neural network however, we can train that network to recognize dogs in images that it has never seen (without explicitly programming how it does this!)"
  },
  {
    "objectID": "ai/ws_dl_nlp_llm_slides.html#old-concept-new-computing-power",
    "href": "ai/ws_dl_nlp_llm_slides.html#old-concept-new-computing-power",
    "title": "A quick introduction to deep learning, NLP, and LLMs ",
    "section": "Old concept … new computing power",
    "text": "Old concept … new computing power\nThe concept is everything but new: Arthur Samuel came up with it in 1949 and built a self-learning Checkers-playing program in 1959\n\n\nMachine learning consists of feeding vast amounts of data to algorithms to strengthen pathways, so the excitement for the approach became somewhat dormant due to the lack of computing power and the lack of training data at the time\nThe advent of powerful computers, GPUs, and massive amounts of data have brought the old concept to the forefront\n\n\n\n\n\n\nFrom xkcd.com"
  },
  {
    "objectID": "ai/ws_dl_nlp_llm_slides.html#supervised-learning",
    "href": "ai/ws_dl_nlp_llm_slides.html#supervised-learning",
    "title": "A quick introduction to deep learning, NLP, and LLMs ",
    "section": "Supervised learning",
    "text": "Supervised learning\n\nRegression is a form of supervised learning with continuous outputs\nClassification is supervised learning with discrete outputs\n\nSupervised learning uses training data in the form of example input/output pairs\nGoal\nFind the relationship between inputs and outputs"
  },
  {
    "objectID": "ai/ws_dl_nlp_llm_slides.html#unsupervised-learning",
    "href": "ai/ws_dl_nlp_llm_slides.html#unsupervised-learning",
    "title": "A quick introduction to deep learning, NLP, and LLMs ",
    "section": "Unsupervised learning",
    "text": "Unsupervised learning\nClustering, social network analysis, market segmentation, PCA … are all forms of unsupervised learning\nUnsupervised learning uses unlabelled data\nGoal\nFind structure within the data"
  },
  {
    "objectID": "ai/ws_dl_nlp_llm_slides.html#reinforcement-learning",
    "href": "ai/ws_dl_nlp_llm_slides.html#reinforcement-learning",
    "title": "A quick introduction to deep learning, NLP, and LLMs ",
    "section": "Reinforcement learning",
    "text": "Reinforcement learning\nThe algorithm explores by performing random actions and these actions are rewarded or punished (bonus points or penalties)\nThis is how algorithms learn to play games"
  },
  {
    "objectID": "ai/ws_dl_nlp_llm_slides.html#decide-on-an-architecture",
    "href": "ai/ws_dl_nlp_llm_slides.html#decide-on-an-architecture",
    "title": "A quick introduction to deep learning, NLP, and LLMs ",
    "section": "Decide on an architecture",
    "text": "Decide on an architecture\n\nThe architecture won’t change during training\nThe type of architecture you choose (e.g. CNN, Transformer) depends on the type of data you have (e.g. vision, textual). The depth and breadth of your network depend on the amount of data and computing resource you have"
  },
  {
    "objectID": "ai/ws_dl_nlp_llm_slides.html#set-some-initial-parameters",
    "href": "ai/ws_dl_nlp_llm_slides.html#set-some-initial-parameters",
    "title": "A quick introduction to deep learning, NLP, and LLMs ",
    "section": "Set some initial parameters",
    "text": "Set some initial parameters\n\nYou can initialize them randomly or get much better ones through transfer learning\nWhile the parameters are also part of the model, those will change during training"
  },
  {
    "objectID": "ai/ws_dl_nlp_llm_slides.html#get-some-labelled-data",
    "href": "ai/ws_dl_nlp_llm_slides.html#get-some-labelled-data",
    "title": "A quick introduction to deep learning, NLP, and LLMs ",
    "section": "Get some labelled data",
    "text": "Get some labelled data\n\nWhen we say that we need a lot of data for machine learning, we mean “lots of labelled data” as this is what gets used for training models"
  },
  {
    "objectID": "ai/ws_dl_nlp_llm_slides.html#make-sure-to-keep-some-data-for-testing",
    "href": "ai/ws_dl_nlp_llm_slides.html#make-sure-to-keep-some-data-for-testing",
    "title": "A quick introduction to deep learning, NLP, and LLMs ",
    "section": "Make sure to keep some data for testing",
    "text": "Make sure to keep some data for testing\n\nThose data won’t be used for training the model. Often people keep around 20% of their data for testing"
  },
  {
    "objectID": "ai/ws_dl_nlp_llm_slides.html#pass-data-and-parameters-through-the-architecture",
    "href": "ai/ws_dl_nlp_llm_slides.html#pass-data-and-parameters-through-the-architecture",
    "title": "A quick introduction to deep learning, NLP, and LLMs ",
    "section": "Pass data and parameters through the architecture",
    "text": "Pass data and parameters through the architecture\n\nThe train data are the inputs and the process of calculating the outputs is the forward pass"
  },
  {
    "objectID": "ai/ws_dl_nlp_llm_slides.html#the-outputs-of-the-model-are-predictions",
    "href": "ai/ws_dl_nlp_llm_slides.html#the-outputs-of-the-model-are-predictions",
    "title": "A quick introduction to deep learning, NLP, and LLMs ",
    "section": "The outputs of the model are predictions",
    "text": "The outputs of the model are predictions"
  },
  {
    "objectID": "ai/ws_dl_nlp_llm_slides.html#compare-those-predictions-to-the-train-labels",
    "href": "ai/ws_dl_nlp_llm_slides.html#compare-those-predictions-to-the-train-labels",
    "title": "A quick introduction to deep learning, NLP, and LLMs ",
    "section": "Compare those predictions to the train labels",
    "text": "Compare those predictions to the train labels\n\nSince our data was labelled, we know what the true outputs are"
  },
  {
    "objectID": "ai/ws_dl_nlp_llm_slides.html#calculate-train-loss",
    "href": "ai/ws_dl_nlp_llm_slides.html#calculate-train-loss",
    "title": "A quick introduction to deep learning, NLP, and LLMs ",
    "section": "Calculate train loss",
    "text": "Calculate train loss\n\nThe deviation of our predictions from the true outputs gives us a measure of training loss"
  },
  {
    "objectID": "ai/ws_dl_nlp_llm_slides.html#adjust-parameters",
    "href": "ai/ws_dl_nlp_llm_slides.html#adjust-parameters",
    "title": "A quick introduction to deep learning, NLP, and LLMs ",
    "section": "Adjust parameters",
    "text": "Adjust parameters\n\nThe parameters get automatically adjusted to reduce the training loss through the mechanism of backpropagation. This is the actual training part\nThis process is repeated many times. Training models is pretty much a giant for loop"
  },
  {
    "objectID": "ai/ws_dl_nlp_llm_slides.html#from-model-to-program",
    "href": "ai/ws_dl_nlp_llm_slides.html#from-model-to-program",
    "title": "A quick introduction to deep learning, NLP, and LLMs ",
    "section": "From model to program",
    "text": "From model to program\n\nRemember that the model architecture is fixed, but that the parameters change at each iteration of the training process"
  },
  {
    "objectID": "ai/ws_dl_nlp_llm_slides.html#section",
    "href": "ai/ws_dl_nlp_llm_slides.html#section",
    "title": "A quick introduction to deep learning, NLP, and LLMs ",
    "section": " ",
    "text": "While the labelled data are key to training, what we are really interested in is the combination of architecture + final parameters"
  },
  {
    "objectID": "ai/ws_dl_nlp_llm_slides.html#section-1",
    "href": "ai/ws_dl_nlp_llm_slides.html#section-1",
    "title": "A quick introduction to deep learning, NLP, and LLMs ",
    "section": " ",
    "text": "When the training is over, the parameters become fixed. Which means that our model now behaves like a classic program"
  },
  {
    "objectID": "ai/ws_dl_nlp_llm_slides.html#evaluate-the-model",
    "href": "ai/ws_dl_nlp_llm_slides.html#evaluate-the-model",
    "title": "A quick introduction to deep learning, NLP, and LLMs ",
    "section": "Evaluate the model",
    "text": "Evaluate the model\n\nWe can now use the testing set (which was never used to train the model) to evaluate our model: if we pass the test inputs through our program, we get some predictions that we can compare to the test labels (which are the true outputs)\nThis gives us the test loss: a measure of how well our model performs"
  },
  {
    "objectID": "ai/ws_dl_nlp_llm_slides.html#use-the-model",
    "href": "ai/ws_dl_nlp_llm_slides.html#use-the-model",
    "title": "A quick introduction to deep learning, NLP, and LLMs ",
    "section": "Use the model",
    "text": "Use the model\n\nNow that we have a program, we can use it on unlabelled inputs to get what people ultimately want: unknown outputs\nThis is when we put our model to actual use to solve some problem"
  },
  {
    "objectID": "ai/ws_dl_nlp_llm_slides.html#learning",
    "href": "ai/ws_dl_nlp_llm_slides.html#learning",
    "title": "A quick introduction to deep learning, NLP, and LLMs ",
    "section": "Learning",
    "text": "Learning\n\n\nThe process of learning in biological NN happens through neuron death or growth and the creation or loss of synaptic connections between neurons\n\n\n\nIn ANN, learning happens through optimization algorithms such as gradient descent which minimize cross entropy loss functions by adjusting the weights and biases connecting each layer of neurons over many iterations"
  },
  {
    "objectID": "ai/ws_dl_nlp_llm_slides.html#fully-connected-neural-networks",
    "href": "ai/ws_dl_nlp_llm_slides.html#fully-connected-neural-networks",
    "title": "A quick introduction to deep learning, NLP, and LLMs ",
    "section": "Fully connected neural networks",
    "text": "Fully connected neural networks\n\n\n\n\n\nFrom Glosser.ca, Wikipedia\n\n\n\nEach neuron receives inputs from every neuron of the previous layer and passes its output to every neuron of the next layer"
  },
  {
    "objectID": "ai/ws_dl_nlp_llm_slides.html#convolutional-neural-networks",
    "href": "ai/ws_dl_nlp_llm_slides.html#convolutional-neural-networks",
    "title": "A quick introduction to deep learning, NLP, and LLMs ",
    "section": "Convolutional neural networks",
    "text": "Convolutional neural networks\n\n\nFrom Programming Journeys by Rensu Theart\n\nConvolutional neural networks (CNN) are used for spatially structured data (e.g. images)\nImages have huge input sizes and would require a very large number of neurons in a fully connected neural net. In convolutional layers, neurons receive input from a subarea (called local receptive field) of the previous layer. This greatly reduces the number of parameters. Optionally, pooling (combining the outputs of neurons in a subarea) reduces the data dimensions"
  },
  {
    "objectID": "ai/ws_dl_nlp_llm_slides.html#recurrent-neural-networks",
    "href": "ai/ws_dl_nlp_llm_slides.html#recurrent-neural-networks",
    "title": "A quick introduction to deep learning, NLP, and LLMs ",
    "section": "Recurrent neural networks",
    "text": "Recurrent neural networks\n\n\nFrom fdeloche, Wikipedia\n\nRecurrent neural networks (RNN) such as Long Short-Term Memory (LSTM) are used for chain structured data (e.g. text)\nThey are not feedforward networks (i.e. networks for which the information moves only in the forward direction without any loop)"
  },
  {
    "objectID": "ai/ws_dl_nlp_llm_slides.html#transformers",
    "href": "ai/ws_dl_nlp_llm_slides.html#transformers",
    "title": "A quick introduction to deep learning, NLP, and LLMs ",
    "section": "Transformers",
    "text": "Transformers\nA combination of two RNNs (the encoder and the decoder) is used in sequence to sequence models for translation or picture captioning\nIn 2014 the concept of attention (giving added weight to important words) was developed, greatly improving the ability of such models to process a lot of data\nThe problem with recurrence is that it is not easily to parallelize (and thus to run fast on GPUs)\nIn 2017, a new model—the transformer—was proposed: by using only attention mechanisms and no recurrence, the transformer achieves better results in an easily parallelizable fashion\nWith the addition of transfer learning, powerful transformers emerged in the field of NLP (e.g. Bidirectional Encoder Representations from Transformers (BERT) from Google and Generative Pre-trained Transformer-3 (GPT-3) from OpenAI)"
  },
  {
    "objectID": "ai/ws_dl_nlp_llm_slides.html#data-bias",
    "href": "ai/ws_dl_nlp_llm_slides.html#data-bias",
    "title": "A quick introduction to deep learning, NLP, and LLMs ",
    "section": "Data bias",
    "text": "Data bias\nBias is always present in data\nDocument the limitations and scope of your data as best as possible\nProblems to watch for:\n\nOut of domain data: data used for training are not relevant to the model application\nDomain shift: model becoming inadapted as conditions evolve\nFeedback loop: initial bias exacerbated over the time"
  },
  {
    "objectID": "ai/ws_dl_nlp_llm_slides.html#transformation-of-subjects",
    "href": "ai/ws_dl_nlp_llm_slides.html#transformation-of-subjects",
    "title": "A quick introduction to deep learning, NLP, and LLMs ",
    "section": "Transformation of subjects",
    "text": "Transformation of subjects\nAlgorithms are supposed to help us, not transform us (e.g. YouTube recommendation algorithms)"
  },
  {
    "objectID": "ai/ws_dl_nlp_llm_slides.html#bugs",
    "href": "ai/ws_dl_nlp_llm_slides.html#bugs",
    "title": "A quick introduction to deep learning, NLP, and LLMs ",
    "section": "Bugs",
    "text": "Bugs\nExample of bug with real life consequences"
  },
  {
    "objectID": "ai/ws_dl_nlp_llm_slides.html#what-is-nlp",
    "href": "ai/ws_dl_nlp_llm_slides.html#what-is-nlp",
    "title": "A quick introduction to deep learning, NLP, and LLMs ",
    "section": "What is NLP?",
    "text": "What is NLP?\nNatural language processing is simply the application of machine learning to human (natural) language"
  },
  {
    "objectID": "ai/ws_dl_nlp_llm_slides.html#applications",
    "href": "ai/ws_dl_nlp_llm_slides.html#applications",
    "title": "A quick introduction to deep learning, NLP, and LLMs ",
    "section": "Applications",
    "text": "Applications\n\nSpam detection\nTranslation\nSentiment analysis\nPredictive text\nText classification\nSpeech recognition\nNatural language generation\nChatbots\nSearch results"
  },
  {
    "objectID": "ai/ws_dl_nlp_llm_slides.html#data-processing",
    "href": "ai/ws_dl_nlp_llm_slides.html#data-processing",
    "title": "A quick introduction to deep learning, NLP, and LLMs ",
    "section": "Data processing",
    "text": "Data processing\n\nTokenization: split text into sentences (sentence tokenization) and words (word tokenization)\nRemove punctuation and stopwords (e.g. “the”, “a”, “and”, “is”, “are”)\nTurn all words to lower case\nKeep only the lemma of words (lemmatization)\n\n\nAn alternative and simpler method is stemming\n\n\nIdentify word collocations (groups of words that often occur together, such as the bigrams “United States” or “open source”)\nTagging\nModel training"
  },
  {
    "objectID": "ai/ws_dl_nlp_llm_slides.html#many-options",
    "href": "ai/ws_dl_nlp_llm_slides.html#many-options",
    "title": "A quick introduction to deep learning, NLP, and LLMs ",
    "section": "Many options",
    "text": "Many options\nHere are just a few:\n\nscikit-learn: a Python ML library built on top of SciPy\nNatural Language Toolkit (NLTK): a suite of Python libraries geared towards teaching and research\nspaCy: Python library geared towards production\ntorchtext, part of the PyTorch project (and many options of added layers on top such as PyTorch-NLP): Python library\nGenSim: Python library\nStanford CoreNLP: Java library\nMany libraries in the Julia programming language"
  },
  {
    "objectID": "ai/ws_dl_nlp_llm_slides.html#which-one-to-choose",
    "href": "ai/ws_dl_nlp_llm_slides.html#which-one-to-choose",
    "title": "A quick introduction to deep learning, NLP, and LLMs ",
    "section": "Which one to choose?",
    "text": "Which one to choose?\nChoose an open source tool (i.e. stay away from proprietary software such as MATLAB)\n\nResearchers who do not have access to the tool cannot reproduce your methods (open tools = open equitable research)\nOnce you graduate, you may not have access to the tool anymore\nYour university may stop paying for a license\nYou may get locked-in\nProprietary tools are often black boxes\nLong-term access is not guaranty (problem to replicate studies)\nThe licenses you have access to may be limiting and a cause of headache\nProprietary tools fall behind popular open-source tools\nProprietary tools often fail to address specialized edge cases needed in research"
  },
  {
    "objectID": "ai/ws_dl_nlp_llm_slides.html#neural-nets",
    "href": "ai/ws_dl_nlp_llm_slides.html#neural-nets",
    "title": "A quick introduction to deep learning, NLP, and LLMs ",
    "section": "Neural nets",
    "text": "Neural nets\n3Blue1Brown by Grant Sanderson has a series of 4 videos on neural networks which is easy to watch, fun, and does an excellent job at introducing the functioning of a simple neural network\n\nWhat are NN? (19 min)\nHow do NN learn? (21 min)\nWhat is backpropagation? (14 min)\nHow does backpropagation work? (10 min)\n\n\nAs you develop your own ML models, if you find that your mathematical background is shaky, 3blue1brown also has an excellent series of videos on linear algebra and an equally great one on calculus"
  },
  {
    "objectID": "ai/ws_dl_nlp_llm_slides.html#open-access-preprints",
    "href": "ai/ws_dl_nlp_llm_slides.html#open-access-preprints",
    "title": "A quick introduction to deep learning, NLP, and LLMs ",
    "section": "Open-access preprints",
    "text": "Open-access preprints\n\nArxiv Sanity Preserver by Andrej Karpathy\nML papers in the computer science category on arXiv\nML papers in the stats category on arXiv\nDistill ML research online journal"
  },
  {
    "objectID": "ai/ws_dl_nlp_llm_slides.html#advice-and-sources",
    "href": "ai/ws_dl_nlp_llm_slides.html#advice-and-sources",
    "title": "A quick introduction to deep learning, NLP, and LLMs ",
    "section": "Advice and sources",
    "text": "Advice and sources\n\nAdvice and sources from ML research student"
  },
  {
    "objectID": "ai/ws_dl_nlp_llm_slides.html#getting-help",
    "href": "ai/ws_dl_nlp_llm_slides.html#getting-help",
    "title": "A quick introduction to deep learning, NLP, and LLMs ",
    "section": "Getting help",
    "text": "Getting help\nStack Overflow:\n\n[machine-learning] tag\n[deep-learning] tag\n[supervised-learning] tag\n[unsupervised-learning] tag\n[semisupervised-learning] tag\n[reinforcement-learning] tag\n[transfer-learning] tag\n[machine-learning-model] tag\n[learning-rate] tag"
  },
  {
    "objectID": "ai/ws_dl_nlp_llm_slides.html#open-datasets",
    "href": "ai/ws_dl_nlp_llm_slides.html#open-datasets",
    "title": "A quick introduction to deep learning, NLP, and LLMs ",
    "section": "Open datasets",
    "text": "Open datasets\n\nbenchmarks.ai\nAIBench\nkaggle\nWikipedia"
  },
  {
    "objectID": "ai/ws_dl_nlp_llm_slides.html#pytorch",
    "href": "ai/ws_dl_nlp_llm_slides.html#pytorch",
    "title": "A quick introduction to deep learning, NLP, and LLMs ",
    "section": "PyTorch",
    "text": "PyTorch\n\nDocumentation\nTutorials\nExamples\n\nGetting help\n\nPyTorch Discourse forum\nStack Overflow [pytorch] tag\n\nPre-trained models\n\nPyTorch Hub"
  },
  {
    "objectID": "ai/ws_dl_nlp_llm_slides.html#python",
    "href": "ai/ws_dl_nlp_llm_slides.html#python",
    "title": "A quick introduction to deep learning, NLP, and LLMs ",
    "section": "Python",
    "text": "Python\nIDE\n\nProject Jupyter\nList of IDEs with description\nComparison of IDEs\nEmacs Python IDE\n\nGetting help\n\nStack Overflow [python] tag\n\n\n\n\n\n\n\n\n\n\n\n Back to workshop page"
  },
  {
    "objectID": "ai/ws_hss_intro_slides.html#ml-allows-to-achieve-previously-impossible-tasks",
    "href": "ai/ws_hss_intro_slides.html#ml-allows-to-achieve-previously-impossible-tasks",
    "title": "Introduction to machine learning for the humanities",
    "section": "ML allows to achieve previously impossible tasks",
    "text": "ML allows to achieve previously impossible tasks\n\nLet’s take the example of image recognition:\n\nIn typical computing, a programmer writes code that gives a computer detailed instructions of what to do\nCoding all the possible ways—pixel by pixel—that an image can represent, say, a dog is an impossibly large task: there are many breeds of dogs, the image can be a picture, a blurred picture, a drawing, a cartoon, the dog can be in all sorts of positions, wearing clothes, etc.\nThere just aren’t enough resources to make the traditional programming approach able to create a computer program that can identify a dog in images\nBy feeding a very large number of dog images to a neural network however, we can train that network to recognize dogs in images that it has never seen (without explicitly programming how it does this!)"
  },
  {
    "objectID": "ai/ws_hss_intro_slides.html#old-concept-new-computing-power",
    "href": "ai/ws_hss_intro_slides.html#old-concept-new-computing-power",
    "title": "Introduction to machine learning for the humanities",
    "section": "Old concept … new computing power",
    "text": "Old concept … new computing power\nThe concept is everything but new: Arthur Samuel came up with it in 1949 and built a self-learning Checkers-playing program in 1959\n\n\nMachine learning consists of feeding vast amounts of data to algorithms to strengthen pathways, so the excitement for the approach became somewhat dormant due to the lack of computing power and the lack of training data at the time\nThe advent of powerful computers, GPUs, and massive amounts of data have brought the old concept to the forefront\n\n\n\n\n\n\nFrom xkcd.com"
  },
  {
    "objectID": "ai/ws_hss_intro_slides.html#supervised-learning",
    "href": "ai/ws_hss_intro_slides.html#supervised-learning",
    "title": "Introduction to machine learning for the humanities",
    "section": "Supervised learning",
    "text": "Supervised learning\n\nRegression is a form of supervised learning with continuous outputs\nClassification is supervised learning with discrete outputs\n\nSupervised learning uses training data in the form of example input/output pairs\nGoal\nFind the relationship between inputs and outputs"
  },
  {
    "objectID": "ai/ws_hss_intro_slides.html#unsupervised-learning",
    "href": "ai/ws_hss_intro_slides.html#unsupervised-learning",
    "title": "Introduction to machine learning for the humanities",
    "section": "Unsupervised learning",
    "text": "Unsupervised learning\nClustering, social network analysis, market segmentation, PCA … are all forms of unsupervised learning\nUnsupervised learning uses unlabelled data\nGoal\nFind structure within the data"
  },
  {
    "objectID": "ai/ws_hss_intro_slides.html#reinforcement-learning",
    "href": "ai/ws_hss_intro_slides.html#reinforcement-learning",
    "title": "Introduction to machine learning for the humanities",
    "section": "Reinforcement learning",
    "text": "Reinforcement learning\nThe algorithm explores by performing random actions and these actions are rewarded or punished (bonus points or penalties)\nThis is how algorithms learn to play games"
  },
  {
    "objectID": "ai/ws_hss_intro_slides.html#decide-on-an-architecture",
    "href": "ai/ws_hss_intro_slides.html#decide-on-an-architecture",
    "title": "Introduction to machine learning for the humanities",
    "section": "Decide on an architecture",
    "text": "Decide on an architecture\n\nThe architecture won’t change during training\nThe type of architecture you choose (e.g. CNN, Transformer) depends on the type of data you have (e.g. vision, textual). The depth and breadth of your network depend on the amount of data and computing resource you have"
  },
  {
    "objectID": "ai/ws_hss_intro_slides.html#set-some-initial-parameters",
    "href": "ai/ws_hss_intro_slides.html#set-some-initial-parameters",
    "title": "Introduction to machine learning for the humanities",
    "section": "Set some initial parameters",
    "text": "Set some initial parameters\n\nYou can initialize them randomly or get much better ones through transfer learning\nWhile the parameters are also part of the model, those will change during training"
  },
  {
    "objectID": "ai/ws_hss_intro_slides.html#get-some-labelled-data",
    "href": "ai/ws_hss_intro_slides.html#get-some-labelled-data",
    "title": "Introduction to machine learning for the humanities",
    "section": "Get some labelled data",
    "text": "Get some labelled data\n\nWhen we say that we need a lot of data for machine learning, we mean “lots of labelled data” as this is what gets used for training models"
  },
  {
    "objectID": "ai/ws_hss_intro_slides.html#make-sure-to-keep-some-data-for-testing",
    "href": "ai/ws_hss_intro_slides.html#make-sure-to-keep-some-data-for-testing",
    "title": "Introduction to machine learning for the humanities",
    "section": "Make sure to keep some data for testing",
    "text": "Make sure to keep some data for testing\n\nThose data won’t be used for training the model. Often people keep around 20% of their data for testing"
  },
  {
    "objectID": "ai/ws_hss_intro_slides.html#pass-data-and-parameters-through-the-architecture",
    "href": "ai/ws_hss_intro_slides.html#pass-data-and-parameters-through-the-architecture",
    "title": "Introduction to machine learning for the humanities",
    "section": "Pass data and parameters through the architecture",
    "text": "Pass data and parameters through the architecture\n\nThe train data are the inputs and the process of calculating the outputs is the forward pass"
  },
  {
    "objectID": "ai/ws_hss_intro_slides.html#the-outputs-of-the-model-are-predictions",
    "href": "ai/ws_hss_intro_slides.html#the-outputs-of-the-model-are-predictions",
    "title": "Introduction to machine learning for the humanities",
    "section": "The outputs of the model are predictions",
    "text": "The outputs of the model are predictions"
  },
  {
    "objectID": "ai/ws_hss_intro_slides.html#compare-those-predictions-to-the-train-labels",
    "href": "ai/ws_hss_intro_slides.html#compare-those-predictions-to-the-train-labels",
    "title": "Introduction to machine learning for the humanities",
    "section": "Compare those predictions to the train labels",
    "text": "Compare those predictions to the train labels\n\nSince our data was labelled, we know what the true outputs are"
  },
  {
    "objectID": "ai/ws_hss_intro_slides.html#calculate-train-loss",
    "href": "ai/ws_hss_intro_slides.html#calculate-train-loss",
    "title": "Introduction to machine learning for the humanities",
    "section": "Calculate train loss",
    "text": "Calculate train loss\n\nThe deviation of our predictions from the true outputs gives us a measure of training loss"
  },
  {
    "objectID": "ai/ws_hss_intro_slides.html#adjust-parameters",
    "href": "ai/ws_hss_intro_slides.html#adjust-parameters",
    "title": "Introduction to machine learning for the humanities",
    "section": "Adjust parameters",
    "text": "Adjust parameters\n\nThe parameters get automatically adjusted to reduce the training loss through the mechanism of backpropagation. This is the actual training part\nThis process is repeated many times. Training models is pretty much a giant for loop"
  },
  {
    "objectID": "ai/ws_hss_intro_slides.html#from-model-to-program",
    "href": "ai/ws_hss_intro_slides.html#from-model-to-program",
    "title": "Introduction to machine learning for the humanities",
    "section": "From model to program",
    "text": "From model to program\n\nRemember that the model architecture is fixed, but that the parameters change at each iteration of the training process"
  },
  {
    "objectID": "ai/ws_hss_intro_slides.html#section",
    "href": "ai/ws_hss_intro_slides.html#section",
    "title": "Introduction to machine learning for the humanities",
    "section": " ",
    "text": "While the labelled data are key to training, what we are really interested in is the combination of architecture + final parameters"
  },
  {
    "objectID": "ai/ws_hss_intro_slides.html#section-1",
    "href": "ai/ws_hss_intro_slides.html#section-1",
    "title": "Introduction to machine learning for the humanities",
    "section": " ",
    "text": "When the training is over, the parameters become fixed. Which means that our model now behaves like a classic program"
  },
  {
    "objectID": "ai/ws_hss_intro_slides.html#evaluate-the-model",
    "href": "ai/ws_hss_intro_slides.html#evaluate-the-model",
    "title": "Introduction to machine learning for the humanities",
    "section": "Evaluate the model",
    "text": "Evaluate the model\n\nWe can now use the testing set (which was never used to train the model) to evaluate our model: if we pass the test inputs through our program, we get some predictions that we can compare to the test labels (which are the true outputs)\nThis gives us the test loss: a measure of how well our model performs"
  },
  {
    "objectID": "ai/ws_hss_intro_slides.html#use-the-model",
    "href": "ai/ws_hss_intro_slides.html#use-the-model",
    "title": "Introduction to machine learning for the humanities",
    "section": "Use the model",
    "text": "Use the model\n\nNow that we have a program, we can use it on unlabelled inputs to get what people ultimately want: unknown outputs\nThis is when we put our model to actual use to solve some problem"
  },
  {
    "objectID": "ai/ws_hss_intro_slides.html#learning",
    "href": "ai/ws_hss_intro_slides.html#learning",
    "title": "Introduction to machine learning for the humanities",
    "section": "Learning",
    "text": "Learning\n\n\nThe process of learning in biological NN happens through neuron death or growth and the creation or loss of synaptic connections between neurons\n\n\n\nIn ANN, learning happens through optimization algorithms such as gradient descent which minimize cross entropy loss functions by adjusting the weights and biases connecting each layer of neurons over many iterations"
  },
  {
    "objectID": "ai/ws_hss_intro_slides.html#fully-connected-neural-networks",
    "href": "ai/ws_hss_intro_slides.html#fully-connected-neural-networks",
    "title": "Introduction to machine learning for the humanities",
    "section": "Fully connected neural networks",
    "text": "Fully connected neural networks\n\n\n\n\n\nFrom Glosser.ca, Wikipedia\n\n\n\nEach neuron receives inputs from every neuron of the previous layer and passes its output to every neuron of the next layer"
  },
  {
    "objectID": "ai/ws_hss_intro_slides.html#convolutional-neural-networks",
    "href": "ai/ws_hss_intro_slides.html#convolutional-neural-networks",
    "title": "Introduction to machine learning for the humanities",
    "section": "Convolutional neural networks",
    "text": "Convolutional neural networks\n\n\nFrom Programming Journeys by Rensu Theart\n\nConvolutional neural networks (CNN) are used for spatially structured data (e.g. images)\nImages have huge input sizes and would require a very large number of neurons in a fully connected neural net. In convolutional layers, neurons receive input from a subarea (called local receptive field) of the previous layer. This greatly reduces the number of parameters. Optionally, pooling (combining the outputs of neurons in a subarea) reduces the data dimensions"
  },
  {
    "objectID": "ai/ws_hss_intro_slides.html#recurrent-neural-networks",
    "href": "ai/ws_hss_intro_slides.html#recurrent-neural-networks",
    "title": "Introduction to machine learning for the humanities",
    "section": "Recurrent neural networks",
    "text": "Recurrent neural networks\n\n\nFrom fdeloche, Wikipedia\n\nRecurrent neural networks (RNN) such as Long Short-Term Memory (LSTM) are used for chain structured data (e.g. text)\nThey are not feedforward networks (i.e. networks for which the information moves only in the forward direction without any loop)"
  },
  {
    "objectID": "ai/ws_hss_intro_slides.html#transformers",
    "href": "ai/ws_hss_intro_slides.html#transformers",
    "title": "Introduction to machine learning for the humanities",
    "section": "Transformers",
    "text": "Transformers\nA combination of two RNNs (the encoder and the decoder) is used in sequence to sequence models for translation or picture captioning\nIn 2014 the concept of attention (giving added weight to important words) was developed, greatly improving the ability of such models to process a lot of data\nThe problem with recurrence is that it is not easily to parallelize (and thus to run fast on GPUs)\nIn 2017, a new model—the transformer—was proposed: by using only attention mechanisms and no recurrence, the transformer achieves better results in an easily parallelizable fashion\nWith the addition of transfer learning, powerful transformers emerged in the field of NLP (e.g. Bidirectional Encoder Representations from Transformers (BERT) from Google and Generative Pre-trained Transformer-3 (GPT-3) from OpenAI)"
  },
  {
    "objectID": "ai/ws_hss_intro_slides.html#data-bias",
    "href": "ai/ws_hss_intro_slides.html#data-bias",
    "title": "Introduction to machine learning for the humanities",
    "section": "Data bias",
    "text": "Data bias\nBias is always present in data\nDocument the limitations and scope of your data as best as possible\nProblems to watch for:\n\nOut of domain data: data used for training are not relevant to the model application\nDomain shift: model becoming inadapted as conditions evolve\nFeedback loop: initial bias exacerbated over the time"
  },
  {
    "objectID": "ai/ws_hss_intro_slides.html#transformation-of-subjects",
    "href": "ai/ws_hss_intro_slides.html#transformation-of-subjects",
    "title": "Introduction to machine learning for the humanities",
    "section": "Transformation of subjects",
    "text": "Transformation of subjects\nAlgorithms are supposed to help us, not transform us (e.g. YouTube recommendation algorithms)"
  },
  {
    "objectID": "ai/ws_hss_intro_slides.html#bugs",
    "href": "ai/ws_hss_intro_slides.html#bugs",
    "title": "Introduction to machine learning for the humanities",
    "section": "Bugs",
    "text": "Bugs\nExample of bug with real life consequences"
  },
  {
    "objectID": "ai/ws_hss_intro_slides.html#many-options",
    "href": "ai/ws_hss_intro_slides.html#many-options",
    "title": "Introduction to machine learning for the humanities",
    "section": "Many options",
    "text": "Many options\nHere are just a few:\n\nscikit-learn: a Python ML library built on top of SciPy\nNatural Language Toolkit (NLTK): a suite of Python libraries geared towards teaching and research\nspaCy: Python library geared towards production\ntorchtext, part of the PyTorch project (and many options of added layers on top such as PyTorch-NLP): Python library\nGenSim: Python library\nStanford CoreNLP: Java library\nMany libraries in the Julia programming language"
  },
  {
    "objectID": "ai/ws_hss_intro_slides.html#which-one-to-choose",
    "href": "ai/ws_hss_intro_slides.html#which-one-to-choose",
    "title": "Introduction to machine learning for the humanities",
    "section": "Which one to choose?",
    "text": "Which one to choose?\nChoose an open source tool (i.e. stay away from proprietary software such as MATLAB)\n\nResearchers who do not have access to the tool cannot reproduce your methods (open tools = open equitable research)\nOnce you graduate, you may not have access to the tool anymore\nYour university may stop paying for a license\nYou may get locked-in\nProprietary tools are often black boxes\nLong-term access is not guaranty (problem to replicate studies)\nThe licenses you have access to may be limiting and a cause of headache\nProprietary tools fall behind popular open-source tools\nProprietary tools often fail to address specialized edge cases needed in research"
  },
  {
    "objectID": "ai/ws_hss_intro_slides.html#neural-nets",
    "href": "ai/ws_hss_intro_slides.html#neural-nets",
    "title": "Introduction to machine learning for the humanities",
    "section": "Neural nets",
    "text": "Neural nets\n3Blue1Brown by Grant Sanderson has a series of 4 videos on neural networks which is easy to watch, fun, and does an excellent job at introducing the functioning of a simple neural network\n\nWhat are NN? (19 min)\nHow do NN learn? (21 min)\nWhat is backpropagation? (14 min)\nHow does backpropagation work? (10 min)\n\n\nAs you develop your own ML models, if you find that your mathematical background is shaky, 3blue1brown also has an excellent series of videos on linear algebra and an equally great one on calculus"
  },
  {
    "objectID": "ai/ws_hss_intro_slides.html#open-access-preprints",
    "href": "ai/ws_hss_intro_slides.html#open-access-preprints",
    "title": "Introduction to machine learning for the humanities",
    "section": "Open-access preprints",
    "text": "Open-access preprints\n\nArxiv Sanity Preserver by Andrej Karpathy\nML papers in the computer science category on arXiv\nML papers in the stats category on arXiv\nDistill ML research online journal"
  },
  {
    "objectID": "ai/ws_hss_intro_slides.html#advice-and-sources",
    "href": "ai/ws_hss_intro_slides.html#advice-and-sources",
    "title": "Introduction to machine learning for the humanities",
    "section": "Advice and sources",
    "text": "Advice and sources\n\nAdvice and sources from ML research student"
  },
  {
    "objectID": "ai/ws_hss_intro_slides.html#getting-help",
    "href": "ai/ws_hss_intro_slides.html#getting-help",
    "title": "Introduction to machine learning for the humanities",
    "section": "Getting help",
    "text": "Getting help\nStack Overflow …\n\n[machine-learning] tag\n[deep-learning] tag\n[supervised-learning] tag\n[unsupervised-learning] tag\n[semisupervised-learning] tag\n[reinforcement-learning] tag\n[transfer-learning] tag\n[machine-learning-model] tag\n[learning-rate] tag"
  },
  {
    "objectID": "ai/ws_hss_intro_slides.html#open-datasets",
    "href": "ai/ws_hss_intro_slides.html#open-datasets",
    "title": "Introduction to machine learning for the humanities",
    "section": "Open datasets",
    "text": "Open datasets\n\nbenchmarks.ai\nAIBench\nkaggle\nWikipedia"
  },
  {
    "objectID": "ai/ws_hss_intro_slides.html#pytorch",
    "href": "ai/ws_hss_intro_slides.html#pytorch",
    "title": "Introduction to machine learning for the humanities",
    "section": "PyTorch",
    "text": "PyTorch\n\nDocumentation\nTutorials\nExamples\n\nGetting help\n\nPyTorch Discourse forum\nStack Overflow [pytorch] tag\n\nPre-trained models\n\nPyTorch Hub"
  },
  {
    "objectID": "ai/ws_hss_intro_slides.html#python",
    "href": "ai/ws_hss_intro_slides.html#python",
    "title": "Introduction to machine learning for the humanities",
    "section": "Python",
    "text": "Python\nIDE\n\nProject Jupyter\nList of IDEs with description\nComparison of IDEs\nEmacs Python IDE\n\nGetting help\n\nStack Overflow [python] tag\n\n\n\n\n\n\n\n\n\n\n\n Back to workshop page"
  },
  {
    "objectID": "bash/intro_aliases.html",
    "href": "bash/intro_aliases.html",
    "title": "Aliases",
    "section": "",
    "text": "Aliases are a convenient way to assign a custom command to a name. You can use new names or re-assign existing command names.\n\nalias myip=\"ip -json route get 8.8.8.8 | jq -r '.[].prefsrc'\"\nalias ls='ls -F'\nYou can retrieve the definition of an alias by running the alias command without argument. To remove an alias, use unalias.\nYou can use the non-aliased version of a command with \\ (e.g. \\ls).",
    "crumbs": [
      "Bash",
      "<b><em>Getting started</em></b>",
      "Aliases"
    ]
  },
  {
    "objectID": "bash/intro_basics.html",
    "href": "bash/intro_basics.html",
    "title": "Bash: the basics",
    "section": "",
    "text": "What does it feel like to work in a shell? Here is a first basic orientation.",
    "crumbs": [
      "Bash",
      "<b><em>Getting started</em></b>",
      "Bash: the basics"
    ]
  },
  {
    "objectID": "bash/intro_basics.html#the-prompt",
    "href": "bash/intro_basics.html#the-prompt",
    "title": "Bash: the basics",
    "section": "The prompt",
    "text": "The prompt\nIn command-line interfaces, a command prompt is a sequence of characters indicating that the interpreter is ready to accept input. It can also provide some information (e.g. time, error types, username and hostname, etc.)\nThe Bash prompt is customizable. By default, it often gives the username and the hostname, and it typically ends with $.",
    "crumbs": [
      "Bash",
      "<b><em>Getting started</em></b>",
      "Bash: the basics"
    ]
  },
  {
    "objectID": "bash/intro_basics.html#commands",
    "href": "bash/intro_basics.html#commands",
    "title": "Bash: the basics",
    "section": "Commands",
    "text": "Commands\nBash comes with a number of commands: directives to the shell to perform particular tasks.\n\nExamples of commands:\n\n\nPrint working directory: pwd\nChange directory: cd\nPrint: echo\nPrint content of a file: cat\nList files and directories in working directory: ls\nCopy: cp\nMove or rename: mv\nCreate a new directory: mkdir\nCreate a new file: touch\n\nTo execute a command, you type it, then press the &lt;enter&gt;.\n\n\nYour turn:\n\nRun your first command:\nls\n\n\nCommand options\nCommands come with a number of flags (options).\n\nExamples of flags for the ls command:\n\n\nList all files and directories (not ignoring hidden files): ls -a\nList files and directories in a long format: ls -l\nList files and directories in a human readable format (using units such as K, M, G): ls -h\n\nFlags can be combined. The order doesn’t matter and the followings are all equivalent:\n\nls -alh\nls -a -l -h\nls -ahl\nls -l -ha\n…\n\n\n\nHelp on commands\nThe command man provides an interface to the system reference manual.\nTo access the manual page of a command, you type:\nman &lt;command&gt;\n\nThe &lt; and &gt; symbols are used to delineate a generic placeholder that you should replace by the value of your choice (here, for instance, man ls).\n\n\nMan pages open in a pager (usually less).\nUseful keybindings when you are in the pager:\nSPACE      scroll one screen down\nb          backa one screen\nq          quit the pager\ng          go to the top of the document\n7g         go to line 7 from the top\nG          go to the bottom of the document\n/          search for a term\n           n will take you to the next result\n           N to the previous result\n\n\n\nYour turn:\n\n\nOpen the man page for the ls command.\nNavigate down a few pages, then navigate back up.\nSearch for the first 5 occurrences of the word “directory”.\nWhat does ls -r do?\nFinally, leave the pager.\n\n\nHelp pages can be accessed with:\n&lt;command&gt; --help\n\n\nYour turn:\n\nAccess the help of the ls command.\n\nTo know the nature of a command (e.g. shell built-in function, an alias that you created, or the path of an utility) run either of:\ncommand -V &lt;command&gt;\ntype &lt;command&gt;\n\n\nYour turn:\n\nWhat is the nature of the pwd command?",
    "crumbs": [
      "Bash",
      "<b><em>Getting started</em></b>",
      "Bash: the basics"
    ]
  },
  {
    "objectID": "bash/intro_basics.html#shell-keybindings",
    "href": "bash/intro_basics.html#shell-keybindings",
    "title": "Bash: the basics",
    "section": "Shell keybindings",
    "text": "Shell keybindings\nHere are a few useful keybindings that you can use in the shell:\n\nClear the terminal (command clear) with C-l (this means: press the Ctrl and l keys at the same time).\nNavigate the command history with C-p and C-n (or up and down arrows).\nYou can auto-complete commands by pressing the &lt;tab&gt; key.",
    "crumbs": [
      "Bash",
      "<b><em>Getting started</em></b>",
      "Bash: the basics"
    ]
  },
  {
    "objectID": "bash/intro_basics.html#comments",
    "href": "bash/intro_basics.html#comments",
    "title": "Bash: the basics",
    "section": "Comments",
    "text": "Comments\nAnything to the right of the symbol # is ignored by the interpreter and is for human consumption only.\n# You can write full-line comments\n\npwd       # You can also write comments after a command\nComments are used to document scripts (text files with a number of commands). DO USE THEM: future you will thank you 🙂.",
    "crumbs": [
      "Bash",
      "<b><em>Getting started</em></b>",
      "Bash: the basics"
    ]
  },
  {
    "objectID": "bash/intro_filesystem.html",
    "href": "bash/intro_filesystem.html",
    "title": "The Unix filesystem",
    "section": "",
    "text": "Bash allows to give instructions to a Unix operating system. The first thing you’ll need to know is how storage is organized on such as system.",
    "crumbs": [
      "Bash",
      "<b><em>Getting started</em></b>",
      "The Unix filesystem"
    ]
  },
  {
    "objectID": "bash/intro_filesystem.html#structure",
    "href": "bash/intro_filesystem.html#structure",
    "title": "The Unix filesystem",
    "section": "Structure",
    "text": "Structure\nThe Unix filesystem is a rooted tree of directories. The root is denoted by /.\nSeveral directories exist under the root. Here are a few:\n\n/bin     This is where binaries are stored.\n/boot    There, you can find the files necessary for booting the system.\n/home    This directory contains all the users’ home directories.\n\nThese directories in turn can contain other directories. /home for instance contains the directories:\n\n/home/user01\n/home/user02\n/home/user03\n…\n\nThe home directory of each user contains many files and directories.",
    "crumbs": [
      "Bash",
      "<b><em>Getting started</em></b>",
      "The Unix filesystem"
    ]
  },
  {
    "objectID": "bash/intro_filesystem.html#navigation",
    "href": "bash/intro_filesystem.html#navigation",
    "title": "The Unix filesystem",
    "section": "Navigation",
    "text": "Navigation\n\nWorking directory\nThe current working directory can be obtained with:\npwd       # print working directory\n\n\nYour turn:\n\nWhat is your current working directory?\n\n\n\nChanging directory\nTo navigate to another directory, use cd (change directory) followed by the path of the directory.\n\nExample:\n\ncd /home\nBecause /home was the parent directory of our working directory (one level above in the rooted tree), we could have also gotten there with cd .. — the two dots represent one level up (a single dot represents the working directory).\n\n\nYour turn:\n\n\nWhat will happen if you run cd .. from /home?\nWhat will happen if you run cd . from /home?\n\n\nFrom any location, you can always go back to your home directory (e.g. /home/user09) by running cd without argument. Alternatively, you can use cd ~. This is because ~ gets expanded by the shell into the path of your home. Finally, you can use cd $HOME. $HOME is an environment variable representing the path of your home.\n\n\nYour turn:\n\nTry using cd - (that’s the minus sign) a few times. What does this command do?\n\n\n\nAbsolute and relative paths\n\nAbsolute paths give the full path from the root (e.g. /bin, /home/user09/file).\nRelative paths give the path relative to the working directory (e.g. ../dir/file, dir/subdir).\n\n\n\nYour turn:\n\nIs ~ an absolute or relative path?\n\n\nIn the filesystem below, the current working directory is /home/user01.\n\nWhat is the output of ls?\nWhat is the output of ls ../..?\nThe output of ls /thesis/src is:\n\n\nls: cannot access ‘/thesis/src’: No such file or directory\n\n  Why?\n\nWhat are 2 ways to navigate to the results directory?\nFrom the results directory, what are 2 ways to print the content of the src directory?\n\n\n\n\n\n\n\n\n\ncluster\n\n\n\n\nuser01--.bashrc\n\n\n\n\nuser01--.bash_profile\n\n\n\n\nuser01--thesis\n\n\n\n\n/--bin\n\n\n\n\n/--boot\n\n\n\n\n/--home\n\n\n\n\nhome--user01\n\n\n\n\nhome--user02\n\n\n\n\nhome--user03\n\n\n\n\nthesis--data\n\n\n\n\nthesis--ms\n\n\n\n\nthesis--results\n\n\n\n\nthesis--src\n\n\n\n\nresults--graph1\n\n\n\n\nresults--graph2\n\n\n\n\nsrc--script1\n\n\n\n\n.bashrc\n.bashrc\n\n\n\n.bash_profile\n.bash_profile\n\n\n\ngraph1\ngraph1\n\n\n\ngraph2\ngraph2\n\n\n\nscript1\nscript1\n\n\n\nuser01\n\nuser01\n\n\n\n/\n\n/\n\n\n\nbin\n\nbin\n\n\n\nboot\n\nboot\n\n\n\nhome\n\nhome\n\n\n\nuser02\n\nuser02\n\n\n\nuser03\n\nuser03\n\n\n\nthesis\n\nthesis\n\n\n\ndata\n\ndata\n\n\n\nms\n\nms\n\n\n\nresults\n\nresults\n\n\n\nsrc\n\nsrc",
    "crumbs": [
      "Bash",
      "<b><em>Getting started</em></b>",
      "The Unix filesystem"
    ]
  },
  {
    "objectID": "bash/intro_filesystem.html#creating-files-and-directories",
    "href": "bash/intro_filesystem.html#creating-files-and-directories",
    "title": "The Unix filesystem",
    "section": "Creating files and directories",
    "text": "Creating files and directories\nFiles can be created with a text editor:\nnano newfile.txt\n\nThis opens the text editor “nano” with a blank file. The file actually gets created when you save it from within the text editor.\n\nor with the command touch:\ntouch newfile.txt\n\nThis creates an empty file.\n\ntouch can create multiple files at once:\ntouch file1 file2 file3\nNew directories can be created with mkdir. This command can also accept multiple arguments to create multiple directories at once:\nmkdir dir1 dir2",
    "crumbs": [
      "Bash",
      "<b><em>Getting started</em></b>",
      "The Unix filesystem"
    ]
  },
  {
    "objectID": "bash/intro_filesystem.html#deleting",
    "href": "bash/intro_filesystem.html#deleting",
    "title": "The Unix filesystem",
    "section": "Deleting",
    "text": "Deleting\nFiles can be deleted with the command rm followed by their paths:\nrm file1 file2\nDirectories can be deleted with rm -r (“recursive”) followed by their paths or—if they are empty—with rmdir:\nrm -r dir1\nrmdir dir2   # only works if dir2 is empty\nBe careful that these commands are irreversible. By default, there is no trash in Linux systems.",
    "crumbs": [
      "Bash",
      "<b><em>Getting started</em></b>",
      "The Unix filesystem"
    ]
  },
  {
    "objectID": "bash/intro_filesystem.html#copying-moving-and-renaming",
    "href": "bash/intro_filesystem.html#copying-moving-and-renaming",
    "title": "The Unix filesystem",
    "section": "Copying, moving, and renaming",
    "text": "Copying, moving, and renaming\nCopying is done with the cp command:\ncp thesis/src/script1 thesis/ms\nMoving and renaming are both done with the mv command:\n# rename script1 to script\nmv thesis/src/script1 thesis/src/script\n\n# move graph1 to the ms directory\nmv thesis/results/graph1 thesis/ms\n# this also works:\n# mv thesis/results/graph1 thesis/ms/graph1\n\n\nYour turn:\n\nWhy is there only one command to move and rename?",
    "crumbs": [
      "Bash",
      "<b><em>Getting started</em></b>",
      "The Unix filesystem"
    ]
  },
  {
    "objectID": "bash/intro_functions.html",
    "href": "bash/intro_functions.html",
    "title": "Functions",
    "section": "",
    "text": "As in programming language, Bash functions are blocks of code that can be accessed by their names.",
    "crumbs": [
      "Bash",
      "<b><em>Getting started</em></b>",
      "Functions"
    ]
  },
  {
    "objectID": "bash/intro_functions.html#function-definition",
    "href": "bash/intro_functions.html#function-definition",
    "title": "Functions",
    "section": "Function definition",
    "text": "Function definition\n\nSyntax\nYou define a new function with the following syntax:\nname() {\n    command1\n    command2\n    ...\n}\n\n\nExample\ngreetings() {\n  echo hello\n}",
    "crumbs": [
      "Bash",
      "<b><em>Getting started</em></b>",
      "Functions"
    ]
  },
  {
    "objectID": "bash/intro_functions.html#storing-functions",
    "href": "bash/intro_functions.html#storing-functions",
    "title": "Functions",
    "section": "Storing functions",
    "text": "Storing functions\nYou can define a new function directly in the terminal. Such function would however only be available during your current session. Since functions contain code that is intended to be run repeatedly, it makes sense to store function definitions in a file. Before functions become available, the file needs to be sourced (e.g. source file.sh).\nA convenient file is ~/.bashrc. The file is automatically sourced every time you start a shell so your functions will always be defined and ready for use.",
    "crumbs": [
      "Bash",
      "<b><em>Getting started</em></b>",
      "Functions"
    ]
  },
  {
    "objectID": "bash/intro_functions.html#example-1",
    "href": "bash/intro_functions.html#example-1",
    "title": "Functions",
    "section": "Example",
    "text": "Example\nLet’s write a function called combine that takes all the files we pass to it, copies them into a randomly named directory, and prints that directory to the terminal:\ncombine() {\n  if [ $# -eq 0 ]; then\n    echo \"No arguments specified. Usage: combine file1 [file2 ...]\"\n    return 1                # Return a non-zero error code\n  fi\n  dir=$RANDOM$RANDOM\n  mkdir $dir\n  cp $@ $dir\n  echo look in the directory $dir\n}\n\n\nYour turn:\n\nWrite a function to swap two file names.\nAdd a check that both files exist before renaming them.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere is a video of a previous version of this workshop.",
    "crumbs": [
      "Bash",
      "<b><em>Getting started</em></b>",
      "Functions"
    ]
  },
  {
    "objectID": "bash/intro_redirections.html",
    "href": "bash/intro_redirections.html",
    "title": "Redirections & pipes",
    "section": "",
    "text": "By default, commands that produce an output print it to the terminal. This output can however be redirected to be printed elsewhere (e.g. to a file) or to be passed as the argument of another command."
  },
  {
    "objectID": "bash/intro_redirections.html#redirections",
    "href": "bash/intro_redirections.html#redirections",
    "title": "Redirections & pipes",
    "section": "Redirections",
    "text": "Redirections\nBy default, commands that produce an output print it to standard output—that is, the terminal. This is what we have been doing so far.\nThe output can however be redirected with the &gt; sign. For instance, it can be redirected to a file, which is very handy if you want to save the result.\n\nExample:\n\nLet’s print the number of lines in each .pdb file in the molecules directory:\n\nwc -l *.pdb\n\nwc: '*.pdb': No such file or directory\n\n\n\n\nYour turn:\n\n\nWhat does the wc command do?\nWhat does the -l flag for this command do?\nHow did you find out?\n\n\nTo save this result into a file called lengths.txt, we run:\n\nwc -l *.pdb &gt; lengths.txt\n\nwc: '*.pdb': No such file or directory\n\n\n\nNote that &gt; always creates a new file. If a file called lengths.txt already exists, it will be overwritten. Be careful not to lose data this way!\nIf you don’t want to lose the content of the old file, you can append the output to the existing file with &gt;&gt; (&gt;&gt; will create a file lengths.txt if it doesn’t exist yet, but if it exists, it will append the new content below the old one).\n\n\n\nYour turn:\n\nHow can you make sure that you did create a file called lengths.txt?\n\nLet’s print its content to the terminal:\n\ncat lengths.txt\n\nAs you can see, it contains the output of the command wc -l *.pdb.\nOf course, we can print the content of the file with modification. For instance, we can sort it:\n\nsort -n lengths.txt\n\nAnd we can redirect this new output to a new file:\n\nsort -n lengths.txt &gt; sorted.txt\n\nInstead of printing an entire file to the terminal, you can print only part of it.\nLet’s print the first line of the new file sorted.txt:\n\nhead -1 sorted.txt"
  },
  {
    "objectID": "bash/intro_redirections.html#pipes",
    "href": "bash/intro_redirections.html#pipes",
    "title": "Redirections & pipes",
    "section": "Pipes",
    "text": "Pipes\nAnother form of redirection is the Bash pipe. Instead of redirecting the output to a different stream for printing, the output is passed as an argument to another command. This is very convenient because it allows to chain multiple commands without having to create files or variables to save intermediate results.\nFor instance, we could run the three commands we ran previously at once, without the creation of the two intermediate files:\n\nwc -l *.pdb | sort -n | head -1\n\nwc: '*.pdb': No such file or directory\n\n\nIn each case, the output of the command on the left-hand side (LHS) is passed as the input of the command on the right-hand side (RHS).\n\n\nYour turn:\n\nIn a directory we want to find the 3 files that have the least number of lines. Which command would work for this?\n\nwc -l * &gt; sort -n &gt; head -3\nwc -l * | sort -n | head 1-3\nwc -l * | sort -n | head -3\nwc -l * | head -3 | sort -n\n\n\nHere is a video of a previous version of this workshop."
  },
  {
    "objectID": "bash/intro_script.html",
    "href": "bash/intro_script.html",
    "title": "Writing scripts",
    "section": "",
    "text": "There are series of commands that you need to run regularly. Instead of having to type them each time, you can write them in a text file (called a script) with a .sh extension and execute that file whenever you want to run that set of commands. This is a great way to automate work.\nThis section covers scripts syntax and execution."
  },
  {
    "objectID": "bash/intro_script.html#writing-and-executing-scripts",
    "href": "bash/intro_script.html#writing-and-executing-scripts",
    "title": "Writing scripts",
    "section": "Writing and executing scripts",
    "text": "Writing and executing scripts\n\nScripts as arguments to bash\nA shell script is simply a text file. You can create it with a text editor such as nano which is installed on most systems.\nLet’s try to create one that we will call test.sh:\nnano test.sh\nIn the file, write the command: echo This is my first script.\nThis is the content of our test.sh file:\n\n\ntest.sh\n\necho This is my first script\n\nNow, how do we run this?\nWe simply pass it as an argument to the bash command:\nbash test.sh\nThis is my first script\nAnd it worked!\n\n\nShebang\nThere is another way to write and execute scripts: we can use a shebang.\nA shebang consists of the characters #! followed by the path of an executable. Here, the executable we want is bash and its path is /bin/bash.\nSo our script becomes:\n\n\ntest.sh\n\n#!/bin/bash\n\necho This is my first script.\n\nNow, the cool thing about this is that we don’t need to pass the script as an argument of the bash command anymore since the information that this should be executed by Bash is already written in the shebang. Instead, we can execute it with ./test.sh.\nBut there is a little twist:\n./test.sh\nbash: ./test.sh: Permission denied\nWe first need to make the file executable by changing its permissions.\n\n\nUnix permissions\nUnix systems such as Linux use POSIX permissions.\nTo add an executable permission to a file, you need to run:\nchmod u+x test.sh\nNow that our script is executable, we can run:\n./test.sh\nThis is my first script\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere and here are two videos of a previous version of this workshop."
  },
  {
    "objectID": "bash/intro_script.html#using-other-computing-languages-in-bash",
    "href": "bash/intro_script.html#using-other-computing-languages-in-bash",
    "title": "Writing scripts",
    "section": "Using other computing languages in Bash",
    "text": "Using other computing languages in Bash\nIt is possible to incorporate scripts written in other computing languages into your bash code.\n\nExample:\n\nfunction test() {\n    randomFile=${RANDOM}${RANDOM}.py\n    cat &lt;&lt; EOF &gt; $randomFile\n#!/usr/bin/python3\nprint(\"do something in Python\")\nEOF\n    chmod u+x $randomFile\n    ./$randomFile\n    /bin/rm $randomFile\n}\n\nEOF is a random delimiter string and &lt;&lt; tells Bash to wait for that delimiter to end the input.\nHere is an example of this syntax:\ncat &lt;&lt; the_end\nThis text\nwill be printed\nin the terminal.\nthe_end"
  },
  {
    "objectID": "bash/intro_transfer.html",
    "href": "bash/intro_transfer.html",
    "title": "Transferring files",
    "section": "",
    "text": "If you want to use the Alliance clusters to run some of your heavy computations, you will have to move files back and forth between your machine and the clusters.\nThis section covers various ways to do this.",
    "crumbs": [
      "Bash",
      "<b><em>Getting started</em></b>",
      "Transferring files"
    ]
  },
  {
    "objectID": "bash/intro_transfer.html#remote-copies-with-scp",
    "href": "bash/intro_transfer.html#remote-copies-with-scp",
    "title": "Transferring files",
    "section": "Remote copies with scp",
    "text": "Remote copies with scp\nSecure copy protocol (SCP) allows to copy files over the Secure Shell Protocol (SSH) with the scp utility. scp follows a syntax similar to that of the cp command.\nNote that you need to run it from your local machines (not from the cluster).\n\nCopy from your machine to the cluster\n# Copy a local file to your home directory on the cluster\nscp /local/path/file userxx@spring.c3.ca:\n# Copy a local file to some path on the cluster\nscp /local/path/file userxx@spring.c3.ca:/remote/path\n\n\nCopy from the cluster to your machine\n# Copy a file from the cluster to some path on your machine\nscp userxx@spring.c3.ca:/remote/path/file /local/path\n# Copy a file from the cluster to your current location on your machine\nscp userxx@spring.c3.ca:/remote/path/file .\nYou can also use wildcards to transfer multiple files:\n# Copy all the Bash scripts from your cluster home dir to some local path\nscp userxx@spring.c3.ca:*.sh /local/path\n\n\nCopying directories\nTo copy a directory, you need to add the -r (recursive) flag:\nscp -r /local/path/folder userxx@spring.c3.ca:/remote/path\n\n\nCopying for Windows users\nMobaXterm users (on Windows) can copy files by dragging them between the local and remote machines in the GUI. Alternatively, they can use the download and upload buttons.\n\n\nYour turn:\n\nCopy a file from your local computer to your home directory in the training cluster.\n\nHere is a video of a previous version of this workshop.",
    "crumbs": [
      "Bash",
      "<b><em>Getting started</em></b>",
      "Transferring files"
    ]
  },
  {
    "objectID": "bash/intro_transfer.html#interactive-transfers-with-sftp",
    "href": "bash/intro_transfer.html#interactive-transfers-with-sftp",
    "title": "Transferring files",
    "section": "Interactive transfers with sftp",
    "text": "Interactive transfers with sftp\nThe Secure File Transfer Protocol (SFTP) is more sophisticated and allows additional operations. The sftp command provided by OpenSSH and other packages launches an SFTP client:\nsftp userxx@spring.c3.ca\n\nLook at your prompt: your usual Bash/Zsh prompt has been replaced with sftp&gt;.\n\nFrom this prompt, you can access a number of SFTP commands. Type help for a list:\nsftp&gt; help\nAvailable commands:\nbye                                Quit sftp\ncd path                            Change remote directory to 'path'\nchgrp [-h] grp path                Change group of file 'path' to 'grp'\nchmod [-h] mode path               Change permissions of file 'path' to 'mode'\nchown [-h] own path                Change owner of file 'path' to 'own'\ncopy oldpath newpath               Copy remote file\ncp oldpath newpath                 Copy remote file\ndf [-hi] [path]                    Display statistics for current directory or\n                                   filesystem containing 'path'\nexit                               Quit sftp\nget [-afpR] remote [local]         Download file\nhelp                               Display this help text\nlcd path                           Change local directory to 'path'\nlls [ls-options [path]]            Display local directory listing\nlmkdir path                        Create local directory\nln [-s] oldpath newpath            Link remote file (-s for symlink)\nlpwd                               Print local working directory\nls [-1afhlnrSt] [path]             Display remote directory listing\nlumask umask                       Set local umask to 'umask'\nmkdir path                         Create remote directory\nprogress                           Toggle display of progress meter\nput [-afpR] local [remote]         Upload file\npwd                                Display remote working directory\nquit                               Quit sftp\nreget [-fpR] remote [local]        Resume download file\nrename oldpath newpath             Rename remote file\nreput [-fpR] local [remote]        Resume upload file\nrm path                            Delete remote file\nrmdir path                         Remove remote directory\nsymlink oldpath newpath            Symlink remote file\nversion                            Show SFTP version\n!command                           Execute 'command' in local shell\n!                                  Escape to local shell\n?                                  Synonym for help\nAs this list shows, you have access to a number of classic Unix command such as cd, pwd, ls, etc. These commands will be executed on the remote machine.\nIn addition, there are a number of commands of the form l&lt;command&gt;. “l” stands for “local”.\nThese commands will be executed on your local machine.\nFor instance, ls will list the files in your current directory in the remote machine while lls (“local ls”) will list the files in your current directory on your computer.\nThis means that you are now able to navigate two file systems at once: your local machine and the remote machine.\n\nHere are a few examples:\n\nsftp&gt; pwd              # print remote working directory\nsftp&gt; lpwd             # print local working directory\nsftp&gt; ls               # list files in remote working directory\nsftp&gt; lls              # list files in local working directory\nsftp&gt; cd               # change the remote directory\nsftp&gt; lcd              # change the local directory\nsftp&gt; put local_file   # upload a file\nsftp&gt; get remote_file  # download a file\n\nCopying directories\nTo upload/download directories, you first need to create them in the destination, then copy the content with the -r (recursive) flag.\n\nIf you have a local directory called dir and you want to copy it to the cluster you need to run:\n\nsftp&gt; mkdir dir    # First create the directory\nsftp&gt; put -r dir   # Then copy the content\nTo terminate the session, press &lt;Ctrl+D&gt;.\n\n\nYour turn:\n\nIn an SFTP session:\n\nList the content of projects (projects is in your home on the training cluster).\nCopy a file from the training cluster to your local computer.",
    "crumbs": [
      "Bash",
      "<b><em>Getting started</em></b>",
      "Transferring files"
    ]
  },
  {
    "objectID": "bash/intro_transfer.html#syncing",
    "href": "bash/intro_transfer.html#syncing",
    "title": "Transferring files",
    "section": "Syncing",
    "text": "Syncing\nIf, instead of an occasional copying of files between your machine and the cluster, you want to keep a directory in sync between both machines, you might want to use rsync instead. You can look at the Alliance wiki page on rsync for complete instructions.",
    "crumbs": [
      "Bash",
      "<b><em>Getting started</em></b>",
      "Transferring files"
    ]
  },
  {
    "objectID": "bash/intro_transfer.html#heavy-transfers",
    "href": "bash/intro_transfer.html#heavy-transfers",
    "title": "Transferring files",
    "section": "Heavy transfers",
    "text": "Heavy transfers\nWhile the methods covered above work very well for limited amounts of data, if you need to make large transfers, you should use globus instead, following the instructions in the Alliance wiki page on this service.",
    "crumbs": [
      "Bash",
      "<b><em>Getting started</em></b>",
      "Transferring files"
    ]
  },
  {
    "objectID": "bash/intro_transfer.html#windows-line-endings",
    "href": "bash/intro_transfer.html#windows-line-endings",
    "title": "Transferring files",
    "section": "Windows line endings",
    "text": "Windows line endings\nOn modern Mac operating systems and on Linux, lines in files are terminated with a newline (\\n). On Windows, they are terminated with a carriage return + newline (\\r\\n).\nWhen you transfer files between Windows and Linux (the cluster uses Linux), this creates a mismatch. Most modern software handle this correctly, but you may occasionally run into problems.\nThe solution is to convert a file from Windows encoding to Unix encoding with:\ndos2unix file\nTo convert a file back to Windows encoding, run:\nunix2dos file",
    "crumbs": [
      "Bash",
      "<b><em>Getting started</em></b>",
      "Transferring files"
    ]
  },
  {
    "objectID": "bash/intro_wildcards.html",
    "href": "bash/intro_wildcards.html",
    "title": "Wildcards",
    "section": "",
    "text": "Wildcards are a convenient way to select items matching patterns.\n\n\n\n\n\n\n\nData for this section\n\n\n\n\n\nFor this section, we will play with files created by The Carpentries.\nYou can download them into a zip file called bash.zip with:\nwget https://bit.ly/bashfile -O bash.zip\nYou can then unzip that file with:\nunzip bash.zip\nYou should now have a data-shell directory with a molecules subdirectory.\ncd into it:\ncd data-shell/molecules\n\n\n\nLet’s list the files in the molecules directory:\nls\ncubane.pdb  ethane.pdb  methane.pdb  octane.pdb  pentane.pdb  propane.pdb\nYou could do the same with:\nls *\nThe star expands to all files/directories matching any pattern. It is a wildcard.\nOf course, you can match more interesting patterns.\nFor instance, to list all files starting with the letter o, we can run:\nls o*\noctane.pdb\nTo list all files containing the letter o anywhere in their name, you can use:\nls *o*\noctane.pdb  propane.pdb\nThis saves a lot of typing and is a powerful way to apply a command to a subset of files/directories.\n\n\nYour turn:\n\nWildcards are often used to select all files with a certain extension.\nLet’s create 3 new files:\ntouch file1.txt file2.txt file3.md\nHow would you list all files with the .txt extension and only those?"
  },
  {
    "objectID": "bash/molecules/intro_find.html#data-for-this-section",
    "href": "bash/molecules/intro_find.html#data-for-this-section",
    "title": "Finding files",
    "section": "Data for this section",
    "text": "Data for this section\nFor this section, we will play with files created by The Carpentries.\nYou can download them into a zip file called bash.zip with:\nwget http://bit.ly/bashfile -O bash.zip\nYou can then unzip that file with:\nunzip bash.zip\nFinally, you can delete the zip file:\nrm bash.zip\nYou should now have a data-shell directory with a molecules subdirectory.\ncd into it:\ncd data-shell/molecules"
  },
  {
    "objectID": "bash/molecules/intro_find.html#command-find",
    "href": "bash/molecules/intro_find.html#command-find",
    "title": "Finding files",
    "section": "Command find",
    "text": "Command find\nSearch for files inside the current working directory:\nfind . -type f\n./methane.pdb\n./pentane.pdb\n./sorted.txt\n./propane.pdb\n./lengths.txt\n./cubane.pdb\n./ethane.pdb\n./octane.pdb\nfind . -type d will instead search for directories inside the current working directory.\nHere are other examples:\nfind . -maxdepth 1 -type f     # depth 1 is the current directory\nfind . -mindepth 2 -type f     # current directory and one level down\nfind . -name haiku.txt      # finds specific file\nls data       # shows one.txt two.txt\nfind . -name *.txt      # still finds one file -- why? answer: expands *.txt to haiku.txt\nfind . -name '*.txt'    # finds all three files -- good!\nLet’s wrap the last command into $()—called command substitution—as if it were a variable:\necho $(find . -name '*.txt')   # will print ./data/one.txt ./data/two.txt ./haiku.txt\nls -l $(find . -name '*.txt')   # will expand to ls -l ./data/one.txt ./data/two.txt ./haiku.txt\nwc -l $(find . -name '*.txt')   # will expand to wc -l ./data/one.txt ./data/two.txt ./haiku.txt\ngrep elegant $(find . -name '*.txt')   # will look for 'elegant' inside all *.txt files\n\n\nYour turn:\n\ngrep’s -v flag inverts pattern matching, so that only lines that do not match the pattern are printed.\nGiven that, which of the following commands will find all files in /data whose names end in ose.dat (e.g. sucrose.dat or maltose.dat), but do not contain the word temp?\n\nfind /data -name '*.dat' | grep ose | grep -v temp\nfind /data -name ose.dat | grep -v temp\ngrep -v temp $(find /data -name '*ose.dat')\nNone of the above\n\n\nHere is a video of a previous version of this workshop."
  },
  {
    "objectID": "bash/molecules/intro_find.html#running-a-command-on-the-results-of-find",
    "href": "bash/molecules/intro_find.html#running-a-command-on-the-results-of-find",
    "title": "Finding files",
    "section": "Running a command on the results of find",
    "text": "Running a command on the results of find\nLet’s say that you want to run a command on each of the files in the output of find. You can always do something using command substitution like this:\nfor f in $(find . -name \"*.txt\")\ndo\n    command on $f\ndone\nAlternatively, you can make it a one-liner:\nfind . -name \"*.txt\" -exec command {} \\;\nAnother—perhaps more elegant—one-line alternative is to use xargs. In its simplest usage, xargs command lets you construct a list of arguments:\nfind . -name \"*.txt\"                   # returns multiple lines\nfind . -name \"*.txt\" | xargs           # use those lines to construct a list\nfind . -name \"*.txt\" | xargs command   # pass this list as arguments to `command`\ncommand $(find . -name \"*.txt\")        # command substitution, achieving the same result (this is riskier!)\ncommand `(find . -name \"*.txt\")`       # alternative syntax for command substitution\nIn these examples, xargs achieves the same result as command substitution, but it is safer in terms of memory usage and the length of lists you can pass.\nWhen would you need to use this? A good example is with the command grep. grep takes a search stream (and not a list of files) as its standard input:\ncat filename | grep pattern\nTo pass a list of files to grep, you can use xargs that takes that list from its standard input and converts it into a list of arguments that is then passed to grep:\nfind . -name \"*.txt\" | xargs grep pattern   # search for `pattern` inside all those files (`grep` does not take a list of files as standard input)\n\n\nHere is a video of a previous version of this workshop."
  },
  {
    "objectID": "bash/molecules/intro_script.html",
    "href": "bash/molecules/intro_script.html",
    "title": "Writing scripts",
    "section": "",
    "text": "There are series of commands that you need to run regularly. Instead of having to type them each time, you can write them in a text file (called a script) with a .sh extension and execute that file whenever you want to run that set of commands. This is a great way to automate work.\nThis section covers scripts syntax and execution.",
    "crumbs": [
      "Bash",
      "<b><em>Getting started</em></b>",
      "Writing scripts"
    ]
  },
  {
    "objectID": "bash/molecules/intro_script.html#writing-and-executing-scripts",
    "href": "bash/molecules/intro_script.html#writing-and-executing-scripts",
    "title": "Writing scripts",
    "section": "Writing and executing scripts",
    "text": "Writing and executing scripts\n\nScripts as arguments to bash\nA shell script is simply a text file. You can create it with a text editor such as nano which is installed on most systems.\nLet’s try to create one that we will call test.sh:\nnano test.sh\nIn the file, write the command: echo This is my first script.\nThis is the content of our test.sh file:\n\n\ntest.sh\n\necho This is my first script\n\nNow, how do we run this?\nWe simply pass it as an argument to the bash command:\nbash test.sh\nThis is my first script\nAnd it worked!\n\n\nShebang\nThere is another way to write and execute scripts: we can use a shebang.\nA shebang consists of the characters #! followed by the path of an executable. Here, the executable we want is bash and its path is /bin/bash.\nSo our script becomes:\n\n\ntest.sh\n\n#!/bin/bash\n\necho This is my first script.\n\nNow, the cool thing about this is that we don’t need to pass the script as an argument of the bash command anymore since the information that this should be executed by Bash is already written in the shebang. Instead, we can execute it with ./test.sh.\nBut there is a little twist:\n./test.sh\nbash: ./test.sh: Permission denied\nWe first need to make the file executable by changing its permissions.\n\n\nUnix permissions\nUnix systems such as Linux use POSIX permissions.\nTo add an executable permission to a file, you need to run:\nchmod u+x test.sh\nNow that our script is executable, we can run:\n./test.sh\nThis is my first script\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere and here are two videos of a previous version of this workshop.",
    "crumbs": [
      "Bash",
      "<b><em>Getting started</em></b>",
      "Writing scripts"
    ]
  },
  {
    "objectID": "bash/molecules/intro_script.html#using-other-computing-languages-in-bash",
    "href": "bash/molecules/intro_script.html#using-other-computing-languages-in-bash",
    "title": "Writing scripts",
    "section": "Using other computing languages in Bash",
    "text": "Using other computing languages in Bash\nIt is possible to incorporate scripts written in other computing languages into your bash code.\n\nExample:\n\nfunction test() {\n    randomFile=${RANDOM}${RANDOM}.py\n    cat &lt;&lt; EOF &gt; $randomFile\n#!/usr/bin/python3\nprint(\"do something in Python\")\nEOF\n    chmod u+x $randomFile\n    ./$randomFile\n    /bin/rm $randomFile\n}\n\nEOF is a random delimiter string and &lt;&lt; tells Bash to wait for that delimiter to end the input.\nHere is an example of this syntax:\ncat &lt;&lt; the_end\nThis text\nwill be printed\nin the terminal.\nthe_end",
    "crumbs": [
      "Bash",
      "<b><em>Getting started</em></b>",
      "Writing scripts"
    ]
  },
  {
    "objectID": "bash/top_intro.html",
    "href": "bash/top_intro.html",
    "title": "Getting started with Bash",
    "section": "",
    "text": "This course is a hands-on introduction to the Linux command line and the interaction with a remote server. It will cover common utilities, loops, redirections, functions, wild cards, Bash scripting, and the basics of working through secure shell.\n\n Start course ➤",
    "crumbs": [
      "Bash",
      "<b><em>Getting started</em></b>"
    ]
  },
  {
    "objectID": "bash/top_ws.html",
    "href": "bash/top_ws.html",
    "title": "Bash workshops",
    "section": "",
    "text": "Scripting for beginners\n\n\n\n\nSearching & manipulating text",
    "crumbs": [
      "Bash",
      "<b><em>Workshops</em></b>"
    ]
  },
  {
    "objectID": "bash/wb_tools2.html",
    "href": "bash/wb_tools2.html",
    "title": "A few more of our favourite tools",
    "section": "",
    "text": "In a previous webinar, we presented three of our favourite command line tools. Today, we will introduce other tools we find really useful in our daily workflow:\n\nlazygit: a wonderful terminal UI for Git,\nbat: a great syntax highlighter,\nripgrep: a fast alternative to grep,\nfd: a /really/ fast alternative to find,\npass: a command line password manager.\n\nAlong the way, I will use a few other neat command line tools such as hyperfine—for sophisticated benchmarking—and diff-so-fancy—which makes your diffs a lot more readable.\nFor the Emacs users among you, we will finish the workshop with two Emacs utilities:\n\nTRAMP: a remote file access system,\nHelm: a “framework for incremental completions and narrowing selections”.",
    "crumbs": [
      "Bash",
      "<b><em>Webinars</em></b>",
      "More fun tools for the CLI"
    ]
  },
  {
    "objectID": "bash/ws_text.html",
    "href": "bash/ws_text.html",
    "title": "Searching & manipulating text",
    "section": "",
    "text": "cd ~/Desktop/data-shell/writing\nmore haiku.txt\nFirst let’s search for text in files:\ngrep not haiku.txt     # let's find all lines that contain the word 'not'\ngrep day haiku.txt     # now search for word 'day'\ngrep -w day haiku.txt     # search for a separate word 'day' (not 'today', etc.)\ngrep -w today haiku.txt   # search for 'today'\ngrep -w Today haiku.txt   # search for 'Today'\ngrep -i -w today haiku.txt       # both upper and lower case 'today'\ngrep -n -i -w today haiku.txt    # -n prints out numbers the matching lines\ngrep -n -i -w -v the haiku.txt   # -v searches for lines that do not contain 'the'\nman grep\nMore than two arguments to grep:\ngrep pattern file1 file2 file3   # all argument after the first one are assumed to be filenames\ngrep pattern *.txt   # the last argument will expand to the list of *.txt files\n\nThe Tao that is seen\nIs not the true Tao, until\nYou bring fresh toner.\nWith searching comes loss\nand the presence of absence:\n\"My Thesis\" not found.\nYesterday it worked.\nToday it is not working.\nSoftware is like that.\nFrom the above text, contained in the file haiku.txt, which command would result in the following output:\nand the presence of absence:\n\ngrep of haiku.txt\ngrep -E of haiku.txt\ngrep -w of haiku.txt \n\nHere is a video on this topic.",
    "crumbs": [
      "Bash",
      "<b><em>Workshops</em></b>",
      "Searching & manipulating text"
    ]
  },
  {
    "objectID": "bash/ws_text.html#searching-inside-files-with-grep",
    "href": "bash/ws_text.html#searching-inside-files-with-grep",
    "title": "Searching & manipulating text",
    "section": "",
    "text": "cd ~/Desktop/data-shell/writing\nmore haiku.txt\nFirst let’s search for text in files:\ngrep not haiku.txt     # let's find all lines that contain the word 'not'\ngrep day haiku.txt     # now search for word 'day'\ngrep -w day haiku.txt     # search for a separate word 'day' (not 'today', etc.)\ngrep -w today haiku.txt   # search for 'today'\ngrep -w Today haiku.txt   # search for 'Today'\ngrep -i -w today haiku.txt       # both upper and lower case 'today'\ngrep -n -i -w today haiku.txt    # -n prints out numbers the matching lines\ngrep -n -i -w -v the haiku.txt   # -v searches for lines that do not contain 'the'\nman grep\nMore than two arguments to grep:\ngrep pattern file1 file2 file3   # all argument after the first one are assumed to be filenames\ngrep pattern *.txt   # the last argument will expand to the list of *.txt files\n\nThe Tao that is seen\nIs not the true Tao, until\nYou bring fresh toner.\nWith searching comes loss\nand the presence of absence:\n\"My Thesis\" not found.\nYesterday it worked.\nToday it is not working.\nSoftware is like that.\nFrom the above text, contained in the file haiku.txt, which command would result in the following output:\nand the presence of absence:\n\ngrep of haiku.txt\ngrep -E of haiku.txt\ngrep -w of haiku.txt \n\nHere is a video on this topic.",
    "crumbs": [
      "Bash",
      "<b><em>Workshops</em></b>",
      "Searching & manipulating text"
    ]
  },
  {
    "objectID": "bash/ws_text.html#text-manipulation",
    "href": "bash/ws_text.html#text-manipulation",
    "title": "Searching & manipulating text",
    "section": "Text manipulation",
    "text": "Text manipulation\n(This example was kindly provided by John Simpson.)\nIn this section we’ll use two tools for text manipulation: sed and tr. Our goal is to calculate the frequency of all dictionary words in the novel “The Invisible Man” by Herbert Wells (public domain). First, let’s apply our knowledge of grep to this text:\n$ cd ~/Desktop/data-shell\n$ ls   # shows wellsInvisibleMan.txt\n$ wc wellsInvisibleMan.txt                          # number of lines, words, characters\n$ grep invisible wellsInvisibleMan.txt              # see the invisible man\n$ grep invisible wellsInvisibleMan.txt | wc -l      # returns 60; adding -w gives the same count\n$ grep -i invisible wellsInvisibleMan.txt | wc -l   # returns 176 (includes: invisible Invisible INVISIBLE)\nLet’s sidetrack for a second and see how we can use the “stream editor” sed:\n$ sed 's/[iI]nvisible/supervisible/g' wellsInvisibleMan.txt &gt; visibleMan.txt   # make him visible\n$ cat wellsInvisibleMan.txt | sed 's/[iI]nvisible/supervisible/g' &gt; visibleMan.txt   # this also works (standard input)\n$ grep supervisible visibleMan.txt   # see what happened to the now visible man\n$ grep -i invisible visibleMan.txt   # see what was not converted\n$ man sed\nNow let’s remove punctuation from the original file using “tr” (translate) command:\n$ cat wellsInvisibleMan.txt | tr -d \"[:punct:]\" &gt; invisibleNoPunct.txt    # tr only takes standard input\n$ tail wellsInvisibleMan.txt\n$ tail invisibleNoPunct.txt\nNext convert all upper case to lower case:\n$ cat invisibleNoPunct.txt | tr '[:upper:]' '[:lower:]' &gt; invisibleClean.txt\n$ tail invisibleClean.txt\nNext replace spaces with new lines:\n$ cat invisibleClean.txt | sed 's/ /\\'$'\\n/g' &gt; invisibleList.txt   # \\'$'\\n is a shortcut for a new line\n$ more invisibleList.txt\nNext remove empty lines:\n$ sed '/^$/d' invisibleList.txt  &gt; invisibleCompact.txt\nNext sort the list alphabetically, count each word’s occurrence, and remove duplicate words:\n$ cat invisibleCompact.txt | sort | uniq -c &gt; invisibleWords.txt\n$ more invisibleWords.txt\nNext sort the list into most frequent words:\n$ cat invisibleWords.txt | sort -gr &gt; invisibleFrequencyList.txt   # use 'man sort'\n$ more invisibleFrequencyList.txt\n\n\n\n\n\n\nYou can watch a video for this topic after the workshop.\nQuick reference:\nsed 's/pattern1/pattern2/' filename    # replace pattern1 with pattern2, one per line\nsed 's/pattern1/pattern2/g' filename   # same but multiple per line\nsed 's|pattern1|pattern2|g' filename   # same\n\ncat wellsInvisibleMan.txt | tr -d \"[:punct:]\" &gt; invisibleNoPunct.txt       # remove punctuation; tr only takes standard input\ncat invisibleNoPunct.txt | tr '[:upper:]' '[:lower:]' &gt; invisibleClean.txt # convert all upper case to lower case:\ncat invisibleClean.txt | sed 's/ /\\'$'\\n/g' &gt; invisibleList.txt            # replace spaces with new lines;\n                                                                           # \\'$'\\n is a shortcut for a new line\nsed '/^$/d' invisibleList.txt  &gt; invisibleCompact.txt   # remove empty lines\ncat invisibleCompact.txt | sort | uniq -c &gt; invisibleWords.txt   # sort the list alphabetically, count each word's occurrence\ncat invisibleWords.txt | sort -gr &gt; invisibleFrequencyList.txt   # sort the list into most frequent words\n\nWrite a script that takes an English-language file and print the list of its 100 most common words, along with the word count. Hint: use the workflow from the text manipulation video. Finally, convert this script into a bash function. (no need to type any answer)",
    "crumbs": [
      "Bash",
      "<b><em>Workshops</em></b>",
      "Searching & manipulating text"
    ]
  },
  {
    "objectID": "bash/ws_text.html#column-based-text-processing-with-awk-scripting-language",
    "href": "bash/ws_text.html#column-based-text-processing-with-awk-scripting-language",
    "title": "Searching & manipulating text",
    "section": "Column-based text processing with awk scripting language",
    "text": "Column-based text processing with awk scripting language\ncd .../data-shell/writing\ncat haiku.txt   # 11 lines\nYou can define inline awk scripts with braces surrounded by single quotation:\nawk '{print $1}' haiku.txt       # $1 is the first field (word) in each line =&gt; processing columns\nawk '{print $0}' haiku.txt       # $0 is the whole line\nawk '{print}' haiku.txt          # the whole line is the default action\nawk -Fa '{print $1}' haiku.txt   # can specify another separator with -F (\"a\" in this case)\nYou can use multiple commands inside your awk script:\necho Hello Tom &gt; hello.txt\necho Hello John &gt;&gt; hello.txt\nawk '{$2=\"Adam\"; print $0}' hello.txt   # we replaced the second word in each line with \"Adam\"\nMost common awk usage is to postprocess output of other commands:\n/bin/ps aux    # display all running processes as multi-column output\n/bin/ps aux | awk '{print $2 \" \" $11}'   # print only the process number and the command\nAwk also takes patterns in addition to scripts:\nawk '/Yesterday|Today/' haiku.txt   # print the lines that contain the words Yesterday or Today\nAnd then you act on these patterns: if the pattern evaluates to True, then run the script:\nawk '/Yesterday|Today/{print $3}' haiku.txt\nawk '/Yesterday|Today/' haiku.txt | awk '{print $3}'   # same as previous line\nAwk has a number of built-in variables; the most commonly used is NR:\nawk 'NR&gt;1' haiku.txt    # if NumberRecord &gt;1 then print it (default action), i.e. skip the first line\nawk 'NR&gt;1{print $0}' haiku.txt   # last command expanded\nawk 'NR&gt;1 && NR &lt; 5' haiku.txt   # print lines 2-4\n\nExercise: write a awk script to process cities.csv to print only town/city names and their population and store it in a separate file populations.csv. Try to do everything in a single-line command.\n\nQuick reference:\nls -l | awk 'NR&gt;3 {print $5 \"  \" $9}'   # print 5th and 9th columns starting with line 4\nawk 'NR&gt;1 && NR &lt; 5' haiku.txt          # print lines 2-4\nawk '/Yesterday|Today/' haiku.txt       # print lines that contain Yesterday or Today\n\nWrite a one-line command that finds 5 largest files in the current directory and prints only their names and file sizes in the human-readable format (indicating bytes, kB, MB, GB, …) in the decreasing file-size order. Hint: use find, xargs, and awk. \n\nLet’s study together these commands:\n$ source ~/projects/def-sponsor00/shared/fzf/.fzf.bash\n$ kill -9 `/bin/ps aux | fzf | awk '{print $2}'`\n\nHere is a video on this topic.",
    "crumbs": [
      "Bash",
      "<b><em>Workshops</em></b>",
      "Searching & manipulating text"
    ]
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact",
    "section": "",
    "text": "Please email us at: training at westdri dot ca."
  },
  {
    "objectID": "emacs/top_intro.html",
    "href": "emacs/top_intro.html",
    "title": "Getting started with Emacs",
    "section": "",
    "text": "Emacs is more than ever a very powerful text editor, with lots of recent exciting developments.\nGetting started can be daunting however and you might be thinking: “If I have to take a course just to be able to edit a text, surely this thing is crazy”.\nThis course intends to show you what makes Emacs such a fantastic tool and get you started in a smooth and gentle way. You will learn the basic concepts of Emacs, how to customize it, how to manage packages efficiently with the modern use-package approach, and basic concepts of Emacs Lisp.\n\n  Coming up in fall 2024.",
    "crumbs": [
      "Emacs",
      "<em><b>Getting started</b></em>"
    ]
  },
  {
    "objectID": "emacs/wb_emacs_ide.html",
    "href": "emacs/wb_emacs_ide.html",
    "title": "Emacs as a programming IDE",
    "section": "",
    "text": "Once upon a time (not that long ago), powerful text editors such as Vim and Emacs were the only nice interfaces to work with code. Nowadays, there are countless sleek and more GUI-oriented tools such as VS Code, RStudio, or JupyterLab that provide amazing IDEs, without the learning curve.\nSo why would one still use Emacs as a programming IDE?\nWhat does that even look like?\nIn this webinar, I will show some of the many reasons why I can’t let go of Emacs, then show how it can be used as a programming IDE for Python, R, and Julia.\n\nSlides (Click and wait: this reveal.js presentation may take a little time to load.)",
    "crumbs": [
      "Emacs",
      "<b><em>Webinars</em></b>",
      "Emacs as a programming IDE"
    ]
  },
  {
    "objectID": "emacs/wb_emacs_new_tools.html",
    "href": "emacs/wb_emacs_new_tools.html",
    "title": "Modern Emacs",
    "section": "",
    "text": "Emacs might have been created in the 70s, but development is alive and well:\n\n10 years ago version 24 brought huge speedups with lexical binding.\nIn 2022, version 28 added—among other things—just-in-time native compilation for elisp code for improved performance.\nVersion 29 last year brought countless exciting new additions such as official tree-sitter support and built-in Eglot and use-package.\nIn addition to Emacs itself, a profusion of modern packages have emerged over the past few years (e.g. the vertico/consult/orderless/marginalia/embark completion system; corfu and cape for at point completion) bringing great speed and sleekness to the user experience.\n\nAll these features are optional however and you need to learn about them to take advantage of their huge benefits. This webinar intends to get you started making Emacs really fast.\n\nSlides (Click and wait: this reveal.js presentation may take a little time to load.)",
    "crumbs": [
      "Emacs",
      "<b><em>Webinars</em></b>",
      "Modern Emacs"
    ]
  },
  {
    "objectID": "emacs/wb_emacs_polymode.html",
    "href": "emacs/wb_emacs_polymode.html",
    "title": "Emacs Polymode",
    "section": "",
    "text": "Polymode enables Emacs to activate different major modes in different parts of a buffer. Among other usages, this is key to literate programming: interweaving code blocks in, for instance, a markdown document, as you would do while creating R Markdown or Quarto documents.\nPolymode makes it easy to navigate between code blocks, select them, run them, etc. In this webinar, I will show this in action.\n\nComing up in fall 2024.",
    "crumbs": [
      "Emacs",
      "<b><em>Webinars</em></b>",
      "Emacs Polymode"
    ]
  },
  {
    "objectID": "git/index.html",
    "href": "git/index.html",
    "title": "Git",
    "section": "",
    "text": "Getting started with  \nA course in version control\n\n\n\n\nWorkshops\nVarious Git topics\n\n\n\n\n\n\n60 min webinars\nVarious Git topics",
    "crumbs": [
      "Git",
      "<br>&nbsp;<img src=\"img/logo_git.png\" class=\"img-fluid\" style=\"width:1.5em\" alt=\"noshadow\"><br><br>"
    ]
  },
  {
    "objectID": "git/intro_branches.html",
    "href": "git/intro_branches.html",
    "title": "Branches",
    "section": "",
    "text": "One of the reasons Git has become so popular is its branching system: unlike in other version control tools in which creating branches is a lengthy and expensive process involving heavy copies, a branch in Git is just a lightweight pointer to a commit. This makes creating branches extremely quick and cheap.",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Branches"
    ]
  },
  {
    "objectID": "git/intro_branches.html#what-is-a-branch",
    "href": "git/intro_branches.html#what-is-a-branch",
    "title": "Branches",
    "section": "What is a branch?",
    "text": "What is a branch?\nA branch is a pointer to a commit (under the hood, it is a small file containing the 40 character hash checksum of the commit it points to).\nRemember that little pointer called main? That’s our main branch: the one Git creates automatically when we create our first commit.\nWhen you run git status and get “On branch main” in the output, or when you run git log and see “(HEAD -&gt; main)” in the log, it means that the HEAD pointer (your position in the Git history) points to the branch main (which itself points to a commit).\n\nI know that is a lot of pointers … but this is really what makes Git so nimble, powerful, and fantastic. Because these pointers are very cheap (tiny files) and so useful.",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Branches"
    ]
  },
  {
    "objectID": "git/intro_branches.html#why-use-multiple-branches",
    "href": "git/intro_branches.html#why-use-multiple-branches",
    "title": "Branches",
    "section": "Why use multiple branches?",
    "text": "Why use multiple branches?\nBranches are useful in so many situations:\n\nIf your changes break code, you still have a fully functional branch to go back to if needed.\nIf you develop a tool being used, this allows you to experiment with new features until they are ready without messing up with the working project.\nYou can create a branch for each alternative approach. This allows you to jump back and forth between various alternatives.\nYou can work on different aspects of the project on different branches. This prevents having messy incomplete work all over the place on the same branch.\nIf you want to revisit an old commit, you can create a branch there and switch to it instead of moving HEAD (creating a detached HEAD situation). This way, if you decide to create new commits from that old one, you don’t risk loosing them.\nBranches are great for collaboration: each person can work on their own branch and merge it back to the main branch when they are done with one section of a project.\n\nAnd since branches are so cheap to create, there is no downside to their creation.",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Branches"
    ]
  },
  {
    "objectID": "git/intro_branches.html#creating-branches-and-switching-between-branches",
    "href": "git/intro_branches.html#creating-branches-and-switching-between-branches",
    "title": "Branches",
    "section": "Creating branches and switching between branches",
    "text": "Creating branches and switching between branches\nYou can create a new branch with:\ngit branch &lt;new-branch-name&gt;\n\nExample:\n\ngit branch test\n\nand you can then switch to it with:\ngit switch &lt;new-branch-name&gt;\n\nExample:\n\ngit switch test\n\nAlternatively, you can do both at once with the convenient:\ngit switch -c &lt;new-branch-name&gt;\n\n-c flag for “create”. So you create a branch and switch to it directly.\n\nI find this last command most useful as it is all too easy otherwise to create a new branch, forget to switch to it, and create commits on the wrong branch …",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Branches"
    ]
  },
  {
    "objectID": "git/intro_branches.html#listing-branches",
    "href": "git/intro_branches.html#listing-branches",
    "title": "Branches",
    "section": "Listing branches",
    "text": "Listing branches\ngit branch\n  main\n* test\nThe * shows the branch you are currently on (i.e. the branch to which HEAD points to). In our example, the project has two branches and we are on the branch test.",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Branches"
    ]
  },
  {
    "objectID": "git/intro_branches.html#comparing-branches",
    "href": "git/intro_branches.html#comparing-branches",
    "title": "Branches",
    "section": "Comparing branches",
    "text": "Comparing branches\nYou can use git diff to compare branches:\ngit diff main test\nThis shows all the lines that have been modified (added or deleted) between the commits both branches point to.",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Branches"
    ]
  },
  {
    "objectID": "git/intro_branches.html#merging-branches",
    "href": "git/intro_branches.html#merging-branches",
    "title": "Branches",
    "section": "Merging branches",
    "text": "Merging branches\nWhen you are happy with the changes you made on your test branch, you can merge it into main.\n\nFast-forward merge\nIf you have only created new commits on the branch test, the merge is called a “fast-forward merge” because main and test have not diverged: it is simply a question of having main catch up to test.\n\nFirst, you switch to main:\ngit switch main\n\nThen you do the fast-forward merge:\ngit merge test\n\nThen, usually, you delete the branch test as it has served its purpose:\ngit branch -d test\n\nAlternatively, you can switch back to test and do the next bit of experimental work on it. This allows to keep main free of mishaps and bad developments.\n\n\nThree-way merge\nIf the branches have diverged (you created commits on both branches), the merge requires the creation of an additional commit called a “merge commit”.\nLet’s go back to our situation before we created the branch test:\n\nThis time, you create a branch called test2:\n\nand you switch to it:\n\nThen you create some commits:\n\n\nNow you switch back to main:\n\nAnd you create commits from main too:\n\n\nTo merge your branch test2 into main, a new commit is now required. Git will create this new commit automatically. As long as there is no conflict, it is just as easy as a fast-forward merge:\ngit merge test2\n\nAfter which, you can delete the (now useless) test branch (with git branch -d test2):",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Branches"
    ]
  },
  {
    "objectID": "git/intro_branches.html#resolving-conflicts",
    "href": "git/intro_branches.html#resolving-conflicts",
    "title": "Branches",
    "section": "Resolving conflicts",
    "text": "Resolving conflicts\nGit works line by line. As long as you aren’t working on the same line(s) of the same file(s) on different branches, there will not be any merging difficulty. If however you modified one or more of the same line(s) of the same file(s) on different branches, Git has no way to decide which version should be kept and will thus not be able to complete the merge. It will then ask you to resolve the conflict(s). Conveniently, it will list the file(s) containing the conflict(s).\nThere are fancy tools to resolve conflicts, but you can do it in any text editor: simply open the file(s) listed by Git as having conflicts and look for the following markers:\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\nThis is your version.\n=======\nThis is the alternative version of the same section of the file.\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; alternative version\nIn our case, it could look something like:\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\nGreat sentence.\n=======\nGreat sentence with some variations.\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; test2\nThese markers are added by Git to signal the areas of conflict. It is up to you to choose between the two versions (or create a third one) and remove the conflict markers. After that, you can stage the file(s) which contained the conflicts to finish the merge (and then you can commit).",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Branches"
    ]
  },
  {
    "objectID": "git/intro_documentation.html",
    "href": "git/intro_documentation.html",
    "title": "Documentation",
    "section": "",
    "text": "Git comes with internal documentation. This section covers how to access it.",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Documentation"
    ]
  },
  {
    "objectID": "git/intro_documentation.html#manual-pages",
    "href": "git/intro_documentation.html#manual-pages",
    "title": "Documentation",
    "section": "Manual pages",
    "text": "Manual pages\nThe manual page for any Git command can be open with:\ngit help &lt;command&gt;\n\nExample:\n\ngit help commit\nOn Unix systems (Linux and MacOS), you can alternatively use the man command this way:\nman git-&lt;command&gt;\n\nExample:\n\nman git-commit\nFinally, many commands come with an help flag:\ngit &lt;command&gt; --help\n\nExample:\n\ngit commit --help\nAll these methods lead to the same thing: the manual page corresponding to the command will open in a pager (usually less). A pager is a program which makes it easier to read documents in the command line.\n\nUseful keybindings when you are in the pager:\nSPACE      scroll one screen down\nb          backa one screen\nq          quit the pager\ng          go to the top of the document\n7g         go to line 7 from the top\nG          go to the bottom of the document\n/          search for a term\n           n will take you to the next result\n           N to the previous result",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Documentation"
    ]
  },
  {
    "objectID": "git/intro_documentation.html#command-options",
    "href": "git/intro_documentation.html#command-options",
    "title": "Documentation",
    "section": "Command options",
    "text": "Command options\nInstead of opening the full manual in the pager, if you want to simply output the various flags (options) for a command directly in the terminal, you can use:\ngit &lt;command&gt; -h\n\nExample:\n\ngit commit -h",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Documentation"
    ]
  },
  {
    "objectID": "git/intro_ignore.html",
    "href": "git/intro_ignore.html",
    "title": "Excluding from version control",
    "section": "",
    "text": "Not everything should be under version control, yet we don’t want a cluttered working directory. The solution: a list of files or patterns that Git disregards.",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Excluding from version control"
    ]
  },
  {
    "objectID": "git/intro_ignore.html#what-to-exclude",
    "href": "git/intro_ignore.html#what-to-exclude",
    "title": "Excluding from version control",
    "section": "What to exclude",
    "text": "What to exclude\nThere are files you really want to put under version control, but there are files you shouldn’t.\nPut under version control:\n\nScripts\nManuscripts and notes\nMakefile & similar\n\nDo NOT put under version control:\n\nNon-text files (e.g. images, office documents)\nYour initial data\nOutputs that can be recreated by running code (e.g. graphs, results)\n\nHowever, you don’t want to have such documents constantly showing up when you run git status. In order to have a clean working directory while keeping them out of version control, you can create a file called .gitignore and add to it a list of files or patterns that you want Git to disregard.\n\n\nYour turn:\n\nIn the case of our mock project,\n\nwhat should we put under version control?\nwhat should we ignore?",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Excluding from version control"
    ]
  },
  {
    "objectID": "git/intro_ignore.html#how-to-exclude",
    "href": "git/intro_ignore.html#how-to-exclude",
    "title": "Excluding from version control",
    "section": "How to exclude",
    "text": "How to exclude\n\nThe .gitignore file\nTo exclude files from version control, create a file called .gitignore in the root of your project and add those files to it, one per line.\n\nExample:\n\n# Create .gitignore and add 'graph.png' to it\necho graph.png &gt; .gitignore\n\n# `&gt;` would overwrite the content. `&gt;&gt;` appends\necho output.txt &gt;&gt; .gitignore\nYou can also ignore entire directories.\n\nExample:\n\necho /results/ &gt;&gt; .gitignore\nFinally, you can use globbing patterns to ignore all files matching a certain pattern.\n\nExample:\n\n# Exclude all .png files\necho *.png &gt;&gt; .gitignore\n\n.gitignore syntax\nEach line in a .gitignore file specifies a pattern.\nBlank lines are ignored and can serve as separators for readability.\nLines starting with # are comments.\nTo add patterns starting with a special character (e.g. #, !), that character needs to be escaped with \\.\nTrailing spaces are ignored unless they are escaped with \\.\n! negates patterns.\nPatterns ending with / match directories. Otherwise patterns match both files and directories.\n/ at the beginning or within a search pattern indicates that the pattern is relative to the directory level of the .gitignore file (usually the root of the project). Otherwise the pattern matches anywhere below the .gitignore level.\n\nExamples:\n/foo/bar/ matches the directory foo/bar, but not the directory a/foo/bar\nfoo/bar/ matches both the directories foo/bar and a/foo/bar\n\n* matches anything except /.\n? matches any one character except /.\nThe range notation (e.g. [a-zA-Z]) can be used to match one of the characters in a range.\nA leading **/ matches all directories.\n\nExample:\n**/foo matches file or directory foo anywhere. This is the same as foo.\n\nA trailing /** matches everything inside what it precedes.\n\nExample:\nabc/** matches all files (recursively) inside directory abc\n\n/**/ matches zero or more directories.\n\nExample:\na/**/b matches a/b, a/x/b, and a/x/y/b\n\n\n\n\nYour turn:\n\nCreate a .gitignore file suitable for our mock project.\n\nThe .gitignore is a file like any other file, so you’ll want to stage and commit it:\ngit status\nOn branch main\nUntracked files:\n  (use \"git add &lt;file&gt;...\" to include in what will be committed)\n    .gitignore\n    src/\n\nnothing added to commit but untracked files present (use \"git add\" to track)\n\nNotice how data/ is not listed in the untracked files anymore.\n\nWe stage our .gitignore file:\ngit add .gitignore\ngit status\nOn branch main\nChanges to be committed:\n  (use \"git restore --staged &lt;file&gt;...\" to unstage)\n    new file:   .gitignore\n\nUntracked files:\n  (use \"git add &lt;file&gt;...\" to include in what will be committed)\n    src/\nAnd we create a new commit:\ngit commit -m \"Add .gitignore file with data and results\"\ngit status\n[main a1df8e5] Add .gitignore file with data and results\n 1 file changed, 2 insertions(+)\n create mode 100644 .gitignore\ngit status\nOn branch main\nUntracked files:\n  (use \"git add &lt;file&gt;...\" to include in what will be committed)\n    src/\n\nnothing added to commit but untracked files present (use \"git add\" to track)\nLet’s create a third commit with the Python script:\ngit add src/script.py\ngit commit -m \"Add first draft of script\"\n[main ca3c036] Add first draft of script\n 1 file changed, 7 insertions(+)\n create mode 100644 src/script.py\ngit status\nOn branch main\nnothing to commit, working tree clean\nWhat does “working tree clean” mean? In the next section, we will talk about the three file trees of Git.",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Excluding from version control"
    ]
  },
  {
    "objectID": "git/intro_intro.html",
    "href": "git/intro_intro.html",
    "title": "What is Git?",
    "section": "",
    "text": "Git is a free and open source version control system—a software that tracks changes to your files, allowing you to revisit or revert to older versions.\n\nSlides (Click and wait: the presentation might take a few instants to load)",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "What is Git?"
    ]
  },
  {
    "objectID": "git/intro_logs.html",
    "href": "git/intro_logs.html",
    "title": "Getting information on commits",
    "section": "",
    "text": "Before we can make use of old commits, we need to be able to get information about them. This is critical to know how to navigate the history of a project.",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Getting information on commits"
    ]
  },
  {
    "objectID": "git/intro_logs.html#displaying-the-commit-history",
    "href": "git/intro_logs.html#displaying-the-commit-history",
    "title": "Getting information on commits",
    "section": "Displaying the commit history",
    "text": "Displaying the commit history\nDo you remember this diagram from the first section of this course?\n\nThis is very useful to get a map of the history of our project. But how can we get a visual for it? The command here is git log and its many options.\nIn its basic form, it outputs the logs of our past commits:\ngit log\ncommit ca3c0360bd8ab961117671dd1f8fb2d2c3d5d7a1 (HEAD -&gt; main)\nAuthor: Marie-Helene Burle &lt;xxx@xxx&gt;\nDate:   Tue Jul 11 23:08:59 2023 -0700\n\n    Add first draft of script\n\ncommit c4ab5e755179a7b28a09c0ca587551bdac504d35\nAuthor: Marie-Helene Burle &lt;xxx@xxx&gt;\nDate:   Tue Jul 11 23:06:40 2023 -0700\n\n    Add .gitignore file with data and results\n\ncommit 61abf96298b54baf6d48cdea2ab1477db1075b5e\nAuthor: Marie-Helene Burle &lt;xxx@xxx&gt;\nDate:   Mon Jul 10 23:23:25 2023 -0700\n\n    Initial commit\nAs you can see, commits are listed from the bottom up.\nCommits can be displayed as one-liners:\ngit log --oneline\nca3c036 (HEAD -&gt; main) Add first draft of script\nc4ab5e7 Add .gitignore file with data and results\n61abf96 Initial commit\nOr as a graph:\ngit log --graph\n* commit ca3c0360bd8ab961117671dd1f8fb2d2c3d5d7a1 (HEAD -&gt; main)\n| Author: Marie-Helene Burle &lt;xxx@xxx&gt;\n| Date:   Tue Jul 11 23:08:59 2023 -0700\n|\n|     Add first draft of script\n|\n* commit c4ab5e755179a7b28a09c0ca587551bdac504d35\n| Author: Marie-Helene Burle &lt;xxx@xxx&gt;\n| Date:   Tue Jul 11 23:06:40 2023 -0700\n|\n|     Add .gitignore file with data and results\n|\n* commit 61abf96298b54baf6d48cdea2ab1477db1075b5e\n  Author: Marie-Helene Burle &lt;xxx@xxx&gt;\n  Date:   Mon Jul 10 23:23:25 2023 -0700\n\n      Initial commit\nOr in any fancy way you like:\ngit log \\\n    --graph \\\n    --date=short \\\n    --pretty=format:'%C(cyan)%h %C(blue)%ar %C(auto)%d'`\n                   `'%C(yellow)%s%+b %C(magenta)%ae'\n* ca3c036 31 minutes ago  (HEAD -&gt; main)Add first draft of script xxx@xxx\n* c4ab5e7 34 minutes ago Add .gitignore file with data and results xxx@xxx\n* 61abf96 24 hours ago Initial commit xxx@xxx\nRun man git-log for a full list of options.",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Getting information on commits"
    ]
  },
  {
    "objectID": "git/intro_logs.html#information-about-a-commit",
    "href": "git/intro_logs.html#information-about-a-commit",
    "title": "Getting information on commits",
    "section": "Information about a commit",
    "text": "Information about a commit\ngit log is useful to get an overview of our project history, but the information we get about each commit is limited. To get additional information about a particular commit, you can use git show followed by the hash of the commit you are interested in.\nFor instance, let’s explore our last commit\ngit show\ncommit ca3c0360bd8ab961117671dd1f8fb2d2c3d5d7a1 (HEAD -&gt; main)\nAuthor: Marie-Helene Burle &lt;xxx@xxx&gt;\nDate:   Tue Jul 11 23:08:59 2023 -0700\n\n    Add first draft of script\n\ndiff --git a/src/script.py b/src/script.py\nnew file mode 100644\nindex 0000000..263ef67\n--- /dev/null\n+++ b/src/script.py\n@@ -0,0 +1,7 @@\n+import pandas as pd\n+from matplotlib import pyplot as plt\n+\n+df = pd.read_csv('../data/dataset.csv')\n+\n+df.plot()\n+plt.savefig('../results/plot.png', dpi=300)\nOr our second commit:\ngit show c4ab5e7  # Replace the hash by the hash of your second commit\ncommit c4ab5e755179a7b28a09c0ca587551bdac504d35\nAuthor: Marie-Helene Burle &lt;xxx@xxx&gt;\nDate:   Tue Jul 11 23:06:40 2023 -0700\n\n    Add .gitignore file with data and results\n\ndiff --git a/.gitignore b/.gitignore\nnew file mode 100644\nindex 0000000..e85f44a\n--- /dev/null\n+++ b/.gitignore\n@@ -0,0 +1,2 @@\n+/data/\n+/results/\nIn addition to displaying the commit metadata, git show also displays the diff of that commit with its parent commit.",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Getting information on commits"
    ]
  },
  {
    "objectID": "git/intro_revisiting_old_commits_alternate.html",
    "href": "git/intro_revisiting_old_commits_alternate.html",
    "title": "Research Computing",
    "section": "",
    "text": "The pointer HEAD, which normally points to the branch main which itself points to latest commit, can be moved around. By moving HEAD to any commit, you can revisit the state of your project at that particular version.\nThe command for this is git checkout followed by the hash of the commit you want to revisit.\n\nFor instance, we could revisit the first commit in our example with:\n\ngit checkout 24duu7i  # Replace the hash by the hash of your first commit\nNote: switching to '24duu7i'.\n\nYou are in 'detached HEAD' state. You can look around, make experimental\nchanges and commit them, and you can discard any commits you make in this\nstate without impacting any branches by switching back to a branch.\n\nIf you want to create a new branch to retain commits you create, you may\ndo so (now or later) by using -c with the switch command. Example:\n\n  git switch -c &lt;new-branch-name&gt;\n\nOr undo this operation with:\n\n  git switch -\n\nTurn off this advice by setting config variable advice.detachedHead to false\n\nHEAD is now at 24duu7i Initial commit\n\nThis is the same as the command git switch --detach 24duu7i: git switch is a command introduced a few years ago because git checkout can be used for many things in Git and it was confusing many users. git switch allows to switch from one branch to another or, with the --detach flag, to switch to a commit as is the case here.\n\nOnce you have seen what you wanted to see, you can go back to your branch main with:\ngit checkout main\nPrevious HEAD position was 24duu7i Initial commit\nSwitched to branch 'main'\n\nThis is the same as the command git switch main.\n\nBe careful not to forget to go back to your branch main before making changes to your project. If you want to move the project to a new direction from some old commit, you need to create a new branch before doing so. When HEAD points directly to a commit (and not to a branch), this is called “Detached HEAD” and it is not a position from which you want to modify the project.\n\nIt is totally fine to move HEAD around and have it point directly to a commit (instead of a branch) as long as you are only looking at a version of your project and get back to a branch before doing some work:"
  },
  {
    "objectID": "git/intro_revisiting_old_commits_alternate.html#revisiting-old-commits",
    "href": "git/intro_revisiting_old_commits_alternate.html#revisiting-old-commits",
    "title": "Research Computing",
    "section": "",
    "text": "The pointer HEAD, which normally points to the branch main which itself points to latest commit, can be moved around. By moving HEAD to any commit, you can revisit the state of your project at that particular version.\nThe command for this is git checkout followed by the hash of the commit you want to revisit.\n\nFor instance, we could revisit the first commit in our example with:\n\ngit checkout 24duu7i  # Replace the hash by the hash of your first commit\nNote: switching to '24duu7i'.\n\nYou are in 'detached HEAD' state. You can look around, make experimental\nchanges and commit them, and you can discard any commits you make in this\nstate without impacting any branches by switching back to a branch.\n\nIf you want to create a new branch to retain commits you create, you may\ndo so (now or later) by using -c with the switch command. Example:\n\n  git switch -c &lt;new-branch-name&gt;\n\nOr undo this operation with:\n\n  git switch -\n\nTurn off this advice by setting config variable advice.detachedHead to false\n\nHEAD is now at 24duu7i Initial commit\n\nThis is the same as the command git switch --detach 24duu7i: git switch is a command introduced a few years ago because git checkout can be used for many things in Git and it was confusing many users. git switch allows to switch from one branch to another or, with the --detach flag, to switch to a commit as is the case here.\n\nOnce you have seen what you wanted to see, you can go back to your branch main with:\ngit checkout main\nPrevious HEAD position was 24duu7i Initial commit\nSwitched to branch 'main'\n\nThis is the same as the command git switch main.\n\nBe careful not to forget to go back to your branch main before making changes to your project. If you want to move the project to a new direction from some old commit, you need to create a new branch before doing so. When HEAD points directly to a commit (and not to a branch), this is called “Detached HEAD” and it is not a position from which you want to modify the project.\n\nIt is totally fine to move HEAD around and have it point directly to a commit (instead of a branch) as long as you are only looking at a version of your project and get back to a branch before doing some work:"
  },
  {
    "objectID": "git/intro_tags.html",
    "href": "git/intro_tags.html",
    "title": "Tags",
    "section": "",
    "text": "When you reach an important point in the development of a project, it is convenient to be able to identify the next commit easily. Rather than having to look for it through date, commit message, or hash, you can create a tag: a pointer to that commit.",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Tags"
    ]
  },
  {
    "objectID": "git/intro_tags.html#leightweight-tag",
    "href": "git/intro_tags.html#leightweight-tag",
    "title": "Tags",
    "section": "Leightweight tag",
    "text": "Leightweight tag\nYou create a tag with:\ngit tag &lt;tag-name&gt;\n\nExample:\n\ngit tag J_Climate_2009\n\nAs you keep developing the project and create new commits, the branch and HEAD pointers will move along, but the tag remains on your important commit.\n\nAt any time, you can get info on the commit thus tagged with:\ngit show J_Climate_2009\nOr you can check it out with:\ngit checkout J_Climate_2009",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Tags"
    ]
  },
  {
    "objectID": "git/intro_tags.html#annotated-tag",
    "href": "git/intro_tags.html#annotated-tag",
    "title": "Tags",
    "section": "Annotated tag",
    "text": "Annotated tag\nA more sophisticated form of tag comes with a message:\ngit tag -a &lt;tag-name&gt; -m \"&lt;message&gt;\"\ngit tag -a J_Climate_2009 -m \"State of project at the publication of paper\"",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Tags"
    ]
  },
  {
    "objectID": "git/intro_tags.html#list-tags",
    "href": "git/intro_tags.html#list-tags",
    "title": "Tags",
    "section": "List tags",
    "text": "List tags\ngit tag",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Tags"
    ]
  },
  {
    "objectID": "git/intro_tags.html#deleting-tags",
    "href": "git/intro_tags.html#deleting-tags",
    "title": "Tags",
    "section": "Deleting tags",
    "text": "Deleting tags\ngit tag -d &lt;tag-name&gt;\ngit tag -d J_Climate_2009",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Tags"
    ]
  },
  {
    "objectID": "git/intro_time_travel.html",
    "href": "git/intro_time_travel.html",
    "title": "Revisiting old commits",
    "section": "",
    "text": "It’s great to record history, but if we don’t know how to make use of it, it isn’t exactly useful.\nIn this workshop, you will travel through the history of a project.",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Revisiting old commits"
    ]
  },
  {
    "objectID": "git/intro_time_travel.html#looking-at-the-past-without-travelling",
    "href": "git/intro_time_travel.html#looking-at-the-past-without-travelling",
    "title": "Revisiting old commits",
    "section": "Looking at the past without travelling",
    "text": "Looking at the past without travelling\nHEAD is a little file in the .git directory which points to our current location in the Git history.\nYou already saw multiple ways to have a glimpse at your project history without moving HEAD:\n\ngit log and its many variations shows a list or a tree of your commits\ngit show displays information about a Git object such as a past commit\n\nThose are useful options, but Git allows you to really travel in your project history: HEAD can be moved around with the command git checkout to point to any branch, tag, or commit.",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Revisiting old commits"
    ]
  },
  {
    "objectID": "git/intro_time_travel.html#travelling-through-history",
    "href": "git/intro_time_travel.html#travelling-through-history",
    "title": "Revisiting old commits",
    "section": "Travelling through history",
    "text": "Travelling through history\nAs soon as you move HEAD to a new Git object with git checkout, the working directory and the index get updated to match the snapshot that Git object is pointing to. That means that your project will suddenly be back to the state in which it was when you committed that snapshot.\n\nMoving HEAD\nLet’s give this a try and move HEAD to a past commit.\n\nIdentifying the commit we want to move HEAD to\nYou can use the ~ notation:\n\nHEAD~ or HEAD~1 means the commit which precedes the one HEAD is pointing to.\nHEAD~2 means the commit before that.\nHEAD~3 refers to the 3rd commit before the current commit.\netc.\n\nYou can also run git log to find the hash of your commit of interest.\n\n\n\nDetached HEAD\nLet’s look at a hypothetical scenario to see what happens when you checkout a commit.\nThis is our starting point:\n\nNow, we move HEAD to the commit 31fukv1:\ngit checkout 31fukv1\n\nNotice that HEAD is not pointing at a branch anymore: it is pointing directly at a commit. This is called a detached HEAD state and Git will give you plenty of warnings about it.\nIf you look at your files, you will see that they match their state when you committed 31fukv1: your working directory got updated to match the current position of HEAD.\nYou can look at your project at that point in its history, then go back to your main branch (here main) with:\ngit checkout main\n\nAnd that’s that. You took a little trip into the past just to have a look, then came back to “the present” and all went well.\n\n\nCreating commits from a detached HEAD\nNow, when you are at commit 31fukv1, maybe you wanted to try something.\n\nYou can safely try anything you want: when you checkout main to come back to “the present”, those experimental changes will get lost.\nBut what if you want to keep those changes you made at 31fukv1?\nIn that case, as you always do, you create a commit to archive those changes into the project history:\n\nYou can make more commits:\n\nThe thing is that you are still in this detached HEAD state. HEAD is not pointing to a branch as it normally is. Is this a problem?\n\nBad workflow\nWell, it becomes a problem if you checkout main from there:\n\nIf you decide that you don’t care about those commits after all, then all is good. But if you care about them, this is a bad situation because those commits you created when you were in a detached HEAD state are now left behind: they are not in the history of any branch or tag.\nThis is bad for three reasons:\n\nThose commits will not show when you run git log, so it is easy to forget about them.\nIt is not easy to go back to them because there aren’t any tag or branch that you can checkout.\nThe garbage collection (which runs every 30 days by default) will delete those commits which are not on any branch or tag. So you will ultimately loose them.\n\n\n\nGood workflow\nA good workflow would have been to create a new branch on 31fukv1 (let’s call it alternative) and switch to it. That way, the commits created from 31fukv1 are on a branch and they will not be deleted by the next garbage collection:\n\nIn this good workflow, it is totally safe to switch back to main:\n\n\nIf you want to list the commits 23f481q and rthy7wg when you are back on main, you need to run git log with the --all flag.\n\n\n\nRecovering commits left behind\nWhat if you left commits behind (not on a branch)?\nYou can retrieve their hash by running:\ngit reflog\nThis tracks the position of HEAD over time.\nYou can then checkout the commit you care about (so you are going back to a detached HEAD state):\ngit checkout &lt;hash-abandonned-commit&gt;\nThis puts you back into a situation where you can rescue the commit(s) by creating a branch:\nDo this as soon as you can since those commits will be deleted at the next garbage collection (and finding their hash with git reflog will become increasingly complicated as you wait).",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Revisiting old commits"
    ]
  },
  {
    "objectID": "git/intro_undo.html",
    "href": "git/intro_undo.html",
    "title": "Undoing",
    "section": "",
    "text": "This section covers a few of the ways actions can be undone in Git.",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Undoing"
    ]
  },
  {
    "objectID": "git/intro_undo.html#amending-the-last-commit",
    "href": "git/intro_undo.html#amending-the-last-commit",
    "title": "Undoing",
    "section": "Amending the last commit",
    "text": "Amending the last commit\nHere is a common scenario: you make a commit, then realize that you forgot to include some changes in that commit; or you aren’t happy with the commit message; or both. You can edit your latest commit with the --amend flag:\ngit commit --amend\nThis will hide your last commit (as if it had never happened), add the changes in the staging area (if any) to the changes in that last commit, open a text editor showing the message of the last commit (you can keep or edit that message), and create a new commit which replaces your last commit.\nSo if you only want to change the commit message, run that command with an empty staging area. If you want to add changes to the last commit, stage them, then run the command.\nIn short, what this does is to replace your last commit with a new commit with the added changes and/or edited message. This prevents having a messy history with commits of the type “add missing file to last commit” or “better message for last commit: bla bla bla”. If you made a typo in your last commit message (and if you care about having a nice, clean history), you can fix it easily this way.\n\n\nYour turn:\n\n\nRun git log --oneline (notice the hash of the last commit)\nEdit your last commit message\nRun git log --oneline again to see that your last commit now has a new hash (so it is a different commit) and a new message\nNow, make some change in your project (add a file, or edit a file… any change you want)\nThen add that new change to your last commit without changing the message",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Undoing"
    ]
  },
  {
    "objectID": "git/intro_undo.html#unstaging",
    "href": "git/intro_undo.html#unstaging",
    "title": "Undoing",
    "section": "Unstaging",
    "text": "Unstaging\nYou know how to add changes to the staging area. But what if you want to unstage changes? You don’t want to loose those changes. But you staged them and then realized that you don’t want to include them in your next commit after all.\nHere is the command for this:\ngit restore --staged &lt;file&gt;\n\nNote that Git will remind you about the existence of this command when you run git status and have staged files ready to be committed.\n\n\n\nYour turn:\n\n\nMake changes to one of your existing files\nStage that file\nRun git status and notice Git’s reminder about this command\nUnstage the changes on that file",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Undoing"
    ]
  },
  {
    "objectID": "git/intro_undo.html#erasing-modifications",
    "href": "git/intro_undo.html#erasing-modifications",
    "title": "Undoing",
    "section": "Erasing modifications",
    "text": "Erasing modifications\nNow, what if you made changes to a file, then decide that they were no good? You can easily get rid of these edits and restore the file to its last committed version:\ngit restore &lt;file&gt;\n\nNote that Git will tell you about this command when you run git status and have unstaged changes in tracked files.\n\n\n\nYour turn:\n\n\nRun git status again and notice Git’s reminder about the existence of this command\nErase that last change of yours\nOpen your file and notice that your edits are gone\n\n\n\nAs you just experienced, this command leads to data loss.\nThose last edits are gone and unrecoverable. Be very careful when using this!",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Undoing"
    ]
  },
  {
    "objectID": "git/intro_undo.html#reverting",
    "href": "git/intro_undo.html#reverting",
    "title": "Undoing",
    "section": "Reverting",
    "text": "Reverting\n\nThe working directory must be clean before you can use git revert.\n\ngit revert creates a new commit which reverses the effect of past commit(s).\n\nTo revert the last commit (current location of HEAD):\n\ngit revert HEAD\n\nYou can use the hash of the last commit instead of HEAD.\n\n\nTo revert the last two commits:\n\ngit revert HEAD~\n\nHEAD~ is equivalent to HEAD~1 and means the commit before the one HEAD is on.\nHere too of course, you can use the hash of the commit before last instead of HEAD~.\n\n\nTo revert the last three commits:\n\ngit revert HEAD~2",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Undoing"
    ]
  },
  {
    "objectID": "git/top_intro.html",
    "href": "git/top_intro.html",
    "title": "Getting started with Git",
    "section": "",
    "text": "This introductory course to Git will allow you to put your projects under version control, create commits with important versions of your files, revisit these versions, and add remotes on services such as GitHub or GitLab.\nYou will get a deep understanding of the functioning of Git and learn to use it from the command line.\n\n Start course ➤",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>"
    ]
  },
  {
    "objectID": "git/wb_dvc.html",
    "href": "git/wb_dvc.html",
    "title": "Version control for data science and machine learning with DVC",
    "section": "",
    "text": "As DVC is a popular tool in machine learning, please find this webinar in the AI section.",
    "crumbs": [
      "Git",
      "<b><em>Webinars</em></b>"
    ]
  },
  {
    "objectID": "git/ws_contrib.html",
    "href": "git/ws_contrib.html",
    "title": "Collaborating to projects on GitHub",
    "section": "",
    "text": "There are countless free and open source tools on GitHub and if you use one such tool and find a problem or think that you can improve the project, or if you would like to request a novel feature, how do you go about it?\nIn this workshop, we will learn how to contribute to open source projects hosted on GitHub by opening issues and submitting pull requests.",
    "crumbs": [
      "Git",
      "<b><em>Workshops</em></b>",
      "Contributing to projects"
    ]
  },
  {
    "objectID": "git/ws_contrib.html#opening-issues",
    "href": "git/ws_contrib.html#opening-issues",
    "title": "Collaborating to projects on GitHub",
    "section": "Opening issues",
    "text": "Opening issues\nThe easiest thing to do, if for instance, you are having problems with the tool, found a bug, or want to submit a feature request, is to open an issue.\nGitHub has also now implemented the ability to open “Discussions”. If enabled by the maintainer of a project, this is the place where you want to ask for help.",
    "crumbs": [
      "Git",
      "<b><em>Workshops</em></b>",
      "Contributing to projects"
    ]
  },
  {
    "objectID": "git/ws_contrib.html#forking-a-project",
    "href": "git/ws_contrib.html#forking-a-project",
    "title": "Collaborating to projects on GitHub",
    "section": "Forking a project",
    "text": "Forking a project\nNow, a more advanced approach is to actually make changes to the code of the project.\nIf you want to develop your own version of the project, you can fork the GitHub repository: go to GitHub and fork the project by clicking on the “Fork” button in the top right corner.\nYou have all privileges on the forked project. So you can make any change you want there. You can clone it to your machine and develop the fork. But your fork does not get updated to the improvements made to the initial project. It is an independent project of its own.\n\nKeeping a fork up to date\nIf you want to keep your fork up to date with the initial project, you need to:\n\n1. Clone your fork on your machine\nThis will automatically set your fork on GitHub as the remote called origin:\n# If you have set SSH for your GitHub account\ngit clone git@github.com:&lt;user&gt;/&lt;repo&gt;.git &lt;name&gt;\n\n# If you haven't set SSH\ngit clone https://github.com/&lt;user&gt;/&lt;repo&gt;.git &lt;name&gt;\n\n\n2. Add the initial project as upstream\nAdd a second remote, this one pointing to the initial project. It is usual to call this remote upstream:\n# If you have set SSH for your GitHub account\ngit remote add upstream git@github.com:&lt;user&gt;/&lt;repo&gt;.git\n\n# If you haven't set SSH\ngit remote add upstream https://github.com/&lt;user&gt;/&lt;repo&gt;.git\n\n\n3. Pull from upstream\nYou can now pull from upstream to keep your fork up to date.\nFrom there on, you can pull from and push to origin (your fork) and you can pull from upstream (the initial repo).\nOf course, if your project and the initial one diverge in places, this will lead to conflicts that you will have to resolve as you merge the pulls from upstream.\nMost of the time however, you don’t want to develop your own version of the project. Instead, you want to make the initial project better by contributing to it. But you can’t push changes to upstream directly since you are not part of that project. You don’t have write access to that repository. If anybody could push to any project, that would be utter chaos.\nSo how do you contribute code to someone else’s project?",
    "crumbs": [
      "Git",
      "<b><em>Workshops</em></b>",
      "Contributing to projects"
    ]
  },
  {
    "objectID": "git/ws_contrib.html#creating-pull-requests",
    "href": "git/ws_contrib.html#creating-pull-requests",
    "title": "Collaborating to projects on GitHub",
    "section": "Creating pull requests",
    "text": "Creating pull requests\nHere is the workflow as described in the Git manual:\n\nPull from upstream to make sure that your contributions are made on an up-to-date version of the project\nCreate and checkout a new branch\nMake and commit your changes on that branch\nPush that branch to your fork (i.e. origin—remember that you do not have write access on upstream)\nGo to the original project GitHub’s page and open a pull request from your fork. Note that after you have pushed your branch to origin, GitHub will automatically offer you to do so.\n\nThe maintainer of the initial project may accept or decline the PR. They may also make comments and ask you to make changes. If so, make new changes and push additional commits to that branch until they are happy with the change.\nOnce the PR is merged by the maintainer, you can delete the branch on your fork and pull from upstream to update your local fork with the recently accepted changes.",
    "crumbs": [
      "Git",
      "<b><em>Workshops</em></b>",
      "Contributing to projects"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Training in Research Computing",
    "section": "",
    "text": "Git\nVersion control & collaboration\n\n\n\n\nR\nStatistical programming\n\n\n\n\nPython\nScientific programming\n\n\n\n\nTools\nTools for research computing\n\n\n\n\n\n\nAI\nDeep learning\n\n\n\n\nBash\nUnix shell scripting\n\n\n\n\nJulia\nFaster programming\n\n\n\n\nEmacs\nPowerful text editing\n\n\n\n\n\n\nMain website\nThis site contains content by Marie-Hélène Burle. To view all training material, please visit our main website."
  },
  {
    "objectID": "julia/hpc_intro.html",
    "href": "julia/hpc_intro.html",
    "title": "Introduction to high performance research computing in Julia",
    "section": "",
    "text": "When you launch a Jupyter session from a JupyterHub, you are running a Slurm job on a compute node. If you want to play for 8 hours in Jupyter, you are requesting an 8 hour job. Now, most of the time you spend on Jupyter is spent typing, running bits and pieces of code, or doing nothing at all. If you ask for GPUs, many CPUs, and lots of RAM, all of it will remain idle most of the time. This is a suboptimal use of resources.\nIn addition, if you ask for lots of resources for a long time, you will have to wait for a while before they get allocated to you.\nLastly, you will go through your allocations quickly.\nAll of this applies equally for interactive sessions launched from an SSH session with salloc.",
    "crumbs": [
      "Julia",
      "<b><em>High-performance Julia</em></b>",
      "Intro to HPC in Julia"
    ]
  },
  {
    "objectID": "julia/hpc_intro.html#interactive-sessions-for-high-performance-computing",
    "href": "julia/hpc_intro.html#interactive-sessions-for-high-performance-computing",
    "title": "Introduction to high performance research computing in Julia",
    "section": "",
    "text": "When you launch a Jupyter session from a JupyterHub, you are running a Slurm job on a compute node. If you want to play for 8 hours in Jupyter, you are requesting an 8 hour job. Now, most of the time you spend on Jupyter is spent typing, running bits and pieces of code, or doing nothing at all. If you ask for GPUs, many CPUs, and lots of RAM, all of it will remain idle most of the time. This is a suboptimal use of resources.\nIn addition, if you ask for lots of resources for a long time, you will have to wait for a while before they get allocated to you.\nLastly, you will go through your allocations quickly.\nAll of this applies equally for interactive sessions launched from an SSH session with salloc.",
    "crumbs": [
      "Julia",
      "<b><em>High-performance Julia</em></b>",
      "Intro to HPC in Julia"
    ]
  },
  {
    "objectID": "julia/hpc_intro.html#a-better-approach",
    "href": "julia/hpc_intro.html#a-better-approach",
    "title": "Introduction to high performance research computing in Julia",
    "section": "A better approach",
    "text": "A better approach\nA more efficient strategy is to develop and test your code with small samples, few iterations, etc. in an interactive job (from an SSH session in the cluster with salloc), on your own computer, or in Jupyter. Once you are confident that your code works, launch an sbatch job from an SSH session in the cluster to run the code as a script on all your data. This ensures that heavy duty resources that you requested are actually put to use to run your heavy calculations and not seating idle while you are thinking, typing, etc.",
    "crumbs": [
      "Julia",
      "<b><em>High-performance Julia</em></b>",
      "Intro to HPC in Julia"
    ]
  },
  {
    "objectID": "julia/hpc_intro.html#logging-on-to-the-cluster",
    "href": "julia/hpc_intro.html#logging-on-to-the-cluster",
    "title": "Introduction to high performance research computing in Julia",
    "section": "Logging on to the cluster",
    "text": "Logging on to the cluster\nOpen a terminal emulator:\nWindows users:  launch MobaXTerm.\nMacOS users:   launch Terminal.\nLinux users:     launch xterm or the terminal emulator of your choice.\nThen access the cluster through secure shell:\n$ ssh &lt;username&gt;@&lt;hostname&gt;    # enter password",
    "crumbs": [
      "Julia",
      "<b><em>High-performance Julia</em></b>",
      "Intro to HPC in Julia"
    ]
  },
  {
    "objectID": "julia/hpc_intro.html#accessing-julia",
    "href": "julia/hpc_intro.html#accessing-julia",
    "title": "Introduction to high performance research computing in Julia",
    "section": "Accessing Julia",
    "text": "Accessing Julia\nThis is done with the Lmod tool through the module command. You can find the full documentation here and below are the subcommands you will need:\n# get help on the module command\n$ module help\n$ module --help\n$ module -h\n\n# list modules that are already loaded\n$ module list\n\n# see which modules are available for Julia\n$ module spider julia\n\n# see how to load julia 1.3\n$ module spider julia/1.3.0\n\n# load julia 1.3 with the required gcc module first\n# (the order is important)\n$ module load gcc/7.3.0 julia/1.3.0\n\n# you can see that we now have Julia loaded\n$ module list",
    "crumbs": [
      "Julia",
      "<b><em>High-performance Julia</em></b>",
      "Intro to HPC in Julia"
    ]
  },
  {
    "objectID": "julia/hpc_intro.html#copying-files-to-the-cluster",
    "href": "julia/hpc_intro.html#copying-files-to-the-cluster",
    "title": "Introduction to high performance research computing in Julia",
    "section": "Copying files to the cluster",
    "text": "Copying files to the cluster\nWe will create a julia_workshop directory in ~/scratch, then copy our julia script in it.\n$ mkdir ~/scratch/julia_job\nOpen a new terminal window and from your local terminal (make sure that you are not on the remote terminal by looking at the bash prompt) run:\n$ scp /local/path/to/sort.jl &lt;username&gt;@&lt;hostname&gt;:scratch/julia_job\n$ scp /local/path/to/psort.jl &lt;username&gt;@&lt;hostname&gt;:scratch/julia_job\n\n# enter password",
    "crumbs": [
      "Julia",
      "<b><em>High-performance Julia</em></b>",
      "Intro to HPC in Julia"
    ]
  },
  {
    "objectID": "julia/hpc_intro.html#job-scripts",
    "href": "julia/hpc_intro.html#job-scripts",
    "title": "Introduction to high performance research computing in Julia",
    "section": "Job scripts",
    "text": "Job scripts\nWe will not run an interactive session with Julia on the cluster: we already have julia scripts ready to run. All we need to do is to write job scripts to submit to Slurm, the job scheduler used by the Alliance clusters.\nWe will create 2 scripts: one to run Julia on one core and one on as many cores as are available.\n\n\nYour turn:\n\nHow many processors are there on our training cluster?\n\nWe can run Julia with multiple threads by running:\n$ JULIA_NUM_THREADS=2 julia\nor:\n$ julia -t 2\nOnce in Julia, you can double check that Julia does indeed have access to 2 threads by running:\nThreads.nthreads()\nSave your job scripts in the files ~/scratch/julia_job/job_julia1c.sh and job_julia2c.sh for one and two cores respectively.\nHere is what our single core Slurm script looks like:\n#!/bin/bash\n#SBATCH --job-name=julia1c          # job name\n#SBATCH --time=00:01:00             # max walltime 1 min\n#SBATCH --cpus-per-task=1           # number of cores\n#SBATCH --mem=1000                  # max memory (default unit is megabytes)\n#SBATCH --output=julia1c%j.out      # file name for the output\n#SBATCH --error=julia1c%j.err       # file name for errors\n# %j gets replaced with the job number\n\necho Running NON parallel script\njulia sort.jl\necho Running parallel script on $SLURM_CPUS_PER_TASK core\njulia -t $SLURM_CPUS_PER_TASK psort.jl\n\n\nYour turn:\n\nWrite the script for 2 cores.\n\nNow, we can submit our jobs to the cluster:\n$ cd ~/scratch/julia_job\n$ sbatch job_julia1c.sh\n$ sbatch job_julia2c.sh\nAnd we can check their status with:\n$ sq      # This is an Alliance alias for `squeue -u $USER $@`\n\nPD stands for pending\nR stands for running",
    "crumbs": [
      "Julia",
      "<b><em>High-performance Julia</em></b>",
      "Intro to HPC in Julia"
    ]
  },
  {
    "objectID": "julia/hpc_performance.html",
    "href": "julia/hpc_performance.html",
    "title": "Performance",
    "section": "",
    "text": "The one thing you need to remember: avoid global variables.\nThis means: avoid variables defined in the global environment.",
    "crumbs": [
      "Julia",
      "<b><em>High-performance Julia</em></b>",
      "Performance"
    ]
  },
  {
    "objectID": "julia/hpc_performance.html#definitions",
    "href": "julia/hpc_performance.html#definitions",
    "title": "Performance",
    "section": "Definitions",
    "text": "Definitions\nScope of variables:   Environment within which a variables exist\nGlobal scope:     Global environment of a module\nLocal scope:      Environment within a function, a loop, a struct, a macro, etc.",
    "crumbs": [
      "Julia",
      "<b><em>High-performance Julia</em></b>",
      "Performance"
    ]
  },
  {
    "objectID": "julia/hpc_performance.html#why-avoid-global-variables",
    "href": "julia/hpc_performance.html#why-avoid-global-variables",
    "title": "Performance",
    "section": "Why avoid global variables?",
    "text": "Why avoid global variables?\nThe Julia compiler is not good at optimizing code using global variables.\nPart of the reason is that their type can change.\n\nExample\nWe will use the @time macro to time a loop:\n\nIn the global environment:\n\n\ntotal = 0\nn = 1e6\n\n@time for i in 1:n\n    global total += i\nend\n\n  0.640359 seconds (4.00 M allocations: 76.360 MiB, 77.50% gc time, 2.08% compilation time)\n\n\n\nNote the garbage collection (gc) time: 14% of total time.\nGarbage collection time is a sign of poor code.\n\n\nIn a local environment (a function):\n\n\nfunction local_loop(total, n)\n    total = total\n    @time for i in 1:n\n        global total += i\n    end\nend\n\nlocal_loop(0, 1e6)\n\n  0.033492 seconds (2.00 M allocations: 30.518 MiB, 18.12% gc time)\n\n\n\nWe get a 7.5 speedup.\nThe memory allocation also decreased by more than half.\n\nFor more accurate performance measurements, you should use the @btime macro from the BenchmarkTools package which excludes compilation time from the timing, averages metrics over multiple runs, and is highly customizable.",
    "crumbs": [
      "Julia",
      "<b><em>High-performance Julia</em></b>",
      "Performance"
    ]
  },
  {
    "objectID": "julia/intro_basics.html",
    "href": "julia/intro_basics.html",
    "title": "Basics of the Julia language",
    "section": "",
    "text": "Comments do not get evaluated by Julia and are for humans only.\n\n# Comments in Julia are identified by hastags\n\n\n#=\nComments can also spread over multiple lines\nif you enclose them with this syntax\n=#\n\n\nx = 2          # Comments can be added at the end of lines\n\n2",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Basics of the Julia language"
    ]
  },
  {
    "objectID": "julia/intro_basics.html#comments",
    "href": "julia/intro_basics.html#comments",
    "title": "Basics of the Julia language",
    "section": "",
    "text": "Comments do not get evaluated by Julia and are for humans only.\n\n# Comments in Julia are identified by hastags\n\n\n#=\nComments can also spread over multiple lines\nif you enclose them with this syntax\n=#\n\n\nx = 2          # Comments can be added at the end of lines\n\n2",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Basics of the Julia language"
    ]
  },
  {
    "objectID": "julia/intro_basics.html#basic-operations",
    "href": "julia/intro_basics.html#basic-operations",
    "title": "Basics of the Julia language",
    "section": "Basic operations",
    "text": "Basic operations\n\n# By default, Julia returns the output\n2 + 3\n\n5\n\n\n\n# Trailing semi-colons suppress the output\n3 + 7;\n\n\n# Alternative syntax that can be used with operators\n+(2, 5)\n\n7\n\n\n\n# Updating operators\na = 3\na += 8    # this is the same as a = a + 8\n\n11\n\n\n\n# Operator precedence follows standard rules\n3 + 2 ^ 3 * 10\n\n83\n\n\n\nMore exotic operators\n\n# Usual division\n6 / 2\n\n3.0\n\n\n\n# Inverse division\n2 \\ 6\n\n3.0\n\n\n\n# Integer division (division truncated to an integer)\n7 ÷ 2\n\n3\n\n\n\n# Remainder\n7 % 2        # equivalent to rem(7, 2)\n\n1\n\n\n\n# Fraction\n4//8\n\n1//2\n\n\n\n# Julia supports fraction operations\n1//2 + 3//4\n\n5//4",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Basics of the Julia language"
    ]
  },
  {
    "objectID": "julia/intro_basics.html#variables",
    "href": "julia/intro_basics.html#variables",
    "title": "Basics of the Julia language",
    "section": "Variables",
    "text": "Variables\n\n\nfrom xkcd.com\n\nA variable is a name bound to a value:\n\na = 3;\n\nIt can be called:\n\na\n\n3\n\n\nOr used in expressions:\n\na + 2\n\n5\n\n\n\nAssignment\nYou can re-assign new values to variables:\n\na = 3;\na = -8.2;\na\n\n-8.2\n\n\nEven values of a different type:\n\na = \"a is now a string\"\n\n\"a is now a string\"\n\n\nYou can define multiple variables at once:\n\na, b, c = 1, 2, 3\nb\n\n2\n\n\n\n\nVariable names\nThese names are extremely flexible and can use Unicode character:\n\\omega       # press TAB\n\\sum         # press TAB\n\\sqrt        # press TAB\n\\in          # press TAB\n\\:phone:     # press TAB\n\nδ = 8.5;\n🐌 = 3;\nδ + 🐌\n\n11.5\n\n\nAdmittedly, using emojis doesn’t seem very useful, but using Greek letters to write equations really makes Julia a great mathematical language:\n\nσ = 3\nδ = π\nϕ = 8\n\n(5σ + 3δ) / ϕ\n\n3.0530972450961724\n\n\n\nNote how the multiplication operator can be omitted when this does not lead to confusion. Also note how the mathematical constant π is available in Julia without having to load any module.\n\nIf you want to know how to type a symbol, ask Julia: type ? and paste it in the REPL.\nThe only hard rules for variable names are:\n\nThey must begin with a letter or an underscore,\nThey cannot take the names of built-in keywords such as if, do, try, else,\nThey cannot take the names of built-in constants (e.g. π) and keywords in use in a session.\n\n\nWe thus get an error here:\n\n\nfalse = 3\n\nLoadError: syntax: invalid assignment location \"false\" around In[24]:1\n\n\nIn addition, the Julia Style Guide recommends to follow these conventions:\n\nUse lower case,\nWord separation can be indicated by underscores, but better not to use them if the names can be read easily enough without them.\n\n\n\nThe ans variable\nThe keyword ans is a variable which, in the REPL, takes the value of the last computation:\na = 3 ^ 2;\nans + 1\n10\n\n\nPrinting\nTo print the value of a variable in an interactive session, you only need to call it:\n\na = 3;\na\n\n3\n\n\nIn non interactive sessions, you have to use the println function:\n\nprintln(a)\n\n3",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Basics of the Julia language"
    ]
  },
  {
    "objectID": "julia/intro_basics.html#quotes",
    "href": "julia/intro_basics.html#quotes",
    "title": "Basics of the Julia language",
    "section": "Quotes",
    "text": "Quotes\nNote the difference between single and double quotes:\n\ntypeof(\"a\")\n\nString\n\n\n\ntypeof('a')\n\nChar\n\n\n\n\"This is a string\"\n\n\"This is a string\"\n\n\n\n'This is not a sring'\n\n\nParseError:\n# Error @ ]8;;file:///home/marie/parvus/prog/mint/julia/In[30]#1:2\\In[30]:1:2]8;;\\\n'This is not a sring'\n#└─────────────────┘ ── character literal contains multiple characters\n\n\n\n\nWe got an error here since ' is used for the character type and can thus only contain a single character.\n\n\n'a'\n\n'a': ASCII/Unicode U+0061 (category Ll: Letter, lowercase)",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Basics of the Julia language"
    ]
  },
  {
    "objectID": "julia/intro_control_flow.html",
    "href": "julia/intro_control_flow.html",
    "title": "Control flow",
    "section": "",
    "text": "Control flow statements alter the linear execution of code, allowing for one or another section of code to be executed, or for one section of code to be executed multiple times.",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Control flow"
    ]
  },
  {
    "objectID": "julia/intro_control_flow.html#conditional-statements",
    "href": "julia/intro_control_flow.html#conditional-statements",
    "title": "Control flow",
    "section": "Conditional statements",
    "text": "Conditional statements\nConditional statements allow to run instructions based on predicates: different sets of instructions will be executed depending on whether the predicates return true or false.\n\nPredicates\n\nHere are a few examples of predicates with classic operators:\n\noccursin(\"that\", \"this and that\")\n4 &lt; 3\na == b\na != b\n2 in 1:3\n3 &lt;= 4 && 4 &gt; 5\n3 &lt;= 4 || 4 &gt; 5\nIn addition, Julia possesses more exotic operators that can be used in predicates:\n\nThe inexact equality comparator, useful to compare floating-point numbers despite computer rounding.\n\n\nThe function isapprox or the equivalent binary operator ≈ (typed with \\approx&lt;tab&gt;) can be used:\n\n0.1 + 0.2 == 0.3\n\nfalse\n\n\n\n0.1 + 0.2 ≈ 0.3\n\ntrue\n\n\n\nisapprox(0.1 + 0.2, 0.3)\n\ntrue\n\n\nThe negatives are the function !isapprox and ≉ (typed with \\napprox&lt;tab&gt;).\n\n\nThe equivalent or triple equal operator compares objects in deeper ways (address in memory for mutable objects and content at the bit level for immutable objects).\n\n\n=== or ≡ (typed with \\equiv&lt;tab&gt;) can be used:\n\na = [1, 2]; b = [1, 2];\n\n\na == b\n\ntrue\n\n\n\na ≡ b     # This can also be written `a === b`\n\nfalse\n\n\n\na ≡ a\n\ntrue\n\n\n\n\n\nIf statements\nif &lt;predicate&gt;\n    &lt;some action&gt;\nend\n\nIf &lt;predicate&gt; evaluates to true, the body of the if statement gets evaluated (&lt;some action&gt; is run),\nIf &lt;predicate&gt; evaluates to false, nothing happens.\n\n\nExample:\n\n\nfunction testsign1(x)\n    if x &gt;= 0\n        println(\"x is positive\")\n    end\nend\n\ntestsign1 (generic function with 1 method)\n\n\n\ntestsign1(3)\n\nx is positive\n\n\n\ntestsign1(-2)\n\n\nNothing gets returned since the predicate returned false.\n\n\n\nIf else statements\nif &lt;predicate&gt;\n    &lt;some action&gt;\nelse\n    &lt;some other action&gt;\nend\n\nIf &lt;predicate&gt; evaluates to true, &lt;some action&gt; is done,\nIf &lt;predicate&gt; evaluates to false, &lt;some other action&gt; is done.\n\n\nExample:\n\n\nfunction testsign2(x)\n    if x &gt;= 0\n        println(\"x is positive\")\n    else\n        println(\"x is negative\")\n    end\nend\n\ntestsign2 (generic function with 1 method)\n\n\n\ntestsign2(3)\n\nx is positive\n\n\n\ntestsign2(-2)\n\nx is negative\n\n\nIf else statements can be written in a terse format using the ternary operator:\n&lt;predicate&gt; ? &lt;some action&gt; : &lt;some other action&gt;\n\nHere is our function testsign2 written in terse format:\n\n\nfunction testsign2(x)\n    x &gt;= 0 ? println(\"x is positive\") : println(\"x is negative\")\nend\n\ntestsign2(-2)\n\nx is negative\n\n\n\nHere is another example:\n\na = 2\nb = 2.0\n\nif a == b\n    println(\"It's true\")\nelse\n    println(\"It's false\")\nend\nAnd in terse format:\n\na == b ? println(\"It's true\") : println(\"It's false\")\n\nIt's true\n\n\n\n\nIf elseif else statements\nif &lt;predicate1&gt;\n    &lt;some action&gt;\nelseif &lt;predicate2&gt;\n    &lt;some other action&gt;\nelse\n    &lt;yet some other action&gt;\nend\n\nExample:\n\n\nfunction testsign3(x)\n    if x &gt; 0\n        println(\"x is positive\")\n    elseif x == 0\n        println(\"x is zero\")\n    else\n        println(\"x is negative\")\n    end\nend\n\ntestsign3 (generic function with 1 method)\n\n\n\ntestsign3(3)\n\nx is positive\n\n\n\ntestsign3(0)\n\nx is zero\n\n\n\ntestsign3(-2)\n\nx is negative",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Control flow"
    ]
  },
  {
    "objectID": "julia/intro_control_flow.html#loops",
    "href": "julia/intro_control_flow.html#loops",
    "title": "Control flow",
    "section": "Loops",
    "text": "Loops\n\nFor loops\nFor loops run a set of instructions for each element of an iterable:\nfor &lt;iterable&gt;\n    &lt;some action&gt;\nend\n\nExamples:\n\n\nfor name = [\"Paul\", \"Lucie\", \"Sophie\"]\n    println(\"Hello $name\")\nend\n\nHello Paul\nHello Lucie\nHello Sophie\n\n\n\nfor i = 1:3, j = 3:5\n    println(i + j)\nend\n\n4\n5\n6\n5\n6\n7\n6\n7\n8\n\n\n\n\nWhile loops\nWhile loops run as long as a condition remains true:\nwhile &lt;predicate&gt;\n    &lt;some action&gt;\nend\n\nExample:\n\n\ni = 0\n\nwhile i &lt;= 10\n    println(i)\n    i += 1\nend\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Control flow"
    ]
  },
  {
    "objectID": "julia/intro_intro.html",
    "href": "julia/intro_intro.html",
    "title": "Introduction to Julia",
    "section": "",
    "text": "Why would I want to learn a new language? I already know R/python.\n\nR and python are interpreted languages: the code is executed directly, without prior-compilation. This is extremely convenient: it is what allows you to run code in an interactive shell. The price to pay is low performance: R and python are simply not good at handling large amounts of data. To overcome this limitation, users often turn to C or C++ for the most computation-intensive parts of their analyses. These are compiled—and extremely efficient—languages, but the need to use multiple languages and the non-interactive nature of compiled languages make this approach tedious.\nJulia uses just-in-time (JIT) compilation: the code is compiled at run time. This combines the interactive advantage of interpreted languages with the efficiency of compiled ones. Basically, it feels like running R or python, while it is almost as fast as C. This makes Julia particularly well suited for big data analyses, machine learning, or heavy modelling.\nIn addition, multiple dispatch (generic functions with multiple methods depending on the types of all the arguments) is at the very core of Julia. This is extremly convenient, cutting on conditionals and repetitions, and allowing for easy extensibility without having to rewrite code.\nFinally, Julia shines by its extremely clean and concise syntax. This last feature makes it easy to learn and really enjoyable to use.\nIn this workshop, which does not require any prior experience in Julia (experience in another language—e.g. R or python—would be best), we will go over the basics of Julia’s syntax and package system; then we will push the performance aspect further by looking at how Julia can make use of clusters for large scale parallel computing.",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Introduction to Julia"
    ]
  },
  {
    "objectID": "julia/intro_intro.html#introducing-julia",
    "href": "julia/intro_intro.html#introducing-julia",
    "title": "Introduction to Julia",
    "section": "Introducing Julia",
    "text": "Introducing Julia\n\nBrief history\nStarted in 2009 by Jeff Bezanson, Stefan Karpinski, Viral B. Shah, and Alan Edelman, the general-purpose programming language Julia was launched in 2012 as free and open source software. Version 1.0 was released in 2018.\nRust developer Graydon Hoare wrote an interesting post which places Julia in a historical context of programming languages.\n\n\nWhy another language?\n\nJIT\nComputer languages mostly fall into two categories: compiled languages and interpreted languages.\n\nCompiled languages\nCompiled languages require two steps:\n\nin a first step the code you write in a human-readable format (the source code, usually in plain text) gets compiled into machine code\nit is then this machine code that is used to process your data\n\nSo you write a script, compile it, then use it.\n\nBecause machine code is a lot easier to process by computers, compiled languages are fast. The two step process however makes prototyping new code less practical, these languages are hard to learn, and debugging compilation errors can be challenging.\n\nExamples of compiled languages include C, C++, Fortran, Go, and Haskell.\n\n\n\nInterpreted languages\nInterpreted languages are executed directly which has many advantages such as dynamic typing and direct feed-back from the code and they are easy to learn, but this comes at the cost of efficiency. The source code can facultatively be bytecompiled into non human-readable, more compact, lower level bytecode which is read by the interpreter more efficiently.\n\n\nExamples of interpreted languages include R, Python, Perl, and JavaScript.\n\n\n\nA common workflow\nSo, with this, what do researchers do?\nA common workflow, with the constraints of either type of languages, consists of:\n\nexploring the data and developing code using a sample of the data or reasonably light computations in an interpreted language,\ntranslating the code into a compiled language,\nfinally throwing the full data and all the heavy duty computation at that optimized code.\n\nThis works and it works well.\nBut, as you can imagine, this roundabout approach is tedious, not to mention the fact that it involves mastering 2 languages.\n\n\nJIT compiled languages\nJulia uses just-in-time compilation or JIT based on LLVM: the source code is compiled at run time. This combines the flexibility of interpretation with the speed of compilation, bringing speed to an interactive language. It also allows for dynamic recompilation, continuous weighing of gains and costs of the compilation of parts of the code, and other on the fly optimizations.\nOf course, there are costs here too. They come in the form of overhead time to compile code the first time it is run and increased memory usage.\n\n\n\nMultiple dispatch\nIn languages with multiple dispatch, functions apply different methods at run time based on the type of the operands. This brings great type stability and improves speed.\nJulia is extremely flexible: type declaration is not required. Out of convenience, you can forego the feature if you want. Specifying types however will greatly optimize your code.\nHere is a good post on type stability, multiple dispatch, and Julia efficiency.",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Introduction to Julia"
    ]
  },
  {
    "objectID": "julia/intro_intro.html#how-to-run-julia",
    "href": "julia/intro_intro.html#how-to-run-julia",
    "title": "Introduction to Julia",
    "section": "How to run Julia?",
    "text": "How to run Julia?\nThere are several ways to run Julia interactively:\n\ndirectly in the REPL (read–eval–print loop: the interactive Julia shell),\nin interactive notebooks (e.g. Jupyter, Pluto),\nin an editor able to run Julia interactively (e.g. Emacs, VS Code, Vim).\n\nLet’s have a look at these interfaces.\n\nThe Julia REPL\nYou can launch the REPL from a terminal directly by typing the julia command.\n\nREPL keybindings\nIn the REPL, you can use standard command line keybindings (Emacs kbd):\nC-c     cancel\nC-d     quit\nC-l     clear console\n\nC-u     kill from the start of line\nC-k     kill until the end of line\n\nC-a     go to start of line\nC-e     go to end of line\n\nC-f     move forward one character\nC-b     move backward one character\n\nM-f     move forward one word\nM-b     move backward one word\n\nC-d     delete forward one character\nC-h     delete backward one character\n\nM-d     delete forward one word\nM-Backspace delete backward one word\n\nC-p     previous command\nC-n     next command\n\nC-r     backward search\nC-s     forward search\n\n\nREPL modes\nThe Julia REPL is unique in that it has four distinct modes:\njulia&gt;     The main mode in which you will be running your code.\nhelp?&gt;     A mode to easily access documentation.\nshell&gt;     A mode in which you can run bash commands from within Julia.\n(env) pkg&gt;   A mode to easily perform actions on packages with Julia package manager.\n(env is the name of your current project environment.\nProject environments are similar to Python’s virtual environments and allow you, for instance, to have different package versions for different projects. By default, it is the current Julia version. So what you will see is (v1.3) pkg&gt;).\nEnter the various modes by typing ?, ;, and ]. Go back to the regular mode with the Backspace key.\n\n\n\nText editors\n\nVS Code\nJulia for Visual Studio Code has become the main Julia IDE.\n\n\nEmacs\n\nthrough the julia-emacs and julia-repl packages\nthrough the ESS package\nthrough the Emacs IPython Notebook package if you want to access Jupyter notebooks in Emacs\n\n\n\nVim\nThrough the julia-vim package.\n\n\n\nInteractive notebooks\n\nJupyter\nProject Jupyter allows to create interactive programming documents through its web-based JupyterLab environment and its Jupyter Notebook.\n\n\nPluto\nThe Julia package Juno is a reactive notebook for Julia.\n\n\n\nQuarto\nQuarto builds interactive documents with code and runs Julia through Jupyter.",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Introduction to Julia"
    ]
  },
  {
    "objectID": "julia/intro_intro.html#startup-options",
    "href": "julia/intro_intro.html#startup-options",
    "title": "Introduction to Julia",
    "section": "Startup options",
    "text": "Startup options\nYou can configure Julia by creating the file ~/.julia/config/startup.jl.",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Introduction to Julia"
    ]
  },
  {
    "objectID": "julia/intro_intro.html#help-and-documentation",
    "href": "julia/intro_intro.html#help-and-documentation",
    "title": "Introduction to Julia",
    "section": "Help and documentation",
    "text": "Help and documentation\nAs we already saw, you can type ? to enter the help mode:\n?sum\nsearch: sum sum! summary cumsum cumsum! isnumeric VersionNumber issubnormal \nget_zero_subnormals set_zero_subnormals\n\n  sum(f, itr; [init])\n\n  Sum the results of calling function f on each element of itr.\n\n  The return type is Int for signed integers of less than system word size, \n  and UInt for unsigned integers of less than system word size. For all other \n  arguments a common return type is found to which all arguments are promoted.\n\n  The value returned for empty itr can be specified by init. It must be the \n  additive identity (i.e. zero) as it is unspecified whether init is used for \n  non-empty collections.\n\nI truncated this output as the documentation also contains many examples.\n\nTo print the list of functions containing a certain word in their description, you can use apropos().\n\nExample:\n\n\napropos(\"truncate\")\n\nBase.dump\nBase.IOBuffer\nBase.open_flags\nBase.open\nBase.IOContext\nBase.truncate\nCore.String\nBase.Broadcast.newindex\nArgTools\nNetworkOptions\nLinearAlgebra.eigen\nTar\nBase.trunc\nDates.Date\nDates.format\nIJulia.set_max_stdio\nIJulia.watch_stream\nAbstractTrees.TreeCharSet\nAbstractTrees.print_tree\nOffsetArrays\nPDMats\nStatsFuns\nDistributions\nDistributions.TruncatedNormal\nDistributions.Truncated\nDistributions.truncated\nDistributions.Distributions\nLazyModules\nSimpleRandom\nMods\nMultisets\nBase.truncate\nPolynomials.truncate!\nSimplePolynomials\nLinearAlgebraX\nPermutations\nDelaunayTriangulation.grow_polygon_outside_of_box\nDelaunayTriangulation.clip_unbounded_polygon_to_bounding_box\nStableHashTraits\nMakie\nMakie.to_vertices",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Introduction to Julia"
    ]
  },
  {
    "objectID": "julia/intro_intro.html#version-information",
    "href": "julia/intro_intro.html#version-information",
    "title": "Introduction to Julia",
    "section": "Version information",
    "text": "Version information\nJulia version only:\n\nversioninfo()\n\nJulia Version 1.10.1\nCommit 7790d6f0641 (2024-02-13 20:41 UTC)\nBuild Info:\n  Official https://julialang.org/ release\nPlatform Info:\n  OS: Linux (x86_64-linux-gnu)\n  CPU: 16 × Intel(R) Core(TM) i7-10875H CPU @ 2.30GHz\n  WORD_SIZE: 64\n  LIBM: libopenlibm\n  LLVM: libLLVM-15.0.7 (ORCJIT, skylake)\nThreads: 1 default, 0 interactive, 1 GC (on 16 virtual cores)\n\n\nMore information, including commit, OS, CPU, and compiler:\n\nVERSION\n\nv\"1.10.1\"",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Introduction to Julia"
    ]
  },
  {
    "objectID": "julia/intro_intro.html#lets-try-a-few-commands",
    "href": "julia/intro_intro.html#lets-try-a-few-commands",
    "title": "Introduction to Julia",
    "section": "Let’s try a few commands",
    "text": "Let’s try a few commands\nx = 10\nx\nx = 2;\nx\ny = x;\ny\nans\nans + 3\n\na, b, c = 1, 2, 3\nb\n\n3 + 2\n+(3, 2)\n\na = 3\n2a\na += 7\na\n\n2\\8\n\na = [1 2; 3 4]\nb = a\na[1, 1] = 0\nb\n\n[1, 2, 3, 4]\n[1 2; 3 4]\n[1 2 3 4]\n[1 2 3 4]'\ncollect(1:4)\ncollect(1:1:4)\n1:4\na = 1:4\ncollect(a)\n\n[1, 2, 3] .* [1, 2, 3]\n\n4//8\n8//1\n1//2 + 3//4\n\na = true\nb = false\na + b\n\n\nYour turn:\n\nWhat does ; at the end of a command do?\nWhat is surprising about 2a?\nWhat does += do?\nWhat does .*do?\n\na = [3, 1, 2]\n\nsort(a)\nprintln(a)\n\nsort!(a)\nprintln(a)\n\n\nYour turn:\n\nWhat does ! at the end of a function name do?",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Introduction to Julia"
    ]
  },
  {
    "objectID": "julia/intro_non_interactive.html",
    "href": "julia/intro_non_interactive.html",
    "title": "Non interactive execution",
    "section": "",
    "text": "Julia scripts have a .jl extension.\nThe include function sources a Julia script (in a REPL session or in another script):\ninclude(\"file.jl\")\nThe code contained in file.jl is thus run non interactively.",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Non interactive execution"
    ]
  },
  {
    "objectID": "julia/intro_non_interactive.html#sourcing-a-file",
    "href": "julia/intro_non_interactive.html#sourcing-a-file",
    "title": "Non interactive execution",
    "section": "",
    "text": "Julia scripts have a .jl extension.\nThe include function sources a Julia script (in a REPL session or in another script):\ninclude(\"file.jl\")\nThe code contained in file.jl is thus run non interactively.",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Non interactive execution"
    ]
  },
  {
    "objectID": "julia/intro_non_interactive.html#running-code-from-the-command-line",
    "href": "julia/intro_non_interactive.html#running-code-from-the-command-line",
    "title": "Non interactive execution",
    "section": "Running code from the command line",
    "text": "Running code from the command line\nYou can run scripts by passing them to the julia command on the command line:\n$ julia script.jl\n\nThis code is run in a terminal, not in Julia, as is indicated by the $ prompt.\n\nYou can also evaluate single expressions in Julia from the command line by using the flag -e:\n$ julia -e 'println(2 + 3)'\n5\n\nPassing arguments\n\nTo the julia command itself\nIf you want to pass arguments to the julia command itself, you need to add them before the script or the Julia expression.\n\nExample:\n\n$ julia -O script.jl\n\n\nTo the script/Julia expression\nTo pass arguments to the script (or Julia expression if you use -e), you add them after the script or expression:\n$ julia script.jl arg1 arg2 arg3\narg1, arg2, arg3 will be passed in the global constant ARGS and interpreted as arguments to the script.\n\nExample passing arguments to an expression:\n\n$ julia -e 'for x in ARGS; println(x); end' 2 3\n2\n3\n\n\nTo both\nTo pass arguments both to the julia command and to the script/expression, you need to add the -- delimiter before the script/expression:\n$ julia [switches] -- [programfile] [args...]\n\nExample:\n\n$ julia -O -- script.jl arg1 arg2",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Non interactive execution"
    ]
  },
  {
    "objectID": "julia/intro_plotting.html",
    "href": "julia/intro_plotting.html",
    "title": "Plotting",
    "section": "",
    "text": "It can be convenient to plot directly in the REPL (for instance when using SSH).\n\nusing UnicodePlots\nhistogram(randn(1000), nbins=40)\n\nPrecompiling UnicodePlots\n  ✓ MarchingCubes\n  ✓ UnicodePlots\n  2 dependencies successfully precompiled in 36 seconds. 42 already precompiled.\nPrecompiling IntervalSetsExt\n  ✓ UnicodePlots → IntervalSetsExt\n  1 dependency successfully precompiled in 1 seconds. 46 already precompiled.\nPrecompiling FreeTypeExt\n  ✓ UnicodePlots → FreeTypeExt\n  1 dependency successfully precompiled in 2 seconds. 53 already precompiled.\n\n\n\n                ┌                                        ┐ \n   [-3.2, -3.0) ┤▌ 1                                       \n   [-3.0, -2.8) ┤▌ 1                                       \n   [-2.8, -2.6) ┤▊ 2                                       \n   [-2.6, -2.4) ┤█▍ 3                                      \n   [-2.4, -2.2) ┤███▏ 7                                    \n   [-2.2, -2.0) ┤█████▋ 13                                 \n   [-2.0, -1.8) ┤██▋ 6                                     \n   [-1.8, -1.6) ┤██████████▍ 24                            \n   [-1.6, -1.4) ┤███████████▋ 27                           \n   [-1.4, -1.2) ┤███████████████▋ 36                       \n   [-1.2, -1.0) ┤████████████████████▍ 47                  \n   [-1.0, -0.8) ┤████████████████████▍ 47                  \n   [-0.8, -0.6) ┤█████████████████████▍ 49                 \n   [-0.6, -0.4) ┤█████████████████████████████████▍ 77     \n   [-0.4, -0.2) ┤████████████████████████████████▎ 74      \n   [-0.2,  0.0) ┤█████████████████████████████████▍ 77     \n   [ 0.0,  0.2) ┤███████████████████████████████▋ 73       \n   [ 0.2,  0.4) ┤████████████████████████████████████  83  \n   [ 0.4,  0.6) ┤█████████████████████████████████▊ 78     \n   [ 0.6,  0.8) ┤█████████████████████████████▌ 68         \n   [ 0.8,  1.0) ┤██████████████████▎ 42                    \n   [ 1.0,  1.2) ┤████████████████████▍ 47                  \n   [ 1.2,  1.4) ┤████████████████▊ 39                      \n   [ 1.4,  1.6) ┤█████████████▏ 30                         \n   [ 1.6,  1.8) ┤████████▋ 20                              \n   [ 1.8,  2.0) ┤███▌ 8                                    \n   [ 2.0,  2.2) ┤██▋ 6                                     \n   [ 2.2,  2.4) ┤██▋ 6                                     \n   [ 2.4,  2.6) ┤█▍ 3                                      \n   [ 2.6,  2.8) ┤▊ 2                                       \n   [ 2.8,  3.0) ┤▊ 2                                       \n   [ 3.0,  3.2) ┤  0                                       \n   [ 3.2,  3.4) ┤▌ 1                                       \n   [ 3.4,  3.6) ┤  0                                       \n   [ 3.6,  3.8) ┤▌ 1                                       \n                └                                        ┘ \n                                 Frequency                 \n\n\n\nMost of the time however, you will want to make nicer looking graphs. There are many options to plot in Julia.\nPlots is a convenient Julia package which allows to use the same code with several graphing backends such as the GR framework (great for speed), Plotly.js (allows interaction with your graphs in a browser), or PyPlot. The default backend is the GR framework.\nStatsPlots is an enhanced version with added stats functionality.\n\nExample:\n\n\n# First run takes time as the package needs to compile\nusing StatsPlots\nStatsPlots.histogram(randn(1000), bins=40)\n\nPrecompiling StatsPlots\n  ✓ Arpack_jll\n  ✓ Widgets\n  ✓ Distances\n  ✓ Arpack\n  ✓ SentinelArrays\n  ✓ Distances → DistancesSparseArraysExt\n  ✓ TableOperations\n  ✓ Distances → DistancesChainRulesCoreExt\n  ✓ MultivariateStats\n  ✓ NearestNeighbors\n  ✓ Clustering\n  ✓ StatsPlots\n  12 dependencies successfully precompiled in 8 seconds. 225 already precompiled.\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere, we need to explicitly run StatsPlots.histogram rather than histogram to prevent a conflict with the function of the same name from the package UnicodePlots.",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Plotting"
    ]
  },
  {
    "objectID": "julia/intro_tabular.html",
    "href": "julia/intro_tabular.html",
    "title": "Working with tabular data:",
    "section": "",
    "text": "Requirements:\n1 - The current Julia stable release\nInstallation instructions can be found here.\n2 - The packages: CSV, DataFrames, TimeSeries, Plots\nPackages can be installed with ] add &lt;package&gt;.\n3 - Covid-19 data from the Johns Hopkins University CSSE repository\nClone (git clone &lt;repo url&gt;) or download and unzip the repository.",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Working with tabular data"
    ]
  },
  {
    "objectID": "julia/intro_tabular.html#load-packages",
    "href": "julia/intro_tabular.html#load-packages",
    "title": "Working with tabular data:",
    "section": "Load packages",
    "text": "Load packages\nusing CSV\nusing DataFrames\nusing Dates          # From the standard Julia library\nusing TimeSeries\nusing NamedArrays\nusing Plots\n\nWe will use the GR framework as a backend for Plots.",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Working with tabular data"
    ]
  },
  {
    "objectID": "julia/intro_tabular.html#data-until-march-22-2020",
    "href": "julia/intro_tabular.html#data-until-march-22-2020",
    "title": "Working with tabular data:",
    "section": "Data until March 22, 2020",
    "text": "Data until March 22, 2020\n\n\nfrom xkcd.com\n\nThe files in the Johns Hopkins University CSSE repository have changed over time.\nIn this workshop, we will use 2 sets of files:\n\na first set from January 22, 2020 until March 22, 2020\na second set from January 22, 2020 to the present\n\nBoth sets contain data on confirmed and dead cases for world countries and in some cases their subregions (provinces, states, etc. which I will globally here call “provinces”).\nThe first set also contains numbers of recovered cases which allow to calculate numbers of currently ill persons (of course, keep in mind that all these data represent various degrees of underestimation and are flawed in many ways, amongst which are varying levels of testing efforts both geographically and over time, under-reporting, etc).\nThe second set does not contain recovered cases (many overwhelmed countries stopped monitoring this at some point).\nWe will play with the first set together and you will then try to play with the second set on your own.\n\nLoad the data\nIf you did not clone or download and unzip the Covid-19 data repository in your working directory, adapt the path consequently.\n#= create a variable with the path we are interested in;\nthis makes the code below a bit shorter =#\ndir = \"COVID-19/csse_covid_19_data/csse_covid_19_time_series\"\n\n# create a list of the full paths of all the files in dir\nlist = joinpath.(relpath(dir), readdir(dir))\n\n#= read in the 3 csv files with confirmed, dead, and recovered numbers\ncorresponding to the first set of data (until March 22, 2020) =#\ndat = DataFrame.(CSV.File.(list[collect(2:4)]))\nWe now have a one-dimensional array of 3 DataFrames called dat.\n\n\nTransform data into long format\n# rename some variables to easier names\nDataFrames.rename!.(dat, Dict.(1 =&gt; Symbol(\"province\"),\n                               2 =&gt; Symbol(\"country\")))\n\n# create a one-dimensional array of strings\nvar = [\"total\", \"dead\", \"recovered\"]\n\n#= transform the data into long format in a vectorized fashion\nusing both our one-dimensional arrays of 3 elements =#\ndatlong = map((x, y) -&gt; stack(x, Not(collect(1:4)),\n                              variable_name = Symbol(\"date\"),\n                              value_name = Symbol(\"$y\")),\n              dat, var)\nWe now have a one-dimensional array of 3 DataFrames in long format called datlong.\n# join all elements of this array into a single DataFrame\nall = join(datlong[1], datlong[2], datlong[3],\n           on = [:date, :country, :province, :Lat, :Long])\n\n# get rid of \"Lat\" and \"Long\" and re-order the columns\nselect!(all, [4, 3, 1, 2, 7, 8])\n\n#= turn the year from 2 digits to 4 digits using regular expression\n(in a vectorised fashion by braodcasting with the dot notation);\nthen turn these values into strings, and finally into dates =#\nall.date = Date.(replace.(string.(all[:, 3]),\n                          r\"(.*)(..)$\" =&gt; s\"\\g&lt;1&gt;20\\2\"), \"m/dd/yy\");\n\n#= replace the missing values by the string \"NA\"\n(these are not real missing values, but rather non applicable ones) =#\nreplace!(all.province, missing =&gt; \"NA\");\nWe now have a single DataFrame called all, in long format, with the variables confirmed, dead, recovered, and ill.\nCalculate the number of currently ill individuals (again, in a vectorized fashion, by broadcasting with the dot notation):\nall.current = all.total .- all.dead .- all.recovered;\n\n\nWorld summary\nTo make a single plot with world totals of confirmed, dead, recovered, and ill cases, we want the sums of these variables for each day. We do this by grouping the data by date:\nworld = by(all, :date,\n           total = :total =&gt; sum,\n           dead = :dead =&gt; sum,\n           recovered = :recovered =&gt; sum,\n           current = :current =&gt; sum)\nNow we can plot our new variable world.\nAs our data is a time series, we need to transform it to a TimeArray thanks to the TimeArray() function from the TimeSeries package.\nplot(TimeArray(world, timestamp = :date),\n     title = \"World\",\n     legend = :outertopright,\n     widen = :false)\n Data until March 22, 2020\n\n\nCountries/provinces summaries\nNow, we want to group the data by country:\ncountries = groupby(all, :country)\nWe also need to know how the authors of the dataset decided to label the various countries and their subregions.\nFor example, if you want to see what the data looks like for France, Canada, and India, you can run:\ncountries[findall(x -&gt; \"France\" in x, keys(countries))]\ncountries[findall(x -&gt; \"Canada\" in x, keys(countries))]\ncountries[findall(x -&gt; \"India\" in x, keys(countries))]\nThen you need to subset the data for the countries or provinces you are interested in.\nHere are some examples:\n# countries for which there are data for several provinces\ncanada = all[all[:, :country] .== \"Canada\", :]\nus = all[all[:, :country] .== \"US\", :]\nchina = all[all[:, :country] .== \"China\", :]\n\n# countries with no province data\nskorea = all[all[:, :country] .== \"Korea, South\", :]\ntaiwan = all[all[:, :country] .== \"Taiwan*\", :]\nsingapore = all[all[:, :country] .== \"Singapore\", :]\nitaly = all[all[:, :country] .== \"Italy\", :]\nspain = all[all[:, :country] .== \"Spain\", :]\n\n#= countries wich have subregions spread widely in the world;\nhere, I took the arbitrary decision to only look at the main subregions =#\nfrance = all[all[:, :province] .== \"France\", :]\nuk = all[all[:, :province] .== \"United Kingdom\", :]\n\n# provinces\nbc = all[all[:, :province] .== \"British Columbia\", :]\nny = all[all[:, :province] .== \"New York\", :]\nCalculate the totals for Canada, US, and China which all have data for subregions:\ncanada, us, china = by.([canada, us, china], :date,\n                        total = :total =&gt; sum,\n                        dead = :dead =&gt; sum,\n                        recovered = :recovered =&gt; sum,\n                        current = :current =&gt; sum)\nloclist1 = [canada, us, china]\nloctitles1 = [\"Canada\", \"US\", \"China\"]\n\npcanada, pus, pchina =\n    map((x, y) -&gt; plot(TimeArray(x, timestamp = :date),\n                       title = \"$y\", legend = :outertopright,\n                       widen = :false, dpi = :300),\n        loclist1, loctitles1)\nloclist2 = [france, bc, ny, taiwan, skorea, singapore, spain, italy, uk]\nloctitles2 = [\"France\", \"BC\", \"NY\", \"Taiwan\", \"South Korea\",\n              \"Singapore\", \"Spain\", \"Italy\", \"UK\"]\n\npfrance, pbc, pny, ptaiwan, pskorea,\npsingapore, pspain, pitaly, puk =\n    map((x, y) -&gt; plot(TimeArray(select(x, Not([:country, :province])),\n                                 timestamp = :date),\n                       title = \"$y\", legend = :outertopright,\n                       widen = :false, dpi = :300),\n        loclist2, loctitles2)\nNow, let’s plot a few countries/provinces:\n\nNorth America\nplot(pcanada, pbc, pus, pny,\n     legend = false, titlefontsize = 7, tickfontsize = 6)\n Data until March 22, 2020\n\n\nAsia\nplot(pchina, ptaiwan, pskorea, psingapore,\n     legend = false, titlefontsize = 7, tickfontsize = 6)\n Data until March 22, 2020\n\n\nEurope\nplot(pfrance, pspain, pitaly, puk,\n     legend = false, titlefontsize = 7, tickfontsize = 6)\n Data until March 22, 2020",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Working with tabular data"
    ]
  },
  {
    "objectID": "julia/intro_tabular.html#data-up-to-the-present",
    "href": "julia/intro_tabular.html#data-up-to-the-present",
    "title": "Working with tabular data:",
    "section": "Data up to the present",
    "text": "Data up to the present\n\nSummary graphs\n\n\nYour turn:\n\nWrite the code to create an up-to-date graph for the world using the files: time_series_covid19_confirmed_global.csv and time_series_covid19_deaths_global.csv.\n\nHere is the result:\n Data until March 25, 2020\n\n\nYour turn:\n\nCreate up-to-date graphs for the countries and/or provinces of your choice.\n\nHere are a few possible results:\n Data until March 25, 2020\n\n\nCountries comparison\nOur side by side graphs don’t make comparisons very easy since they vary greatly in their axes scales.\nOf course, we could constrain them to have the same axes, but then, why not plot multiple countries or provinces in the same graph?\ncanada[!, :loc] .= \"Canada\";\nchina[!, :loc] .= \"China\";\n\nall = join(all, canada, china, on = [:date, :total, :dead, :loc],\n           kind = :outer)\n\nconfirmed = unstack(all[:, collect(3:5)], :loc, :total)\n\nconf_sel = select(confirmed,\n                  [:date, :Italy, :Spain, :China, :Iran,\n                   :France, :US, Symbol(\"South Korea\"), :Canada])\n\nplot(TimeArray(conf_sel, timestamp = :date),\n     title = \"Confirmed across a few countries\",\n     legend = :outertopright, widen = :false)\n Data until March 25, 2020\n\n\nYour turn:\n\nWrite the code to make a similar graph with the number of deaths in a few countries of your choice.\n\nHere is a possible result:\n Data until March 25, 2020",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Working with tabular data"
    ]
  },
  {
    "objectID": "julia/top_wb.html",
    "href": "julia/top_wb.html",
    "title": "Julia webinars",
    "section": "",
    "text": "Data visualization with Makie\n\n\n\n\nFirst dab at Julia\n\n\n\n\nDeep learning with Flux",
    "crumbs": [
      "Julia",
      "<b><em>Webinars</em></b>"
    ]
  },
  {
    "objectID": "julia/wb_flux.html",
    "href": "julia/wb_flux.html",
    "title": "Machine learning in Julia with Flux",
    "section": "",
    "text": "Please find this webinar in the AI section.",
    "crumbs": [
      "Julia",
      "<b><em>Webinars</em></b>",
      "Deep learning with Flux"
    ]
  },
  {
    "objectID": "julia/wb_makie_slides.html#plotting-in-julia",
    "href": "julia/wb_makie_slides.html#plotting-in-julia",
    "title": "Makie",
    "section": "Plotting in Julia",
    "text": "Plotting in Julia\n\nMany options:\n\nPlots.jl: high-level API for working with different back-ends (GR, Pyplot, Plotly…)\nPyPlot.jl: Julia interface to Matplotlib’s matplotlib.pyplot\nPlotlyJS.jl: Julia interface to plotly.js\nPlotlyLight.jl: the fastest plotting option in Julia by far, but limited features\nGadfly.jl: following the grammar of graphics popularized by Hadley Wickham in R\nVegaLite.jl: grammar of interactive graphics\nPGFPlotsX.jl: Julia interface to the PGFPlots LaTeX package\nUnicodePlots.jl: plots in the terminal 🙂\n\n\n\n\nMakie.jl: powerful plotting ecosystem: animation, 3D, GPU optimization"
  },
  {
    "objectID": "julia/wb_makie_slides.html#makie-ecosystem",
    "href": "julia/wb_makie_slides.html#makie-ecosystem",
    "title": "Makie",
    "section": "Makie ecosystem",
    "text": "Makie ecosystem\n\n\nMain package:\n\nMakie: plots functionalities. Backend needed to render plots into images or vector graphics\n\n\n\n\n\nBackends:\n\nCairoMakie: vector graphics or high-quality 2D plots. Creates, but does not display plots (you need an IDE that does or you can use ElectronDisplay.jl)\nGLMakie: based on OpenGL; 3D rendering and interactivity in GLFW window (no vector graphics)\nWGLMakie: web version of GLMakie (plots rendered in a browser instead of a window)"
  },
  {
    "objectID": "julia/wb_makie_slides.html#extensions",
    "href": "julia/wb_makie_slides.html#extensions",
    "title": "Makie",
    "section": "Extensions",
    "text": "Extensions\n\nGeoMakie.jl add geographical plotting utilities to Makie\nAlgebraOfGraphics.jl turns plotting into a simple algebra of building blocks\nGraphMakie.jl to create network graphs"
  },
  {
    "objectID": "julia/wb_makie_slides.html#cheatsheet-2d",
    "href": "julia/wb_makie_slides.html#cheatsheet-2d",
    "title": "Makie",
    "section": "Cheatsheet 2D",
    "text": "Cheatsheet 2D\n\n\nFrom: Storopoli, Huijzer and Alonso (2021). Julia Data Science. https://juliadatascience.io. ISBN: 97984898"
  },
  {
    "objectID": "julia/wb_makie_slides.html#cheatsheet-3d",
    "href": "julia/wb_makie_slides.html#cheatsheet-3d",
    "title": "Makie",
    "section": "Cheatsheet 3D",
    "text": "Cheatsheet 3D\n\n\nFrom: Storopoli, Huijzer and Alonso (2021). Julia Data Science. https://juliadatascience.io. ISBN: 97984898"
  },
  {
    "objectID": "julia/wb_makie_slides.html#resources",
    "href": "julia/wb_makie_slides.html#resources",
    "title": "Makie",
    "section": "Resources",
    "text": "Resources\n\nOfficial documentation\nJulia Data Science book, chapter 5\nMany examples in the project Beautiful Makie"
  },
  {
    "objectID": "julia/wb_makie_slides.html#troubleshooting",
    "href": "julia/wb_makie_slides.html#troubleshooting",
    "title": "Makie",
    "section": "Troubleshooting",
    "text": "Troubleshooting\nInstalling GLMakie can be challenging. This page may lead you towards solutions\nCairoMakie and WGLMakie should install without issues"
  },
  {
    "objectID": "julia/wb_makie_slides.html#figure",
    "href": "julia/wb_makie_slides.html#figure",
    "title": "Makie",
    "section": "Figure",
    "text": "Figure\nLoad the package (here, we are using CairoMakie):\n\nusing CairoMakie                        # no need to import Makie itself\n\n\n\nCreate a Figure (container object):\n\nfig = Figure()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntypeof(fig)\n\nFigure"
  },
  {
    "objectID": "julia/wb_makie_slides.html#axis",
    "href": "julia/wb_makie_slides.html#axis",
    "title": "Makie",
    "section": "Axis",
    "text": "Axis\n\n\nThen, you can create an Axis:\n\nax = Axis(Figure()[1, 1])\n\nAxis with 0 plots:\n\n\n\n\n\n\n\ntypeof(ax)\n\nAxis"
  },
  {
    "objectID": "julia/wb_makie_slides.html#plot",
    "href": "julia/wb_makie_slides.html#plot",
    "title": "Makie",
    "section": "Plot",
    "text": "Plot\nFinally, we can add a plot:\n\nfig = Figure()\nax = Axis(fig[1, 1])\nx = LinRange(-10, 10, 20)\ny = x\nscatter!(ax, x, y)  # Functions with ! transform their arguments\nfig"
  },
  {
    "objectID": "julia/wb_makie_slides.html#d",
    "href": "julia/wb_makie_slides.html#d",
    "title": "Makie",
    "section": "2D",
    "text": "2D\n\nusing CairoMakie\nusing StatsBase, LinearAlgebra\nusing Interpolations, OnlineStats\nusing Distributions\nCairoMakie.activate!(type = \"png\")\n\nfunction eq_hist(matrix; nbins = 256 * 256)\n    h_eq = fit(Histogram, vec(matrix), nbins = nbins)\n    h_eq = normalize(h_eq, mode = :density)\n    cdf = cumsum(h_eq.weights)\n    cdf = cdf / cdf[end]\n    edg = h_eq.edges[1]\n    interp_linear = LinearInterpolation(edg, [cdf..., cdf[end]])\n    out = reshape(interp_linear(vec(matrix)), size(matrix))\n    return out\nend\n\nfunction getcounts!(h, fn; n = 100)\n    for _ in 1:n\n        vals = eigvals(fn())\n        x0 = real.(vals)\n        y0 = imag.(vals)\n        fit!(h, zip(x0,y0))\n    end\nend\n\nm(;a=10rand()-5, b=10rand()-5) = [0 0 0 a; -1 -1 1 0; b 0 0 0; -1 -1 -1 -1]\n\nh = HeatMap(range(-3.5,3.5,length=1200), range(-3.5,3.5, length=1200))\ngetcounts!(h, m; n=2_000_000)\n\nwith_theme(theme_black()) do\n    fig = Figure(figure_padding=0,resolution=(600,600))\n    ax = Axis(fig[1,1]; aspect = DataAspect())\n    heatmap!(ax,-3.5..3.5, -3.5..3.5, eq_hist(h.counts); colormap = :bone_1)\n    hidedecorations!(ax)\n    hidespines!(ax)\n    fig\nend"
  },
  {
    "objectID": "julia/wb_makie_slides.html#d-output",
    "href": "julia/wb_makie_slides.html#d-output",
    "title": "Makie",
    "section": "2D",
    "text": "2D"
  },
  {
    "objectID": "julia/wb_makie_slides.html#d-1",
    "href": "julia/wb_makie_slides.html#d-1",
    "title": "Makie",
    "section": "3D",
    "text": "3D\nusing GLMakie, Random\nGLMakie.activate!()\n\nRandom.seed!(13)\nx = -6:0.5:6\ny = -6:0.5:6\nz = 6exp.( -(x.^2 .+ y' .^ 2)./4)\n\nbox = Rect3(Point3f(-0.5), Vec3f(1))\nn = 100\ng(x) = x^(1/10)\nalphas = [g(x) for x in range(0,1,length=n)]\ncmap_alpha = resample_cmap(:linear_worb_100_25_c53_n256, n, alpha = alphas)\n\nwith_theme(theme_dark()) do\n    fig, ax, = meshscatter(x, y, z;\n                           marker=box,\n                           markersize = 0.5,\n                           color = vec(z),\n                           colormap = cmap_alpha,\n                           colorrange = (0,6),\n                           axis = (;\n                                   type = Axis3,\n                                   aspect = :data,\n                                   azimuth = 7.3,\n                                   elevation = 0.189,\n            perspectiveness = 0.5),\n        figure = (;\n            resolution =(1200,800)))\n    meshscatter!(ax, x .+ 7, y, z./2;\n        markersize = 0.25,\n        color = vec(z./2),\n        colormap = cmap_alpha,\n        colorrange = (0, 6),\n        ambient = Vec3f(0.85, 0.85, 0.85),\n        backlight = 1.5f0)\n    xlims!(-5.5,10)\n    ylims!(-5.5,5.5)\n    hidedecorations!(ax; grid = false)\n    hidespines!(ax)\n    fig\nend"
  },
  {
    "objectID": "julia/wb_makie_slides.html#d-2",
    "href": "julia/wb_makie_slides.html#d-2",
    "title": "Makie",
    "section": "3D",
    "text": "3D"
  },
  {
    "objectID": "julia/wb_makie_slides.html#compiling-sysimages",
    "href": "julia/wb_makie_slides.html#compiling-sysimages",
    "title": "Makie",
    "section": "Compiling sysimages",
    "text": "Compiling sysimages\nWhile Makie is extremely powerful, its compilation time and its time to first plot are extremely long\nFor this reason, it might save you a lot of time to create a sysimage (a file containing information from a Julia session such as loaded packages, global variables, compiled code, etc.) with PackageCompiler.jl\n\nThe upcoming Julia 1.9 will do this automatically"
  },
  {
    "objectID": "julia/wb_makie_slides.html#cairomakie",
    "href": "julia/wb_makie_slides.html#cairomakie",
    "title": "Makie",
    "section": "CairoMakie",
    "text": "CairoMakie\nCairoMakie will run without problem on the Alliance clusters\nIt is not designed for interactivity, so saving to file is what makes the most sense\n\nExample:\n\nsave(\"graph.png\", fig)\n Remember however that CairoMakie is 2D only (for now)"
  },
  {
    "objectID": "julia/wb_makie_slides.html#glmakie",
    "href": "julia/wb_makie_slides.html#glmakie",
    "title": "Makie",
    "section": "GLMakie",
    "text": "GLMakie\nGLMakie relies on GLFW to create windows with OpenGL\nGLFW doesn’t support creating contexts without an associated window\nThe dependency GLFW.jl will thus not install in the clusters—even with X11 forwarding—unless you use VDI nodes, VNC, or Virtual GL"
  },
  {
    "objectID": "julia/wb_makie_slides.html#wglmakie",
    "href": "julia/wb_makie_slides.html#wglmakie",
    "title": "Makie",
    "section": "WGLMakie",
    "text": "WGLMakie\nYou can setup a server with JSServe.jl as per the documentation\nHowever, this method is intended for the creation of interactive widgets, e.g. for a website\nWhile this is really cool, it isn’t optimized for performance\nThere might also be a way to create an SSH tunnel to your local browser, although there is no documentation on this\nBest probably is to save to file"
  },
  {
    "objectID": "julia/wb_makie_slides.html#conclusion-makie-on-production-clusters",
    "href": "julia/wb_makie_slides.html#conclusion-makie-on-production-clusters",
    "title": "Makie",
    "section": "Conclusion: Makie on production clusters",
    "text": "Conclusion: Makie on production clusters\n\n2D plots: use CairoMakie and save to file\n3D plots: use WGLMakie and save to file\n\n\n\n\n\n Back to webinar page"
  },
  {
    "objectID": "python/hpc_jax.html",
    "href": "python/hpc_jax.html",
    "title": "High-performance computing with JAX",
    "section": "",
    "text": "JAX can be used for any high-performance scientific computing with arrays, but because it is a good starting point to build deep learning libraries, please find this course in the AI section.",
    "crumbs": [
      "Python",
      "<b><em>Faster Python</em></b>",
      "JAX: accelerated arrays & AD"
    ]
  },
  {
    "objectID": "python/index.html",
    "href": "python/index.html",
    "title": "Python",
    "section": "",
    "text": "Getting started in  \nAn intro course to Python\n\n\n\n\nFaster  \nSpeeding up Python computations\n\n\n\n\n\n\nWorkshops\nVarious Python topics\n\n\n\n\n60 min webinars\nVarious Python topics",
    "crumbs": [
      "Python",
      "<br>&nbsp;<img src=\"img/logo_python.svg\" class=\"img-fluid\" style=\"width:1.55em\" alt=\"noshadow\"><br><br>"
    ]
  },
  {
    "objectID": "python/intro_collections.html",
    "href": "python/intro_collections.html",
    "title": "Collections",
    "section": "",
    "text": "Values can be stored in collections. This section introduces tuples, dictionaries, sets, and arrays in Python.",
    "crumbs": [
      "Python",
      "<b><em>Getting started</em></b>",
      "Collections"
    ]
  },
  {
    "objectID": "python/intro_collections.html#lists",
    "href": "python/intro_collections.html#lists",
    "title": "Collections",
    "section": "Lists",
    "text": "Lists\nLists are declared in square brackets:\n\nl = [2, 1, 3]\nl\n\n[2, 1, 3]\n\n\n\ntype(l)\n\nlist\n\n\nThey are mutable:\n\nl.append(0)\nl\n\n[2, 1, 3, 0]\n\n\nLists are ordered:\n\n['b', 'a'] == ['a', 'b']\n\nFalse\n\n\nThey can have repeat values:\n\n['a', 'a', 'a', 't']\n\n['a', 'a', 'a', 't']\n\n\nLists can be homogeneous:\n\n['b', 'a', 'x', 'e']\n\n['b', 'a', 'x', 'e']\n\n\n\ntype('b') == type('a') == type('x') == type('e')\n\nTrue\n\n\nor heterogeneous:\n\n[3, 'some string', 2.9, 'z']\n\n[3, 'some string', 2.9, 'z']\n\n\n\ntype(3) == type('some string') == type(2.9) == type('z')\n\nFalse\n\n\nThey can even be nested:\n\n[3, ['b', 'e', 3.9, ['some string', 9.9]], 8]\n\n[3, ['b', 'e', 3.9, ['some string', 9.9]], 8]\n\n\nThe length of a list is the number of items it contains and can be obtained with the function len:\n\nlen([3, ['b', 'e', 3.9, ['some string', 9.9]], 8])\n\n3\n\n\nTo extract an item from a list, you index it:\n\n[3, ['b', 'e', 3.9, ['some string', 9.9]], 8][0]\n\n3\n\n\n\nPython starts indexing at 0, so what we tend to think of as the “first” element of a list is for Python the “zeroth” element.\n\n\n[3, ['b', 'e', 3.9, ['some string', 9.9]], 8][1]\n\n['b', 'e', 3.9, ['some string', 9.9]]\n\n\n\n[3, ['b', 'e', 3.9, ['some string', 9.9]], 8][2]\n\n8\n\n\n\n# Of course you can't extract items that don't exist\n[3, ['b', 'e', 3.9, ['some string', 9.9]], 8][3]\n\nIndexError: list index out of range\n\n\nYou can index from the end of the list with negative values (here you start at -1 for the last element):\n\n[3, ['b', 'e', 3.9, ['some string', 9.9]], 8][-1]\n\n8\n\n\n\n\nYour turn:\n\nHow could you extract the string 'some string' from the list [3, ['b', 'e', 3.9, ['some string', 9.9]], 8]?\n\nYou can also slice a list:\n\n[3, ['b', 'e', 3.9, ['some string', 9.9]], 8][0:1]\n\n[3]\n\n\n\nNotice how slicing returns a list.\nNotice also how the left index is included but the right index excluded.\n\nIf you omit the first index, the slice starts at the beginning of the list:\n\n[1, 2, 3, 4, 5, 6, 7, 8, 9][:6]\n\n[1, 2, 3, 4, 5, 6]\n\n\nIf you omit the second index, the slice goes to the end of the list:\n\n[1, 2, 3, 4, 5, 6, 7, 8, 9][6:]\n\n[7, 8, 9]\n\n\nWhen slicing, you can specify the stride:\n\n[1, 2, 3, 4, 5, 6, 7, 8, 9][2:7:2]\n\n[3, 5, 7]\n\n\n\nThe default stride is 1:\n\n[1, 2, 3, 4, 5, 6, 7, 8, 9][2:7] == [1, 2, 3, 4, 5, 6, 7, 8, 9][2:7:1]\n\nTrue\n\n\n\nYou can reverse the order of a list with a -1 stride applied on the whole list:\n\n[1, 2, 3, 4, 5, 6, 7, 8, 9][::-1]\n\n[9, 8, 7, 6, 5, 4, 3, 2, 1]\n\n\nYou can test whether an item is in a list:\n\n3 in [3, ['b', 'e', 3.9, ['some string', 9.9]], 8]\n\nTrue\n\n\n\n9 in [3, ['b', 'e', 3.9, ['some string', 9.9]], 8]\n\nFalse\n\n\nor not in a list:\n\n3 not in [3, ['b', 'e', 3.9, ['some string', 9.9]], 8]\n\nFalse\n\n\nYou can get the index (position) of an item inside a list:\n\n[3, ['b', 'e', 3.9, ['some string', 9.9]], 8].index(3)\n\n0\n\n\n\nNote that this only returns the index of the first occurrence:\n\n[3, 3, ['b', 'e', 3.9, ['some string', 9.9]], 8].index(3)\n\n0\n\n\n\nLists are mutable (they can be modified). For instance, you can replace items in a list by other items:\n\nL = [3, ['b', 'e', 3.9, ['some string', 9.9]], 8]\nL\n\n[3, ['b', 'e', 3.9, ['some string', 9.9]], 8]\n\n\n\nL[1] = 2\nL\n\n[3, 2, 8]\n\n\nYou can delete items from a list using their indices with list.pop:\n\nL.pop(2)\nL\n\n[3, 2]\n\n\n\nHere, because we are using list.pop, 2 represents the index (the 3rd item).\n\nor with del:\n\ndel L[0]\nL\n\n[2]\n\n\n\nNotice how a list can have a single item:\n\nlen(L)\n\n1\n\n\nIt is then called a “singleton list”.\n\nYou can also delete items from a list using their values with list.remove:\n\nL.remove(2)\nL\n\n[]\n\n\n\nHere, because we are using list.remove, 2 is the value 2.\n\n\nNotice how a list can even be empty:\n\nlen(L)\n\n0\n\n\nYou can actually initialise empty lists:\n\nM = []\ntype(M)\n\nlist\n\n\n\nYou can add items to a list. One at a time:\n\nL.append(7)\nL\n\n[7]\n\n\nAnd if you want to add multiple items at once?\n\n# This doesn't work...\nL.append(3, 6, 9)\n\nTypeError: list.append() takes exactly one argument (3 given)\n\n\n\n# This doesn't work either (that's not what we wanted)\nL.append([3, 6, 9])\nL\n\n[7, [3, 6, 9]]\n\n\n\n\nYour turn:\n\nFix this mistake we just made and remove the nested list [3, 6, 9].\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nOne option is:\n\ndel L[1]\n\n\n\n\nTo add multiple values to a list (and not a nested list), you need to use list.extend:\n\nL.extend([3, 6, 9])\nL\n\n[7, 3, 6, 9]\n\n\nIf you don’t want to add an item at the end of a list, you can use list.insert(&lt;index&gt;, &lt;object&gt;):\n\nL.insert(3, 'test')\nL\n\n[7, 3, 6, 'test', 9]\n\n\n\n\nYour turn:\n\nLet’s have the following list:\n\nL = [7, [3, 6, 9], 3, 'test', 6, 9]\n\nInsert the string 'nested' in the zeroth position of the nested list [3, 6, 9] in L.\n\nYou can sort an homogeneous list:\n\nL = [3, 9, 10, 0]\nL.sort()\nL\n\n[0, 3, 9, 10]\n\n\n\nL = ['some string', 'b', 'a']\nL.sort()\nL\n\n['a', 'b', 'some string']\n\n\n\nHeterogeneous lists cannot be sorted:\n\nL = [3, ['b', 'e', 3.9, ['some string', 9.9]], 8]\nL.sort()\n\nTypeError: '&lt;' not supported between instances of 'list' and 'int'\n\n\n\nYou can also get the min and max value of homogeneous lists:\n\nmin([3, 9, 10, 0])\n\n0\n\n\n\nmax(['some string', 'b', 'a'])\n\n'some string'\n\n\n\nFor heterogeneous lists, this also doesn’t work:\n\nmin([3, ['b', 'e', 3.9, ['some string', 9.9]], 8])\n\nTypeError: '&lt;' not supported between instances of 'list' and 'int'\n\n\n\nLists can be concatenated with +:\n\nL + [3, 6, 9]\n\n[3, ['b', 'e', 3.9, ['some string', 9.9]], 8, 3, 6, 9]\n\n\nor repeated with *:\n\nL * 3\n\n[3,\n ['b', 'e', 3.9, ['some string', 9.9]],\n 8,\n 3,\n ['b', 'e', 3.9, ['some string', 9.9]],\n 8,\n 3,\n ['b', 'e', 3.9, ['some string', 9.9]],\n 8]\n\n\n\nTo sum up, lists are declared in square brackets. They are mutable, ordered (thus indexable), and possibly heterogeneous collections of values.",
    "crumbs": [
      "Python",
      "<b><em>Getting started</em></b>",
      "Collections"
    ]
  },
  {
    "objectID": "python/intro_collections.html#strings",
    "href": "python/intro_collections.html#strings",
    "title": "Collections",
    "section": "Strings",
    "text": "Strings\nStrings behave (a little) like lists of characters in that they have a length (the number of characters):\n\nS = 'This is a string.'\nlen(S)\n\n17\n\n\nThey have a min and a max:\n\nmin(S)\n\n' '\n\n\n\nmax(S)\n\n't'\n\n\nYou can index them:\n\nS[3]\n\n's'\n\n\nSlice them:\n\nS[10:16]\n\n'string'\n\n\n\n\nYour turn:\n\nReverse the order of the string S.\n\nThey can also be concatenated with +:\n\nT = 'This is another string.'\nprint(S + ' ' + T)\n\nThis is a string. This is another string.\n\n\nor repeated with *:\n\nprint(S * 3)\n\nThis is a string.This is a string.This is a string.\n\n\n\nprint((S + ' ') * 3)\n\nThis is a string. This is a string. This is a string. \n\n\nThis is where the similarities stop however: methods such as list.sort, list.append, etc. will not work on strings.",
    "crumbs": [
      "Python",
      "<b><em>Getting started</em></b>",
      "Collections"
    ]
  },
  {
    "objectID": "python/intro_collections.html#arrays",
    "href": "python/intro_collections.html#arrays",
    "title": "Collections",
    "section": "Arrays",
    "text": "Arrays\nPython comes with a built-in array module. When you need arrays for storing and retrieving data, this module is perfectly suitable and extremely lightweight. This tutorial covers the syntax in detail.\nWhenever you plan on performing calculations on your data however (which is the vast majority of cases), you should instead use the NumPy package, covered in another section.",
    "crumbs": [
      "Python",
      "<b><em>Getting started</em></b>",
      "Collections"
    ]
  },
  {
    "objectID": "python/intro_collections.html#tuples",
    "href": "python/intro_collections.html#tuples",
    "title": "Collections",
    "section": "Tuples",
    "text": "Tuples\nTuples are defined with parentheses:\n\nt = (3, 1, 4, 2)\nt\n\n(3, 1, 4, 2)\n\n\n\ntype(t)\n\ntuple\n\n\nTuples are ordered:\n\n(2, 3) == (3, 2)\n\nFalse\n\n\nThis means that they are indexable and sliceable:\n\n(2, 4, 6)[2]\n\n6\n\n\n\n(2, 4, 6)[::-1]\n\n(6, 4, 2)\n\n\nThey can be nested:\n\ntype((3, 1, (0, 2)))\n\ntuple\n\n\n\nlen((3, 1, (0, 2)))\n\n3\n\n\n\nmax((3, 1, 2))\n\n3\n\n\nThey can be heterogeneous:\n\ntype(('string', 2, True))\n\ntuple\n\n\nYou can create empty tuples:\n\ntype(())\n\ntuple\n\n\nYou can also create singleton tuples, but the syntax is a bit odd:\n\n# This is not a tuple...\ntype((1))\n\nint\n\n\n\n# This is the weird way to define a singleton tuple\ntype((1,))\n\ntuple\n\n\nHowever, the big difference with lists is that tuples are immutable:\n\nT = (2, 5)\nT[0] = 8\n\nTypeError: 'tuple' object does not support item assignment\n\n\nTuples are quite fascinating:\n\na, b = 1, 2\na, b\n\n(1, 2)\n\n\n\na, b = b, a\na, b\n\n(2, 1)\n\n\n\nTuples are declared in parentheses. They are immutable, ordered (thus indexable), and possibly heterogeneous collections of values.",
    "crumbs": [
      "Python",
      "<b><em>Getting started</em></b>",
      "Collections"
    ]
  },
  {
    "objectID": "python/intro_collections.html#sets",
    "href": "python/intro_collections.html#sets",
    "title": "Collections",
    "section": "Sets",
    "text": "Sets\nSets are declared in curly braces:\n\ns = {3, 2, 5}\ns\n\n{2, 3, 5}\n\n\n\ntype(s)\n\nset\n\n\nSets are unordered:\n\n{2, 4, 1} == {4, 2, 1}\n\nTrue\n\n\nConsequently, it makes no sense to index a set.\nSets can be heterogeneous:\n\nS = {2, 'a', 'string'}\nisinstance(S, set)\n\nTrue\n\n\n\ntype(2) == type('a') == type('string')\n\nFalse\n\n\nThere are no duplicates in a set:\n\n{2, 2, 'a', 2, 'string', 'a'}\n\n{2, 'a', 'string'}\n\n\nYou can define an empty set, but only with the set function (because empty curly braces define a dictionary):\n\nt = set()\nt\n\nset()\n\n\n\nlen(t)\n\n0\n\n\n\ntype(t)\n\nset\n\n\nSince strings an iterables, you can use set to get a set of the unique characters:\n\nset('abba')\n\n{'a', 'b'}\n\n\n\n\nYour turn:\n\nHow could you create a set with the single element 'abba' in it?\n\n\nSets are declared in curly brackets. They are mutable, unordered (thus non indexable), possibly heterogeneous collections of unique values.",
    "crumbs": [
      "Python",
      "<b><em>Getting started</em></b>",
      "Collections"
    ]
  },
  {
    "objectID": "python/intro_collections.html#dictionaries",
    "href": "python/intro_collections.html#dictionaries",
    "title": "Collections",
    "section": "Dictionaries",
    "text": "Dictionaries\nDictionaries are declared in curly braces. They associate values to keys:\n\nd = {'key1': 'value1', 'key2': 'value2'}\nd\n\n{'key1': 'value1', 'key2': 'value2'}\n\n\n\ntype(d)\n\ndict\n\n\nDictionaries are unordered:\n\n{'a': 1, 'b': 2} == {'b': 2, 'a': 1}\n\nTrue\n\n\nConsequently, the pairs themselves cannot be indexed. However, you can access values in a dictionary from their keys:\n\nD = {'c': 1, 'a': 3, 'b': 2}\nD['b']\n\n2\n\n\n\nD.get('b')\n\n2\n\n\n\nD.items()\n\ndict_items([('c', 1), ('a', 3), ('b', 2)])\n\n\n\nD.values()\n\ndict_values([1, 3, 2])\n\n\n\nD.keys()\n\ndict_keys(['c', 'a', 'b'])\n\n\nTo return a sorted list of keys:\n\nsorted(D)\n\n['a', 'b', 'c']\n\n\nYou can create empty dictionaries:\n\nE = {}\ntype(E)\n\ndict\n\n\nDictionaries are mutable, so you can add, remove, or replace items.\nLet’s add an item to our empty dictionary E:\n\nE['author'] = 'Proust'\nE\n\n{'author': 'Proust'}\n\n\nWe can add another one:\n\nE['title'] = 'In search of lost time'\nE\n\n{'author': 'Proust', 'title': 'In search of lost time'}\n\n\nWe can modify one:\n\nE['author'] = 'Marcel Proust'\nE\n\n{'author': 'Marcel Proust', 'title': 'In search of lost time'}\n\n\n\n\nYour turn:\n\nAdd a third item to E with the number of volumes.\n\nWe can also remove items:\n\nE.pop('author')\nE\n\n{'title': 'In search of lost time'}\n\n\nAnother method to remove items:\n\ndel E['title']\nE\n\n{}\n\n\n\nDictionaries are declared in curly braces. They are mutable and unordered collections of key/value pairs. They play the role of an associative array.",
    "crumbs": [
      "Python",
      "<b><em>Getting started</em></b>",
      "Collections"
    ]
  },
  {
    "objectID": "python/intro_collections.html#conversion-between-collections",
    "href": "python/intro_collections.html#conversion-between-collections",
    "title": "Collections",
    "section": "Conversion between collections",
    "text": "Conversion between collections\nFrom tuple to list:\n\nlist((3, 8, 1))\n\n[3, 8, 1]\n\n\nFrom tuple to set:\n\nset((3, 2, 3, 3))\n\n{2, 3}\n\n\nFrom list to tuple:\n\ntuple([3, 1, 4])\n\n(3, 1, 4)\n\n\nFrom list to set:\n\nset(['a', 2, 4])\n\n{2, 4, 'a'}\n\n\nFrom set to tuple:\n\ntuple({2, 3})\n\n(2, 3)\n\n\nFrom set to list:\n\nlist({2, 3})\n\n[2, 3]",
    "crumbs": [
      "Python",
      "<b><em>Getting started</em></b>",
      "Collections"
    ]
  },
  {
    "objectID": "python/intro_collections.html#collections-module",
    "href": "python/intro_collections.html#collections-module",
    "title": "Collections",
    "section": "Collections module",
    "text": "Collections module\nPython has a built-in collections module providing the additional data structures: deque, defaultdict, namedtuple, OrderedDict, Counter, ChainMap, UserDict, UserList, and UserList.",
    "crumbs": [
      "Python",
      "<b><em>Getting started</em></b>",
      "Collections"
    ]
  },
  {
    "objectID": "python/intro_functions.html",
    "href": "python/intro_functions.html",
    "title": "Writing functions",
    "section": "",
    "text": "Python comes with a number of built-in functions. Packages can provide additional ones. In many cases however, you will want to create your own functions to perform exactly the computations that you need.\nIn this section, we will see how to define new functions.",
    "crumbs": [
      "Python",
      "<b><em>Getting started</em></b>",
      "Functions"
    ]
  },
  {
    "objectID": "python/intro_functions.html#syntax",
    "href": "python/intro_functions.html#syntax",
    "title": "Writing functions",
    "section": "Syntax",
    "text": "Syntax\nThe function definition syntax follows:\ndef &lt;name&gt;(&lt;arguments&gt;):\n    &lt;body&gt;\nOnce defined, new functions can be used as any other function.\nLet’s give this a try by creating some greeting functions.",
    "crumbs": [
      "Python",
      "<b><em>Getting started</em></b>",
      "Functions"
    ]
  },
  {
    "objectID": "python/intro_functions.html#function-without-argument",
    "href": "python/intro_functions.html#function-without-argument",
    "title": "Writing functions",
    "section": "Function without argument",
    "text": "Function without argument\nLet’s start with the simple case in which our function does not accept any argument:\n\ndef hello():\n    print('Hello')\n\nThen we call it:\n\nhello()\n\nHello\n\n\nThis was great, but …\n\nhello('Marie')\n\nTypeError: hello() takes 0 positional arguments but 1 was given\n\n\n… it does not accept arguments.",
    "crumbs": [
      "Python",
      "<b><em>Getting started</em></b>",
      "Functions"
    ]
  },
  {
    "objectID": "python/intro_functions.html#function-with-one-argument",
    "href": "python/intro_functions.html#function-with-one-argument",
    "title": "Writing functions",
    "section": "Function with one argument",
    "text": "Function with one argument\nLet’s step this up with a function which can accept an argument:\n\ndef greetings(name):\n    print('Hello ' + name)\n\nThis time, this works:\n\ngreetings('Marie')\n\nHello Marie\n\n\nHowever, this does not work anymore:\n\ngreetings()\n\nTypeError: greetings() missing 1 required positional argument: 'name'\n\n\n:(",
    "crumbs": [
      "Python",
      "<b><em>Getting started</em></b>",
      "Functions"
    ]
  },
  {
    "objectID": "python/intro_functions.html#function-with-a-facultative-argument",
    "href": "python/intro_functions.html#function-with-a-facultative-argument",
    "title": "Writing functions",
    "section": "Function with a facultative argument",
    "text": "Function with a facultative argument\nLet’s make this even more fancy: a function with a facultative argument. That is, a function which accepts an argument, but also has a default value for when we do not provide any argument:\n\ndef howdy(name='you'):\n    print('Hello ' + name)\n\nWe can call it without argument (making use of the default value):\n\nhowdy()\n\nHello you\n\n\nAnd we can call it with an argument:\n\nhowdy('Marie')\n\nHello Marie\n\n\nThis was better, but …\n\nhowdy('Marie', 'Paul')\n\nTypeError: howdy() takes from 0 to 1 positional arguments but 2 were given\n\n\n… this does not work.",
    "crumbs": [
      "Python",
      "<b><em>Getting started</em></b>",
      "Functions"
    ]
  },
  {
    "objectID": "python/intro_functions.html#function-with-two-arguments",
    "href": "python/intro_functions.html#function-with-two-arguments",
    "title": "Writing functions",
    "section": "Function with two arguments",
    "text": "Function with two arguments\nWe could create a function which takes two arguments:\n\ndef hey(name1, name2):\n    print('Hello ' + name1 + ', ' + name2)\n\nWhich solves our problem:\n\nhey('Marie', 'Paul')\n\nHello Marie, Paul\n\n\nBut it is terribly limiting:\n\n# This doesn't work\nhey()\n\nTypeError: hey() missing 2 required positional arguments: 'name1' and 'name2'\n\n\n\n# And neither does this\nhey('Marie')\n\nTypeError: hey() missing 1 required positional argument: 'name2'\n\n\n\n# Nor to mention this...\nhey('Marie', 'Paul', 'Alex')\n\nTypeError: hey() takes 2 positional arguments but 3 were given",
    "crumbs": [
      "Python",
      "<b><em>Getting started</em></b>",
      "Functions"
    ]
  },
  {
    "objectID": "python/intro_functions.html#function-with-any-number-of-arguments",
    "href": "python/intro_functions.html#function-with-any-number-of-arguments",
    "title": "Writing functions",
    "section": "Function with any number of arguments",
    "text": "Function with any number of arguments\nLet’s create a truly great function which handles all our cases:\n\ndef hi(name='you', *args):\n    result = ''\n    for i in args:\n        result += (', ' + i)\n    print('Hello ' + name + result)\n\nAnd let’s test it:\n\nhi()\nhi('Marie')\nhi('Marie', 'Paul')\nhi('Marie', 'Paul', 'Alex')\n\nHello you\nHello Marie\nHello Marie, Paul\nHello Marie, Paul, Alex\n\n\nEverything works!",
    "crumbs": [
      "Python",
      "<b><em>Getting started</em></b>",
      "Functions"
    ]
  },
  {
    "objectID": "python/intro_functions.html#documenting-functions",
    "href": "python/intro_functions.html#documenting-functions",
    "title": "Writing functions",
    "section": "Documenting functions",
    "text": "Documenting functions\nIt is a good habit to document what your functions do. As with comments, those “documentation strings” or “docstrings” will help future you or other users of your code.\nPEP 257—docstring conventions—suggests to use single-line docstrings surrounded by triple quotes.\nRemember the function definition syntax we saw at the start of this chapter? To be more exhaustive, we should have written it this way:\ndef &lt;name&gt;(&lt;arguments&gt;):\n    \"\"\"&lt;docstrings&gt;\"\"\"\n    &lt;body&gt;\n\nExample:\n\n\ndef hi(name='you', *args):\n    \"\"\"Print a greeting\"\"\"\n    result = ''\n    for i in args:\n        result += (', ' + i)\n    print('Hello ' + name + result)\n\nPEP 8—the style guide for Python code—suggests a maximum of 72 characters per line for docstrings.\nIf your docstring is longer, you should create a multi-line one. In that case, PEP 257 suggests to have a summary line at the top (right after the opening set of triple quotes), then leave a blank line, then have your long docstrings (which can occupy multiple lines), and finally have the closing set of triple quotes on a line of its own:\ndef &lt;name&gt;(&lt;arguments&gt;):\n    \"\"\"&lt;summary docstrings line&gt;\"\"\"\n\n    &lt;more detailed description&gt;\n    \"\"\"\n    &lt;body&gt;\n\nExample:\n\n\ndef hi(name='you', *args):\n    \"\"\"Print a greeting\n\n    Accepts any number of arguments\n    \"\"\"\n    result = ''\n    for i in args:\n        result += (', ' + i)\n    print('Hello ' + name + result)\n\n\nYou can (and should) document modules, classes, and methods in the same way.\n\nYou can now access the documentation of your function as you would any Python function:\n\nhelp(hi)\n\nHelp on function hi in module __main__:\n\nhi(name='you', *args)\n    Print a greeting\n    \n    Accepts any number of arguments\n\n\n\nOr:\n\nprint(hi.__doc__)\n\nPrint a greeting\n\n    Accepts any number of arguments",
    "crumbs": [
      "Python",
      "<b><em>Getting started</em></b>",
      "Functions"
    ]
  },
  {
    "objectID": "python/intro_packages.html",
    "href": "python/intro_packages.html",
    "title": "Modules, packages, and libraries",
    "section": "",
    "text": "“Modules” are Python files containing reusable code (e.g. functions, constants, utilities).\n“Packages” are collections of modules.\n“Libraries”, technically, are collections of packages, although “packages” and “libraries” are often used loosely and interchangeably in Python."
  },
  {
    "objectID": "python/intro_packages.html#definitions",
    "href": "python/intro_packages.html#definitions",
    "title": "Modules, packages, and libraries",
    "section": "",
    "text": "“Modules” are Python files containing reusable code (e.g. functions, constants, utilities).\n“Packages” are collections of modules.\n“Libraries”, technically, are collections of packages, although “packages” and “libraries” are often used loosely and interchangeably in Python."
  },
  {
    "objectID": "python/intro_packages.html#installing-external-packages",
    "href": "python/intro_packages.html#installing-external-packages",
    "title": "Modules, packages, and libraries",
    "section": "Installing external packages",
    "text": "Installing external packages\nYou can install external packages containing additional functions, constants, and utilities to extend the capabilities of Python.\nThe Python Package Index is a public repository of open source packages contributed by users.\nInstallation of packages can be done via pip:\npip install --no-index --upgrade pip\npython -m pip install &lt;package&gt;\nOn your local machine, particularly if you are on Windows and want to install a complex software stack, conda can makes things easy by installing from the Anaconda Distribution. This is however never what you want to do when using the Alliance clusters.\nOn the clusters, you always want to:\n\ncreate a virtual environment,\ninstall packages in it with pip."
  },
  {
    "objectID": "python/intro_packages.html#virtual-environments",
    "href": "python/intro_packages.html#virtual-environments",
    "title": "Modules, packages, and libraries",
    "section": "Virtual environments",
    "text": "Virtual environments\nInstead of installing packages system wide or for your user, you can create a semi-isolated Python environment in which you install the packages needed for a particular project. This makes reproducibility and collaboration easier. It also helps handle dependency conflicts."
  },
  {
    "objectID": "python/intro_packages.html#installing-packages-on-the-clusters",
    "href": "python/intro_packages.html#installing-packages-on-the-clusters",
    "title": "Modules, packages, and libraries",
    "section": "Installing packages on the clusters",
    "text": "Installing packages on the clusters\nOn the Alliance clusters, you should install packages inside a virtual environment."
  },
  {
    "objectID": "python/intro_run.html",
    "href": "python/intro_run.html",
    "title": "Running Python",
    "section": "",
    "text": "There are many ways to run Python interactively. For this workshop, you have two options:",
    "crumbs": [
      "Python",
      "<b><em>Getting started</em></b>",
      "Running Python"
    ]
  },
  {
    "objectID": "python/intro_run.html#use-your-own-machine",
    "href": "python/intro_run.html#use-your-own-machine",
    "title": "Running Python",
    "section": "Use your own machine",
    "text": "Use your own machine\nIf you have installed Python 3.7 or greater and the packages NumPy, Matplotlib, scikit-image, pandas, Xarray, you can simply run this workshop on your machine, either directly in the Python shell or, if you also have Jupyter, in JupyterLab.",
    "crumbs": [
      "Python",
      "<b><em>Getting started</em></b>",
      "Running Python"
    ]
  },
  {
    "objectID": "python/intro_run.html#use-our-temporary-jupyterhub",
    "href": "python/intro_run.html#use-our-temporary-jupyterhub",
    "title": "Running Python",
    "section": "Use our temporary JupyterHub",
    "text": "Use our temporary JupyterHub\nIf you don’t have the required software on your machine, this is the easiest solution.\n\nLog in to the JupyterHub on our training cluster\n\nGo to https://jupyter.bobthewren.c3.ca,\nSign in with the username and password you just got,\nSet the server options according to the image below:\n\n\n\nThese are the only values that you should edit:\nChange the time to 7.0\n\n\nPress start.\n\n\nNote that, unlike other JupyterHubs you might have used (e.g. Syzygy), this JupyterHub is not permanent and can only be used for this course.\n\nIf you don’t need all the time you asked for after all, it is a great thing to log out (the resources you are using on this cluster are shared amongst many people and when resources are allocated to you, they aren’t available to other people. So it is a good thing not to ask for unnecessary resources and have them sit idle when others could be using them).\nTo log out, click on “File” in the top menu and select “Log out” at the very bottom.\nIf you would like to make a change to the information you entered on the server option page after you have pressed “start”, log out in the same way, log back in, edit the server options, and press start again.\n\n\nStart a Python notebook\nTo start a Jupyter notebook with the Python kernel, click on the button “Python 3” in the “Notebook” section (top row of buttons).",
    "crumbs": [
      "Python",
      "<b><em>Getting started</em></b>",
      "Running Python"
    ]
  },
  {
    "objectID": "python/top_intro.html",
    "href": "python/top_intro.html",
    "title": "Getting started in Python",
    "section": "",
    "text": "This introductory course in Python does not assume any prior programming experience.\n\n Start course ➤",
    "crumbs": [
      "Python",
      "<b><em>Getting started</em></b>"
    ]
  },
  {
    "objectID": "python/wb_jax.html",
    "href": "python/wb_jax.html",
    "title": "Accelerated array computing and flexible differentiation with JAX",
    "section": "",
    "text": "JAX can be used for any accelerated computing with arrays, but because its flexible automatic differentiation capabilities make it a great starting point to build deep learning libraries, please find this webinar in the AI section.",
    "crumbs": [
      "Python",
      "<em><b>Webinars</b></em>",
      "Accelerated arrays & AD with JAX"
    ]
  },
  {
    "objectID": "python/wb_polars_slides.html#section",
    "href": "python/wb_polars_slides.html#section",
    "title": "DataFrames on steroids with Polars",
    "section": "",
    "text": "Polars is a DataFrame library written in Rust, available for Rust, Python, R, and NodeJS"
  },
  {
    "objectID": "python/wb_polars_slides.html#why-a-new-dataframe-library",
    "href": "python/wb_polars_slides.html#why-a-new-dataframe-library",
    "title": "DataFrames on steroids with Polars",
    "section": "Why a new DataFrame library?",
    "text": "Why a new DataFrame library?\nPolars is much more performant than pandas:\n\nuses Apache Arrow to process queries in a vectorized fashion\nuses SIMD for CPU usage optimization\ncan handle datasets too big to fit in RAM"
  },
  {
    "objectID": "python/ws_polars.html",
    "href": "python/ws_polars.html",
    "title": "DataFrames with Polars",
    "section": "",
    "text": "Polars is a Python library built to manipulate data frames and time series.\nFor this section, we will use the Covid-19 data from the Johns Hopkins University CSSE repository.\nYou can visualize this data in a dashboard created by the Johns Hopkins University Center for Systems Science and Engineering."
  },
  {
    "objectID": "python/ws_polars.html#setup",
    "href": "python/ws_polars.html#setup",
    "title": "DataFrames with Polars",
    "section": "Setup",
    "text": "Setup\nFirst, we need to load the polars library and read in the data from the web:\n\n# Load the Polars library and create a shorter name for it\nimport polars as pl\n\n# The global confirmed cases are available in CSV format at the url:\nurl = \"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv\"\n\n# Polars allows to read in data from the web directly\ncases = pl.read_csv(url)"
  },
  {
    "objectID": "python/ws_polars.html#first-look-at-the-data",
    "href": "python/ws_polars.html#first-look-at-the-data",
    "title": "DataFrames with Polars",
    "section": "First look at the data",
    "text": "First look at the data\nWhat does our data look like?\n\ncases\n\n\n\nshape: (289, 1_147)\n\n\n\nProvince/State\nCountry/Region\nLat\nLong\n1/22/20\n1/23/20\n1/24/20\n1/25/20\n1/26/20\n1/27/20\n1/28/20\n1/29/20\n1/30/20\n1/31/20\n2/1/20\n2/2/20\n2/3/20\n2/4/20\n2/5/20\n2/6/20\n2/7/20\n2/8/20\n2/9/20\n2/10/20\n2/11/20\n2/12/20\n2/13/20\n2/14/20\n2/15/20\n2/16/20\n2/17/20\n2/18/20\n2/19/20\n2/20/20\n2/21/20\n2/22/20\n2/23/20\n…\n2/1/23\n2/2/23\n2/3/23\n2/4/23\n2/5/23\n2/6/23\n2/7/23\n2/8/23\n2/9/23\n2/10/23\n2/11/23\n2/12/23\n2/13/23\n2/14/23\n2/15/23\n2/16/23\n2/17/23\n2/18/23\n2/19/23\n2/20/23\n2/21/23\n2/22/23\n2/23/23\n2/24/23\n2/25/23\n2/26/23\n2/27/23\n2/28/23\n3/1/23\n3/2/23\n3/3/23\n3/4/23\n3/5/23\n3/6/23\n3/7/23\n3/8/23\n3/9/23\n\n\nstr\nstr\nf64\nf64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\n…\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\n\n\n\n\nnull\n\"Afghanistan\"\n33.93911\n67.709953\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n…\n208552\n208669\n208669\n208621\n208627\n208704\n208721\n208771\n208771\n208943\n208971\n208982\n209011\n209036\n209056\n209072\n209083\n209084\n209107\n209153\n209181\n209181\n209215\n209230\n209246\n209274\n209308\n209322\n209340\n209358\n209362\n209369\n209390\n209406\n209436\n209451\n209451\n\n\nnull\n\"Albania\"\n41.1533\n20.1683\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n…\n334177\n334187\n334203\n334204\n334211\n334211\n334211\n334222\n334229\n334229\n334234\n334255\n334255\n334264\n334264\n334273\n334291\n334305\n334314\n334315\n334336\n334336\n334345\n334356\n334373\n334378\n334380\n334391\n334408\n334408\n334427\n334427\n334427\n334427\n334427\n334443\n334457\n\n\nnull\n\"Algeria\"\n28.0339\n1.6596\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n…\n271385\n271386\n271394\n271394\n271394\n271395\n271399\n271403\n271406\n271406\n271409\n271409\n271409\n271409\n271421\n271424\n271424\n271425\n271425\n271426\n271428\n271431\n271432\n271436\n271439\n271440\n271440\n271441\n271448\n271463\n271469\n271469\n271477\n271477\n271490\n271494\n271496\n\n\nnull\n\"Andorra\"\n42.5063\n1.5218\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n…\n47839\n47839\n47850\n47850\n47850\n47850\n47850\n47850\n47860\n47860\n47860\n47860\n47860\n47860\n47860\n47866\n47866\n47866\n47866\n47866\n47866\n47866\n47866\n47866\n47866\n47866\n47866\n47866\n47875\n47875\n47875\n47875\n47875\n47875\n47875\n47890\n47890\n\n\nnull\n\"Angola\"\n-11.2027\n17.8739\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n…\n105184\n105184\n105184\n105184\n105184\n105184\n105184\n105184\n105184\n105184\n105184\n105184\n105184\n105184\n105184\n105184\n105184\n105184\n105184\n105184\n105184\n105205\n105205\n105205\n105205\n105205\n105255\n105255\n105277\n105277\n105277\n105277\n105277\n105277\n105277\n105288\n105288\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\nnull\n\"West Bank and …\n31.9522\n35.2332\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n…\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n\n\nnull\n\"Winter Olympic…\n39.9042\n116.4074\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n…\n535\n535\n535\n535\n535\n535\n535\n535\n535\n535\n535\n535\n535\n535\n535\n535\n535\n535\n535\n535\n535\n535\n535\n535\n535\n535\n535\n535\n535\n535\n535\n535\n535\n535\n535\n535\n535\n\n\nnull\n\"Yemen\"\n15.552727\n48.516388\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n…\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n\n\nnull\n\"Zambia\"\n-13.133897\n27.849332\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n…\n340944\n341261\n341397\n341573\n341573\n341690\n341690\n341936\n342114\n342114\n342288\n342288\n342317\n342317\n342317\n342317\n342317\n342317\n342317\n342724\n342782\n342831\n342831\n342831\n342831\n342831\n342831\n343012\n343012\n343079\n343079\n343079\n343135\n343135\n343135\n343135\n343135\n\n\nnull\n\"Zimbabwe\"\n-19.015438\n29.154857\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n…\n262324\n262324\n262324\n262324\n262324\n262324\n262324\n263083\n263083\n263083\n263083\n263083\n263083\n263083\n263642\n263642\n263642\n263642\n263642\n263642\n263642\n263921\n263921\n263921\n263921\n263921\n263921\n263921\n264127\n264127\n264127\n264127\n264127\n264127\n264127\n264276\n264276\n\n\n\n\n\n\n\n\n# Quick summary of the data\ncases.describe()\n\n\n\nshape: (9, 1_148)\n\n\n\nstatistic\nProvince/State\nCountry/Region\nLat\nLong\n1/22/20\n1/23/20\n1/24/20\n1/25/20\n1/26/20\n1/27/20\n1/28/20\n1/29/20\n1/30/20\n1/31/20\n2/1/20\n2/2/20\n2/3/20\n2/4/20\n2/5/20\n2/6/20\n2/7/20\n2/8/20\n2/9/20\n2/10/20\n2/11/20\n2/12/20\n2/13/20\n2/14/20\n2/15/20\n2/16/20\n2/17/20\n2/18/20\n2/19/20\n2/20/20\n2/21/20\n2/22/20\n…\n2/1/23\n2/2/23\n2/3/23\n2/4/23\n2/5/23\n2/6/23\n2/7/23\n2/8/23\n2/9/23\n2/10/23\n2/11/23\n2/12/23\n2/13/23\n2/14/23\n2/15/23\n2/16/23\n2/17/23\n2/18/23\n2/19/23\n2/20/23\n2/21/23\n2/22/23\n2/23/23\n2/24/23\n2/25/23\n2/26/23\n2/27/23\n2/28/23\n3/1/23\n3/2/23\n3/3/23\n3/4/23\n3/5/23\n3/6/23\n3/7/23\n3/8/23\n3/9/23\n\n\nstr\nstr\nstr\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\n…\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nf64\n\n\n\n\n\"count\"\n\"91\"\n\"289\"\n287.0\n287.0\n289.0\n289.0\n289.0\n289.0\n289.0\n289.0\n289.0\n289.0\n289.0\n289.0\n289.0\n289.0\n289.0\n289.0\n289.0\n289.0\n289.0\n289.0\n289.0\n289.0\n289.0\n289.0\n289.0\n289.0\n289.0\n289.0\n289.0\n289.0\n289.0\n289.0\n289.0\n289.0\n…\n289.0\n289.0\n289.0\n289.0\n289.0\n289.0\n289.0\n289.0\n289.0\n289.0\n289.0\n289.0\n289.0\n289.0\n289.0\n289.0\n289.0\n289.0\n289.0\n289.0\n289.0\n289.0\n289.0\n289.0\n289.0\n289.0\n289.0\n289.0\n289.0\n289.0\n289.0\n289.0\n289.0\n289.0\n289.0\n289.0\n289.0\n\n\n\"null_count\"\n\"198\"\n\"0\"\n2.0\n2.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n…\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n\"mean\"\nnull\nnull\n19.718719\n22.182084\n1.927336\n2.273356\n3.266436\n4.972318\n7.33564\n10.134948\n19.307958\n21.346021\n28.50173\n34.349481\n41.653979\n58.086505\n68.813149\n82.695502\n95.653979\n106.595156\n119.031142\n128.480969\n138.968858\n147.99654\n155.065744\n156.512111\n208.941176\n231.529412\n238.944637\n246.49827\n253.539792\n260.051903\n261.782007\n263.723183\n265.903114\n272.0\n…\n2.3219e6\n2.3228e6\n2.3236e6\n2.3240e6\n2.3243e6\n2.3248e6\n2.3255e6\n2.3263e6\n2.3272e6\n2.3277e6\n2.3281e6\n2.3284e6\n2.3289e6\n2.3295e6\n2.3304e6\n2.3311e6\n2.3318e6\n2.3321e6\n2.3324e6\n2.3327e6\n2.3333e6\n2.3342e6\n2.3349e6\n2.3354e6\n2.3356e6\n2.3358e6\n2.3362e6\n2.3368e6\n2.3375e6\n2.3382e6\n2.3388e6\n2.3390e6\n2.3392e6\n2.3394e6\n2.3398e6\n2.3405e6\n2.3411e6\n\n\n\"std\"\nnull\nnull\n25.956609\n77.870931\n26.173664\n26.270191\n32.707271\n45.523871\n63.623197\n85.724481\n210.329649\n211.628535\n291.630499\n345.759248\n425.471558\n661.171033\n799.258224\n985.038344\n1160.785189\n1304.872173\n1472.006165\n1598.378021\n1747.162098\n1870.449899\n1966.785835\n1967.121793\n2838.127785\n3202.384554\n3310.754868\n3424.390905\n3530.604364\n3630.11703\n3650.660287\n3674.805102\n3687.812705\n3771.362586\n…\n8.4392e6\n8.4439e6\n8.4473e6\n8.4483e6\n8.4491e6\n8.4511e6\n8.4544e6\n8.4592e6\n8.4639e6\n8.4655e6\n8.4669e6\n8.4674e6\n8.4695e6\n8.4726e6\n8.4776e6\n8.4812e6\n8.4843e6\n8.4852e6\n8.4857e6\n8.4865e6\n8.4900e6\n8.4957e6\n8.4993e6\n8.5010e6\n8.5014e6\n8.5019e6\n8.5037e6\n8.5066e6\n8.5113e6\n8.5145e6\n8.5180e6\n8.5184e6\n8.5186e6\n8.5193e6\n8.5216e6\n8.5250e6\n8.5278e6\n\n\n\"min\"\n\"Alberta\"\n\"Afghanistan\"\n-71.9499\n-178.1165\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n…\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n\"25%\"\nnull\nnull\n4.210484\n-23.0418\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n…\n14567.0\n14567.0\n14567.0\n14567.0\n14567.0\n14567.0\n14567.0\n14567.0\n14567.0\n14567.0\n14567.0\n14567.0\n14567.0\n14567.0\n14567.0\n14567.0\n14567.0\n14567.0\n14567.0\n14567.0\n14567.0\n14567.0\n14567.0\n14567.0\n14567.0\n14567.0\n14567.0\n14567.0\n14567.0\n14567.0\n14567.0\n14567.0\n14567.0\n14567.0\n14567.0\n14567.0\n14567.0\n\n\n\"50%\"\nnull\nnull\n21.512583\n20.9394\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n…\n103248.0\n103248.0\n103248.0\n103248.0\n103248.0\n103248.0\n103248.0\n103248.0\n103248.0\n103248.0\n103248.0\n103248.0\n103248.0\n103248.0\n103248.0\n103248.0\n103248.0\n103248.0\n103248.0\n103248.0\n103248.0\n103248.0\n103248.0\n103248.0\n103248.0\n103248.0\n103248.0\n103248.0\n103248.0\n103248.0\n103248.0\n103248.0\n103248.0\n103248.0\n103248.0\n103248.0\n103248.0\n\n\n\"75%\"\nnull\nnull\n40.463667\n90.3563\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n…\n1.049457e6\n1.049537e6\n1.04964e6\n1.049729e6\n1.049828e6\n1.049944e6\n1.049944e6\n1.05e6\n1.050127e6\n1.050241e6\n1.05033e6\n1.050402e6\n1.050402e6\n1.050549e6\n1.050621e6\n1.050736e6\n1.050824e6\n1.050961e6\n1.051079e6\n1.051162e6\n1.051241e6\n1.051336e6\n1.051457e6\n1.051589e6\n1.051732e6\n1.051862e6\n1.05193e6\n1.051998e6\n1.052122e6\n1.052247e6\n1.052382e6\n1.052519e6\n1.052664e6\n1.052664e6\n1.052926e6\n1.053068e6\n1.053213e6\n\n\n\"max\"\n\"Zhejiang\"\n\"Zimbabwe\"\n71.7069\n178.065\n444.0\n444.0\n549.0\n761.0\n1058.0\n1423.0\n3554.0\n3554.0\n4903.0\n5806.0\n7153.0\n11177.0\n13522.0\n16678.0\n19665.0\n22112.0\n24953.0\n27100.0\n29631.0\n31728.0\n33366.0\n33366.0\n48206.0\n54406.0\n56249.0\n58182.0\n59989.0\n61682.0\n62031.0\n62442.0\n62662.0\n64084.0\n…\n1.02479379e8\n1.02561054e8\n1.02599784e8\n1.02603942e8\n1.02606868e8\n1.02631179e8\n1.02676694e8\n1.02760211e8\n1.02842061e8\n1.02855101e8\n1.0285951e8\n1.02862879e8\n1.02886005e8\n1.02934224e8\n1.03023231e8\n1.0308391e8\n1.03131898e8\n1.03134605e8\n1.03136077e8\n1.03138119e8\n1.03198669e8\n1.03308832e8\n1.03365511e8\n1.03378408e8\n1.03381157e8\n1.03382763e8\n1.03399827e8\n1.03443455e8\n1.03533872e8\n1.03589757e8\n1.0364869e8\n1.03650837e8\n1.03646975e8\n1.03655539e8\n1.0369091e8\n1.03755771e8\n1.03802702e8\n\n\n\n\n\n\n\n\nOf course, this value is meaningless for Lat and Long!\n\n\n# Data types of the various columns\n# cases.dtypes\n\n\n# cases.info()\n\n\ncases.shape\n\n(289, 1147)"
  },
  {
    "objectID": "python/ws_polars.html#cases-per-country-by-date",
    "href": "python/ws_polars.html#cases-per-country-by-date",
    "title": "DataFrames with Polars",
    "section": "Cases per country by date",
    "text": "Cases per country by date\nThe dataset is a time series: this means that we have the cumulative numbers up to each date.\n\n# Let's get rid of the latitude and longitude to simplify our data\nsimple = cases.drop(columns=['Lat', 'Long'])\nsimple\n\n/tmp/ipykernel_17571/697546061.py:2: DeprecationWarning: named `columns` param is deprecated; use positional `*args` instead.\n  simple = cases.drop(columns=['Lat', 'Long'])\n\n\n\n\nshape: (289, 1_145)\n\n\n\nProvince/State\nCountry/Region\n1/22/20\n1/23/20\n1/24/20\n1/25/20\n1/26/20\n1/27/20\n1/28/20\n1/29/20\n1/30/20\n1/31/20\n2/1/20\n2/2/20\n2/3/20\n2/4/20\n2/5/20\n2/6/20\n2/7/20\n2/8/20\n2/9/20\n2/10/20\n2/11/20\n2/12/20\n2/13/20\n2/14/20\n2/15/20\n2/16/20\n2/17/20\n2/18/20\n2/19/20\n2/20/20\n2/21/20\n2/22/20\n2/23/20\n2/24/20\n2/25/20\n…\n2/1/23\n2/2/23\n2/3/23\n2/4/23\n2/5/23\n2/6/23\n2/7/23\n2/8/23\n2/9/23\n2/10/23\n2/11/23\n2/12/23\n2/13/23\n2/14/23\n2/15/23\n2/16/23\n2/17/23\n2/18/23\n2/19/23\n2/20/23\n2/21/23\n2/22/23\n2/23/23\n2/24/23\n2/25/23\n2/26/23\n2/27/23\n2/28/23\n3/1/23\n3/2/23\n3/3/23\n3/4/23\n3/5/23\n3/6/23\n3/7/23\n3/8/23\n3/9/23\n\n\nstr\nstr\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\n…\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\ni64\n\n\n\n\nnull\n\"Afghanistan\"\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n5\n5\n…\n208552\n208669\n208669\n208621\n208627\n208704\n208721\n208771\n208771\n208943\n208971\n208982\n209011\n209036\n209056\n209072\n209083\n209084\n209107\n209153\n209181\n209181\n209215\n209230\n209246\n209274\n209308\n209322\n209340\n209358\n209362\n209369\n209390\n209406\n209436\n209451\n209451\n\n\nnull\n\"Albania\"\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n…\n334177\n334187\n334203\n334204\n334211\n334211\n334211\n334222\n334229\n334229\n334234\n334255\n334255\n334264\n334264\n334273\n334291\n334305\n334314\n334315\n334336\n334336\n334345\n334356\n334373\n334378\n334380\n334391\n334408\n334408\n334427\n334427\n334427\n334427\n334427\n334443\n334457\n\n\nnull\n\"Algeria\"\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n…\n271385\n271386\n271394\n271394\n271394\n271395\n271399\n271403\n271406\n271406\n271409\n271409\n271409\n271409\n271421\n271424\n271424\n271425\n271425\n271426\n271428\n271431\n271432\n271436\n271439\n271440\n271440\n271441\n271448\n271463\n271469\n271469\n271477\n271477\n271490\n271494\n271496\n\n\nnull\n\"Andorra\"\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n…\n47839\n47839\n47850\n47850\n47850\n47850\n47850\n47850\n47860\n47860\n47860\n47860\n47860\n47860\n47860\n47866\n47866\n47866\n47866\n47866\n47866\n47866\n47866\n47866\n47866\n47866\n47866\n47866\n47875\n47875\n47875\n47875\n47875\n47875\n47875\n47890\n47890\n\n\nnull\n\"Angola\"\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n…\n105184\n105184\n105184\n105184\n105184\n105184\n105184\n105184\n105184\n105184\n105184\n105184\n105184\n105184\n105184\n105184\n105184\n105184\n105184\n105184\n105184\n105205\n105205\n105205\n105205\n105205\n105255\n105255\n105277\n105277\n105277\n105277\n105277\n105277\n105277\n105288\n105288\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\nnull\n\"West Bank and …\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n…\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n\n\nnull\n\"Winter Olympic…\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n…\n535\n535\n535\n535\n535\n535\n535\n535\n535\n535\n535\n535\n535\n535\n535\n535\n535\n535\n535\n535\n535\n535\n535\n535\n535\n535\n535\n535\n535\n535\n535\n535\n535\n535\n535\n535\n535\n\n\nnull\n\"Yemen\"\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n…\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n\n\nnull\n\"Zambia\"\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n…\n340944\n341261\n341397\n341573\n341573\n341690\n341690\n341936\n342114\n342114\n342288\n342288\n342317\n342317\n342317\n342317\n342317\n342317\n342317\n342724\n342782\n342831\n342831\n342831\n342831\n342831\n342831\n343012\n343012\n343079\n343079\n343079\n343135\n343135\n343135\n343135\n343135\n\n\nnull\n\"Zimbabwe\"\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n…\n262324\n262324\n262324\n262324\n262324\n262324\n262324\n263083\n263083\n263083\n263083\n263083\n263083\n263083\n263642\n263642\n263642\n263642\n263642\n263642\n263642\n263921\n263921\n263921\n263921\n263921\n263921\n263921\n264127\n264127\n264127\n264127\n264127\n264127\n264127\n264276\n264276\n\n\n\n\n\n\n\n Some countries (e.g. Australia) are split between several provinces or states so we will have to add the values of all their provinces/states to get their totals.\nHere is how to make the sum for all Australian states:\nLet’s first select all the data for Australia: we want all the rows for which the Country/Region column is equal to Australia.\nFirst, we want to select the Country/Region column. There are several ways to index in Polars.\nWhen indexing columns, one can use square brackets directly after the DataFrame to index:\n\nsimple['Country/Region']\n\n\n\nshape: (289,)\n\n\n\nCountry/Region\n\n\nstr\n\n\n\n\n\"Afghanistan\"\n\n\n\"Albania\"\n\n\n\"Algeria\"\n\n\n\"Andorra\"\n\n\n\"Angola\"\n\n\n…\n\n\n\"West Bank and …\n\n\n\"Winter Olympic…\n\n\n\"Yemen\"\n\n\n\"Zambia\"\n\n\n\"Zimbabwe\"\n\n\n\n\n\n\n\nHowever, it is more efficient to use the .loc or .iloc methods.\n\nUse .loc when using labels or booleans:\n\n\n# simple.loc[:, 'Country/Region']\n\n\nUse .iloc when using indices:\n\n\n# simple.iloc[:, 1]\n\n\nCountry/Region is the 2nd column, but indexing starts at 0 in Python.\n\nThen we need a conditional to filter the rows for which the value is equal to Australia:\n\n# simple.loc[:, 'Country/Region'] == 'Australia'\n\nFinally, we index, out of our entire data frame, the rows for which that condition returns True:\n\n# simple.loc[simple.loc[:, 'Country/Region'] == 'Australia']\n\n\nHere we use .loc to index based on a boolean array.\n\nWe can now make the sum for all of Australia for each day:\n\ntotal_australia = simple.loc[simple.loc[:, 'Country/Region'] == 'Australia'].sum(numeric_only=True)\ntotal_australia\n\nAttributeError: 'DataFrame' object has no attribute 'loc'\n\n\nWe can do this for all countries by grouping them:\n\ntotals = simple.groupby('Country/Region').sum(numeric_only=True)\ntotals\n\n/tmp/ipykernel_17571/4189970077.py:1: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n  totals = simple.groupby('Country/Region').sum(numeric_only=True)\n\n\nTypeError: GroupBy.sum() got an unexpected keyword argument 'numeric_only'\n\n\n Now, we can look at the totals for any date:\n\ntotals.loc[:, '6/12/21']\n\nNameError: name 'totals' is not defined\n\n\nTo make it easier to read, let’s order those numbers by decreasing order:\n\ntotals.loc[:, '6/12/21'].sort_values(ascending=False)\n\nNameError: name 'totals' is not defined\n\n\nWe can also index the data for a particular country by indexing a row instead of a column:\n\ntotals.loc['Albania', :]\n\nNameError: name 'totals' is not defined\n\n\nWhen indexing rows, this syntax can be simplified to:\n\ntotals.loc['Albania']\n\nNameError: name 'totals' is not defined"
  },
  {
    "objectID": "python/ws_polars.html#global-totals",
    "href": "python/ws_polars.html#global-totals",
    "title": "DataFrames with Polars",
    "section": "Global totals",
    "text": "Global totals\nNow, what if we want to have the world totals for each day? We calculate the columns totals (i.e. the sum across countries):\n\ntotals.sum()\n\nNameError: name 'totals' is not defined\n\n\n\n\nYour turn:\n\nHow many confirmed cases were there in Venezuela by March 10, 2021?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFirst, we need to select the data for Venezuela:\n\nvenez = totals.loc['Venezuela']\nvenez\n\nNameError: name 'totals' is not defined\n\n\nThen, we need to select for the proper date:\n\nanswer = venez.loc['3/10/21']\nanswer\n\nNameError: name 'venez' is not defined\n\n\nWe could have done it at once by indexing the row and column:\n\ntotals.loc['Venezuela', '3/10/21']\n\nNameError: name 'totals' is not defined"
  },
  {
    "objectID": "python/ws_polars.html#polars-documentation",
    "href": "python/ws_polars.html#polars-documentation",
    "title": "DataFrames with Polars",
    "section": "Polars documentation",
    "text": "Polars documentation"
  },
  {
    "objectID": "r/hpc_clusters.html",
    "href": "r/hpc_clusters.html",
    "title": "R on HPC clusters",
    "section": "",
    "text": "In this section, you will learn how to use R on an Alliance cluster: load modules, install packages, and run jobs.",
    "crumbs": [
      "R",
      "<b><em>High-performance R</em></b>",
      "R on HPC clusters"
    ]
  },
  {
    "objectID": "r/hpc_clusters.html#modules",
    "href": "r/hpc_clusters.html#modules",
    "title": "R on HPC clusters",
    "section": "Modules",
    "text": "Modules\nOn the Alliance clusters, a number of utilities are available right away (e.g. Bash utilities, git, tmux, various text editors). Before you can use more specialized software however, you have to load the module corresponding to the version of your choice as well as any potential dependencies.\n\nThe cluster setup for this course has everything loaded, so this step is not necessary today, but it is very important to learn it.\n\n\nR\nFirst, of course, we need an R module.\nTo see which versions of R are available on a cluster, run:\nmodule spider r\nTo see the dependencies of a particular version (e.g. r/4.3.1), run:\nmodule spider r/4.3.1\nThis shows us that we need StdEnv/2020 to load r/4.3.1.\n\n\nC compiler\nIf you plan on installing any R package, you will also need a C compiler.\nIn theory, one could use the proprietary Intel compiler which is loaded by default on the Alliance clusters, but it is recommended to replace it with the GCC compiler (R packages can be compiled by any C compiler—also including Clang and LLVM—but the default GCC compiler is the best way to avoid headaches).\n\n\nYour turn:\n\n\nHow can you check which gcc versions are available on our training cluster?\nWhat are the dependencies required by gcc/11.3.0?\n\n\n\n\nLoading the modules\nOnce you know which modules you need, you can load them. The order is important: the dependencies (here StdEnv/2020) must be listed before the modules which depend on them.\nmodule load StdEnv/2020 gcc/11.3.0 r/4.3.1",
    "crumbs": [
      "R",
      "<b><em>High-performance R</em></b>",
      "R on HPC clusters"
    ]
  },
  {
    "objectID": "r/hpc_clusters.html#installing-r-packages",
    "href": "r/hpc_clusters.html#installing-r-packages",
    "title": "R on HPC clusters",
    "section": "Installing R packages",
    "text": "Installing R packages\n\nFor this course, all packages have already been installed in a communal library. You thus don’t have to install anything.\n\nTo install a package, launch the interactive R console with:\nR\nIn the R console, run:\ninstall.packages(\"&lt;package_name&gt;\", repos=\"&lt;url-cran-mirror&gt;\")\n\nrepos argument: chose a CRAN mirror close to the location of your cluster or use https://cloud.r-project.org/.\n\n\nThe first time you install a package, R will ask you whether you want to create a personal library in your home directory. Answer yes to both questions. Your packages will now install under ~/.\n\n\nSome packages require additional modules to be loaded before they can be installed. Other packages need additional R packages as dependencies. In either case, you will get explicit error messages. Adding the argument dependencies = T helps in the second case, but you will still have to add packages manually from time to time.\n\nTo leave the R console, press &lt;Ctrl+D&gt;.",
    "crumbs": [
      "R",
      "<b><em>High-performance R</em></b>",
      "R on HPC clusters"
    ]
  },
  {
    "objectID": "r/hpc_clusters.html#running-r-jobs",
    "href": "r/hpc_clusters.html#running-r-jobs",
    "title": "R on HPC clusters",
    "section": "Running R jobs",
    "text": "Running R jobs\nThere are two types of jobs that can be launched on an Alliance cluster: interactive jobs and batch jobs. We will practice both and discuss their respective merits and when to use which.\nFor this course, I purposefully built a rather small cluster (10 nodes with 4 CPUs and 15GB each) to give a tangible illustration of the constraints of resource sharing.\n\nInteractive jobs\n\nWhile it is fine to run R on the login node when you install packages, you must start a SLURM job before any heavy computation.\n\nTo run R interactively, you should launch an salloc session.\n\nExample to launch an interactive job on a single CPU with 3500MB of memory for 2h:\n\nsalloc --time=2:00:00 --mem-per-cpu=3500M\nThis takes you to a compute node where you can now launch R to run computations:\nR\n\nThis however leads to the same inefficient use of resources as happens when running an RStudio server: all the resources that you requested are blocked for you while your job is running, whether you are making use of them (running heavy computations) or not (thinking, typing code, running computations that use only a fraction of the requested resources).\nInteractive jobs are thus best kept to develop code.\n\n\n\nScripts\nTo run an R script called &lt;your_script&gt;.R, you first need to write a job script:\n\nExample to run a script on 4 CPUs with 3500MB per CPU for 15min:\n\n\n&lt;your_job&gt;.sh\n\n#!/bin/bash\n#SBATCH --account=def-&lt;your_account&gt;\n#SBATCH --time=15\n#SBATCH --mem-per-cpu=3500M\n#SBATCH --cpus-per-task=4\n#SBATCH --job-name=\"&lt;your_job&gt;\"\nmodule load StdEnv/2020 gcc/11.3.0 r/4.3.1\nRscript &lt;your_script&gt;.R\n\n\n\nNote that R scripts are run with the command Rscript (not R).\n\nThen launch your job with:\nsbatch &lt;your_job&gt;.sh\nYou can monitor your job with sq (an alias for squeue -u $USER $@).\n\nBatch jobs are the best approach to run parallel computations, particularly when they require a lot of hardware.\nIt will save you lots of waiting time (Alliance clusters) or money (commercial clusters).",
    "crumbs": [
      "R",
      "<b><em>High-performance R</em></b>",
      "R on HPC clusters"
    ]
  },
  {
    "objectID": "r/hpc_foreach.html",
    "href": "r/hpc_foreach.html",
    "title": "foreach and doFuture",
    "section": "",
    "text": "One of the options to parallelize code with the future package is to use foreach with doFuture. In this section, we will go over an example with the random forest algorithm.",
    "crumbs": [
      "R",
      "<b><em>High-performance R</em></b>",
      "foreach and doFuture"
    ]
  },
  {
    "objectID": "r/hpc_foreach.html#our-example-code-random-forest",
    "href": "r/hpc_foreach.html#our-example-code-random-forest",
    "title": "foreach and doFuture",
    "section": "Our example code: random forest",
    "text": "Our example code: random forest\n\nOn the iris dataset\nRandom forest is a commonly used ensemble learning technique for classification and regression. The idea is to combine the results from many decision trees on bootstrap samples of the dataset to improve the predictive accuracy and control over-fitting. The algorithm used was developed by Tin Kam Ho, then improved by Leo Breiman and Adele Cutler. An implementation in R is provided by the randomForest() function from the randomForest package. Let’s use it to classify the iris dataset that comes packaged with R.\nFirst, let’s have a look at the dataset:\n\n# Structure of the dataset\nstr(iris)\n\n'data.frame':   150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\n# Dimensions of the dataset\ndim(iris)\n\n[1] 150   5\n\n# First 6 data points\nhead(iris)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\n# The 3 species (3 levels of the factor)\nlevels(iris$Species)\n\n[1] \"setosa\"     \"versicolor\" \"virginica\" \n\n\nThe goal is to create a random forest model (let’s call it rf) that can classify an iris flower in one of the 3 species based on the 4 measurements of its sepals and petals.\n\nlibrary(randomForest)\n\nrandomForest 4.7-1.1\n\n\nType rfNews() to see new features/changes/bug fixes.\n\nset.seed(123)\nrf &lt;- randomForest(Species ~ ., data=iris)\n\n\nOur response variable (Species) is a factor, so classification is assumed.\nThe . on the right side of the formula represents all other variables (so we are using all variables, except for the response variable Species of course, as feature variables).\n\n\nrf\n\n\nCall:\n randomForest(formula = Species ~ ., data = iris) \n               Type of random forest: classification\n                     Number of trees: 500\nNo. of variables tried at each split: 2\n\n        OOB estimate of  error rate: 4.67%\nConfusion matrix:\n           setosa versicolor virginica class.error\nsetosa         50          0         0        0.00\nversicolor      0         47         3        0.06\nvirginica       0          4        46        0.08\n\n\nAs can be seen by the confusion matrix, our model performs well.\nWe can use it on new data to make predictions. Let’s try with some made-up data:\n\nnew_data &lt;- data.frame(\n  Sepal.Length = c(5.3, 4.6, 6.5),\n  Sepal.Width = c(3.1, 3.9, 2.5),\n  Petal.Length = c(1.5, 1.5, 5.0),\n  Petal.Width = c(0.2, 0.1, 2.1)\n)\n\nnew_data\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width\n1          5.3         3.1          1.5         0.2\n2          4.6         3.9          1.5         0.1\n3          6.5         2.5          5.0         2.1\n\npredict(rf, new_data)\n\n        1         2         3 \n   setosa    setosa virginica \nLevels: setosa versicolor virginica\n\n\n\n\nLet’s make it big\nNow, the iris dataset only has 150 observations and we used the default number of trees (500) of the randomForest() function, so things ran fast. Often, random forests are run on large datasets. Let’s artificially increase the iris dataset and use more trees to create a situation in which parallelization would make sense.\nOne easy way is to replicate each row 100 times (and we can then delete the row names that get created by this operation):\n\nbig_iris &lt;- iris[rep(seq_len(nrow(iris)), each = 1e2), ]\nrownames(big_iris) &lt;- NULL\n\n\ndim(big_iris)\n\n[1] 15000     5\n\n\nAnd then we can run randomForest() on this dataset and 2000 trees.",
    "crumbs": [
      "R",
      "<b><em>High-performance R</em></b>",
      "foreach and doFuture"
    ]
  },
  {
    "objectID": "r/hpc_foreach.html#hidden-parallelism-check",
    "href": "r/hpc_foreach.html#hidden-parallelism-check",
    "title": "foreach and doFuture",
    "section": "Hidden parallelism check",
    "text": "Hidden parallelism check\nBefore parallelizing your code, remember to check whether the package you are using is already doing any parallelization under the hood (after all, maybe the randomForest package runs things in parallel. We don’t know).\nOne way to do this is to test the package on your local machine and, while some sample code is running, to open htop and see how many cores are used.\nWhy do this on your local machine? because on the cluster, if you launch htop while your batch job is running, you will be looking at processes running on the login node while your code is running on compute node(s). So this will not help you. You could salloc on the/one of the compute node(s) running your job and run htop there, but in production clusters, compute nodes are large and you will see all the processes from all the other users using that compute node. So this test is just easier done locally.\nOn my machine I ran:\nlibrary(randomForest)\n\nbig_iris &lt;- iris[rep(seq_len(nrow(iris)), each = 1e2), ]\nrownames(big_iris) &lt;- NULL\n\nset.seed(123)\nrf &lt;- randomForest(Species ~ ., data=big_iris, ntree=2000)\nAnd I could confirm that the function does not run in parallel.\nSo let’s parallelize this code.",
    "crumbs": [
      "R",
      "<b><em>High-performance R</em></b>",
      "foreach and doFuture"
    ]
  },
  {
    "objectID": "r/hpc_foreach.html#the-foreach-package",
    "href": "r/hpc_foreach.html#the-foreach-package",
    "title": "foreach and doFuture",
    "section": "The foreach package",
    "text": "The foreach package\nThe foreach package provides a construct for repeated executions, i.e. it can replace for loops, while loops, repeat loops, and functional programming code written with the *apply functions or the purrr package. The foreach vignette gives many examples.",
    "crumbs": [
      "R",
      "<b><em>High-performance R</em></b>",
      "foreach and doFuture"
    ]
  },
  {
    "objectID": "r/hpc_foreach.html#the-dofuture-package",
    "href": "r/hpc_foreach.html#the-dofuture-package",
    "title": "foreach and doFuture",
    "section": "The doFuture package",
    "text": "The doFuture package\nThe most useful part of foreach is that it allows for easily parallelization with countless backends: doFuture, doMC, doMPI, doParallel, doRedis, doRNG, doSNOW, and doAzureParallel.\nThe doFuture package is the most modern of these backends. It allows to evaluate foreach expressions across the evaluation strategies of the future package very easily. All you have to do is to register it as a backend, declare the evaluation strategy of futures of your choice, make sure to generate parallel-safe random numbers for reproducibility (if your code uses randomness), and replace %do% with %dofuture%.",
    "crumbs": [
      "R",
      "<b><em>High-performance R</em></b>",
      "foreach and doFuture"
    ]
  },
  {
    "objectID": "r/hpc_foreach.html#benchmarks",
    "href": "r/hpc_foreach.html#benchmarks",
    "title": "foreach and doFuture",
    "section": "Benchmarks",
    "text": "Benchmarks\nWe will run and benchmark all versions of our code by submitting batch jobs to Slurm.\n\nInitial code\nFirst, let’s benchmark the initial (non parallel, not using foreach) code. We need to create an R script. Let’s call it reference.R (I will use Emacs, but you can use the nano text editor with the command nano to write the script if you want):\n\n\nreference.R\n\nlibrary(randomForest)\nlibrary(bench)\n\nbig_iris &lt;- iris[rep(seq_len(nrow(iris)), each = 1e2), ]\nrownames(big_iris) &lt;- NULL\n\ncat(\"\\nBenchmarking results:\\n\\n\")\n\nset.seed(123)\nmark(rf &lt;- randomForest(Species ~ ., data=big_iris, ntree=2000))\n\nThen we need to create a Bash script for Slurm. Let’s call it reference.sh:\n\n\nreference.sh\n\n#!/bin/bash\n#SBATCH --time=5\n#SBATCH --mem-per-cpu=7500M\n\nRscript reference.R\n\n\nYou can see the full list of sbatch options here.\n\nAnd now we submit the job with:\nsbatch reference.sh\nYou can monitor your job with sq. The result will be written to a file called slurm-xx.out with xx being the number of the job that just ran. To see the result, we can simply print the content of that file to screen with cat (you can run ls to see the list of files in the current directory). Make sure that your job has finished running before printing the result (otherwise you might get a partial output which can be confusing).\ncat slurm-xx.out    # Replace xx by the job number\nrandomForest 4.7-1.1\nType rfNews() to see new features/changes/bug fixes.\n\nBenchmarking results:\n\n# A tibble: 1 × 13\n  expression      min median `itr/sec` mem_alloc `gc/sec` n_itr  n_gc total_time\n  &lt;bch:expr&gt;    &lt;bch&gt; &lt;bch:&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;   &lt;bch:tm&gt;\n1 rf &lt;- random… 6.33s  6.33s     0.158        NA    0.474     1     3      6.33s\n# ℹ 4 more variables: result &lt;list&gt;, memory &lt;list&gt;, time &lt;list&gt;, gc &lt;list&gt;\nWarning message:\nSome expressions had a GC in every iteration; so filtering is disabled.\n\n\nforeach expression\nNow, let’s try the foreach version:\n\n\nforeach.R\n\nlibrary(foreach)\nlibrary(randomForest)\nlibrary(bench)\n\nbig_iris &lt;- iris[rep(seq_len(nrow(iris)), each = 1e2), ]\nrownames(big_iris) &lt;- NULL\n\ncat(\"\\nBenchmarking results:\\n\\n\")\n\nset.seed(123)\nmark(\n  rf &lt;- foreach(ntree = rep(250, 8), .combine = combine) %do%\n    randomForest(Species ~ ., data=big_iris, ntree=ntree)\n)\n\n\n\nforeach.sh\n\n#!/bin/bash\n#SBATCH --time=5\n#SBATCH --mem-per-cpu=7500M\n\nRscript foreach.R\n\nsbatch foreach.sh\nrandomForest 4.7-1.1\nType rfNews() to see new features/changes/bug fixes.\n\nBenchmarking results:\n\n# A tibble: 1 × 13\n  expression      min median `itr/sec` mem_alloc `gc/sec` n_itr  n_gc total_time\n  &lt;bch:expr&gt;    &lt;bch&gt; &lt;bch:&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;   &lt;bch:tm&gt;\n1 rf &lt;- foreac… 7.04s  7.04s     0.142        NA     4.55     1    32      7.04s\n# ℹ 4 more variables: result &lt;list&gt;, memory &lt;list&gt;, time &lt;list&gt;, gc &lt;list&gt;\nWarning message:\nSome expressions had a GC in every iteration; so filtering is disabled.\n\nThe foreach expression is slower than the standard expression (it is always the case: foreach slows things down before this overhead gets offset by parallelization).\n\n\n\nPlan sequential\nYou might wonder why the sequential evaluation strategy exists (i.e. why go through all the trouble of writing your code with foreach and doFuture to then run it without parallelism?).\nThere are many reasons:\n\nIt can be very useful for debugging.\nIt makes it easy to switch the futures execution strategy back and forth for different sections of the code (maybe you don’t want to run everything in parallel).\nIt allows other people to run the same code on their different hardware without changing it (if they don’t have the resources to run things in parallel, they only have to change the execution strategy).\n\nTo turn the code into a parallelizable version with doFuture, we replace %do% with %dofuture%.\nHere, we also need to use the option .options.future = list(seed = TRUE): whenever your parallel code rely on a random process, it isn’t enough to use set.seed() to ensure reproducibility, you also need to generate parallel-safe random numbers. In random forest, each tree is trained on a random subset of the data and random variables are selected for splitting at each node. The option .options.future = list(seed = TRUE) pregenerates the random seeds using L’Ecuyer-CMRG RNG streams1.\n1 L’Ecuyer, P. (1999). Good parameters and implementations for combined multiple recursive random number generators. Operations Research, 47, 159–164.This is the parallelizable foreach code, but run sequentially:\n\n\nsequential.R\n\nlibrary(doFuture)    # Also loads foreach and future\nlibrary(randomForest)\nlibrary(bench)\n\nregisterDoFuture()   # Set the parallel backend\nplan(sequential)     # Set the evaluation strategy\n\nbig_iris &lt;- iris[rep(seq_len(nrow(iris)), each = 1e2), ]\nrownames(big_iris) &lt;- NULL\n\ncat(\"\\nBenchmarking results:\\n\\n\")\n\nset.seed(123)\nmark(\n  rf &lt;- foreach(\n    ntree = rep(250, 8),\n    .options.future = list(seed = TRUE),\n    .combine = combine\n  ) %dofuture%\n    randomForest(Species ~ ., data=big_iris, ntree=ntree)\n)\n\n\n\nsequential.sh\n\n#!/bin/bash\n#SBATCH --time=5\n#SBATCH --mem-per-cpu=7500M\n\nRscript sequential.R\n\nsbatch sequential.sh\nLoading required package: foreach\nLoading required package: future\nrandomForest 4.7-1.1\nType rfNews() to see new features/changes/bug fixes.\n\nBenchmarking results:\n\n# A tibble: 1 × 13\n  expression      min median `itr/sec` mem_alloc `gc/sec` n_itr  n_gc total_time\n  &lt;bch:expr&gt;    &lt;bch&gt; &lt;bch:&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;   &lt;bch:tm&gt;\n1 rf &lt;- foreac… 8.39s  8.39s     0.119        NA     3.81     1    32      8.39s\n# ℹ 4 more variables: result &lt;list&gt;, memory &lt;list&gt;, time &lt;list&gt;, gc &lt;list&gt;\nWarning message:\nSome expressions had a GC in every iteration; so filtering is disabled.\n\nEach time we add unnecessary complexity in the code, things run a little slower.\n\n\n\nMulti-processing in shared memory\nNow, it is time to parallelize. First, we will use multiple cores on a single node (shared-memory parallelism).\n\nNumber of cores\nThe future package provides the availableCores() function to detect the number of available cores. We will run it as part of our script as a check on our available hardware.\nThe cluster for this course is made of 20 nodes with 4 CPUs each. We want to test shared memory parallelism, so our job needs to stay within one node. We can thus ask for a maximum of 4 CPUs and we want to ensure that we aren’t getting them on different nodes. Let’s go with that maximum of 4 cores.\n\n\nMultisession\nShared memory multi-processing can be run with plan(multisession) that will spawn new R sessions in the background to evaluate futures.\n\n\nmultisession.R\n\nlibrary(doFuture)\nlibrary(randomForest)\nlibrary(bench)\n\n# Check number of cores:\ncat(\"\\nWe have\", availableCores(), \"cores.\\n\\n\")\n\nregisterDoFuture()   # Set the parallel backend\nplan(multisession)   # Set the evaluation strategy\n\nbig_iris &lt;- iris[rep(seq_len(nrow(iris)), each = 1e2), ]\nrownames(big_iris) &lt;- NULL\n\ncat(\"\\nBenchmarking results:\\n\\n\")\n\nset.seed(123)\nmark(\n  rf &lt;- foreach(\n    ntree = rep(250, 8),\n    .options.future = list(seed = TRUE),\n    .combine = combine\n  ) %dofuture%\n    randomForest(Species ~ ., data=big_iris, ntree=ntree)\n)\n\n\n\nmultisession.sh\n\n#!/bin/bash\n#SBATCH --time=5\n#SBATCH --mem-per-cpu=7500M\n#SBATCH --cpus-per-task=4\n\nRscript multisession.R\n\nsbatch multisession.sh\nLoading required package: foreach\nLoading required package: future\nrandomForest 4.7-1.1\nType rfNews() to see new features/changes/bug fixes.\n\nWe have 4 cores.\n\nBenchmarking results:\n\n# A tibble: 1 × 13\n  expression      min median `itr/sec` mem_alloc `gc/sec` n_itr  n_gc total_time\n  &lt;bch:expr&gt;    &lt;bch&gt; &lt;bch:&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;   &lt;bch:tm&gt;\n1 rf &lt;- foreac… 2.72s  2.72s     0.368        NA     2.21     1     6      2.72s\n# ℹ 4 more variables: result &lt;list&gt;, memory &lt;list&gt;, time &lt;list&gt;, gc &lt;list&gt;\nWarning message:\nSome expressions had a GC in every iteration; so filtering is disabled.\nSpeedup: 3.1.\n\nNot too bad, considering that the ideal speedup, without any overhead, would be 4.\n\n\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=4\ncould be used instead of:\n#SBATCH --cpus-per-task=4\nWhat matters is to have 4 cores running on the same node to be in a shared memory parallelism scenario.\n\n\n\nMulticore\nShared memory multi-processing can also be run with plan(multicore) (except on Windows) that will fork the current R process to evaluate futures.\n\n\nmulticore.R\n\nlibrary(doFuture)\nlibrary(randomForest)\nlibrary(bench)\n\n# Check number of cores:\ncat(\"\\nWe have\", availableCores(), \"cores.\\n\\n\")\n\nregisterDoFuture()   # Set the parallel backend\nplan(multicore)      # Set the evaluation strategy\n\nbig_iris &lt;- iris[rep(seq_len(nrow(iris)), each = 1e2), ]\nrownames(big_iris) &lt;- NULL\n\ncat(\"\\nBenchmarking results:\\n\\n\")\n\nset.seed(123)\nmark(\n  rf &lt;- foreach(\n    ntree = rep(250, 8),\n    .options.future = list(seed = TRUE),\n    .combine = combine\n  ) %dofuture%\n    randomForest(Species ~ ., data=big_iris, ntree=ntree)\n)\n\n\n\nmulticore.sh\n\n#!/bin/bash\n#SBATCH --time=5\n#SBATCH --mem-per-cpu=7500M\n#SBATCH --cpus-per-task=4\n\nRscript multicore.R\n\nsbatch multicore.sh\nLoading required package: foreach\nLoading required package: future\nrandomForest 4.7-1.1\nType rfNews() to see new features/changes/bug fixes.\n\nWe have 4 cores.\n\nBenchmarking results:\n\n# A tibble: 1 × 13\n  expression      min median `itr/sec` mem_alloc `gc/sec` n_itr  n_gc total_time\n  &lt;bch:expr&gt;    &lt;bch&gt; &lt;bch:&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;   &lt;bch:tm&gt;\n1 rf &lt;- foreac… 3.15s  3.15s     0.318        NA     13.7     1    43      3.15s\n# ℹ 4 more variables: result &lt;list&gt;, memory &lt;list&gt;, time &lt;list&gt;, gc &lt;list&gt;\nWarning message:\nSome expressions had a GC in every iteration; so filtering is disabled.\nSpeedup: 2.7.\n\nWhile in theory we should get a similar speedup, we are getting a lower one here.\n\n\n\n\nMulti-processing in distributed memory\nLet’s run our distributed parallel code using 8 cores across 2 nodes.\nWe need to create a cluster of workers. We do this by creating a character vector with the names of the nodes our tasks are running on and passing it to the makeCluster() function from the parallel package (included in R):\n# Create a character vector with the nodes names\nhosts &lt;- system(\"srun hostname -s\", intern = T)\n\n# Create the cluster of workers\ncl &lt;- parallel::makeCluster(hosts)\nWe can verify that we did get 8 tasks by accessing the SLURM_NTASKS environment variable from within R:\nas.numeric(Sys.getenv(\"SLURM_NTASKS\"))\nHere is the R script:\n\n\ndistributed.R\n\nlibrary(doFuture)\nlibrary(randomForest)\nlibrary(bench)\n\n# Check number of tasks:\ncat(\"\\nWe have\", as.numeric(Sys.getenv(\"SLURM_NTASKS\")), \"tasks.\\n\\n\")\n\n# Create a character vector with the nodes names\nhosts &lt;- system(\"srun hostname -s\", intern = T)\n\n# Look at the location of our tasks:\ncat(\"\\nOur tasks are running on the following nodes: \", hosts)\n\n# Create the cluster of workers\ncl &lt;- parallel::makeCluster(hosts)\n\nregisterDoFuture()           # Set the parallel backend\nplan(cluster, workers = cl)  # Set the evaluation strategy\n\nbig_iris &lt;- iris[rep(seq_len(nrow(iris)), each = 1e2), ]\nrownames(big_iris) &lt;- NULL\n\ncat(\"\\nBenchmarking results:\\n\\n\")\n\nset.seed(123)\nmark(\n  rf &lt;- foreach(\n    ntree = rep(250, 8),\n    .options.future = list(seed = TRUE),\n    .combine = combine\n  ) %dofuture%\n    randomForest(Species ~ ., data=big_iris, ntree=ntree)\n)\n\n\nThe cluster of workers can be stopped with:\nparallel::stopCluster(cl)\nHere, this is not necessary since our job stops running as soon as the execution is complete, but in other systems, this will prevent you from monopolizing hardware or paying unnecessarily.\n\nAnd now we need to ask Slurm for 8 tasks on 2 nodes:\n\n\ndistributed.sh\n\n#!/bin/bash\n#SBATCH --time=5\n#SBATCH --mem-per-cpu=7500M\n#SBATCH --ntasks-per-node=4\n#SBATCH --nodes=2\n\nRscript distributed.R\n\nsbatch distributed.sh\nLoading required package: foreach\nLoading required package: future\nrandomForest 4.7-1.1\nType rfNews() to see new features/changes/bug fixes.\n\nWe have 8 tasks.\n\nOur tasks are running on the following nodes: \"node1\" \"node1\" \"node1\" \"node1\" \"node2\" \"node2\" \"node2\" \"node2\"\n\nBenchmarking results:\n\n# A tibble: 1 × 13\n  expression      min median `itr/sec` mem_alloc `gc/sec` n_itr  n_gc total_time\n  &lt;bch:expr&gt;    &lt;bch&gt; &lt;bch:&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;   &lt;bch:tm&gt;\n1 rf &lt;- foreac…  1.6s   1.6s     0.624        NA     3.12     1     5       1.6s\n# ℹ 4 more variables: result &lt;list&gt;, memory &lt;list&gt;, time &lt;list&gt;, gc &lt;list&gt;\nWarning message:\nSome expressions had a GC in every iteration; so filtering is disabled.\nSpeedup: 5.2.\n\nThe overhead is larger in distributed parallelism due to message passing between nodes. We are further from the ideal speedup of 8, but we still got a speedup larger than what we could have obtained with shared-memory parallelism.\n\n\n#SBATCH --ntasks=8\ncould be used instead of:\n#SBATCH --ntasks-per-node=4\n#SBATCH --nodes=2\nHowever the latter is slightly better because it allows us to use 2 full nodes instead of having tasks running on any number of nodes. However, it also means that we might have to wait longer for our job to run as it is more restrictive.",
    "crumbs": [
      "R",
      "<b><em>High-performance R</em></b>",
      "foreach and doFuture"
    ]
  },
  {
    "objectID": "r/hpc_memory.html",
    "href": "r/hpc_memory.html",
    "title": "Memory management",
    "section": "",
    "text": "Memory can be a limiting factor and releasing it when not needed can be critical to avoid out of memory states. On the other hand, memoisation is an optimization technique which consists of caching the results of heavy computations for re-use.\nMemory and speed are thus linked in a trade-off.",
    "crumbs": [
      "R",
      "<b><em>High-performance R</em></b>",
      "Memory management"
    ]
  },
  {
    "objectID": "r/hpc_memory.html#releasing-memory",
    "href": "r/hpc_memory.html#releasing-memory",
    "title": "Memory management",
    "section": "Releasing memory",
    "text": "Releasing memory\nIt is best to avoid creating very large intermediate objects that take space in memory unnecessarily.\n\nOne option is to use nested functions or functions chained with pipes.\nAnother option is to create the intermediate objects within the local environment of a function as they will automatically be deleted as soon as the function has finished running.\n\nLet’s go over a basic example: let’s extract the sepal width variable from the iris dataset (one of the datasets that come packaged with R), take the natural logarithm of the values, and round them to one decimal place.\nFirst, let’s delete all objects inside our environment to make our little test as clean as possible:\n\nrm(list=ls())\nls()\n\ncharacter(0)\n\n\nNow, we could perform our task this way:\n\nsepalwidth &lt;- iris$Sepal.Width\nsepalwidth_ln &lt;- log(sepalwidth)\nround(sepalwidth_ln, 1)\n\n  [1] 1.3 1.1 1.2 1.1 1.3 1.4 1.2 1.2 1.1 1.1 1.3 1.2 1.1 1.1 1.4 1.5 1.4 1.3\n [19] 1.3 1.3 1.2 1.3 1.3 1.2 1.2 1.1 1.2 1.3 1.2 1.2 1.1 1.2 1.4 1.4 1.1 1.2\n [37] 1.3 1.3 1.1 1.2 1.3 0.8 1.2 1.3 1.3 1.1 1.3 1.2 1.3 1.2 1.2 1.2 1.1 0.8\n [55] 1.0 1.0 1.2 0.9 1.1 1.0 0.7 1.1 0.8 1.1 1.1 1.1 1.1 1.0 0.8 0.9 1.2 1.0\n [73] 0.9 1.0 1.1 1.1 1.0 1.1 1.1 1.0 0.9 0.9 1.0 1.0 1.1 1.2 1.1 0.8 1.1 0.9\n [91] 1.0 1.1 1.0 0.8 1.0 1.1 1.1 1.1 0.9 1.0 1.2 1.0 1.1 1.1 1.1 1.1 0.9 1.1\n[109] 0.9 1.3 1.2 1.0 1.1 0.9 1.0 1.2 1.1 1.3 1.0 0.8 1.2 1.0 1.0 1.0 1.2 1.2\n[127] 1.0 1.1 1.0 1.1 1.0 1.3 1.0 1.0 1.0 1.1 1.2 1.1 1.1 1.1 1.1 1.1 1.0 1.2\n[145] 1.2 1.1 0.9 1.1 1.2 1.1\n\n\nBut this creates the unnecessary intermediate variables sepalwidth and sepalwidth_ln which get stored in memory:\n\nls()\n\n[1] \"sepalwidth\"    \"sepalwidth_ln\"\n\n\nFor very large objects, this is not ideal.\nLet’s clear objects in our environment again:\n\nrm(list=ls())\nls()\n\ncharacter(0)\n\n\nA better option is to use nested functions:\n\nround(log(iris$Sepal.Width), 1)\n\n  [1] 1.3 1.1 1.2 1.1 1.3 1.4 1.2 1.2 1.1 1.1 1.3 1.2 1.1 1.1 1.4 1.5 1.4 1.3\n [19] 1.3 1.3 1.2 1.3 1.3 1.2 1.2 1.1 1.2 1.3 1.2 1.2 1.1 1.2 1.4 1.4 1.1 1.2\n [37] 1.3 1.3 1.1 1.2 1.3 0.8 1.2 1.3 1.3 1.1 1.3 1.2 1.3 1.2 1.2 1.2 1.1 0.8\n [55] 1.0 1.0 1.2 0.9 1.1 1.0 0.7 1.1 0.8 1.1 1.1 1.1 1.1 1.0 0.8 0.9 1.2 1.0\n [73] 0.9 1.0 1.1 1.1 1.0 1.1 1.1 1.0 0.9 0.9 1.0 1.0 1.1 1.2 1.1 0.8 1.1 0.9\n [91] 1.0 1.1 1.0 0.8 1.0 1.1 1.1 1.1 0.9 1.0 1.2 1.0 1.1 1.1 1.1 1.1 0.9 1.1\n[109] 0.9 1.3 1.2 1.0 1.1 0.9 1.0 1.2 1.1 1.3 1.0 0.8 1.2 1.0 1.0 1.0 1.2 1.2\n[127] 1.0 1.1 1.0 1.1 1.0 1.3 1.0 1.0 1.0 1.1 1.2 1.1 1.1 1.1 1.1 1.1 1.0 1.2\n[145] 1.2 1.1 0.9 1.1 1.2 1.1\n\n\nAn equivalent option is to chain functions:\n\niris$Sepal.Width |&gt; log() |&gt; round(1)\n\n  [1] 1.3 1.1 1.2 1.1 1.3 1.4 1.2 1.2 1.1 1.1 1.3 1.2 1.1 1.1 1.4 1.5 1.4 1.3\n [19] 1.3 1.3 1.2 1.3 1.3 1.2 1.2 1.1 1.2 1.3 1.2 1.2 1.1 1.2 1.4 1.4 1.1 1.2\n [37] 1.3 1.3 1.1 1.2 1.3 0.8 1.2 1.3 1.3 1.1 1.3 1.2 1.3 1.2 1.2 1.2 1.1 0.8\n [55] 1.0 1.0 1.2 0.9 1.1 1.0 0.7 1.1 0.8 1.1 1.1 1.1 1.1 1.0 0.8 0.9 1.2 1.0\n [73] 0.9 1.0 1.1 1.1 1.0 1.1 1.1 1.0 0.9 0.9 1.0 1.0 1.1 1.2 1.1 0.8 1.1 0.9\n [91] 1.0 1.1 1.0 0.8 1.0 1.1 1.1 1.1 0.9 1.0 1.2 1.0 1.1 1.1 1.1 1.1 0.9 1.1\n[109] 0.9 1.3 1.2 1.0 1.1 0.9 1.0 1.2 1.1 1.3 1.0 0.8 1.2 1.0 1.0 1.0 1.2 1.2\n[127] 1.0 1.1 1.0 1.1 1.0 1.3 1.0 1.0 1.0 1.1 1.2 1.1 1.1 1.1 1.1 1.1 1.0 1.2\n[145] 1.2 1.1 0.9 1.1 1.2 1.1\n\n\nAnother option is to create the intermediate variables in the local environment of a function:\n\nget_sepalwidth &lt;- function(dataset) {\n  sepalwidth &lt;- dataset$Sepal.Width\n  sepalwidth_ln &lt;- log(sepalwidth)\n  round(sepalwidth_ln, 1)\n}\n\nget_sepalwidth(iris)\n\n  [1] 1.3 1.1 1.2 1.1 1.3 1.4 1.2 1.2 1.1 1.1 1.3 1.2 1.1 1.1 1.4 1.5 1.4 1.3\n [19] 1.3 1.3 1.2 1.3 1.3 1.2 1.2 1.1 1.2 1.3 1.2 1.2 1.1 1.2 1.4 1.4 1.1 1.2\n [37] 1.3 1.3 1.1 1.2 1.3 0.8 1.2 1.3 1.3 1.1 1.3 1.2 1.3 1.2 1.2 1.2 1.1 0.8\n [55] 1.0 1.0 1.2 0.9 1.1 1.0 0.7 1.1 0.8 1.1 1.1 1.1 1.1 1.0 0.8 0.9 1.2 1.0\n [73] 0.9 1.0 1.1 1.1 1.0 1.1 1.1 1.0 0.9 0.9 1.0 1.0 1.1 1.2 1.1 0.8 1.1 0.9\n [91] 1.0 1.1 1.0 0.8 1.0 1.1 1.1 1.1 0.9 1.0 1.2 1.0 1.1 1.1 1.1 1.1 0.9 1.1\n[109] 0.9 1.3 1.2 1.0 1.1 0.9 1.0 1.2 1.1 1.3 1.0 0.8 1.2 1.0 1.0 1.0 1.2 1.2\n[127] 1.0 1.1 1.0 1.1 1.0 1.3 1.0 1.0 1.0 1.1 1.2 1.1 1.1 1.1 1.1 1.1 1.0 1.2\n[145] 1.2 1.1 0.9 1.1 1.2 1.1\n\n\nNone of these options left intermediate variables in our environment:\n\nls()\n\n[1] \"get_sepalwidth\"\n\n\nNote that in the case of a very large function, it might still be beneficial to run rm() inside the function to clear the memory for other processes coming next within that function. But this is a pretty rare case.\nIf you really have to create large intermediate objects in the global environment, make sure to delete them as soon as you don’t need them anymore (e.g. rm(sepalwidth, sepalwidth_ln)).\n\nrm() deletes the names of variables (the pointers to objects in memory). But as soon as all the pointers to an object in memory are deleted, the garbage collector clears its value and releases the memory it used.",
    "crumbs": [
      "R",
      "<b><em>High-performance R</em></b>",
      "Memory management"
    ]
  },
  {
    "objectID": "r/hpc_memory.html#caching",
    "href": "r/hpc_memory.html#caching",
    "title": "Memory management",
    "section": "Caching",
    "text": "Caching\nMemoisation is a technique by which some results are cached to avoid re-calculating them. This is convenient in a variety of settings (e.g. to reduce calls to an API, to avoid repeating heavy computations). In particular, it improves the efficiency of recursive function calls dramatically.\nLet’s consider the calculation of the Fibonacci numbers as an example. Those numbers form a sequence starting with 0 and 11, after which each number is the sum of the previous two.\n1 Alternative versions have the sequence start with 1, 1 or with 1, 2.2 There are more efficient ways to calculate the Fibonacci numbers, but this inefficient function is a great example to show the advantage of memoisation.Here is a function that would return the nth Fibonacci number2:\nfib &lt;- function(n) {\n  if(n == 0) {\n    return(0)\n  } else if(n == 1) {\n    return(1)\n  } else {\n    Recall(n - 1) + Recall(n - 2)\n  }\n}\nIt can be written more tersely as:\n\nfib &lt;- function(n) {\n  if(n == 0) return(0)\n  if(n == 1) return(1)\n  Recall(n - 1) + Recall(n - 2)\n}\n\n\nRecall() is a placeholder for the name of the recursive function. We could have used fib() instead, but Recall() is more robust as it allows for function renaming.\n\nMemoisation is very useful here because, for each Fibonacci number, we need to calculate the two preceding Fibonacci numbers and to calculate each of those we need to calculate the two Fibonacci numbers preceding that one and to calculate… etc. That is a large number of calculations, but, thanks to caching, we don’t have to calculate any one of them more than once.\nThe packages R.cache and memoise both allow for memoisation with an incredibly simple syntax.\nApplying the latter to our function gives us:\n\nlibrary(memoise)\n\nfibmem &lt;- memoise(\n  function(n) {\n    if(n == 0) return(0)\n    if(n == 1) return(1)\n    Recall(n - 1) + Recall(n - 2)\n  }\n)\n\nWe can do some benchmarking to see the speedup for the 30th Fibonacci number:\n\nlibrary(bench)\n\nn &lt;- 30\nmark(fib(n), fibmem(n))\n\nWarning: Some expressions had a GC in every iteration; so filtering is\ndisabled.\n\n\n# A tibble: 2 × 6\n  expression      min   median `itr/sec` mem_alloc `gc/sec`\n  &lt;bch:expr&gt; &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt;\n1 fib(n)        1.62s    1.62s     0.616    32.9KB     18.5\n2 fibmem(n)   41.22µs  44.62µs 20807.       68.3KB     14.6\n\n\nThe speedup is over 35,000!",
    "crumbs": [
      "R",
      "<b><em>High-performance R</em></b>",
      "Memory management"
    ]
  },
  {
    "objectID": "r/hpc_parallel_r.html",
    "href": "r/hpc_parallel_r.html",
    "title": "Running R code in parallel",
    "section": "",
    "text": "The parallel package has been part of the base package group since R version 2.14.0.\nThis means that it is comes with R, however it needs to be loaded in a session before its content can be accessed:\nlibrary(parallel)\nMost parallel approaches in R build on this package.\nAll other packages mentioned in this lesson are external packages and need to be installed with install.packages()."
  },
  {
    "objectID": "r/hpc_parallel_r.html#base-r-parallel-package",
    "href": "r/hpc_parallel_r.html#base-r-parallel-package",
    "title": "Running R code in parallel",
    "section": "",
    "text": "The parallel package has been part of the base package group since R version 2.14.0.\nThis means that it is comes with R, however it needs to be loaded in a session before its content can be accessed:\nlibrary(parallel)\nMost parallel approaches in R build on this package.\nAll other packages mentioned in this lesson are external packages and need to be installed with install.packages()."
  },
  {
    "objectID": "r/hpc_parallel_r.html#parallelly-package",
    "href": "r/hpc_parallel_r.html#parallelly-package",
    "title": "Running R code in parallel",
    "section": "parallelly package",
    "text": "parallelly package\nThe parallelly package—part of the futureverse suite of packages developed by Henrik Bengtsson—adds functionality to the parallel package."
  },
  {
    "objectID": "r/hpc_partition.html",
    "href": "r/hpc_partition.html",
    "title": "Partitioning data with multidplyr",
    "section": "",
    "text": "The package multidplyr provides simple techniques to partition data across a set of workers on the same node.",
    "crumbs": [
      "R",
      "<b><em>High-performance R</em></b>",
      "Partitioning data"
    ]
  },
  {
    "objectID": "r/hpc_partition.html#data-partitioning-for-memory",
    "href": "r/hpc_partition.html#data-partitioning-for-memory",
    "title": "Partitioning data with multidplyr",
    "section": "Data partitioning for memory",
    "text": "Data partitioning for memory\n\nCase example\nWhat if we have an even bigger dataset?\nThe randomForest() function has limitations:\n\nIt is a memory hog.\nIt doesn’t run if your data frame has too many rows.\n\nIf you try to run:\n\n\nbigger.R\n\nlibrary(randomForest)\n\nbigger_iris &lt;- iris[rep(seq_len(nrow(iris)), each = 1e3), ]\nrownames(bigger_iris) &lt;- NULL\n\nset.seed(123)\nrf &lt;- randomForest(Species ~ ., data = bigger_iris)\n\nrf\n\non a single core, you will get:\nrandomForest 4.7-1.1\nType rfNews() to see new features/changes/bug fixes.\n/var/spool/slurmd/job00016/slurm_script: line 5: 74451 Killed                  Rscript data_partition.R\nslurmstepd: error: Detected 1 oom-kill event(s) in StepId=16.batch. Some of your processes may have been killed by the cgroup out-of-memory handler.\nYou have ran out of memory.\nReducing the number of trees won’t help as the problem comes from the size of the data frame.\nSimilarly, using foreach and doFuture as we did previously won’t help either because that spreads the number of trees on various cores, but again, the problem doesn’t come from the number of trees, but for the size of the dataset.\n\nWith plan(multisession), you would get:\nCluster with multisession\nError in unserialize(node$con) :\n  MultisessionFuture (doFuture2-3) failed to receive message results from cluster RichSOCKnode #3 (PID 445273 on localhost ‘localhost’). The reason reported was ‘error reading from connection’. Post-mortem diagnostic: No process exists with this PID, i.e. the localhost worker is no longer alive. The total size of the 3 globals exported is 5.15 MiB. There are three globals: ‘big_iris’ (5.15 MiB of class ‘list’), ‘...future.seeds_ii’ (160 bytes of class ‘list’) and ‘...future.x_ii’ (112 bytes of class ‘list’)\nAnd with plan(multicore):\nCluster with multicore\nError: Failed to retrieve the result of MulticoreFuture (doFuture2-2) from the forked worker (on localhost; PID 444769). Post-mortem diagnostic: No process exists with this PID, i.e. the forked localhost worker is no longer alive. The total size of the 3 globals exported is 5.15 MiB. There are three globals: ‘big_iris’ (5.15 MiB of class ‘list’), ‘...future.seeds_ii’ (160 bytes of class ‘list’) and ‘...future.x_ii’ (112 bytes of class ‘list’)\nIn addition: Warning message:\nIn mccollect(jobs = jobs, wait = TRUE) :\n  1 parallel job did not deliver a result\n\nYou can even try spreading the trees on multiple nodes, but things will fail as well, without any error message.\nOf course, you could always try on a different machine—one with more memory. I used my machine which has more memory than this training cluster and it worked.\nBut then, what if big_iris is even bigger? Say, if we have this for instance:\nbigger_iris &lt;- iris[rep(seq_len(nrow(iris)), each = 1e4), ]\nThen no amount of memory will save you and you will get errors similar to this:\nError in randomForest.default(m, y, ...) : \n  long vectors (argument 28) are not supported in .C\nThat’s because randomForest() does not accept datasets with too many rows.\n\nThe bottom line is that there are situation in which the data is just too big. In such cases, you want to look at data parallelism: instead of splitting your code into tasks that can run in parallel as we did previously, you split the data into chunks and run the code in parallel on those chunks.\n\n\nOf course, you could also simply run the code on a subset of your data. In many situation, reducing your data by sampling it properly will be good enough. But there are situations in which you want to use a huge dataset.\n\nYou could split the data manually and run the code on each chunk, but it would be tedious and very lengthy. And to run the code on all the chunks in parallel, you could implement that yourself. There is a much simpler option provided by the multidplyr package.\n\n\nUsing multidplyr\nTo see what happens as we use multidplyr, let’s first run the code in an interactive session on one node with 4 cores:\n# Launch the interactive job\nsalloc --time=50 --mem-per-cpu=7500M --cpus-per-task=4\n\n# Then launch R\nR\nFirst, we load the packages that are running in the main session:\n\nlibrary(multidplyr)\nlibrary(dplyr, warn.conflicts = FALSE)\n\n\nWe load dplyr for the do() function.\nNotice that we aren’t loading the randomForest package yet: that’s because we will use it on workers, not in the main session.\n\nThen we need to create a cluster of workers. Let’s use 4 workers that we will run on a full node:\n\ncl &lt;- new_cluster(4)\ncl\n\n4 session cluster [....]\n\n\nNow we can load the randomForest package on each worker:\ncluster_library(cl, \"randomForest\")\nrandomForest 4.7-1.1\nType rfNews() to see new features/changes/bug fixes.\n\nAttaching package: ‘randomForest’\n\nThe following object is masked from ‘package:dplyr’:\n\n    combine\nOf course, we need to generate our big dataset:\n\nbigger_iris &lt;- iris[rep(seq_len(nrow(iris)), each = 1e3), ]\nrownames(bigger_iris) &lt;- NULL\n\nThen we create a partitioned data frame on the workers with the partition() function. The function will try to split the data as heavenly as possible among workers.\nIf you group observations by some variable (with dplyr::group_by()) beforehand, multidplyr will ensure that all data points in a group end up on the same worker. This is very convenient in a lot of cases, but is not relevant here. Without grouping observations first, it is unclear how partition() chooses which observation goes to which worker. In our data, we have all the setosa observations first, then all the versicolor, and finally all the virginica. We want to make sure that the randomForest() function runs on a sample of all 3 species. We will thus randomly shuffle the data before partitioning it (when we were parallelizing by splitting the trees, we didn’t have to worry about that since each subset of trees was running on the entire dataset):\n\n# Shuffle the rows of the data frame randomly\nset.seed(11)\nbigger_iris_shuffled &lt;- bigger_iris[sample(nrow(bigger_iris)), ]\n\n# You can check that they are shuffled\nhead(bigger_iris_shuffled)\n\n       Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n65570           6.7         3.1          4.4         1.4 versicolor\n19004           5.1         3.8          1.5         0.3     setosa\n73612           6.1         2.8          4.7         1.2 versicolor\n28886           5.2         3.4          1.4         0.2     setosa\n121310          5.6         2.8          4.9         2.0  virginica\n21667           5.1         3.7          1.5         0.4     setosa\n\n\n# Create the partitioned data frame\nsplit_iris &lt;- partition(bigger_iris_shuffled, cl)\nsplit_iris\nSource: party_df [150,000 x 5]\nShards: 4 [37,500--37,500 rows]\n\n# A data frame: 150,000 × 5\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n         &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;\n1          6.7         3.1          4.4         1.4 versicolor\n2          5.6         2.8          4.9         2   virginica\n3          6.4         2.8          5.6         2.2 virginica\n4          5.6         2.5          3.9         1.1 versicolor\n5          4.7         3.2          1.6         0.2 setosa\n6          6.7         3            5           1.7 versicolor\n# ℹ 149,994 more rows\n# ℹ Use `print(n = ...)` to see more rows\nIf we want the code to be reproducible, we should set the seed on each worker:\ncluster_send(cl, set.seed(123))\n\nRun cluster_send() to send code to each worker when you aren’t interested in any result (as is the case here) and cluster_call() if you want a computation to be executed on each worker and a result to be returned.\n\nNow we can run the randomForest() function on each worker:\nsplit_rfs &lt;- split_iris %&gt;%\n  do(rf = randomForest(Species ~ ., data = .))\nsplit_rfs is a partitioned data frame containing the results from each worker (the intermediate randomForest models):\nsplit_rfs\nSource: party_df [4 x 1]\nShards: 4 [1--1 rows]\n\n# A data frame: 4 × 1\n  rf\n  &lt;list&gt;\n1 &lt;rndmFrs.&gt;\n2 &lt;rndmFrs.&gt;\n3 &lt;rndmFrs.&gt;\n4 &lt;rndmFrs.&gt;\nNow we need to bring the partitioned results in the main process:\nrfs &lt;- split_rfs %&gt;% collect()\nrfs is a data frame with a single column called rf:\nrfs\n# A tibble: 4 × 1\n  rf\n  &lt;list&gt;\n1 &lt;rndmFrs.&gt;\n2 &lt;rndmFrs.&gt;\n3 &lt;rndmFrs.&gt;\n4 &lt;rndmFrs.&gt;\nWhich means that rfs$rf is a list:\ntypeof(rfs$rf)\n[1] \"list\"\nEach element of this list is a randomForest object (the 4 intermediate models created by the 4 workers):\nrfs$rf\n[[1]]\n\nCall:\n randomForest(formula = Species ~ ., data = .)\n               Type of random forest: classification\n                     Number of trees: 500\nNo. of variables tried at each split: 2\n\n        OOB estimate of  error rate: 0%\nConfusion matrix:\n           setosa versicolor virginica class.error\nsetosa      12500          0         0           0\nversicolor      0      12500         0           0\nvirginica       0          0     12500           0\n\n[[2]]\n\nCall:\n randomForest(formula = Species ~ ., data = .)\n               Type of random forest: classification\n                     Number of trees: 500\nNo. of variables tried at each split: 2\n\n        OOB estimate of  error rate: 0%\nConfusion matrix:\n           setosa versicolor virginica class.error\nsetosa      12500          0         0           0\nversicolor      0      12500         0           0\nvirginica       0          0     12500           0\n\n[[3]]\n\nCall:\n randomForest(formula = Species ~ ., data = .)\n               Type of random forest: classification\n                     Number of trees: 500\nNo. of variables tried at each split: 2\n\n        OOB estimate of  error rate: 0%\nConfusion matrix:\n           setosa versicolor virginica class.error\nsetosa      12500          0         0           0\nversicolor      0      12500         0           0\nvirginica       0          0     12500           0\n\n[[4]]\n\nCall:\n randomForest(formula = Species ~ ., data = .)\n               Type of random forest: classification\n                     Number of trees: 500\nNo. of variables tried at each split: 2\n\n        OOB estimate of  error rate: 0%\nConfusion matrix:\n           setosa versicolor virginica class.error\nsetosa      12500          0         0           0\nversicolor      0      12500         0           0\nvirginica       0          0     12500           0\n\nIf you don’t need to explore the intermediate objects, you can combine the commands as:\nrfs &lt;- split_iris %&gt;%\n  do(rf = randomForest(Species ~ ., data = .)) %&gt;%\n  collect()\n\nFinally, we need to combine the 4 randomForest models into a single one. This can be done with the combine() function from the randomForest package (the same function we already used in our foreach expressions):\nrf_all &lt;- do.call(combine, rfs$rf)\n\nBe careful that randomForest and dplyr both have a combine() function. The one we want here is the one from the randomForest package. To avoid all conflict and confusion, you can use randomForest::combine(). combine() is ok if you make sure to load dplyr before randomForest since latest loaded functions overwrite earlier loaded ones.\n\nWhy are we using do.call()? If we use:\ncombine(rfs$rf)\nWe get the silly message:\nError in combine(rfs$rf) :\n  Argument must be a list of randomForest objects\nThat is because randomForest::combine() expects a list of randomForest objects, but cannot accept an object of type list.\nHere is our final randomForest model:\nrf_all\nCall:\n randomForest(formula = Species ~ ., data = .)\n               Type of random forest: classification\n                     Number of trees: 2000\nNo. of variables tried at each split: 2\nThis is it: by splitting our data frame on 4 cores, we could run the code and create a randomForest model using whole of the data.\nWe can test our model:\nnew_data &lt;- data.frame(\n  Sepal.Length = c(5.3, 4.6, 6.5),\n  Sepal.Width = c(3.1, 3.9, 2.5),\n  Petal.Length = c(1.5, 1.5, 5.0),\n  Petal.Width = c(0.2, 0.1, 2.1)\n)\n\npredict(rf_all, new_data)\n        1         2         3\n   setosa    setosa virginica\nLevels: setosa versicolor virginica\nRunning this in an interactive session was useful to see what happens, but the way you would actually do this is by writing a script (let’s call it partition.R):\n\n\npartition.R\n\nlibrary(multidplyr)\nlibrary(dplyr, warn.conflicts = FALSE)\n\n# Create cluster of workers\ncl &lt;- new_cluster(4)\n\n# Load randomForest on each worker\ncluster_library(cl, \"randomForest\")\n\n# Create our big data frame\nbigger_iris &lt;- iris[rep(seq_len(nrow(iris)), each = 1e3), ]\nrownames(bigger_iris) &lt;- NULL\n\n# Create a partitioned data frame on the workers\nsplit_iris &lt;- partition(bigger_iris, cl)\n\n# Set the seed on each worker\ncluster_send(cl, set.seed(123))\n\n# Run the randomForest() function on each worker\nrfs &lt;- split_iris %&gt;%\n  do(rf = randomForest(Species ~ ., data = .)) %&gt;%\n  collect()\n\n# Combine the randomForest models into one\nrf_all &lt;- do.call(combine, rfs$rf)\n\nAnd run it with a Bash partition.sh script:\n\n\npartition.sh\n\n#!/bin/bash\n#SBATCH --time=10\n#SBATCH --mem-per-cpu=7500M\n#SBATCH --cpus-per-task=4\n\nRscript partition.R\n\n\n\nConclusion\nmultidplyr allowed us to split our data frame across multiple workers on one node and this solved the memory issue we had with our large dataset.",
    "crumbs": [
      "R",
      "<b><em>High-performance R</em></b>",
      "Partitioning data"
    ]
  },
  {
    "objectID": "r/hpc_partition.html#data-partitioning-for-speed",
    "href": "r/hpc_partition.html#data-partitioning-for-speed",
    "title": "Partitioning data with multidplyr",
    "section": "Data partitioning for speed",
    "text": "Data partitioning for speed\nBeside the memory advantage, are we getting any speedup from data parallelization? i.e. how does this code compare with the parallelization we did as regard the number of trees with foreach and doFuture?\nWe want to make sure to compare the same things. So we go back to our smaller big_iris and we up the number of trees back to 2000.\nWe will compare it with the plans multisession and multicore that we performed earlier. The minimum and median times for these two options for shared memory parallelism were of 2.72s and 3.15s respectively.\n\n\npartition_bench.R\n\nlibrary(multidplyr)\nlibrary(dplyr, warn.conflicts = FALSE)\nlibrary(bench)\n\ncl &lt;- new_cluster(4)\ncluster_library(cl, \"randomForest\")\n\nbig_iris &lt;- iris[rep(seq_len(nrow(iris)), each = 1e2), ]\nrownames(big_iris) &lt;- NULL\n\ncluster_send(cl, set.seed(123))\n\npart_rf &lt;- function(data, cluster) {\n  split_data &lt;- partition(data, cluster)\n  rfs &lt;- split_data %&gt;%\n    do(rf = randomForest(Species ~ ., data = ., ntree = 2000)) %&gt;%\n    collect()\n  do.call(combine, rfs$rf)\n}\n\nmark(rf_all &lt;- part_rf(big_iris, cl))\n\n\n\npartition_bench.sh\n\n#!/bin/bash\n#SBATCH --time=10\n#SBATCH --mem-per-cpu=7500M\n#SBATCH --cpus-per-task=4\n\nRscript partition_bench.R\n\nsbatch partition_bench.sh\nrandomForest 4.7-1.1\nType rfNews() to see new features/changes/bug fixes.\n\nAttaching package: ‘randomForest’\n\nThe following object is masked from ‘package:dplyr’:\n\n    combine\n\n# A tibble: 1 × 13\n  expression      min median `itr/sec` mem_alloc `gc/sec` n_itr  n_gc total_time\n  &lt;bch:expr&gt;    &lt;bch&gt; &lt;bch:&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;   &lt;bch:tm&gt;\n1 rf_all &lt;- pa… 2.48s  2.48s     0.403        NA     2.02     1     5      2.48s\n# ℹ 4 more variables: result &lt;list&gt;, memory &lt;list&gt;, time &lt;list&gt;, gc &lt;list&gt;\nWarning message:\nSome expressions had a GC in every iteration; so filtering is disabled.\n\nWhat about distributed memory?\nCan multidplyr run in distributed memory? There is nothing on this in the documentation, so I tried it.\nI upped the number of workers to 8 and ran the code on 2 nodes with 4 cores per node and got no speedup. I also created a dataset 10 times bigger (with each = 1e4), which creates an OOM on 4 cores one a single node and tried it on 11 nodes with 4 cores (10 to match the 10 times size increase, plus one to play safe). This didn’t solve the OOM issue. I tried various other tests, all with no success.\nIn conclusion, it seems that multidply’s way of creating a cluster of workers doesn’t have a mechanism to spread them across nodes and that the package thus does not allow to split data across nodes.\nIn cases where your data is so big that it doesn’t fit in the memory of a single node, it doesn’t seem that any R package currently allow to split the data automatically for you.\n\n\nConclusion\nAs we could see, we got similar results: in this case, it is the same to spread the number of trees running on the full data on 4 cores (as we did with foreach and doFuture or to run all the trees on the data spread on 4 cores.\nThe difference being that foreach and doFuture allowed us to spread the trees across nodes while multidplyr does not allow this for the data.",
    "crumbs": [
      "R",
      "<b><em>High-performance R</em></b>",
      "Partitioning data"
    ]
  },
  {
    "objectID": "r/hpc_partition.html#direct-data-loading",
    "href": "r/hpc_partition.html#direct-data-loading",
    "title": "Partitioning data with multidplyr",
    "section": "Direct data loading",
    "text": "Direct data loading\nThe method we used is very convenient, but it involves copying the data to the workers. If you want to save some memory, you can load the split data directly to the workers.\nFor this, first, split your data into several files and have all those files (and only those files) in a directory.\nThen, you can run:\nlibrary(multidplyr)\nlibrary(dplyr)\nlibrary(vroom)\n\n# Create the cluster of workers\ncl &lt;- new_cluster(4)\n\n# Create a character vector with the list of data files\nfiles &lt;- dir(\"/path/to/data/directory\", full.names = TRUE)\n\n# Split up the vector amongst the workers\ncluster_assign_partition(cl, files = files)\n\n# Create a data frame called split_iris on each worker\ncluster_send(cl, split_iris &lt;- vroom(files))\n\n# Create the partitioned data frame from the workers' data frames\nsplit_iris &lt;- party_df(cl, \"split_iris\")\nFrom here on, you can work as we did earlier.",
    "crumbs": [
      "R",
      "<b><em>High-performance R</em></b>",
      "Partitioning data"
    ]
  },
  {
    "objectID": "r/hpc_rcpp.html",
    "href": "r/hpc_rcpp.html",
    "title": "Writing C++ in R with Rcpp",
    "section": "",
    "text": "Sometimes, parallelization is not an option, either because the code is hard to parallelize or because of lack of hardware. In such cases, one way to increase speed is to replace slow R code with C++. The package Rcpp makes this particularly easy by creating mappings between both languages.",
    "crumbs": [
      "R",
      "<b><em>High-performance R</em></b>",
      "Writing C++ in R with Rcpp"
    ]
  },
  {
    "objectID": "r/hpc_rcpp.html#back-to-fibonacci",
    "href": "r/hpc_rcpp.html#back-to-fibonacci",
    "title": "Writing C++ in R with Rcpp",
    "section": "Back to Fibonacci",
    "text": "Back to Fibonacci\nDo you remember the Fibonacci numbers? Here was a naive implementation in R:\n\nfib &lt;- function(n) {\n  if(n == 0) return(0)\n  if(n == 1) return(1)\n  Recall(n - 1) + Recall(n - 2)\n}\n\nThis function gives the nth number in the sequence.\n\nExample:\n\n\nfib(30)\n\n[1] 832040",
    "crumbs": [
      "R",
      "<b><em>High-performance R</em></b>",
      "Writing C++ in R with Rcpp"
    ]
  },
  {
    "objectID": "r/hpc_rcpp.html#rcpp",
    "href": "r/hpc_rcpp.html#rcpp",
    "title": "Writing C++ in R with Rcpp",
    "section": "Rcpp",
    "text": "Rcpp\nLet’s translate this function in C++ within R!\nFirst we need to load the Rcpp package:\n\nlibrary(Rcpp)\n\nWe then use the function cppFunction() to assign to an R function a function written in C++:\n\nfibRcpp &lt;- cppFunction( '\nint fibonacci(const int x) {\n   if (x == 0) return(0);\n   if (x == 1) return(1);\n   return (fibonacci(x - 1)) + fibonacci(x - 2);\n}\n' )\n\nWe can call our function as any R function:\nfibRcpp(30)\n[1] 832040\nWe can compare both functions:\nlibrary(bench)\n\nn &lt;- 30\nmark(fib(n), fibRcpp(n))\n# A tibble: 2 × 13\n  expression      min   median `itr/sec` mem_alloc `gc/sec` n_itr  n_gc\n  &lt;bch:expr&gt; &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;\n1 fib(n)        1.66s    1.66s     0.601    44.7KB     22.8     1    38\n2 fibRcpp(n)   1.08ms   1.08ms   901.       2.49KB      0     451     0\n  total_time result    memory                 time            \n    &lt;bch:tm&gt; &lt;list&gt;    &lt;list&gt;                 &lt;list&gt;          \n1      1.66s &lt;dbl [1]&gt; &lt;Rprofmem [6,778 × 3]&gt; &lt;bench_tm [1]&gt;  \n2   500.37ms &lt;int [1]&gt; &lt;Rprofmem [1 × 3]&gt;     &lt;bench_tm [451]&gt;\n  gc                \n  &lt;list&gt;            \n1 &lt;tibble [1 × 3]&gt;  \n2 &lt;tibble [451 × 3]&gt;\nWarning message:\nSome expressions had a GC in every iteration; so filtering is disabled.\nThe speedup is 1,537, which is amazing.\nIn this particular example, we saw that memoisation gives an even more incredible speedup (35,000!), but while memoisation will only work in very specific situations (e.g. recursive function calls), using C++ code is a general method to provide speedup. It is particularly useful when:\n\nthere are large numbers of function calls (R is particularly slow with function calls),\nyou need data structures that are missing in R,\nyou want to create efficient packages (fast R packages are written in C++ and many use Rcpp).\n\n\nIn this example, we declared the C++ function directly in R. It is possible to use source files instead.",
    "crumbs": [
      "R",
      "<b><em>High-performance R</em></b>",
      "Writing C++ in R with Rcpp"
    ]
  },
  {
    "objectID": "r/hpc_run.html",
    "href": "r/hpc_run.html",
    "title": "SSH login",
    "section": "",
    "text": "This section will show you how to access our temporary remote cluster through SSH.",
    "crumbs": [
      "R",
      "<b><em>High-performance R</em></b>",
      "SSH login"
    ]
  },
  {
    "objectID": "r/hpc_run.html#why-not-use-rstudio-server",
    "href": "r/hpc_run.html#why-not-use-rstudio-server",
    "title": "SSH login",
    "section": "Why not use RStudio server?",
    "text": "Why not use RStudio server?\nIn our introduction to R, we used an RStudio server running on a remote cluster. In this course, we will log in a similar remote supercomputer using Secure Shell, then run R scripts from the command line.\nWhy are we not making use of the interactivity of R which is an interpreted language and why are we not using the added comfort of an IDE? The short answer is: resource efficiency.\nOnce you have developed your code in an interactive fashion in the IDE of your choice using small hardware resources on a sample of your data, running scripts allows you to only request large resources when you need them (i.e. when your code is running). This prevents heavy resources from sitting idle when not in use, as would happen in an interactive session while you type, think, etc. It will save you money on commercial clusters and waiting time on the Alliance clusters.\nThis course being about high-performance R, let’s learn to use it through scripts.",
    "crumbs": [
      "R",
      "<b><em>High-performance R</em></b>",
      "SSH login"
    ]
  },
  {
    "objectID": "r/hpc_run.html#logging-in-the-temporary-cluster-through-ssh",
    "href": "r/hpc_run.html#logging-in-the-temporary-cluster-through-ssh",
    "title": "SSH login",
    "section": "Logging in the temporary cluster through SSH",
    "text": "Logging in the temporary cluster through SSH\nYou do not need to install anything on your machine for this course as we will provide access to a temporary remote cluster.\n\nA username, hostname, and password will be given to you during the workshop.\n\n\nNote that this temporary cluster will only be available for the duration of this course.\n\n\nOpen a terminal emulator\nWindows users:  Install the free version of MobaXTerm and launch it.\nMacOS users:   Launch Terminal.\nLinux users:     Open the terminal emulator of your choice.\n\n\nAccess the cluster through secure shell\n\nWindows users\nFollow the first 18% of this demo.\nFor “Remote host”, use the hostname we gave you.\nSelect the box “Specify username” and provide your username.\n\nNote that the password is entered through blind typing, meaning that you will not see anything happening as you type it. This is a Linux feature. While it is a little disturbing at first, do know that it is working. Make sure to type it slowly to avoid typos, then press the “enter” key on your keyboard.\n\n\n\nMacOS and Linux users\nIn the terminal, run:\nssh &lt;username&gt;@&lt;hostname&gt;\n\nReplace the username and hostname by their values. For instance:\nssh user021@somecluster.c3.ca\n\nYou will be asked a question, answer “Yes”.\nWhen prompted, type the password.\n\nNote that the password is entered through blind typing, meaning that you will not see anything happening as you type it. This is a Linux feature. While it is a little disturbing at first, do know that it is working. Make sure to type it slowly to avoid typos, then press the “enter” key on your keyboard.\n\n\n\n\nTroubleshooting\nProblems logging in are almost always due to typos. If you cannot log in, retry slowly, entering your password carefully.",
    "crumbs": [
      "R",
      "<b><em>High-performance R</em></b>",
      "SSH login"
    ]
  },
  {
    "objectID": "r/hss_automation.html",
    "href": "r/hss_automation.html",
    "title": "Automation",
    "section": "",
    "text": "One of the strengths of programming is the ability to automate tasks.\nIn this section, we will see how a loop can automate the creation of file names.\n\nLet’s say that we now want to import data from 5 files arc1.csv, …, arc5.csv and create 5 data frames with their data.\nWe need a character vector with the file names.\nWe could create it this way:\n\nfiles &lt;- c(\n  \"https://mint.westdri.ca/r/hss_data/arc1.csv\",\n  \"https://mint.westdri.ca/r/hss_data/arc2.csv\",\n  \"https://mint.westdri.ca/r/hss_data/arc3.csv\",\n  \"https://mint.westdri.ca/r/hss_data/arc4.csv\",\n  \"https://mint.westdri.ca/r/hss_data/arc5.csv\"\n)\n\nIt works of course:\n\nfiles\n\n[1] \"https://mint.westdri.ca/r/hss_data/arc1.csv\"\n[2] \"https://mint.westdri.ca/r/hss_data/arc2.csv\"\n[3] \"https://mint.westdri.ca/r/hss_data/arc3.csv\"\n[4] \"https://mint.westdri.ca/r/hss_data/arc4.csv\"\n[5] \"https://mint.westdri.ca/r/hss_data/arc5.csv\"\n\n\nBut if we had 50 files instead of 5, it would be quite a tedium! And if we had 500 files, it would be unrealistic. A better approach is to write a loop.\nIn order to store the results of a loop, we need to create an empty object and assign to it the result of the loop at each iteration. It is very important to pre-allocate memory: by creating an empty object of the final size, the necessary memory to hold this object is requested once (then the object gets filled in while the loop runs). Without this, more memory would have to be allocated at each iteration of the loop and this is highly inefficient.\nSo let’s create an empty vector of length 5 and of type character:\n\nfiles &lt;- character(5)\n\nNow we can fill in our vector with the proper values with the loop:\n\nfor (i in 1:5) {\n  files[i] &lt;- paste0(\"https://mint.westdri.ca/r/hss_data/arc\", i, \".csv\")\n}\n\nThis gives us the same result, but the big difference is that it is scalable:\n\nfiles\n\n[1] \"https://mint.westdri.ca/r/hss_data/arc1.csv\"\n[2] \"https://mint.westdri.ca/r/hss_data/arc2.csv\"\n[3] \"https://mint.westdri.ca/r/hss_data/arc3.csv\"\n[4] \"https://mint.westdri.ca/r/hss_data/arc4.csv\"\n[5] \"https://mint.westdri.ca/r/hss_data/arc5.csv\"\n\n\nNow, if our files were not named following such a nice sequence, we would have to modify our loop a little. Below are two examples:\n\nfiles &lt;- character(5)\n\nfor (i in seq_along(c(3, 6, 9, 10, 14))) {\n  files[i] &lt;- paste0(\n    \"https://mint.westdri.ca/r/hss_data/arc\",\n    c(3, 6, 9, 10, 14)[i],\n    \".csv\"\n  )\n}\n\nfiles\n\n[1] \"https://mint.westdri.ca/r/hss_data/arc3.csv\" \n[2] \"https://mint.westdri.ca/r/hss_data/arc6.csv\" \n[3] \"https://mint.westdri.ca/r/hss_data/arc9.csv\" \n[4] \"https://mint.westdri.ca/r/hss_data/arc10.csv\"\n[5] \"https://mint.westdri.ca/r/hss_data/arc14.csv\"\n\n\n\nfiles &lt;- character(5)\n\nfor (i in seq_along(c(\"_a\", \"_b\", \"_c\", \"_d\", \"_e\"))) {\n  files[i] &lt;- paste0(\n    \"https://mint.westdri.ca/r/hss_data/arc\",\n    c(\"_a\", \"_b\", \"_c\", \"_d\", \"_e\")[i],\n    \".csv\"\n  )\n}\n\nfiles\n\n[1] \"https://mint.westdri.ca/r/hss_data/arc_a.csv\"\n[2] \"https://mint.westdri.ca/r/hss_data/arc_b.csv\"\n[3] \"https://mint.westdri.ca/r/hss_data/arc_c.csv\"\n[4] \"https://mint.westdri.ca/r/hss_data/arc_d.csv\"\n[5] \"https://mint.westdri.ca/r/hss_data/arc_e.csv\"\n\n\n\nIf you had all the files in one directory, an alternative approach would be to create a list of all the names matching a regular expression.\nIn our case, we would use:\nfiles &lt;- list.files(pattern=\"^arc\\\\d+\\\\.csv$\")",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Automation"
    ]
  },
  {
    "objectID": "r/hss_explore.html",
    "href": "r/hss_explore.html",
    "title": "Data exploration",
    "section": "",
    "text": "An important first step of data analysis is to have a look at the data. In this section, we will explore the us_contagious_diseases dataset from the dslabs package.",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Data exploration"
    ]
  },
  {
    "objectID": "r/hss_explore.html#load-the-dslabs-package",
    "href": "r/hss_explore.html#load-the-dslabs-package",
    "title": "Data exploration",
    "section": "Load the dslabs package",
    "text": "Load the dslabs package\nThis package contains a number of datasets. To access any of them, we first need to load the package:\n\nlibrary(dslabs)\n\n\nlibrary() is a function:\n\nclass(library)\n\n[1] \"function\"\n\n\nFunctions are the “verbs” of programming languages. They do things.\nlibrary() is a function that loads packages into the current session so that their content becomes available.\ndslabs is the argument that we pass to the function library(): it is this particular packages that we are loading in the session here.\nclass() is also a function: it tells what class an object belongs to. In class(library), library is the argument of the function class().",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Data exploration"
    ]
  },
  {
    "objectID": "r/hss_explore.html#printing-data-to-screen",
    "href": "r/hss_explore.html#printing-data-to-screen",
    "title": "Data exploration",
    "section": "Printing data to screen",
    "text": "Printing data to screen\nTo print all the data, we would simply run us_contagious_diseases. There are a lot of rows however, so we only want to print a subset to the screen.\nTo print the first six rows, we use the function head(), using our data as the argument:\n\nhead(us_contagious_diseases)\n\n      disease   state year weeks_reporting count population\n1 Hepatitis A Alabama 1966              50   321    3345787\n2 Hepatitis A Alabama 1967              49   291    3364130\n3 Hepatitis A Alabama 1968              52   314    3386068\n4 Hepatitis A Alabama 1969              49   380    3412450\n5 Hepatitis A Alabama 1970              51   413    3444165\n6 Hepatitis A Alabama 1971              51   378    3481798\n\n\nIf you look at the documentation of the head() function (by running ?head), you can see that it accepts another argument that allows us to set the number of rows to print.\nLet’s print the first 15 rows:\n\nhead(us_contagious_diseases, n = 15)\n\n       disease   state year weeks_reporting count population\n1  Hepatitis A Alabama 1966              50   321    3345787\n2  Hepatitis A Alabama 1967              49   291    3364130\n3  Hepatitis A Alabama 1968              52   314    3386068\n4  Hepatitis A Alabama 1969              49   380    3412450\n5  Hepatitis A Alabama 1970              51   413    3444165\n6  Hepatitis A Alabama 1971              51   378    3481798\n7  Hepatitis A Alabama 1972              45   342    3524543\n8  Hepatitis A Alabama 1973              45   467    3571209\n9  Hepatitis A Alabama 1974              45   244    3620548\n10 Hepatitis A Alabama 1975              46   286    3671246\n11 Hepatitis A Alabama 1976              50   220    3721914\n12 Hepatitis A Alabama 1977              43   206    3771085\n13 Hepatitis A Alabama 1978              41   203    3817217\n14 Hepatitis A Alabama 1979              47   257    3858703\n15 Hepatitis A Alabama 1980              37   200    3893888\n\n\n\nBy default, n = 6 which is why head() prints six rows unless we specify otherwise. The L in the documentation of the print() function (n = 6L) means that 6 is an integer. You can ignore this for now.\nArguments can be passed to functions as positional arguments (then they have to respect the position of the function definition) or as named arguments (in that case, you need to use the arguments names).\nThat means that iff we keep the arguments in the right order, we can omit the name of the argument (n here) and only write its value (15). :\n\nhead(us_contagious_diseases, 15)\n\n       disease   state year weeks_reporting count population\n1  Hepatitis A Alabama 1966              50   321    3345787\n2  Hepatitis A Alabama 1967              49   291    3364130\n3  Hepatitis A Alabama 1968              52   314    3386068\n4  Hepatitis A Alabama 1969              49   380    3412450\n5  Hepatitis A Alabama 1970              51   413    3444165\n6  Hepatitis A Alabama 1971              51   378    3481798\n7  Hepatitis A Alabama 1972              45   342    3524543\n8  Hepatitis A Alabama 1973              45   467    3571209\n9  Hepatitis A Alabama 1974              45   244    3620548\n10 Hepatitis A Alabama 1975              46   286    3671246\n11 Hepatitis A Alabama 1976              50   220    3721914\n12 Hepatitis A Alabama 1977              43   206    3771085\n13 Hepatitis A Alabama 1978              41   203    3817217\n14 Hepatitis A Alabama 1979              47   257    3858703\n15 Hepatitis A Alabama 1980              37   200    3893888\n\n\nIf the arguments are given to the function out of order however, we do need to use their names.\nThis won’t work because R needs an integer for n or for the 2nd argument:\n\nhead(15, us_contagious_diseases)\n\nError in head.default(15, us_contagious_diseases): invalid 'n' - must be numeric, possibly NA.\n\n\nThis however works:\n\nhead(n = 15, us_contagious_diseases)\n\n       disease   state year weeks_reporting count population\n1  Hepatitis A Alabama 1966              50   321    3345787\n2  Hepatitis A Alabama 1967              49   291    3364130\n3  Hepatitis A Alabama 1968              52   314    3386068\n4  Hepatitis A Alabama 1969              49   380    3412450\n5  Hepatitis A Alabama 1970              51   413    3444165\n6  Hepatitis A Alabama 1971              51   378    3481798\n7  Hepatitis A Alabama 1972              45   342    3524543\n8  Hepatitis A Alabama 1973              45   467    3571209\n9  Hepatitis A Alabama 1974              45   244    3620548\n10 Hepatitis A Alabama 1975              46   286    3671246\n11 Hepatitis A Alabama 1976              50   220    3721914\n12 Hepatitis A Alabama 1977              43   206    3771085\n13 Hepatitis A Alabama 1978              41   203    3817217\n14 Hepatitis A Alabama 1979              47   257    3858703\n15 Hepatitis A Alabama 1980              37   200    3893888\n\n\n\nWe can also print the last 6 rows of the data:\n\ntail(us_contagious_diseases)\n\n       disease   state year weeks_reporting count population\n16060 Smallpox Wyoming 1947              49     1     276297\n16061 Smallpox Wyoming 1948              24     1     280803\n16062 Smallpox Wyoming 1949               0     0     285544\n16063 Smallpox Wyoming 1950               1     2     290529\n16064 Smallpox Wyoming 1951               1     1     295744\n16065 Smallpox Wyoming 1952               1     1     301083\n\n\n\n\nYour turn:\n\nHow would you print the last 10 rows of the data?",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Data exploration"
    ]
  },
  {
    "objectID": "r/hss_explore.html#structure-of-the-data-object",
    "href": "r/hss_explore.html#structure-of-the-data-object",
    "title": "Data exploration",
    "section": "Structure of the data object",
    "text": "Structure of the data object\nus_contagious_diseases is an R object containing the dataset, but what kind of object is it?\n\nclass(us_contagious_diseases)\n\n[1] \"data.frame\"\n\n\nOur data is in a class of R object called a data frame.\nWe can get its full structure with:\n\nstr(us_contagious_diseases)\n\n'data.frame':   16065 obs. of  6 variables:\n $ disease        : Factor w/ 7 levels \"Hepatitis A\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ state          : Factor w/ 51 levels \"Alabama\",\"Alaska\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ year           : num  1966 1967 1968 1969 1970 ...\n $ weeks_reporting: num  50 49 52 49 51 51 45 45 45 46 ...\n $ count          : num  321 291 314 380 413 378 342 467 244 286 ...\n $ population     : num  3345787 3364130 3386068 3412450 3444165 ...\n\n\nThe names of the variables can be obtained with:\n\nnames(us_contagious_diseases)\n\n[1] \"disease\"         \"state\"           \"year\"            \"weeks_reporting\"\n[5] \"count\"           \"population\"     \n\n\nYou can display the data frame in a tabular fashion thanks to:\nView(us_contagious_diseases)",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Data exploration"
    ]
  },
  {
    "objectID": "r/hss_explore.html#dimensions-of-our-data-frame",
    "href": "r/hss_explore.html#dimensions-of-our-data-frame",
    "title": "Data exploration",
    "section": "Dimensions of our data frame",
    "text": "Dimensions of our data frame\n\ndim(us_contagious_diseases)\n\n[1] 16065     6\n\nncol(us_contagious_diseases)\n\n[1] 6\n\nnrow(us_contagious_diseases)\n\n[1] 16065\n\n\n\nlength(us_contagious_diseases)\n\n[1] 6\n\nlength(us_contagious_diseases$disease)\n\n[1] 16065",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Data exploration"
    ]
  },
  {
    "objectID": "r/hss_explore.html#summary-statistics",
    "href": "r/hss_explore.html#summary-statistics",
    "title": "Data exploration",
    "section": "Summary statistics",
    "text": "Summary statistics\n\nsummary(us_contagious_diseases)\n\n        disease            state            year      weeks_reporting\n Hepatitis A:2346   Alabama   :  315   Min.   :1928   Min.   : 0.00  \n Measles    :3825   Alaska    :  315   1st Qu.:1950   1st Qu.:31.00  \n Mumps      :1785   Arizona   :  315   Median :1975   Median :46.00  \n Pertussis  :2856   Arkansas  :  315   Mean   :1971   Mean   :37.38  \n Polio      :2091   California:  315   3rd Qu.:1990   3rd Qu.:50.00  \n Rubella    :1887   Colorado  :  315   Max.   :2011   Max.   :52.00  \n Smallpox   :1275   (Other)   :14175                                 \n     count          population      \n Min.   :     0   Min.   :   86853  \n 1st Qu.:     7   1st Qu.: 1018755  \n Median :    69   Median : 2749249  \n Mean   :  1492   Mean   : 4107584  \n 3rd Qu.:   525   3rd Qu.: 4996229  \n Max.   :132342   Max.   :37607525  \n                  NA's   :214",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Data exploration"
    ]
  },
  {
    "objectID": "r/hss_help.html",
    "href": "r/hss_help.html",
    "title": "Help and documentation",
    "section": "",
    "text": "One of the strengths of R is its great documentation. Here, we will learn how to make use of it.",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Help and documentation"
    ]
  },
  {
    "objectID": "r/hss_help.html#general-documentation",
    "href": "r/hss_help.html#general-documentation",
    "title": "Help and documentation",
    "section": "General documentation",
    "text": "General documentation\nTo get started with R, you can launch the general documentation with:\nhelp.start()",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Help and documentation"
    ]
  },
  {
    "objectID": "r/hss_help.html#help-on-functions",
    "href": "r/hss_help.html#help-on-functions",
    "title": "Help and documentation",
    "section": "Help on functions",
    "text": "Help on functions\nTo get help on specific objects (e.g. the function sum), you can run:\nhelp(sum)\nor:\n?sum\n\nThe documentation pages always follow the same format:\n\nName of the object and the package it comes from\nA short description of the object\nThe code to use it\nAn explanation of the arguments (in the case of functions)\nExplanation with greater details\nThe value returned (in the case of functions)\nExamples of code snippets that can be run",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Help and documentation"
    ]
  },
  {
    "objectID": "r/hss_manipulate.html",
    "href": "r/hss_manipulate.html",
    "title": "Data extraction",
    "section": "",
    "text": "It is often useful to focus on sections of the data to plot or analyse. In this section, we will see how to extract various elements of the us_contagious_diseases dataset from the dslabs package.",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Data extraction"
    ]
  },
  {
    "objectID": "r/hss_manipulate.html#load-packages",
    "href": "r/hss_manipulate.html#load-packages",
    "title": "Data extraction",
    "section": "Load packages",
    "text": "Load packages\nOne of the tidyverse packages is very useful for data manipulation: dplyr. Let’s load the dslabs package again as well as dplyr:\n\nlibrary(dslabs)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Data extraction"
    ]
  },
  {
    "objectID": "r/hss_manipulate.html#indexing",
    "href": "r/hss_manipulate.html#indexing",
    "title": "Data extraction",
    "section": "Indexing",
    "text": "Indexing\nYou can extract a subset of the data using their position by indexing. Indexing in R starts with 1 (in many languages, the first index is 0) and it is done with square brackets. Since a data frame has two dimensions, there are two possible indices in the square brackets:\n\nthe row index,\nthe column index.\n\nYou can index a single element:\n\nus_contagious_diseases[1, 1]\n\n[1] Hepatitis A\nLevels: Hepatitis A Measles Mumps Pertussis Polio Rubella Smallpox\n\nus_contagious_diseases[1, 2]\n\n[1] Alabama\n51 Levels: Alabama Alaska Arizona Arkansas California Colorado ... Wyoming\n\n\nOr a full row:\n\nus_contagious_diseases[1, ]\n\n      disease   state year weeks_reporting count population\n1 Hepatitis A Alabama 1966              50   321    3345787\n\nus_contagious_diseases[3000, ]\n\n     disease                state year weeks_reporting count population\n3000 Measles District Of Columbia 1981              27     2     631010\n\n\n\n\nYour turn:\n\nHow would you index the year column?",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Data extraction"
    ]
  },
  {
    "objectID": "r/hss_manipulate.html#filtering-rows",
    "href": "r/hss_manipulate.html#filtering-rows",
    "title": "Data extraction",
    "section": "Filtering rows",
    "text": "Filtering rows\nYou can also filter data points based on their values:\n\nus_contagious_diseases |&gt;\n  filter(state == \"California\") |&gt;\n  count()\n\n    n\n1 315\n\n\n\n\nYour turn:\n\nHow many data points are there for the state of Arizona?\n\n\nus_contagious_diseases |&gt;\n  filter(state == \"California\" & year &gt; 2000)\n\n       disease      state year weeks_reporting count population\n1  Hepatitis A California 2001              40  1599   34199784\n2  Hepatitis A California 2002              49  1364   34529758\n3  Hepatitis A California 2003              46  1045   34861711\n4  Hepatitis A California 2004              48   788   35195792\n5  Hepatitis A California 2005              49   905   35532154\n6  Hepatitis A California 2006              52   688   35870957\n7  Hepatitis A California 2007              51   312   36212364\n8  Hepatitis A California 2008              52   337   36556548\n9  Hepatitis A California 2009              52   239   36903684\n10 Hepatitis A California 2010              49   201   37253956\n11 Hepatitis A California 2011              49   176   37607525\n12     Measles California 2001              40    34   34199784\n13     Measles California 2002              33     0   34529758\n14       Mumps California 2001              49    37   34199784\n15       Mumps California 2002              49    66   34529758\n16   Pertussis California 2001              40   440   34199784\n17   Pertussis California 2002              43   698   34529758\n18   Pertussis California 2003              41   635   34861711\n19   Pertussis California 2004              36   498   35195792\n20   Pertussis California 2005              45  1609   35532154\n21   Pertussis California 2006              42   831   35870957\n22   Pertussis California 2007              29    95   36212364\n23   Pertussis California 2008              39   276   36556548\n24   Pertussis California 2009              40   415   36903684\n25   Pertussis California 2010              48  1265   37253956\n26   Pertussis California 2011              49  1145   37607525\n27     Rubella California 2001               1     0   34199784\n28     Rubella California 2002              29     2   34529758\n\n\n\nus_contagious_diseases |&gt;\n  filter(state == \"California\" & year &gt; 2000) |&gt;\n  arrange(year)\n\n       disease      state year weeks_reporting count population\n1  Hepatitis A California 2001              40  1599   34199784\n2      Measles California 2001              40    34   34199784\n3        Mumps California 2001              49    37   34199784\n4    Pertussis California 2001              40   440   34199784\n5      Rubella California 2001               1     0   34199784\n6  Hepatitis A California 2002              49  1364   34529758\n7      Measles California 2002              33     0   34529758\n8        Mumps California 2002              49    66   34529758\n9    Pertussis California 2002              43   698   34529758\n10     Rubella California 2002              29     2   34529758\n11 Hepatitis A California 2003              46  1045   34861711\n12   Pertussis California 2003              41   635   34861711\n13 Hepatitis A California 2004              48   788   35195792\n14   Pertussis California 2004              36   498   35195792\n15 Hepatitis A California 2005              49   905   35532154\n16   Pertussis California 2005              45  1609   35532154\n17 Hepatitis A California 2006              52   688   35870957\n18   Pertussis California 2006              42   831   35870957\n19 Hepatitis A California 2007              51   312   36212364\n20   Pertussis California 2007              29    95   36212364\n21 Hepatitis A California 2008              52   337   36556548\n22   Pertussis California 2008              39   276   36556548\n23 Hepatitis A California 2009              52   239   36903684\n24   Pertussis California 2009              40   415   36903684\n25 Hepatitis A California 2010              49   201   37253956\n26   Pertussis California 2010              48  1265   37253956\n27 Hepatitis A California 2011              49   176   37607525\n28   Pertussis California 2011              49  1145   37607525\n\n\n\nus_contagious_diseases |&gt;\n  filter(state == \"California\" & year &gt; 2000) |&gt;\n  arrange(count)\n\n       disease      state year weeks_reporting count population\n1      Measles California 2002              33     0   34529758\n2      Rubella California 2001               1     0   34199784\n3      Rubella California 2002              29     2   34529758\n4      Measles California 2001              40    34   34199784\n5        Mumps California 2001              49    37   34199784\n6        Mumps California 2002              49    66   34529758\n7    Pertussis California 2007              29    95   36212364\n8  Hepatitis A California 2011              49   176   37607525\n9  Hepatitis A California 2010              49   201   37253956\n10 Hepatitis A California 2009              52   239   36903684\n11   Pertussis California 2008              39   276   36556548\n12 Hepatitis A California 2007              51   312   36212364\n13 Hepatitis A California 2008              52   337   36556548\n14   Pertussis California 2009              40   415   36903684\n15   Pertussis California 2001              40   440   34199784\n16   Pertussis California 2004              36   498   35195792\n17   Pertussis California 2003              41   635   34861711\n18 Hepatitis A California 2006              52   688   35870957\n19   Pertussis California 2002              43   698   34529758\n20 Hepatitis A California 2004              48   788   35195792\n21   Pertussis California 2006              42   831   35870957\n22 Hepatitis A California 2005              49   905   35532154\n23 Hepatitis A California 2003              46  1045   34861711\n24   Pertussis California 2011              49  1145   37607525\n25   Pertussis California 2010              48  1265   37253956\n26 Hepatitis A California 2002              49  1364   34529758\n27 Hepatitis A California 2001              40  1599   34199784\n28   Pertussis California 2005              45  1609   35532154\n\n\n\nus_contagious_diseases |&gt;\n  filter(state == \"California\" & year &gt; 2000) |&gt;\n  arrange(desc(count))\n\n       disease      state year weeks_reporting count population\n1    Pertussis California 2005              45  1609   35532154\n2  Hepatitis A California 2001              40  1599   34199784\n3  Hepatitis A California 2002              49  1364   34529758\n4    Pertussis California 2010              48  1265   37253956\n5    Pertussis California 2011              49  1145   37607525\n6  Hepatitis A California 2003              46  1045   34861711\n7  Hepatitis A California 2005              49   905   35532154\n8    Pertussis California 2006              42   831   35870957\n9  Hepatitis A California 2004              48   788   35195792\n10   Pertussis California 2002              43   698   34529758\n11 Hepatitis A California 2006              52   688   35870957\n12   Pertussis California 2003              41   635   34861711\n13   Pertussis California 2004              36   498   35195792\n14   Pertussis California 2001              40   440   34199784\n15   Pertussis California 2009              40   415   36903684\n16 Hepatitis A California 2008              52   337   36556548\n17 Hepatitis A California 2007              51   312   36212364\n18   Pertussis California 2008              39   276   36556548\n19 Hepatitis A California 2009              52   239   36903684\n20 Hepatitis A California 2010              49   201   37253956\n21 Hepatitis A California 2011              49   176   37607525\n22   Pertussis California 2007              29    95   36212364\n23       Mumps California 2002              49    66   34529758\n24       Mumps California 2001              49    37   34199784\n25     Measles California 2001              40    34   34199784\n26     Rubella California 2002              29     2   34529758\n27     Measles California 2002              33     0   34529758\n28     Rubella California 2001               1     0   34199784",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Data extraction"
    ]
  },
  {
    "objectID": "r/hss_manipulate.html#selecting-columns",
    "href": "r/hss_manipulate.html#selecting-columns",
    "title": "Data extraction",
    "section": "Selecting columns",
    "text": "Selecting columns\nWe saw how to index columns from their position. It is also possible to select them based on their names:\n\nhead(us_contagious_diseases$year, 50)\n\n [1] 1966 1967 1968 1969 1970 1971 1972 1973 1974 1975 1976 1977 1978 1979 1980\n[16] 1981 1982 1983 1984 1985 1986 1987 1988 1989 1990 1991 1992 1993 1994 1995\n[31] 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010\n[46] 2011 1966 1967 1968 1969\n\n\nIf you want to select several columns, you can use the select() function from dplyr:\n\nus_contagious_diseases |&gt;\n  filter(state == \"California\" & year &gt; 2000 & disease == \"Hepatitis A\") |&gt;\n  select(year, count, population)\n\n   year count population\n1  2001  1599   34199784\n2  2002  1364   34529758\n3  2003  1045   34861711\n4  2004   788   35195792\n5  2005   905   35532154\n6  2006   688   35870957\n7  2007   312   36212364\n8  2008   337   36556548\n9  2009   239   36903684\n10 2010   201   37253956\n11 2011   176   37607525",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Data extraction"
    ]
  },
  {
    "objectID": "r/hss_manipulate.html#grouping-data",
    "href": "r/hss_manipulate.html#grouping-data",
    "title": "Data extraction",
    "section": "Grouping data",
    "text": "Grouping data\nIt is often useful to group data by categories to compute some summary statistics.\nFor instance, we can group by year and calculate the total numbers of infections:\n\nus_contagious_diseases |&gt;\n  group_by(year) |&gt;\n  summarise(total = sum(count))\n\n# A tibble: 84 × 2\n    year  total\n   &lt;dbl&gt;  &lt;dbl&gt;\n 1  1928 524563\n 2  1929 380196\n 3  1930 439289\n 4  1931 482886\n 5  1932 404683\n 6  1933 391485\n 7  1934 739509\n 8  1935 739224\n 9  1936 292530\n10  1937 314425\n# ℹ 74 more rows\n\n\nAlternatively, we can group by state and get the totals:\n\nus_contagious_diseases |&gt;\n  group_by(state) |&gt; \n  summarise(total = sum(count))\n\n# A tibble: 51 × 2\n   state                  total\n   &lt;fct&gt;                  &lt;dbl&gt;\n 1 Alabama               257979\n 2 Alaska                 29136\n 3 Arizona               240233\n 4 Arkansas              177556\n 5 California           1906067\n 6 Colorado              322845\n 7 Connecticut           463148\n 8 Delaware               44427\n 9 District Of Columbia   77012\n10 Florida               268383\n# ℹ 41 more rows\n\n\nWe can also group by year and state and get the totals:\n\nus_contagious_diseases |&gt;\n  group_by(year, state) |&gt; \n  summarise(total = sum(count))\n\n`summarise()` has grouped output by 'year'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 4,284 × 3\n# Groups:   year [84]\n    year state                total\n   &lt;dbl&gt; &lt;fct&gt;                &lt;dbl&gt;\n 1  1928 Alabama               9246\n 2  1928 Alaska                   0\n 3  1928 Arizona               1268\n 4  1928 Arkansas              9157\n 5  1928 California            4960\n 6  1928 Colorado              2510\n 7  1928 Connecticut          10247\n 8  1928 Delaware               607\n 9  1928 District Of Columbia  2609\n10  1928 Florida               1892\n# ℹ 4,274 more rows",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Data extraction"
    ]
  },
  {
    "objectID": "r/hss_publish.html",
    "href": "r/hss_publish.html",
    "title": "Publishing",
    "section": "",
    "text": "You might have heard of R Markdown: a way to intertwine code and prose in a single scientific document. The company behind R Markdown has now developed its successor: Quarto.\n\nQuarto allows the creation of webpages, websites, presentations, books, pdf, etc. from code in R, Python, or Julia and markdown text.\nIf you are interested in an introduction to this tool, you can have a look at our workshop or our webinar on Quarto.\nBy the way, this entire website was created with Quarto 🙂",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Publishing"
    ]
  },
  {
    "objectID": "r/hss_run.html",
    "href": "r/hss_run.html",
    "title": "Running R",
    "section": "",
    "text": "This section covers the various ways R can be run, then shows you how to access our temporary RStudio server for this course.",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Running R"
    ]
  },
  {
    "objectID": "r/hss_run.html#running-r",
    "href": "r/hss_run.html#running-r",
    "title": "Running R",
    "section": "Running R",
    "text": "Running R\nR being an interpreted language, it can be run non-interactively or interactively.\n\nRunning R non-interactively\nIf you write code in a text file (called a script), you can then execute it with:\nRscript my_script.R\n\nThe command to execute scripts is Rscript rather than R.\nBy convention, R scripts take the extension .R.\n\n\n\nRunning R interactively\nThere are several ways to run R interactively.\n\nDirectly in the console (the name for the R shell):\n\n\n\nIn Jupyter with the R kernel (IRkernel package).\nIn another IDE (e.g. in Emacs with ESS).\nIn the RStudio IDE.\n\nThe RStudio IDE is popular and this is what we will use today. RStudio can can be run locally, but for this course, we will use an RStudio server.",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Running R"
    ]
  },
  {
    "objectID": "r/hss_run.html#accessing-our-rstudio-server",
    "href": "r/hss_run.html#accessing-our-rstudio-server",
    "title": "Running R",
    "section": "Accessing our RStudio server",
    "text": "Accessing our RStudio server\nYou do not need to install anything on your machine for this course as we will provide access to a temporary RStudio server.\n\nA username, a password, and the URL of the RStudio server will be given to you during the workshop.\n\nSign in using the username and password you will be given while ignoring the OTP entry. This will take you to the server options page of a JupyterHub.\n\nSelect the following server options:\n\nTime: 2 hours\nNumber of cores: 1\nMemory: 3700 MB\nUser interface: JupyterLab\n\n\nThen press “Start” to launch the JupyterHub. There, click on the “RStudio” button and the RStudio server will open in a new tab.\n\nNote that this temporary cluster will only be available for the duration of this course.",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Running R"
    ]
  },
  {
    "objectID": "r/hss_run.html#using-rstudio",
    "href": "r/hss_run.html#using-rstudio",
    "title": "Running R",
    "section": "Using RStudio",
    "text": "Using RStudio\nFor those unfamiliar with the RStudio IDE, you can download the following cheatsheet:\n\n\n\nfrom Posit Cheatsheets",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Running R"
    ]
  },
  {
    "objectID": "r/hss_vis.html",
    "href": "r/hss_vis.html",
    "title": "Data visualization",
    "section": "",
    "text": "To understand data, it is often extremely useful to visualize them. In this section, we will plot the US infectious disease data.",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Data visualization"
    ]
  },
  {
    "objectID": "r/hss_vis.html#load-packages",
    "href": "r/hss_vis.html#load-packages",
    "title": "Data visualization",
    "section": "Load packages",
    "text": "Load packages\nThe most popular R package for data visualization is the tidyverse package ggplot2. Let’s load it in addition to our previous packages:\n\nlibrary(dslabs)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\n\nLet’s start by plotting the total number of cases for all states and all diseases per year. We already saw in the previous section how to group and summarise the data. Let’s create a new data frame with our prepared data:\n\nus_year_totals &lt;- us_contagious_diseases |&gt;\n  group_by(year) |&gt;\n  summarise(total = sum(count))\n\nThis is what our data frame looks like:\n\nhead(us_year_totals)\n\n# A tibble: 6 × 2\n   year  total\n  &lt;dbl&gt;  &lt;dbl&gt;\n1  1928 524563\n2  1929 380196\n3  1930 439289\n4  1931 482886\n5  1932 404683\n6  1933 391485\n\n\nNow we can use it to make a first plot.",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Data visualization"
    ]
  },
  {
    "objectID": "r/hss_vis.html#the-canvas",
    "href": "r/hss_vis.html#the-canvas",
    "title": "Data visualization",
    "section": "The Canvas",
    "text": "The Canvas\nThe first component of a plot is the data:\n\nggplot(us_year_totals)\n\n\n\n\n\n\n\n\nThe second component sets the way variables are mapped on the axes. This is done with the aes() (aesthetics) function:\n\nggplot(us_year_totals, aes(year, total))",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Data visualization"
    ]
  },
  {
    "objectID": "r/hss_vis.html#geometric-representations-of-the-data",
    "href": "r/hss_vis.html#geometric-representations-of-the-data",
    "title": "Data visualization",
    "section": "Geometric representations of the data",
    "text": "Geometric representations of the data\nOnto this canvas, we can add “geoms” (geometrical objects) representing the data. The type of “geom” defines the type of representation (e.g. boxplot, histogram, bar chart).\nTo represent the data as a scatterplot, we use the geom_point() function:\n\nggplot(us_year_totals, aes(year, total)) +\n  geom_point()\n\n\n\n\n\n\n\n\nThis immediately shows that the number of contagious infections in the US has declined sharply since the early 60s.\nMultiple “geoms” can be added on top of each other. For instance, we can add a smoothed conditional means function with geom_smooth(). That will help us see patterns in the data:\n\nggplot(us_year_totals, aes(year, total)) +\n  geom_point() +\n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThere is a bump of cases in the early 40s. Due to WWII maybe?\nThe default smoothing function uses the LOESS (locally estimated scatterplot smoothing) method, which is a nonlinear regression. We can change the method to a linear model:\n\nggplot(us_year_totals, aes(year, total)) +\n  geom_point() +\n  geom_smooth(method = lm)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nLine width, color, and whether or not the standard error (se) is shown can be customized:\n\nggplot(us_year_totals, aes(year, total)) +\n  geom_point() +\n  geom_smooth(\n    method = lm,\n    se = FALSE,\n    color = \"#999999\",\n    linewidth = 0.5\n  )\n\n`geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Data visualization"
    ]
  },
  {
    "objectID": "r/hss_vis.html#colour-representations",
    "href": "r/hss_vis.html#colour-representations",
    "title": "Data visualization",
    "section": "Colour representations",
    "text": "Colour representations\nSo far, we have pooled the data for all diseases together, but maybe different diseases show different trends.\nLet’s create a new data frame with the totals per year and per disease so that we can create plots with more information:\n\nus_year_disease_totals &lt;- us_contagious_diseases |&gt;\n  group_by(year, disease) |&gt;\n  summarise(total = sum(count), .groups = 'drop')\n\nhead(us_year_disease_totals)\n\n# A tibble: 6 × 3\n   year disease   total\n  &lt;dbl&gt; &lt;fct&gt;     &lt;dbl&gt;\n1  1928 Measles  483337\n2  1928 Polio      4756\n3  1928 Smallpox  36470\n4  1929 Measles  339061\n5  1929 Polio      2746\n6  1929 Smallpox  38389\n\n\nNow we can use a different colour for each disease:\n\nggplot(us_year_disease_totals, aes(year, total)) +\n  geom_point(aes(color = disease))\n\n\n\n\n\n\n\n\nThis shows how prevalent measles was until the 70s.\nWhen plotting quickly to understand the data, aesthetics don’t matter. If you want to produce plots for publications or presentations, of course you should then spend some time tweaking their style and readability.",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Data visualization"
    ]
  },
  {
    "objectID": "r/hss_vis.html#colour-scales",
    "href": "r/hss_vis.html#colour-scales",
    "title": "Data visualization",
    "section": "Colour scales",
    "text": "Colour scales\nMany colour scales exist. scale_color_brewer(), based on color brewer 2.0, is one of many methods to change the color scale. Here is the list of available scales for this particular method:\n\n\n\n\n\nWhen choosing a colour scale, it is very important to remember that various forms of colour blindness are common. Try to choose distinctive colours. Some palettes are specifically designed to work well for everyone.\nHere, let’s try the Dark2 palette:\n\nggplot(us_year_disease_totals, aes(year, total)) +\n  geom_point(aes(color = disease)) +\n  scale_color_brewer(palette = \"Dark2\")",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Data visualization"
    ]
  },
  {
    "objectID": "r/hss_vis.html#labels",
    "href": "r/hss_vis.html#labels",
    "title": "Data visualization",
    "section": "Labels",
    "text": "Labels\nLet’s improve our axes labels and legend and add a title to the plot:\n\nggplot(us_year_disease_totals, aes(year, total)) +\n  geom_point(aes(color = disease)) +\n  scale_color_brewer(palette = \"Dark2\") +\n  labs(\n    title = \"Infectious diseases in the US\",\n    x = \"Year\",\n    y = \"Number of cases\",\n    color = \"Diseases\"\n  )",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Data visualization"
    ]
  },
  {
    "objectID": "r/hss_vis.html#themes",
    "href": "r/hss_vis.html#themes",
    "title": "Data visualization",
    "section": "Themes",
    "text": "Themes\nggplot2 comes with a number of preset themes.\nEdward Tufte developed, amongst others, the principle of data-ink ratio which emphasizes that ink should be used primarily where it communicates meaningful messages. It is indeed common to see charts where more ink is used in labels or background than in the actual representation of the data.\nThe default ggplot2 theme could be criticized as not following this principle. Let’s change it:\n\nggplot(us_year_disease_totals, aes(year, total)) +\n  geom_point(aes(color = disease)) +\n  scale_color_brewer(palette = \"Dark2\") +\n  labs(\n    title = \"Infectious diseases in the US\",\n    x = \"Year\",\n    y = \"Number of cases\",\n    color = \"Diseases\"\n  ) +\n  theme_classic()\n\n\n\n\n\n\n\n\nThe theme() function allows to tweak the theme in any number of ways. For instance, what if we don’t like the default position of the title and we would rather have it centered?\n\nggplot(us_year_disease_totals, aes(year, total)) +\n  geom_point(aes(color = disease)) +\n  scale_color_brewer(palette = \"Dark2\") +\n  labs(\n    title = \"Infectious diseases in the US\",\n    x = \"Year\",\n    y = \"Number of cases\",\n    color = \"Diseases\"\n  ) +\n  theme_classic() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\nWe can also move the legend to give more space to the actual graph:\n\nggplot(us_year_disease_totals, aes(year, total)) +\n  geom_point(aes(color = disease)) +\n  scale_color_brewer(palette = \"Dark2\") +\n  labs(\n    title = \"Infectious diseases in the US\",\n    x = \"Year\",\n    y = \"Number of cases\",\n    color = \"Diseases\"\n  ) +\n  theme_classic() +\n  theme(plot.title = element_text(hjust = 0.5), legend.position = \"bottom\")",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Data visualization"
    ]
  },
  {
    "objectID": "r/hss_vis.html#facets",
    "href": "r/hss_vis.html#facets",
    "title": "Data visualization",
    "section": "Facets",
    "text": "Facets\nInstead of plotting the data for all diseases on a single graph, we can create facets:\n\nggplot(us_year_disease_totals, aes(year, total)) +\n  geom_point(aes(color = disease), show.legend = FALSE) +\n  scale_color_brewer(palette = \"Dark2\") +\n  labs(\n    title = \"Infectious diseases in the US\",\n    x = \"Year\",\n    y = \"Number of cases\"\n  ) +\n  facet_wrap(~ disease) +\n  theme(plot.title = element_text(hjust = 0.5))",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Data visualization"
    ]
  },
  {
    "objectID": "r/hss_vis.html#saving-plots",
    "href": "r/hss_vis.html#saving-plots",
    "title": "Data visualization",
    "section": "Saving plots",
    "text": "Saving plots\nPlots can be saved to file thanks to the ggsave() function from ggplot2.\nLet’s save our last plot:\nggsave(\"us_infectious_diseases.png\")\n\nBy default, ggsave() saves the last plot and guesses the file type from the file name extension. Arguments exist to select another plot to save to file, set the height and width, the resolution, add a background, etc. See ?ggsave for a list of options.",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Data visualization"
    ]
  },
  {
    "objectID": "r/hss_vis.html#ggplot2-extensions",
    "href": "r/hss_vis.html#ggplot2-extensions",
    "title": "Data visualization",
    "section": "ggplot2 extensions",
    "text": "ggplot2 extensions\nThanks to its popularity, ggplot2 has seen a proliferation of packages extending its capabilities. A full list can be found here.",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Data visualization"
    ]
  },
  {
    "objectID": "r/index.html",
    "href": "r/index.html",
    "title": "R",
    "section": "",
    "text": "Getting started with  \nAn intro course in R\n\n\n\n\n  for the humanities\nR course for HSS\n\n\n\n\n\n\nHigh-performance  \nA course on HPC in R\n\n\n\n\nWorkshops\nVarious R topics\n\n\n\n\n\n\n60 min webinars\nVarious R topics",
    "crumbs": [
      "R",
      "<br>&nbsp;<img src=\"img/logo_r.png\" class=\"img-fluid\" style=\"width:1.5em\" alt=\"noshadow\"><br><br>"
    ]
  },
  {
    "objectID": "r/intro_control_flow.html",
    "href": "r/intro_control_flow.html",
    "title": "Control flow",
    "section": "",
    "text": "Control flow statements alter the linear execution of code, allowing for one or another section of code to be executed, or for one section of code to be executed multiple times.",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "Control flow"
    ]
  },
  {
    "objectID": "r/intro_control_flow.html#conditionals",
    "href": "r/intro_control_flow.html#conditionals",
    "title": "Control flow",
    "section": "Conditionals",
    "text": "Conditionals\nConditionals determine which section of code is to be ran based on predicates. A predicate is a test that returns either TRUE or FALSE.\nHere is an example:\n\ntest_sign &lt;- function(x) {\n  if (x &gt; 0) {\n    \"x is positif\"\n  } else if (x &lt; 0) {\n    \"x is negatif\"\n  } else {\n    \"x is equal to zero\"\n  }\n}\n\ntest_sign() is a function that accepts one argument. Depending on the value of that argument, one of three snippets of code is executed:\n\ntest_sign(3)\n\n[1] \"x is positif\"\n\ntest_sign(-2)\n\n[1] \"x is negatif\"\n\ntest_sign(0)\n\n[1] \"x is equal to zero\"",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "Control flow"
    ]
  },
  {
    "objectID": "r/intro_control_flow.html#loops",
    "href": "r/intro_control_flow.html#loops",
    "title": "Control flow",
    "section": "Loops",
    "text": "Loops\nLoops allow to run the same instruction on various elements:\n\nfor (i in 1:10) {\n  print(i)\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n[1] 7\n[1] 8\n[1] 9\n[1] 10",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "Control flow"
    ]
  },
  {
    "objectID": "r/intro_functions.html",
    "href": "r/intro_functions.html",
    "title": "Function definition",
    "section": "",
    "text": "R comes with a number of built-in functions. Packages can provide additional ones. In many cases however, you will want to create your own functions to perform exactly the computations that you need.\nIn this section, we will see how to define new functions.",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "Function definition"
    ]
  },
  {
    "objectID": "r/intro_functions.html#syntax",
    "href": "r/intro_functions.html#syntax",
    "title": "Function definition",
    "section": "Syntax",
    "text": "Syntax\nHere is the syntax to define a new function:\nname &lt;- function(arguments) {\n  body\n}",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "Function definition"
    ]
  },
  {
    "objectID": "r/intro_functions.html#example",
    "href": "r/intro_functions.html#example",
    "title": "Function definition",
    "section": "Example",
    "text": "Example\nLet’s define a function that we call compare which will compare the value between 2 numbers:\n\ncompare &lt;- function(x, y) {\n  x == y\n}\n\n\ncompare is the name of our function.\nx and y are the placeholders for the arguments that our function will accept (our function will need 2 arguments to run successfully).\nx == y is the body of the function, that is, the computation performed by our function.\n\nWe can now use our function:\n\ncompare(2, 3)\n\n[1] FALSE",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "Function definition"
    ]
  },
  {
    "objectID": "r/intro_functions.html#what-is-returned-by-a-function",
    "href": "r/intro_functions.html#what-is-returned-by-a-function",
    "title": "Function definition",
    "section": "What is returned by a function?",
    "text": "What is returned by a function?\nIn R, the result of the last statement is printed automatically:\n\ntest &lt;- function(x, y) {\n  x\n  y\n}\ntest(2, 3)\n\n[1] 3\n\n\nIf you want to also print other results, you need to explicitly use the print() function:\n\ntest &lt;- function(x, y) {\n  print(x)\n  y\n}\ntest(2, 3)\n\n[1] 2\n\n\n[1] 3\n\n\nNote that, unlike print(), the function return() exits the function:\n\ntest &lt;- function(x, y) {\n  return(x)\n  y\n}\ntest(2, 3)\n\n[1] 2\n\n\n\ntest &lt;- function(x, y) {\n  return(x)\n  return(y)\n}\ntest(2, 3)\n\n[1] 2",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "Function definition"
    ]
  },
  {
    "objectID": "r/intro_packages.html",
    "href": "r/intro_packages.html",
    "title": "Packages",
    "section": "",
    "text": "Packages are a set of functions, constants, and/or data developed by the community that add functionality to R.\nIn this section, we look at where to find packages and how to install them.",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "Packages"
    ]
  },
  {
    "objectID": "r/intro_packages.html#looking-for-packages",
    "href": "r/intro_packages.html#looking-for-packages",
    "title": "Packages",
    "section": "Looking for packages",
    "text": "Looking for packages\n\nPackage finder.\nYour peers and the literature.\nList of CRAN packages.\nList of CRAN task views (list of packages with information for a large number of wide topics).",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "Packages"
    ]
  },
  {
    "objectID": "r/intro_packages.html#managing-r-packages",
    "href": "r/intro_packages.html#managing-r-packages",
    "title": "Packages",
    "section": "Managing R packages",
    "text": "Managing R packages\n\nFor this course, you won’t have to install any package as they have already been installed in our RStudio server.\n\nR packages can be installed, updated, and removed from within R:\ninstall.packages(\"&lt;package_name&gt;\", repos=\"&lt;url-cran-mirror&gt;\")\nremove.packages(\"&lt;package-name&gt;\")\nupdate_packages()\n\nExample:\n\ninstall.packages(\"rvest\", repos=\"https://mirror.rcg.sfu.ca/mirror/CRAN/\")\n\nrepos argument: chose a CRAN mirror close to the location of your cluster or use https://cloud.r-project.org/.\n\n\nThe first time you install a package, R will ask you whether you want to create a personal library in your home directory. Answer yes to both questions. Your packages will now install under ~/.\n\n\nSome packages require additional modules to be loaded before they can be installed. Other packages need additional R packages as dependencies. In either case, you will get explicit error messages. Adding the argument dependencies = T helps in the second case, but you will still have to add packages manually from time to time.",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "Packages"
    ]
  },
  {
    "objectID": "r/intro_packages.html#loading-packages",
    "href": "r/intro_packages.html#loading-packages",
    "title": "Packages",
    "section": "Loading packages",
    "text": "Loading packages\nTo make a package available in an R session, you load it with the library() function.\n\nExample:\n\nlibrary(readxl)\nAlternatively, you can access a function from a package without loading it with the syntax: package::function().\n\nExample:\n\nreadxl::read_excel(\"file.xlsx\")",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "Packages"
    ]
  },
  {
    "objectID": "r/intro_packages.html#package-documentation",
    "href": "r/intro_packages.html#package-documentation",
    "title": "Packages",
    "section": "Package documentation",
    "text": "Package documentation\n\nSelect a package from the list of CRAN packages.\nGoogle “cran” and the name of your package (e.g. “cran dplyr”).\nLook up a package in the package documentation.\nGet a list of functions within a package with the help() function (installed, but not loaded in session):\n\n\nExample to get a list of functions in the dplyr package:\n\nhelp(package = \"dplyr\")\n\nGet help on a function within a package:\n\nIf you are using RStudio or the HTML format for your R help and you already ran the command to get the list of functions within a package (e.g. help(package = \"dplyr\")), you can get help on any function by clicking on its name.\nIf you are using the text format for help (for instance, if you are running R remotely on the command line), you can get help for any function by adding its name at as the first argument of the previous command.\n\nExample to get help on the function bind() of the package dplyr:\n\nhelp(bind, package = \"dplyr\")\nOf course, if the dplyr package is already loaded in your session, you can simply run help(bind).\n\nGet a list of all help files with alias or concept or title matching a regular expression in all installed packages:\n\n\nExample to get a list of all help files with alias or concept or title matching bind:\n\n??bind\nYou can then open those help files as seen previously.\n\nGet a list of all vignettes for all installed packages:\n\nIf you are using RStudio or the HTML help format:\nbrowseVignettes()\nIf you are using the text help format:\nvignette()\n\nGet a list of vignettes available for a package (not all packages have vignettes):\n\n\nExample to get a list of vignettes for the package dplyr:\n\nIf you are using RStudio or the HTML help format:\nvignette(package = \"dplyr\")\nIf you are using the text help format:\nbrowseVignettes(package = \"dplyr\")\nYou can then open those help vignettes as seen previously.",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "Packages"
    ]
  },
  {
    "objectID": "r/intro_publishing.html",
    "href": "r/intro_publishing.html",
    "title": "Publishing",
    "section": "",
    "text": "You might have heard of R Markdown: a way to intertwine code and prose in a single scientific document. The company behind R Markdown has now developed its successor: Quarto.\n\nQuarto allows the creation of webpages, websites, presentations, books, pdf, etc. from code in R, Python, or Julia and markdown text.\nIf you are interested in an introduction to this tool, you can have a look at our workshop or our webinar on Quarto.\nBy the way, this entire website was created with Quarto 🙂",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "Publishing"
    ]
  },
  {
    "objectID": "r/intro_run.html",
    "href": "r/intro_run.html",
    "title": "Running R",
    "section": "",
    "text": "This section covers the various ways R can be run, then shows you how to access our temporary RStudio server for this course.",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "Running R"
    ]
  },
  {
    "objectID": "r/intro_run.html#running-r",
    "href": "r/intro_run.html#running-r",
    "title": "Running R",
    "section": "Running R",
    "text": "Running R\nR being an interpreted language, it can be run non-interactively or interactively.\n\nRunning R non-interactively\nIf you write code in a text file (called a script), you can then execute it with:\nRscript my_script.R\n\nThe command to execute scripts is Rscript rather than R.\nBy convention, R scripts take the extension .R.\n\n\n\nRunning R interactively\nThere are several ways to run R interactively.\n\nDirectly in the console (the name for the R shell):\n\n\n\nIn Jupyter with the R kernel (IRkernel package).\nIn another IDE (e.g. in Emacs with ESS).\nIn the RStudio IDE.\n\nThe RStudio IDE is popular and this is what we will use today. RStudio can can be run locally, but for this course, we will use an RStudio server.",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "Running R"
    ]
  },
  {
    "objectID": "r/intro_run.html#accessing-our-rstudio-server",
    "href": "r/intro_run.html#accessing-our-rstudio-server",
    "title": "Running R",
    "section": "Accessing our RStudio server",
    "text": "Accessing our RStudio server\nYou do not need to install anything on your machine for this course as we will provide access to a temporary RStudio server.\n\nA username, a password, and the URL of the RStudio server will be given to you during the workshop.\n\nSign in using the username and password you will be given while ignoring the OTP entry. This will take you to the server options page of a JupyterHub.\n\nSelect the following server options:\n\nTime: 2 hours\nNumber of cores: 1\nMemory: 3700 MB\nUser interface: JupyterLab\n\n\nThen press “Start” to launch the JupyterHub. There, click on the “RStudio” button and the RStudio server will open in a new tab.\n\nNote that this temporary cluster will only be available for the duration of this course.",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "Running R"
    ]
  },
  {
    "objectID": "r/intro_run.html#using-rstudio",
    "href": "r/intro_run.html#using-rstudio",
    "title": "Running R",
    "section": "Using RStudio",
    "text": "Using RStudio\nFor those unfamiliar with the RStudio IDE, you can download the following cheatsheet:\n\n\n\nfrom Posit Cheatsheets",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "Running R"
    ]
  },
  {
    "objectID": "r/intro_why.html",
    "href": "r/intro_why.html",
    "title": "R: why and for whom?",
    "section": "",
    "text": "There are other high level programming languages such as Python or Julia, so when might it make sense for you to turn to R?",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "R: why and for whom?"
    ]
  },
  {
    "objectID": "r/intro_why.html#why-r",
    "href": "r/intro_why.html#why-r",
    "title": "R: why and for whom?",
    "section": "Why R?",
    "text": "Why R?\nHere are a number of reasons why you might want to consider using R:\n\nFree and open source\nHigh-level and easy to learn\nLarge community\nVery well documented\nUnequalled number of statistics and modelling packages\nIntegrated package manager\nEasy connection with fast compiled languages such as C and C++\nPowerful IDEs (e.g. RStudio, ESS, Jupyter)",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "R: why and for whom?"
    ]
  },
  {
    "objectID": "r/intro_why.html#for-whom",
    "href": "r/intro_why.html#for-whom",
    "title": "R: why and for whom?",
    "section": "For whom?",
    "text": "For whom?\nFor whom is R particularly well suited?\n\nFields with heavy statistics, modelling, or Bayesian analysis such as biology, linguistics, economics, or statistics\nData science using a lot of tabular data",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "R: why and for whom?"
    ]
  },
  {
    "objectID": "r/intro_why.html#downsides-of-r",
    "href": "r/intro_why.html#downsides-of-r",
    "title": "R: why and for whom?",
    "section": "Downsides of R",
    "text": "Downsides of R\nOf course, R also has its downsides:\n\nInconsistent syntax full of quirks\nSlow\nLarge memory usage",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "R: why and for whom?"
    ]
  },
  {
    "objectID": "r/top_hss.html",
    "href": "r/top_hss.html",
    "title": "R for the humanities",
    "section": "",
    "text": "R is a free and open-source programming language for statistical computing, modelling, and graphics, with an unbeatable collection of statistical packages. It is extremely popular in the humanities and social sciences.\nThis course does not assume any programming knowledge.\n\n Start course ➤",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>"
    ]
  },
  {
    "objectID": "r/top_wb.html",
    "href": "r/top_wb.html",
    "title": "R webinars",
    "section": "",
    "text": "GIS mapping\n\n\n\n\nHPC in R",
    "crumbs": [
      "R",
      "<em><b>Webinars</b></em>"
    ]
  },
  {
    "objectID": "r/wb_gis_mapping.html",
    "href": "r/wb_gis_mapping.html",
    "title": "GIS mapping with R",
    "section": "",
    "text": "In this webinar, we will see how to create all sorts of GIS maps with the packages sf, tmap, raster, leaflet, ggplot2, grid (part of Base R), and mapview:\n\nsimple maps\ninset maps\nfaceted maps\nanimated maps\ninteractive maps\nraster maps\n\nFinally, we will learn how to add basemaps from OpenStreetMap and Google Maps.\nSlides (Click and wait: this reveal.js presentation may take a little time to load.)",
    "crumbs": [
      "R",
      "<em><b>Webinars</b></em>",
      "GIS mapping with R"
    ]
  },
  {
    "objectID": "r/wb_gis_mapping.html#gis-concepts",
    "href": "r/wb_gis_mapping.html#gis-concepts",
    "title": "GIS mapping with R",
    "section": "GIS concepts",
    "text": "GIS concepts\n\nTypes of spatial data\n\nVector data\nVector data represent discrete objects.\nThey contain:\n\na geometry: the shape and location of the objects,\nattributes: additional variables (e.g. name, year, type).\n\nCommon file formats include GeoJSON and shapefile.\n\nExamples: countries, roads, rivers, towns.\n\n\n\nRaster data\nRaster data represent continuous phenomena or spatial fields.\nCommon file formats include TIFF, GeoTIFF, NetCDF, and Esri grid.\n\nExamples: temperature, air quality, elevation, water depth.\n\n\n\n\nVector data\nVector data come in multiple types:\n\npoint:     single set of coordinates,\nmulti-point:   multiple sets of coordinates,\npolyline:    multiple sets for which the order matters,\nmulti-polyline:  multiple of the above,\npolygon:    same as polyline but first and last sets are the same,\nmulti-polygon:  multiple of the above.\n\n\n\nRaster data\nGrid of equally sized rectangular cells containing values for some variables.\nSize of cells = resolution.\nFor computing efficiency, rasters do not have coordinates of each cell, but the bounding box and the number of rows and columns.\n\n\nCoordinate Reference Systems (CRS)\nA location on Earth’s surface can be identified by its coordinates and some reference system called CRS.\nThe coordinates (x, y) are called longitude and latitude.\nThere can be a 3rd coordinate (z) for elevation or other measurement—usually a vertical one.\nAnd a 4th (m) for some other data attribute—usually a horizontal measurement.\nIn 3D, longitude and latitude are expressed in angular units (e.g. degrees) and the reference system needed is an angular CRS or geographic coordinate system (GCS).\nIn 2D, they are expressed in linear units (e.g. meters) and the reference system needed is a planar CRS or projected coordinate system (PCS).\n\n\nDatums\nSince the Earth is not a perfect sphere, we use spheroidal models to represent its surface. Those are called geodetic datums.\nSome datums are global, others local (more accurate in a particular area of the globe, but only useful there).\n\nExamples of commonly used global datums:\n\nWGS84 (World Geodesic System 1984),\nNAD83 (North American Datum of 1983).\n\n\n\n\nAngular CRS\nAn angular CRS contains a datum, an angular unit and references such as a prime meridian (e.g. the Royal Observatory, Greenwich, England).\nIn an angular CRS or GCS:\n\nLongitude (\\(\\lambda\\)) represents the angle between the prime meridian and the meridian that passes through that location.\nLatitude (\\(\\phi\\)) represents the angle between the line that passes through the center of the Earth and that location and its projection on the equatorial plane.\n\nLongitude and latitude are thus angular coordinates.\n\n\nProjections\nTo create a two-dimensional map, you need to project this 3D angular CRS into a 2D one.\nVarious projections offer different characteristics. For instance:\n\nsome respect areas (equal-area),\nsome respect the shape of geographic features (conformal),\nsome almost respect both for small areas.\n\nIt is important to choose one with sensible properties for your goals.\n\nExamples of projections:\n\nMercator,\nUTM,\nRobinson.\n\n\n\n\nPlanar CRS\nA planar CRS is defined by a datum, a projection and a set of parameters such as a linear unit and the origins.\nCommon planar CRS have been assigned a unique ID called EPSG code which is much more convenient to use.\nIn a planar CRS, coordinates will not be in degrees anymore but in meters (or other length unit).\n\n\nProjecting into a new CRS\nYou can change the projection of your data.\nVector data won’t suffer any loss of precision, but raster data will.\n→ best to try to avoid reprojecting rasters: if you want to combine various datasets which have different projections, reproject vector data instead.",
    "crumbs": [
      "R",
      "<em><b>Webinars</b></em>",
      "GIS mapping with R"
    ]
  },
  {
    "objectID": "r/wb_gis_mapping.html#gis-in-r",
    "href": "r/wb_gis_mapping.html#gis-in-r",
    "title": "GIS mapping with R",
    "section": "GIS in R",
    "text": "GIS in R\n\nResources\n\nOpen GIS data\nFree GIS Data: list of free GIS datasets.\n\n\nBooks\nGeocomputation with R by Robin Lovelace, Jakub Nowosad and Jannes Muenchow.\nSpatial Data Science by Edzer Pebesma and Roger Bivand.\nSpatial Data Science with R by Robert J. Hijmans.\nUsing Spatial Data with R by Claudia A. Engel.\n\n\nTutorial\nAn Introduction to Spatial Data Analysis and Visualisation in R by the CDRC.\n\n\n\nResources\n\nWebsite\nr-spatial by Edzer Pebesma, Marius Appel and Daniel Nüst.\n\n\nCRAN package list\nAnalysis of Spatial Data.\n\n\nMailing list\nR Special Interest Group on using Geographical data and Mapping.",
    "crumbs": [
      "R",
      "<em><b>Webinars</b></em>",
      "GIS mapping with R"
    ]
  },
  {
    "objectID": "r/wb_gis_mapping.html#packages",
    "href": "r/wb_gis_mapping.html#packages",
    "title": "GIS mapping with R",
    "section": "Packages",
    "text": "Packages\nThere is now a rich ecosystem of GIS packages in R1.\n1 Bivand, R.S. Progress in the R ecosystem for representing and handling spatial data. J Geogr Syst (2020). https://doi.org/10.1007/s10109-020-00336-0.\nData manipulation\n\nOlder packages\n\nsp\nraster\nrgdal\nrgeos\n\n\n\nNewer generation\n\nsf:    vector data,\nterra:  raster data (also has vector data capabilities).\n\n\n\n\nMapping\n\nStatic maps\n\nggplot2 + ggspatial\ntmap\n\n\n\nDynamic maps\n\nleaflet\nggplot2 + gganimate\nmapview\nggmap\ntmap\n\n\n\n\nsf: Simple Features in R\nGeospatial vectors: points, lines, polygons.\nSimple Features—defined by the Open Geospatial Consortium (OGC) and formalized by ISO—is a set of standards now used by most GIS libraries.\nWell-known text (WKT) is a markup language for representing vector geometry objects according to those standards.\nA compact computer version also exists—well-known binary (WKB)—used by spatial databases.\nThe package sp predates Simple Features.\nsf—launched in 2016—implements these standards in R in the form of sf objects: data.frames (or tibbles) containing the attributes, extended by sfc objects or simple feature geometries list-columns.\n\n\nsf\nSome useful links:\n\nGitHub repo,\nPaper,\nResources,\nCheatsheet,\n6 vignettes: 1, 2, 3, 4, 5, 6.\n\nAnd the cheatsheet:\nxxxxx\n\n\n\nsf objects\n\n\n\n\n\n\n\nsf functions\nMost functions start with st_ (which refers to “spatial type”).\n\n\nterra: Geospatial rasters\nFaster and simpler replacement for the raster package by the same team.\nMostly implemented in C++.\nCan work with datasets too large to be loaded into memory.\n\n\nterra\nSome useful links:\n\nGitHub repo,\nResources,\nFull manual.\n\n\n\ntmap: Layered grammar of graphics GIS maps\nSome useful links:\n\nGitHub repo,\nResources.\n\n\nHelp pages and vignettes\n?tmap-element\nvignette(\"tmap-getstarted\")\n# All the usual help pages, e.g.:\n?tm_layout\n\n\ntmap functions\nMain functions start with tmap_\nFunctions creating map elements start with tm_\n\n\ntmap functioning\nVery similar to ggplot2\nTypically, a map contains:\n\nOne or multiple layer(s) (the order matters as they stack on top of each other)\nSome layout (e.g. customization of title, background, margins): tm_layout\nA compass: tm_compass\nA scale bar: tm_scale_bar\n\nEach layer contains:\n\nSome data: tm_shape\nHow that data will be represented: e.g. tm_polygons, tm_lines, tm_raster\n\n\n\ntmap example\n\n\n\n\n\n\n\n\n\n\n\nggplot2 (the standard in R plots)\nSome useful links:\n\nGitHub repo,\nResources,\nCheatsheet.\n\n\n\ngeom_sf allows to plot sf objects (i.e. make maps).",
    "crumbs": [
      "R",
      "<em><b>Webinars</b></em>",
      "GIS mapping with R"
    ]
  },
  {
    "objectID": "r/wb_gis_mapping.html#example-glaciers-melt-in-north-america",
    "href": "r/wb_gis_mapping.html#example-glaciers-melt-in-north-america",
    "title": "GIS mapping with R",
    "section": "Example: Glaciers melt in North America",
    "text": "Example: Glaciers melt in North America\n\nData\nFor this webinar, we will use:\n\nthe Alaska as well as the Western Canada and USA subsets of the Randolph Glacier Inventory version 6.02,\nthe USGS time series of the named glaciers of Glacier National Park3,\nthe Alaska as well as the Western Canada and USA subsets of the consensus estimate for the ice thickness distribution of all glaciers on Earth dataset4.\n\n2 RGI Consortium (2017). Randolph Glacier Inventory – A Dataset of Global Glacier Outlines: Version 6.0: Technical Report, Global Land Ice Measurements from Space, Colorado, USA. Digital Media. DOI: https://doi.org/10.7265/N5-RGI-60.3 Fagre, D.B., McKeon, L.A., Dick, K.A. and Fountain, A.G., 2017, Glacier margin time series (1966, 1998, 2005, 2015) of the named glaciers of Glacier National Park, MT, USA: U.S. Geological Survey data release. DOI: https://doi.org/10.5066/F7P26WB1.4 Farinotti, Daniel, 2019, A consensus estimate for the ice thickness distribution of all glaciers on Earth - dataset, Zurich. ETH Zurich. DOI: https://doi.org/10.3929/ethz-b-000315707.The datasets can be downloaded as zip files from these websites\n\n\nPackages\nPackages need to be installed before they can be loaded in a session.\nPackages on CRAN can be installed with:\ninstall.packages(\"&lt;package-name&gt;\")\nbasemaps is not on CRAN and needs to be installed from GitHub thanks to devtools:\ninstall.packages(\"devtools\")\ndevtools::install_github(\"16EAGLE/basemaps\")\nWe load all the packages that we will need at the top of the script:\nlibrary(sf)                 # spatial vector data manipulation\nlibrary(tmap)               # map production and tiled web map\nlibrary(dplyr)              # non GIS specific (tabular data manipulation)\nlibrary(magrittr)           # non GIS specific (pipes)\nlibrary(purrr)              # non GIS specific (functional programming)\nlibrary(rnaturalearth)      # basemap data access functions\nlibrary(rnaturalearthdata)  # basemap data\nlibrary(mapview)            # tiled web map\nlibrary(grid)               # (part of base R) used to create inset map\nlibrary(ggplot2)            # alternative to tmap for map production\nlibrary(ggspatial)          # spatial framework for ggplot2\nlibrary(terra)              # gridded spatial data manipulation\nlibrary(ggmap)              # download basemap data\nlibrary(basemaps)           # download basemap data\nlibrary(magick)             # wrapper around ImageMagick STL\nlibrary(leaflet)            # integrate Leaflet JS in R",
    "crumbs": [
      "R",
      "<em><b>Webinars</b></em>",
      "GIS mapping with R"
    ]
  },
  {
    "objectID": "r/wb_gis_mapping.html#reading-and-preparing-data",
    "href": "r/wb_gis_mapping.html#reading-and-preparing-data",
    "title": "GIS mapping with R",
    "section": "Reading and preparing data",
    "text": "Reading and preparing data\n\nRandolph Glacier Inventory\nThis dataset contains the contour of all glaciers on Earth.\nWe will focus on glaciers in Western North America.\nYou can download and unzip 02_rgi60_WesternCanadaUS and 01_rgi60_Alaska from the Randolph Glacier Inventory version 6.0.\n\n\nReading in data\nData get imported and turned into sf objects with the function sf::st_read:\nak &lt;- st_read(\"data/01_rgi60_Alaska\")\n\nMake sure to use the absolute paths or the paths relative to your working directory (which can be obtained with getwd).\n\nak &lt;- st_read(\"data/01_rgi60_Alaska\")\nReading layer `01_rgi60_Alaska' from data source `./data/01_rgi60_Alaska'\n               using driver `ESRI Shapefile'\nSimple feature collection with 27108 features and 22 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -176.1425 ymin: 52.05727 xmax: -126.8545 ymax: 69.35167\nGeodetic CRS:  WGS 84\n\n\nYour turn:\n\nRead in the data for the rest of north western America (from 02_rgi60_WesternCanadaUS) and create an sf object called wes.\n\n\n\nFirst look at the data\nak\nSimple feature collection with 27108 features and 22 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -176.1425 ymin: 52.05727 xmax: -126.8545 ymax: 69.35167\nGeodetic CRS:  WGS 84\nFirst 10 features:\n           RGIId        GLIMSId  BgnDate  EndDate    CenLon   CenLat O1Region\n1  RGI60-01.00001 G213177E63689N 20090703 -9999999 -146.8230 63.68900        1\n2  RGI60-01.00002 G213332E63404N 20090703 -9999999 -146.6680 63.40400        1\n3  RGI60-01.00003 G213920E63376N 20090703 -9999999 -146.0800 63.37600        1\n4  RGI60-01.00004 G213880E63381N 20090703 -9999999 -146.1200 63.38100        1\n5  RGI60-01.00005 G212943E63551N 20090703 -9999999 -147.0570 63.55100        1\n6  RGI60-01.00006 G213756E63571N 20090703 -9999999 -146.2440 63.57100        1\n7  RGI60-01.00007 G213771E63551N 20090703 -9999999 -146.2295 63.55085        1\n8  RGI60-01.00008 G213704E63543N 20090703 -9999999 -146.2960 63.54300        1\n9  RGI60-01.00009 G212400E63659N 20090703 -9999999 -147.6000 63.65900        1\n10 RGI60-01.00010 G212830E63513N 20090703 -9999999 -147.1700 63.51300        1\nO2Region   Area Zmin Zmax Zmed Slope Aspect  Lmax Status Connect Form\n1         2  0.360 1936 2725 2385    42    346   839      0       0    0\n2         2  0.558 1713 2144 2005    16    162  1197      0       0    0\n3         2  1.685 1609 2182 1868    18    175  2106      0       0    0\n4         2  3.681 1273 2317 1944    19    195  4175      0       0    0\n5         2  2.573 1494 2317 1914    16    181  2981      0       0    0\n6         2 10.470 1201 3547 1740    22     33 10518      0       0    0\n7         2  0.649 1918 2811 2194    23    151  1818      0       0    0\n8         2  0.200 2826 3555 3195    45     80   613      0       0    0\n9         2  1.517 1750 2514 1977    18    274  2255      0       0    0\n10        2  3.806 1280 1998 1666    17     35  3332      0       0    0\nTermType Surging Linkages Name                       geometry\n1         0       9        9 &lt;NA&gt; POLYGON ((-146.818 63.69081...\n2         0       9        9 &lt;NA&gt; POLYGON ((-146.6635 63.4076...\n3         0       9        9 &lt;NA&gt; POLYGON ((-146.0723 63.3834...\n4         0       9        9 &lt;NA&gt; POLYGON ((-146.149 63.37919...\n5         0       9        9 &lt;NA&gt; POLYGON ((-147.0431 63.5502...\n6         0       9        9 &lt;NA&gt; POLYGON ((-146.2436 63.5562...\n7         0       9        9 &lt;NA&gt; POLYGON ((-146.2495 63.5531...\n8         0       9        9 &lt;NA&gt; POLYGON ((-146.2992 63.5443...\n9         0       9        9 &lt;NA&gt; POLYGON ((-147.6147 63.6643...\n10        0       9        9 &lt;NA&gt; POLYGON ((-147.1494 63.5098...\n\n\nStructure of the data\nstr(ak)\nClasses ‘sf’ and 'data.frame':  27108 obs. of  23 variables:\n$ RGIId   : chr  \"RGI60-01.00001\" \"RGI60-01.00002\" \"RGI60-01.00003\" ...\n$ GLIMSId : chr  \"G213177E63689N\" \"G213332E63404N\" \"G213920E63376N\" ...\n$ BgnDate : chr  \"20090703\" \"20090703\" \"20090703\" \"20090703\" ...\n$ EndDate : chr  \"-9999999\" \"-9999999\" \"-9999999\" \"-9999999\" ...\n$ CenLon  : num  -147 -147 -146 -146 -147 ...\n$ CenLat  : num  63.7 63.4 63.4 63.4 63.6 ...\n$ O1Region: chr  \"1\" \"1\" \"1\" \"1\" ...\n$ O2Region: chr  \"2\" \"2\" \"2\" \"2\" ...\n$ Area    : num  0.36 0.558 1.685 3.681 2.573 ...\n$ Zmin    : int  1936 1713 1609 1273 1494 1201 1918 2826 1750 1280 ...\n$ Zmax    : int  2725 2144 2182 2317 2317 3547 2811 3555 2514 1998 ...\n$ Zmed    : int  2385 2005 1868 1944 1914 1740 2194 3195 1977 1666 ...\n$ Slope   : num  42 16 18 19 16 22 23 45 18 17 ...\n$ Aspect  : int  346 162 175 195 181 33 151 80 274 35 ...\n$ Lmax    : int  839 1197 2106 4175 2981 10518 1818 613 2255 3332 ...\n$ Status  : int  0 0 0 0 0 0 0 0 0 0 ...\n$ Connect : int  0 0 0 0 0 0 0 0 0 0 ...\n$ Form    : int  0 0 0 0 0 0 0 0 0 0 ...\n$ TermType: int  0 0 0 0 0 0 0 0 0 0 ...\n$ Surging : int  9 9 9 9 9 9 9 9 9 9 ...\n$ Linkages: int  9 9 9 9 9 9 9 9 9 9 ...\n$ Name    : chr  NA NA NA NA ...\n$ geometry:sfc_POLYGON of length 27108; first list element: List of 1\n..$ : num [1:65, 1:2] -147 -147 -147 -147 -147 ...\n..- attr(*, \"class\")= chr [1:3] \"XY\" \"POLYGON\" \"sfg\"\n- attr(*, \"sf_column\")= chr \"geometry\"\n- attr(*, \"agr\")= Factor w/ 3 levels \"constant\",\"aggregate\",..: NA NA NA ...\n..- attr(*, \"names\")= chr [1:22] \"RGIId\" \"GLIMSId\" \"BgnDate\" \"EndDate\" ...\n\n\nInspect your data\n\n\nYour turn:\n\nInspect the wes object you created.\n\n\n\nGlacier National Park dataset\nThis dataset contains a time series of the retreat of 39 glaciers of Glacier National Park, MT, USA for the years 1966, 1998, 2005 and 2015.\nYou can download and unzip the 4 sets of files from the USGS website.\n\n\nRead in and clean datasets\nCreate a function that reads and cleans the data:\nprep &lt;- function(dir) {\n  g &lt;- st_read(dir)\n  g %&lt;&gt;% rename_with(~ tolower(gsub(\"Area....\", \"area\", .x)))\n  g %&lt;&gt;% dplyr::select(\n    year,\n    objectid,\n    glacname,\n    area,\n    shape_leng,\n    x_coord,\n    y_coord,\n    source_sca,\n    source\n  )\n}\nCreate a vector of dataset names:\ndirs &lt;- grep(\"data/GNPglaciers_.*\", list.dirs(), value = T)\nPass each element of that vector through prep() thanks to map():\ngnp &lt;- map(dirs, prep)\n\nWe use dplyr::select because terra also has a select function.\n\n\n\nCombine datasets into one sf object\nCheck that the CRS are all the same:\nall(sapply(\n  list(st_crs(gnp[[1]]),\n       st_crs(gnp[[2]]),\n       st_crs(gnp[[3]]),\n       st_crs(gnp[[4]])),\n  function(x) x == st_crs(gnp[[1]])\n))\n[1] TRUE\nWe can rbind the elements of our list:\ngnp &lt;- do.call(\"rbind\", gnp)\nYou can inspect your new sf object by calling it or with str.\n\n\nEstimate for ice thickness\nThis dataset contains an estimate for the ice thickness of all glaciers on Earth.\nThe nomenclature follows the Randolph Glacier Inventory.\nIce thickness being a spatial field, this is raster data.\nWe will use data in RGI60-02.16664_thickness.tif from the ETH Zürich Research Collection which corresponds to one of the glaciers (Agassiz) of Glacier National Park.\n\n\nLoad raster data\nRead in data and create a SpatRaster object:\nras &lt;- rast(\"data/RGI60-02/RGI60-02.16664_thickness.tif\")\n\n\nInspect our SpatRaster object\nras\nclass       : SpatRaster \ndimensions  : 93, 74, 1  (nrow, ncol, nlyr)\nresolution  : 25, 25  (x, y)\nextent      : 707362.5, 709212.5, 5422962, 5425288  (xmin, xmax, ymin, ymax)\ncoord. ref. : +proj=utm +zone=11 +datum=WGS84 +units=m +no_defs \nsource      : RGI60-02.16664_thickness.tif \nname        : RGI60-02.16664_thickness \nnlyr gives us the number of bands (a single one here). You can also run str(ras).\n\n\nOur data\nWe now have 3 sf objects and 1 SpatRaster object:\n\nak: contour of glaciers in AK,\nwes: contour of glaciers in the rest of Western North America,\ngnp: time series of 39 glaciers in Glacier National Park, MT, USA,\nras: ice thickness of the Agassiz Glacier from Glacier National Park.",
    "crumbs": [
      "R",
      "<em><b>Webinars</b></em>",
      "GIS mapping with R"
    ]
  },
  {
    "objectID": "r/wb_gis_mapping.html#making-maps",
    "href": "r/wb_gis_mapping.html#making-maps",
    "title": "GIS mapping with R",
    "section": "Making maps",
    "text": "Making maps\n\nLet’s map our sf object ak\nAt a bare minimum, we need tm_shape with the data and some info as to how to represent that data:\ntm_shape(ak) +\n  tm_polygons()\n\n\n\nWe need to label and customize it\ntm_shape(ak) +\n  tm_polygons() +\n  tm_layout(\n    title = \"Glaciers of Alaska\",\n    title.position = c(\"center\", \"top\"),\n    title.size = 1.1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.06, 0.01, 0.09, 0.01),\n    outer.margins = 0,\n    frame.lwd = 0.2\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    size = 1.2,\n    text.size = 0.6\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 500, 1000),\n    position = c(\"right\", \"BOTTOM\")\n  )\n\n\n\nMake a map of the wes object\n\n\nYour turn:\n\nMake a map with the wes object you created with the data for Western North America excluding AK.\n\n\n\n\nNow, let’s make a map with ak and wes\nThe Coordinate Reference Systems (CRS) must be the same.\nsf has a function to retrieve the CRS of an sf object: st_crs.\nst_crs(ak) == st_crs(wes)\n[1] TRUE\nSo we’re good (we will see later what to do if this is not the case).\n\n\nOur combined map\nLet’s start again with a minimum map without any layout to test things out:\ntm_shape(ak) +\n  tm_polygons() +\n  tm_shape(wes) +\n  tm_polygons()\n\nUh … oh …\n\n\nWhat went wrong?\nMaps are bound by “bounding boxes”. In tmap, they are called bbox.\ntmap sets the bbox the first time tm_shape is called. In our case, the bbox was thus set to the bbox of the ak object.\nWe need to create a new bbox for our new map.\n\n\nRetrieving bounding boxes\nsf has a function to retrieve the bbox of an sf object: st_bbox\nThe bbox of ak is:\nst_bbox(ak)\nxmin         ymin       xmax         ymax\n-176.14247   52.05727   -126.85450   69.35167\n\n\nCombining bounding boxes\nbbox objects can’t be combined directly.\nHere is how we can create a new bbox encompassing both of our bboxes:\n\nfirst, we transform our bboxes to sfc objects with st_as_sfc,\nthen we combine those objects into a new sfc object with st_union,\nfinally, we retrieve the bbox of that object with st_bbox:\n\nnwa_bbox &lt;- st_bbox(\n  st_union(\n    st_as_sfc(st_bbox(wes)),\n    st_as_sfc(st_bbox(ak))\n  )\n)\n\n\nBack to our map\nWe can now use our new bounding box for the map of Western North America:\ntm_shape(ak, bbox = nwa_bbox) +\n  tm_polygons() +\n  tm_shape(wes) +\n  tm_polygons() +\n  tm_layout(\n    title = \"Glaciers of Western North America\",\n    title.position = c(\"center\", \"top\"),\n    title.size = 1.1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.06, 0.01, 0.09, 0.01),\n    outer.margins = 0,\n    frame.lwd = 0.2\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    size = 1.2,\n    text.size = 0.6\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 1000, 2000),\n    position = c(\"right\", \"BOTTOM\")\n  )\n\n\n\nLet’s add a basemap\nWe will use data from Natural Earth, a public domain map dataset.\nThere are much more fancy options, but they usually involve creating accounts (e.g. with Google) to access some API.\nIn addition, this dataset can be accessed direction from within R thanks to the rOpenSci packages:\n\nrnaturalearth: provides the functions,\nrnaturalearthdata: provides the data.\n\n\n\nCreate an sf object with states/provinces\nstates_all &lt;- ne_states(\n  country = c(\"canada\", \"united states of america\"),\n  returnclass = \"sf\"\n)\n\nne_ stands for “Natural Earth”.\n\n\n\nSelect relevant states/provinces\nstates &lt;- states_all %&gt;%\n  filter(name_en == \"Alaska\" |\n           name_en == \"British Columbia\" |\n           name_en == \"Yukon\" |\n           name_en == \"Northwest Territories\" |\n           name_en ==  \"Alberta\" |\n           name_en == \"California\" |\n           name_en == \"Washington\" |\n           name_en == \"Oregon\" |\n           name_en == \"Idaho\" |\n           name_en == \"Montana\" |\n           name_en == \"Wyoming\" |\n           name_en == \"Colorado\" |\n           name_en == \"Nevada\" |\n           name_en == \"Utah\"\n         )\n\n\nAdd the basemap to our map\n\nWhat do we need to make sure of first?\n\nst_crs(states) == st_crs(ak)\n[1] TRUE\nWe add the basemap as a 3rd layer.\nMind the order! If you put the basemap last, it will cover your data.\nOf course, we will use our nwa_bbox bounding box again.\nWe will also break tm_polygons into tm_borders and tm_fill for ak and wes in order to colourise them with slightly different colours:\ntm_shape(states, bbox = nwa_bbox) +\n  tm_polygons(col = \"#f2f2f2\", lwd = 0.2) +\n  tm_shape(ak) +\n  tm_borders(col = \"#3399ff\") +\n  tm_fill(col = \"#86baff\") +\n  tm_shape(wes) +\n  tm_borders(col = \"#3399ff\") +\n  tm_fill(col = \"#86baff\") +\n  tm_layout(\n    title = \"Glaciers of Western North America\",\n    title.position = c(\"center\", \"top\"),\n    title.size = 1.1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.06, 0.01, 0.09, 0.01),\n    outer.margins = 0,\n    frame.lwd = 0.2\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    size = 1.2,\n    text.size = 0.6\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 1000, 2000),\n    position = c(\"right\", \"BOTTOM\")\n  )\n\n\n\ntmap styles\ntmap has a number of styles that you can try.\nFor instance, to set the style to “classic”, run the following before making your map:\ntmap_style(\"classic\")\n\nOther options are:\n“white” (default), “gray”, “natural”, “cobalt”, “col_blind”, “albatross”, “beaver”, “bw”, and “watercolor”.\n\n\nTo return to the default, you need to run:\ntmap_style(\"white\")\nor:\ntmap_options_reset()\nwhich will reset every tmap option.",
    "crumbs": [
      "R",
      "<em><b>Webinars</b></em>",
      "GIS mapping with R"
    ]
  },
  {
    "objectID": "r/wb_gis_mapping.html#inset-maps",
    "href": "r/wb_gis_mapping.html#inset-maps",
    "title": "GIS mapping with R",
    "section": "Inset maps",
    "text": "Inset maps\nNow, how can we combine this with our gnp object?\nWe could add it as an inset of our Western North America map.\n\nFirst, let’s map it\nLet’s use the same tm_borders and tm_fill we just used:\ntm_shape(gnp) +\n  tm_borders(col = \"#3399ff\") +\n  tm_fill(col = \"#86baff\") +\n  tm_layout(\n    title = \"Glaciers of Glacier National Park\",\n    title.position = c(\"center\", \"top\"),\n    legend.title.color = \"#fcfcfc\",\n    legend.text.size = 1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.07, 0.03, 0.07, 0.03),\n    outer.margins = 0\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    text.size = 0.7\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 10, 20),\n    position = c(\"right\", \"BOTTOM\"),\n    text.size = 1\n  )\n\n\n\nCreate an inset map\nAs always, first we check that the CRS are the same:\nst_crs(gnp) == st_crs(ak)\n[1] FALSE\nAH!\n\n\nCRS transformation\nWe need to reproject gnp into the CRS of our other sf objects (e.g. ak):\ngnp &lt;- st_transform(gnp, st_crs(ak))\nWe can verify that the CRS are now the same:\nst_crs(gnp) == st_crs(ak)\n[1] TRUE\n\n\nInset maps: first step\nAdd a rectangle showing the location of the GNP map in the main North America map.\nWe need to create a new sfc object from the gnp bbox so that we can add it to our previous map as a new layer:\ngnp_zone &lt;- st_bbox(gnp) %&gt;%\n  st_as_sfc()\n\n\nInset maps: second step\nCreate a tmap object of the main map. Of course, we need to edit the title. Also, note the presence of our new layer:\nmain_map &lt;- tm_shape(states, bbox = nwa_bbox) +\n  tm_polygons(col = \"#f2f2f2\", lwd = 0.2) +\n  tm_shape(ak) +\n  tm_borders(col = \"#3399ff\") +\n  tm_fill(col = \"#86baff\") +\n  tm_shape(wes) +\n  tm_borders(col = \"#3399ff\") +\n  tm_fill(col = \"#86baff\") +\n  tm_shape(gnp_zone) +\n  tm_borders(lwd = 1.5, col = \"#ff9900\") +\n  tm_layout(\n    title = \"Glaciers of Glacier National Park\",\n    title.position = c(\"center\", \"top\"),\n    title.size = 1.1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.06, 0.01, 0.09, 0.01),\n    outer.margins = 0,\n    frame.lwd = 0.2\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    size = 1.2,\n    text.size = 0.6\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 500, 1000),\n    position = c(\"right\", \"BOTTOM\")\n  )\n\n\nInset maps: third step\nCreate a tmap object of the inset map.\nWe make sure to matching colours and edit the layouts for better readability:\ninset_map &lt;- tm_shape(gnp) +\n  tm_borders(col = \"#3399ff\") +\n  tm_fill(col = \"#86baff\") +\n  tm_layout(\n    legend.show = F,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.03, 0.03, 0.03, 0.03),\n    outer.margins = 0,\n    frame = \"#ff9900\",\n    frame.lwd = 3\n  )\n\n\nInset maps: final step\nCombine the two tmap objects.\nWe print the main map and add the inset map with grid::viewport:\nmain_map\nprint(inset_map, vp = viewport(0.41, 0.26, width = 0.5, height = 0.5))",
    "crumbs": [
      "R",
      "<em><b>Webinars</b></em>",
      "GIS mapping with R"
    ]
  },
  {
    "objectID": "r/wb_gis_mapping.html#mapping-a-subset-of-the-data",
    "href": "r/wb_gis_mapping.html#mapping-a-subset-of-the-data",
    "title": "GIS mapping with R",
    "section": "Mapping a subset of the data",
    "text": "Mapping a subset of the data\nTo see the retreat of the ice, we need to zoom in.\nLet’s focus on a single glacier: Agassiz Glacier.\n\nMap of the Agassiz Glacier\nSelect the data points corresponding to the Agassiz Glacier:\nag &lt;- gnp %&gt;% filter(glacname == \"Agassiz Glacier\")\ntm_shape(ag) +\n  tm_polygons() +\n  tm_layout(\n    title = \"Agassiz Glacier\",\n    title.position = c(\"center\", \"top\"),\n    legend.position = c(\"left\", \"bottom\"),\n    legend.title.color = \"#fcfcfc\",\n    legend.text.size = 1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.07, 0.03, 0.07, 0.03),\n    outer.margins = 0\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    text.size = 0.7\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 0.5, 1),\n    position = c(\"right\", \"BOTTOM\"),\n    text.size = 1\n  )\n\nNot great …\n\n\nMap based on attribute variables\ntm_shape(ag) +\n  tm_polygons(\"year\", palette = \"Blues\") +\n  tm_layout(\n    title = \"Agassiz Glacier\",\n    title.position = c(\"center\", \"top\"),\n    legend.position = c(\"left\", \"bottom\"),\n    legend.title.color = \"#fcfcfc\",\n    legend.text.size = 1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.07, 0.03, 0.07, 0.03),\n    outer.margins = 0\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    text.size = 0.7\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 0.5, 1),\n    position = c(\"right\", \"BOTTOM\"),\n    text.size = 1\n  )\n\n\n\nUsing ggplot2 instead of tmap\nAs an alternative to tmap, ggplot2 can plot maps with the geom_sf function:\nggplot(ag) +\n  geom_sf(aes(fill = year)) +\n  scale_fill_brewer(palette = \"Blues\") +\n  labs(title = \"Agassiz Glacier\") +\n  annotation_scale(location = \"bl\", width_hint = 0.4) +\n  annotation_north_arrow(location = \"tr\", which_north = \"true\",\n                         pad_x = unit(0.75, \"in\"), pad_y = unit(0.5, \"in\"),\n                         style = north_arrow_fancy_orienteering) +\n  theme_bw() +\n  theme(plot.title = element_text(hjust = 0.5))\nThe package ggspatial adds a lot of functionality to ggplot2 for spatial data.",
    "crumbs": [
      "R",
      "<em><b>Webinars</b></em>",
      "GIS mapping with R"
    ]
  },
  {
    "objectID": "r/wb_gis_mapping.html#faceted-maps",
    "href": "r/wb_gis_mapping.html#faceted-maps",
    "title": "GIS mapping with R",
    "section": "Faceted maps",
    "text": "Faceted maps\n\nFaceted map of the retreat of Agassiz\ntm_shape(ag) +\n  tm_polygons(col = \"#86baff\") +\n  tm_layout(\n    main.title = \"Agassiz Glacier\",\n    main.title.position = c(\"center\", \"top\"),\n    main.title.size = 1.2,\n    legend.position = c(\"left\", \"bottom\"),\n    legend.title.color = \"#fcfcfc\",\n    legend.text.size = 1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0, 0.03, 0, 0.03),\n    outer.margins = 0,\n    panel.label.bg.color = \"#fcfcfc\",\n    frame = F,\n    asp = 0.6\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    size = 1,\n    text.size = 0.6\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 0.5, 1),\n    position = c(\"right\", \"BOTTOM\"),\n    text.size = 0.6\n  ) +\n  tm_facets(\n    by = \"year\",\n    free.coords = F,\n    ncol = 4\n  )",
    "crumbs": [
      "R",
      "<em><b>Webinars</b></em>",
      "GIS mapping with R"
    ]
  },
  {
    "objectID": "r/wb_gis_mapping.html#animated-maps",
    "href": "r/wb_gis_mapping.html#animated-maps",
    "title": "GIS mapping with R",
    "section": "Animated maps",
    "text": "Animated maps\n\nAnimated map of the Retreat of Agassiz\nFirst, we need to create a tmap object with facets:\nagassiz_anim &lt;- tm_shape(ag) +\n  tm_polygons(col = \"#86baff\") +\n  tm_layout(\n    title = \"Agassiz Glacier\",\n    title.position = c(\"center\", \"top\"),\n    legend.position = c(\"left\", \"bottom\"),\n    legend.title.color = \"#fcfcfc\",\n    legend.text.size = 1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.08, 0, 0.08, 0),\n    outer.margins = 0,\n    panel.label.bg.color = \"#fcfcfc\"\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    text.size = 0.7\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 0.5, 1),\n    position = c(\"right\", \"BOTTOM\"),\n    text.size = 1\n  ) +\n  tm_facets(\n    along = \"year\",\n    free.coords = F\n  )\nThen we can pass that object to tmap_animation:\ntmap_animation(\n  agassiz_anim,\n  filename = \"ag.gif\",\n  dpi = 300,\n  inner.margins = c(0.08, 0, 0.08, 0),\n  delay = 100\n)\n\n\n\nMap of ice thickness of Agassiz\nNow, let’s map the estimated ice thickness on Agassiz Glacier.\nThis time, we use tm_raster:\ntm_shape(ras) +\n  tm_raster(title = \"\") +\n  tm_layout(\n    title = \"Ice thickness (m) of Agassiz Glacier\",\n    title.position = c(\"center\", \"top\"),\n    legend.position = c(\"left\", \"bottom\"),\n    legend.bg.color = \"#ffffff\",\n    legend.text.size = 1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.07, 0.03, 0.07, 0.03),\n    outer.margins = 0\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    text.size = 0.7\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 0.5, 1),\n    position = c(\"right\", \"BOTTOM\"),\n    text.size = 1\n  )\n\n\n\nCombining with Randolph data\nAs always, we check whether the CRS are the same:\nst_crs(ag) == st_crs(ras)\n[1] FALSE\nWe need to reproject ag (remember that it is best to avoid reprojecting raster data):\nag %&lt;&gt;% st_transform(st_crs(ras))\nThe retreat and ice thickness layers will hide each other (the order matters!). One option is to use tm_borders for one of them, but we can also use transparency (alpha). We also adjust the legend:\ntm_shape(ras) +\n  tm_raster(title = \"Ice (m)\") +\n  tm_shape(ag) +\n  tm_polygons(\"year\", palette = \"Blues\", alpha = 0.2, title = \"Contour\") +\n  tm_layout(\n    title = \"Ice thickness (m) and retreat of Agassiz Glacier\",\n    title.position = c(\"center\", \"top\"),\n    legend.position = c(\"left\", \"bottom\"),\n    legend.bg.color = \"#ffffff\",\n    legend.text.size = 0.7,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.07, 0.03, 0.07, 0.03),\n    outer.margins = 0\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    text.size = 0.7\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 0.5, 1),\n    position = c(\"right\", \"BOTTOM\"),\n    text.size = 1\n  )\n\n\n\nRefining raster maps\nLet’s go back to our ice thickness map:\n\nWe can change the palette to blue with tm_raster(palette = \"Blues\"):\n\n\nWe can create a more suitable interval scale\nFirst, let’s see what the maximum value is:\nglobal(ras, \"max\")\nmax\nRGI60-02.16664_thickness 70.10873\nThen we can set the breaks with tm_raster(breaks = seq(0, 80, 5))\nWe also need to tweak the layout, legend, etc.:\ntm_shape(ras) +\n  tm_raster(title = \"\", palette = \"Blues\", breaks = seq(0, 80, 5)) +\n  tm_layout(\n    title = \"Ice thickness (m) of Agassiz Glacier\",\n    title.position = c(\"center\", \"top\"),\n    legend.position = c(\"left\", \"bottom\"),\n    legend.bg.color = \"#ffffff\",\n    legend.text.size = 0.7,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.07, 0.03, 0.07, 0.03),\n    outer.margins = 0\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    text.size = 0.7\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 0.5, 1),\n    position = c(\"right\", \"BOTTOM\"),\n    text.size = 1\n  )\n\nOr we can use a continuous colour scheme with tm_raster(style = \"cont\"):",
    "crumbs": [
      "R",
      "<em><b>Webinars</b></em>",
      "GIS mapping with R"
    ]
  },
  {
    "objectID": "r/wb_gis_mapping.html#basemaps",
    "href": "r/wb_gis_mapping.html#basemaps",
    "title": "GIS mapping with R",
    "section": "Basemaps",
    "text": "Basemaps\n\nBasemap with ggmap\nbasemap &lt;- get_map(\n  bbox = c(\n    left = st_bbox(ag)[1],\n    bottom = st_bbox(ag)[2],\n    right = st_bbox(ag)[3],\n    top = st_bbox(ag)[4]\n  ),\n  source = \"osm\"\n)\n\nggmap is a powerful package, but Google now requires an API key obtained through registration\n\n\n\nBasemap with basemaps\nThe package basemaps allows to download open source basemap data from several sources, but those cannot easily be combined with sf objects\nThis plots a satellite image of the Agassiz Glacier:\nbasemap_plot(ag, map_service = \"esri\", map_type = \"world_imagery\")\n\n\nSatellite image of the Agassiz Glacier",
    "crumbs": [
      "R",
      "<em><b>Webinars</b></em>",
      "GIS mapping with R"
    ]
  },
  {
    "objectID": "r/wb_gis_mapping.html#tiled-web-maps-with-leaflet-js",
    "href": "r/wb_gis_mapping.html#tiled-web-maps-with-leaflet-js",
    "title": "GIS mapping with R",
    "section": "Tiled web maps with Leaflet JS",
    "text": "Tiled web maps with Leaflet JS\n\nmapview\nmapview(gnp)\n\n\n\nCartoDB.Positron\n\n\n\n\n\nOpenStreetMap\n\n\n\n\n\nOpenTopoMap\n\n\n\n\n\nEsri.WorldImagery\n\n\n\n\ntmap\nSo far, we have used the plot mode of tmap. There is also a view mode which allows interactive viewing in a browser through Leaflet\nChange to view mode:\ntmap_mode(\"view\")\n\nYou can also toggle between modes with ttm\n\nRe-plot the last map we plotted with tmap:\ntmap_last()\n\n\nleaflet\nleaflet creates a map widget to which you add layers\nmap &lt;- leaflet()\naddTiles(map)",
    "crumbs": [
      "R",
      "<em><b>Webinars</b></em>",
      "GIS mapping with R"
    ]
  },
  {
    "objectID": "r/wb_gis_mapping.html#spatial-data-analysis",
    "href": "r/wb_gis_mapping.html#spatial-data-analysis",
    "title": "GIS mapping with R",
    "section": "Spatial data analysis",
    "text": "Spatial data analysis\n\nResources\nHere are some resources on the topic to get started.\n\nR companion to Geographic Information Analysis\nSpatial data analysis",
    "crumbs": [
      "R",
      "<em><b>Webinars</b></em>",
      "GIS mapping with R"
    ]
  },
  {
    "objectID": "r/wb_gis_mapping.html#image-credits",
    "href": "r/wb_gis_mapping.html#image-credits",
    "title": "GIS mapping with R",
    "section": "Image credits",
    "text": "Image credits\nSzűcs Róbert, Grasshopper Geography",
    "crumbs": [
      "R",
      "<em><b>Webinars</b></em>",
      "GIS mapping with R"
    ]
  },
  {
    "objectID": "r/wb_hpc.html",
    "href": "r/wb_hpc.html",
    "title": "High-performance research computing in R",
    "section": "",
    "text": "R is not famous for its speed. With code optimization and parallelization, it can however be used for heavy computations.\nThis webinar will introduce you to working with R from the command line on the Alliance clusters with a focus on performance. We will discuss code benchmarking and various parallelization techniques.\n\nSlides (Click and wait: this reveal.js presentation may take a little time to load.)",
    "crumbs": [
      "R",
      "<em><b>Webinars</b></em>",
      "HPC in R"
    ]
  },
  {
    "objectID": "r/wb_package.html",
    "href": "r/wb_package.html",
    "title": "Creating R packages",
    "section": "",
    "text": "Coming up in spring 2024."
  },
  {
    "objectID": "r/ws_gis_intro.html",
    "href": "r/ws_gis_intro.html",
    "title": "Introduction to GIS with R",
    "section": "",
    "text": "This workshop is an introduction to GIS in R. We will learn how to import GIS data, explore it, and map it.\nIn particular, we will create maps (inset maps, faceted maps, animated maps, interactive maps, and raster maps), thanks to the packages sf, tmap, raster, leaflet, ggplot2, grid (part of Base R), and mapview.\nWe will also learn how to add basemaps from OpenStreetMap and Google Maps.",
    "crumbs": [
      "R",
      "<b><em>Workshops</em></b>",
      "GIS with R"
    ]
  },
  {
    "objectID": "r/ws_gis_intro.html#getting-the-data",
    "href": "r/ws_gis_intro.html#getting-the-data",
    "title": "Introduction to GIS with R",
    "section": "Getting the data",
    "text": "Getting the data\n\nDatasets\nFor this webinar, we will use:\n\nthe Alaska as well as the Western Canada and USA subsets of the Randolph Glacier Inventory version 6.01\nthe USGS time series of the named glaciers of Glacier National Park2 The datasets can be downloaded as zip files from these websites.\n\n1 RGI Consortium (2017). Randolph Glacier Inventory – A Dataset of Global Glacier Outlines: Version 6.0: Technical Report, Global Land Ice Measurements from Space, Colorado, USA. Digital Media. DOI: https://doi.org/10.7265/N5-RGI-60.2 Fagre, D.B., McKeon, L.A., Dick, K.A., and Fountain, A.G., 2017, Glacier margin time series (1966, 1998, 2005, 2015) of the named glaciers of Glacier National Park, MT, USA: U.S. Geological Survey data release, https://doi.org/10.5066/F7P26WB1.\n\nBasemaps\nFor our basemaps, we will use data from:\n\nNatural Earth: this dataset can be accessed direction from within R thanks to the packages rnaturalearth (which provides the functions) and rnaturalearthdata (which provides the data)",
    "crumbs": [
      "R",
      "<b><em>Workshops</em></b>",
      "GIS with R"
    ]
  },
  {
    "objectID": "r/ws_gis_intro.html#loading-and-exploring-data",
    "href": "r/ws_gis_intro.html#loading-and-exploring-data",
    "title": "Introduction to GIS with R",
    "section": "Loading and exploring data",
    "text": "Loading and exploring data\nFirst, let’s load the necessary packages for this webinar:\nlibrary(sf)\nlibrary(tmap)\nlibrary(dplyr)\nlibrary(magrittr)\nlibrary(purrr)\nlibrary(rnaturalearth)\nlibrary(rnaturalearthdata)\nlibrary(mapview)\nlibrary(grid) # part of base R (already installed), but needs to be explicitly loaded\nWe will start by mapping all the glaciers of Western North America thanks to:\n\nthe Alaska subset of the Randolph Glacier Inventory\nthe Western Canada and USA subset of the Randolph Glacier Inventory\n\nDownload and unzip 02_rgi60_WesternCanadaUS and 01_rgi60_Alaska from the Randolph Glacier Inventory version 6.0.\nData get imported and turned into sf objects by the function sf::st_read():\nak &lt;- st_read(\"01_rgi60_Alaska\")\nwes &lt;- st_read(\"02_rgi60_WesternCanadaUS\")\n\nMake sure to use the absolute paths or the proper paths relative to your working directory (which can be obtained with getwd() and modified with setwd()).\n\nYou can print and explore your new objects:\nak\nwes\n\nstr(ak)\nstr(wes)\nsf objects are data.frame-like objects with a geometry list-column as their last column. That column is itself an object of class sfc (simple feature geometry list column).",
    "crumbs": [
      "R",
      "<b><em>Workshops</em></b>",
      "GIS with R"
    ]
  },
  {
    "objectID": "r/ws_gis_intro.html#mapping-with-tmap",
    "href": "r/ws_gis_intro.html#mapping-with-tmap",
    "title": "Introduction to GIS with R",
    "section": "Mapping with tmap",
    "text": "Mapping with tmap\ntmap follows a grammar of graphic similar to that of ggplot2: you first need to set a shape (a spatial data object) by passing an sf object to tm_shape(). Then you plot one or several layers with one of several tmap functions and you use the + sign between each element.\nTo see the available options, run:\n?tmap-element\nWe could thus plot the glaciers of Alaska with any of the options below:\ntm_shape(ak) +\n  tm_borders()\n\ntm_shape(ak) +\n  tm_fill()\n\ntm_shape(ak) +\n  tm_polygons()      # shows both borders and fill\nHere, we will use tm_polygons() which combines tm_borders() and tm_fill().",
    "crumbs": [
      "R",
      "<b><em>Workshops</em></b>",
      "GIS with R"
    ]
  },
  {
    "objectID": "r/ws_gis_intro.html#layout-elements-and-attribute-layers",
    "href": "r/ws_gis_intro.html#layout-elements-and-attribute-layers",
    "title": "Introduction to GIS with R",
    "section": "Layout elements and attribute layers",
    "text": "Layout elements and attribute layers\nA map without title, compass, or scale bars is not very useful though. We need to add layout elements and attribute layers to the map.\nYou can loop up the many arguments of the tmap functions in the help pages to see how you can customize your maps:\n?tm_layout\n?tm_compass\n?tm_scale_bar\nLet’s now map the glaciers of Alaska:\ntm_shape(ak) +\n  tm_polygons() +\n  tm_layout(\n    title = \"Glaciers of Alaska\",\n    title.position = c(\"center\", \"top\"),\n    title.size = 1.1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.06, 0.01, 0.09, 0.01),\n    outer.margins = 0,\n    frame.lwd = 0.2\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    size = 1.2,\n    text.size = 0.6\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 500, 1000),\n    position = c(\"right\", \"BOTTOM\")\n  )",
    "crumbs": [
      "R",
      "<b><em>Workshops</em></b>",
      "GIS with R"
    ]
  },
  {
    "objectID": "r/ws_gis_intro.html#union-of-bounding-boxes",
    "href": "r/ws_gis_intro.html#union-of-bounding-boxes",
    "title": "Introduction to GIS with R",
    "section": "Union of bounding boxes",
    "text": "Union of bounding boxes\nNow, if we want to plot all the glaciers of Western North America, we want to combine both sf objects in the same map. A map can contain multiple shapes: you only need to “add” a tm_shape and its element(s). Before doing so however, it is very important to ensure that they have the same coordinate reference system (CRS):\nst_crs(ak)\nst_crs(wes)\n\nst_crs(ak) == st_crs(wes)\nThey do, so we are good to go.\n\nAs with ggplot2 or GIS graphical user interfaces, the order matters since the layers stack up on top of each other.\n\ntm_shape(ak) +\n  tm_polygons() +\n  tm_shape(wes) +\n  tm_polygons()\nIf you run the code above however, you may be surprised that you are still only plotting the map of Alaska.\nThis is because each map comes with a spatial bounding box (bbox).\nst_bbox(ak)\nst_bbox(wes)\nIn the code above, the bbox is set by the first shape, i.e. our entire map uses the bbox of the Alaska sf object.\nWe first need to create a new bounding box encompassing both bounding boxes:\nnwa_bbox &lt;- st_bbox(\n  st_union(\n    st_as_sfc(st_bbox(wes)),\n    st_as_sfc(st_bbox(ak))\n  )\n)\nWe can now plot the glaciers of Western North America:\ntm_shape(ak, bbox = nwa_bbox) +\n  tm_polygons() +\n  tm_shape(wes) +\n  tm_polygons() +\n  tm_layout(\n    title = \"Glaciers of Western North America\",\n    title.position = c(\"center\", \"top\"),\n    title.size = 1.1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.06, 0.01, 0.09, 0.01),\n    outer.margins = 0,\n    frame.lwd = 0.2\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    size = 1.2,\n    text.size = 0.6\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 1000, 2000),\n    position = c(\"right\", \"BOTTOM\")\n  )",
    "crumbs": [
      "R",
      "<b><em>Workshops</em></b>",
      "GIS with R"
    ]
  },
  {
    "objectID": "r/ws_gis_intro.html#maps-based-on-an-attribute-variable",
    "href": "r/ws_gis_intro.html#maps-based-on-an-attribute-variable",
    "title": "Introduction to GIS with R",
    "section": "Maps based on an attribute variable",
    "text": "Maps based on an attribute variable\nWhat is interesting about glacier maps is to see their evolution through time as glaciers retreat due to climate change. While the Randolph Glacier Inventory (RGI) has an amazing map in terms of spacial coverage, it doesn’t yet have much temporal data.\nTo look at glacier retreat, we will look at the USGS time series of the named glaciers of Glacier National Park3. These 4 datasets have the contour lines of 39 glaciers for the years 1966, 1998, 2005, and 2015.\n3 Fagre, D.B., McKeon, L.A., Dick, K.A., and Fountain, A.G., 2017, Glacier margin time series (1966, 1998, 2005, 2015) of the named glaciers of Glacier National Park, MT, USA: U.S. Geological Survey data release, https://doi.org/10.5066/F7P26WB1.We could load and clean these datasets one by one. Copying and pasting code however is inefficient and error-prone. A better approach is to do this in a functional programming framework: create a function which does all the data loading and cleaning, then pass each element of a vector of the paths of all 4 datasets to it using purrr::map().\n“Cleaning” here consists of selecting the variables we are interested in, putting them in the same order in each dataset (they were not initially) and giving the exact same name across all datasets (there were case inconsistencies between datasets and R is case sensitive).\n# create a function that reads and cleans the data\nprep &lt;- function(dir) {\n  g &lt;- st_read(dir)\n  g %&lt;&gt;% rename_with(~ tolower(gsub(\"Area....\", \"area\", .x)))\n  g %&lt;&gt;% select(\n    year,\n    objectid,\n    glacname,\n    area,\n    shape_leng,\n    x_coord,\n    y_coord,\n    source_sca,\n    source\n  )\n}\n\n# create a vector of dataset names\ndirs &lt;- grep(\"GNPglaciers_.*\", list.dirs(), value = T)\n\n# pass each element of that vector through prep() thanks to map()\ngnp &lt;- map(dirs, prep)\nmap() returns a list, so we now have a list (gnp) of 4 elements: the 4 sf objects containing our cleaned datasets. A list is not really convenient and we will turn it into a single sf object.\nBefore doing so however, we want to make sure that they all have the same CRS:\nst_crs(gnp[[1]]) == st_crs(gnp[[2]])\nst_crs(gnp[[1]]) == st_crs(gnp[[3]])\nst_crs(gnp[[1]]) == st_crs(gnp[[4]])\nThey do, so we can turn gnp into a single sf object:\ngnp &lt;- do.call(\"rbind\", gnp)\n\ngnp\nstr(gnp)\nWe can now map the data:\ntm_shape(gnp) +\n  tm_polygons(\"year\", palette = \"Blues\") +\n  tm_layout(\n    title = \"Glaciers of Glacier National Park\",\n    title.position = c(\"center\", \"top\"),\n    legend.title.color = \"#fcfcfc\",\n    legend.text.size = 1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.07, 0.03, 0.07, 0.03),\n    outer.margins = 0\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    text.size = 0.7\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 10, 20),\n    position = c(\"right\", \"BOTTOM\"),\n    text.size = 1\n  )\n\nI didn’t want to show the legend title and because there is no option to remove it, I set its color to that of the background.",
    "crumbs": [
      "R",
      "<b><em>Workshops</em></b>",
      "GIS with R"
    ]
  },
  {
    "objectID": "r/ws_gis_intro.html#crs-transformation",
    "href": "r/ws_gis_intro.html#crs-transformation",
    "title": "Introduction to GIS with R",
    "section": "CRS transformation",
    "text": "CRS transformation\nWouldn’t it be nice to have this map as an inset of the previous map so that we can situate it within North America?\nBefore we can do this, we need to make sure that both maps use the same CRS:\nst_crs(ak)\nst_crs(gnp)\n\nWe could use wes instead of ak since we know that both sf objects have the same CRS.\n\nThey don’t have the same CRS, so we reproject gnp by transforming its data from its current CRS to that of ak.\ngnp &lt;- st_transform(gnp, st_crs(ak))\nst_crs(gnp)",
    "crumbs": [
      "R",
      "<b><em>Workshops</em></b>",
      "GIS with R"
    ]
  },
  {
    "objectID": "r/ws_gis_intro.html#inset-map",
    "href": "r/ws_gis_intro.html#inset-map",
    "title": "Introduction to GIS with R",
    "section": "Inset map",
    "text": "Inset map\nNow we can create our map with an inset: the map of the Western North America glaciers (from the sf object nwa) will be our main map and the map of Glacier National Park (from the sf object gnp) will be the inset.\nIf the goal of this new map is to show the location of the gnp map within the nwa one, we need to add a rectangle showing the bounding box of gnp in the nwa map as a new layer.\nFor this, we create a new sfc_POLYGON from the bounding box of gnp:\ngnp_zone &lt;- st_bbox(gnp) %&gt;%\n  st_as_sfc()\nWe will use it as the following layer within the new map:\ntm_shape(gnp_zone) +\n  tm_borders(lwd = 1.5, col = \"#ff9900\")\nWe assign our new map (with an updated suitable title) to the object main_map:\nmain_map &lt;- tm_shape(ak, bbox = nwa_bbox) +\n  tm_polygons() +\n  tm_shape(wes) +\n  tm_polygons() +\n  tm_shape(gnp_zone) +\n  tm_borders(lwd = 1.5, col = \"#ff9900\") +\n  tm_layout(\n    title = \"Glaciers of Glacier National Park\",\n    title.position = c(\"center\", \"top\"),\n    title.size = 1.1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.06, 0.01, 0.09, 0.01),\n    outer.margins = 0,\n    frame.lwd = 0.2\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    size = 1.2,\n    text.size = 0.6\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 500, 1000),\n    position = c(\"right\", \"BOTTOM\")\n  )\nNext, we will change the frame of the gnp inset to match the color of this new rectangle (to make it visually clear that this is a close-up view of that rectangle). We can also remove the title, compass and scale bar since this is an inset within a map which already have them. We assign this new map to the object inset_map:\ninset_map &lt;- tm_shape(gnp) +\n  tm_polygons(\"year\", palette = \"Blues\") +\n  tm_layout(\n    legend.title.color = \"#fcfcfc\",\n    legend.text.size = 0.7,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.03, 0.03, 0.03, 0.03),\n    outer.margins = 0,\n    frame = \"#ff9900\",\n    frame.lwd = 3\n  )\nFinally, we combine the two maps with grid::viewport():\nmain_map\nprint(inset_map, vp = viewport(0.41, 0.26, width = 0.5, height = 0.5))",
    "crumbs": [
      "R",
      "<b><em>Workshops</em></b>",
      "GIS with R"
    ]
  },
  {
    "objectID": "r/ws_gis_intro.html#tiled-web-maps-with-leaflet",
    "href": "r/ws_gis_intro.html#tiled-web-maps-with-leaflet",
    "title": "Introduction to GIS with R",
    "section": "Tiled web maps with Leaflet",
    "text": "Tiled web maps with Leaflet\nTiled web maps are interactive maps in a browser using web servers such as Google Maps or OpenStreetMap. Several packages allow to use Leaflet (an open-source JavaScript library for interactive maps) to create tile maps.\n\nWith mapview\nThe simplest option is to use mapview::mapview():\nmapview(gnp)\nThis will open a page in your browser in which you can pan, zoom, select/deselect data layers, and choose from a number of basemap layer options:\n CartoDB.Positron\n OpenTopoMap\n OpenStreetMap\n Esri.WorldImagery\n\n\nWith tmap\ntmap has similar capabilities.\nThe package has 2 modes:\n\nplot is the default mode for static maps that we used earlier.\nview is an interactive viewing mode using Leaflet in a browser. There, as with mapview, you can zoom in/out, select/deselect the different layers, and choose to display one of Esri.WorldGrayCanvas, OpenStreetMap, or Esri.WorldTopoMap basemaps.\n\nYou can toggle between the plot and view modes with ttm(), after which you can re-plot your last plot in the new mode with tmap_last(). You can also do both of these at once with ttmp().\nAlternatively, you can switch to either mode with tmap_mode(\"view\") and tmap_mode(\"plot\").\n\nExample:\n\nEarlier, we plotted all the glaciers of Western North America using tmap:\n\nAfter displaying this map, we could have run:\ntmap_mode(\"view\")\ntmap_last()\nAnd Leaflet would have open the following interactive map in our browser:\n\n\nAfterwards, if you want to create new static plots, don’t forget to get back to plot mode with tmap_mode(\"plot\").",
    "crumbs": [
      "R",
      "<b><em>Workshops</em></b>",
      "GIS with R"
    ]
  },
  {
    "objectID": "r/ws_gis_intro.html#mapping-a-subset-of-the-data",
    "href": "r/ws_gis_intro.html#mapping-a-subset-of-the-data",
    "title": "Introduction to GIS with R",
    "section": "Mapping a subset of the data",
    "text": "Mapping a subset of the data\nEach glacier has 4 borders: one for each year of survey. They are however quite hard to see on such a large map.\nLet’s zoom on the Agassiz glacier:\n# select the data points corresponding to the Agassiz Glacier\nag &lt;- g %&gt;% filter(glacname == \"Agassiz Glacier\")\nAnd map it:\ntm_shape(ag) +\n  tm_polygons(\"year\", palette = \"Blues\") +\n  tm_layout(\n    title = \"Agassiz Glacier\",\n    title.position = c(\"center\", \"top\"),\n    legend.position = c(\"left\", \"bottom\"),\n    legend.title.color = \"#fcfcfc\",\n    legend.text.size = 1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.07, 0.03, 0.07, 0.03),\n    outer.margins = 0\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    text.size = 0.7\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 0.5, 1),\n    position = c(\"right\", \"BOTTOM\"),\n    text.size = 1\n  )\n\nNow we can clearly see the retreat of the Agassiz Glacier between 1966 and 2015.",
    "crumbs": [
      "R",
      "<b><em>Workshops</em></b>",
      "GIS with R"
    ]
  },
  {
    "objectID": "r/ws_gis_intro.html#faceted-map",
    "href": "r/ws_gis_intro.html#faceted-map",
    "title": "Introduction to GIS with R",
    "section": "Faceted map",
    "text": "Faceted map\nInstead of having all temporal data in a single map however, it can be split across facets:\ntm_shape(ag) +\n  tm_polygons(col = \"#86baff\") +\n  tm_layout(\n    main.title = \"Agassiz Glacier\",\n    main.title.position = c(\"center\", \"top\"),\n    main.title.size = 1.2,\n    legend.position = c(\"left\", \"bottom\"),\n    legend.title.color = \"#fcfcfc\",\n    legend.text.size = 1,\n    bg.color = \"#fcfcfc\",\n    # inner.margins = c(0, 0.03, 0, 0.03),\n    outer.margins = 0,\n    panel.label.bg.color = \"#fcfcfc\",\n    frame = F,\n    asp = 0.6\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    size = 1,\n    text.size = 0.6\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 0.5, 1),\n    position = c(\"right\", \"BOTTOM\"),\n    text.size = 0.6\n  ) +\n  tm_facets(\n    by = \"year\",\n    free.coords = F,\n    ncol = 4\n  )",
    "crumbs": [
      "R",
      "<b><em>Workshops</em></b>",
      "GIS with R"
    ]
  },
  {
    "objectID": "r/ws_gis_intro.html#animated-map",
    "href": "r/ws_gis_intro.html#animated-map",
    "title": "Introduction to GIS with R",
    "section": "Animated map",
    "text": "Animated map\nThe temporal data of the Agassiz Glacier retreat can also be conveyed through an animation:\nagassiz_anim &lt;- tm_shape(ag) +\n  tm_borders() +\n  tm_fill(col = \"#86baff\") +\n  tm_layout(\n    title = \"Agassiz Glacier\",\n    title.position = c(\"center\", \"top\"),\n    legend.position = c(\"left\", \"bottom\"),\n    legend.title.color = \"#fcfcfc\",\n    legend.text.size = 1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.08, 0, 0.08, 0),\n    outer.margins = 0\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    text.size = 0.7\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 0.5, 1),\n    position = c(\"right\", \"BOTTOM\"),\n    text.size = 1\n  ) +\n  tm_facets(\n    along = \"year\",\n    free.coords = F\n  )\n\ntmap_animation(\n  agassiz_anim,\n  filename = \"ag.gif\",\n  dpi = 300,\n  inner.margins = c(0.08, 0, 0.08, 0),\n  delay = 100\n)",
    "crumbs": [
      "R",
      "<b><em>Workshops</em></b>",
      "GIS with R"
    ]
  },
  {
    "objectID": "r/ws_gis_intro.html#additional-resources",
    "href": "r/ws_gis_intro.html#additional-resources",
    "title": "Introduction to GIS with R",
    "section": "Additional resources",
    "text": "Additional resources\nOpen GIS data:\nFree GIS Data: list of free GIS datasets\nBooks\nGeocomputation with R by Robin Lovelace, Jakub Nowosad, and Jannes Muenchow\nSpatial Data Science by Edzer Pebesma, Roger Bivand\nSpatial Data Science with R by Robert J. Hijmans\nUsing Spatial Data with R by Claudia A. Engel\nTutorial\nAn Introduction to Spatial Data Analysis and Visualisation in R by the CDRC\nWebsite\nr-spatial by Edzer Pebesma, Marius Appel, and Daniel Nüst\nCRAN package list\nAnalysis of Spatial Data\nMailing list\nR Special Interest Group on using Geographical data and Mapping",
    "crumbs": [
      "R",
      "<b><em>Workshops</em></b>",
      "GIS with R"
    ]
  },
  {
    "objectID": "r/ws_r_demo.html",
    "href": "r/ws_r_demo.html",
    "title": "A little demo of programming in R",
    "section": "",
    "text": "R is a free and open-source programming language with a large collection of packages for statistical computing, modeling, and graphics. It is extremely popular in several academic fields including statistics, biology, economics, data mining, data analysis, and linguistics.\nThis high-level presentation will give you a sense of what R can be used for through a series of examples (data visualization, web scraping, and GIS). I will also talk about the strengths and weaknesses of R and who would benefit most from learning it.\n\nSlides (Click and wait: the presentation might take a few instants to load)",
    "crumbs": [
      "R",
      "<b><em>Workshops</em></b>",
      "R: a demo"
    ]
  },
  {
    "objectID": "r/ws_webscraping.html",
    "href": "r/ws_webscraping.html",
    "title": "Web scraping with R",
    "section": "",
    "text": "The internet is a trove of information. A lot of it is publicly available and thus suitable for use in research. Extracting that information and putting it in an organized format for analysis can, however, be extremely tedious. Web scraping tools allow to automate parts of that process and R is a popular language for the task.\nIn this workshop, we will guide you through a simple example using the package rvest.",
    "crumbs": [
      "R",
      "<b><em>Workshops</em></b>",
      "Web scraping with R"
    ]
  },
  {
    "objectID": "r/ws_webscraping.html#html-and-css",
    "href": "r/ws_webscraping.html#html-and-css",
    "title": "Web scraping with R",
    "section": "HTML and CSS",
    "text": "HTML and CSS\nHyperText Markup Language (HTML) is the standard markup language for websites: it encodes the information related to the formatting and structure of webpages. Additionally, some of the customization can be stored in Cascading Style Sheets (CSS) files.\nHTML uses tags of the form:\n&lt;some_tag&gt;Your content&lt;/some_tag&gt;\nSome tags have attributes:\n&lt;some_tag attribute_name=\"attribute value\"&gt;Your content&lt;/some_tag&gt;\n\nExamples:\n\nSite structure:\n\n&lt;h2&gt;This is a heading of level 2&lt;/h2&gt;\n&lt;p&gt;This is a paragraph&lt;/p&gt;\n\nFormatting:\n\n&lt;b&gt;This is bold&lt;/b&gt;\n&lt;a href=\"https://some.url\"&gt;This is the text for a link&lt;/a&gt;",
    "crumbs": [
      "R",
      "<b><em>Workshops</em></b>",
      "Web scraping with R"
    ]
  },
  {
    "objectID": "r/ws_webscraping.html#web-scrapping",
    "href": "r/ws_webscraping.html#web-scrapping",
    "title": "Web scraping with R",
    "section": "Web scrapping",
    "text": "Web scrapping\nWeb scraping is a general term for a set of tools which allow for the extraction of data from the web automatically.\nWhile most of the data on the internet is publicly available, it is illegal to scrape some sites and you should always look into the policy of a site before attempting to scrape it. Some sites will also block you if you submit too many requests in a short amount of time, so if you plan on scraping sites at a fairly large scale, you should look into the polite package which will help you scrape responsibly.",
    "crumbs": [
      "R",
      "<b><em>Workshops</em></b>",
      "Web scraping with R"
    ]
  },
  {
    "objectID": "r/ws_webscraping.html#example-for-this-workshop",
    "href": "r/ws_webscraping.html#example-for-this-workshop",
    "title": "Web scraping with R",
    "section": "Example for this workshop",
    "text": "Example for this workshop\nWe will use a website from the University of Tennessee containing a database of PhD theses from that university.\nOur goal is to scrape data from this site to produce a dataframe with the date, major, and advisor for each dissertation.\n\nWe will only do this for the first page which contains the links to the 100 most recent theses. If you really wanted to gather all the data, you would have to do this for all pages.",
    "crumbs": [
      "R",
      "<b><em>Workshops</em></b>",
      "Web scraping with R"
    ]
  },
  {
    "objectID": "r/ws_webscraping.html#lets-look-at-the-sites",
    "href": "r/ws_webscraping.html#lets-look-at-the-sites",
    "title": "Web scraping with R",
    "section": "Let’s look at the sites",
    "text": "Let’s look at the sites\nFirst of all, let’s have a close look at the websites we want to scrape to think carefully about what we want to do. Before starting to write code, it is always a good idea to think about what you are trying to achieve with your code.\nTo create a dataframe with the data for all the dissertations on that first page, we need to do two things:\n\nStep 1: from the dissertations database first page, we want to scrape the list of URLs for the dissertation pages.\nStep 2: once we have the URLs, we want to scrape those pages too to get the date, major, and advisor for each dissertation.",
    "crumbs": [
      "R",
      "<b><em>Workshops</em></b>",
      "Web scraping with R"
    ]
  },
  {
    "objectID": "r/ws_webscraping.html#package",
    "href": "r/ws_webscraping.html#package",
    "title": "Web scraping with R",
    "section": "Package",
    "text": "Package\nTo do all this, we will use the package rvest, part of the tidyverse (a modern set of R packages). It is a package influenced by the popular Python package Beautiful Soup and it makes scraping websites with R really easy.\nLet’s load it:\n\nlibrary(rvest)",
    "crumbs": [
      "R",
      "<b><em>Workshops</em></b>",
      "Web scraping with R"
    ]
  },
  {
    "objectID": "r/ws_webscraping.html#read-in-html-from-main-site",
    "href": "r/ws_webscraping.html#read-in-html-from-main-site",
    "title": "Web scraping with R",
    "section": "Read in HTML from main site",
    "text": "Read in HTML from main site\nAs mentioned above, our site is the database of PhD dissertations from the University of Tennessee.\nLet’s create a character vector with the URL:\n\nurl &lt;- \"https://trace.tennessee.edu/utk_graddiss/index.html\"\n\nFirst, we read in the html data from that page:\n\nhtml &lt;- read_html(url)\n\nLet’s have a look at the raw data:\n\nhtml\n\n{html_document}\n&lt;html lang=\"en\"&gt;\n[1] &lt;head&gt;\\n&lt;meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8 ...\n[2] &lt;body&gt;\\n&lt;!-- FILE /srv/sequoia/main/data/trace.tennessee.edu/assets/heade ...",
    "crumbs": [
      "R",
      "<b><em>Workshops</em></b>",
      "Web scraping with R"
    ]
  },
  {
    "objectID": "r/ws_webscraping.html#test-run",
    "href": "r/ws_webscraping.html#test-run",
    "title": "Web scraping with R",
    "section": "Test run",
    "text": "Test run\n\nIdentify the relevant HTML markers\nThe html code for this webpage contains the data we are interested in, but it is mixed in with a lot of HTML formatting and data we don’t care about. We need to extract the data relevant to us and turn it into a workable format.\nThe first step is to find the HTML markers that contain our data. One option is to use a web inspector or—even easier—the SelectorGadget, a JavaScript bookmarklet built by Andrew Cantino.\nTo use this tool, go to the SelectorGadget website and drag the link of the bookmarklet to your bookmarks bar.\nNow, go to the dissertations database first page and click on the bookmarklet in your bookmarks bar. You will see a floating box at the bottom of your screen. As you move your mouse across the screen, an orange rectangle appears around each element over which you pass.\nClick on one of the dissertation links: now, there is an a appearing in the box at the bottom as well as the number of elements selected. The selected elements are highlighted in yellow. Those elements are links (in HTML, a tags define hyperlinks).\nAs you can see, all the links we want are selected. However, there are many other links we don’t want that are also highlighted. In fact, all links in the document are selected. We need to remove the categories of links that we don’t want. To do this, hover above any of the links we don’t want. You will see a red rectangle around it. Click on it: now all similar links are gone. You might have to do this a few times until only the relevant links (i.e. those that lead to the dissertation information pages) remain highlighted.\nAs there are 100 such links per page, the count of selected elements in the bottom floating box should be down to 100.\nIn the main section of the floating box, you can now see: .article-listing a. This means that the data we want are under the HTML elements .article-listing a (the class .article-listing and the tag a).\n\n\nExtract test URL\nIt is a good idea to test things out on a single element before doing a massive batch scraping of a site, so let’s test our method for the first dissertation.\nTo start, we need to extract the first URL. The function html_element() from the package rvest extracts the first element matching some character. Let’s pass to this function our html object and the character \".article-listing a\" and assign the result to an object that we will call test:\n\ntest &lt;- html %&gt;% html_element(\".article-listing a\")\n\n\n%&gt;% is a pipe from the magrittr tidyverse package. It passes the output from the left-hand side expression as the first argument of the right-hand side expression. We could have written this as:\ntest &lt;- html_element(html, \".article-listing a\")\n\nOur new object is a list:\n\ntypeof(test)\n\n[1] \"list\"\n\n\nLet’s print it:\n\ntest\n\n{html_node}\n&lt;a href=\"https://trace.tennessee.edu/utk_graddiss/8076\"&gt;\n\n\nThe URL is in there, so we successfully extracted the correct element, but we need to do more cleaning.\na is one of the HTML tags that have an attribute (href) as you can see when you print test. It is actually the value of that attribute that we want. To extract an attribute value, we use the function html_attr():\n\nurl_test &lt;- test %&gt;% html_attr(\"href\")\nurl_test\n\n[1] \"https://trace.tennessee.edu/utk_graddiss/8076\"\n\n\nThis is our URL.\n\nstr(url_test)\n\n chr \"https://trace.tennessee.edu/utk_graddiss/8076\"\n\n\nIt is saved in a character vector, which is perfect.\n\nInstead of creating the intermediate objects html and test, we could have chained the functions:\n\nurl_test &lt;- read_html(url) %&gt;%\n  html_element(\".article-listing a\") %&gt;%\n  html_attr(\"href\")\n\n\n\n\nRead in HTML data for test URL\nNow that we have the URL for the first dissertation information page, we want to extract the date, major, and advisor for that dissertation.\nWe just saw that url_test is a character vector representing a URL. We know how to deal with this.\nThe first thing to do—as we did earlier with the database site—is to read in the html data. Let’s assign it to a new object that we will call html_test:\n\nhtml_test &lt;- read_html(url_test)\nhtml_test\n\n{html_document}\n&lt;html lang=\"en\"&gt;\n[1] &lt;head&gt;\\n&lt;meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8 ...\n[2] &lt;body&gt;\\n&lt;!-- FILE /srv/sequoia/main/data/trace.tennessee.edu/assets/heade ...\n\n\n\n\nGet data for test URL\nNow, we want to extract the publication date. Thanks to the SelectorGadget, following the method we saw earlier, we can see that we now need the element marked by #publication_date p.\nWe start by extracting the data as we did earlier by passing our object html_test and the character \"#publication_date p\" to html_element().\nWhile earlier we wanted the value of a tag attribute (i.e. part of the metadata), here we want the actual text (i.e. part of the actual content). To extract text from a snippet of HTML, we pass it to html_text2().\nLet’s run both operations at once to save the creation of an intermediate object:\n\ndate_test &lt;- html_test %&gt;%\n  html_element(\"#publication_date p\") %&gt;%\n  html_text2()\n\n\nNote the difference with what we did earlier to extract the URL: if we had used html_text2() then we would have gotten the text part of the link (\"The Novel Chlorination of Zirconium Metal and Its Application to a Recycling Protocol for Zircaloy Cladding from Spent Nuclear Fuel Rods\") rather than the URL (\"https://trace.tennessee.edu/utk_graddiss/7600\").\n\nLet’s verify that our date object indeed contains the date:\n\ndate_test\n\n[1] \"5-2023\"\n\n\nWe also want the major for this thesis. The SelectorGadget allows us to find that this time, it is the #department p element that we need. Let’s extract it in the same fashion:\n\nmajor_test &lt;- html_test %&gt;%\n  html_element(\"#department p\") %&gt;%\n  html_text2()\nmajor_test\n\n[1] \"Life Sciences\"\n\n\nAnd for the advisor, we need the #advisor1 p element:\n\nadvisor_test &lt;- html_test %&gt;%\n  html_element(\"#advisor1 p\") %&gt;%\n  html_text2()\nadvisor_test\n\n[1] \"Bode A. Olukolu\"\n\n\n\n\nYour turn:\n\nTry using the SelectorGadget to identify the element necessary to extract the abstract of this dissertation.\nNow, write the code to extract it and make sure you actually get what you want.\n\nWe now have the date, major, and advisor for the first dissertation. We can create a matrix by passing them as arguments to cbind():\n\nresult_test &lt;- cbind(date_test, major_test, advisor_test)\nresult_test\n\n     date_test major_test      advisor_test     \n[1,] \"5-2023\"  \"Life Sciences\" \"Bode A. Olukolu\"",
    "crumbs": [
      "R",
      "<b><em>Workshops</em></b>",
      "Web scraping with R"
    ]
  },
  {
    "objectID": "r/ws_webscraping.html#full-run",
    "href": "r/ws_webscraping.html#full-run",
    "title": "Web scraping with R",
    "section": "Full run",
    "text": "Full run\n\nExtract all URLs\nNow that we have tested our code on the first dissertation, we can apply it on all 100 dissertations of the first page of the database.\nInstead of using html_element(), this time we will use html_elements() which extracts all matching elements (instead of just the first one):\n\ndat &lt;- html %&gt;% html_elements(\".article-listing a\")\ndat\n\n{xml_nodeset (100)}\n [1] &lt;a href=\"https://trace.tennessee.edu/utk_graddiss/8076\"&gt;Understanding ho ...\n [2] &lt;a href=\"https://trace.tennessee.edu/utk_graddiss/9158\"&gt;Generating Diver ...\n [3] &lt;a href=\"https://trace.tennessee.edu/utk_graddiss/8080\"&gt;FABRICATION, MEA ...\n [4] &lt;a href=\"https://trace.tennessee.edu/utk_graddiss/8086\"&gt;Development and  ...\n [5] &lt;a href=\"https://trace.tennessee.edu/utk_graddiss/8078\"&gt;The Light from P ...\n [6] &lt;a href=\"https://trace.tennessee.edu/utk_graddiss/9185\"&gt;Image Deblurring ...\n [7] &lt;a href=\"https://trace.tennessee.edu/utk_graddiss/8584\"&gt;Clickable Lipid  ...\n [8] &lt;a href=\"https://trace.tennessee.edu/utk_graddiss/8703\"&gt;Retinoic Acid, I ...\n [9] &lt;a href=\"https://trace.tennessee.edu/utk_graddiss/8987\"&gt;Development and  ...\n[10] &lt;a href=\"https://trace.tennessee.edu/utk_graddiss/8734\"&gt;Defining Systemi ...\n[11] &lt;a href=\"https://trace.tennessee.edu/utk_graddiss/8073\"&gt;Investigating th ...\n[12] &lt;a href=\"https://trace.tennessee.edu/utk_graddiss/8088\"&gt;The Disparate Ef ...\n[13] &lt;a href=\"https://trace.tennessee.edu/utk_graddiss/9077\"&gt;Social Wellness  ...\n[14] &lt;a href=\"https://trace.tennessee.edu/utk_graddiss/8077\"&gt;A HIERARCHICAL P ...\n[15] &lt;a href=\"https://trace.tennessee.edu/utk_graddiss/8094\"&gt;Nurse Staffing a ...\n[16] &lt;a href=\"https://trace.tennessee.edu/utk_graddiss/8714\"&gt;Ruinous Natures: ...\n[17] &lt;a href=\"https://trace.tennessee.edu/utk_graddiss/9050\"&gt;Toward Accelerat ...\n[18] &lt;a href=\"https://trace.tennessee.edu/utk_graddiss/8737\"&gt;Implementing the ...\n[19] &lt;a href=\"https://trace.tennessee.edu/utk_graddiss/8074\"&gt;Riding the Wave: ...\n[20] &lt;a href=\"https://trace.tennessee.edu/utk_graddiss/9177\"&gt;A TWO-DIAMETER H ...\n...\n\n\n\ntypeof(dat)\n\n[1] \"list\"\n\nlength(dat)\n\n[1] 100\n\ntypeof(dat[[1]])\n\n[1] \"list\"\n\n\nWe now have a list of lists.\nAs we did for a single URL in the test run, we now want to extract all the URLs. We will do this using a loop.\nBefore running for loops, it is important to initialize empty loops. It is much more efficient than growing the result at each iteration.\nSo let’s initialize an empty list that we call list_urls of the appropriate size:\n\nlist_urls &lt;- vector(\"list\", length(dat))\n\nNow we can run a loop to fill in our list:\n\nfor (i in seq_along(dat)) {\n  list_urls[[i]] &lt;- dat[[i]] %&gt;% html_attr(\"href\")\n}\n\nLet’s print again the first element of list_urls to make sure all looks good:\n\nlist_urls[[1]]\n\n[1] \"https://trace.tennessee.edu/utk_graddiss/8076\"\n\n\nWe now have a list of URLs (in the form of character vectors) as we wanted.\n\n\nExtract data from each page\nWe will now extract the data (date, major, and advisor) for all URLs in our list.\nAgain, before running a for loop, we need to allocate memory first by creating an empty container (here a list):\n\nlist_data &lt;- vector(\"list\", length(list_urls))\n\nWe move the code we tested for a single URL inside a loop and we add one result to the list_data list at each iteration until we have all 100 dissertation sites scraped. Because there are quite a few of us running the code at the same time, we don’t want the site to block our request. To play safe, we will add a little delay (0.1 second) at each iteration (many sites will block requests if they are too frequent):\n\nfor (i in seq_along(list_urls)) {\n  html &lt;- read_html(list_urls[[i]])\n  date &lt;- html %&gt;%\n    html_element(\"#publication_date p\") %&gt;%\n    html_text2()\n  major &lt;- html %&gt;%\n    html_element(\"#department p\") %&gt;%\n    html_text2()\n  advisor &lt;- html %&gt;%\n    html_element(\"#advisor1 p\") %&gt;%\n    html_text2()\n  Sys.sleep(0.1)  # Add a little delay\n  list_data[[i]] &lt;- cbind(date, major, advisor)\n}\n\nLet’s make sure all looks good by printing the first element of list_data:\n\nlist_data[[1]]\n\n     date     major           advisor          \n[1,] \"5-2023\" \"Life Sciences\" \"Bode A. Olukolu\"",
    "crumbs": [
      "R",
      "<b><em>Workshops</em></b>",
      "Web scraping with R"
    ]
  },
  {
    "objectID": "r/ws_webscraping.html#store-results-in-dataframe",
    "href": "r/ws_webscraping.html#store-results-in-dataframe",
    "title": "Web scraping with R",
    "section": "Store results in DataFrame",
    "text": "Store results in DataFrame\nWe can turn this big list into a dataframe:\n\nresult &lt;- do.call(rbind.data.frame, list_data)\n\nresult is a long dataframe, so we will only print the first few elements:\n\nhead(result)\n\n     date                          major               advisor\n1  5-2023                  Life Sciences       Bode A. Olukolu\n2 12-2023         Industrial Engineering            Hugh Medal\n3  5-2023            Nuclear Engineering           Erik Lukosi\n4  5-2023 Energy Science and Engineering   Kyle R. Gluesenkamp\n5  5-2023                        English Margaret Lazarus Dean\n6 12-2023         Industrial Engineering          Hoon Hwangbo\n\n\nIf you like the tidyverse, you can turn it into a tibble:\n\nresult &lt;- result %&gt;% tibble::as_tibble()\n\n\nThe notation tibble::as_tibble() means that we are using the function as_tibble() from the package tibble. A tibble is the tidyverse version of a dataframe. One advantage is that it will only print the first 10 rows by default instead of printing the whole dataframe, so you don’t have to use head() when printing long dataframes:\n\nresult\n\n# A tibble: 100 × 3\n   date    major                          advisor              \n   &lt;chr&gt;   &lt;chr&gt;                          &lt;chr&gt;                \n 1 5-2023  Life Sciences                  Bode A. Olukolu      \n 2 12-2023 Industrial Engineering         Hugh Medal           \n 3 5-2023  Nuclear Engineering            Erik Lukosi          \n 4 5-2023  Energy Science and Engineering Kyle R. Gluesenkamp  \n 5 5-2023  English                        Margaret Lazarus Dean\n 6 12-2023 Industrial Engineering         Hoon Hwangbo         \n 7 8-2023  Chemistry                      Michael D. Best      \n 8 8-2023  Nutritional Sciences           Jiangang Chen        \n 9 12-2023 Mechanical Engineering         Dustin L. Crouch     \n10 8-2023  Counselor Education            Melinda M. Gibbons   \n# ℹ 90 more rows\n\n\n\nWe can capitalize the headers:\n\nnames(result) &lt;- c(\"Date\", \"Major\", \"Advisor\")\n\nThis is what our final result looks like:\n\nresult\n\n# A tibble: 100 × 3\n   Date    Major                          Advisor              \n   &lt;chr&gt;   &lt;chr&gt;                          &lt;chr&gt;                \n 1 5-2023  Life Sciences                  Bode A. Olukolu      \n 2 12-2023 Industrial Engineering         Hugh Medal           \n 3 5-2023  Nuclear Engineering            Erik Lukosi          \n 4 5-2023  Energy Science and Engineering Kyle R. Gluesenkamp  \n 5 5-2023  English                        Margaret Lazarus Dean\n 6 12-2023 Industrial Engineering         Hoon Hwangbo         \n 7 8-2023  Chemistry                      Michael D. Best      \n 8 8-2023  Nutritional Sciences           Jiangang Chen        \n 9 12-2023 Mechanical Engineering         Dustin L. Crouch     \n10 8-2023  Counselor Education            Melinda M. Gibbons   \n# ℹ 90 more rows",
    "crumbs": [
      "R",
      "<b><em>Workshops</em></b>",
      "Web scraping with R"
    ]
  },
  {
    "objectID": "r/ws_webscraping.html#save-results-to-file",
    "href": "r/ws_webscraping.html#save-results-to-file",
    "title": "Web scraping with R",
    "section": "Save results to file",
    "text": "Save results to file\nAs a final step, we will save our data to a CSV file:\nwrite.csv(result, \"dissertations_data.csv\", row.names = FALSE)",
    "crumbs": [
      "R",
      "<b><em>Workshops</em></b>",
      "Web scraping with R"
    ]
  },
  {
    "objectID": "r/ws_webscraping.html#functions-recap",
    "href": "r/ws_webscraping.html#functions-recap",
    "title": "Web scraping with R",
    "section": "Functions recap",
    "text": "Functions recap\nBelow is a recapitulation of the rvest functions we have used today:\n\n\n\nFunctions\nUsage\n\n\n\n\nread_html()\nRead in HTML from URL\n\n\nhtml_element()\nExtract first matching element\n\n\nhtml_elements()\nExtract all matching elements\n\n\nhtml_attr()\nExtract the value of an attribute\n\n\nhtml_text2()\nExtract text",
    "crumbs": [
      "R",
      "<b><em>Workshops</em></b>",
      "Web scraping with R"
    ]
  },
  {
    "objectID": "r/ws_webscraping.html#recording",
    "href": "r/ws_webscraping.html#recording",
    "title": "Web scraping with R",
    "section": "Recording",
    "text": "Recording\n\nVideo of this workshop for the Digital Research Alliance of Canada HSS Winter Series 2023:",
    "crumbs": [
      "R",
      "<b><em>Workshops</em></b>",
      "Web scraping with R"
    ]
  },
  {
    "objectID": "talks/2023_driconnect_slides.html#topics",
    "href": "talks/2023_driconnect_slides.html#topics",
    "title": "The instruments for advanced research computing are here, but are researchers ready?",
    "section": "Topics",
    "text": "Topics\n\nUnix shell\nHPC\nVersion control with Git/DataLad\nScientific programming in R/Python/Julia\nParallel computing in R/Julia/Chapel\nDeep learning with PyTorch\nScientific visualization\nContainers/Alliance clouds/VMs\nWebscraping in R/Python\nGIS in R\nScientific publishing with Quarto"
  },
  {
    "objectID": "talks/2023_driconnect_slides.html#fast",
    "href": "talks/2023_driconnect_slides.html#fast",
    "title": "The instruments for advanced research computing are here, but are researchers ready?",
    "section": "Fast",
    "text": "Fast\n\n\n\n\n\n\n\n Back to talk page"
  },
  {
    "objectID": "tools/index.html",
    "href": "tools/index.html",
    "title": "Tools",
    "section": "",
    "text": "Workshops\nVarious tools\n\n\n\n\n60 min webinars\nVarious tools",
    "crumbs": [
      "Tools",
      "<br>&nbsp;<em><b>Tools</b></em><br><br>"
    ]
  },
  {
    "objectID": "tools/wb_dvc.html",
    "href": "tools/wb_dvc.html",
    "title": "Version control for data science and machine learning with DVC",
    "section": "",
    "text": "As DVC is a popular tool in machine learning, please find this webinar in the AI section.",
    "crumbs": [
      "Tools",
      "<b><em>Webinars</em></b>",
      "Data version control"
    ]
  },
  {
    "objectID": "tools/wb_help_slides.html#when-you-are-stuck-1",
    "href": "tools/wb_help_slides.html#when-you-are-stuck-1",
    "title": "So, you are stuck … now what?",
    "section": "When you are stuck",
    "text": "When you are stuck\n\nFirst, look for information that is already out there\n\n\nThen, ask for help"
  },
  {
    "objectID": "tools/wb_help_slides.html#look-for-information",
    "href": "tools/wb_help_slides.html#look-for-information",
    "title": "So, you are stuck … now what?",
    "section": "Look for information",
    "text": "Look for information\n\nRead carefully any error message\nRead the documentation (local or online)\nMake sure you have up-to-date versions\nGoogle (using carefully selected keywords or the error message)\nLook for open issues & bug reports"
  },
  {
    "objectID": "tools/wb_help_slides.html#error-messages",
    "href": "tools/wb_help_slides.html#error-messages",
    "title": "So, you are stuck … now what?",
    "section": "Error messages",
    "text": "Error messages\nRead them!\nFamiliarise yourself with the error types in the languages you use\n\nExample: Python’s syntax errors vs exceptions\n\nWarnings ≠ errors\nLook for bits you understand (don’t get put off by what you don’t understand)\nIdentify the locations of the errors to go investigate that part of the code"
  },
  {
    "objectID": "tools/wb_help_slides.html#documentation",
    "href": "tools/wb_help_slides.html#documentation",
    "title": "So, you are stuck … now what?",
    "section": "Documentation",
    "text": "Documentation\n\nYou need to find it\n\n\nYou need to understand it"
  },
  {
    "objectID": "tools/wb_help_slides.html#finding-documentation",
    "href": "tools/wb_help_slides.html#finding-documentation",
    "title": "So, you are stuck … now what?",
    "section": "Finding documentation",
    "text": "Finding documentation\n\nOnline:\nTake the time to look for the official documentation & other high quality sources for the languages & tools you use.\n\n\n\nExamples:\nPython: Reference manual, Standard library manual, Tutorial\nNumPy: Tutorial\nR: Open source book “R for Data Science”, Open source book “Advanced R”\nJulia: Documentation\nBash: Manual\nGit: Manual, Open source book\n\n\n\nIn the program itself\n\n\nUnderstanding the documentation"
  },
  {
    "objectID": "tools/wb_help_slides.html#up-to-date-versions",
    "href": "tools/wb_help_slides.html#up-to-date-versions",
    "title": "So, you are stuck … now what?",
    "section": "Up-to-date versions",
    "text": "Up-to-date versions\n\nFirst, you need to know what needs to be updated.\n\n\nKeeping a system up to date includes updating:\n\nthe OS\nthe program\n(any potential IDE)\npackages\n\n\n\nThen, you need to update regularly."
  },
  {
    "objectID": "tools/wb_help_slides.html#google",
    "href": "tools/wb_help_slides.html#google",
    "title": "So, you are stuck … now what?",
    "section": "Google",
    "text": "Google\nGoogle’s algorithms are great at guessing what we are looking for.\n\nBut there is a frequency problem:\nSearches relating to programming-specific questions represent too small a fraction of the overall searches for results to be relevant unless you use key vocabulary.\n\n\nBe precise.\n\n\nLearn the vocabulary of your language/tool to know what to search for."
  },
  {
    "objectID": "tools/wb_help_slides.html#open-issues-bug-reports",
    "href": "tools/wb_help_slides.html#open-issues-bug-reports",
    "title": "So, you are stuck … now what?",
    "section": "Open issues & bug reports",
    "text": "Open issues & bug reports\nIf the tool you are using is open source, look for issues matching your problem in the source repository (e.g. on GitHub or GitLab)."
  },
  {
    "objectID": "tools/wb_help_slides.html#what-if-the-answer-isnt-out-there",
    "href": "tools/wb_help_slides.html#what-if-the-answer-isnt-out-there",
    "title": "So, you are stuck … now what?",
    "section": "What if the answer isn’t out there?",
    "text": "What if the answer isn’t out there?\nWhen everything has failed & you have to ask for help, you need to know:\n\n\nWhere to ask\n\n\n\n\nHow to ask"
  },
  {
    "objectID": "tools/wb_help_slides.html#where-to-ask-1",
    "href": "tools/wb_help_slides.html#where-to-ask-1",
    "title": "So, you are stuck … now what?",
    "section": "Where to ask",
    "text": "Where to ask\nQ&A sites\nMostly, Stack Overflow & the Stack Exchange network.\nCo-founded in 2008 & 2009 by Jeff Atwood & Joel Spolsky.\nForums\nMostly, Discourse.\nCo-founded in 2013 by Jeff Atwood, Robin Ward & Sam Saffron.\nA few other older forums."
  },
  {
    "objectID": "tools/wb_help_slides.html#where-to-ask-2",
    "href": "tools/wb_help_slides.html#where-to-ask-2",
    "title": "So, you are stuck … now what?",
    "section": "Where to ask",
    "text": "Where to ask\nWhich one to choose is a matter of personal preference.\nPossible considerations:\n\nSome niche topics have very active communities on Discourse\nStack Overflow & some older forums can be intimidating with higher expectations for the questions quality & a more direct handling of mistakes\nFor conversations, advice, or multiple step questions, go to Discourse\nStack Overflow has over 13 million users\nStack Overflow & co have a very efficient approach"
  },
  {
    "objectID": "tools/wb_help_slides.html#stack-overflow-co",
    "href": "tools/wb_help_slides.html#stack-overflow-co",
    "title": "So, you are stuck … now what?",
    "section": "Stack Overflow & co",
    "text": "Stack Overflow & co\nPick the best site to ask your question.\nA few of the Stack Exchange network sites:\nStack Overflow: programming\nSuper User: computer hardware & software\nUnix & Linux: *nix OS TEX: TeX/LaTeX\nCross Validated: stats; data mining, collecting, analysis & visualization; ML\nData Science: focus on implementation & processes\nOpen Data\nGIS"
  },
  {
    "objectID": "tools/wb_help_slides.html#how-to-ask-1",
    "href": "tools/wb_help_slides.html#how-to-ask-1",
    "title": "So, you are stuck … now what?",
    "section": "How to ask",
    "text": "How to ask\n\nFamiliarize yourself with the site by reading posts\n\n\nRead the “Tour” page (SO/SE) or take the “New user tutorial” (Discourse)\n\n\nMake sure the question has not already been asked\n\n\nFormat the question properly\n\n\nGive a minimum reproducible example\n\n\nDo not share sensitive data\n\n\nShow your attempts\n\n\nAvoid cross-posting. If you really have to, make sure to cross-reference"
  },
  {
    "objectID": "tools/wb_help_slides.html#how-to-ask-so-co",
    "href": "tools/wb_help_slides.html#how-to-ask-so-co",
    "title": "So, you are stuck … now what?",
    "section": "How to ask: SO & co",
    "text": "How to ask: SO & co\n\nDon’t ask opinion-based questions\n\n\nDon’t ask for package, tool, or service recommendations\n\n\nDon’t ask more than one question in a single post\n\n\nCheck your spelling, grammar, punctuation, capitalized sentences, etc.\n\n\nAvoid greetings, signatures, thank-yous; keep it to the point\n\n\nAvoid apologies about being a beginner, this being your first post, the question being stupid, etc: do the best you can & skip the personal, self-judgmental & irrelevant bits"
  },
  {
    "objectID": "tools/wb_help_slides.html#formatting-your-question",
    "href": "tools/wb_help_slides.html#formatting-your-question",
    "title": "So, you are stuck … now what?",
    "section": "Formatting your question",
    "text": "Formatting your question\nNowadays, most sites (including Stack Overflow & Discourse) allow markdown rendering.\nSome older forums implement other markup languages (e.g. BBCode).\nThe information is always easy to find. Spend the time to format your question properly. People will be much less inclined to help you if you don’t show any effort & if your question is a nightmare to read."
  },
  {
    "objectID": "tools/wb_help_slides.html#example-of-a-typical-downvoted-question",
    "href": "tools/wb_help_slides.html#example-of-a-typical-downvoted-question",
    "title": "So, you are stuck … now what?",
    "section": "Example of a typical downvoted question",
    "text": "Example of a typical downvoted question\nCode:\nhowdy!!\ni am new to R sorry for a very silly question.i looked all oever the itnernwet, but i dint find\nanyanswer. i tried to use ggplot i get the error: Error in loadNamespace(i, c(lib.loc, .libPaths()),\nversionCheck = vI[[i]]) : there is no package called 'stringi'\nthank youu very much!!!!!\nmarie\nRendered output:"
  },
  {
    "objectID": "tools/wb_help_slides.html#same-question-fixed",
    "href": "tools/wb_help_slides.html#same-question-fixed",
    "title": "So, you are stuck … now what?",
    "section": "Same question, fixed",
    "text": "Same question, fixed\nWhen I try to load the package `ggplot2` with:\n\n```{r}\nlibrary(ggplot2)\n```\nI get the error:\n\n&gt; Error in loadNamespace(i, c(lib.loc, .libPaths()), versionCheck = vI[[i]]) :\nthere is no package called 'stringi'\n\nWhat am I doing wrong?"
  },
  {
    "objectID": "tools/wb_help_slides.html#still-not-good-enough",
    "href": "tools/wb_help_slides.html#still-not-good-enough",
    "title": "So, you are stuck … now what?",
    "section": "Still not good enough",
    "text": "Still not good enough\nThis question is actually a duplicate of a question asked which is itself a duplicate of another question."
  },
  {
    "objectID": "tools/wb_help_slides.html#creating-a-minimal-reproducible-example",
    "href": "tools/wb_help_slides.html#creating-a-minimal-reproducible-example",
    "title": "So, you are stuck … now what?",
    "section": "Creating a minimal reproducible example",
    "text": "Creating a minimal reproducible example\nThere are great posts on how to create a good minimal reproducible example. In particular:\nHow to create a Minimal, Reproducible Example\nFor R (but concepts apply to any language):\nHow to make a great R reproducible example\nWhat’s a reproducible example (reprex) and how do I do one?"
  },
  {
    "objectID": "tools/wb_help_slides.html#creating-a-minimal-reproducible-example-1",
    "href": "tools/wb_help_slides.html#creating-a-minimal-reproducible-example-1",
    "title": "So, you are stuck … now what?",
    "section": "Creating a minimal reproducible example",
    "text": "Creating a minimal reproducible example\n\nLoad all necessary packages\nLoad or create necessary data\nSimplify the data & the code as much as possible while still reproducing the problem\nUse simple variable names"
  },
  {
    "objectID": "tools/wb_help_slides.html#data-for-your-example-your-own-data",
    "href": "tools/wb_help_slides.html#data-for-your-example-your-own-data",
    "title": "So, you are stuck … now what?",
    "section": "Data for your example: your own data",
    "text": "Data for your example: your own data\nDo not upload data somewhere on the web to be downloaded.\nMake sure that the data is anonymised.\nDon’t keep more variables & more data points than are necessary to reproduce the problem.\nSimplify the variable names.\nIn R, you can use functions such as dput() to turn your reduced, anonymised data into text that is easy to copy/paste & can then be used to recreate the data."
  },
  {
    "objectID": "tools/wb_help_slides.html#data-for-your-example-create-a-toy-dataset",
    "href": "tools/wb_help_slides.html#data-for-your-example-create-a-toy-dataset",
    "title": "So, you are stuck … now what?",
    "section": "Data for your example: create a toy dataset",
    "text": "Data for your example: create a toy dataset\nYou can also create a toy dataset.\nFunctions that create random data, series, or repetitions are very useful here."
  },
  {
    "objectID": "tools/wb_help_slides.html#data-for-your-example-pre-packaged-datasets",
    "href": "tools/wb_help_slides.html#data-for-your-example-pre-packaged-datasets",
    "title": "So, you are stuck … now what?",
    "section": "Data for your example: pre-packaged datasets",
    "text": "Data for your example: pre-packaged datasets\nSome languages/packages come with pre-packaged datasets. If your code involves such languages/packages, you can make use of these datasets to create your reproducible example.\nFor example, R comes with many datasets directly available, including iris, mtcars, trees, airquality. In the R console, try:\n?iris\n?mtcars"
  },
  {
    "objectID": "tools/wb_help_slides.html#additional-considerations",
    "href": "tools/wb_help_slides.html#additional-considerations",
    "title": "So, you are stuck … now what?",
    "section": "Additional considerations",
    "text": "Additional considerations\nEven if you always find answers to your questions without having to post yourself, consider signing up to these sites:\n\nIt allows you to upvote (SO/SE) or like (Discourse) the questions & answers that help you—and why not thank in this fashion those that are making your life easier?\nIt makes you a part of these communities.\nOnce you are signed up, maybe you will start being more involved & contribute with questions & answers of your own."
  },
  {
    "objectID": "tools/wb_help_slides.html#a-last-word",
    "href": "tools/wb_help_slides.html#a-last-word",
    "title": "So, you are stuck … now what?",
    "section": "A last word",
    "text": "A last word\nWhile it takes some work to ask a good question, do not let this discourage you from posting on Stack Overflow: if you ask a good question, you will get many great answers.\nYou will learn in the process of developing your question (you may actually find the answer in that process) & you will learn from the answers.\nIt is forth the effort.\nHere is the Stack Overflow documentation on how to ask a good question.\n\n\n\n\n Back to webinar page"
  },
  {
    "objectID": "tools/wb_quarto_slides.html#markup-languages",
    "href": "tools/wb_quarto_slides.html#markup-languages",
    "title": "The new R Markdown:",
    "section": "Markup languages",
    "text": "Markup languages\n\nControl the formatting of text documents\nPowerful but the unrendered text is visually cluttered and hard to read"
  },
  {
    "objectID": "tools/wb_quarto_slides.html#markup-languages-1",
    "href": "tools/wb_quarto_slides.html#markup-languages-1",
    "title": "The new R Markdown:",
    "section": "Markup languages",
    "text": "Markup languages\n\nControl the formatting of text documents\nPowerful but the unrendered text is visually cluttered and hard to read\n\n\nExample: Tex—often with macro package LaTeX—to create pdfs\n\n\\documentclass{article}\n\\title{My title}\n\\author{My name}\n\\usepackage{datetime}\n\\newdate{date}{24}{11}{2022}\n\\date{\\displaydate{date}}\n\\begin{document}\n \\maketitle\n \\section{First section}\n Some text in the first section.\n\\end{document}"
  },
  {
    "objectID": "tools/wb_quarto_slides.html#markup-languages-2",
    "href": "tools/wb_quarto_slides.html#markup-languages-2",
    "title": "The new R Markdown:",
    "section": "Markup languages",
    "text": "Markup languages\n\nControl the formatting of text documents\nPowerful but the unrendered text is visually cluttered and hard to read\n\n\nExample: HTML—often with css/scss files—to create webpages\n\n&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en-US\"&gt;\n  &lt;head&gt;\n    &lt;meta charset=\"utf-8\" /&gt;\n    &lt;meta name=\"viewport\" content=\"width=device-width\" /&gt;\n    &lt;title&gt;My title&lt;/title&gt;\n    &lt;address class=\"author\"&gt;My name&lt;/address&gt;\n    &lt;input type=\"date\" value=\"2022-11-24\" /&gt;\n  &lt;/head&gt;\n  &lt;h1&gt;First section&lt;/h1&gt;\n  &lt;body&gt;\n    Some text in the first section.\n  &lt;/body&gt;\n&lt;/html&gt;"
  },
  {
    "objectID": "tools/wb_quarto_slides.html#markdown",
    "href": "tools/wb_quarto_slides.html#markdown",
    "title": "The new R Markdown:",
    "section": "Markdown",
    "text": "Markdown\n\nRemoves the visual clutter and makes texts readable prior to rendering\nCreated in 2004\nBy now quasi-ubiquitous\nInitially created for webpages\nRaw HTML can be inserted when easy syntax falls short\n\n\nPandoc’s extended Markdown\nPandoc (free and open-source markup formats converter) supports an extended Markdown syntax with functionality for figures, tables, callout blocks, LaTeX equations, citations…\nRemains as readable as basic Markdown, but can be rendered in any format (pdf, books, entire websites, Word documents…)"
  },
  {
    "objectID": "tools/wb_quarto_slides.html#markdown-1",
    "href": "tools/wb_quarto_slides.html#markdown-1",
    "title": "The new R Markdown:",
    "section": "Markdown",
    "text": "Markdown\n\nRemoves the visual clutter and makes texts readable prior to rendering\nCreated in 2004\nBy now quasi-ubiquitous\nInitially created for webpages\nRaw HTML can be inserted when easy syntax falls short\n\n\nPrevious example using Pandoc’s Markdown:\n\n---\ntitle: My title\nauthor: My name\ndate: 2022-11-24\n---\n# First section\nSome text in the first section."
  },
  {
    "objectID": "tools/wb_quarto_slides.html#how-it-works",
    "href": "tools/wb_quarto_slides.html#how-it-works",
    "title": "The new R Markdown:",
    "section": "How it works",
    "text": "How it works\nCode blocks are executed by Jupyter (Python or Julia) or knitr (R), then pandoc renders the document into any format\n\nJulia/Python:\n From Quarto documentation\nR:\n From Quarto documentation"
  },
  {
    "objectID": "tools/wb_quarto_slides.html#how-it-works-1",
    "href": "tools/wb_quarto_slides.html#how-it-works-1",
    "title": "The new R Markdown:",
    "section": "How it works",
    "text": "How it works\nCode blocks are executed by Jupyter (Python or Julia) or knitr (R), then pandoc renders the document into any format\nCan be used from .qmd text files or directly from RStudio or Jupyter notebooks."
  },
  {
    "objectID": "tools/wb_quarto_slides.html#supported-languages",
    "href": "tools/wb_quarto_slides.html#supported-languages",
    "title": "The new R Markdown:",
    "section": "Supported languages",
    "text": "Supported languages\nSyntax highlighting in pretty much any language\n\nExecutable code blocks in Python, R, Julia, Observable JS\n\n\nOutput formats\n- HTML\n- PDF\n- MS Word\n- OpenOffice\n- ePub\n- Revealjs\n- PowerPoint\n- Beamer\n- GitHub Markdown\n- CommonMark\n- Hugo\n- Docusaurus\n- Markua\n- MediaWiki\n- DokuWiki\n- ZimWiki\n- Jira Wiki\n- XWiki\n- JATS\n- Jupyter\n- ConTeXt\n- RTF\n- reST\n- AsciiDoc\n- Org-Mode\n- Muse\n- GNU\n- Groff"
  },
  {
    "objectID": "tools/wb_quarto_slides.html#document-structure-syntax-front-matter",
    "href": "tools/wb_quarto_slides.html#document-structure-syntax-front-matter",
    "title": "The new R Markdown:",
    "section": "Document structure & syntax: front matter",
    "text": "Document structure & syntax: front matter\nWritten in YAML\nSets the options for the document. Let’s see a few examples.\n\n\nCan be very basic:\n\n---\ntitle: \"My title\"\nauthor: \"My name\"\nformat: html\n---"
  },
  {
    "objectID": "tools/wb_quarto_slides.html#document-structure-syntax-front-matter-1",
    "href": "tools/wb_quarto_slides.html#document-structure-syntax-front-matter-1",
    "title": "The new R Markdown:",
    "section": "Document structure & syntax: front matter",
    "text": "Document structure & syntax: front matter\nWritten in YAML\nSets the options for the document. Let’s see a few examples.\n\nOr more sophisticated:\n\n---\ntitle: \"Some title\"\nsubtitle: \"Some subtitle\"\ninstitute: \"Simon Fraser University\"\ndate: \"2022-11-24\"\nexecute:\n  error: true\n  echo: true\nformat:\n  revealjs:\n    theme: [default, custom.scss]\n    highlight-style: monokai\n    code-line-numbers: false\n    embed-resources: true\n---"
  },
  {
    "objectID": "tools/wb_quarto_slides.html#document-structure-syntax-text",
    "href": "tools/wb_quarto_slides.html#document-structure-syntax-text",
    "title": "The new R Markdown:",
    "section": "Document structure & syntax: text",
    "text": "Document structure & syntax: text\nWritten in Pandoc’s extended Markdown"
  },
  {
    "objectID": "tools/wb_quarto_slides.html#document-structure-syntax-code-blocks",
    "href": "tools/wb_quarto_slides.html#document-structure-syntax-code-blocks",
    "title": "The new R Markdown:",
    "section": "Document structure & syntax: code blocks",
    "text": "Document structure & syntax: code blocks\nSyntax highlighting only:\n{.language} code\n\nSyntax highlighting and code execution:\n```{language}\ncode\n```\n\n\nOptions can be added to individual blocks:\n```{language}\n#| option: value\n\ncode\n```"
  },
  {
    "objectID": "tools/wb_quarto_slides.html#rendering",
    "href": "tools/wb_quarto_slides.html#rendering",
    "title": "The new R Markdown:",
    "section": "Rendering",
    "text": "Rendering\nTwo commands:\nquarto render file.qmd     # Renders the document\nquarto preview file.qmd    # Displays a live preview"
  },
  {
    "objectID": "tools/wb_quarto_slides.html#some-advantages-of-quarto",
    "href": "tools/wb_quarto_slides.html#some-advantages-of-quarto",
    "title": "The new R Markdown:",
    "section": "Some advantages of Quarto",
    "text": "Some advantages of Quarto\nGeneral considerations\n\nExtremely well documented\nSolid team behind the work\nFree and open source\nUses only well established and well tested tools"
  },
  {
    "objectID": "tools/wb_quarto_slides.html#some-advantages-of-quarto-1",
    "href": "tools/wb_quarto_slides.html#some-advantages-of-quarto-1",
    "title": "The new R Markdown:",
    "section": "Some advantages of Quarto",
    "text": "Some advantages of Quarto\nWebpages/websites\n\nFast, easy, and clean\nSites work on screens of any size out of the box (uses Bootstrap 5)\nCan be customized with CSS/SCSS, but good out of the box\nCode blocks can have a copy button\nGreat search functionality\nSite/pages can be hosted anywhere easily"
  },
  {
    "objectID": "tools/wb_quarto_slides.html#some-advantages-of-quarto-2",
    "href": "tools/wb_quarto_slides.html#some-advantages-of-quarto-2",
    "title": "The new R Markdown:",
    "section": "Some advantages of Quarto",
    "text": "Some advantages of Quarto\nAdvantages of code execution\n\nPeople can see code outputs without running code\nForces to test every bit of code\nNo need for a complex system linking code scripts with publishing documents"
  },
  {
    "objectID": "tools/wb_quarto_slides.html#resources",
    "href": "tools/wb_quarto_slides.html#resources",
    "title": "The new R Markdown:",
    "section": "Resources",
    "text": "Resources\nOfficial sites\nWebsite\nRepo\nDocumentation index\nInstallation\nYou can find information in the Quarto documentation or in our previous workshop on Quarto\nBasic examples\nYou can find several examples in our previous workshop on Quarto"
  },
  {
    "objectID": "tools/wb_tools2.html",
    "href": "tools/wb_tools2.html",
    "title": "A few more of our favourite tools",
    "section": "",
    "text": "Please find this webinar in the Bash section.",
    "crumbs": [
      "Tools",
      "<b><em>Webinars</em></b>",
      "More command line tools"
    ]
  },
  {
    "objectID": "ai/jx/fl_optimization.html#using-multiple-accelerators",
    "href": "ai/jx/fl_optimization.html#using-multiple-accelerators",
    "title": "Optimizations",
    "section": "Using multiple accelerators",
    "text": "Using multiple accelerators\nParallel runs on multiple GPUs/TPUs"
  }
]