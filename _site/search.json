[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About this site",
    "section": "",
    "text": "This site, built with Quarto, contains Marie-H√©l√®ne Burle‚Äôs latest content.\nHer older training material can be found on the archived sites:\n\nShell, Git, R, Python, etc.\nJulia training material\nMachine learning training material\n\n\nMain WestDRI training website\nThis is the mint (‚Äúmint is not training‚Äù) WestDRI website.\nTo view all WestDRI training material, please visit the training WestDRI website.\n\n\nOther websites\nIn addition, here are a few of our websites for various training events:\n\nAutumn School 2022\nTraining Modules 2022\nTraining Modules 2021\nSummer School 2020\nCoding Fundamentals for Humanists 20221\nCoding Fundamentals for Humanists 2021\nHSS Winter Series 2023\nHSS Winter Series 2022\n\n\n\n1¬†For the Digital Humanities Summer Institute"
  },
  {
    "objectID": "bash/intro_scripting.html#background",
    "href": "bash/intro_scripting.html#background",
    "title": "Automation & scripting in bash for beginners",
    "section": "Background",
    "text": "Background\n\nWhat are Unix shells?\nA Unix shell is a command line interpreter: the user enters commands as text, either interactively in the command line or in a script, and the shell passes them to the operating system.\n\nBash\nBash (Bourne Again SHell), released in 1989, is part of the GNU Project and is the default Unix shell on many systems (MacOS recently changed its default to zsh).\n\n\nOther shells\nPrior to Bash, the default was the Bourne shell (sh).\nA new and popular shell (backward compatible with Bash) is zsh. It extends Bash‚Äôs capabilities.\nAnother shell in the same family is the KornShell (ksh).\nAll these shells are quite similar. The C shell (csh) however was modeled on the C programming language.\nBash is the most common shell and the one which makes the most sense to learn as a first Unix shell.\n\n\n\nWhy use a shell?\nWhile automating GUI operations is really difficult, it is easy to rerun a script (a file with a number of commands). Unix shells thus allow the creation of reproducible workflows and the automation of repetitive tasks.\nThey are powerful to launch tools, modify files, search text, or combine commands.\nThey also allow to work on remote machines and HPC systems."
  },
  {
    "objectID": "bash/intro_scripting.html#how-we-will-use-bash-today",
    "href": "bash/intro_scripting.html#how-we-will-use-bash-today",
    "title": "Automation & scripting in bash for beginners",
    "section": "How we will use Bash today",
    "text": "How we will use Bash today\nBash is a Unix shell. You thus need a Unix or Unix-like operating system.\nWe will connect to a remote HPC system via SSH (secure shell). HPC systems always run Linux.\nThose on Linux or MacOS can alternatively use Bash directly on their machine. On MacOS, the default is now zsh (you can see that by typing echo $SHELL in Terminal), but zsh is fully compatible with Bash commands, so it is totally fine to use it instead. If you really want to use Bash, simply launch it by typing in Terminal: bash.\n\nConnecting to a remote HPC system via SSH\n\nUsernames and password\nWe will give you a link to an etherpad during the workshop. Add your name next to a free username to claim it.\nWe will also give you the password for our training cluster. When prompted, enter it.\n\nNote that you will not see any character as you type the password: this is called blind typing and is a Linux safety feature. Type slowly and make sure not to make typos. It can be unsettling at first not to get any feed-back while typing.\n\n\n\nLinux and MacOS users\nLinux users: open the terminal emulator of your choice.\nMacOS users: open ‚ÄúTerminal‚Äù.\nThen type:\nssh userxx@bashworkshop.c3.ca  # Replace userxx by your username (e.g. user09)\n\n\nWindows users\nWe suggest using the free version of MobaXterm.\nMobaXterm comes with a terminal emulator and a GUI interface for SSH sessions.\nOpen MobaXterm, click on ‚ÄúSession‚Äù, then ‚ÄúSSH‚Äù, and fill in the Remote host name and your username. Here is a live demo."
  },
  {
    "objectID": "bash/intro_scripting.html#bash-the-basics",
    "href": "bash/intro_scripting.html#bash-the-basics",
    "title": "Automation & scripting in bash for beginners",
    "section": "Bash: the basics",
    "text": "Bash: the basics\n\nThe prompt\nIn command-line interfaces, a command prompt is a sequence of characters indicating that the interpreter is ready to accept input. It can also provide some information (e.g.¬†time, error types, username and hostname, etc.)\nThe Bash prompt is customizable. By default, it often gives the username and the hostname, and it typically ends with $.\n\n\nHelp on commands\nMan pages:\nman <command>\n\nMan pages open in a pager (usually less).\nNavigate up/down with the space bar and the b key.\nQuit the pager with the q key.\n\nHelp pages:\n<command> --help\nInspect commands:\ncommand -V <command>\n\n\nExamples of commands\n\nPrint working directory: pwd\nChange directory: cd\nPrint: echo\nPrint content of a file: cat\nList: ls\nCopy: cp\nMove or rename: mv\nCreate a new directory: mkdir\nCreate a new file: touch\n\n\n\nKeybindings\nClear the terminal (command clear) with C-l (this means: press the Ctrl and L keys at the same time).\nNavigate command history with C-p and C-n (or up and down arrows).\nYou can auto-complete commands by pressing the tab key."
  },
  {
    "objectID": "bash/intro_scripting.html#bash-scripting-the-basics",
    "href": "bash/intro_scripting.html#bash-scripting-the-basics",
    "title": "Automation & scripting in bash for beginners",
    "section": "Bash scripting: the basics",
    "text": "Bash scripting: the basics\nInstead of typing commands one at a time directly in a terminal, you can write them down, one per line, in a text file called a script.\nThey will be run in the order in which they are written when you execute the script.\nThis is a great way to automate tasks: to rerun this sequence of commands, you simply have to rerun the script.\n\nFile name\nShell scripts, including Bash scripts, are usually given the extension sh (e.g.¬†my_script.sh).\nYou can store scripts anywhere, but a common practice is to store them in a ~/bin directory.\n\n\nSyntax\n\nShebang\nScripts can be written for any interpreter (e.g.¬†Bash, Python, R, etc.) The way to tell the system which one to use is to use a shebang (#!) followed by the path of the interpreter on the first line of the script.\nTo use Bash, start your scripts with:\n#!/bin/bash\nYou may also encounter this notation:\n#!/usr/bin/env bash\nIf you are curious, you can read the answers to this Stack Overflow question for the differences between the two.\n\n\nComments\nAnything to the left of # is ignored by the interpreter and is for human consumption only.\n# You can write full-line comments\n\npwd       # You can also write comments after commands\n\n\n\nExecuting scripts\nThere are two ways to execute a script:\nbash my_script.sh\n./my_script.sh  # The dot represents the current directory\nIn the latter case, you need to make sure that your script is executable by first running:\nchmod u+x my_script.sh  # This makes the script executable by the user (i.e. you)\n\n\nOur first script\nOpen a text editor (e.g.¬†nano) and type:\n#!/bin/bash\n\necho \"This is our first script.\"\nSave and close the file.\n\n\nYour turn:\n\nNow run the script with one, then the other method.\nWhat does this script do?"
  },
  {
    "objectID": "bash/intro_scripting.html#variables",
    "href": "bash/intro_scripting.html#variables",
    "title": "Automation & scripting in bash for beginners",
    "section": "Variables",
    "text": "Variables\n\nDeclaring variables\nYou can declare a variable (i.e.¬†a name that holds a value) with the = sign.\n!! Make sure not to put spaces around the equal sign.\nvariable=Test\n\n\nQuotes\nLet‚Äôs experiment with quotes:\n\nvariable=This string is the value of the variable\necho $variable\n\nbash: line 1: string: command not found\n\n\nOops‚Ä¶\n\nvariable=\"This string is the value of the variable\"\necho $variable\n\nThis string is the value of the variable\n\n\n\nvariable='This string is the value of the variable'\necho $variable\n\nThis string is the value of the variable\n\n\n\nvariable='This string's the value of the variable'\necho $variable\n\nbash: -c: line 1: unexpected EOF while looking for matching `''\nbash: -c: line 3: syntax error: unexpected end of file\n\n\nOops‚Ä¶\nOne solution to this is to use double quotes:\n\nvariable=\"This string's the value of the variable\"\necho $variable\n\nThis string's the value of the variable\n\n\nAlternatively, single quotes can be escaped:\n\nvariable='This string'\"'\"'s the value of the variable'\necho $variable\n\nThis string's the value of the variable\n\n\n\nAdmittedly, this last one is a little crazy. It is the way to escape single quotes in single-quoted strings.\nThe first ' ends the first string, both \" create a double-quoted string with ' (escaped) in it, then the last ' starts the second string.\nEscaping double quotes is a lot easier and simply requires \\\".\n\n\n\nExpanding a variable‚Äôs value\nTo expand a variable (to access its value), you need to prepend its name with $:\n\nvariable=Test\necho variable\n\nvariable\n\n\nMmmm‚Ä¶ not really want we want!\n\nvariable=Test\necho $variable\n\nTest\n\n\n\nvariable=Test; echo \"$variable\"\n\nTest\n\n\n!! Single quotes don‚Äôt expand variables.\n\nvariable=Test; echo '$variable'\n\n$variable\n\n\n\n\nPassing variables to a Bash script\nCreate a script called name.sh with the following content:\n#!/bin/bash\n\necho \"My name is $1.\"  # $1 refers to the first variable passed to the script\nYou can now pass a variable to this script with:\nbash name.sh Marie\nMy name is Marie.\nYou can pass several variables to a script. Copy name.sh to name2.sh and edit name2.sh to look like the following:\n#!/bin/bash\n\necho \"My name is $1 and I am $2 years old.\"\nbash name2.sh Marie 43\nMy name is Marie and I am 43 years old.\nYou can also pass any number of variables to a script:\n#!/bin/bash\n\necho $@\nbash script.sh argument1 argument2 argument3 argument4\nargument1 argument2 argument3 argument4\n\n\nBrace expansion\n\necho {1..5}\n\n1 2 3 4 5\n\n\n\necho {01..10}\n\n01 02 03 04 05 06 07 08 09 10\n\n\n\necho {1..5}.txt\n\n1.txt 2.txt 3.txt 4.txt 5.txt\n\n\n\necho {r..v}\n\nr s t u v\n\n\n\necho {file1,file2}.sh\n\nfile1.sh file2.sh\n\n\n!! Make sure not to add a space after the comma.\ntouch {file1,file2}.sh\ntouch file{3..6}.sh\n\necho {list,of,strings}\n\nlist of strings\n\n\n\n\nWildcards\nWildcards are really powerful to apply a command to all the elements having a common pattern.\nFor instance, we can delete all the files we created earlier (file1.sh, file2.sh, etc.) with a single command:\nrm file*.sh\n!! Be very careful that rm is irreversible. Deleted files do not go to the trash: they are gone."
  },
  {
    "objectID": "bash/intro_scripting.html#loops",
    "href": "bash/intro_scripting.html#loops",
    "title": "Automation & scripting in bash for beginners",
    "section": "Loops",
    "text": "Loops\nTo apply a set of commands to all the elements of a list, you can use for loops. The general structure is as follows:\nfor <iterable> in <list>\ndo\n    <statement1>\n    <statement2>\n    ...\ndone\nLet‚Äôs create the script names.sh:\n#!/bin/bash\n\nfor name in $@\ndo\n    echo $name\ndone\nNow let‚Äôs run it with a list of arguments:\nbash names.sh Patrick Paul Marie Alex\nPatrick\nPaul\nMarie\nAlex\n\n\nYour turn:\n\nCompare the outputs of the following 2 scripts:\n\nscript1.sh:\n\n#!/bin/bash\n\necho $@\n\nscript2.sh:\n\n#!/bin/bash\n\nfor i in $@\ndo\n    echo $i\ndone\nHow do you explain the difference between running:\nbash script1.sh arg1 arg2 arg3\nand running:\nbash script2.sh arg1 arg2 arg3"
  },
  {
    "objectID": "bash/intro_scripting.html#lets-put-it-all-together-to-automate-some-task",
    "href": "bash/intro_scripting.html#lets-put-it-all-together-to-automate-some-task",
    "title": "Automation & scripting in bash for beginners",
    "section": "Let‚Äôs put it all together to automate some task",
    "text": "Let‚Äôs put it all together to automate some task\nThis is a rather silly example, but bear with me and let‚Äôs imagine that it actually makes sense (of course, you don‚Äôt write that many thesis chapters so you would probably never automate these tasks‚Ä¶)\nSo‚Ä¶ let‚Äôs imagine that each time you write a thesis chapter, you do the same things:\n\nyou create a directory with the name of the chapter,\nyou create a number of subdirectories (for your source code, your manuscript, your data, and your results),\nyou create a Python script in the source code directory,\nyou create a markdown document in your manuscript directory,\nyou put the whole thing under version control with Git,\nyou create a .gitignore file in which you put the data subdirectory.\n\n\n\nYour turn:\n\nWrite a script that would do all this, then test the script.\nGive it a try on your own before looking at the solution below‚Ä¶\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nHere is what the script looks like (let‚Äôs call it chapter.sh):\n#!/bin/bash\n\nmkdir $1\ncd $1\nmkdir src data results ms\ntouch src/$1.py ms/$1.md\ngit init\necho data/ > .gitignore\nYou then run the script:\nbash chapter.sh chapter1\nYou can verify that all the files and directories got created with:\ntree chapter1\nchapter1/\n‚îú‚îÄ‚îÄ data\n‚îú‚îÄ‚îÄ ms\n‚îÇ¬†¬† ‚îî‚îÄ‚îÄ chapter1.md\n‚îú‚îÄ‚îÄ results\n‚îî‚îÄ‚îÄ src\n    ‚îî‚îÄ‚îÄ chapter1.py\nand:\nls -aF chapter1\n./  ../  data/  .git/  .gitignore  ms/  results/  src/\nYou can also verify the content of your .gitignore file with:\ncat chapter1/.gitignore\ndata/"
  },
  {
    "objectID": "bash/intro_scripting.html#resources",
    "href": "bash/intro_scripting.html#resources",
    "title": "Automation & scripting in bash for beginners",
    "section": "Resources",
    "text": "Resources\nOne very useful (although very dense) resource is the Bash manual.\nYou can also get information on Bash from within Bash with:\ninfo bash\nand:\nman bash\nThere are also countless resources online and don‚Äôt forget to Google anything you don‚Äôt know how to do: you will almost certainly find the answer on StackOverflow or some Stack Exchange site."
  },
  {
    "objectID": "bash/tools1.html",
    "href": "bash/tools1.html",
    "title": "Fun tools to simplify your life in the command line",
    "section": "",
    "text": "Working in the command-line has many advantages and it is often necessary, but can it be fun?\nIn this webinar, aimed at any command-line user, I intend to demonstrate that yes, it can! by introducing three free and open source utilities which make navigating your system and your outputs a lot easier:\n\nfzf is a simple, yet extremely powerful interactive fuzzy finder allowing for incremental completion and narrowing selection of any command line output. I will show you how to build simple shell functions which harvest its power to instantly refresh your memory on your custom keybindings or aliases, navigate your command history, find and kill processes, and explore and checkout your git commits. After this, you will be able to use fzf for any number of other applications in your work in the command-line.\nautojump lets you jump anywhere you want in your directories in just a few keystrokes (no more of this painful navigation writing down long paths).\nWith the ranger file manager, you can browse (with preview!), open, copy, move, delete, etc. your files and directories in a friendly way from the command line. Added bonus: you can use fzf and autojump within ranger!\n\nWarning: too much fun in the command-line can lead to addiction and geek behaviours. Use in moderation."
  },
  {
    "objectID": "bash/tools2.html",
    "href": "bash/tools2.html",
    "title": "A few of our favourite tools",
    "section": "",
    "text": "In a previous webinar, we presented three of our favourite command line tools. Today, we will introduce other tools we find really useful in our daily workflow:\n\nlazygit: a wonderful terminal UI for Git,\nbat: a great syntax highlighter,\nripgrep: a fast alternative to grep,\nfd: a /really/ fast alternative to find,\npass: a command line password manager.\n\nAlong the way, I will use a few other neat command line tools such as hyperfine‚Äîfor sophisticated benchmarking‚Äîand diff-so-fancy‚Äîwhich makes your diffs a lot more readable.\nFor the Emacs users among you, we will finish the workshop with two Emacs utilities:\n\nTRAMP: a remote file access system,\nHelm: a ‚Äúframework for incremental completions and narrowing selections‚Äù."
  },
  {
    "objectID": "calendar.html",
    "href": "calendar.html",
    "title": "Upcoming training events",
    "section": "",
    "text": "Our training events also get posted in our main site."
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact",
    "section": "",
    "text": "Please email us at: training at westdri dot ca."
  },
  {
    "objectID": "git/branches.html",
    "href": "git/branches.html",
    "title": "Branches",
    "section": "",
    "text": "One of the reasons Git has become so popular is its branching system: unlike in other version control tools in which creating branches is a lengthy and expensive process involving heavy copies, a branch in Git is just a lightweight pointer to a commit. This makes creating branches extremely quick and cheap."
  },
  {
    "objectID": "git/branches.html#what-is-a-branch",
    "href": "git/branches.html#what-is-a-branch",
    "title": "Branches",
    "section": "What is a branch?",
    "text": "What is a branch?\nA branch is a pointer to a commit (under the hood, it is a small file containing the 40 character hash checksum of the commit it points to).\nWhen you run git status and get ‚ÄúOn branch main‚Äù in the output, or when you run git log and see ‚Äú(HEAD -> main)‚Äù in the log, it means that the HEAD pointer (your position in the Git history) points to the branch main (which itself points to a commit).\n\nI know that is a lot of pointers ‚Ä¶ but this is really what makes Git so nimble, powerful, and fantastic. Because these pointers are very cheap (tiny files) and so useful."
  },
  {
    "objectID": "git/branches.html#why-use-multiple-branches",
    "href": "git/branches.html#why-use-multiple-branches",
    "title": "Branches",
    "section": "Why use multiple branches?",
    "text": "Why use multiple branches?\nBranches are useful in so many situations:\n\nIf your changes break code, you still have a fully functional branch to go back to if needed.\nIf you develop a tool being used, this allows you to experiment with new features until they are ready without messing up with the working project.\nYou can create a branch for each alternative approach. This allows you to jump back and forth between various alternatives.\nYou can work on different aspects of the project on different branches. This prevents having messy incomplete work all over the place on the same branch.\nIf you want to revisit an old commit, you can create a branch there and switch to it instead of moving HEAD (creating a detached HEAD situation). This way, if you decide to create new commits from that old one, you don‚Äôt risk loosing them.\nBranches are great for collaboration: each person can work on their own branch and merge it back to the main branch when they are done with one section of a project.\n\nAnd since branches are so cheap to create, there is no downside to their creation."
  },
  {
    "objectID": "git/branches.html#creating-branches-and-switching-between-branches",
    "href": "git/branches.html#creating-branches-and-switching-between-branches",
    "title": "Branches",
    "section": "Creating branches and switching between branches",
    "text": "Creating branches and switching between branches\nYou can create a new branch with:\ngit branch <new-branch-name>\n\nExample:\n\ngit branch test\n\nand you can then switch to it with:\ngit switch <new-branch-name>\n\nExample:\n\ngit switch test\n\nAlternatively, you can do both at once with the convenient:\ngit switch -c <new-branch-name>\n\n-c flag for ‚Äúcreate‚Äù. So you create a branch and switch to it directly.\n\nI find this last command most useful as it is all too easy otherwise to create a new branch, forget to switch to it, and create commits on the wrong branch ‚Ä¶"
  },
  {
    "objectID": "git/branches.html#listing-branches",
    "href": "git/branches.html#listing-branches",
    "title": "Branches",
    "section": "Listing branches",
    "text": "Listing branches\ngit branch\n  main\n* test\nThe * shows the branch you are currently on (i.e.¬†the branch to which HEAD points to). In our example, the project has two branches and we are on the branch test."
  },
  {
    "objectID": "git/branches.html#comparing-branches",
    "href": "git/branches.html#comparing-branches",
    "title": "Branches",
    "section": "Comparing branches",
    "text": "Comparing branches\nYou can use git diff to compare branches:\ngit diff main test\nThis shows all the lines that have been modified (added or deleted) between the commits both branches point to."
  },
  {
    "objectID": "git/branches.html#merging-branches",
    "href": "git/branches.html#merging-branches",
    "title": "Branches",
    "section": "Merging branches",
    "text": "Merging branches\nWhen you are happy with the changes you made on your test branch, you can merge it into main.\n\nFast-forward merge\nIf you have only created new commits on the branch test, the merge is called a ‚Äúfast-forward merge‚Äù because main and test have not diverged: it is simply a question of having main catch up to test.\n\nFirst, you switch to main:\ngit switch main\n\nThen you do the fast-forward merge:\ngit merge test\n\nThen, usually, you delete the branch test as it has served its purpose:\ngit branch -d test\n\nAlternatively, you can switch back to test and do the next bit of experimental work on it. This allows to keep main free of mishaps and bad developments.\n\n\nThree-way merge\nIf the branches have diverged (you created commits on both branches), the merge requires the creation of an additional commit called a ‚Äúmerge commit‚Äù.\nLet‚Äôs go back to our situation before we created the branch test:\n\nThis time, you create a branch called test2:\n\nand you switch to it:\n\nThen you create some commits:\n\n\nNow you switch back to main:\n\nAnd you create commits from main too:\n\n\nTo merge your branch test2 into main, a new commit is now required. Git will create this new commit automatically. As long as there is no conflict, it is just as easy as a fast-forward merge:\ngit merge test2\n\nAfter which, you can delete the (now useless) test branch (with git branch -d test2):"
  },
  {
    "objectID": "git/branches.html#resolving-conflicts",
    "href": "git/branches.html#resolving-conflicts",
    "title": "Branches",
    "section": "Resolving conflicts",
    "text": "Resolving conflicts\nGit works line by line. As long as you aren‚Äôt working on the same line(s) of the same file(s) on different branches, there will not be any merging difficulty. If however you modified one or more of the same line(s) of the same file(s) on different branches, Git has no way to decide which version should be kept and will thus not be able to complete the merge. It will then ask you to resolve the conflict(s). Conveniently, it will list the file(s) containing the conflict(s).\nThere are fancy tools to resolve conflicts, but you can do it in any text editor: simply open the file(s) listed by Git as having conflicts and look for the following markers:\n<<<<<<< HEAD\nThis is your version.\n=======\nThis is the alternative version of the same section of the file.\n>>>>>>> alternative version\nIn our case, it could look something like:\n<<<<<<< HEAD\nGreat sentence.\n=======\nGreat sentence with some variations.\n>>>>>>> test2\nThese markers are added by Git to signal the areas of conflict. It is up to you to choose between the two versions (or create a third one) and remove the conflict markers. After that, you can stage the file(s) which contained the conflicts to finish the merge (and then you can commit)."
  },
  {
    "objectID": "git/collab.html",
    "href": "git/collab.html",
    "title": "Collaborating through Git & GitHub",
    "section": "",
    "text": "Using Internet hosting services such as GitHub, Git is a powerful collaboration tool.\nIn this workshop, we will cover the three classic collaboration situations and see how a collaborative workflow works."
  },
  {
    "objectID": "git/collab.html#three-situations",
    "href": "git/collab.html#three-situations",
    "title": "Collaborating through Git & GitHub",
    "section": "Three situations",
    "text": "Three situations\nWhen you collaborate on a project through Git and a remote such as GitHub, there are three situations:\n\nyou create a project on your machine and want others to contribute to it (1),\nyou want to contribute to a project started by others and\n\nyou have write access to it (2),\nyou do not have write access to it (3).\n\n\n\n(1) You start the project\nIn this first situation, you are the author of a project (you have a project under version control on your own machine) and you want to initiate a collaboration with others on it using GitHub as a remote.\n\nCreate a remote on GitHub\nYou need to create a remote on GitHub.\n\nCreate a free GitHub account\nIf you don‚Äôt already have one, sign up for a free GitHub account.\n\nTo avoid having to type your password all the time, you should set up SSH for your account.\n\n\n\nCreate an empty repository on GitHub\n\nGo to the GitHub website, login, and go to your home page.\nLook for the Repositories tab & click the green New button.\nEnter the name you want for your repo, without spaces.\nMake the repository public or private.\n\n\n\nLink empty repository to your repo\nClick on the Code green drop-down button, select SSH if you have set SSH for your GitHub account or HTTPS and copy the address.\nIn the command line, cd inside your project, and add the remote:\ngit remote add <remote-name> <remote-address>\nremote-name is a convenience name to identify that remote. You can choose any name, but since Git automatically call the remote origin when you clone a repo, it is common practice to use origin as the name for the first remote.\n\nExample (using an SSH address):\n\ngit remote add origin git@github.com:<user>/<repo>.git\n\nExample (using an HTTPS address):\n\ngit remote add origin https://github.com/<user>/<repo>.git\nIf you don‚Äôt want to grant others write access to the project, and you only accept contributions through pull requests, you are set.\nIf you want to grant your collaborators write access to the project however, you need to add them to it.\n\n\n\nInvite collaborators\n\nGo to your GitHub project page.\nClick on the Settings tab.\nClick on the Manage access section on the left-hand side (you will be prompted for your GitHub password).\nClick on the Invite a collaborator green button.\nInvite your collaborators with one of their GitHub user name, their email address, or their full name.\n\n\n\n\n(2) Write access to project\nIn this second situation, someone else started a project and they are inviting you to collaborate to it, giving you write access to the project.\nIn this case, you need to clone the project: cd to the location where you want your local copy, then:\ngit clone <remote-address> <local-name>\nThis sets the project as a remote to your new local copy and that remote is automatically called origin.\nWithout <local-name>, the repo will have the name of the last part of the remote address.\n\n\n(3) No write access to project\nIn this third situation, someone else started a project and you want to collaborate to it, but you do not have write access to it.\nIn this case, you will have to submit pull requests.\nHere is the workflow for a pull request (PR):\n\nFork the project on GitHub.\nClone your fork on your machine.\nAdd the initial project as a second remote & call it upstream.\nPull from upstream to update your local project.\nCreate & checkout a new branch.\nMake & commit your changes on that branch.\nPush that branch to your fork (i.e.¬†origin ‚Äî remember that you do not have write access to upstream).\nGo to the original project GitHub‚Äôs page & open a pull request."
  },
  {
    "objectID": "git/collab.html#collaborative-workflow",
    "href": "git/collab.html#collaborative-workflow",
    "title": "Collaborating through Git & GitHub",
    "section": "Collaborative workflow",
    "text": "Collaborative workflow\n\nPulling and pushing\nWhen you collaborate with others using GitHub (or other remote), you and others will work simultaneously on some project. How does this work?\nTo upload your changes to the remote on GitHub you push to it with git push.\nIf one of your collaborators has made changes to the remote (pushing from their own local version of the project), you won‚Äôt be able to push. Instead, you will get the following message:\nTo xxx.git\n ! [rejected]        main -> main (fetch first)\nerror: failed to push some refs to 'xxx.git'\nhint: Updates were rejected because the remote contains work that you do\nhint: not have locally. This is usually caused by another repository pushing\nhint: to the same ref. You may want to first integrate the remote changes\nhint: (e.g., 'git pull ...') before pushing again.\nhint: See the 'Note about fast-forwards' in 'git push --help' for details.\nThe solution?\nYou first have to download (git pull) their work onto your machine, merge it with yours (which will happen automatically if there are no conflicts), before you can push your work to GitHub.\nNow‚Ä¶ what if there are conflicts?\n\n\nResolving conflicts\n\n\nFrom crystallize.com\n\nGit works line by line. As long as your collaborators and you aren‚Äôt working on the same line(s) of the same file(s) at the same time, there will not be any problem. If however you modified one or more of the same line(s) of the same file(s), Git will not be able to decide which version should be kept. When you git pull their work on your machine, the automatic merging will get interrupted and Git will ask you to resolve the conflict(s) before the merge can resume. It will conveniently tell you which file(s) contain the conflict(s).\nThere are fancy tools to resolve conflicts, but you can do it in any text editor: simply open the file(s) listed by Git as having conflicts and look for the following markers:\n<<<<<<< HEAD\nThis is your version.\n=======\nThis is the alternative version of the same section of the file.\n>>>>>>> alternative version\nThese markers are added by Git to signal the areas of conflict. It is up to you to choose between the two versions (or create a third one) and remove the conflict markers. After that, you can stage the file(s) which contained the conflicts to finish the merge (and then you can commit)."
  },
  {
    "objectID": "git/contrib.html",
    "href": "git/contrib.html",
    "title": "Collaborating to projects on GitHub",
    "section": "",
    "text": "There are countless free and open source tools on GitHub and if you use one such tool and find a problem or think that you can improve the project, or if you would like to request a novel feature, how do you go about it?\nIn this workshop, we will learn how to contribute to open source projects hosted on GitHub by opening issues and submitting pull requests."
  },
  {
    "objectID": "git/contrib.html#opening-issues",
    "href": "git/contrib.html#opening-issues",
    "title": "Collaborating to projects on GitHub",
    "section": "Opening issues",
    "text": "Opening issues\nThe easiest thing to do, if for instance, you are having problems with the tool, found a bug, or want to submit a feature request, is to open an issue.\nGitHub has also now implemented the ability to open ‚ÄúDiscussions‚Äù. If enabled by the maintainer of a project, this is the place where you want to ask for help."
  },
  {
    "objectID": "git/contrib.html#forking-a-project",
    "href": "git/contrib.html#forking-a-project",
    "title": "Collaborating to projects on GitHub",
    "section": "Forking a project",
    "text": "Forking a project\nNow, a more advanced approach is to actually make changes to the code of the project.\nIf you want to develop your own version of the project, you can fork the GitHub repository: go to GitHub and fork the project by clicking on the ‚ÄúFork‚Äù button in the top right corner.\nYou have all privileges on the forked project. So you can make any change you want there. You can clone it to your machine and develop the fork. But your fork does not get updated to the improvements made to the initial project. It is an independent project of its own.\n\nKeeping a fork up to date\nIf you want to keep your fork up to date with the initial project, you need to:\n\n1. Clone your fork on your machine\nThis will automatically set your fork on GitHub as the remote called origin:\n# If you have set SSH for your GitHub account\ngit clone git@github.com:<user>/<repo>.git <name>\n\n# If you haven't set SSH\ngit clone https://github.com/<user>/<repo>.git <name>\n\n\n2. Add the initial project as upstream\nAdd a second remote, this one pointing to the initial project. It is usual to call this remote upstream:\n# If you have set SSH for your GitHub account\ngit remote add upstream git@github.com:<user>/<repo>.git\n\n# If you haven't set SSH\ngit remote add upstream https://github.com/<user>/<repo>.git\n\n\n3. Pull from upstream\nYou can now pull from upstream to keep your fork up to date.\nFrom there on, you can pull from and push to origin (your fork) and you can pull from upstream (the initial repo).\nOf course, if your project and the initial one diverge in places, this will lead to conflicts that you will have to resolve as you merge the pulls from upstream.\nMost of the time however, you don‚Äôt want to develop your own version of the project. Instead, you want to make the initial project better by contributing to it. But you can‚Äôt push changes to upstream directly since you are not part of that project. You don‚Äôt have write access to that repository. If anybody could push to any project, that would be utter chaos.\nSo how do you contribute code to someone else‚Äôs project?"
  },
  {
    "objectID": "git/contrib.html#creating-pull-requests",
    "href": "git/contrib.html#creating-pull-requests",
    "title": "Collaborating to projects on GitHub",
    "section": "Creating pull requests",
    "text": "Creating pull requests\nHere is the workflow as described in the Git manual:\n\nPull from upstream to make sure that your contributions are made on an up-to-date version of the project\nCreate and checkout a new branch\nMake and commit your changes on that branch\nPush that branch to your fork (i.e.¬†origin‚Äîremember that you do not have write access on upstream)\nGo to the original project GitHub‚Äôs page and open a pull request from your fork. Note that after you have pushed your branch to origin, GitHub will automatically offer you to do so.\n\nThe maintainer of the initial project may accept or decline the PR. They may also make comments and ask you to make changes. If so, make new changes and push additional commits to that branch until they are happy with the change.\nOnce the PR is merged by the maintainer, you can delete the branch on your fork and pull from upstream to update your local fork with the recently accepted changes."
  },
  {
    "objectID": "git/getting_started.html",
    "href": "git/getting_started.html",
    "title": "Getting started with Git",
    "section": "",
    "text": "Git is a free and open source version control system (a software that tracks changes to your files, allowing you to revisit or revert to older versions).\nIn this introductory workshop you will learn the basics of working with Git on the command line."
  },
  {
    "objectID": "git/getting_started.html#what-is-a-version-control-system",
    "href": "git/getting_started.html#what-is-a-version-control-system",
    "title": "Getting started with Git",
    "section": "What is a version control system?",
    "text": "What is a version control system?\n\n\n\n\nfrom PhD\n\n\nWhenever we work on an important document, we intuitively realize that it is important to keep key versions (e.g.¬†the version of a manuscript that we sent to our supervisor, the revised version after we addressed their comments, the revised version after we addressed reviewer comments, etc.). We have all been there ‚Ä¶ The versions accumulate with names that are often less than helpful ‚Ä¶\n\n\n\n\n\nfrom Geek&Poke\n\n\n‚Ä¶ and soon enough, it is hell. This is a form of versioning, but a terribly messy and inefficient one. Version control systems are software that allow to handle this much more effectively."
  },
  {
    "objectID": "git/getting_started.html#which-version-control-system-should-i-use",
    "href": "git/getting_started.html#which-version-control-system-should-i-use",
    "title": "Getting started with Git",
    "section": "Which version control system should I use?",
    "text": "Which version control system should I use?\nGit is an open source distributed version control system (DVCS) created in 2005 by Linus Torvalds for the versioning of the Linux kernel during its development.\nIn distributed version control systems, the full history of projects lives on everybody‚Äôs machine‚Äîas opposed to being only stored on a central server as was the case with centralized version control systems (CVCS). This allows for offline work, huge speedups, easy branching, and multiple backups. DVCS have taken over CVCS.\nIf the trends of Google searches of the existing version control systems are any indication of their popularity, we can say that Git has crushed the competition since 2010.\n\n\nNowadays, it is indeed extremely rare to come across any other version control system.\nGit is simply that good üôÇ"
  },
  {
    "objectID": "git/getting_started.html#installation-and-setup",
    "href": "git/getting_started.html#installation-and-setup",
    "title": "Getting started with Git",
    "section": "Installation and setup",
    "text": "Installation and setup\n\nInstalling Git\n\nMacOS & Linux users\nInstall Git from the official website.\n\n\nWindows users\nInstall Git for Windows. This will also install Git Bash, a Bash emulator.\n\n\n\nUsing Git\nWe will use Git from the command line throughout this workshop.\nMacOS users: ‚ÄÉ‚ÄÉ‚ÄÇopen Terminal.\nWindows users: ‚ÄÉ‚ÄÇopen Git Bash.\nLinux users: ‚ÄÉ‚ÄÉ‚ÄÉopen the terminal emulator of your choice.\n\n\nGit commands\nAll commands start with git.\nA typical command is of the form:\ngit <command> [flags] [arguments]\n\nExample:\n\ngit config --global \"Your Name\"\n\n\nConfiguring Git\nBefore you can use Git, you need to set some basic configuration. You will do this in the terminal you just opened.\n\nUser identity\ngit config --global user.name \"<Your Name>\"\ngit config --global user.email \"<your@email>\"\n\nExample:\n\ngit config --global user.name \"John Doe\"\ngit config --global user.email \"john.doe@gmail.com\"\n\n\nText editor\ngit config --global core.editor \"<text-editor>\"\n\nExample for nano:\n\ngit config --global core.editor \"nano\"\n\n\nLine ending\n\nmacOS, Linux, or WSL\ngit config --global core.autocrlf input\n\n\nWindows\ngit config --global core.autocrlf true\n\n\n\nList settings\ngit config --list\n\nYou can also set project-specific configurations (e.g.¬†maybe you want to use a different email address for a certain project).\nIn that case, navigate to your project and run the command without the --global flag.\n\nExample:\n\ncd /path/to/project\ngit config user.email \"your_other@email\""
  },
  {
    "objectID": "git/getting_started.html#documentation",
    "href": "git/getting_started.html#documentation",
    "title": "Getting started with Git",
    "section": "Documentation",
    "text": "Documentation\n\nInternal documentation\n\nMan pages\ngit <command> --help\ngit help <command>\nman git-<command>\n\nExample:\n\ngit commit --help\ngit help commit\nman git-commit\n\nUseful keybindings when you are in the pager\nSPACE      scroll one screen down\nb          scroll one screen up\nq          quit\n\n\n\nCommand options\ngit <command> -h\n\nExample:\n\ngit commit -h\n\n\n\nOnline documentation\n\nOfficial Git manual\nOpen source Pro Git book\n\n\nCourses & workshops\n\nWestern Canada Research Computing Git workshops\nWestGrid Summer School 2020 Git course\nWestGrid Autumn School 2020 Git course\nSoftware Carpentry Git lesson\n\n\n\nQ & A\n\nStack Overflow [git] tag"
  },
  {
    "objectID": "git/getting_started.html#lets-create-a-mock-project",
    "href": "git/getting_started.html#lets-create-a-mock-project",
    "title": "Getting started with Git",
    "section": "Let‚Äôs create a mock project",
    "text": "Let‚Äôs create a mock project\nLet‚Äôs imagine that you have been working on chapter 3 of your thesis for some time, without using a version control system. We will put that chapter under version control and see how you should work from now on.\nFirst, we need to create a mock set of documents.\n\nNavigate to a suitable location\n\ncd </some/suitable/location/in/your/computer>\n\nCreate the directory at the root of chapter 3\n\nmkdir chapter3\n\nMake sure not to use any spaces in the name: Git doesn‚Äôt work well with spaces.\n\n\nCreate a number of subdirectories\n\nmkdir chapter3/src chapter3/ms chapter3/data chapter3/results\n\nCreate a mock manuscript\n\necho \"# Chapter 3\n## Introduction\nBla bla bla bla bla.\n## Methods\nBla bla bla.\" > chapter3/ms/chapter3.md\n\nGit can only version text files. If you write your papers or thesis chapter in text files (e.g.¬†markdown, LaTeX, org-mode), you will be able to put them under version control, which is really convenient. If you use a word processor, you won‚Äôt be able to.\n\n\nCreate a mock R script\n\necho \"library(ggplot2)\nlibrary(dplyr)\n\ndf <- data.frame(\n  x = (1:5),\n  y = (1:5)\n)\n\nggplot(df, aes(x, y)) + geom_point()\" > chapter3/src/chapter3.R\n\nEven if you use a word processor for your writing, your scripts (e.g.¬†in Python, R, etc.) will be written in text files. So you will always be able to put at least those files under version control."
  },
  {
    "objectID": "git/getting_started.html#initializing-a-git-repository",
    "href": "git/getting_started.html#initializing-a-git-repository",
    "title": "Getting started with Git",
    "section": "Initializing a Git repository",
    "text": "Initializing a Git repository\nMake sure to enter the project before initializing the repository.\ncd chapter3\nNow, you can run the command that will turn your chapter3 directory into a Git repository:\ngit init\nInitialized empty Git repository in chapter3/.git/\n\nGit is very verbose: you will often get useful feed-back after running commands.\n\nWhen you run this command, Git creates a .git repository. This is where it will store all its files.\nYou can see that this repository was created by running:\nls -a\n.\n..\n.git\ndata\nms\nresults\nsrc\n\nIf you run git init in the wrong location, you can easily fix this: simply delete the .git directory that you created!"
  },
  {
    "objectID": "git/getting_started.html#creating-commits",
    "href": "git/getting_started.html#creating-commits",
    "title": "Getting started with Git",
    "section": "Creating commits",
    "text": "Creating commits\nYou can think of a commit as a snapshot of a particular version of your project.\nYou should create a new commit whenever you think that your project is at a point to which you might want to go back to.\nLet‚Äôs create a first commit with the state of our chapter 3 before we do any more work to it:\ngit add .\ngit commit -m \"Initial commit\"\n[main (root-commit) 24duu7i] Initial commit\n 2 files changed, 18 insertions(+)\n create mode 100644 ms/chapter3.md\n create mode 100644 src/chapter3.R\nTo create a commit, we first need to add the file(s) we want to add to our commit to the staging area (also called ‚Äúindex‚Äù). This is done with the command git add. To add all the files, we can use git add . (. represents the current directory).\nOnce we have added some files to the staging area, we can create a commit. But each commit has a message associated to it. One way to add this message is to use the command to create commits (git commit) with the -m flag (for ‚Äúmessage‚Äù). Here, our message is simply ‚ÄúInitial commit‚Äù.\n\nGit saves the history of a project as a series of snapshots:\n\nThose snapshots are called commits:\n\nEach commit is identified by a unique hash and contains these metadata:\n\nauthor,\ndate and time,\nthe hash of parent commit(s),\na message.\n\nAs soon as you create the first commit, a pointer called a branch is created and it points to that commit. By default, that first branch is called main:\n\nAnother pointer (HEAD) points to the branch main. HEAD indicates where we are in the project history.\n\nWe can now do some work in our chapter 3. For instance, let‚Äôs imagine that we are adding a result section to our chapter3.md file.\necho \"\n## Results\n\nWe now have a bunch of results in our markdown manuscript.\" >> ms/chapter3.md\nMake sure to use >> here and not >: >> prepends content while > replaces any existing content.\nIf this new addition is important enough to justify making a new commit (how often you commit is up to you), we can do so:\ngit add ms/chapter3.md\ngit commit -m \"Add result section to manuscript\"\n[main 451c47b] Add result section to manuscript\n 1 file changed, 4 insertions(+)\n\nAs you create more commits, the history of your project grows ‚Ä¶\n\n‚Ä¶ and the pointers HEAD and main automatically move to the last commit:\n\nFor simplicity, the diagrams can be simplified this way:\n\n\n\nAdvice for great commit messages\n \n\nfrom xkcd.com\n\n\nUse the present tense\nThe first line is a summary of the commit and is less than 50 characters long\nLeave a blank line below\nThen add the body of your commit message with more details\n\n\nExample of a good commit message:\n\ngit commit -m \"Reduce boundary conditions by a factor of 0.3\n\nUpdate boundaries\nRerun model and update table\nRephrase method section in ms\"\nFuture you will thank you! (And so will your collaborators)."
  },
  {
    "objectID": "git/getting_started.html#understanding-the-staging-area",
    "href": "git/getting_started.html#understanding-the-staging-area",
    "title": "Getting started with Git",
    "section": "Understanding the staging area",
    "text": "Understanding the staging area\nNew Git users are often confused about the two-step commit process (first, you stage with git add, then you commit with git commit). This intermediate step seems, at first, totally unnecessary. In fact, it is very useful: without it, commits would always include all new changes made to a project and they would thus be very messy. The staging area allows to prepare (‚Äústage‚Äù) the next commit. This way, you only commit what you want when you want.\n\nLet‚Äôs go over a simple example:\n\nWe don‚Äôt always work linearly. Maybe you are working on a section of your manuscript when you realize by chance that there is a mistake in your script. You fix that mistake. On your next commit, it might make little sense to commit together that fix and your manuscript changes since they are not related. If your commits are random bag of changes, it will be very hard for future you to navigate your project history.\nIt is a lot better to only stage your script fix, commit it, then only stage your manuscript update, and commit this in a different commit.\nThe staging area allows you to pick and chose the changes from one or various files that constitute some coherent change to the project and that make sense to commit together."
  },
  {
    "objectID": "git/getting_started.html#inspecting-changes",
    "href": "git/getting_started.html#inspecting-changes",
    "title": "Getting started with Git",
    "section": "Inspecting changes",
    "text": "Inspecting changes\n\nList of modified files\nOne command you will run often when working with Git is git status:\ngit status\nOn branch main\nnothing to commit, working tree clean\nThis means that we are on the branch main (that‚Äôs the only branch in our repo at this point) and that all changes to our project have been committed.\nLet‚Äôs modify a file and see what happens:\necho \"\n## Conclusion\n\nAnd finally, the great conclusion of our paper.\" >> ms/chapter3.md\ngit status\nOn branch main\nChanges not staged for commit:\n  (use \"git add <file>...\" to update what will be committed)\n  (use \"git restore <file>...\" to discard changes in working directory)\n        modified:   ms/chapter3.md\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\nLet‚Äôs modify another file:\necho \"\na = 23\" >> src/chapter3.R\ngit status\nOn branch main\nChanges not staged for commit:\n  (use \"git add <file>...\" to update what will be committed)\n  (use \"git restore <file>...\" to discard changes in working directory)\n        modified:   ms/chapter3.md\n        modified:   src/chapter3.R\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\nMaybe we don‚Äôt want to create a commit with all those changes, so we only stage the changes made to the manuscript:\ngit add ms/chapter3.md\nThen we check the status of our repository again:\ngit status\nOn branch main\nChanges to be committed:\n  (use \"git restore --staged <file>...\" to unstage)\n        modified:   ms/chapter3.md\n\nChanges not staged for commit:\n  (use \"git add <file>...\" to update what will be committed)\n  (use \"git restore <file>...\" to discard changes in working directory)\n        modified:   src/chapter3.R\nIf we ran git commit at this point, we would create a new commit with the changes made to the manuscript.\n\n\nList of actual changes\nWhile git status gives us the list of new files and files with changes, it doesn‚Äôt allow us to see what those changes are. For this, we need a new command: git diff.\n\nDifferences between the working directory and the index\ngit diff shows the difference between the working directory (our actual files) and the index (staging area):\ngit diff\ndiff --git a/src/chapter3.R b/src/chapter3.R\nindex 95f1592..2bf030d 100644\n--- a/src/chapter3.R\n+++ b/src/chapter3.R\n@@ -7,3 +7,5 @@ df <- data.frame(\n )\n\n ggplot(df, aes(x, y)) + geom_point()\n+\n+a = 23\nThis allows us to see that src/chapter3.R has a new line (a = 23) and that it is not yet staged.\n\n\nDifferences between the index and your last commit\nTo see what would be committed if you ran git commit (so, to see the difference between the index and the last commit), you need to run instead:\ngit diff --cached\ndiff --git a/ms/chapter3.md b/ms/chapter3.md\nindex 9408f32..80d2c5c 100644\n--- a/ms/chapter3.md\n+++ b/ms/chapter3.md\n@@ -11,3 +11,7 @@ Bla bla bla.\n ## Results\n\n We now have a bunch of results in our markdown manuscript.\n+\n+## Conclusion\n+\n+And finally, the great conclusion of our paper.\nThis shows us the changes that we have staged but not yet committed (the changes to our manuscript).\n\n\nDifferences between the working directory and your last commit\nThis means, both of the above. This can been displayed with:\ngit diff HEAD\ndiff --git a/ms/chapter3.md b/ms/chapter3.md\nindex 9408f32..80d2c5c 100644\n--- a/ms/chapter3.md\n+++ b/ms/chapter3.md\n@@ -11,3 +11,7 @@ Bla bla bla.\n ## Results\n\n We now have a bunch of results in our markdown manuscript.\n+\n+## Conclusion\n+\n+And finally, the great conclusion of our paper.\ndiff --git a/src/chapter3.R b/src/chapter3.R\nindex 95f1592..2bf030d 100644\n--- a/src/chapter3.R\n+++ b/src/chapter3.R\n@@ -7,3 +7,5 @@ df <- data.frame(\n )\n\n ggplot(df, aes(x, y)) + geom_point()\n+\n+a = 23\nNow, let‚Äôs clean up our working directory by creating two new commits:\ngit commit -m \"Add conclusion to the manuscript\"\n[main c7fc9c1] Add conclusion to the manuscript\n 1 file changed, 4 insertions(+)\ngit add .\ngit commit -m \"Define the variable a in R script\"\n[main a049a2f] Define the variable a in R script\n 1 file changed, 2 insertions(+)\nIf we look at the status of our repository now, we can see that it is clean again:\ngit status\nOn branch main\nnothing to commit, working tree clean"
  },
  {
    "objectID": "git/getting_started.html#ignoring",
    "href": "git/getting_started.html#ignoring",
    "title": "Getting started with Git",
    "section": "Ignoring",
    "text": "Ignoring\nNot everything should be under version control. For instance, you don‚Äôt want to put under version control non-text files or your initial data. You also shouldn‚Äôt put under version control documents that can be easily recreated such as graphs and script outputs.\nHowever, you don‚Äôt want to have such documents constantly showing up when you run git status. In order to have a clean working directory while keeping them out of version control, you can create a file called .gitignore and add to it a list of files or patterns that you want Git to disregard.\nFor instance:\necho \"/data/\n/results/\" > .gitignore\nThis creates a .gitignore file with two entries (/data/ and /results/) and from now on, any file in either of these directories will be ignored by Git.\nThe .gitignore is a file like any other file, so you‚Äôll want to stage and commit it:\ngit status\nOn branch main\nUntracked files:\n  (use \"git add <file>...\" to include in what will be committed)\n        .gitignore\n\nnothing added to commit but untracked files present (use \"git add\" to track)\ngit add .gitignore\ngit commit -m \"Add .gitignore file with data and results\"\n[main a1df8e5] Add .gitignore file with data and results\n 1 file changed, 2 insertions(+)\n create mode 100644 .gitignore\ngit status\nOn branch main\nnothing to commit, working tree clean"
  },
  {
    "objectID": "git/getting_started.html#displaying-the-commit-history",
    "href": "git/getting_started.html#displaying-the-commit-history",
    "title": "Getting started with Git",
    "section": "Displaying the commit history",
    "text": "Displaying the commit history\nSo far, we have created 5 commits. To display them, you use the command git log:\ngit log\ncommit a1df8e56ad45ddd514ff951f2d65e4e1d40a641c (HEAD -> main)\nAuthor: Marie-Helene Burle <xxx@xxx>\nDate:   Mon Oct 3 22:57:59 2022 -0700\n\n    Add .gitignore file with data and results\n\ncommit a049a2f6834801bf76fa3c2c191a59a3ec589d6e\nAuthor: Marie-Helene Burle <xxx@xxx>\nDate:   Mon Oct 3 21:17:23 2022 -0700\n\n    Define the variable a in R script\n\ncommit c7fc9c1743d8a40c3f72d9450b9440dca1cb5922\nAuthor: Marie-Helene Burle <xxx@xxx>\nDate:   Mon Oct 3 21:16:43 2022 -0700\n\n    Add conclusion to the manuscript\n\ncommit 451c47b386895b8b0b5bdd1a8734ef1d51f9ccc9\nAuthor: Marie-Helene Burle <xxx@xxx>\nDate:   Mon Oct 3 18:35:51 2022 -0700\n\n    Add result section to manuscript\n\ncommit 24duu7id631a7390a910fa13cd4954cf9e8a3061\nAuthor: Marie-Helene Burle <xxx@xxx>\nDate:   Mon Oct 3 18:19:28 2022 -0700\n\n    Initial commit\nAs you can see, commits are listed from the bottom up. You can customize the output of git log by playing with the many existing flags (you can run man git-log to get the list of all flags).\nFor instance, you can display each commit as a one-liner:\ngit log --oneline\na1df8e5 (HEAD -> main) Add .gitignore file with data and results\na049a2f Define the variable a in R script\nc7fc9c1 Add conclusion to the manuscript\n451c47b Add result section to manuscript\n24duu7i Initial commit\nYou can display it as a graph:\ngit log --graph\n* commit a1df8e56ad45ddd514ff951f2d65e4e1d40a641c (HEAD -> main)\n| Author: Marie-Helene Burle <xxx@xxx>\n| Date:   Mon Oct 3 22:57:59 2022 -0700\n|\n|     Add .gitignore file with data and results\n|\n* commit a049a2f6834801bf76fa3c2c191a59a3ec589d6e\n| Author: Marie-Helene Burle <xxx@xxx>\n| Date:   Mon Oct 3 21:17:23 2022 -0700\n|\n|     Define the variable a in R script\n|\n* commit c7fc9c1743d8a40c3f72d9450b9440dca1cb5922\n| Author: Marie-Helene Burle <xxx@xxx>\n| Date:   Mon Oct 3 21:16:43 2022 -0700\n|\n|     Add conclusion to the manuscript\n|\n* commit 451c47b386895b8b0b5bdd1a8734ef1d51f9ccc9\n| Author: Marie-Helene Burle <xxx@xxx>\n| Date:   Mon Oct 3 18:35:51 2022 -0700\n|\n|     Add result section to manuscript\n|\n* commit 24duu7id631a7390a910fa13cd4954cf9e8a3061\n  Author: Marie-Helene Burle <xxx@xxx>\n  Date:   Mon Oct 3 18:19:28 2022 -0700\n\n      Initial commit\n\nHere is an example of more complex customization:\n\ngit log \\\n    --graph \\\n    --date=short \\\n    --pretty=format:'%C(cyan)%h %C(blue)%ar %C(auto)%d'`\n                   `'%C(yellow)%s%+b %C(magenta)%ae'\n* a1df8e5 88 seconds ago  (HEAD -> main)Add .gitignore file with data and results xxx@xxx\n* a049a2f 2 hours ago Define the variable a in R script xxx@xxx\n* c7fc9c1 2 hours ago Add conclusion to the manuscript xxx@xxx\n* 451c47b 4 hours ago Add result section to manuscript xxx@xxx\n* 24duu7i 5 hours ago Initial commit xxx@xxx"
  },
  {
    "objectID": "git/getting_started.html#getting-information-about-a-commit",
    "href": "git/getting_started.html#getting-information-about-a-commit",
    "title": "Getting started with Git",
    "section": "Getting information about a commit",
    "text": "Getting information about a commit\ngit log is useful to get an overview of our project history, but the information we get about each commit is limited. To get additional information about a particular commit, you can use git show followed by the hash of the commit you are interested about.\nFor instance, let‚Äôs explore our second commit:\ngit show 451c47b  # Replace the hash by the hash of your second commit\ncommit 451c47b386895b8b0b5bdd1a8734ef1d51f9ccc9 (HEAD -> main)\nAuthor: Marie-Helene Burle <xxx@xxx>\nDate:   Mon Oct 3 18:35:51 2022 -0700\n\n    Add result section to manuscript\n\ndiff --git a/ms/chapter3.md b/ms/chapter3.md\nindex b88424b..9408f32 100644\n--- a/ms/chapter3.md\n+++ b/ms/chapter3.md\n@@ -7,3 +7,7 @@ Bla bla bla bla bla.\n ## Methods\n\n Bla bla bla.\n+\n+## Results\n+\n+We now have a bunch of results in our markdown manuscript.\nIn addition to displaying the commit metadata, this also displays the difference with the previous commit."
  },
  {
    "objectID": "git/getting_started.html#revisiting-old-commits",
    "href": "git/getting_started.html#revisiting-old-commits",
    "title": "Getting started with Git",
    "section": "Revisiting old commits",
    "text": "Revisiting old commits\nThe pointer HEAD, which normally points to the branch main which itself points to latest commit, can be moved around. By moving HEAD to any commit, you can revisit the state of your project at that particular version.\nThe command for this is git checkout followed by the hash of the commit you want to revisit.\n\nFor instance, we could revisit the first commit in our example with:\n\ngit checkout 24duu7i  # Replace the hash by the hash of your first commit\nNote: switching to '24duu7i'.\n\nYou are in 'detached HEAD' state. You can look around, make experimental\nchanges and commit them, and you can discard any commits you make in this\nstate without impacting any branches by switching back to a branch.\n\nIf you want to create a new branch to retain commits you create, you may\ndo so (now or later) by using -c with the switch command. Example:\n\n  git switch -c <new-branch-name>\n\nOr undo this operation with:\n\n  git switch -\n\nTurn off this advice by setting config variable advice.detachedHead to false\n\nHEAD is now at 24duu7i Initial commit\n\nThis is the same as the command git switch --detach 24duu7i: git switch is a command introduced a few years ago because git checkout can be used for many things in Git and it was confusing many users. git switch allows to switch from one branch to another or, with the --detach flag, to switch to a commit as is the case here.\n\nOnce you have seen what you wanted to see, you can go back to your branch main with:\ngit checkout main\nPrevious HEAD position was 24duu7i Initial commit\nSwitched to branch 'main'\n\nThis is the same as the command git switch main.\n\nBe careful not to forget to go back to your branch main before making changes to your project. If you want to move the project to a new direction from some old commit, you need to create a new branch before doing so. When HEAD points directly to a commit (and not to a branch), this is called ‚ÄúDetached HEAD‚Äù and it is not a position from which you want to modify the project.\n\nIt is totally fine to move HEAD around and have it point directly to a commit (instead of a branch) as long as you are only looking at a version of your project and get back to a branch before doing some work:"
  },
  {
    "objectID": "git/getting_started.html#branches",
    "href": "git/getting_started.html#branches",
    "title": "Getting started with Git",
    "section": "Branches",
    "text": "Branches\nOne of the reasons Git has become so popular is its branch system.\nRemember that little pointer called main? That‚Äôs our main branch: the one Git creates automatically when we create our first commit.\nA branch in Git is just that: a little pointer. This makes creating branches extremely quick and cheap. But they are extremely convenient.\nInstead of checking out a commit as we just saw (which creates a detached HEAD state), we can instead create a new branch on that commit and switch to it with:\ngit switch -c newbranch 24duu7i\nSwitched to a new branch 'newbranch'\nThis creates a new branch called newbranch on our first commit and switches HEAD to it. If you do this instead of entering a detached HEAD state, it is totally safe to make chang es and create commits from there. You can easily switch HEAD back and forth between the two branches with:\ngit switch main        # Moves HEAD back to the branch main\nSwitched to branch 'main'\ngit switch newbranch\nSwitched to branch 'newbranch'\ngit status\nOn branch newbranch\nnothing to commit, working tree clean\nIf you already checked out the commit 24duu7i with git checkout 24duu7i, you can create the new branch newbranch on that commit and switch to it with:\ngit switch -c newbranch\nSwitched to a new branch 'newbranch'\nThose are equivalent workflows. Just don‚Äôt forget never to work from a detached HEAD state. You can look around in that state, but that‚Äôs it. Why? Because commits that are not part of a branch get automatically deleted on a regular basis when Git runs its garbage collection. So any commits you make from a detached HEAD will eventually be lost. And that‚Äôs probably not what you want.\nIn short, git switch allows you to switch HEAD from one branch to another. With the -c flag, you can create a new branch before switching to it. And by adding some starting point such as a commit, the new branch gets created on that commit rather than on the position of HEAD.\nNow, have a look at what happens if you run git log from newbranch:\ngit log\ncommit 24duu7id631a7390a910fa13cd4954cf9e8a3061 (HEAD -> newbranch)\nAuthor: Marie-Helene Burle <xxx@xxx>\nDate:   Mon Oct 3 18:19:28 2022 -0700\n\n    Initial commit\nHorror! It looks like all our commits except for the first one are gone!\nIn fact, they still exist, but by default, git log only shows what is on the current branch. To see all the commits that are on any branch in your project, you need to add the --all flag:\ngit log --all\ncommit 863afd650ecaeab85da2f8ed0d3c88a778754727 (main)\nAuthor: Marie-Helene Burle <xxx@xxx>\nDate:   Tue Oct 4 10:32:39 2022 -0700\n\n    Add .gitignore file with data and results\n\ncommit dc780c75c76220a39f7c89a76bebb670dad25b8e\nAuthor: Marie-Helene Burle <xxx@xxx>\nDate:   Tue Oct 4 10:32:12 2022 -0700\n\n    Define the variable a in R script\n\ncommit 5ba96b254b505f7d04f59f988a621a746a0c6896\nAuthor: Marie-Helene Burle <xxx@xxx>\nDate:   Tue Oct 4 10:28:51 2022 -0700\n\n    Add conclusion to the manuscript\n\ncommit 451c47b386895b8b0b5bdd1a8734ef1d51f9ccc9\nAuthor: Marie-Helene Burle <xxx@xxx>\nDate:   Mon Oct 3 18:35:51 2022 -0700\n\n    Add result section to manuscript\n\ncommit 24duu7id631a7390a910fa13cd4954cf9e8a3061 (HEAD -> newbranch)\nAuthor: Marie-Helene Burle <xxx@xxx>\nDate:   Mon Oct 3 18:19:28 2022 -0700\n\n    Initial commit\n\nIn this log, we can now see main, but that HEAD points to newbranch.\n\n\nListing branches\ngit branch\n  main\n* newbranch\nThe * shows the branch you are currently on (i.e.¬†the branch to which HEAD points to).\n\n\nComparing branches\nYou can use git diff to compare branches:\ngit diff newbranch main\ndiff --git a/.gitignore b/.gitignore\nindex 0000000..e85f44a\n--- /dev/null\n+++ b/.gitignore\n@@ -0,0 +1,2 @@\n+/data/\n+/results/\ndiff --git a/ms/chapter3.md b/ms/chapter3.md\nindex b88424b..80d2c5c 100644\n--- a/ms/chapter3.md\n+++ b/ms/chapter3.md\n@@ -7,3 +7,11 @@ Bla bla bla bla bla.\n ## Methods\n\n Bla bla bla.\n+\n+## Results\n+\n+We now have a bunch of results in our markdown manuscript.\n+\n+## Conclusion\n+\n+And finally, the great conclusion of our paper.\ndiff --git a/src/chapter3.R b/src/chapter3.R\nindex 95f1592..2bf030d 100644\n--- a/src/chapter3.R\n+++ b/src/chapter3.R\n@@ -7,3 +7,5 @@ df <- data.frame(\n )\n\n ggplot(df, aes(x, y)) + geom_point()\n+\n+a = 23\nThis shows all the lines that have been modified (added or deleted) between the commits both branches point to.\n\n\nMerging branches\nIf you want to merge branches, switch to the branch you want to merge into the other one, then run git merge.\nFor instance, if we want to merge newbranch onto main, we would first switch to newbranch (we are already on it, so nothing to do here), then:\ngit merge main\nUpdating 24duu7i..863afd6\nFast-forward\n .gitignore     | 2 ++\n ms/chapter3.md | 8 ++++++++\n src/chapter3.R | 2 ++\n 3 files changed, 12 insertions(+)\n create mode 100644 .gitignore\nThis merge is called a ‚Äúfast-forward‚Äù merge because main and newbranch had not diverged. It was simply a question of having newbranch catch up to main.\nIf you run git log again, you will see that newbrach has now caught up with main:\ngit log\ncommit 863afd650ecaeab85da2f8ed0d3c88a778754727 (HEAD -> newbranch, main)\nAuthor: Marie-Helene Burle <xxx@xxx>\nDate:   Tue Oct 4 10:32:39 2022 -0700\n\n    Add .gitignore file with data and results\n\ncommit dc780c75c76220a39f7c89a76bebb670dad25b8e\nAuthor: Marie-Helene Burle <xxx@xxx>\nDate:   Tue Oct 4 10:32:12 2022 -0700\n\n    Define the variable a in R script\n\ncommit 5ba96b254b505f7d04f59f988a621a746a0c6896\nAuthor: Marie-Helene Burle <xxx@xxx>\nDate:   Tue Oct 4 10:28:51 2022 -0700\n\n    Add conclusion to the manuscript\n\ncommit 451c47b386895b8b0b5bdd1a8734ef1d51f9ccc9\nAuthor: Marie-Helene Burle <xxx@xxx>\nDate:   Mon Oct 3 18:35:51 2022 -0700\n\n    Add result section to manuscript\n\ncommit 24duu7id631a7390a910fa13cd4954cf9e8a3061\nAuthor: Marie-Helene Burle <xxx@xxx>\nDate:   Mon Oct 3 18:19:28 2022 -0700\n\n    Initial commit\n\nHere is a classic situation of fast-forward merge.\nInstead of working on your branch main, you create a test branch and work on it (so HEAD is on the branch test and both move along as you create commits):\n\n\n\nWhen you are happy with the changes you made on your test branch, you decide to merge main onto it.\nFirst, you switch to main:\n\nThen you do the fast-forward merge from main onto test (so main catches up to test):\n\nThen, usually, you delete the branch test as it has served its purpose (with git branch -d test). Alternatively, you can switch back to it and do the next bit of experimental work in it. This allows to keep main free of possible mishaps and bad developments (if you aren‚Äôt happy with the work you did on your test branch, you can simply delete it and Git will clean the commits that are on it but not on main during the next garbage collection.\n\n\nIf the branches have diverged (you created commits from both main and newbranch), the merge would require the creation of an additional commit called a ‚Äúmerge commit‚Äù.\n\nHere is a classic situation of merge with a commit.\nYou create a test branch and switch to it:\n\nThen you create some commits:\n\n\nNow you switch back to main:\n\nAnd you create commits from main too:\n\n\nTo merge your main branch and your test branch, a new commit is now required (note that the command is the same as in the case of a fast-forward merge: git merge. Git will create the new commit automatically. As long as there is no conflict, it is just as easy as a fast-forward merge. We will talk later about resolving conflicts).\n\nAfter which, you can delete the (now useless) test branch (with git branch -d test2):\n\n\n\n\nResolving conflicts\nGit works line by line. As long as you aren‚Äôt working on the same line(s) of the same file(s) on different branches, there will not be any merging difficulty. If however you modified one or more of the same line(s) of the same file(s) on different branches, Git has no way to decide which version should be kept and will thus not be able to complete the merge. It will then ask you to resolve the conflict(s). Conveniently, it will list the file(s) containing the conflict(s).\nThere are fancy tools to resolve conflicts, but you can do it in any text editor: simply open the file(s) listed by Git as having conflicts and look for the following markers:\n<<<<<<< HEAD\nThis is your version.\n=======\nThis is the alternative version of the same section of the file.\n>>>>>>> alternative version\nThese markers are added by Git to signal the areas of conflict. It is up to you to choose between the two versions (or create a third one) and remove the conflict markers. After that, you can stage the file(s) which contained the conflicts to finish the merge (and then you can commit)."
  },
  {
    "objectID": "git/getting_started.html#troubleshooting",
    "href": "git/getting_started.html#troubleshooting",
    "title": "Getting started with Git",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n\n‚ÄúListen‚Äù to Git!\nGit is extremely verbose: by default, it will return lots of information. Read it!\nThese messages may feel overwhelming at first, but:\n\nthey will make more and more sense as you gain expertise,\nthey often give you clues as to what the problem is,\neven if you don‚Äôt understand them, you can use them as Google search terms.\n\n\n\n(Re-read) the doc\nAs I have no memory, I need to check the man pages all the time. That‚Äôs ok! It is quick and easy.\nFor more detailed information and examples, I really like the Official Git manual.\n\n\nSearch online\n\nGoogle\nStack Overflow [git] tag\n\n\n\nDon‚Äôt panic\nBe analytical. It is easy to panic and feel lost if something doesn‚Äôt work as expected. Take a breath and start with the basis:\n\nmake sure you are in the repo (pwd) and the files are where you think they are (ls -a),\ninspect the repository (git status, git diff, git log). Make sure not to overlook what Git is ‚Äútelling‚Äù you there."
  },
  {
    "objectID": "git/getting_started.html#conclusion",
    "href": "git/getting_started.html#conclusion",
    "title": "Getting started with Git",
    "section": "Conclusion",
    "text": "Conclusion\nGit is a powerful and fairly complex tool, but you don‚Äôt have to master it entirely to start using it. Start by putting your projects (e.g.¬†thesis chapters, papers) under version control and by creating commits whenever you reach important stages in your work.\nThings will grow from there.\n\n\nfrom xkcd.com"
  },
  {
    "objectID": "git/ignore.html",
    "href": "git/ignore.html",
    "title": "Excluding from version control",
    "section": "",
    "text": "This workshop looks in more details at what and how to exclude from version control."
  },
  {
    "objectID": "git/ignore.html#what-to-exclude",
    "href": "git/ignore.html#what-to-exclude",
    "title": "Excluding from version control",
    "section": "What to exclude",
    "text": "What to exclude\nThere are files you really want to put under version control, but there are files you shouldn‚Äôt.\nPut under version control:\n\nScripts\nManuscripts and notes\nMakefile & similar\n\nDo NOT put under version control:\n\nNon-text files (e.g.¬†images, office documents)\nOutputs that can be recreated by running code (e.g.¬†graphs, results)"
  },
  {
    "objectID": "git/ignore.html#how-to-exclude",
    "href": "git/ignore.html#how-to-exclude",
    "title": "Excluding from version control",
    "section": "How to exclude",
    "text": "How to exclude\n\nThe .gitignore file\nTo exclude files from version control, create a file called .gitignore in the root of your project and add those files to it, one per line.\n\nExample:\n\n# Create .gitignore and add 'graph.png' to it\necho graph.png > .gitignore\n\n# `>` would overwrite the content. `>>` appends\necho output.txt >> .gitignore\n\n# You can also ignore entire directories\necho /result/ >> .gitignore\n\n\nGlobing patterns\nYou can use globbing patterns.\n\nExample:\n\n# Exclude all .png files\necho *.png >> .gitignore\n\n\n.gitignore syntax\nEach line in a .gitignore file specifies a pattern.\nBlank lines are ignored and can serve as separators for readability.\nLines starting with # are comments.\nTo add patterns starting with a special character (e.g.¬†#, !), that character needs to be escaped with \\.\nTrailing spaces are ignored unless they are escaped with \\.\n! negates patterns.\nPatterns ending with / match directories. Otherwise patterns match both files and directories.\n/ at the beginning or within a search pattern indicates that the pattern is relative to the directory level of the .gitignore file (usually the root of the project). Otherwise the pattern matches anywhere below the .gitignore level.\n\nExamples:\n/foo/bar/ matches the directory foo/bar, but not the directory a/foo/bar\nfoo/bar/ matches both the directories foo/bar and a/foo/bar\n\n* matches anything except /.\n? matches any one character except /.\nThe range notation (e.g.¬†[a-zA-Z]) can be used to match one of the characters in a range.\nA leading **/ matches all directories.\n\nExample:\n**/foo matches file or directory foo anywhere. This is the same as foo.\n\nA trailing /** matches everything inside what it precedes.\n\nExample:\nabc/** matches all files (recursively) inside directory abc\n\n/**/ matches zero or more directories.\n\nExample:\na/**/b matches a/b, a/x/b, and a/x/y/b"
  },
  {
    "objectID": "git/practice_repo/search.html",
    "href": "git/practice_repo/search.html",
    "title": "Searching a version-controlled project",
    "section": "",
    "text": "What is the point of creating all these commits if you are unable to make use of them because you can‚Äôt find the information you need in them?\nIn this workshop, we will learn how to search:\n\nyour files (at any of their versions) and\nyour commit logs.\n\nBy the end of the workshop, you should be able to retrieve anything you need from your versioned project."
  },
  {
    "objectID": "git/practice_repo/search.html#installation",
    "href": "git/practice_repo/search.html#installation",
    "title": "Searching a version-controlled project",
    "section": "Installation",
    "text": "Installation\nMacOS & Linux users:\nInstall Git from the official website.\nWindows users:\nInstall Git for Windows. This will also install ‚ÄúGit Bash‚Äù, a Bash emulator."
  },
  {
    "objectID": "git/practice_repo/search.html#using-git",
    "href": "git/practice_repo/search.html#using-git",
    "title": "Searching a version-controlled project",
    "section": "Using Git",
    "text": "Using Git\nWe will use Git from the command line throughout this workshop.\nMacOS users: ‚ÄÉ‚ÄÉ‚ÄÇopen ‚ÄúTerminal‚Äù.\nWindows users: ‚ÄÉ‚ÄÇopen ‚ÄúGit Bash‚Äù.\nLinux users: ‚ÄÉ‚ÄÉ‚ÄÉopen the terminal emulator of your choice."
  },
  {
    "objectID": "git/practice_repo/search.html#practice-repo",
    "href": "git/practice_repo/search.html#practice-repo",
    "title": "Searching a version-controlled project",
    "section": "Practice repo",
    "text": "Practice repo\n\nGet a repo\nYou are welcome to use a repository of yours to follow this workshop. Alternatively, you can clone a practice repo I have on GitHub:\n\nNavigate to an appropriate location:\n\ncd /path/to/appropriate/location\n\nClone the repo:\n\n# If you have set SSH for your GitHub account\ngit clone git@github.com:prosoitos/practice_repo.git\n\n# If you haven't set SSH\ngit clone https://github.com/prosoitos/practice_repo.git\n\nEnter the repo:\n\ncd practice_repo"
  },
  {
    "objectID": "git/practice_repo/search.html#searching-files",
    "href": "git/practice_repo/search.html#searching-files",
    "title": "Searching a version-controlled project",
    "section": "Searching files",
    "text": "Searching files\nThe first thing that can happen is that you are looking for a certain pattern somewhere in your project (for instance a certain function or a certain word).\n\ngit grep\nThe main command to look through versioned files is git grep.\nYou might be familiar with the command-line utility grep which allows to search for lines matching a certain pattern in files. git grep does a similar job with these differences:\n\nit is much faster since all files under version control are already indexed by Git,\nyou can easily search any commit without having to check it out,\nit has features lacking in grep such as, for instance, pattern arithmetic or tree search using globs.\n\n\n\nLet‚Äôs try it\nBy default, git grep searches recursively through the tracked files in the working directory (that is, the current version of the tracked files).\nFirst, let‚Äôs look for the word test in the current version of the tracked files in the test repo:\n\ngit grep test\n\nadrian.txt:Adrian's test text file.\nformerlyadrian.txt:Adrian's test text file.\nms/protocol.md:This is my test.\nms/smabraha.txt:This is a test file that I wanted to make, then push it somehow\nredone17.txt:this is a test file from redone17\nsrc/test_manuel.py:def test(model, device, test_loader):\nsrc/test_manuel.py:    test_loss = 0\nsrc/test_manuel.py:        for data, target in test_loader:\nsrc/test_manuel.py:            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\nsrc/test_manuel.py:    test_loss /= len(test_loader.dataset)\nsrc/test_manuel.py:        test_loss, correct, len(test_loader.dataset),\nsrc/test_manuel.py:        100. * correct / len(test_loader.dataset)))\nsrc/test_manuel.py:    test_data = datasets.MNIST(\nsrc/test_manuel.py:    test_loader = torch.utils.data.DataLoader(test_data, batch_size=100)\nsrc/test_manuel.py:        test(model, device, test_loader)\ntestAV1.txt:This is a test\ntext-collab.txt:This is the collaboration testing\n\n\nLet‚Äôs add blank lines between the results of each file for better readability:\n\ngit grep --break test\n\nadrian.txt:Adrian's test text file.\n\nformerlyadrian.txt:Adrian's test text file.\n\nms/protocol.md:This is my test.\n\nms/smabraha.txt:This is a test file that I wanted to make, then push it somehow\n\nredone17.txt:this is a test file from redone17\n\nsrc/test_manuel.py:def test(model, device, test_loader):\nsrc/test_manuel.py:    test_loss = 0\nsrc/test_manuel.py:        for data, target in test_loader:\nsrc/test_manuel.py:            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\nsrc/test_manuel.py:    test_loss /= len(test_loader.dataset)\nsrc/test_manuel.py:        test_loss, correct, len(test_loader.dataset),\nsrc/test_manuel.py:        100. * correct / len(test_loader.dataset)))\nsrc/test_manuel.py:    test_data = datasets.MNIST(\nsrc/test_manuel.py:    test_loader = torch.utils.data.DataLoader(test_data, batch_size=100)\nsrc/test_manuel.py:        test(model, device, test_loader)\n\ntestAV1.txt:This is a test\n\ntext-collab.txt:This is the collaboration testing\n\n\nLet‚Äôs also put the file names on separate lines:\n\ngit grep --break --heading test\n\nadrian.txt\nAdrian's test text file.\n\nformerlyadrian.txt\nAdrian's test text file.\n\nms/protocol.md\nThis is my test.\n\nms/smabraha.txt\nThis is a test file that I wanted to make, then push it somehow\n\nredone17.txt\nthis is a test file from redone17\n\nsrc/test_manuel.py\ndef test(model, device, test_loader):\n    test_loss = 0\n        for data, target in test_loader:\n            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n    test_loss /= len(test_loader.dataset)\n        test_loss, correct, len(test_loader.dataset),\n        100. * correct / len(test_loader.dataset)))\n    test_data = datasets.MNIST(\n    test_loader = torch.utils.data.DataLoader(test_data, batch_size=100)\n        test(model, device, test_loader)\n\ntestAV1.txt\nThis is a test\n\ntext-collab.txt\nThis is the collaboration testing\n\n\nWe can display the line numbers for the results with the -n flag:\n\ngit grep --break --heading -n test\n\nadrian.txt\n1:Adrian's test text file.\n\nformerlyadrian.txt\n1:Adrian's test text file.\n\nms/protocol.md\n9:This is my test.\n\nms/smabraha.txt\n1:This is a test file that I wanted to make, then push it somehow\n\nredone17.txt\n1:this is a test file from redone17\n\nsrc/test_manuel.py\n50:def test(model, device, test_loader):\n52:    test_loss = 0\n55:        for data, target in test_loader:\n58:            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n62:    test_loss /= len(test_loader.dataset)\n65:        test_loss, correct, len(test_loader.dataset),\n66:        100. * correct / len(test_loader.dataset)))\n84:    test_data = datasets.MNIST(\n90:    test_loader = torch.utils.data.DataLoader(test_data, batch_size=100)\n97:        test(model, device, test_loader)\n\ntestAV1.txt\n1:This is a test\n\ntext-collab.txt\n1:This is the collaboration testing\n\n\nNotice how the results for the file src/test_manuel.py involve functions. It would be very convenient to have the names of the functions in which test appears.\nWe can do this with the -p flag:\n\ngit grep --break --heading -p test src/test_manuel.py\n\nsrc/test_manuel.py\ndef train(model, device, train_loader, optimizer, epoch):\ndef test(model, device, test_loader):\n    test_loss = 0\n        for data, target in test_loader:\n            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n    test_loss /= len(test_loader.dataset)\n        test_loss, correct, len(test_loader.dataset),\n        100. * correct / len(test_loader.dataset)))\ndef main():\n    test_data = datasets.MNIST(\n    test_loader = torch.utils.data.DataLoader(test_data, batch_size=100)\n        test(model, device, test_loader)\n\n\n\nWe added the argument src/test_manuel.py to limit the search to that file.\n\nWe can now see that the word test appears in the functions test and main.\nNow, instead of printing all the matching lines, let‚Äôs print the number of matches per file:\n\ngit grep -c test\n\nadrian.txt:1\nformerlyadrian.txt:1\nms/protocol.md:1\nms/smabraha.txt:1\nredone17.txt:1\nsrc/test_manuel.py:10\ntestAV1.txt:1\ntext-collab.txt:1\n\n\n\n\nMore complex patterns\ngit grep in fact searches for regular expressions. test is a regular expression matching test, but we can look for more complex patterns.\nLet‚Äôs look for image:\n\ngit grep image\n\n\nNo output means that the search is not returning any result.\n\nLet‚Äôs make this search case insensitive:\n\ngit grep -i image\n\nsrc/new_file.py:from PIL import Image\nsrc/new_file.py:berlin1_lr = Image.open(\"/home/marie/parvus/pwg/wtm/slides/static/img/upscaling/lr/berlin_1945_1.jpg\")\nsrc/new_file.py:berlin1_hr = Image.open(\"/home/marie/parvus/pwg/wtm/slides/static/img/upscaling/hr/berlin_1945_1.png\")\n\n\nWe are now getting some results as Image was present in three lines of the file src/new_file.py.\nLet‚Äôs now search for data:\n\ngit grep data\n\n.gitignore:data/\nms/protocol.md:Collected and analyzed amazing data\nsrc/new_file.py:from datasets import load_dataset\nsrc/new_file.py:set5 = load_dataset('eugenesiow/Set5', 'bicubic_x4', split='validation')\nsrc/test_manuel.py:from torchvision import datasets, transforms\nsrc/test_manuel.py:    for batch_idx, (data, target) in enumerate(train_loader):\nsrc/test_manuel.py:        data, target = data.to(device), target.to(device)\nsrc/test_manuel.py:        output = model(data)\nsrc/test_manuel.py:                epoch, batch_idx * len(data), len(train_loader.dataset),\nsrc/test_manuel.py:        for data, target in test_loader:\nsrc/test_manuel.py:            data, target = data.to(device), target.to(device)\nsrc/test_manuel.py:            output = model(data)\nsrc/test_manuel.py:    test_loss /= len(test_loader.dataset)\nsrc/test_manuel.py:        test_loss, correct, len(test_loader.dataset),\nsrc/test_manuel.py:        100. * correct / len(test_loader.dataset)))\nsrc/test_manuel.py:    train_data = datasets.MNIST(\nsrc/test_manuel.py:        '~/parvus/pwg/wtm/tml/data',\nsrc/test_manuel.py:        # '~/projects/def-sponsor00/data',\nsrc/test_manuel.py:    test_data = datasets.MNIST(\nsrc/test_manuel.py:        '~/parvus/pwg/wtm/tml/data',\nsrc/test_manuel.py:        # '~/projects/def-sponsor00/data',\nsrc/test_manuel.py:    train_loader = torch.utils.data.DataLoader(train_data, batch_size=50)\nsrc/test_manuel.py:    test_loader = torch.utils.data.DataLoader(test_data, batch_size=100)\n\n\nWe are getting results for the word data, but also for the pattern data in longer expressions such as train_data or dataset. If we only want results for the word data, we can use the -w flag:\n\ngit grep -w data\n\n.gitignore:data/\nms/protocol.md:Collected and analyzed amazing data\nsrc/test_manuel.py:    for batch_idx, (data, target) in enumerate(train_loader):\nsrc/test_manuel.py:        data, target = data.to(device), target.to(device)\nsrc/test_manuel.py:        output = model(data)\nsrc/test_manuel.py:                epoch, batch_idx * len(data), len(train_loader.dataset),\nsrc/test_manuel.py:        for data, target in test_loader:\nsrc/test_manuel.py:            data, target = data.to(device), target.to(device)\nsrc/test_manuel.py:            output = model(data)\nsrc/test_manuel.py:        '~/parvus/pwg/wtm/tml/data',\nsrc/test_manuel.py:        # '~/projects/def-sponsor00/data',\nsrc/test_manuel.py:        '~/parvus/pwg/wtm/tml/data',\nsrc/test_manuel.py:        # '~/projects/def-sponsor00/data',\nsrc/test_manuel.py:    train_loader = torch.utils.data.DataLoader(train_data, batch_size=50)\nsrc/test_manuel.py:    test_loader = torch.utils.data.DataLoader(test_data, batch_size=100)\n\n\nNow, let‚Äôs use a more complex regular expression. We want the counts for the pattern \".*_.*\" (i.e.¬†any name with a snail case such as train_loader):\n\ngit grep -c \".*_.*\"\n\n.gitignore:2\nsrc/new_file.py:22\nsrc/test_manuel.py:29\n\n\nLet‚Äôs print the first 3 results per file:\n\ngit grep -m 3 \".*_.*\"\n\n.gitignore:hidden_file\n.gitignore:search_cache/\nsrc/new_file.py:from datasets import load_dataset\nsrc/new_file.py:set5 = load_dataset('eugenesiow/Set5', 'bicubic_x4', split='validation')\nsrc/new_file.py:set5.column_names\nsrc/test_manuel.py:from torch.optim.lr_scheduler import StepLR\nsrc/test_manuel.py:    def __init__(self):\nsrc/test_manuel.py:        super(Net, self).__init__()\n\n\nAs you can see, our results also include __init__ which is not what we were looking for. So let‚Äôs exclude __:\n\ngit grep -m 3 -e \".*_.*\" --and --not -e \"__\"\n\n.gitignore:hidden_file\n.gitignore:search_cache/\nsrc/new_file.py:from datasets import load_dataset\nsrc/new_file.py:set5 = load_dataset('eugenesiow/Set5', 'bicubic_x4', split='validation')\nsrc/new_file.py:set5.column_names\nsrc/test_manuel.py:from torch.optim.lr_scheduler import StepLR\nsrc/test_manuel.py:        x = F.max_pool2d(x, 2)\nsrc/test_manuel.py:        output = F.log_softmax(x, dim=1)\n\n\n\nFor simple searches, you don‚Äôt have to use the -e flag before the pattern you are searching for. Here however, our command has gotten complex enough that we have to use it before each pattern.\n\nLet‚Äôs make sure this worked as expected:\n\ngit grep -c \".*_.*\"\necho \"---\"\ngit grep -c \"__\"\necho \"---\"\ngit grep -ce \".*_.*\" --and --not -e \"__\"\n\n.gitignore:2\nsrc/new_file.py:22\nsrc/test_manuel.py:29\n---\nsrc/test_manuel.py:2\n---\n.gitignore:2\nsrc/new_file.py:22\nsrc/test_manuel.py:27\n\n\nThere were 2 lines matching __ in src/test_manuel.py and we have indeed excluded them from our search.\nExtended regular expressions are also covered with the flag -E.\n\n\nSearching other trees\nSo far, we have searched the current version of tracked files, but we can just as easily search files at any commit.\nLet‚Äôs search for test in the tracked files 20 commits ago:\n\ngit grep test HEAD~20\n\nHEAD~20:adrian.txt:Adrian's test text file.\nHEAD~20:formerlyadrian.txt:Adrian's test text file.\nHEAD~20:ms/protocol.md:This is my test.\nHEAD~20:ms/smabraha.txt:This is a test file that I wanted to make, then push it somehow\nHEAD~20:redone17.txt:this is a test file from redone17\nHEAD~20:testAV1.txt:This is a test\nHEAD~20:text-collab.txt:This is the collaboration testing\n\n\n\nAs you can see, the file src/test_manuel.py is not in the results. Either it didn‚Äôt exist or it didn‚Äôt have the word test at that commit.\n\nIf you want to search tracked files AND untracked files, you need to use the --untracked flag.\nLet‚Äôs create a new (thus untracked) file with some content including the word test:\n\necho \"This is a test\" > newfile\n\nNow compare the following:\n\ngit grep -c test\n\nadrian.txt:1\nformerlyadrian.txt:1\nms/protocol.md:1\nms/smabraha.txt:1\nredone17.txt:1\nsrc/test_manuel.py:10\ntestAV1.txt:1\ntext-collab.txt:1\n\n\nwith:\n\ngit grep -c --untracked test\n\nadrian.txt:1\nformerlyadrian.txt:1\nms/protocol.md:1\nms/smabraha.txt:1\nnewfile:1\nredone17.txt:1\nsrc/test_manuel.py:10\ntestAV1.txt:1\ntext-collab.txt:1\n\n\n\nThis last result also returned our untracked file newfile.\n\nIf you want to search untracked and ignored files (meaning all your files), use the flags --untracked --no-exclude-standard.\nLet‚Äôs see what the .gitignore file contains:\n\ncat .gitignore\n\ndata/\noutput/\nhidden_file\nsearch_cache/\nsearch.qmd\nsearch.rmarkdown\n\n\nThe directory data is in .gitignore. This means that it is not under version control and it thus doesn‚Äôt exist in our repo (since we cloned our repo, we only have the version-controlled files). Let‚Äôs create it:\nmkdir data\nNow, let‚Äôs create a file in it that contains test:\n\necho \"And another test\" > data/file\n\nWe can rerun our previous two searches to verify that files excluded from version control are not searched:\n\ngit grep -c test\n\nadrian.txt:1\nformerlyadrian.txt:1\nms/protocol.md:1\nms/smabraha.txt:1\nredone17.txt:1\nsrc/test_manuel.py:10\ntestAV1.txt:1\ntext-collab.txt:1\n\n\n\ngit grep -c --untracked test\n\nadrian.txt:1\nformerlyadrian.txt:1\nms/protocol.md:1\nms/smabraha.txt:1\nnewfile:1\nredone17.txt:1\nsrc/test_manuel.py:10\ntestAV1.txt:1\ntext-collab.txt:1\n\n\nAnd now, let‚Äôs try:\n\ngit grep -c --untracked --no-exclude-standard test\n\nadrian.txt:1\ndata/file:1\nformerlyadrian.txt:1\nms/protocol.md:1\nms/smabraha.txt:1\nnewfile:1\nredone17.txt:1\nsearch.qmd:41\nsearch.rmarkdown:41\nsrc/test_manuel.py:10\ntestAV1.txt:1\ntext-collab.txt:1\n\n\n\ndata/file, despite being excluded from version control, is also searched.\n\n\n\nSearching all commits\nWe saw that git grep <pattern> <commit> can search a pattern in any commit. Now, what if we all to search all commits for a pattern?\nFor this, we pass the expression $(git rev-list --all) in lieu of <commit>.\ngit rev-list --all creates a list of all the commits in a way that can be used as an argument to other functions. The $() allows to run the expression inside it and pass the result as and argument.\nTo search for test in all the commits, we thus run:\ngit grep \"test\" $(git rev-list --all)\nI am not running this command has it has a huge output. Instead, I will limit the search to the last two commits:\n\ngit grep \"test\" $(git rev-list --all -2)\n\ne3cfb2ebbcb77e52851c32fbba1f6b8b0c788a29:adrian.txt:Adrian's test text file.\ne3cfb2ebbcb77e52851c32fbba1f6b8b0c788a29:formerlyadrian.txt:Adrian's test text file.\ne3cfb2ebbcb77e52851c32fbba1f6b8b0c788a29:ms/protocol.md:This is my test.\ne3cfb2ebbcb77e52851c32fbba1f6b8b0c788a29:ms/smabraha.txt:This is a test file that I wanted to make, then push it somehow\ne3cfb2ebbcb77e52851c32fbba1f6b8b0c788a29:redone17.txt:this is a test file from redone17\ne3cfb2ebbcb77e52851c32fbba1f6b8b0c788a29:src/test_manuel.py:def test(model, device, test_loader):\ne3cfb2ebbcb77e52851c32fbba1f6b8b0c788a29:src/test_manuel.py:    test_loss = 0\ne3cfb2ebbcb77e52851c32fbba1f6b8b0c788a29:src/test_manuel.py:        for data, target in test_loader:\ne3cfb2ebbcb77e52851c32fbba1f6b8b0c788a29:src/test_manuel.py:            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\ne3cfb2ebbcb77e52851c32fbba1f6b8b0c788a29:src/test_manuel.py:    test_loss /= len(test_loader.dataset)\ne3cfb2ebbcb77e52851c32fbba1f6b8b0c788a29:src/test_manuel.py:        test_loss, correct, len(test_loader.dataset),\ne3cfb2ebbcb77e52851c32fbba1f6b8b0c788a29:src/test_manuel.py:        100. * correct / len(test_loader.dataset)))\ne3cfb2ebbcb77e52851c32fbba1f6b8b0c788a29:src/test_manuel.py:    test_data = datasets.MNIST(\ne3cfb2ebbcb77e52851c32fbba1f6b8b0c788a29:src/test_manuel.py:    test_loader = torch.utils.data.DataLoader(test_data, batch_size=100)\ne3cfb2ebbcb77e52851c32fbba1f6b8b0c788a29:src/test_manuel.py:        test(model, device, test_loader)\ne3cfb2ebbcb77e52851c32fbba1f6b8b0c788a29:testAV1.txt:This is a test\ne3cfb2ebbcb77e52851c32fbba1f6b8b0c788a29:text-collab.txt:This is the collaboration testing\n15fdec6afb552e4ba2ec5f7a0b371543c9966c27:adrian.txt:Adrian's test text file.\n15fdec6afb552e4ba2ec5f7a0b371543c9966c27:formerlyadrian.txt:Adrian's test text file.\n15fdec6afb552e4ba2ec5f7a0b371543c9966c27:ms/protocol.md:This is my test.\n15fdec6afb552e4ba2ec5f7a0b371543c9966c27:ms/smabraha.txt:This is a test file that I wanted to make, then push it somehow\n15fdec6afb552e4ba2ec5f7a0b371543c9966c27:redone17.txt:this is a test file from redone17\n15fdec6afb552e4ba2ec5f7a0b371543c9966c27:src/test_manuel.py:def test(model, device, test_loader):\n15fdec6afb552e4ba2ec5f7a0b371543c9966c27:src/test_manuel.py:    test_loss = 0\n15fdec6afb552e4ba2ec5f7a0b371543c9966c27:src/test_manuel.py:        for data, target in test_loader:\n15fdec6afb552e4ba2ec5f7a0b371543c9966c27:src/test_manuel.py:            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n15fdec6afb552e4ba2ec5f7a0b371543c9966c27:src/test_manuel.py:    test_loss /= len(test_loader.dataset)\n15fdec6afb552e4ba2ec5f7a0b371543c9966c27:src/test_manuel.py:        test_loss, correct, len(test_loader.dataset),\n15fdec6afb552e4ba2ec5f7a0b371543c9966c27:src/test_manuel.py:        100. * correct / len(test_loader.dataset)))\n15fdec6afb552e4ba2ec5f7a0b371543c9966c27:src/test_manuel.py:    test_data = datasets.MNIST(\n15fdec6afb552e4ba2ec5f7a0b371543c9966c27:src/test_manuel.py:    test_loader = torch.utils.data.DataLoader(test_data, batch_size=100)\n15fdec6afb552e4ba2ec5f7a0b371543c9966c27:src/test_manuel.py:        test(model, device, test_loader)\n15fdec6afb552e4ba2ec5f7a0b371543c9966c27:testAV1.txt:This is a test\n15fdec6afb552e4ba2ec5f7a0b371543c9966c27:text-collab.txt:This is the collaboration testing\n\n\n\nIn combination with the fuzzy finder tool fzf, this can make finding a particular commit extremely easy.\nFor instance, the code below allows you to dynamically search in the result through incremental completion:\ngit grep \"test\" $(git rev-list --all) | fzf --cycle -i -e\nOr even better, you can automatically copy the short form of the hash of the selected commit to clipboard so that you can use it with git show, git checkout, etc.:\ngit grep \"test\" $(git rev-list --all) |\n    fzf --cycle -i -e |\n    cut -c 1-7 |\n    xclip -r -selection clipboard\n\nHere, I am using xclip to copy to the clipboard as I am on Linux. Depending on your OS you might need to use a different tool.\n\nOf course, you can create a function in your .bashrc file with such code so that you wouldn‚Äôt have to type it each time:\ngrep_all_commits () {\n    git grep \"$1\" $(git rev-list --all) |\n        fzf --cycle -i -e |\n        cut -c 1-7 |\n        xclip -r -selection clipboard\n}\nAlternatively, you can pass the result directly into whatever git command you want to use that commit for.\nHere is an example with git show:\ngit grep \"test\" $(git rev-list --all) |\n    fzf --cycle -i -e |\n    cut -c 1-7 |\n    git show\nAnd if you wanted to get really fancy, you could go with:\ngit grep \"test\" $(git rev-list --all) |\n    fzf --cycle -i -e --no-multi \\\n        --ansi --preview=\"$_viewGitLogLine\" \\\n        --header \"enter: view, C-c: copy hash\" \\\n        --bind \"enter:execute:$_viewGitLogLine | less -R\" \\\n        --bind \"ctrl-c:execute:$_gitLogLineToHash |\n        xclip -r -selection clipboard\"\nWrapped in a function:\ngrep_all_commits_preview () {\n    git grep \"$1\" $(git rev-list --all) |\n        fzf --cycle -i -e --no-multi \\\n            --ansi --preview=\"$_viewGitLogLine\" \\\n            --header \"enter: view, C-c: copy hash\" \\\n            --bind \"enter:execute:$_viewGitLogLine |\n              less -R\" \\\n            --bind \"ctrl-c:execute:$_gitLogLineToHash |\n        xclip -r -selection clipboard\"\n}\nThis last function allows you to search through all the results in an incremental fashion while displaying a preview of the selected diff (the changes made at that particular commit). If you want to see more of the diff than the preview displays, press <enter> (then q to quit the pager), if you want to copy the hash of a commit, press C-c (Control + c).\nWith this function, you can now instantly get a preview of the changes made to any line containing an expression for any file, at any commit, and copy the hash of the selected commit. This is really powerful.\n\n\n\nAliases\nIf you don‚Äôt want to type a series of flags all the time, you can configure aliases for Git. For instance, Alex Razoumov uses the alias git search for git grep --break --heading -n -i.\nLet‚Äôs add to it the -p flag. Here is how you would set this alias:\ngit config --global alias.search 'grep --break --heading -n -i -p'\n\nThis setting gets added to your main Git configuration file (on Linux, by default, at ~/.gitconfig).\n\nFrom there on, you can use your alias with:\n\ngit search test\n\nadrian.txt\n1:Adrian's test text file.\n\nformerlyadrian.txt\n1:Adrian's test text file.\n\nms/protocol.md\n6=using our awesome Rscript.\n9:This is my test.\n\nms/smabraha.txt\n1:This is a test file that I wanted to make, then push it somehow\n\nredone17.txt\n1:this is a test file from redone17\n\nsrc/test_manuel.py\n35=def train(model, device, train_loader, optimizer, epoch):\n50:def test(model, device, test_loader):\n52:    test_loss = 0\n55:        for data, target in test_loader:\n58:            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n62:    test_loss /= len(test_loader.dataset)\n64:    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n65:        test_loss, correct, len(test_loader.dataset),\n66:        100. * correct / len(test_loader.dataset)))\n69=def main():\n84:    test_data = datasets.MNIST(\n90:    test_loader = torch.utils.data.DataLoader(test_data, batch_size=100)\n97:        test(model, device, test_loader)\n\ntestAV1.txt\n1:This is a test\n\ntext-collab.txt\n1:This is the collaboration testing"
  },
  {
    "objectID": "git/practice_repo/search.html#searching-logs",
    "href": "git/practice_repo/search.html#searching-logs",
    "title": "Searching a version-controlled project",
    "section": "Searching logs",
    "text": "Searching logs\nThe second thing that can happen is that you are looking for some pattern in your version control logs.\n\ngit log\ngit log allows to get information on commit logs.\nBy default, it outputs all the commits of the current branch.\nLet‚Äôs show the logs of the last 3 commits:\n\ngit log -3\n\ncommit e3cfb2ebbcb77e52851c32fbba1f6b8b0c788a29\nAuthor: Marie-Helene Burle <marie.burle@westdri.ca>\nDate:   Sat Jan 7 22:32:23 2023 -0800\n\n    Update gitignore with Quarto files\n\ncommit 15fdec6afb552e4ba2ec5f7a0b371543c9966c27\nAuthor: Marie-Helene Burle <marie.burle@westgrid.ca>\nDate:   Fri Jan 6 10:18:28 2023 -0800\n\n    Update README.org\n\ncommit 15d4ee937db18fb26f84d17ec4be3f0c81614a1c\nAuthor: Marie-Helene Burle <marie.burle@westgrid.ca>\nDate:   Wed Mar 16 10:55:28 2022 -0700\n\n    change values training\n\n\nThe output can be customized thanks to a plethora of options.\nFor instance, here are the logs of the last 15 commits, in a graph, with one line per commit:\n\ngit log --graph --oneline -n 15\n\n* e3cfb2e Update gitignore with Quarto files\n* 15fdec6 Update README.org\n* 15d4ee9 change values training\n* 06efa34 add lots of code\n* 1457143 remove stupid line\n* 711e1dc add real py content to test_manual.py\n* 90016aa adding new python file\n*   2c0f612 Merge branch 'main' of github.com:prosoitos/git_workshop_collab\n|\\  \n| *   6f7d03d Merge branch 'main' of https://github.com/prosoitos/git_workshop_collab into main\n| |\\  \n| * \\   3c53269 Merge branch 'main' of https://github.com/prosoitos/git_workshop_collab into main\n| |\\ \\  \n| * \\ \\   eef5b78 Merge branch 'main' of https://github.com/prosoitos/git_workshop_collab into main\n| |\\ \\ \\  \n| * | | | a55ca0d new comment add just as test\n* | | | |   dedc94f Merge branch 'main' of github.com:prosoitos/git_workshop_collab\n|\\ \\ \\ \\ \\  \n| | |_|_|/  \n| |/| | |   \n| * | | |   b861a65 Merge branch 'main' of https://github.com/prosoitos/git_workshop_collab\n| |\\ \\ \\ \\  \n| | | |_|/  \n| | |/| |   \n| | * | |   35e8d5a Merge branch 'main' of github.com:prosoitos/git_workshop_collab\n| | |\\ \\ \\  \n| | | | |/  \n| | | |/|   \n\n\nBut git log has also flags that allow to search for patterns.\n\n\nSearching commit messages\nOne of the reasons it is so important to write informative commit messages is that they are key to finding commits later on.\nTo look for a pattern in all your commit messages, use git log --grep=<pattern>.\nLet‚Äôs look for test in the commit messages and limit the output to 3 commits:\n\ngit log --grep=test -3\n\ncommit 711e1dc53011e5071b17dc7c35b516f6e066f396\nAuthor: Marie-Helene Burle <marie.burle@westgrid.ca>\nDate:   Tue Mar 15 11:52:48 2022 -0700\n\n    add real py content to test_manual.py\n\ncommit a55ca0d60d82578c94bd49fc4ca987727b851216\nAuthor: Manuelhrokr <zl.manuel@protonmail.com>\nDate:   Thu Feb 17 15:19:42 2022 -0700\n\n    new comment add just as test\n\ncommit ea74e46f487fba09c31524a110fdf060796e3cf8\nAuthor: mpkin <mikin@physics.ubc.ca>\nDate:   Thu Sep 23 14:51:24 2021 -0700\n\n    Add test_mk.txt\n\n\nFor a more compact output:\n\ngit log --grep=\"test\" -3 --oneline\n\n711e1dc add real py content to test_manual.py\na55ca0d new comment add just as test\nea74e46 Add test_mk.txt\n\n\n\nHere too you can use this in combination to fzf with for instance:\ngit log --grep=\"test\" | fzf --cycle -i -e\nOr:\ngit log --grep=\"test\" --oneline |\n    fzf --cycle -i -e --no-multi \\\n        --ansi --preview=\"$_viewGitLogLine\" \\\n        --header \"enter: view, C-c: copy hash\" \\\n        --bind \"enter:execute:$_viewGitLogLine | less -R\" \\\n        --bind \"ctrl-c:execute:$_gitLogLineToHash |\n        xclip -r -selection clipboard\"\n\n\n\nChanges made to a pattern\nRemember that test was present in the file src/test_manuel.py. If we want to see when the pattern was first created and then each time it was modified, we use the -L flag in this fashion:\ngit log -L :<pattern>:file\nIn our case:\n\ngit log -L :test:src/test_manuel.py\n\ncommit 711e1dc53011e5071b17dc7c35b516f6e066f396\nAuthor: Marie-Helene Burle <marie.burle@westgrid.ca>\nDate:   Tue Mar 15 11:52:48 2022 -0700\n\n    add real py content to test_manual.py\n\ndiff --git a/src/test_manuel.py b/src/test_manuel.py\n--- a/src/test_manuel.py\n+++ b/src/test_manuel.py\n@@ -1,1 +50,19 @@\n-test\n+def test(model, device, test_loader):\n+    model.eval()\n+    test_loss = 0\n+    correct = 0\n+    with torch.no_grad():\n+        for data, target in test_loader:\n+            data, target = data.to(device), target.to(device)\n+            output = model(data)\n+            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n+            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n+            correct += pred.eq(target.view_as(pred)).sum().item()\n+\n+    test_loss /= len(test_loader.dataset)\n+\n+    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n+        test_loss, correct, len(test_loader.dataset),\n+        100. * correct / len(test_loader.dataset)))\n+\n+\n\ncommit 90016aa3ed3a6cf71e206392bbf10adfe1a14c17\nAuthor: Manuelhrokr <zl.manuel@protonmail.com>\nDate:   Thu Feb 17 15:33:03 2022 -0700\n\n    adding new python file\n\ndiff --git a/src/test_manuel.py b/src/test_manuel.py\n--- /dev/null\n+++ b/src/test_manuel.py\n@@ -0,0 +1,1 @@\n+test\n\n\nThis is very useful if you want to see, for instance, changes made to a function in a script.\n\n\nChanges in number of occurrences of a pattern\nNow, if we want to list all commits that created a change in the number of occurrences of test in our project, we run:\n\ngit log -S test --oneline\n\n711e1dc add real py content to test_manual.py\n90016aa adding new python file\n652faa5 delete my file\nb684eac Deleted file\n6717236 For collab\nca1845d delete alex.txt\n6b56198 editing adrians text file\n01a7358 test dtrad\ne44a454 Create testAV1.txt\n5ee88e6 For collab\ncf3d4ea Collab-test\n13faa1e test, test\n0366115 Adrian's test file\n9ebd3ce This is my test\n6dfefa8 create redone17.txt\ne43163c added alex.txt\n\n\nThis can be useful to identify the commit you need."
  },
  {
    "objectID": "git/practice_repo/search.html#tldl",
    "href": "git/practice_repo/search.html#tldl",
    "title": "Searching a version-controlled project",
    "section": "TLDL",
    "text": "TLDL\nHere are the search functions you are the most likely to use:\n\nSearch for a pattern in the current version of your tracked files:\n\ngit grep <pattern>\n\nSearch for a pattern in your files at a certain commit:\n\ngit grep <pattern> <commit>\n\nSearch for a pattern in your files in all the commits:\n\ngit grep <pattern> $(git rev-list --all)\n\nSearch for a pattern in your commit messages:\n\ngit log --grep=<pattern>\nNow you should be able to find pretty much anything in your projects and their histories."
  },
  {
    "objectID": "git/remotes.html",
    "href": "git/remotes.html",
    "title": "Remotes",
    "section": "",
    "text": "Remotes are copies of a project and its history.\nThey can be located anywhere, including on external drive or on the same machine as the project, although they are often on a different machine to serve as backup, or on a network (e.g.¬†internet) to serve as a syncing hub for collaborations.\nPopular online Git repository managers & hosting services:\n\nGitHub\nGitLab\nBitbucket\n\nLet‚Äôs see how to create and manage remotes."
  },
  {
    "objectID": "git/remotes.html#creating-a-remote-on-github",
    "href": "git/remotes.html#creating-a-remote-on-github",
    "title": "Remotes",
    "section": "Creating a remote on GitHub",
    "text": "Creating a remote on GitHub\n\nCreate a free GitHub account\nIf you don‚Äôt already have one, sign up for a free GitHub account.\n\nTo avoid having to type your password all the time, you should set up SSH for your account.\n\n\n\nCreate an empty repository on GitHub\n\nGo to the GitHub website, login, and go to your home page.\nLook for the Repositories tab & click the green New button.\nEnter the name you want for your repo, without spaces.\nMake the repository public or private.\n\n\n\nLink empty repository to your repo\nClick on the Code green drop-down button, select SSH if you have set SSH for your GitHub account or HTTPS and copy the address.\nIn the command line, cd inside your project, and add the remote:\ngit remote add <remote-name> <remote-address>\nremote-name is a convenience name to identify that remote. You can choose any name, but since Git automatically call the remote origin when you clone a repo, it is common practice to use origin as the name for the first remote.\n\nExample (using an SSH address):\n\ngit remote add origin git@github.com:<user>/<repo>.git\n\nExample (using an HTTPS address):\n\ngit remote add origin https://github.com/<user>/<repo>.git"
  },
  {
    "objectID": "git/remotes.html#getting-information-on-remotes",
    "href": "git/remotes.html#getting-information-on-remotes",
    "title": "Remotes",
    "section": "Getting information on remotes",
    "text": "Getting information on remotes\nList remotes:\ngit remote\nList remotes with their addresses:\ngit remote -v\nGet more information on a remote:\ngit remote show <remote-name>\n\nExample:\n\ngit remote show origin"
  },
  {
    "objectID": "git/remotes.html#managing-remotes",
    "href": "git/remotes.html#managing-remotes",
    "title": "Remotes",
    "section": "Managing remotes",
    "text": "Managing remotes\nRename a remote:\ngit remote rename <old-remote-name> <new-remote-name>\nDelete a remote:\ngit remote remove <remote-name>\nChange the address of a remote:\ngit remote set-url <remote-name> <new-url> [<old-url>]"
  },
  {
    "objectID": "git/remotes.html#getting-data-from-a-remote",
    "href": "git/remotes.html#getting-data-from-a-remote",
    "title": "Remotes",
    "section": "Getting data from a remote",
    "text": "Getting data from a remote\nIf you collaborate on a project, you have to get the data added by your teammates to keep your local project up to date.\nTo download new data from a remote, you have 2 options:\n\ngit fetch\ngit pull\n\n\nFetching changes\nFetching downloads the data from a remote that you don‚Äôt already have in your local version of the project:\ngit fetch <remote-name>\nThe branches on the remote are now accessible locally as <remote-name>/<branch>. You can inspect them or you can merge them into your local branches.\n\nExample:\n\ngit fetch origin\n\n\nPulling changes\nPulling fetches the changes & merges them onto your local branches:\ngit pull <remote-name> <branch>\n\nExample:\n\ngit pull origin main\nIf your branch is already tracking a remote branch, you can omit the arguments:\ngit pull"
  },
  {
    "objectID": "git/remotes.html#pushing-to-a-remote",
    "href": "git/remotes.html#pushing-to-a-remote",
    "title": "Remotes",
    "section": "Pushing to a remote",
    "text": "Pushing to a remote\nUploading data to the remote is called pushing:\ngit push <remote-name> <branch-name>\n\nExample:\n\ngit push origin main\nYou can set an upstream branch to track a local branch with the -u flag:\ngit push -u <remote-name> <branch-name>\n\nExample:\n\ngit push -u origin main\nFrom now on, all you have to run when you are on main is:\ngit push\n \n\nby jscript"
  },
  {
    "objectID": "git/stash.html",
    "href": "git/stash.html",
    "title": "Stashing",
    "section": "",
    "text": "Stashing is a way to put changes aside for some time. Changes can be reapplied later (and/or applied on other branches).\nWhy would you want to do that? And how?"
  },
  {
    "objectID": "git/stash.html#what-are-git-stashes",
    "href": "git/stash.html#what-are-git-stashes",
    "title": "Stashing",
    "section": "What are Git stashes?",
    "text": "What are Git stashes?\nHaving a clean working tree is necessary for many Git operations and recommended for others. If you have changes from an unfinished piece of work getting in the way, you could do an ugly commit to get rid of them. But if you care about having an organized history with meaningful commits (or if you don‚Äôt want others to see your messy drafts), stashing is a much better alternative."
  },
  {
    "objectID": "git/stash.html#creating-a-stash",
    "href": "git/stash.html#creating-a-stash",
    "title": "Stashing",
    "section": "Creating a stash",
    "text": "Creating a stash\nYou can stash the changes in your modified files with:\ngit stash\nTo also include new (untracked) files, you have to use the -u flag:\ngit stash -u"
  },
  {
    "objectID": "git/stash.html#listing-stashes",
    "href": "git/stash.html#listing-stashes",
    "title": "Stashing",
    "section": "Listing stashes",
    "text": "Listing stashes\nOf course, you don‚Äôt want to lose or forget about your stashes.\nYou can list them with:\ngit stash list\nStashes are also shown when you run git log (with any of its variations) with the --all flag.\n\nExample:\n\ngit log --graph --oneline --all"
  },
  {
    "objectID": "git/stash.html#re-applying-changes-from-a-stash",
    "href": "git/stash.html#re-applying-changes-from-a-stash",
    "title": "Stashing",
    "section": "Re-applying changes from a stash",
    "text": "Re-applying changes from a stash\nTo re-apply the changes (or apply them on another branch), you run:\ngit stash apply\nIf you had staged changes, you can also restore the state of the index by running instead:\ngit stash apply --index\nIf you created multiple stashes, this will apply the last one you created. If this is not what you want, you have to specify which stash you want to use with the reflog syntax:\n\nstash@{0} is the last stash (so you can omit it)\n\nstash@{1} is the one before it\n\nstash@{2} the one before that\n\netc.\n\n\nTo apply the stash before last:\n\ngit stash apply stash@{1}"
  },
  {
    "objectID": "git/stash.html#deleting-a-stash",
    "href": "git/stash.html#deleting-a-stash",
    "title": "Stashing",
    "section": "Deleting a stash",
    "text": "Deleting a stash\nYou delete the last (or the only) stash with:\ngit stash drop\nHere again, if you want to delete another stash, specify it with its reflog index.\n\nTo delete the antepenultimate stash:\n\ngit stash drop stash@{2}\nYou can apply and delete a stash at the same time with:\ngit stash pop\nThis is convenient, but less flexible."
  },
  {
    "objectID": "git/tags.html",
    "href": "git/tags.html",
    "title": "Tags",
    "section": "",
    "text": "When you reach an important point in the development of a project, it is convenient to be able to identify the next commit easily. Rather than having to look for it through date, commit message, or hash, you can create a tag: a pointer to that commit."
  },
  {
    "objectID": "git/tags.html#leightweight-tag",
    "href": "git/tags.html#leightweight-tag",
    "title": "Tags",
    "section": "Leightweight tag",
    "text": "Leightweight tag\nYou create a tag with:\ngit tag <tag-name>\n\nExample:\n\ngit tag J_Climate_2009\n\nAs you keep developing the project and create new commits, the branch and HEAD pointers will move along, but the tag remains on your important commit.\n\nAt any time, you can get info on the commit thus tagged with:\ngit show J_Climate_2009\nOr you can check it out with:\ngit checkout J_Climate_2009"
  },
  {
    "objectID": "git/tags.html#annotated-tag",
    "href": "git/tags.html#annotated-tag",
    "title": "Tags",
    "section": "Annotated tag",
    "text": "Annotated tag\nA more sophisticated form of tag comes with a message:\ngit tag -a <tag-name> -m \"<message>\"\ngit tag -a J_Climate_2009 -m \"State of project at the publication of paper\""
  },
  {
    "objectID": "git/tags.html#list-tags",
    "href": "git/tags.html#list-tags",
    "title": "Tags",
    "section": "List tags",
    "text": "List tags\ngit tag"
  },
  {
    "objectID": "git/tags.html#deleting-tags",
    "href": "git/tags.html#deleting-tags",
    "title": "Tags",
    "section": "Deleting tags",
    "text": "Deleting tags\ngit tag -d <tag-name>\ngit tag -d J_Climate_2009"
  },
  {
    "objectID": "git/three_trees.html",
    "href": "git/three_trees.html",
    "title": "The three trees of Git",
    "section": "",
    "text": "One useful mental representation of the functioning of Git is to imagine three file trees."
  },
  {
    "objectID": "git/three_trees.html#the-three-trees-of-git",
    "href": "git/three_trees.html#the-three-trees-of-git",
    "title": "The three trees of Git",
    "section": "The three trees of Git",
    "text": "The three trees of Git\n\nWorking directory\nLet‚Äôs imagine that you are starting to work on a project.\nFirst, you create a directory.\nIn it, you create several sub-directories.\nIn those, you create a number of files.\nYou can open these files, read them, edit them, etc. This is something you are very familiar with.\nIn the Git world, this is the working directory or working tree of the project.\nThat is: an uncompressed version of your files that you can access and edit.\nYou can think of it as a sandbox because this is where you can experiment with the project. This is where the project gets developed.\nNow, Git has two other important pieces in its architecture.\n\n\nIndex\nIf you want the project history to be useful to future you, it has to be nice and tidy. You don‚Äôt want to record snapshots haphazardly or you will never be able to find anything back.\nBefore you record a snapshot, you carefully select the elements of the project as it is now that would be useful to write to the project history together. The index or staging area is what allows to do that: it contains the suggested future snapshot.\n\n\nHEAD\nFinally, the last tree in Git architecture is one snapshot in the project history that serves as a reference version of the project: if you want to see what you have been experimenting on in your ‚Äúsandbox‚Äù, you need to compare the state of the working directory with some snapshot.\nRemember that HEAD is a pointer pointing at a branch, that a branch is itself a pointer pointing at a commit, and finally that a commit is a Git object pointing at a snapshot. When the HEAD pointer moves around, whatever snapshot it points to populates the HEAD tree.\nAs we saw earlier, when you create a commit, HEAD automatically points to the new commit. So the HEAD tree is often filled with the last snapshot you created. But‚Äîas we will see later‚Äîwe can move the HEAD pointer around through other ways. So the HEAD tree can be populated by any snapshot in your project history."
  },
  {
    "objectID": "git/three_trees.html#status-of-the-three-trees",
    "href": "git/three_trees.html#status-of-the-three-trees",
    "title": "The three trees of Git",
    "section": "Status of the three trees",
    "text": "Status of the three trees\nTo display the status of these trees, you run:\ngit status"
  },
  {
    "objectID": "git/three_trees.html#three-trees-in-action",
    "href": "git/three_trees.html#three-trees-in-action",
    "title": "The three trees of Git",
    "section": "Three trees in action",
    "text": "Three trees in action\n\nClean working tree\nWe say that the working tree is ‚Äúclean‚Äù when all changes tracked by Git were staged and committed:\n\nHere is an example for a project with a single file called File at version v1.\n\n\n\n\nMaking changes to the working tree\nWhen you edit files in your project, you make changes in the working directory or working tree.\n\nFor instance, you make changes to File. Let‚Äôs say that it is now at version v2:\n\n\nThe other two trees remain at version v1.\nIf you run git status, this is what you get:\nOn branch main\nChanges not staged for commit:\n  (use \"git add <file>...\" to update what will be committed)\n  (use \"git restore <file>...\" to discard changes in working directory)\n        modified:   File\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\n\n\nStaging changes\nYou stage that file (meaning that you will include the changes of that file in the next commit) with:\ngit add File\nAfter which, your Git trees look like this:\n\nNow, the index also has File at version v2 and git status returns:\nOn branch main\nChanges to be committed:\n  (use \"git restore --staged <file>...\" to unstage)\n        modified:   File\n\n\nCommitting changes\nFinally, you create a snapshot and the commit pointing to it‚Äîrecording the staged changes to history‚Äîwith:\ngit commit -m \"Added File\"\n-m is a flag that allows to provide the commit message directly in the command line. If you don‚Äôt use it, Git will open a text editor so that you can type the message. Without a message, there can be no commit.\nNow your trees look like this:\n\nOur working tree is clean again and git status returns:\nOn branch main\nnothing to commit, working tree clean\nThis means that there are no uncommitted changes in the working tree or the staging area: all the changes have been written to history.\n\nYou don‚Äôt have to stage all the changes in the working directory before making a commit; that is actually the whole point of the staging area.\nThis means that the working directory is not necessarily clean after you have created a new commit."
  },
  {
    "objectID": "git/time_travel.html",
    "href": "git/time_travel.html",
    "title": "Time travel",
    "section": "",
    "text": "It‚Äôs great to record history, but if we don‚Äôt know how to make use of it, it isn‚Äôt exactly useful.\nIn this workshop, you will travel through the history of a project."
  },
  {
    "objectID": "git/time_travel.html#looking-at-the-past-without-travelling",
    "href": "git/time_travel.html#looking-at-the-past-without-travelling",
    "title": "Time travel",
    "section": "Looking at the past without travelling",
    "text": "Looking at the past without travelling\nHEAD is a little file in the .git directory which points to our current location in the Git history.\nYou already saw multiple ways to have a glimpse at your project history without moving HEAD:\n\ngit log and its many variations shows a list or a tree of your commits\ngit show displays information about a Git object such as a past commit\n\nThose are useful options, but Git allows you to really travel in your project history: HEAD can be moved around with the command git checkout to point to any branch, tag, or commit."
  },
  {
    "objectID": "git/time_travel.html#travelling-through-history",
    "href": "git/time_travel.html#travelling-through-history",
    "title": "Time travel",
    "section": "Travelling through history",
    "text": "Travelling through history\nAs soon as you move HEAD to a new Git object with git checkout, the working directory and the index get updated to match the snapshot that Git object is pointing to. That means that your project will suddenly be back to the state in which it was when you committed that snapshot.\n\nMoving HEAD\nLet‚Äôs give this a try and move HEAD to a past commit.\n\nIdentifying the commit we want to move HEAD to\nYou can use the ~ notation:\n\nHEAD~ or HEAD~1 means the commit which precedes the one HEAD is pointing to.\nHEAD~2 means the commit before that.\nHEAD~3 refers to the 3rd commit before the current commit.\netc.\n\nYou can also run git log to find the hash of your commit of interest.\n\n\n\nDetached HEAD\nLet‚Äôs look at a hypothetical scenario to see what happens when you checkout a commit.\nThis is our starting point:\n\nNow, we move HEAD to the commit 31fukv1:\ngit checkout 31fukv1\n\nNotice that HEAD is not pointing at a branch anymore: it is pointing directly at a commit. This is called a detached HEAD state and Git will give you plenty of warnings about it.\nIf you look at your files, you will see that they match their state when you committed 31fukv1: your working directory got updated to match the current position of HEAD.\nYou can look at your project at that point in its history, then go back to your main branch (here main) with:\ngit checkout main\n\nAnd that‚Äôs that. You took a little trip into the past just to have a look, then came back to ‚Äúthe present‚Äù and all went well.\n\n\nCreating commits from a detached HEAD\nNow, when you are at commit 31fukv1, maybe you wanted to try something.\n\nYou can safely try anything you want: when you checkout main to come back to ‚Äúthe present‚Äù, those experimental changes will get lost.\nBut what if you want to keep those changes you made at 31fukv1?\nIn that case, as you always do, you create a commit to archive those changes into the project history:\n\nYou can make more commits:\n\nThe thing is that you are still in this detached HEAD state. HEAD is not pointing to a branch as it normally is. Is this a problem?\n\nBad workflow\nWell, it becomes a problem if you checkout main from there:\n\nIf you decide that you don‚Äôt care about those commits after all, then all is good. But if you care about them, this is a bad situation because those commits you created when you were in a detached HEAD state are now left behind: they are not in the history of any branch or tag.\nThis is bad for three reasons:\n\nThose commits will not show when you run git log, so it is easy to forget about them.\nIt is not easy to go back to them because there aren‚Äôt any tag or branch that you can checkout.\nThe garbage collection (which runs every 30 days by default) will delete those commits which are not on any branch or tag. So you will ultimately loose them.\n\n\n\nGood workflow\nA good workflow would have been to create a new branch on 31fukv1 (let‚Äôs call it alternative) and switch to it. That way, the commits created from 31fukv1 are on a branch and they will not be deleted by the next garbage collection:\n\nIn this good workflow, it is totally safe to switch back to main:\n\n\nIf you want to list the commits 23f481q and rthy7wg when you are back on main, you need to run git log with the --all flag.\n\n\n\nRecovering commits left behind\nWhat if you left commits behind (not on a branch)?\nYou can retrieve their hash by running:\ngit reflog\nThis tracks the position of HEAD over time.\nYou can then checkout the commit you care about (so you are going back to a detached HEAD state):\ngit checkout <hash-abandonned-commit>\nThis puts you back into a situation where you can rescue the commit(s) by creating a branch:\nDo this as soon as you can since those commits will be deleted at the next garbage collection (and finding their hash with git reflog will become increasingly complicated as you wait)."
  },
  {
    "objectID": "git/tools.html",
    "href": "git/tools.html",
    "title": "Tools for a friendlier Git",
    "section": "",
    "text": "Two great open-source tools to work with Git in a nice visual manner while remaining in the command line."
  },
  {
    "objectID": "git/tools.html#fzf",
    "href": "git/tools.html#fzf",
    "title": "Tools for a friendlier Git",
    "section": "fzf",
    "text": "fzf\nfzf is a fantastic multi-platform command line fuzzy finder with a huge versatility.\n\nIn this video, I demo quickly how it can be used with Git:"
  },
  {
    "objectID": "git/tools.html#lazygit",
    "href": "git/tools.html#lazygit",
    "title": "Tools for a friendlier Git",
    "section": "lazygit",
    "text": "lazygit\nlazygit is an excellent multi-platform user interface for Git which works in the command line."
  },
  {
    "objectID": "git/undo.html",
    "href": "git/undo.html",
    "title": "Undoing",
    "section": "",
    "text": "This workshop covers a few of the ways actions can be undone in Git."
  },
  {
    "objectID": "git/undo.html#amending-the-last-commit",
    "href": "git/undo.html#amending-the-last-commit",
    "title": "Undoing",
    "section": "Amending the last commit",
    "text": "Amending the last commit\nHere is a common scenario: you make a commit, then realize that you forgot to include some changes in that commit; or you aren‚Äôt happy with the commit message; or both. You can edit your latest commit with the --amend flag:\ngit commit --amend\nThis will hide your last commit (as if it had never happened), add the changes in the staging area (if any) to the changes in that last commit, open a text editor showing the message of the last commit (you can keep or edit that message), and create a new commit which replaces your last commit.\nSo if you only want to change the commit message, run that command with an empty staging area. If you want to add changes to the last commit, stage them, then run the command.\nIn short, what this does is to replace your last commit with a new commit with the added changes and/or edited message. This prevents having a messy history with commits of the type ‚Äúadd missing file to last commit‚Äù or ‚Äúbetter message for last commit: bla bla bla‚Äù. If you made a typo in your last commit message (and if you care about having a nice, clean history), you can fix it easily this way.\n\n\nYour turn:\n\n\nRun git log --oneline (notice the hash of the last commit)\nEdit your last commit message\nRun git log --oneline again to see that your last commit now has a new hash (so it is a different commit) and a new message\nNow, make some change in your project (add a file, or edit a file‚Ä¶ any change you want)\nThen add that new change to your last commit without changing the message"
  },
  {
    "objectID": "git/undo.html#unstaging",
    "href": "git/undo.html#unstaging",
    "title": "Undoing",
    "section": "Unstaging",
    "text": "Unstaging\nYou know how to add changes to the staging area. But what if you want to unstage changes? You don‚Äôt want to loose those changes. But you staged them and then realized that you don‚Äôt want to include them in your next commit after all.\nHere is the command for this:\ngit restore --staged <file>\n\nNote that Git will remind you about the existence of this command when you run git status and have staged files ready to be committed.\n\n\n\nYour turn:\n\n\nMake changes to one of your existing files\nStage that file\nRun git status and notice Git‚Äôs reminder about this command\nUnstage the changes on that file"
  },
  {
    "objectID": "git/undo.html#erasing-modifications",
    "href": "git/undo.html#erasing-modifications",
    "title": "Undoing",
    "section": "Erasing modifications",
    "text": "Erasing modifications\nNow, what if you made changes to a file, then decide that they were no good? You can easily get rid of these edits and restore the file to its last committed version:\ngit restore <file>\n\nNote that Git will tell you about this command when you run git status and have unstaged changes in tracked files.\n\n\n\nYour turn:\n\n\nRun git status again and notice Git‚Äôs reminder about the existence of this command\nErase that last change of yours\nOpen your file and notice that your edits are gone\n\n\n\nAs you just experienced, this command leads to data loss.\nThose last edits are gone and unrecoverable. Be very careful when using this!"
  },
  {
    "objectID": "git/undo.html#reverting",
    "href": "git/undo.html#reverting",
    "title": "Undoing",
    "section": "Reverting",
    "text": "Reverting\n\nThe working directory must be clean before you can use git revert.\n\ngit revert creates a new commit which reverses the effect of past commit(s).\n\nTo revert the last commit (current location of HEAD):\n\ngit revert HEAD\n\nYou can use the hash of the last commit instead of HEAD.\n\n\nTo revert the last two commits:\n\ngit revert HEAD~\n\nHEAD~ is equivalent to HEAD~1 and means the commit before the one HEAD is on.\nHere too of course, you can use the hash of the commit before last instead of HEAD~.\n\n\nTo revert the last three commits:\n\ngit revert HEAD~2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "WestDRI",
    "section": "",
    "text": "Git\nVersion control with Git and collaboration with GitHub/GitLab\n\n\n\n\nR\nResearch computing in R\n\n\n\n\nJulia\nResearch computing in the Julia programming language\n\n\n\n\nPython\nResearch computing in Python\n\n\n\n\n\n\nMachine learning\nDeep learning with the PyTorch framework\n\n\n\n\nBash\nBash/Zsh scripting, Unix commands, and useful CLI utilities\n\n\n\n\nResearch tools\nOpen source tools for computing and publishing\n\n\n\n¬†\n\n\n\n\n\nMain WestDRI website:\nThis site contains content by Marie-H√©l√®ne Burle. To view all training material, please visit WestDRI‚Äôs main website."
  },
  {
    "objectID": "julia/basics.html",
    "href": "julia/basics.html",
    "title": "Basics of the Julia language",
    "section": "",
    "text": "Comments do not get evaluated by Julia and are for humans only.\n\n# Comments in Julia are identified by hastags\n\n\n#=\nComments can also spread over multiple lines\nif you enclose them with this syntax\n=#\n\n\nx = 2          # Comments can be added at the end of lines\n\n2"
  },
  {
    "objectID": "julia/basics.html#basic-operations",
    "href": "julia/basics.html#basic-operations",
    "title": "Basics of the Julia language",
    "section": "Basic operations",
    "text": "Basic operations\n\n# By default, Julia returns the output\n2 + 3\n\n5\n\n\n\n# Trailing semi-colons suppress the output\n3 + 7;\n\n\n# Alternative syntax that can be used with operators\n+(2, 5)\n\n7\n\n\n\n# Updating operators\na = 3\na += 8    # this is the same as a = a + 8\n\n11\n\n\n\n# Operator precedence follows standard rules\n3 + 2 ^ 3 * 10\n\n83\n\n\n\nMore exotic operators\n\n# Usual division\n6 / 2\n\n3.0\n\n\n\n# Inverse division\n2 \\ 6\n\n3.0\n\n\n\n# Integer division (division truncated to an integer)\n7 √∑ 2\n\n3\n\n\n\n# Remainder\n7 % 2        # equivalent to rem(7, 2)\n\n1\n\n\n\n# Fraction\n4//8\n\n1//2\n\n\n\n# Julia supports fraction operations\n1//2 + 3//4\n\n5//4"
  },
  {
    "objectID": "julia/basics.html#variables",
    "href": "julia/basics.html#variables",
    "title": "Basics of the Julia language",
    "section": "Variables",
    "text": "Variables\n\n\nfrom xkcd.com\n\nA variable is a name bound to a value:\n\na = 3;\n\nIt can be called:\n\na\n\n3\n\n\nOr used in expressions:\n\na + 2\n\n5\n\n\n\nAssignment\nYou can re-assign new values to variables:\n\na = 3;\na = -8.2;\na\n\n-8.2\n\n\nEven values of a different type:\n\na = \"a is now a string\"\n\n\"a is now a string\"\n\n\nYou can define multiple variables at once:\n\na, b, c = 1, 2, 3\nb\n\n2\n\n\n\n\nVariable names\nThese names are extremely flexible and can use Unicode character:\n\\omega       # press TAB\n\\sum         # press TAB\n\\sqrt        # press TAB\n\\in          # press TAB\n\\:phone:     # press TAB\n\nŒ¥ = 8.5;\nüêå = 3;\nŒ¥ + üêå\n\n11.5\n\n\nAdmittedly, using emojis doesn‚Äôt seem very useful, but using Greek letters to write equations really makes Julia a great mathematical language:\n\nœÉ = 3\nŒ¥ = œÄ\nœï = 8\n\n(5œÉ + 3Œ¥) / œï\n\n3.0530972450961724\n\n\n\nNote how the multiplication operator can be omitted when this does not lead to confusion. Also note how the mathematical constant œÄ is available in Julia without having to load any module.\n\nIf you want to know how to type a symbol, ask Julia: type ? and paste it in the REPL.\nThe only hard rules for variable names are:\n\nThey must begin with a letter or an underscore,\nThey cannot take the names of built-in keywords such as if, do, try, else,\nThey cannot take the names of built-in constants (e.g.¬†œÄ) and keywords in use in a session.\n\n\nWe thus get an error here:\n\n\nfalse = 3\n\nLoadError: syntax: invalid assignment location \"false\" around In[24]:1\n\n\nIn addition, the Julia Style Guide recommends to follow these conventions:\n\nUse lower case,\nWord separation can be indicated by underscores, but better not to use them if the names can be read easily enough without them.\n\n\n\nThe ans variable\nThe keyword ans is a variable which, in the REPL, takes the value of the last computation:\na = 3 ^ 2;\nans + 1\n10\n\n\nPrinting\nTo print the value of a variable in an interactive session, you only need to call it:\n\na = 3;\na\n\n3\n\n\nIn non interactive sessions, you have to use the println function:\n\nprintln(a)\n\n3"
  },
  {
    "objectID": "julia/basics.html#quotes",
    "href": "julia/basics.html#quotes",
    "title": "Basics of the Julia language",
    "section": "Quotes",
    "text": "Quotes\nNote the difference between single and double quotes:\n\ntypeof(\"a\")\n\nString\n\n\n\ntypeof('a')\n\nChar\n\n\n\n\"This is a string\"\n\n\"This is a string\"\n\n\n\n'This is not a sring'\n\nLoadError: syntax: character literal contains multiple characters\n\n\n\nWe got an error here since ' is used for the character type and can thus only contain a single character.\n\n\n'a'\n\n'a': ASCII/Unicode U+0061 (category Ll: Letter, lowercase)"
  },
  {
    "objectID": "julia/collections.html",
    "href": "julia/collections.html",
    "title": "Collections",
    "section": "",
    "text": "Values can be stored in collections. This workshop introduces tuples, dictionaries, sets, and arrays in Julia."
  },
  {
    "objectID": "julia/collections.html#tuples",
    "href": "julia/collections.html#tuples",
    "title": "Collections",
    "section": "Tuples",
    "text": "Tuples\nTuples are immutable, indexable, and possibly heterogeneous collections of elements. The order of elements matters.\n\n# Possibly heterogeneous (values can be of different types)\ntypeof((2, 'a', 1.0, \"test\"))\n\nTuple{Int64, Char, Float64, String}\n\n\n\n# Indexable (note that indexing in Julia starts with 1)\nx = (2, 'a', 1.0, \"test\");\nx[3]\n\n1.0\n\n\n\n# Immutable (they cannot be modified)\n# So this returns an error\nx[3] = 8\n\nLoadError: MethodError: no method matching setindex!(::Tuple{Int64, Char, Float64, String}, ::Int64, ::Int64)\n\n\n\nNamed tuples\nTuples can have named components:\n\ntypeof((a=2, b='a', c=1.0, d=\"test\"))\n\nNamedTuple{(:a, :b, :c, :d), Tuple{Int64, Char, Float64, String}}\n\n\n\nx = (a=2, b='a', c=1.0, d=\"test\");\nx.c\n\n1.0"
  },
  {
    "objectID": "julia/collections.html#dictionaries",
    "href": "julia/collections.html#dictionaries",
    "title": "Collections",
    "section": "Dictionaries",
    "text": "Dictionaries\nJulia also has dictionaries: associative collections of key/value pairs:\n\nx = Dict(\"Name\"=>\"Roger\", \"Age\"=>52, \"Index\"=>0.3)\n\nDict{String, Any} with 3 entries:\n  \"Index\" => 0.3\n  \"Age\"   => 52\n  \"Name\"  => \"Roger\"\n\n\n\"Name\", \"Age\", and \"Index\" are the keys; \"Roger\", 52, and 0.3 are the values.\nThe => operator is the same as the Pair function:\n\np = \"foo\" => 7\n\n\"foo\" => 7\n\n\n\nq = Pair(\"bar\", 8)\n\n\"bar\" => 8\n\n\nDictionaries can be heterogeneous (as in this example) and the order doesn‚Äôt matter. They are also indexable:\n\nx[\"Name\"]\n\n\"Roger\"\n\n\nAnd mutable (they can be modified):\n\nx[\"Name\"] = \"Alex\";\nx\n\nDict{String, Any} with 3 entries:\n  \"Index\" => 0.3\n  \"Age\"   => 52\n  \"Name\"  => \"Alex\""
  },
  {
    "objectID": "julia/collections.html#sets",
    "href": "julia/collections.html#sets",
    "title": "Collections",
    "section": "Sets",
    "text": "Sets\nSets are collections without duplicates. The order of elements doesn‚Äôt matter.\n\nset1 = Set([9, 4, 8, 2, 7, 8])\n\nSet{Int64} with 5 elements:\n  4\n  7\n  2\n  9\n  8\n\n\n\nNotice how this is a set of 5 (and not 6) elements: the duplicated 8 didn‚Äôt matter.\n\n\nset2 = Set([10, 2, 3])\n\nSet{Int64} with 3 elements:\n  2\n  10\n  3\n\n\nYou can compare sets:\n\n# The union is the set of elements that are in one OR the other set\nunion(set1, set2)\n\nSet{Int64} with 7 elements:\n  4\n  7\n  2\n  10\n  9\n  8\n  3\n\n\n\n# The intersect is the set of elements that are in one AND the other set\nintersect(set1, set2)\n\nSet{Int64} with 1 element:\n  2\n\n\n\n# The setdiff is the set of elements that are in the first set but not in the second\n# Note that the order matters here\nsetdiff(set1, set2)\n\nSet{Int64} with 4 elements:\n  4\n  7\n  9\n  8\n\n\nSets can be heterogeneous:\n\nSet([\"test\", 9, :a])\n\nSet{Any} with 3 elements:\n  :a\n  \"test\"\n  9"
  },
  {
    "objectID": "julia/collections.html#arrays",
    "href": "julia/collections.html#arrays",
    "title": "Collections",
    "section": "Arrays",
    "text": "Arrays\n\nVectors\nUnidimensional arrays in Julia are called vectors.\n\nVectors of one element\n\n[3]\n\n1-element Vector{Int64}:\n 3\n\n\n\n[3.4]\n\n1-element Vector{Float64}:\n 3.4\n\n\n\n[\"Hello, World!\"]\n\n1-element Vector{String}:\n \"Hello, World!\"\n\n\n\n\nVectors of multiple elements\n\n[3, 4]\n\n2-element Vector{Int64}:\n 3\n 4\n\n\n\n\n\nTwo dimensional arrays\n\n[3 4]\n\n1√ó2 Matrix{Int64}:\n 3  4\n\n\n\n[[1, 3] [1, 2]]\n\n2√ó2 Matrix{Int64}:\n 1  1\n 3  2\n\n\n\n\nSyntax subtleties\nThese 3 syntaxes are equivalent:\n\n[2 4 8]\n\n1√ó3 Matrix{Int64}:\n 2  4  8\n\n\n\nhcat(2, 4, 8)\n\n1√ó3 Matrix{Int64}:\n 2  4  8\n\n\n\ncat(2, 4, 8, dims=2)\n\n1√ó3 Matrix{Int64}:\n 2  4  8\n\n\nThese 4 syntaxes are equivalent:\n\n[2\n 4\n 8]\n\n3-element Vector{Int64}:\n 2\n 4\n 8\n\n\n\n[2; 4; 8]\n\n3-element Vector{Int64}:\n 2\n 4\n 8\n\n\n\nvcat(2, 4, 8)\n\n3-element Vector{Int64}:\n 2\n 4\n 8\n\n\n\ncat(2, 4, 8, dims=1)\n\n3-element Vector{Int64}:\n 2\n 4\n 8\n\n\nElements separated by semi-colons or end of lines get expanded vertically.\nThose separated by commas do not get expanded.\nElements separated by spaces or tabs get expanded horizontally.\n\n\nYour turn:\n\nCompare the outputs of the following:\n\n\n[1:2; 3:4]\n\n4-element Vector{Int64}:\n 1\n 2\n 3\n 4\n\n\n\n[1:2\n 3:4]\n\n4-element Vector{Int64}:\n 1\n 2\n 3\n 4\n\n\n\n[1:2, 3:4]\n\n2-element Vector{UnitRange{Int64}}:\n 1:2\n 3:4\n\n\n\n[1:2 3:4]\n\n2√ó2 Matrix{Int64}:\n 1  3\n 2  4\n\n\n\n\nArrays and types\nIn Julia, arrays can be heterogeneous:\n\n[3, \"hello\"]\n\n2-element Vector{Any}:\n 3\n  \"hello\"\n\n\nThis is possible because all elements of an array, no matter of what types, will always sit below the Any type in the type hierarchy.\n\n\nInitializing arrays\nBelow are examples of some of the functions initializing arrays:\n\nrand(2, 3, 4)\n\n2√ó3√ó4 Array{Float64, 3}:\n[:, :, 1] =\n 0.181869  0.0915411  0.651725\n 0.327594  0.503996   0.27463\n\n[:, :, 2] =\n 0.304913  0.376308   0.227217\n 0.901566  0.0548263  0.346646\n\n[:, :, 3] =\n 0.324947  0.310066  0.821691\n 0.831401  0.422848  0.555132\n\n[:, :, 4] =\n 0.00253272  0.280317  0.27852\n 0.176464    0.114991  0.206782\n\n\n\nrand(Int64, 2, 3, 4)\n\n2√ó3√ó4 Array{Int64, 3}:\n[:, :, 1] =\n -8987527465178754461   3428059773622617463   4155247183085351984\n  5107481386831561392  -1632077291384995587  -1972479445349631935\n\n[:, :, 2] =\n -2189845061699179087  -1582333260449727536   -494174093546093300\n -7552585497212562283  -9001205848815962154  -3489427829148735653\n\n[:, :, 3] =\n -3510160236216512912  3760518427195529092  -5729286918391534193\n -5831521150042076744  7623549659816828566  -8248804342961297846\n\n[:, :, 4] =\n -1139332079754777915   6506942486810367180    826351831923977874\n -4656394141551938766  -8814845756355561796  -3497203115541871575\n\n\n\nzeros(Int64, 2, 5)\n\n2√ó5 Matrix{Int64}:\n 0  0  0  0  0\n 0  0  0  0  0\n\n\n\nones(2, 5)\n\n2√ó5 Matrix{Float64}:\n 1.0  1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0  1.0\n\n\n\nreshape([1, 2, 4, 2], (2, 2))\n\n2√ó2 Matrix{Int64}:\n 1  4\n 2  2\n\n\n\nfill(\"test\", (2, 2))\n\n2√ó2 Matrix{String}:\n \"test\"  \"test\"\n \"test\"  \"test\"\n\n\n\n\nBroadcasting\nTo apply a function to each element of a collection rather than to the collection as a whole, Julia uses broadcasting.\n\na = [-3, 2, -5]\n\n3-element Vector{Int64}:\n -3\n  2\n -5\n\n\nabs(a)\nLoadError: MethodError: no method matching abs(::Vector{Int64})\nThis doesn‚Äôt work because the function abs only applies to single elements.\nBy broadcasting abs, you apply it to each element of a:\n\nbroadcast(abs, a)\n\n3-element Vector{Int64}:\n 3\n 2\n 5\n\n\nThe dot notation is equivalent:\n\nabs.(a)\n\n3-element Vector{Int64}:\n 3\n 2\n 5\n\n\nIt can also be applied to the pipe, to unary and binary operators, etc.\n\na .|> abs\n\n3-element Vector{Int64}:\n 3\n 2\n 5\n\n\n\n\nYour turn:\n\nTry to understand the difference between the following 2 expressions:\n\n\nabs.(a) == a .|> abs\n\ntrue\n\n\n\nabs.(a) .== a .|> abs\n\n3-element BitVector:\n 1\n 1\n 1\n\n\n\nHint: 0/1 are a short-form notations for false/true in arrays of Booleans.\n\n\n\nComprehensions\nJulia has an array comprehension syntax similar to Python‚Äôs:\n\n[ 3i + j for i=1:10, j=3 ]\n\n10-element Vector{Int64}:\n  6\n  9\n 12\n 15\n 18\n 21\n 24\n 27\n 30\n 33"
  },
  {
    "objectID": "julia/collections.html#indexing",
    "href": "julia/collections.html#indexing",
    "title": "Collections",
    "section": "Indexing",
    "text": "Indexing\nAs in other mathematically oriented languages such as R, Julia starts indexing at 1.\nIndexing is done with square brackets:\n\na = [1 2; 3 4]\n\n2√ó2 Matrix{Int64}:\n 1  2\n 3  4\n\n\n\na[1, 1]\n\n1\n\n\n\na[1, :]\n\n2-element Vector{Int64}:\n 1\n 2\n\n\n\na[:, 1]\n\n2-element Vector{Int64}:\n 1\n 3\n\n\n\n# Here, we are indexing a tuple\n(2, 4, 1.0, \"test\")[2]\n\n4\n\n\n\n\nYour turn:\n\nIndex the element on the 3rd row and 2nd column of b:\n\nb = [\"wrong\" \"wrong\" \"wrong\"; \"wrong\" \"wrong\" \"wrong\"; \"wrong\" \"you got it\" \"wrong\"]\n\n3√ó3 Matrix{String}:\n \"wrong\"  \"wrong\"       \"wrong\"\n \"wrong\"  \"wrong\"       \"wrong\"\n \"wrong\"  \"you got it\"  \"wrong\"\n\n\n\n\n\nYour turn:\n\na = [1 2; 3 4]\na[1, 1]\na[1, :]\nHow can I get the second column?\nHow can I get the tuple (2, 4)? (a tuple is a list of elements)\n\nAs in Python, by default, arrays are passed by sharing:\n\na = [1, 2, 3];\na[1] = 0;\na\n\n3-element Vector{Int64}:\n 0\n 2\n 3\n\n\nThis prevents the unwanted copying of arrays."
  },
  {
    "objectID": "julia/control_flow.html",
    "href": "julia/control_flow.html",
    "title": "Control flow",
    "section": "",
    "text": "Conditional statements allow to run instructions based on predicates: different sets of instructions will be executed depending on whether the predicates return true or false.\n\n\n\nHere are a few examples of predicates with classic operators:\n\noccursin(\"that\", \"this and that\")\n4 < 3\na == b\na != b\n2 in 1:3\n3 <= 4 && 4 > 5\n3 <= 4 || 4 > 5\nIn addition, Julia possesses more exotic operators that can be used in predicates:\n\nThe inexact equality comparator, useful to compare floating-point numbers despite computer rounding.\n\n\nThe function isapprox or the equivalent binary operator ‚âà (typed with \\approx<tab>) can be used:\n\n0.1 + 0.2 == 0.3\n\nfalse\n\n\n\n0.1 + 0.2 ‚âà 0.3\n\ntrue\n\n\n\nisapprox(0.1 + 0.2, 0.3)\n\ntrue\n\n\nThe negatives are the function !isapprox and ‚ââ (typed with \\napprox<tab>).\n\n\nThe equivalent or triple equal operator compares objects in deeper ways (address in memory for mutable objects and content at the bit level for immutable objects).\n\n\n=== or ‚â° (typed with \\equiv<tab>) can be used:\n\na = [1, 2]; b = [1, 2];\n\n\na == b\n\ntrue\n\n\n\na ‚â° b     # This can also be written `a === b`\n\nfalse\n\n\n\na ‚â° a\n\ntrue\n\n\n\n\n\n\nif <predicate>\n    <some action>\nend\n\nIf <predicate> evaluates to true, the body of the if statement gets evaluated (<some action> is run),\nIf <predicate> evaluates to false, nothing happens.\n\n\nExample:\n\n\nfunction testsign1(x)\n    if x >= 0\n        println(\"x is positive\")\n    end\nend\n\ntestsign1 (generic function with 1 method)\n\n\n\ntestsign1(3)\n\nx is positive\n\n\n\ntestsign1(-2)\n\n\nNothing gets returned since the predicate returned false.\n\n\n\n\nif <predicate>\n    <some action>\nelse\n    <some other action>\nend\n\nIf <predicate> evaluates to true, <some action> is done,\nIf <predicate> evaluates to false, <some other action> is done.\n\n\nExample:\n\n\nfunction testsign2(x)\n    if x >= 0\n        println(\"x is positive\")\n    else\n        println(\"x is negative\")\n    end\nend\n\ntestsign2 (generic function with 1 method)\n\n\n\ntestsign2(3)\n\nx is positive\n\n\n\ntestsign2(-2)\n\nx is negative\n\n\nIf else statements can be written in a terse format using the ternary operator:\n<predicate> ? <some action> : <some other action>\n\nHere is our function testsign2 written in terse format:\n\n\nfunction testsign2(x)\n    x >= 0 ? println(\"x is positive\") : println(\"x is negative\")\nend\n\ntestsign2(-2)\n\nx is negative\n\n\n\nHere is another example:\n\na = 2\nb = 2.0\n\nif a == b\n    println(\"It's true\")\nelse\n    println(\"It's false\")\nend\nAnd in terse format:\n\na == b ? println(\"It's true\") : println(\"It's false\")\n\nIt's true\n\n\n\n\n\nif <predicate1\n    <some action>\nelseif <predicate2>\n    <some other action>\nelse\n    <yet some other action>\nend\n\nExample:\n\n\nfunction testsign3(x)\n    if x > 0\n        println(\"x is positive\")\n    elseif x == 0\n        println(\"x is zero\")\n    else\n        println(\"x is negative\")\n    end\nend\n\ntestsign3 (generic function with 1 method)\n\n\n\ntestsign3(3)\n\nx is positive\n\n\n\ntestsign3(0)\n\nx is zero\n\n\n\ntestsign3(-2)\n\nx is negative"
  },
  {
    "objectID": "julia/control_flow.html#loops",
    "href": "julia/control_flow.html#loops",
    "title": "Control flow",
    "section": "Loops",
    "text": "Loops\n\nFor loops\nFor loops run a set of instructions for each element of an iterator:\nfor <iterator>\n    <some action>\nend\n\nExamples:\n\n\nfor name = [\"Paul\", \"Lucie\", \"Sophie\"]\n    println(\"Hello $name\")\nend\n\nHello Paul\nHello Lucie\nHello Sophie\n\n\n\nfor i = 1:3, j = 3:5\n    println(i + j)\nend\n\n4\n5\n6\n5\n6\n7\n6\n7\n8\n\n\n\n\nWhile loops\nWhile loops run as long as the condition remains true:\nwhile <predicate>\n    <some action>\nend\n\nExample:\n\n\ni = 0\n\nwhile i <= 10\n    println(i)\n    i += 1\nend\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10"
  },
  {
    "objectID": "julia/distributed.html",
    "href": "julia/distributed.html",
    "title": "Distributed computing",
    "section": "",
    "text": "Julia supports distributed computing thanks to the module Distributed from its standard library.\nThere are two ways to launch several Julia processes (called ‚Äúworkers‚Äù):\n\n\nJulia can be started with the -p flag followed by the number of workers by running (in a terminal):\njulia -p n\nThis launches n workers, available for parallel computations, in addition to the process running the interactive prompt (so there are n + 1 Julia processes in total).\nThe module Distributed is needed whenever you want to use several workers, but the -p flag loads it automatically.\n\nExample:\n\njulia -p 4\nWithin Julia, you can see how many workers are running with:\nnworkers()\nThe total number of processes can be seen with:\nnprocs()\n\n\n\nAlternatively, workers can be started from within a Julia session. In this case, you need to load the module Distributed explicitly:\nusing Distributed\nTo launch n workers:\naddprocs(n)\n\nExample:\n\naddprocs(4)"
  },
  {
    "objectID": "julia/distributed.html#managing-workers",
    "href": "julia/distributed.html#managing-workers",
    "title": "Distributed computing",
    "section": "Managing workers",
    "text": "Managing workers\nTo list all the worker process identifiers:\nworkers()\n\nThe process running the Julia prompt has id 1.\n\nTo kill a worker:\nrmprocs(<pid>)\nwhere <pid> is the process identifier of the worker you want to kill (you can kill several workers by providing a list of pids)."
  },
  {
    "objectID": "julia/distributed.html#using-workers",
    "href": "julia/distributed.html#using-workers",
    "title": "Distributed computing",
    "section": "Using workers",
    "text": "Using workers\nThere are a number of macros that are very convenient here:\n\nTo execute an expression on all processes, there is @everywhere\n\nFor instance, if your parallel code requires a module or an external package to run, you need to load that module or package with @everywhere:\n@everywhere using DataFrames\nIf the parallel code requires a script to run:\n@everywhere include(\"script.jl\")\nIf it requires a function that you are defining, you need to define it on all the workers:\n@everywhere function <name>(<arguments>)\n    <body>\nend\n\nTo assign a task to a particular worker, you use @spawnat\n\nThe first argument indicates the process id, the second argument is the expression that should be evaluated:\n@spawnat <pid> <expression>\n@spawnat returns of Future: the placeholder for a computation of unknown status and time. The function fetch waits for a Future to complete and returns the result of the computation.\n\nExample:\n\nThe function myid gives the id of the current process. As I mentioned earlier, the process running the interactive Julia prompt has the pid 1. So myid() normally returns 1.\nBut we can ‚Äúspawn‚Äù myid on one of the worker, for instance the first worker (so pid 2):\n@spawnat 2 myid()\nAs you can see, we get a Future as a result. But if we pass it through fetch, we get the result of myid ran on the worker with pid 2:\nfetch(@spawnat 2 myid())\nIf you want tasks to be assigned to any worker automatically, you can pass the symbol :any to @spawnat instead of the worker id:\n@spawnat :any myid()\nTo get the result:\nfetch(@spawnat :any myid())\nIf you run this multiple times, you will see that myid is run on any of your available workers. This will however never return 1, except when you only have one running Julia process (in that case, the process running the prompt is considered a worker)."
  },
  {
    "objectID": "julia/firstdab.html",
    "href": "julia/firstdab.html",
    "title": "First dab at Julia",
    "section": "",
    "text": "Julia is fast: just-in-time (JIT) compilation and multiple dispatch bring efficiency to interactivity. People often say that using Julia feels like running R or python with a speed almost comparable to that of C.\nBut Julia also comes with parallel computing and multi-threading capabilities.\nIn this webinar, after a quickly presentation of some of the key features of Julia‚Äôs beautifully concise syntax, I will dive into using Julia for HPC."
  },
  {
    "objectID": "julia/flux.html",
    "href": "julia/flux.html",
    "title": "Machine learning in Julia with Flux",
    "section": "",
    "text": "This webinar, aimed at users with no experience in machine learning, is an introduction to the basic concepts of neural networks, followed by a simple example‚Äîthe classic classification of the MNIST database of handwritten digits‚Äîusing the Julia package Flux."
  },
  {
    "objectID": "julia/functions.html",
    "href": "julia/functions.html",
    "title": "Functions",
    "section": "",
    "text": "Functions are objects containing a set of instructions.\nWhen you pass a tuple of argument(s) (possibly an empty tuple) to them, you get one or more values as output."
  },
  {
    "objectID": "julia/functions.html#operators",
    "href": "julia/functions.html#operators",
    "title": "Functions",
    "section": "Operators",
    "text": "Operators\nOperators are functions and can be written in a way that shows the tuple of arguments more explicitly.\n\nFor instance, you can use the addition operator (+) in 2 ways:\n\n\n3 + 2\n+(3, 2)\n\n5\n\n\nThe multiplication operator can be omitted when this does not create any ambiguity:\n\na = 3;\n2a\n\n6\n\n\nJulia has ‚Äúassignment by operation‚Äù operators:\n\na = 2;\na += 7    # this is the same as a = a + 7\n\n9\n\n\nThere is a left division operator:\n\n2\\8 == 8/2\n\ntrue\n\n\nJulia supports fraction operations:\n\n4//8\n\n1//2\n\n\n\n1//2 + 3//4\n\n5//4"
  },
  {
    "objectID": "julia/functions.html#function-definition",
    "href": "julia/functions.html#function-definition",
    "title": "Functions",
    "section": "Function definition",
    "text": "Function definition\nThere are 2 ways to define a new function:\n\nLong form\nfunction <name>(<arguments>)\n    <body>\nend\n\nExample:\n\n\nfunction hello1()\n    println(\"Hello\")\nend\n\nhello1 (generic function with 1 method)\n\n\n\n\nAssignment form\n<name>(<arguments>) = <body>\n\nExample:\n\n\nhello1() = println(\"Hello\")\n\nhello1 (generic function with 1 method)\n\n\nThe function hello1 defined with this terse syntax is exactly the same as the one we defined above.\n\n\nStylistic convention\nJulia suggests to use lower case without underscores as function names when the name is readable enough."
  },
  {
    "objectID": "julia/functions.html#calling-functions",
    "href": "julia/functions.html#calling-functions",
    "title": "Functions",
    "section": "Calling functions",
    "text": "Calling functions\nSince you pass a tuple to a function when you run it, you call a function by appending parentheses to its name:\n\nhello1()\n\nHello\n\n\n\nHere, our function does not take any argument, so the tuple is empty."
  },
  {
    "objectID": "julia/functions.html#arguments",
    "href": "julia/functions.html#arguments",
    "title": "Functions",
    "section": "Arguments",
    "text": "Arguments\n\nNo argument\nOur function hello1 does not accept any argument. If we pass an argument, we get an error message:\nhello1(\"Bob\")\nLoadError: MethodError: no method matching hello1(::String)\n\n\nOne argument\nTo define a function which accepts an argument, we need to add a placeholder for it in the function definition.\n\nSo let‚Äôs try this:\n\n\nfunction hello2(name)\n    println(\"Hello name\")\nend\n\nhello2 (generic function with 1 method)\n\n\n\nhello2(\"Bob\")\n\nHello name\n\n\nMmm ‚Ä¶ not quite ‚Ä¶ this function works but does not give the result we wanted.\nHere, we need to use string interpolation:\n\nfunction hello3(name)\n    println(\"Hello $name\")\nend\n\nhello3 (generic function with 1 method)\n\n\n$name in the body of the function points to name in the tuple of argument.\nWhen we run the function, $name is replaced by the value we used in lieu of name in the function definition:\n\nhello3(\"Bob\")\n\nHello Bob\n\n\nHere is the corresponding assignment form for hello3:\n\nhello3(name) = println(\"Hello $name\")\n\nhello3 (generic function with 1 method)\n\n\n\nNote that this dollar sign is only required with strings. Here is an example with integers:\n\n\nfunction addTwo(a)\n    a + 2\nend\n\naddTwo (generic function with 1 method)\n\n\nAnd the corresponding assignment form:\n\naddTwo(a) = a + 2\n\naddTwo (generic function with 1 method)\n\n\n\naddTwo(4)\n\n6\n\n\n\n\nMultiple arguments\nNow, let‚Äôs write a function which accepts 2 arguments. For this, we put 2 placeholders in the tuple passed to the function in the function definition:\n\nfunction hello4(name1, name2)\n    println(\"Hello $name1 and $name2\")\nend\n\nhello4 (generic function with 1 method)\n\n\nThis means that this function expects a tuple of 2 values:\n\nhello4(\"Bob\", \"Pete\")\n\nHello Bob and Pete\n\n\n\n\nYour turn:\n\nSee what happens when you pass no argument, a single argument, or three arguments to this function.\n\n\n\nDefault arguments\nYou can set a default value for some or all arguments. In this case, the function will run with or without a value passed for those arguments. If no value is given, the default is used. If a value is given, it will replace the default.\n\nExample:\n\n\nfunction hello5(name=\"\")\n    println(\"Hello $name\")\nend\n\nhello5 (generic function with 2 methods)\n\n\n\nhello5()\n\nHello \n\n\n\nhello5(\"Bob\")\n\nHello Bob\n\n\n\nAnother example:\n\n\nfunction addSomethingOrTwo(a, b=2)\n    a + b\nend\n\naddSomethingOrTwo (generic function with 2 methods)\n\n\n\naddSomethingOrTwo(3)\n\n5\n\n\n\naddSomethingOrTwo(3, 4)\n\n7"
  },
  {
    "objectID": "julia/functions.html#returning-the-result",
    "href": "julia/functions.html#returning-the-result",
    "title": "Functions",
    "section": "Returning the result",
    "text": "Returning the result\nIn Julia, functions return the value(s) of the last expression automatically. If you want to return something else instead, you need to use the return statement. This causes the function to exit early.\n\nLook at these 5 functions:\n\nfunction test1(x, y)\n    x + y\nend\n\nfunction test2(x, y)\n    return x + y\nend\n\nfunction test3(x, y)\n    x * y\n    x + y\nend\n\nfunction test4(x, y)\n    return x * y\n    x + y\nend\n\nfunction test5(x, y)\n    return x * y\n    return x + y\nend\n\nfunction test6(x, y)\n    x * y, x + y\nend\n\n\nYour turn:\n\nWithout running the code, try to guess the outputs of:\n\ntest1(1, 2)\ntest2(1, 2)\ntest3(1, 2)\ntest4(1, 2)\ntest5(1, 2)\ntest6(1, 2)\n\n\nYour turn:\n\nNow, run the code and draw some conclusions on the behaviour of the return statement."
  },
  {
    "objectID": "julia/functions.html#anonymous-functions",
    "href": "julia/functions.html#anonymous-functions",
    "title": "Functions",
    "section": "Anonymous functions",
    "text": "Anonymous functions\nAnonymous functions are functions which aren‚Äôt given a name:\nfunction (<arguments>)\n    <body>\nend\nIn compact form:\n<arguments> -> <body>\n\nExample:\n\n\nfunction (name)\n    println(\"Hello $name\")\nend\n\n#11 (generic function with 1 method)\n\n\nCompact form:\n\nname -> println(\"Hello $name\")\n\n#13 (generic function with 1 method)\n\n\n\nWhen would you want to use anonymous functions?\nThis is very useful for functional programming (when you apply a function‚Äîfor instance map‚Äîto other functions to apply them in a vectorized manner which avoids repetitions).\n\nExample:\n\n\nmap(name -> println(\"Hello $name\"), [\"Bob\", \"Lucie\", \"Sophie\"]);\n\nHello Bob\nHello Lucie\nHello Sophie"
  },
  {
    "objectID": "julia/functions.html#pipes",
    "href": "julia/functions.html#pipes",
    "title": "Functions",
    "section": "Pipes",
    "text": "Pipes\n|> is the pipe in Julia. It redirects the output of the expression on the left as the input of the expression on the right.\n\nThe following 2 expressions are equivalent:\n\nprintln(\"Hello\")\n\"Hello\" |> println\n\nHere is another example:\n\n\nsqrt(2) == 2 |> sqrt\n\ntrue"
  },
  {
    "objectID": "julia/functions.html#function-composition",
    "href": "julia/functions.html#function-composition",
    "title": "Functions",
    "section": "Function composition",
    "text": "Function composition\nYou can pass a function inside another function:\n<function2>(<function1>(<arguments>))\n<arguments> will be passed to <function1> and the result will then be passed to <function2>.\nAn equivalent syntax is to use the composition operator ‚àò (in the REPL, type \\circ then press tab):\n(<function2> ‚àò <function1>)(<arguments>)\n\nExample:\n\n\n# sum is our first function\nsum(1:3)\n\n6\n\n\n\n# sqrt is the second function\nsqrt(sum(1:3))\n\n2.449489742783178\n\n\n\n# This is equivalent\n(sqrt ‚àò sum)(1:3)\n\n2.449489742783178\n\n\n\n\nYour turn:\n\nWrite three other equivalent expressions using the pipe.\n\n\nAnother example:\n\n\nexp(+(-3, 1))\n\n(exp ‚àò +)(-3, 1)\n\n0.1353352832366127\n\n\n\n\nYour turn:\n\nTry to write the same expression in another 2 different ways."
  },
  {
    "objectID": "julia/functions.html#mutating-functions",
    "href": "julia/functions.html#mutating-functions",
    "title": "Functions",
    "section": "Mutating functions",
    "text": "Mutating functions\nFunctions usually do not modify their argument(s):\n\na = [-2, 3, -5]\n\n3-element Vector{Int64}:\n -2\n  3\n -5\n\n\n\nsort(a)\n\n3-element Vector{Int64}:\n -5\n -2\n  3\n\n\n\na\n\n3-element Vector{Int64}:\n -2\n  3\n -5\n\n\nJulia has a set of functions which modify their argument(s). By convention, their names end with !\n\nThe function sort has a mutating equivalent sort!:\n\n\nsort!(a);\na\n\n3-element Vector{Int64}:\n -5\n -2\n  3\n\n\n\nIf you write functions which modify their arguments, make sure to follow this convention too."
  },
  {
    "objectID": "julia/functions.html#broadcasting",
    "href": "julia/functions.html#broadcasting",
    "title": "Functions",
    "section": "Broadcasting",
    "text": "Broadcasting\nTo apply a function to each element of a collection rather than to the collection as a whole, Julia uses broadcasting.\n\nLet‚Äôs create a collection (here a tuple):\n\n\na = (2, 3)\n\n(2, 3)\n\n\n\nIf we pass a to the string function, that function applies to the whole collection:\n\n\nstring(a)\n\n\"(2, 3)\"\n\n\n\nIn contrast, we can broadcast the function string to all elements of a:\n\n\nbroadcast(string, a)\n\n(\"2\", \"3\")\n\n\n\nAn alternative syntax is to add a period after the function name:\n\n\nstring.(a)\n\n(\"2\", \"3\")\n\n\n\nHere is another example:\n\na = [-3, 2, -5]\nabs(a)\nERROR: MethodError: no method matching abs(::Array{Int64,1})\nThis doesn‚Äôt work because the function abs only applies to single elements.\nBy broadcasting abs, you apply it to each element of a:\n\nbroadcast(abs, a)\n\n(2, 3)\n\n\nThe dot notation is equivalent:\n\nabs.(a)\n\n(2, 3)\n\n\nIt can also be applied to the pipe, to unary and binary operators, etc.\n\nExample:\n\n\na .|> abs\n\n(2, 3)\n\n\n\n\nYour turn:\n\nTry to understand the difference between the following 2 expressions:\n\n\nabs.(a) == a .|> abs\nabs.(a) .== a .|> abs\n\n(true, true)"
  },
  {
    "objectID": "julia/functions.html#multiple-dispatch",
    "href": "julia/functions.html#multiple-dispatch",
    "title": "Functions",
    "section": "Multiple dispatch",
    "text": "Multiple dispatch\nIn some programming languages, functions can be polymorphic (multiple versions exist under the same function name). The process of selecting which version to use is called dispatch.\nThere are multiple types of dispatch depending on the language:\n\nDynamic dispatch: the process of selecting one version of a function at run time.\nSingle dispatch: the choice of version is based on a single object.\n\n\nThis is typical of object-oriented languages such as Python, C++, Java, Smalltalk, etc.\n\n\nMultiple dispatch: the choice of version is based on the combination of all operands and their types.\n\n\nThis the case of Lisp and Julia. In Julia, these versions are called methods."
  },
  {
    "objectID": "julia/functions.html#methods",
    "href": "julia/functions.html#methods",
    "title": "Functions",
    "section": "Methods",
    "text": "Methods\nRunning methods(+) let‚Äôs you see that the function + has 206 methods!\nMethods can be added to existing functions.\n\n\nYour turn:\n\nRun the following and try to understand the outputs:\nabssum(x::Int64, y::Int64) = abs(x + y)\nabssum(x::Float64, y::Float64) = abs(x + y)\n\nabssum(2, 4)\nabssum(2.0, 4.0)\nabssum(2, 4.0)\nWhat could you do if you wanted the last expression to work?"
  },
  {
    "objectID": "julia/intro.html",
    "href": "julia/intro.html",
    "title": "Introduction to Julia",
    "section": "",
    "text": "Why would I want to learn a new language? I already know R/python.\n\nR and python are interpreted languages: the code is executed directly, without prior-compilation. This is extremely convenient: it is what allows you to run code in an interactive shell. The price to pay is low performance: R and python are simply not good at handling large amounts of data. To overcome this limitation, users often turn to C or C++ for the most computation-intensive parts of their analyses. These are compiled‚Äîand extremely efficient‚Äîlanguages, but the need to use multiple languages and the non-interactive nature of compiled languages make this approach tedious.\nJulia uses just-in-time (JIT) compilation: the code is compiled at run time. This combines the interactive advantage of interpreted languages with the efficiency of compiled ones. Basically, it feels like running R or python, while it is almost as fast as C. This makes Julia particularly well suited for big data analyses, machine learning, or heavy modelling.\nIn addition, multiple dispatch (generic functions with multiple methods depending on the types of all the arguments) is at the very core of Julia. This is extremly convenient, cutting on conditionals and repetitions, and allowing for easy extensibility without having to rewrite code.\nFinally, Julia shines by its extremely clean and concise syntax. This last feature makes it easy to learn and really enjoyable to use.\nIn this workshop, which does not require any prior experience in Julia (experience in another language‚Äîe.g.¬†R or python‚Äîwould be best), we will go over the basics of Julia‚Äôs syntax and package system; then we will push the performance aspect further by looking at how Julia can make use of clusters for large scale parallel computing."
  },
  {
    "objectID": "julia/intro.html#introducing-julia",
    "href": "julia/intro.html#introducing-julia",
    "title": "Introduction to Julia",
    "section": "Introducing Julia",
    "text": "Introducing Julia\n\nBrief history\nStarted in 2009 by Jeff Bezanson, Stefan Karpinski, Viral B. Shah, and Alan Edelman, the general-purpose programming language Julia was launched in 2012 as free and open source software. Version 1.0 was released in 2018.\nRust developer Graydon Hoare wrote an interesting post which places Julia in a historical context of programming languages.\n\n\nWhy another language?\n\nJIT\nComputer languages mostly fall into two categories: compiled languages and interpreted languages.\n\nCompiled languages\nCompiled languages require two steps:\n\nin a first step the code you write in a human-readable format (the source code, usually in plain text) gets compiled into machine code\nit is then this machine code that is used to process your data\n\nSo you write a script, compile it, then use it.\n\nBecause machine code is a lot easier to process by computers, compiled languages are fast. The two step process however makes prototyping new code less practical, these languages are hard to learn, and debugging compilation errors can be challenging.\n\nExamples of compiled languages include C, C++, Fortran, Go, and Haskell.\n\n\n\nInterpreted languages\nInterpreted languages are executed directly which has many advantages such as dynamic typing and direct feed-back from the code and they are easy to learn, but this comes at the cost of efficiency. The source code can facultatively be bytecompiled into non human-readable, more compact, lower level bytecode which is read by the interpreter more efficiently.\n\n\nExamples of interpreted languages include R, Python, Perl, and JavaScript.\n\n\n\nA common workflow\nSo, with this, what do researchers do?\nA common workflow, with the constraints of either type of languages, consists of:\n\nexploring the data and developing code using a sample of the data or reasonably light computations in an interpreted language,\ntranslating the code into a compiled language,\nfinally throwing the full data and all the heavy duty computation at that optimized code.\n\nThis works and it works well.\nBut, as you can imagine, this roundabout approach is tedious, not to mention the fact that it involves mastering 2 languages.\n\n\nJIT compiled languages\nJulia uses just-in-time compilation or JIT based on LLVM: the source code is compiled at run time. This combines the flexibility of interpretation with the speed of compilation, bringing speed to an interactive language. It also allows for dynamic recompilation, continuous weighing of gains and costs of the compilation of parts of the code, and other on the fly optimizations.\nOf course, there are costs here too. They come in the form of overhead time to compile code the first time it is run and increased memory usage.\n\n\n\nMultiple dispatch\nIn languages with multiple dispatch, functions apply different methods at run time based on the type of the operands. This brings great type stability and improves speed.\nJulia is extremely flexible: type declaration is not required. Out of convenience, you can forego the feature if you want. Specifying types however will greatly optimize your code.\nHere is a good post on type stability, multiple dispatch, and Julia efficiency."
  },
  {
    "objectID": "julia/intro.html#how-to-run-julia",
    "href": "julia/intro.html#how-to-run-julia",
    "title": "Introduction to Julia",
    "section": "How to run Julia?",
    "text": "How to run Julia?\nThere are several ways to run Julia interactively:\n\ndirectly in the REPL (read‚Äìeval‚Äìprint loop: the interactive Julia shell),\nin interactive notebooks (e.g.¬†Jupyter, Pluto),\nin an editor able to run Julia interactively (e.g.¬†Emacs, VS Code, Vim).\n\nLet‚Äôs have a look at these interfaces.\n\nThe Julia REPL\nYou can launch the REPL from a terminal directly by typing the julia command.\n\nREPL keybindings\nIn the REPL, you can use standard command line keybindings (Emacs kbd):\nC-c     cancel\nC-d     quit\nC-l     clear console\n\nC-u     kill from the start of line\nC-k     kill until the end of line\n\nC-a     go to start of line\nC-e     go to end of line\n\nC-f     move forward one character\nC-b     move backward one character\n\nM-f     move forward one word\nM-b     move backward one word\n\nC-d     delete forward one character\nC-h     delete backward one character\n\nM-d     delete forward one word\nM-Backspace delete backward one word\n\nC-p     previous command\nC-n     next command\n\nC-r     backward search\nC-s     forward search\n\n\nREPL modes\nThe Julia REPL is unique in that it has four distinct modes:\njulia> ‚ÄÉ‚ÄÉ The main mode in which you will be running your code.\nhelp?> ‚ÄÉ‚ÄÉ A mode to easily access documentation.\nshell> ‚ÄÉ‚ÄÉ A mode in which you can run bash commands from within Julia.\n(env) pkg> ¬† A mode to easily perform actions on packages with Julia package manager.\n(env is the name of your current project environment.\nProject environments are similar to Python‚Äôs virtual environments and allow you, for instance, to have different package versions for different projects. By default, it is the current Julia version. So what you will see is (v1.3) pkg>).\nEnter the various modes by typing ?, ;, and ]. Go back to the regular mode with the Backspace key.\n\n\n\nText editors\n\nVS Code\nJulia for Visual Studio Code has become the main Julia IDE.\n\n\nEmacs\n\nthrough the julia-emacs and julia-repl packages\nthrough the ESS package\nthrough the Emacs IPython Notebook package if you want to access Jupyter notebooks in Emacs\n\n\n\nVim\nThrough the julia-vim package.\n\n\n\nInteractive notebooks\n\nJupyter\nProject Jupyter allows to create interactive programming documents through its web-based JupyterLab environment and its Jupyter Notebook.\n\n\nPluto\nThe Julia package Juno is a reactive notebook for Julia.\n\n\n\nQuarto\nQuarto builds interactive documents with code and runs Julia through Jupyter."
  },
  {
    "objectID": "julia/intro.html#startup-options",
    "href": "julia/intro.html#startup-options",
    "title": "Introduction to Julia",
    "section": "Startup options",
    "text": "Startup options\nYou can configure Julia by creating the file ~/.julia/config/startup.jl."
  },
  {
    "objectID": "julia/intro.html#help-and-documentation",
    "href": "julia/intro.html#help-and-documentation",
    "title": "Introduction to Julia",
    "section": "Help and documentation",
    "text": "Help and documentation\nAs we already saw, you can type ? to enter the help mode:\n?sum\nsearch: sum sum! summary cumsum cumsum! isnumeric VersionNumber issubnormal \nget_zero_subnormals set_zero_subnormals\n\n  sum(f, itr; [init])\n\n  Sum the results of calling function f on each element of itr.\n\n  The return type is Int for signed integers of less than system word size, \n  and UInt for unsigned integers of less than system word size. For all other \n  arguments a common return type is found to which all arguments are promoted.\n\n  The value returned for empty itr can be specified by init. It must be the \n  additive identity (i.e. zero) as it is unspecified whether init is used for \n  non-empty collections.\n\nI truncated this output as the documentation also contains many examples.\n\nTo print the list of functions containing a certain word in their description, you can use apropos().\n\nExample:\n\n\napropos(\"truncate\")\n\nBase.IOBuffer\nBase.IOContext\n\n\nBase.dump\nBase.open\nBase.open_flags\nBase.truncate\nCore.String\nBase.Broadcast.newindex\n\n\nArgTools\nNetworkOptions\nLinearAlgebra.eigen\nTar\nDates.Date\nBase.trunc\nDates.format\n\n\nIJulia.set_max_stdio\nIJulia.watch_stream\nAbstractTrees.TreeCharSet\nAbstractTrees.print_tree\nSIMD\n\n\nOffsetArrays\nPDMats\nStatsFuns\nDistributions\nDistributions.truncated\nDistributions.TruncatedNormal\nDistributions.Truncated\nDistributions.Distributions\n\n\nLazyModules\n\n\nMakie.to_vertices"
  },
  {
    "objectID": "julia/intro.html#version-information",
    "href": "julia/intro.html#version-information",
    "title": "Introduction to Julia",
    "section": "Version information",
    "text": "Version information\nJulia version only:\n\nversioninfo()\n\nJulia Version 1.8.4\nCommit 00177ebc4fc (2022-12-23 21:32 UTC)\nPlatform Info:\n  OS: Linux (x86_64-linux-gnu)\n  CPU: 16 √ó Intel(R) Core(TM) i7-10875H CPU @ 2.30GHz\n  WORD_SIZE: 64\n  LIBM: libopenlibm\n  LLVM: libLLVM-13.0.1 (ORCJIT, skylake)\n  Threads: 1 on 16 virtual cores\n\n\nMore information, including commit, OS, CPU, and compiler:\n\nVERSION\n\nv\"1.8.4\""
  },
  {
    "objectID": "julia/intro.html#lets-try-a-few-commands",
    "href": "julia/intro.html#lets-try-a-few-commands",
    "title": "Introduction to Julia",
    "section": "Let‚Äôs try a few commands",
    "text": "Let‚Äôs try a few commands\nx = 10\nx\nx = 2;\nx\ny = x;\ny\nans\nans + 3\n\na, b, c = 1, 2, 3\nb\n\n3 + 2\n+(3, 2)\n\na = 3\n2a\na += 7\na\n\n2\\8\n\na = [1 2; 3 4]\nb = a\na[1, 1] = 0\nb\n\n[1, 2, 3, 4]\n[1 2; 3 4]\n[1 2 3 4]\n[1 2 3 4]'\ncollect(1:4)\ncollect(1:1:4)\n1:4\na = 1:4\ncollect(a)\n\n[1, 2, 3] .* [1, 2, 3]\n\n4//8\n8//1\n1//2 + 3//4\n\na = true\nb = false\na + b\n\n\nYour turn:\n\nWhat does ; at the end of a command do?\nWhat is surprising about 2a?\nWhat does += do?\nWhat does .*do?\n\na = [3, 1, 2]\n\nsort(a)\nprintln(a)\n\nsort!(a)\nprintln(a)\n\n\nYour turn:\n\nWhat does ! at the end of a function name do?"
  },
  {
    "objectID": "julia/intro_hpc.html",
    "href": "julia/intro_hpc.html",
    "title": "Introduction to high performance research computing in Julia",
    "section": "",
    "text": "When you launch a Jupyter session from a JupyterHub, you are running a Slurm job on a compute node. If you want to play for 8 hours in Jupyter, you are requesting an 8 hour job. Now, most of the time you spend on Jupyter is spent typing, running bits and pieces of code, or doing nothing at all. If you ask for GPUs, many CPUs, and lots of RAM, all of it will remain idle most of the time. This is a suboptimal use of resources.\nIn addition, if you ask for lots of resources for a long time, you will have to wait for a while before they get allocated to you.\nLastly, you will go through your allocations quickly.\nAll of this applies equally for interactive sessions launched from an SSH session with salloc."
  },
  {
    "objectID": "julia/intro_hpc.html#a-better-approach",
    "href": "julia/intro_hpc.html#a-better-approach",
    "title": "Introduction to high performance research computing in Julia",
    "section": "A better approach",
    "text": "A better approach\nA more efficient strategy is to develop and test your code with small samples, few iterations, etc. in an interactive job (from an SSH session in the cluster with salloc), on your own computer, or in Jupyter. Once you are confident that your code works, launch an sbatch job from an SSH session in the cluster to run the code as a script on all your data. This ensures that heavy duty resources that you requested are actually put to use to run your heavy calculations and not seating idle while you are thinking, typing, etc."
  },
  {
    "objectID": "julia/intro_hpc.html#logging-on-to-the-cluster",
    "href": "julia/intro_hpc.html#logging-on-to-the-cluster",
    "title": "Introduction to high performance research computing in Julia",
    "section": "Logging on to the cluster",
    "text": "Logging on to the cluster\nOpen a terminal emulator:\nWindows users: ‚ÄÉlaunch MobaXTerm.\nMacOS users: ‚ÄÉ‚ÄÉlaunch Terminal.\nLinux users: ‚ÄÉ‚ÄÉ‚ÄÇ¬†launch xterm or the terminal emulator of your choice.\nThen access the cluster through secure shell:\n$ ssh <username>@<hostname>    # enter password"
  },
  {
    "objectID": "julia/intro_hpc.html#accessing-julia",
    "href": "julia/intro_hpc.html#accessing-julia",
    "title": "Introduction to high performance research computing in Julia",
    "section": "Accessing Julia",
    "text": "Accessing Julia\nThis is done with the Lmod tool through the module command. You can find the full documentation here and below are the subcommands you will need:\n# get help on the module command\n$ module help\n$ module --help\n$ module -h\n\n# list modules that are already loaded\n$ module list\n\n# see which modules are available for Julia\n$ module spider julia\n\n# see how to load julia 1.3\n$ module spider julia/1.3.0\n\n# load julia 1.3 with the required gcc module first\n# (the order is important)\n$ module load gcc/7.3.0 julia/1.3.0\n\n# you can see that we now have Julia loaded\n$ module list"
  },
  {
    "objectID": "julia/intro_hpc.html#copying-files-to-the-cluster",
    "href": "julia/intro_hpc.html#copying-files-to-the-cluster",
    "title": "Introduction to high performance research computing in Julia",
    "section": "Copying files to the cluster",
    "text": "Copying files to the cluster\nWe will create a julia_workshop directory in ~/scratch, then copy our julia script in it.\n$ mkdir ~/scratch/julia_job\nOpen a new terminal window and from your local terminal (make sure that you are not on the remote terminal by looking at the bash prompt) run:\n$ scp /local/path/to/sort.jl <username>@<hostname>:scratch/julia_job\n$ scp /local/path/to/psort.jl <username>@<hostname>:scratch/julia_job\n\n# enter password"
  },
  {
    "objectID": "julia/intro_hpc.html#job-scripts",
    "href": "julia/intro_hpc.html#job-scripts",
    "title": "Introduction to high performance research computing in Julia",
    "section": "Job scripts",
    "text": "Job scripts\nWe will not run an interactive session with Julia on the cluster: we already have julia scripts ready to run. All we need to do is to write job scripts to submit to Slurm, the job scheduler used by the Alliance clusters.\nWe will create 2 scripts: one to run Julia on one core and one on as many cores as are available.\n\n\nYour turn:\n\nHow many processors are there on our training cluster?\n\nWe can run Julia with multiple threads by running:\n$ JULIA_NUM_THREADS=2 julia\nor:\n$ julia -t 2\nOnce in Julia, you can double check that Julia does indeed have access to 2 threads by running:\nThreads.nthreads()\nSave your job scripts in the files ~/scratch/julia_job/job_julia1c.sh and job_julia2c.sh for one and two cores respectively.\nHere is what our single core Slurm script looks like:\n#!/bin/bash\n#SBATCH --job-name=julia1c          # job name\n#SBATCH --time=00:01:00             # max walltime 1 min\n#SBATCH --cpus-per-task=1           # number of cores\n#SBATCH --mem=1000                  # max memory (default unit is megabytes)\n#SBATCH --output=julia1c%j.out      # file name for the output\n#SBATCH --error=julia1c%j.err       # file name for errors\n# %j gets replaced with the job number\n\necho Running NON parallel script\njulia sort.jl\necho Running parallel script on $SLURM_CPUS_PER_TASK core\njulia -t $SLURM_CPUS_PER_TASK psort.jl\n\n\nYour turn:\n\nWrite the script for 2 cores.\n\nNow, we can submit our jobs to the cluster:\n$ cd ~/scratch/julia_job\n$ sbatch job_julia1c.sh\n$ sbatch job_julia2c.sh\nAnd we can check their status with:\n$ sq      # This is an Alliance alias for `squeue -u $USER $@`\n\nPD stands for pending\nR stands for running"
  },
  {
    "objectID": "julia/macros.html",
    "href": "julia/macros.html",
    "title": "Macros",
    "section": "",
    "text": "Julia code is itself data and can be manipulated by the language while it is running."
  },
  {
    "objectID": "julia/macros.html#metaprogramming",
    "href": "julia/macros.html#metaprogramming",
    "title": "Macros",
    "section": "Metaprogramming",
    "text": "Metaprogramming\n\nLarge influence from Lisp.\nSince Julia is entirely written in Julia, it is particularly well suited for metaprogramming."
  },
  {
    "objectID": "julia/macros.html#parsing-and-evaluating",
    "href": "julia/macros.html#parsing-and-evaluating",
    "title": "Macros",
    "section": "Parsing and evaluating",
    "text": "Parsing and evaluating\nLet‚Äôs start with something simple:\n\n2 + 3\n\n5\n\n\nHow is this run internally?\nThe string \"2 + 3\" gets parsed into an expression:\n\nMeta.parse(\"2 + 3\")\n\n:(2 + 3)\n\n\nThen that expression gets evaluated:\n\neval(Meta.parse(\"2 + 3\"))\n\n5"
  },
  {
    "objectID": "julia/macros.html#macros",
    "href": "julia/macros.html#macros",
    "title": "Macros",
    "section": "Macros",
    "text": "Macros\nThey resemble functions and just like functions, they accept as input a tuple of arguments.\nBUT macros return an expression which is compiled directly rather than requiring a runtime eval call.\nSo they execute before the rest of the code is run.\nMacro‚Äôs names are preceded by @ (e.g.¬†@time).\nJulia comes with many macros and you can create your own with:\nmacro <name>()\n    <body>\nend"
  },
  {
    "objectID": "julia/macros.html#stylistic-conventions",
    "href": "julia/macros.html#stylistic-conventions",
    "title": "Macros",
    "section": "Stylistic conventions",
    "text": "Stylistic conventions\nAs with functions, Julia suggests to use lower case, without underscores, as macro names."
  },
  {
    "objectID": "julia/makie.html",
    "href": "julia/makie.html",
    "title": "Makie",
    "section": "",
    "text": "There are several popular data visualization libraries for the Julia programming language (e.g.¬†Plots, Gadfly, VegaLite, Makie). They vary in their precompilation time, time to first plot, layout capabilities, ability to handle 3D data, ease of use, and syntax style. In this landscape, Makie focuses on high performance, fancy layouts, and extensibility.\nMakie comes with multiple backends. In this workshop, we will cover:\n\nGLMakie (ideal for interactive 2D and 3D plotting)\nWGLMakie (an equivalent that runs within browsers)\nCairoMakie (best for high-quality vector graphics)\n\nWe will also see how to run Makie in the Alliance clusters.\n\nSlides (click and wait: my reveal.js presentations are heavy and take some time to load‚Ä¶)"
  },
  {
    "objectID": "julia/makie_slides.html#plotting-in-julia",
    "href": "julia/makie_slides.html#plotting-in-julia",
    "title": "Makie",
    "section": "Plotting in Julia",
    "text": "Plotting in Julia\n\nMany options:\n\nPlots.jl: high-level API for working with different back-ends (GR, Pyplot, Plotly‚Ä¶)\nPyPlot.jl: Julia interface to Matplotlib‚Äôs matplotlib.pyplot\nPlotlyJS.jl: Julia interface to plotly.js\nPlotlyLight.jl: the fastest plotting option in Julia by far, but limited features\nGadfly.jl: following the grammar of graphics popularized by Hadley Wickham in R\nVegaLite.jl: grammar of interactive graphics\nPGFPlotsX.jl: Julia interface to the PGFPlots LaTeX package\nUnicodePlots.jl: plots in the terminal üôÇ\n\n\n\n\nMakie.jl: powerful plotting ecosystem: animation, 3D, GPU optimization"
  },
  {
    "objectID": "julia/makie_slides.html#makie-ecosystem",
    "href": "julia/makie_slides.html#makie-ecosystem",
    "title": "Makie",
    "section": "Makie ecosystem",
    "text": "Makie ecosystem\n\n\nMain package:\n\nMakie: plots functionalities. Backend needed to render plots into images or vector graphics\n\n\n\n\n\nBackends:\n\nCairoMakie: vector graphics or high-quality 2D plots. Creates, but does not display plots (you need an IDE that does or you can use ElectronDisplay.jl)\nGLMakie: based on OpenGL; 3D rendering and interactivity in GLFW window (no vector graphics)\nWGLMakie: web version of GLMakie (plots rendered in a browser instead of a window)"
  },
  {
    "objectID": "julia/makie_slides.html#extensions",
    "href": "julia/makie_slides.html#extensions",
    "title": "Makie",
    "section": "Extensions",
    "text": "Extensions\n\nGeoMakie.jl add geographical plotting utilities to Makie\nAlgebraOfGraphics.jl turns plotting into a simple algebra of building blocks\nGraphMakie.jl to create network graphs"
  },
  {
    "objectID": "julia/makie_slides.html#cheatsheet-2d",
    "href": "julia/makie_slides.html#cheatsheet-2d",
    "title": "Makie",
    "section": "Cheatsheet 2D",
    "text": "Cheatsheet 2D\n\n\nFrom: Storopoli, Huijzer and Alonso (2021). Julia Data Science. https://juliadatascience.io. ISBN: 97984898"
  },
  {
    "objectID": "julia/makie_slides.html#cheatsheet-3d",
    "href": "julia/makie_slides.html#cheatsheet-3d",
    "title": "Makie",
    "section": "Cheatsheet 3D",
    "text": "Cheatsheet 3D\n\n\nFrom: Storopoli, Huijzer and Alonso (2021). Julia Data Science. https://juliadatascience.io. ISBN: 97984898"
  },
  {
    "objectID": "julia/makie_slides.html#resources",
    "href": "julia/makie_slides.html#resources",
    "title": "Makie",
    "section": "Resources",
    "text": "Resources\n\nOfficial documentation\nJulia Data Science book, chapter 5\nMany examples in the project Beautiful Makie"
  },
  {
    "objectID": "julia/makie_slides.html#troubleshooting",
    "href": "julia/makie_slides.html#troubleshooting",
    "title": "Makie",
    "section": "Troubleshooting",
    "text": "Troubleshooting\nInstalling GLMakie can be challenging. This page may lead you towards solutions\nCairoMakie and WGLMakie should install without issues"
  },
  {
    "objectID": "julia/makie_slides.html#figure",
    "href": "julia/makie_slides.html#figure",
    "title": "Makie",
    "section": "Figure",
    "text": "Figure\nLoad the package\nHere, we are using CairoMakie\n\nusing CairoMakie                        # no need to import Makie itself\n\n\n\n Create a Figure (container object)\n\nfig = Figure()\n\n\n\n\n\n\n\n \n\ntypeof(fig)\n\nFigure"
  },
  {
    "objectID": "julia/makie_slides.html#axis",
    "href": "julia/makie_slides.html#axis",
    "title": "Makie",
    "section": "Axis",
    "text": "Axis\n\n\nThen, you can create an Axis\n\nax = Axis(Figure()[1, 1])\n\nAxis with 0 plots:\n\n\n\n\n\n\n\ntypeof(ax)\n\nAxis"
  },
  {
    "objectID": "julia/makie_slides.html#plot",
    "href": "julia/makie_slides.html#plot",
    "title": "Makie",
    "section": "Plot",
    "text": "Plot\nFinally, we can add a plot\n\nfig = Figure()\nax = Axis(fig[1, 1])\nx = LinRange(-10, 10, 20)\ny = x\nscatter!(ax, x, y)  # Functions with ! transform their arguments\nfig"
  },
  {
    "objectID": "julia/makie_slides.html#d",
    "href": "julia/makie_slides.html#d",
    "title": "Makie",
    "section": "2D",
    "text": "2D\n\n\nusing CairoMakie\nusing StatsBase, LinearAlgebra\nusing Interpolations, OnlineStats\nusing Distributions\nCairoMakie.activate!(type = \"png\")\n\nfunction eq_hist(matrix; nbins = 256 * 256)\n    h_eq = fit(Histogram, vec(matrix), nbins = nbins)\n    h_eq = normalize(h_eq, mode = :density)\n    cdf = cumsum(h_eq.weights)\n    cdf = cdf / cdf[end]\n    edg = h_eq.edges[1]\n    interp_linear = LinearInterpolation(edg, [cdf..., cdf[end]])\n    out = reshape(interp_linear(vec(matrix)), size(matrix))\n    return out\nend\n\nfunction getcounts!(h, fn; n = 100)\n    for _ in 1:n\n        vals = eigvals(fn())\n        x0 = real.(vals)\n        y0 = imag.(vals)\n        fit!(h, zip(x0,y0))\n    end\nend\n\nm(;a=10rand()-5, b=10rand()-5) = [0 0 0 a; -1 -1 1 0; b 0 0 0; -1 -1 -1 -1]\n\nh = HeatMap(range(-3.5,3.5,length=1200), range(-3.5,3.5, length=1200))\ngetcounts!(h, m; n=2_000_000)\n\nwith_theme(theme_black()) do\n    fig = Figure(figure_padding=0,resolution=(600,600))\n    ax = Axis(fig[1,1]; aspect = DataAspect())\n    heatmap!(ax,-3.5..3.5, -3.5..3.5, eq_hist(h.counts); colormap = :bone_1)\n    hidedecorations!(ax)\n    hidespines!(ax)\n    fig\nend"
  },
  {
    "objectID": "julia/makie_slides.html#d-output",
    "href": "julia/makie_slides.html#d-output",
    "title": "Makie",
    "section": "2D",
    "text": "2D"
  },
  {
    "objectID": "julia/makie_slides.html#d-1",
    "href": "julia/makie_slides.html#d-1",
    "title": "Makie",
    "section": "3D",
    "text": "3D\n\n\nusing GLMakie, Random\nGLMakie.activate!()\n\nRandom.seed!(13)\nx = -6:0.5:6\ny = -6:0.5:6\nz = 6exp.( -(x.^2 .+ y' .^ 2)./4)\n\nbox = Rect3(Point3f(-0.5), Vec3f(1))\nn = 100\ng(x) = x^(1/10)\nalphas = [g(x) for x in range(0,1,length=n)]\ncmap_alpha = resample_cmap(:linear_worb_100_25_c53_n256, n, alpha = alphas)\n\nwith_theme(theme_dark()) do\n    fig, ax, = meshscatter(x, y, z;\n                           marker=box,\n                           markersize = 0.5,\n                           color = vec(z),\n                           colormap = cmap_alpha,\n                           colorrange = (0,6),\n                           axis = (;\n                                   type = Axis3,\n                                   aspect = :data,\n                                   azimuth = 7.3,\n                                   elevation = 0.189,\n            perspectiveness = 0.5),\n        figure = (;\n            resolution =(1200,800)))\n    meshscatter!(ax, x .+ 7, y, z./2;\n        markersize = 0.25,\n        color = vec(z./2),\n        colormap = cmap_alpha,\n        colorrange = (0, 6),\n        ambient = Vec3f(0.85, 0.85, 0.85),\n        backlight = 1.5f0)\n    xlims!(-5.5,10)\n    ylims!(-5.5,5.5)\n    hidedecorations!(ax; grid = false)\n    hidespines!(ax)\n    fig\nend"
  },
  {
    "objectID": "julia/makie_slides.html#d-1-output",
    "href": "julia/makie_slides.html#d-1-output",
    "title": "Makie",
    "section": "3D",
    "text": "3D"
  },
  {
    "objectID": "julia/makie_slides.html#cairomakie",
    "href": "julia/makie_slides.html#cairomakie",
    "title": "Makie",
    "section": "CairoMakie",
    "text": "CairoMakie\nCairoMakie will run without problem on the Alliance clusters\nIt is not designed for interactivity, so saving to file is what makes the most sense \n\nExample:\n\nsave(\"graph.png\", fig)\n\nRemember however that CairoMakie is 2D only (for now)"
  },
  {
    "objectID": "julia/makie_slides.html#glmakie",
    "href": "julia/makie_slides.html#glmakie",
    "title": "Makie",
    "section": "GLMakie",
    "text": "GLMakie\n\nGLMakie relies on GLFW to create windows with OpenGL\nGLFW doesn‚Äôt support creating contexts without an associated window\nThe dependency GLFW.jl will thus not install in the clusters‚Äîeven with X11 forwarding‚Äîunless you use VDI nodes, VNC, or Virtual GL"
  },
  {
    "objectID": "julia/makie_slides.html#wglmakie",
    "href": "julia/makie_slides.html#wglmakie",
    "title": "Makie",
    "section": "WGLMakie",
    "text": "WGLMakie\n\nYou can setup a server with JSServe.jl as per the documentation\nHowever, this method is intended at creating interactive widget, e.g.¬†for a website\nWhile this is really cool, it isn‚Äôt optimized for performance\nThere might also be a way to create an SSH tunnel to your local browser, although there is no documentation on this\nBest probably is to save to file"
  },
  {
    "objectID": "julia/makie_slides.html#conclusion-about-the-makie-ecosystem-on-production-clusters",
    "href": "julia/makie_slides.html#conclusion-about-the-makie-ecosystem-on-production-clusters",
    "title": "Makie",
    "section": "Conclusion about the Makie ecosystem on production clusters",
    "text": "Conclusion about the Makie ecosystem on production clusters\n\n2D plots: use CairoMakie and save to file\n3D plots: use WGLMakie and save to file"
  },
  {
    "objectID": "julia/multithreading.html",
    "href": "julia/multithreading.html",
    "title": "Multi-threading",
    "section": "",
    "text": "Julia, which was built with efficiency in mind, aimed from the start to have parallel programming abilities. These however came gradually: first, there were coroutines, which is not parallel programming, but allows independent executions of elements of code; then there was a macro allowing for loops to run on several cores, but this would not work on nested loops and it did not integrate with the coroutines or I/O. With version 1.3 however multi-threading capabilities were born.\nWhat is great about Julia‚Äôs new task parallelism is that it is incredibly easy to use: no need to write low-level code as with MPI to set where tasks are run. Everything is automatic."
  },
  {
    "objectID": "julia/multithreading.html#launching-julia-on-multiple-threads",
    "href": "julia/multithreading.html#launching-julia-on-multiple-threads",
    "title": "Multi-threading",
    "section": "Launching Julia on multiple threads",
    "text": "Launching Julia on multiple threads\nTo use Julia with multiple threads, we need to launch julia with the JULIA_NUM_THREADS environment variable or with the flag --threads/-t:\n$ JULIA_NUM_THREADS=n julia\nor\n$ julia -t n\nFirst, we need to know how many threads we actually have on our machine.\nThere are many Linux tools for this, but here are two particularly convenient options:\n# To get the total number of available processes\n$ nproc\n\n# For more information (# of sockets, cores per socket, threads per core)\n$ lscpu | grep -E '(S|s)ocket|Thread|^CPU\\(s\\)'\nSince I have 4 available processes (2 cores with 2 threads each), I can launch Julia on 4 threads:\n$ JULIA_NUM_THREADS=4 julia\nThis can also be done from within the Juno IDE.\nTo see how many threads we are using, as well as the ID of the current thread, you can run:\nThreads.nthreads()\nThreads.threadid()"
  },
  {
    "objectID": "julia/multithreading.html#for-loops-on-multiple-threads",
    "href": "julia/multithreading.html#for-loops-on-multiple-threads",
    "title": "Multi-threading",
    "section": "For loops on multiple threads",
    "text": "For loops on multiple threads\n\n\nYour turn:\n\nLaunch Julia on 1 thread and run the function below. Then run Julia on the maximum number of threads you have on your machine and run the same function.\n\nThreads.@threads for i = 1:10\n    println(\"i = $i on thread $(Threads.threadid())\")\nend\nUtilities such as htop allow you to visualize the working threads."
  },
  {
    "objectID": "julia/multithreading.html#generalization-of-multi-threading",
    "href": "julia/multithreading.html#generalization-of-multi-threading",
    "title": "Multi-threading",
    "section": "Generalization of multi-threading",
    "text": "Generalization of multi-threading\nLet‚Äôs consider the example presented in a Julia blog post in July 2019.\nBoth scripts sort a one dimensional array of 20,000,000 floats between 0 and 1, one with parallelism and one without.\nScript 1, without parallelism: sort.jl.\n# Create one dimensional array of 20,000,000 floats between 0 and 1\na = rand(20000000);\n\n# Use the MergeSort algorithm of the sort function\n# (in the standard Julia Base library)\nb = copy(a); @time sort!(b, alg = MergeSort);\n\n# Let's run the function a second time to remove the effect\n# of the initial compilation\nb = copy(a); @time sort!(b, alg = MergeSort);\nScript 2, with parallelism: psort.jl.\nimport Base.Threads.@spawn\n\n# The psort function is the same as the MergeSort algorithm\n# of the Base sort function with the addition of\n# the @spawn macro on one of the recursive calls\n\n# Sort the elements of `v` in place, from indices `lo` to `hi` inclusive\n\nfunction psort!(v, lo::Int=1, hi::Int = length(v))\n    \n    # 1 or 0 elements: nothing to do\n    if lo >= hi\n        return v\n    end\n    \n    # Below some cutoff: run in serial\n    if hi - lo < 100000\n        sort!(view(v, lo:hi), alg = MergeSort)\n        return v\n    end\n    \n    # Find the midpoint\n    mid = (lo + hi) >>> 1\n    \n    # Task to sort the lower half\n    # will run in parallel with the current call sorting the upper half\n    half = @spawn psort!(v, lo, mid)\n    psort!(v, mid + 1, hi)\n    # Wait for the lower half to finish\n    wait(half)\n\n    # Workspace for merging\n    temp = v[lo:mid]\n    \n    # Merge the two sorted sub-arrays\n    i, k, j = 1, lo, mid + 1\n    @inbounds while k < j <= hi\n        if v[j] < temp[i]\n            v[k] = v[j]\n            j += 1\n        else\n            v[k] = temp[i]\n            i += 1\n        end\n        k += 1\n    end\n    @inbounds while k < j\n        v[k] = temp[i]\n        k += 1\n        i += 1\n    end\n    \n    return v\nend\n\na = rand(20000000);\n\n# Now, let's use our function\nb = copy(a); @time psort!(b);\n\n# And running it a second time to remove\n# the effect of the initial compilation\nb = copy(a); @time psort!(b);\nNow, we can test both scripts with one or multiple threads.\n\nSingle thread, non-parallel script:\n\n$ julia /path/to/sort.jl\n2.234024 seconds (111.88 k allocations: 82.489 MiB, 0.21% gc time)\n2.158333 seconds (11 allocations: 76.294 MiB, 0.51% gc time)\n\nNote the lower time for the 2nd run due to pre-compilation.\n\n\nSingle thread, parallel script:\n\n$ julia /path/to/psort.jl\n2.748138 seconds (336.77 k allocations: 703.200 MiB, 2.24% gc time)\n2.438032 seconds (3.58 k allocations: 686.932 MiB, 0.27% gc time)\n\nEven longer time: normal, there was more to run (import package, read function).\n\n\n2 threads, non-parallel script:\n\n$ JULIA_NUM_THREADS=2 julia /path/to/sort.jl\n2.233720 seconds (111.87 k allocations: 82.145 MiB, 0.21% gc time)\n2.155232 seconds (11 allocations: 76.294 MiB, 0.54% gc time)\n\nRemarkably similar to the single thread: the addition of a thread did not change anything.\n\n\n2 threads, parallel script:\n\n$ JULIA_NUM_THREADS=2 julia /path/to/psort.jl\n1.773643 seconds (336.99 k allocations: 703.171 MiB, 4.08% gc time)\n1.460539 seconds (3.79 k allocations: 686.935 MiB, 0.47% gc time)\n\n33% faster.\nNot twice as fast as one could have hoped since processes have to wait for each other. But that‚Äôs a good improvement.\n\n\n4 threads, non-parallel script:\n\n$ JULIA_NUM_THREADS=4 julia /path/to/sort.jl\n2.231717 seconds (111.87 k allocations: 82.145 MiB, 0.21% gc time)\n2.153509 seconds (11 allocations: 76.294 MiB, 0.53% gc time)\n\nAgain: same result as the single thread.\n\n\n4 threads, parallel script:\n\n$ JULIA_NUM_THREADS=4 julia /path/to/psort.jl\n1.291714 seconds (336.98 k allocations: 703.171 MiB, 3.48% gc time)\n1.194282 seconds (3.78 k allocations: 686.935 MiB, 5.19% gc time)\n\nEven though we only split our code in 2 tasks, there is still an improvement over the 2 thread run."
  },
  {
    "objectID": "julia/non_interactive.html",
    "href": "julia/non_interactive.html",
    "title": "Non interactive execution",
    "section": "",
    "text": "Julia scripts have a .jl extension.\nThe include function sources a Julia script (in a REPL session or in another script):\ninclude(\"file.jl\")\nThe code contained in file.jl is thus run non interactively."
  },
  {
    "objectID": "julia/non_interactive.html#running-code-from-the-command-line",
    "href": "julia/non_interactive.html#running-code-from-the-command-line",
    "title": "Non interactive execution",
    "section": "Running code from the command line",
    "text": "Running code from the command line\nYou can run scripts by passing them to the julia command on the command line:\n$ julia script.jl\n\nThis code is run in a terminal, not in Julia, as is indicated by the $ prompt.\n\nYou can also evaluate single expressions in Julia from the command line by using the flag -e:\n$ julia -e 'println(2 + 3)'\n5\n\nPassing arguments\n\nTo the julia command itself\nIf you want to pass arguments to the julia command itself, you need to add them before the script or the Julia expression.\n\nExample:\n\n$ julia -O script.jl\n\n\nTo the script/Julia expression\nTo pass arguments to the script (or Julia expression if you use -e), you add them after the script or expression:\n$ julia script.jl arg1 arg2 arg3\narg1, arg2, arg3 will be passed in the global constant ARGS and interpreted as arguments to the script.\n\nExample passing arguments to an expression:\n\n$ julia -e 'for x in ARGS; println(x); end' 2 3\n2\n3\n\n\nTo both\nTo pass arguments both to the julia command and to the script/expression, you need to add the -- delimiter before the script/expression:\n$ julia [switches] -- [programfile] [args...]\n\nExample:\n\n$ julia -O -- script.jl arg1 arg2"
  },
  {
    "objectID": "julia/packages.html",
    "href": "julia/packages.html",
    "title": "Packages",
    "section": "",
    "text": "Julia comes with a collection of packages. In Linux, they are in /usr/share/julia/stdlib/vx.x.\nHere is the list:\nBase64\nCRC32c\nDates\nDelimitedFiles\nDistributed\nFileWatching\nFuture\nInteractiveUtils\nLibdl\nLibGit2\nLinearAlgebra\nLogging\nMarkdown\nMmap\nPkg\nPrintf\nProfile\nRandom\nREPL\nSerialization\nSHA\nSharedArrays\nSockets\nSparseArrays\nStatistics\nSuiteSparse\nTest\nUnicode\nUUIDs"
  },
  {
    "objectID": "julia/packages.html#installing-additional-packages",
    "href": "julia/packages.html#installing-additional-packages",
    "title": "Packages",
    "section": "Installing additional packages",
    "text": "Installing additional packages\nYou can install additional packages.\nThese go to your personal library in ~/.julia (this is also where your REPL history is saved).\nAll registered packages are on GitHub and can easily be searched here.\nThe GitHub star system allows you to easily judge the popularity of a package and to see whether it is under current development.\nIn addition to these, there are unregistered packages and you can build your own.\n\n\nYour turn:\n\nTry to find a list of popular plotting packages.\n\nYou can manage your personal library easily in package mode with the commands:\n(env) pkg> add <package>   # install <package>\n(env) pkg> rm <package>    # uninstall <package>\n(env) pkg> up <package>    # upgrade <package>\n(env) pkg> st              # st or status: list installed packages\n(env) pkg> up              # up or upgrade: upgrade all packages\n\nReplace <package> by the name of the package (e.g.¬†Plots ).\n\nYou can install, uninstall, or update several packages at once by listing them with a space:\n(env) pkg> add <package1> <package2> <package3>\nAn alternative to this convenience mode is to load the package manager (package Pkg, part of stdlib) and use it as you would any other package:\nusing Pkg\n\nPkg.add(\"<package>\")        # install <package>\nPkg.rm(\"<package>\")         # uninstall <package>\nPkg.status(\"<package>\")     # status of <package>\nPkg.update(\"<package>\")     # update <package>\nPkg.update()                # status of all installed packages\nPkg.status()                # update all packages\n\nThe short forms up and st do not work in this context.\n\nTo install, uninstall, or update several packages at once in this context, you need to create an array:\nPkg.add([\"<package1>\", \"<package2>\", \"<package3>\"])\n\n\nYour turn:\n\nCheck your list of packages; install the packages Plots, GR, Distributions, StatsPlots, and UnicodePlot; then check that list again.\n\n\n\nYour turn:\n\nNow go explore your ~/.julia. If you don‚Äôt find it, make sure that your file explorer allows you to see hidden files."
  },
  {
    "objectID": "julia/packages.html#loading-packages",
    "href": "julia/packages.html#loading-packages",
    "title": "Packages",
    "section": "Loading packages",
    "text": "Loading packages\nWhether a package from the standard library or one you installed, before you can use a package you need to load it. This has to be done at each new Julia session so the code to load packages should be part of your scripts.\nThis is done with the using command (e.g.¬†using Plots)."
  },
  {
    "objectID": "julia/performance.html",
    "href": "julia/performance.html",
    "title": "Performance",
    "section": "",
    "text": "The one thing you need to remember: avoid global variables.\nThis means: avoid variables defined in the global environment."
  },
  {
    "objectID": "julia/performance.html#definitions",
    "href": "julia/performance.html#definitions",
    "title": "Performance",
    "section": "Definitions",
    "text": "Definitions\nScope of variables: ‚ÄÉ¬†Environment within which a variables exist\nGlobal scope: ‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÇGlobal environment of a module\nLocal scope: ‚ÄÇ‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÇEnvironment within a function, a loop, a struct, a macro, etc."
  },
  {
    "objectID": "julia/performance.html#why-avoid-global-variables",
    "href": "julia/performance.html#why-avoid-global-variables",
    "title": "Performance",
    "section": "Why avoid global variables?",
    "text": "Why avoid global variables?\nThe Julia compiler is not good at optimizing code using global variables.\nPart of the reason is that their type can change.\n\nExample\nWe will use the @time macro to time a loop:\n\nIn the global environment:\n\n\ntotal = 0\nn = 1e6\n\n@time for i in 1:n\n    global total += i\nend\n\n  0.180548 seconds (4.00 M allocations: 76.491 MiB, 34.82% compilation time)\n\n\n\nNote the garbage collection (gc) time: 14% of total time.\nGarbage collection time is a sign of poor code.\n\n\nIn a local environment (a function):\n\n\nfunction local_loop(total, n)\n    total = total\n    @time for i in 1:n\n        global total += i\n    end\nend\n\nlocal_loop(0, 1e6)\n\n  0.032134 seconds (2.00 M allocations: 30.518 MiB)\n\n\n\nWe get a 7.5 speedup.\nThe memory allocation also decreased by more than half.\n\nFor more accurate performance measurements, you should use the @btime macro from the BenchmarkTools package which excludes compilation time from the timing, averages metrics over multiple runs, and is highly customizable."
  },
  {
    "objectID": "julia/plotting.html",
    "href": "julia/plotting.html",
    "title": "Plotting",
    "section": "",
    "text": "It can be convenient to plot directly in the REPL (for instance when using SSH).\n\nusing UnicodePlots\nhistogram(randn(1000), nbins=40)\n\n\n                ‚îå                                        ‚îê \n   [-3.2, -3.0) ‚î§‚ñå 1                                       \n   [-3.0, -2.8) ‚î§‚ñå 1                                       \n   [-2.8, -2.6) ‚î§‚ñà‚ñã 4                                      \n   [-2.6, -2.4) ‚î§‚ñà‚ñç 3                                      \n   [-2.4, -2.2) ‚î§‚ñà‚ñç 3                                      \n   [-2.2, -2.0) ‚î§‚ñà‚ñã 4                                      \n   [-2.0, -1.8) ‚î§‚ñà‚ñà‚ñà‚ñà‚ñã 11                                  \n   [-1.8, -1.6) ‚î§‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå 13                                 \n   [-1.6, -1.4) ‚î§‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã 35                        \n   [-1.4, -1.2) ‚î§‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå 30                          \n   [-1.2, -1.0) ‚î§‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã 40                      \n   [-1.0, -0.8) ‚î§‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé 53                \n   [-0.8, -0.6) ‚î§‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé 65           \n   [-0.6, -0.4) ‚î§‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå 68          \n   [-0.4, -0.2) ‚î§‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé 65           \n   [-0.2,  0.0) ‚î§‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  86  \n   [ 0.0,  0.2) ‚î§‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  86  \n   [ 0.2,  0.4) ‚î§‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé 72        \n   [ 0.4,  0.6) ‚î§‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå 68          \n   [ 0.6,  0.8) ‚î§‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä 57               \n   [ 0.8,  1.0) ‚î§‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã 71         \n   [ 1.0,  1.2) ‚î§‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  43                    \n   [ 1.2,  1.4) ‚î§‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 36                       \n   [ 1.4,  1.6) ‚î§‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ 31                          \n   [ 1.6,  1.8) ‚î§‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 12                                 \n   [ 1.8,  2.0) ‚î§‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 12                                 \n   [ 2.0,  2.2) ‚î§‚ñà‚ñà‚ñà‚ñä 9                                    \n   [ 2.2,  2.4) ‚î§‚ñà‚ñà‚ñå 6                                     \n   [ 2.4,  2.6) ‚î§‚ñà‚ñà‚ñà‚ñà‚ñé 10                                  \n   [ 2.6,  2.8) ‚î§‚ñä 2                                       \n   [ 2.8,  3.0) ‚î§‚ñå 1                                       \n   [ 3.0,  3.2) ‚î§‚ñå 1                                       \n   [ 3.2,  3.4) ‚î§‚ñå 1                                       \n                ‚îî                                        ‚îò \n                                 Frequency                 \n\n\n\nMost of the time however, you will want to make nicer looking graphs. There are many options to plot in Julia.\nPlots is a convenient Julia package which allows to use the same code with several graphing backends such as the GR framework (great for speed), Plotly.js (allows interaction with your graphs in a browser), or PyPlot. The default backend is the GR framework.\nStatsPlots is an enhanced version with added stats functionality.\n\nExample:\n\n\n# First run takes time as the package needs to compile\nusing StatsPlots\nStatsPlots.histogram(randn(1000), bins=40)\n\n\n\n\n\nHere, we need to explicitly run StatsPlots.histogram rather than histogram to prevent a conflict with the function of the same name from the package UnicodePlots."
  },
  {
    "objectID": "julia/resources.html",
    "href": "julia/resources.html",
    "title": "Resources",
    "section": "",
    "text": "Official Julia website\nOfficial Julia manual\nOnline training material\nThe Julia YouTube channel\nThe Julia Wikibook\nA blog aggregator for Julia"
  },
  {
    "objectID": "julia/resources.html#getting-help",
    "href": "julia/resources.html#getting-help",
    "title": "Resources",
    "section": "Getting help",
    "text": "Getting help\n\nDiscourse forum\n[julia] tag on Stack Overflow\nSlack team (you need to agree to the community code of conduct at slackinvite.julialang.org to receive an invitation)\n#julialang hashtag on Twitter\nSubreddit\nGitter channel\n#julia IRC channel on Freenode"
  },
  {
    "objectID": "julia/tabular.html",
    "href": "julia/tabular.html",
    "title": "Working with tabular data:",
    "section": "",
    "text": "Requirements:\n1 - The current Julia stable release\nInstallation instructions can be found here.\n2 - The packages: CSV, DataFrames, TimeSeries, Plots\nPackages can be installed with ] add <package>.\n3 - Covid-19 data from the Johns Hopkins University CSSE repository\nClone (git clone <repo url>) or download and unzip the repository."
  },
  {
    "objectID": "julia/tabular.html#load-packages",
    "href": "julia/tabular.html#load-packages",
    "title": "Working with tabular data:",
    "section": "Load packages",
    "text": "Load packages\nusing CSV\nusing DataFrames\nusing Dates          # From the standard Julia library\nusing TimeSeries\nusing NamedArrays\nusing Plots\n\nWe will use the GR framework as a backend for Plots."
  },
  {
    "objectID": "julia/tabular.html#data-until-march-22-2020",
    "href": "julia/tabular.html#data-until-march-22-2020",
    "title": "Working with tabular data:",
    "section": "Data until March 22, 2020",
    "text": "Data until March 22, 2020\n\n\nfrom xkcd.com\n\nThe files in the Johns Hopkins University CSSE repository have changed over time.\nIn this workshop, we will use 2 sets of files:\n\na first set from January 22, 2020 until March 22, 2020\na second set from January 22, 2020 to the present\n\nBoth sets contain data on confirmed and dead cases for world countries and in some cases their subregions (provinces, states, etc. which I will globally here call ‚Äúprovinces‚Äù).\nThe first set also contains numbers of recovered cases which allow to calculate numbers of currently ill persons (of course, keep in mind that all these data represent various degrees of underestimation and are flawed in many ways, amongst which are varying levels of testing efforts both geographically and over time, under-reporting, etc).\nThe second set does not contain recovered cases (many overwhelmed countries stopped monitoring this at some point).\nWe will play with the first set together and you will then try to play with the second set on your own.\n\nLoad the data\nIf you did not clone or download and unzip the Covid-19 data repository in your working directory, adapt the path consequently.\n#= create a variable with the path we are interested in;\nthis makes the code below a bit shorter =#\ndir = \"COVID-19/csse_covid_19_data/csse_covid_19_time_series\"\n\n# create a list of the full paths of all the files in dir\nlist = joinpath.(relpath(dir), readdir(dir))\n\n#= read in the 3 csv files with confirmed, dead, and recovered numbers\ncorresponding to the first set of data (until March 22, 2020) =#\ndat = DataFrame.(CSV.File.(list[collect(2:4)]))\nWe now have a one-dimensional array of 3 DataFrames called dat.\n\n\nTransform data into long format\n# rename some variables to easier names\nDataFrames.rename!.(dat, Dict.(1 => Symbol(\"province\"),\n                               2 => Symbol(\"country\")))\n\n# create a one-dimensional array of strings\nvar = [\"total\", \"dead\", \"recovered\"]\n\n#= transform the data into long format in a vectorized fashion\nusing both our one-dimensional arrays of 3 elements =#\ndatlong = map((x, y) -> stack(x, Not(collect(1:4)),\n                              variable_name = Symbol(\"date\"),\n                              value_name = Symbol(\"$y\")),\n              dat, var)\nWe now have a one-dimensional array of 3 DataFrames in long format called datlong.\n# join all elements of this array into a single DataFrame\nall = join(datlong[1], datlong[2], datlong[3],\n           on = [:date, :country, :province, :Lat, :Long])\n\n# get rid of \"Lat\" and \"Long\" and re-order the columns\nselect!(all, [4, 3, 1, 2, 7, 8])\n\n#= turn the year from 2 digits to 4 digits using regular expression\n(in a vectorised fashion by braodcasting with the dot notation);\nthen turn these values into strings, and finally into dates =#\nall.date = Date.(replace.(string.(all[:, 3]),\n                          r\"(.*)(..)$\" => s\"\\g<1>20\\2\"), \"m/dd/yy\");\n\n#= replace the missing values by the string \"NA\"\n(these are not real missing values, but rather non applicable ones) =#\nreplace!(all.province, missing => \"NA\");\nWe now have a single DataFrame called all, in long format, with the variables confirmed, dead, recovered, and ill.\nCalculate the number of currently ill individuals (again, in a vectorized fashion, by broadcasting with the dot notation):\nall.current = all.total .- all.dead .- all.recovered;\n\n\nWorld summary\nTo make a single plot with world totals of confirmed, dead, recovered, and ill cases, we want the sums of these variables for each day. We do this by grouping the data by date:\nworld = by(all, :date,\n           total = :total => sum,\n           dead = :dead => sum,\n           recovered = :recovered => sum,\n           current = :current => sum)\nNow we can plot our new variable world.\nAs our data is a time series, we need to transform it to a TimeArray thanks to the TimeArray() function from the TimeSeries package.\nplot(TimeArray(world, timestamp = :date),\n     title = \"World\",\n     legend = :outertopright,\n     widen = :false)\n Data until March 22, 2020\n\n\nCountries/provinces summaries\nNow, we want to group the data by country:\ncountries = groupby(all, :country)\nWe also need to know how the authors of the dataset decided to label the various countries and their subregions.\nFor example, if you want to see what the data looks like for France, Canada, and India, you can run:\ncountries[findall(x -> \"France\" in x, keys(countries))]\ncountries[findall(x -> \"Canada\" in x, keys(countries))]\ncountries[findall(x -> \"India\" in x, keys(countries))]\nThen you need to subset the data for the countries or provinces you are interested in.\nHere are some examples:\n# countries for which there are data for several provinces\ncanada = all[all[:, :country] .== \"Canada\", :]\nus = all[all[:, :country] .== \"US\", :]\nchina = all[all[:, :country] .== \"China\", :]\n\n# countries with no province data\nskorea = all[all[:, :country] .== \"Korea, South\", :]\ntaiwan = all[all[:, :country] .== \"Taiwan*\", :]\nsingapore = all[all[:, :country] .== \"Singapore\", :]\nitaly = all[all[:, :country] .== \"Italy\", :]\nspain = all[all[:, :country] .== \"Spain\", :]\n\n#= countries wich have subregions spread widely in the world;\nhere, I took the arbitrary decision to only look at the main subregions =#\nfrance = all[all[:, :province] .== \"France\", :]\nuk = all[all[:, :province] .== \"United Kingdom\", :]\n\n# provinces\nbc = all[all[:, :province] .== \"British Columbia\", :]\nny = all[all[:, :province] .== \"New York\", :]\nCalculate the totals for Canada, US, and China which all have data for subregions:\ncanada, us, china = by.([canada, us, china], :date,\n                        total = :total => sum,\n                        dead = :dead => sum,\n                        recovered = :recovered => sum,\n                        current = :current => sum)\nloclist1 = [canada, us, china]\nloctitles1 = [\"Canada\", \"US\", \"China\"]\n\npcanada, pus, pchina =\n    map((x, y) -> plot(TimeArray(x, timestamp = :date),\n                       title = \"$y\", legend = :outertopright,\n                       widen = :false, dpi = :300),\n        loclist1, loctitles1)\nloclist2 = [france, bc, ny, taiwan, skorea, singapore, spain, italy, uk]\nloctitles2 = [\"France\", \"BC\", \"NY\", \"Taiwan\", \"South Korea\",\n              \"Singapore\", \"Spain\", \"Italy\", \"UK\"]\n\npfrance, pbc, pny, ptaiwan, pskorea,\npsingapore, pspain, pitaly, puk =\n    map((x, y) -> plot(TimeArray(select(x, Not([:country, :province])),\n                                 timestamp = :date),\n                       title = \"$y\", legend = :outertopright,\n                       widen = :false, dpi = :300),\n        loclist2, loctitles2)\nNow, let‚Äôs plot a few countries/provinces:\n\nNorth America\nplot(pcanada, pbc, pus, pny,\n     legend = false, titlefontsize = 7, tickfontsize = 6)\n Data until March 22, 2020\n\n\nAsia\nplot(pchina, ptaiwan, pskorea, psingapore,\n     legend = false, titlefontsize = 7, tickfontsize = 6)\n Data until March 22, 2020\n\n\nEurope\nplot(pfrance, pspain, pitaly, puk,\n     legend = false, titlefontsize = 7, tickfontsize = 6)\n Data until March 22, 2020"
  },
  {
    "objectID": "julia/tabular.html#data-up-to-the-present",
    "href": "julia/tabular.html#data-up-to-the-present",
    "title": "Working with tabular data:",
    "section": "Data up to the present",
    "text": "Data up to the present\n\nSummary graphs\n\n\nYour turn:\n\nWrite the code to create an up-to-date graph for the world using the files: time_series_covid19_confirmed_global.csv and time_series_covid19_deaths_global.csv.\n\nHere is the result:\n Data until March 25, 2020\n\n\nYour turn:\n\nCreate up-to-date graphs for the countries and/or provinces of your choice.\n\nHere are a few possible results:\n Data until March 25, 2020\n\n\nCountries comparison\nOur side by side graphs don‚Äôt make comparisons very easy since they vary greatly in their axes scales.\nOf course, we could constrain them to have the same axes, but then, why not plot multiple countries or provinces in the same graph?\ncanada[!, :loc] .= \"Canada\";\nchina[!, :loc] .= \"China\";\n\nall = join(all, canada, china, on = [:date, :total, :dead, :loc],\n           kind = :outer)\n\nconfirmed = unstack(all[:, collect(3:5)], :loc, :total)\n\nconf_sel = select(confirmed,\n                  [:date, :Italy, :Spain, :China, :Iran,\n                   :France, :US, Symbol(\"South Korea\"), :Canada])\n\nplot(TimeArray(conf_sel, timestamp = :date),\n     title = \"Confirmed across a few countries\",\n     legend = :outertopright, widen = :false)\n Data until March 25, 2020\n\n\nYour turn:\n\nWrite the code to make a similar graph with the number of deaths in a few countries of your choice.\n\nHere is a possible result:\n Data until March 25, 2020"
  },
  {
    "objectID": "julia/types.html",
    "href": "julia/types.html",
    "title": "Types",
    "section": "",
    "text": "Type safety (catching errors of inadequate type) performed at compilation time.\n\nExamples: C, C++, Java, Fortran, Haskell.\n\n\n\n\nType safety performed at runtime.\n\nExamples: Python, JavaScript, PHP, Ruby, Lisp.\n\n\n\n\n\nJulia type system is dynamic (types are unknown until runtime), but types can be declared, optionally bringing the advantages of static type systems.\nThis gives users the freedom to choose between an easy and convenient language, or a clearer, faster, and more robust one (or a combination of the two)."
  },
  {
    "objectID": "julia/types.html#julia-types-a-hierarchical-tree",
    "href": "julia/types.html#julia-types-a-hierarchical-tree",
    "title": "Types",
    "section": "Julia types: a hierarchical tree",
    "text": "Julia types: a hierarchical tree\nAt the bottom: ‚ÄÉconcrete types.\nAbove: ‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉabstract types (concepts for collections of concrete types).\nAt the top: ‚ÄÉ‚ÄÉ‚ÄÇ¬†the Any type, encompassing all types.\n\n\nFrom O‚ÄôReilly\n\nOne common type missing in this diagram is the boolean type.\nIt is a subtype of the integer type, as can be tested with the subtype operator <:\n\nBool <: Integer\n\ntrue\n\n\nIt can also be made obvious by the following:\n\nfalse == 0\n\ntrue\n\n\n\ntrue == 1\n\ntrue\n\n\n\na = true;\nb = false;\n3a + 2b\n\n3"
  },
  {
    "objectID": "julia/types.html#optional-type-declaration",
    "href": "julia/types.html#optional-type-declaration",
    "title": "Types",
    "section": "Optional type declaration",
    "text": "Optional type declaration\nDone with ::\n<value>::<type>\n\nExample:\n\n\n2::Int\n\n2"
  },
  {
    "objectID": "julia/types.html#illustration-of-type-safety",
    "href": "julia/types.html#illustration-of-type-safety",
    "title": "Types",
    "section": "Illustration of type safety",
    "text": "Illustration of type safety\nThis works:\n\n2::Int\n\n2\n\n\nThis doesn‚Äôt work:\n\n2.0::Int\n\nLoadError: TypeError: in typeassert, expected Int64, got a value of type Float64\n\n\nType declaration is not yet supported on global variables; this is used in local contexts such as inside a function.\n\nExample:\n\n\nfunction floatsum(a, b)\n    (a + b)::Float64\nend\n\nfloatsum (generic function with 1 method)\n\n\nThis works:\n\nfloatsum(2.3, 1.0)\n\n3.3\n\n\nThis doesn‚Äôt work:\n\nfloatsum(2, 4)\n\nLoadError: TypeError: in typeassert, expected Float64, got a value of type Int64"
  },
  {
    "objectID": "julia/types.html#information-and-conversion",
    "href": "julia/types.html#information-and-conversion",
    "title": "Types",
    "section": "Information and conversion",
    "text": "Information and conversion\nThe typeof function gives the type of an object:\n\ntypeof(2)\n\nInt64\n\n\n\ntypeof(2.0)\n\nFloat64\n\n\n\ntypeof(\"Hello, World!\")\n\nString\n\n\n\ntypeof(true)\n\nBool\n\n\n\ntypeof((2, 4, 1.0, \"test\"))\n\nTuple{Int64, Int64, Float64, String}\n\n\nConversion between types is possible in some cases:\n\nInt(2.0)\n\n2\n\n\n\ntypeof(Int(2.0))\n\nInt64\n\n\n\nChar(2.0)\n\n'\\x02': ASCII/Unicode U+0002 (category Cc: Other, control)\n\n\n\ntypeof(Char(2.0))\n\nChar"
  },
  {
    "objectID": "julia/types.html#stylistic-convention",
    "href": "julia/types.html#stylistic-convention",
    "title": "Types",
    "section": "Stylistic convention",
    "text": "Stylistic convention\nThe names of types start with a capital letter and camel case is used in multiple-word names."
  },
  {
    "objectID": "ml/audio_dataloader.html",
    "href": "ml/audio_dataloader.html",
    "title": "Creating an audio DataLoader",
    "section": "",
    "text": "import torch\nimport torchaudio\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "ml/audio_dataloader.html#download-and-unzip-data",
    "href": "ml/audio_dataloader.html#download-and-unzip-data",
    "title": "Creating an audio DataLoader",
    "section": "Download and unzip data",
    "text": "Download and unzip data\nPyTorch comes with many classic datasets.\n\nExamples:\n\nlist of available datasets for vision\nlist of audio datasets\nlist of texts datasets\n\n\nThis is convenient to develop and test your model, or to compare its performance with existing models using these datasets.\nHere, we will use the YESNO dataset which can be accessed through the torchaudio.datasets.YESNO class:\nhelp(torchaudio.datasets.YESNO)\nHelp on class YESNO in module torchaudio.datasets.yesno:\n\nclass YESNO(torch.utils.data.dataset.Dataset)\n\n |  YESNO(root: Union[str, pathlib.Path], url: str =\n |    'http://www.openslr.org/resources/1/waves_yesno.tar.gz', \n |    folder_in_archive: str = 'waves_yesno', \n |    download: bool = False) -> None\n |  \n |  Args:\n |    root (str or Path): Path to the directory where the dataset is found \n |      or downloaded.\n |    url (str, optional): The URL to download the dataset from.\n |      (default: \"http://www.openslr.org/resources/1/waves_yesno.tar.gz\")\n |    folder_in_archive (str, optional):\n |      The top-level directory of the dataset. (default: \"waves_yesno\")\n |    download (bool, optional):\n |      Whether to download the dataset if it is not found at root path. \n |      (default: False).\nThe root argument sets the location of the downloaded data.\n\nWhere to store this data in the cluster\nWe will all use the same data. It would make little sense to all download it in our home directory.\n\nIn the Alliance clusters, a good place to store data shared amongst members of a project is in the /project file system.\nYou usually belong to /project/def-<group>, where <group> is the name of your PI. You can access it from your home directory through the symbolic link ~/projects/def-<group>.\n\nIn our training cluster, we are all part of the group def-sponsor00, accessible through /project/def-sponsor00 (or the hyperlink ~/projects/def-sponsor00).\nWe will thus use ~/projects/def-sponsor00/data as the root argument for torchaudio.datasets.yesno):\nyesno_data = torchaudio.datasets.YESNO(\n    '~/projects/def-sponsor00/data/',\n    download=True)"
  },
  {
    "objectID": "ml/audio_dataloader.html#explore-the-data",
    "href": "ml/audio_dataloader.html#explore-the-data",
    "title": "Creating an audio DataLoader",
    "section": "Explore the data",
    "text": "Explore the data\nA data point in YESNO is a tuple of waveform, sample_rate, and labels (the labels are 1 for ‚Äúyes‚Äù and 0 for ‚Äúno‚Äù).\nLet‚Äôs have a look at the first data point:\n\nyesno_data[0]\n\n(tensor([[ 3.0518e-05,  6.1035e-05,  3.0518e-05,  ..., -1.8616e-03,\n          -2.2583e-03, -1.3733e-03]]),\n 8000,\n [0, 0, 0, 0, 1, 1, 1, 1])\n\n\nOr, more nicely:\n\nwaveform, sample_rate, labels = yesno_data[0]\nprint(\"Waveform: {}\\nSample rate: {}\\nLabels: {}\".format(waveform, sample_rate, labels))\n\nWaveform: tensor([[ 3.0518e-05,  6.1035e-05,  3.0518e-05,  ..., -1.8616e-03,\n         -2.2583e-03, -1.3733e-03]])\nSample rate: 8000\nLabels: [0, 0, 0, 0, 1, 1, 1, 1]\n\n\nYou can also plot the data. For this, we will use pyplot from matplotlib.\nLet‚Äôs look at the waveform:\n\nplt.figure()\nplt.plot(waveform.t().numpy())"
  },
  {
    "objectID": "ml/audio_dataloader.html#split-the-data-into-a-training-set-and-a-testing-set",
    "href": "ml/audio_dataloader.html#split-the-data-into-a-training-set-and-a-testing-set",
    "title": "Creating an audio DataLoader",
    "section": "Split the data into a training set and a testing set",
    "text": "Split the data into a training set and a testing set\n\ntrain_size = int(0.8 * len(yesno_data))\ntest_size = len(yesno_data) - train_size\ntrain_dataset, test_dataset = torch.utils.data.random_split(yesno_data, [train_size, test_size])"
  },
  {
    "objectID": "ml/audio_dataloader.html#create-training-and-testing-dataloaders",
    "href": "ml/audio_dataloader.html#create-training-and-testing-dataloaders",
    "title": "Creating an audio DataLoader",
    "section": "Create training and testing DataLoaders",
    "text": "Create training and testing DataLoaders\nDataLoaders are Python iterables created by the torch.utils.data.DataLoader class from a dataset and a sampler.\nWe already have a dataset (yesno_data). Now we need a sampler (or sampling strategy) to draw samples from it. The sampling strategy contains the batch size, whether the data get shuffled prior to sampling, the number of workers used if the data is loaded in parallel, etc.\nTo create a training DataLoader with shuffled data and batch size of 1 (the default), we run:\n\ntrain_loader = torch.utils.data.DataLoader(train_dataset, shuffle=True)\n\ndata_loader is an iterable of 0.8*60=48 elements (80% of the 60 samples in the YESNO dataset):\n\nlen(train_loader)\n\n48\n\n\nWe do the same to create the testing DataLoader:\n\ntest_loader = torch.utils.data.DataLoader(test_dataset, shuffle=True)"
  },
  {
    "objectID": "ml/audio_dataloader.html#why-do-we-need-to-create-a-dataloader",
    "href": "ml/audio_dataloader.html#why-do-we-need-to-create-a-dataloader",
    "title": "Creating an audio DataLoader",
    "section": "Why do we need to create a DataLoader?",
    "text": "Why do we need to create a DataLoader?\nA DataLoader is the iterable that ‚Äúpresents‚Äù data to a model. When we train a model, we run it for each element of the DataLoader in a for loop:\nfor i in data_loader:\n    <run some model>"
  },
  {
    "objectID": "ml/autograd.html",
    "href": "ml/autograd.html",
    "title": "Automatic differentiation",
    "section": "",
    "text": "Imagine how hard it would be to write the chain rules of neural networks (with so many derivatives!) in backpropagation manually.\nPyTorch has automatic differentiation abilities‚Äîmeaning that it can track all the operations conducted on tensors and do the backprop for you‚Äîthanks to its package torch.autograd.\nLet‚Äôs have a first look at it."
  },
  {
    "objectID": "ml/autograd.html#tracking-computations",
    "href": "ml/autograd.html#tracking-computations",
    "title": "Automatic differentiation",
    "section": "Tracking computations",
    "text": "Tracking computations\nPyTorch does not track all the computations on all the tensors (this would be extremely memory intensive!). To start tracking computations on a vector, set the .requires_grad attribute to True:\nimport torch\n\nx = torch.rand(3, 7, requires_grad=True)\nprint(x)\n\nThe grad_fun attribute\nWhenever a tensor is created by an operation involving a tracked tensor, it has a grad_fun attribute:\nx = torch.ones(2, 4, requires_grad=True)\nprint(x)\ny = x + 1\nprint(y)\nprint(y.grad_fn)\n\n\nJudicious tracking\nYou don‚Äôt want to track more than is necessary. There are multiple ways to avoid tracking what you don‚Äôt want to.\nYou can simply stop tracking computations on a tensor with the method detach:\nx = torch.rand(4, 3, requires_grad=True)\nprint(x)\nprint(x.detach_())\nYou can change its requires_grad flag:\nx = torch.rand(4, 3, requires_grad=True)\nprint(x)\nprint(x.requires_grad_(False))\nAlternatively, you can wrap any code you don‚Äôt want to track with with torch.no_grad():\nwith torch.no_grad():\n    <some code>"
  },
  {
    "objectID": "ml/autograd.html#calculating-gradients",
    "href": "ml/autograd.html#calculating-gradients",
    "title": "Automatic differentiation",
    "section": "Calculating gradients",
    "text": "Calculating gradients\nAfter you have performed a number of operations on x and obtained a final object (let‚Äôs call it loss since in the context of neural networks, the output of the loss function is the starting place of the backpropagation process), you can get the gradient of any object y with:\nloss.backward()\nprint(y.grad)"
  },
  {
    "objectID": "ml/autograd.html#example",
    "href": "ml/autograd.html#example",
    "title": "Automatic differentiation",
    "section": "Example",
    "text": "Example\nLet‚Äôs go over a simple example:\n\nlet real be the tensor of some real values\nlet predicted be the tensor given by some model trying to predict these real values after an iteration\n\nWe will calculate the first derivative (first step of the backpropagation) manually and with the torch.autograd package to really understand what that package does.\nLet‚Äôs fill real and predicted with random values since we don‚Äôt have a real situation with a real network (but let‚Äôs make sure to start recording the history of computations performed on predicted):\nreal = torch.rand(3, 8)\nprint(real)\n\npredicted = torch.rand(3, 8, requires_grad=True)\nprint(predicted)\nSeveral loss functions can be used in machine learning, let‚Äôs use:\n\\[\\text{loss}=\\sum_{}^{} (\\text{predicted} - \\text{real})^2\\]\nloss = (predicted - real).pow(2).sum()\nNow, to train a model, after each forward-pass, we need to go through the backpropagation to adjust the weights and biases of the model. That means, we need to calculate all the derivatives, starting from the derivative of the predicted values up to the derivatives of the weights and biases.\nHere, we will only do the very first step: calculate the derivative of predicted.\n\nManual derivative calculation\nThe formula for this first derivative, with the loss function we used, is:\n\\[\\text{gradient}_\\text{predicted}=2(\\text{predicted} - \\text{real})\\]\nThere is no point in adding this operation to predicted‚Äôs computation history, so we will exclude it with with torch.no_grad():\nwith torch.no_grad():\n    manual_gradient_predicted = 2.0 * (predicted - real)\n\nprint(manual_gradient_predicted)\n\n\nAutomatic derivative calculation\nNow, with torch.autograd:\nloss.backward()\nSince we tracked computations on predicted, we can calculate its gradient with:\nauto_gradient_predicted = predicted.grad\nprint(auto_gradient_predicted)\n\n\nComparison\nThe result is the same, as can be tested with:\nprint(manual_gradient_predicted.eq(auto_gradient_predicted).all())\nThe calculation of this first derivative of backpropagation was simple enough. But to propagate all the derivatives calculations backward through the chain rule would quickly turn into a deep calculus problem.\nWith torch.autograd, calculating the gradients of all the other elements of the network is as simple as calling them with the attribute grad once the function torch.Tensor.backward() has been run."
  },
  {
    "objectID": "ml/checkpoints.html",
    "href": "ml/checkpoints.html",
    "title": "Saving/loading models and checkpointing",
    "section": "",
    "text": "You can save a model by serializing its internal state dictionary. The state dictionary is a Python dictionary that contains the parameters of your model.\ntorch.save(model.state_dict(), \"model.pth\")"
  },
  {
    "objectID": "ml/checkpoints.html#loading-models",
    "href": "ml/checkpoints.html#loading-models",
    "title": "Saving/loading models and checkpointing",
    "section": "Loading models",
    "text": "Loading models\nTo recreate your model, you first need to recreate its structure:\nmodel = Net()\nThen you can load the state dictionary containing the parameters values into it:\nmodel.load_state_dict(torch.load(\"model.pth\"))"
  },
  {
    "objectID": "ml/checkpoints.html#create-a-checkpoint",
    "href": "ml/checkpoints.html#create-a-checkpoint",
    "title": "Saving/loading models and checkpointing",
    "section": "Create a checkpoint",
    "text": "Create a checkpoint\ntorch.save({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'loss': loss,\n            ...\n            }, PATH)"
  },
  {
    "objectID": "ml/checkpoints.html#resume-training-from-a-checkpoint",
    "href": "ml/checkpoints.html#resume-training-from-a-checkpoint",
    "title": "Saving/loading models and checkpointing",
    "section": "Resume training from a checkpoint",
    "text": "Resume training from a checkpoint\nmodel = TheModelClass(*args, **kwargs)\noptimizer = TheOptimizerClass(*args, **kwargs)\n\ncheckpoint = torch.load(PATH)\nmodel.load_state_dict(checkpoint['model_state_dict'])\noptimizer.load_state_dict(checkpoint['optimizer_state_dict'])\nepoch = checkpoint['epoch']\nloss = checkpoint['loss']\n\nmodel.train()"
  },
  {
    "objectID": "ml/choosing_frameworks.html",
    "href": "ml/choosing_frameworks.html",
    "title": "Which framework to choose?",
    "section": "",
    "text": "With the growing popularity of machine learning, many frameworks have appeared in various languages. One of the questions you might be facing is: which tool should I choose?\nThe main focus here is on the downsides of proprietary tools."
  },
  {
    "objectID": "ml/choosing_frameworks.html#points-worth-considering",
    "href": "ml/choosing_frameworks.html#points-worth-considering",
    "title": "Which framework to choose?",
    "section": "Points worth considering",
    "text": "Points worth considering\nThere are a several points you might want to consider in making that choice. For instance, what tools do people use in your field? (what tools are used in the papers you read?) What tools are your colleagues and collaborators using?\nLooking a bit further into the future, whether you are considering staying in academia or working in industry could also influence your choice. If the former, paying attention to the literature is key, if the latter, it may be good to have a look at the trends in job postings.\nSome frameworks offer collections of already-made toolkits. They are thus easy to start using and do not require a lot of programming experience. On the other hand, they may feel like black boxes and they are not very customizable. Scikit-learn and Keras‚Äîusually run on top of TensorFlow‚Äîfall in that category. Lower level tools allow you full control and tuning of your models, but can come with a steeper learning curve.\nPyTorch, developed by Facebook‚Äôs AI Research lab, has seen a huge increase in popularity in research in recent years due to its highly pythonic syntax, very convenient tensors, just-in-time (JIT) compilation, dynamic computation graphs, and because it is free and open-source.\nSeveral libraries are now adding a higher level on top of PyTorch: fastai, which we will use in this course, PyTorch Lightning, and PyTorch Ignite. fastai, in addition to the convenience of being able to write a model in a few lines of code, allows to dive as low as you choose into the PyTorch code, thus making it unconstrained by the optional ease of use. It also adds countless functionality. The downside of using this added layer is that it can make it less straightforward to install on a machine or to tweak and customize.\nThe most popular machine learning library currently remains TensorFlow, developed by the Google Brain Team. While it has a Python API, its syntax can be more obscure.\nJulia‚Äôs syntax is well suited for the implementation of mathematical models, GPU kernels can be written directly in Julia, and Julia‚Äôs speed is attractive in computation hungry fields. So Julia has also seen the development of many ML packages such as Flux or Knet. The user base of Julia remains quite small however.\nMy main motivation in writing this section however is to raise awareness about one question that should really be considered: whether the tool you decide to learn and use in your research is open-source or proprietary."
  },
  {
    "objectID": "ml/choosing_frameworks.html#proprietary-tools-a-word-of-caution",
    "href": "ml/choosing_frameworks.html#proprietary-tools-a-word-of-caution",
    "title": "Which framework to choose?",
    "section": "Proprietary tools: a word of caution",
    "text": "Proprietary tools: a word of caution\nAs a student, it is tempting to have the following perspective:\n\nMy university pays for this very expensive license. I have free access to this very expensive tool. It would be foolish not to make use of it while I can!\n\nWhen there are no equivalent or better open-source tools, that might be true. But when superior open-source tools exist, these university licenses are more of a trap than a gift.\nHere are some of the reasons you should be wary of proprietary tools:\n\nResearchers who do not have access to the tool cannot reproduce your methods\nLarge Canadian universities may offer a license for the tool, but grad students in other countries, independent researchers, researchers in small organizations, etc. may not have access to a free license (or tens of thousands of dollars to pay for it).\n\n\nOnce you graduate, you may not have access to the tool anymore\nOnce you leave your Canadian institution, you may become one of those researchers who do not have access to that tool. This means that you will not be able to re-run your thesis analyses, re-use your methods, and apply the skills you learnt. The time you spent learning that expensive tool you could play with for free may feel a lot less like a gift then.\n\n\nYour university may stop paying for a license\nAs commercial tools fall behind successful open-source ones, some universities may decide to stop purchasing a license. It happened during my years at SFU with an expensive and clunky citation manager which, after having been promoted for years by the library through countless workshops, got abandoned in favour of a much better free and open-source one.\n\n\nYou may get locked-in\nProprietary tools often come with proprietary formats and, depending on the tool, it may be painful (or impossible) to convert your work to another format. When that happens, you are locked-in.\n\n\nProprietary tools are often black boxes\nIt is often impossible to see the source code of proprietary software.\n\n\nLong-term access\nIt is often very difficult to have access to old versions of proprietary tools (and this can be necessary to reproduce old studies). When companies disappear, the tools they produced usually disappear with them. open-source tools, particularly those who have their code under version control in repositories such as GitHub, remain fully accessible (including all stages of development), and if they get abandoned, their maintenance can be taken over or restarted by others.\n\n\nThe licenses you have access to may be limiting and a cause of headache\nFor instance, the Alliance does not have an unlimited number of MATLAB licenses. Since these licenses come with complex rules (one license needed for each node, additional licenses for each toolbox, additional licenses for newer tools, etc.), it can quickly become a nightmare to navigate through it all. You may want to have a look at some of the comments in this thread.\n\n\nProprietary tools fall behind popular open-source tools\nEven large teams of software engineers cannot compete against an active community of researchers developing open-source tools. When open-source tools become really popular, the number of users contributing to their development vastly outnumbers what any company can provide. The testing, licensing, and production of proprietary tools are also too slow to keep up with quickly evolving fields of research. (Of course, open-source tools which do not take off and remain absolutely obscure do not see the benefit of a vast community.)\n\n\nProprietary tools often fail to address specialized edge cases needed in research\nIt is not commercially sound to develop cutting edge capabilities so specialized in a narrow subfield that they can only target a minuscule number of customers. But this is often what research needs. With open-source tools, researchers can develop the capabilities that fit their very specific needs. So while commercial tools are good and reliable for large audiences, they are often not the best in research. This explains the success of R over tools such as SASS or Stata in the past decade."
  },
  {
    "objectID": "ml/choosing_frameworks.html#conclusion",
    "href": "ml/choosing_frameworks.html#conclusion",
    "title": "Which framework to choose?",
    "section": "Conclusion",
    "text": "Conclusion\nAll that said, sometimes you don‚Äôt have a choice over the tool to use for your research as this may be dictated by the culture in your field or by your supervisor. But if you are free to choose and if superior or equal open-source alternatives exist and are popular, do not fall in the trap of thinking that because your university and the Alliance pay for a license, you should make use of it. It may be free for you‚Äîfor now‚Äîbut it can have hidden costs."
  },
  {
    "objectID": "ml/concept.html",
    "href": "ml/concept.html",
    "title": "Overarching concept of deep learning",
    "section": "",
    "text": "Neural networks learn by adjusting their parameters automatically in an iterative manner. This is derived from Arthur Samuel‚Äôs concept.\nIt is important to get a good understanding of this process, so let‚Äôs go over it step by step."
  },
  {
    "objectID": "ml/concept.html#decide-on-an-architecture",
    "href": "ml/concept.html#decide-on-an-architecture",
    "title": "Overarching concept of deep learning",
    "section": "Decide on an architecture",
    "text": "Decide on an architecture\n\nThe architecture won‚Äôt change during training. This is set. The type of architecture you choose (e.g.¬†CNN, Transformer, etc.) depends on the type of data you have (e.g.¬†vision, textual, etc.). The depth and breadth of your network depend on the amount of data and computing resource you have."
  },
  {
    "objectID": "ml/concept.html#set-some-initial-parameters",
    "href": "ml/concept.html#set-some-initial-parameters",
    "title": "Overarching concept of deep learning",
    "section": "Set some initial parameters",
    "text": "Set some initial parameters\n\nYou can initialize them randomly or get much better ones through transfer learning.\nWhile the parameters are also part of the model, those will change during training."
  },
  {
    "objectID": "ml/concept.html#get-some-labelled-data",
    "href": "ml/concept.html#get-some-labelled-data",
    "title": "Overarching concept of deep learning",
    "section": "Get some labelled data",
    "text": "Get some labelled data\n\nWhen we say that we need a lot of data for machine learning, we mean ‚Äúlots of labelled data‚Äù as this is what gets used for training models."
  },
  {
    "objectID": "ml/concept.html#make-sure-to-keep-some-data-for-testing",
    "href": "ml/concept.html#make-sure-to-keep-some-data-for-testing",
    "title": "Overarching concept of deep learning",
    "section": "Make sure to keep some data for testing",
    "text": "Make sure to keep some data for testing\n\nThose data won‚Äôt be used for training the model. Often people keep around 20% of their data for testing."
  },
  {
    "objectID": "ml/concept.html#train-data-and-parameters-are-passed-through-the-architecture",
    "href": "ml/concept.html#train-data-and-parameters-are-passed-through-the-architecture",
    "title": "Overarching concept of deep learning",
    "section": "Train data and parameters are passed through the architecture",
    "text": "Train data and parameters are passed through the architecture\n\nThe train data are the inputs and the process of calculating the outputs is the forward pass."
  },
  {
    "objectID": "ml/concept.html#the-outputs-of-the-model-are-predictions",
    "href": "ml/concept.html#the-outputs-of-the-model-are-predictions",
    "title": "Overarching concept of deep learning",
    "section": "The outputs of the model are predictions",
    "text": "The outputs of the model are predictions"
  },
  {
    "objectID": "ml/concept.html#compare-those-predictions-to-the-train-labels",
    "href": "ml/concept.html#compare-those-predictions-to-the-train-labels",
    "title": "Overarching concept of deep learning",
    "section": "Compare those predictions to the train labels",
    "text": "Compare those predictions to the train labels\n\nSince our data was labelled, we know what the true outputs are."
  },
  {
    "objectID": "ml/concept.html#calculate-train-loss",
    "href": "ml/concept.html#calculate-train-loss",
    "title": "Overarching concept of deep learning",
    "section": "Calculate train loss",
    "text": "Calculate train loss\n\nThe deviation of our predictions from the true outputs gives us a measure of training loss."
  },
  {
    "objectID": "ml/concept.html#parameters-adjustement",
    "href": "ml/concept.html#parameters-adjustement",
    "title": "Overarching concept of deep learning",
    "section": "Parameters adjustement",
    "text": "Parameters adjustement\n\nThe parameters get automatically adjusted to reduce the training loss through the mechanism of backpropagation.\nThis is the actual training part.\nThis process is repeated many times. Training models is pretty much a giant for loop."
  },
  {
    "objectID": "ml/concept.html#from-model-to-program",
    "href": "ml/concept.html#from-model-to-program",
    "title": "Overarching concept of deep learning",
    "section": "From model to program",
    "text": "From model to program\n\nRemember that the model architecture is fixed, but that the parameters change at each iteration of the training process.\nWhile the labelled data are key to training, what we are really interested in is the combination of architecture + final parameters.\n\nWhen the training is over, the parameters become fixed. Which means that our model now behaves like a classic program."
  },
  {
    "objectID": "ml/concept.html#evaluating-the-model",
    "href": "ml/concept.html#evaluating-the-model",
    "title": "Overarching concept of deep learning",
    "section": "Evaluating the model",
    "text": "Evaluating the model\n\nWe can now use the testing set (which was never used to train the model) to evaluate our model: if we pass the test inputs through our program, we get some predictions that we can compare to the test labels (which are the true outputs).\nThis gives us the test loss: a measure of how well our model performs."
  },
  {
    "objectID": "ml/concept.html#use-the-model",
    "href": "ml/concept.html#use-the-model",
    "title": "Overarching concept of deep learning",
    "section": "Use the model",
    "text": "Use the model\n\nNow that we have a program, we can use it on unlabelled inputs to get what people ultimately want: unknown outputs. This is when we put our model to actual use to solve some problem."
  },
  {
    "objectID": "ml/fastai.html",
    "href": "ml/fastai.html",
    "title": "fastai",
    "section": "",
    "text": "fastai is a deep learning library which provides practitioners with high-level components that can quickly and easily provide state-of-the-art results in standard deep learning domains. This webinar will take a closer look at the features and functionality of fastai."
  },
  {
    "objectID": "ml/flux.html",
    "href": "ml/flux.html",
    "title": "Machine learning in Julia with Flux",
    "section": "",
    "text": "Find this webinar in the Julia section."
  },
  {
    "objectID": "ml/high_level_frameworks.html",
    "href": "ml/high_level_frameworks.html",
    "title": "High-level frameworks for PyTorch",
    "section": "",
    "text": "Several popular higher-level frameworks are built on top of PyTorch and make the code easier to write and run:\nThe following tag trends on Stack Overflow might give an idea of the popularity of these frameworks over time (catalyst doesn‚Äôt have any Stack Overflow tag):\nIf this data is to be believed, ignite never really took off (it also has a lower number of stars on GitHub), fast-ai was extremely popular when it came out, but its usage is going down, and PyTorch-lightning is currently the most popular."
  },
  {
    "objectID": "ml/high_level_frameworks.html#should-you-use-one-and-which-one",
    "href": "ml/high_level_frameworks.html#should-you-use-one-and-which-one",
    "title": "High-level frameworks for PyTorch",
    "section": "Should you use one (and which one)?",
    "text": "Should you use one (and which one)?\nLearning raw PyTorch is probably the best option for research. PyTorch is stable and here to stay. Higher-level frameworks may rise and drop in popularity and today‚Äôs popular one may see little usage tomorrow.\nRaw PyTorch is also the most flexible, the closest to the actual computations happening in your model, and probably the easiest to debug.\nDepending on your deep learning trajectory, you might find some of these tools useful though:\n\nIf you work in industry, you might want or need to get results quickly\nSome operations (e.g.¬†parallel execution on multiple GPUs) can be tricky in raw PyTorch, while being extremely streamlined when using e.g.¬†PyTorch-lightning\nEven in research, it might make sense to spend more time thinking about the structure of your model and the functioning of a network instead of getting bogged down in writing code\n\n\nBefore moving to any of these tools, it is probably a good idea to get a good knowledge of raw PyTorch: use these tools to simplify your workflow, not cloud your understanding of what your code is doing."
  },
  {
    "objectID": "ml/hpc.html",
    "href": "ml/hpc.html",
    "title": "Machine learning on production clusters",
    "section": "",
    "text": "This lesson is a summary of relevant information while using Python in an HPC context for deep learning.\nWhen you ssh into one of the Alliance clusters, you log into the login node.\nEverybody using a cluster uses that node to enter the cluster. Do not run anything computationally intensive on this node or you would make the entire cluster very slow for everyone. To run your code, you need to start an interactive job or submit a batch job to Slurm (the job scheduler used by the Alliance clusters)."
  },
  {
    "objectID": "ml/hpc.html#plots",
    "href": "ml/hpc.html#plots",
    "title": "Machine learning on production clusters",
    "section": "Plots",
    "text": "Plots\nDo not run code that displays plots on screen. Instead, have them written to files."
  },
  {
    "objectID": "ml/hpc.html#data",
    "href": "ml/hpc.html#data",
    "title": "Machine learning on production clusters",
    "section": "Data",
    "text": "Data\n\nCopy files to/from the cluster\n\nFew files\nIf you need to copy files to or from the cluster, you can use scp from your local machine.\n\nCopy file from your computer to the cluster\n[local]$ scp </local/path/to/file> <user>@<hostname>:<path/in/cluster>\n\nExpressions between the < and > signs need to be replaced by the relevant information (without those signs).\n\n\n\nCopy file from the cluster to your computer\n[local]$ scp <user>@<hostname>:<cluster/path/to/file> </local/path>\n\n\n\nLarge amount of data\nUse Globus for large data transfers.\n\nThe Alliance is starting to store classic ML datasets on its clusters. So if your research uses a common dataset, it may be worth inquiring whether it might be available before downloading a copy.\n\n\n\n\nLarge collections of files\nThe Alliance clusters are optimized for very large files and are slowed by large collections of small files. Datasets with many small files need to be turned into single-file archives with tar. Failing to do so will affect performance not just for you, but for all users of the cluster.\n$ tar cf <data>.tar <path/to/dataset/directory>/*\n\n\nIf you want to also compress the files, replace tar cf with tar czf\nAs a modern alternative to tar, you can use Dar"
  },
  {
    "objectID": "ml/hpc.html#interactive-jobs",
    "href": "ml/hpc.html#interactive-jobs",
    "title": "Machine learning on production clusters",
    "section": "Interactive jobs",
    "text": "Interactive jobs\nInteractive jobs are useful for code testing and development. They are not however the most efficient way to run code, so you should limit their use to testing and development.\nYou start an interactive job with:\n$ salloc --account=def-<account> --cpus-per-task=<n> --gres=gpu:<n> --mem=<mem> --time=<time>\nOur training cluster does not have GPUs, so for this workshop, do not use the --gres=gpu:<n> option.\nFor the workshop, you also don‚Äôt have to worry about the --account=def-<account> option (or, if you want, you can use --account=def-sponsor00).\nOur training cluster has a total of 60 CPUs on 5 compute nodes. Since there are many of you in this workshop, please be very mindful when running interactive jobs: if you request a lot of CPUs for a long time, the other workshop attendees won‚Äôt be able to use the cluster anymore until your interactive job requested time ends (even if you aren‚Äôt running any code).\nHere are my suggestions so that we don‚Äôt run into this problem:\n\nOnly start interactive jobs when you need to understand what Python is doing at every step, or to test, explore, and develop code (so where an interactive Python shell is really beneficial). Once you have a model, submit a batch job to Slurm instead\nWhen running interactive jobs on this training cluster, only request 1 CPU (so --cpus-per-task=1)\nOnly request the time that you will really use (e.g.¬†for the lesson on Python tensors, maybe 30 min to 1 hour seems reasonable)\nIf you don‚Äôt need your job allocation anymore before it runs out, you can relinquish it with Ctrl+d\n\n\nBe aware that, on Cedar, you are not allowed to submit jobs from ~/home. Instead, you have to submit jobs from ~/scratch or ~/project."
  },
  {
    "objectID": "ml/hpc.html#batch-jobs",
    "href": "ml/hpc.html#batch-jobs",
    "title": "Machine learning on production clusters",
    "section": "Batch jobs",
    "text": "Batch jobs\nAs soon as you have a working Python script, you want to submit a batch job instead of running an interactive job. To do that, you need to write an sbatch script.\n\nJob script\n\nHere is an example script:\n\n#!/bin/bash\n#SBATCH --job-name=<name>*            # job name\n#SBATCH --account=def-<account>\n#SBATCH --time=<time>                 # max walltime in D-HH:MM or HH:MM:SS\n#SBATCH --cpus-per-task=<number>      # number of cores\n#SBATCH --gres=gpu:<type>:<number>    # type and number of GPU(s) per node\n#SBATCH --mem=<mem>                   # max memory (default unit is MB) per node\n#SBATCH --output=%x_%j.out*           # file name for the output\n#SBATCH --error=%x_%j.err*            # file name for errors\n#SBATCH --mail-user=<email_address>*\n#SBATCH --mail-type=ALL*\n\n# Load modules\n# (Do not use this in our workshop since we aren't using GPUs)\n# (Note: loading the Python module is not necessary\n# when you activate a Python virtual environment)\n# module load cudacore/.10.1.243 cuda/10 cudnn/7.6.5\n\n# Create a variable with the directory for your ML project\nSOURCEDIR=~/<path/project/dir>\n\n# Activate your Python virtual environment\nsource ~/env/bin/activate\n\n# Transfer and extract data to a compute node\nmkdir $SLURM_TMPDIR/data\ntar xf ~/projects/def-<user>/<data>.tar -C $SLURM_TMPDIR/data\n\n# Run your Python script on the data\npython $SOURCEDIR/<script>.py $SLURM_TMPDIR/data\n\n\n%x will get replaced by the script name and %j by the job number\nIf you compressed your data with tar czf, you need to extract it with tar xzf\nSBATCH options marked with a * are optional\nThere are various other options for email notifications\n\n\nYou may wonder why we transferred data to a compute node. This makes any I/O operation involving your data a lot faster, so it will speed up your code. Here is how this works:\nFirst, we create a temporary data directory in $SLURM_TMPDIR:\n$ mkdir $SLURM_TMPDIR/data\n\nThe variable $SLURM_TMPDIR is created by Slurm on the compute node where a job is running. Its path is /localscratch/<user>.<jobid>.0. Anything in it gets deleted when the job is done.\n\nThen we extract the data into it:\n$ tar xf ~/projects/def-<user>/<data>.tar -C $SLURM_TMPDIR/data\nIf your data is not in a tar file, you can simply copy it to the compute node running your job:\n$ cp -r ~/projects/def-<user>/<data> $SLURM_TMPDIR/data\n\n\nJob handling\n\nSubmit a job\n$ cd </dir/containing/job>\n$ sbatch <jobscript>.sh\n\n\nCheck the status of your job(s)\n$ sq\n\nPD = pending\nR = running\nCG = completing (Slurm is doing the closing processes)\nNo information = your job has finished running\n\n\n\nCancel a job\n$ scancel <jobid>\n\n\nDisplay efficiency measures of a completed job\n$ seff <jobid>"
  },
  {
    "objectID": "ml/hpc.html#gpus",
    "href": "ml/hpc.html#gpus",
    "title": "Machine learning on production clusters",
    "section": "GPU(s)",
    "text": "GPU(s)\n\nGPU types\nSeveral Alliance clusters have GPUs. Their numbers and types differ:\n From the Alliance Wiki\nThe default is 12G P100, but you can request another type with SBATCH --gres=gpu:<type>:<number> (example: --gres=gpu:p100l:1 to request a 16G P100 on Cedar). Please refer to the Alliance Wiki for more details.\n\n\nNumber of GPU(s)\nTry running your model on a single GPU first.\nIt is very likely that you do not need more than one GPU. Asking for more than you need will greatly increase your waiting time until your job is run. The lesson on distributed computing with PyTorch gives a few information as to when you might benefit from using several GPUs and provides some links to more resources. We will also offer workshops on distributed ML in the future. In any event, you should test your model before asking for several GPUs.\n\n\nCPU/GPU ratio\nHere are the Alliance recommendations:\nB√©luga:\nNo more than 10 CPU per GPU.\nCedar:\nP100 GPU: no more than 6 CPU per GPU.\nV100 GPU: no more than 8 CPU per GPU.\nGraham:\nNo more than 16 CPU per GPU."
  },
  {
    "objectID": "ml/hpc.html#code-testing",
    "href": "ml/hpc.html#code-testing",
    "title": "Machine learning on production clusters",
    "section": "Code testing",
    "text": "Code testing\nIt might be wise to test your code in an interactive job before submitting a really big batch job to Slurm.\n\nActivate your Python virtual environment\n$ source ~/env/bin/activate\n\n\nStart an interactive job\n\nExample:\n\n$ salloc --account=def-<account> --gres=gpu:1 --cpus-per-task=6 --mem=32000 --time=0:30:0\n\n\nPrepare the data\nCreate a temporary data directory in $SLURM_TMPDIR:\n(env) $ mkdir $SLURM_TMPDIR/data\n\nThe variable $SLURM_TMPDIR is created by Slurm on the compute node where a job is running. Its path is /localscratch/<user>.<jobid>.0. Anything in it gets deleted when the job is done.\n\nExtract the data into it:\n(env) $ tar xf ~/projects/def-<user>/<data>.tar -C $SLURM_TMPDIR/data\n\n\nTry to run your code\nPlay in Python to test your code:\n(env) $ python\n>>> import torch\n>>> ...\nTo exit the virtual environment, run:\n(env) $ deactivate"
  },
  {
    "objectID": "ml/hpc.html#checkpoints",
    "href": "ml/hpc.html#checkpoints",
    "title": "Machine learning on production clusters",
    "section": "Checkpoints",
    "text": "Checkpoints\nLong jobs should have a checkpoint at least every 24 hours. This ensures that an outage won‚Äôt lead to days of computation lost and it will help get the job started by the scheduler sooner.\nFor instance, you might want to have checkpoints every n epochs (choose n so that n epochs take less than 24 hours to run).\nIn PyTorch, you can create dictionaries with all the information necessary and save them as .tar files with torch.save(). You can then load them back with torch.load().\nThe information you want to save in each checkpoint includes the model‚Äôs state_dict, the optimizer‚Äôs state_dict, the epoch at which you stopped, the latest training loss, and anything else needed to restart training where you left off.\n\nFor example, saving a checkpoint during training could look something like this:\n\ntorch.save({\n    'epoch': <last epoch run>,\n    'model_state_dict': net.state_dict(),\n    'optimizer_state_dict': optimizer.state_dict(),\n    'loss': <latest loss>,\n}, <path/to/checkpoint-file.tar>)\n\nTo restart, initialize the model and optimizer, load the dictionary, and resume training:\n\n# Initialize the model and optimizer\nmodel = <your model>\noptimizer = <your optimizer>\n\n# Load the dictionary\ncheckpoint = torch.load(<path/to/checkpoint-file.tar>)\nmodel.load_state_dict(checkpoint['model_state_dict'])\noptimizer.load_state_dict(checkpoint['optimizer_state_dict'])\nepoch = checkpoint['epoch']\nloss = checkpoint['loss']\n\n# Resume training\nmodel.train()"
  },
  {
    "objectID": "ml/hpc.html#tensorboard-on-the-cluster",
    "href": "ml/hpc.html#tensorboard-on-the-cluster",
    "title": "Machine learning on production clusters",
    "section": "TensorBoard on the cluster",
    "text": "TensorBoard on the cluster\nTensorBoard allows to visually track your model metrics (e.g.¬†loss, accuracy, model graph, etc.). It requires a lot of processing power however, so if you want to use it on an Alliance cluster, do not run it from the login node. Instead, run it as part of your job. This section guides you through the whole workflow.\n\nLaunch TensorBoard\nFirst, you need to launch TensorBoard in the background (with a trailing &) before running your Python script. To do so, ad to your sbatch script:\ntensorboard --logdir=/tmp/<your log dir> --host 0.0.0.0 &\n\nExample:\n\n#!/bin/bash\n#SBATCH ...\n...\n\ntensorboard --logdir=/tmp/<your log dir> --host 0.0.0.0 &\npython $SOURCEDIR/<script>.py $SLURM_TMPDIR/data\n\n\nCreate a connection between the compute node and your computer\nOnce the job is running, you need to create a connection between the compute node running TensorBoard and your computer.\nFirst, you need to find the hostname of the compute node running the Tensorboard server. This is the value under NODELIST for your job when you run:\n$ sq\nThen, from your computer, enter this ssh command:\n[local]$ ssh -N -f -L localhost:6006:<node hostname>:6006 <user>@<cluster>.computecanada.ca\n\nReplace <node hostname> by the compute node hostname you just identified, <user> by your user name, and <cluster> by the name of the Alliance cluster hostname‚Äîe.g.¬†beluga, cedar, graham.\n\n\n\nAccess TensorBoard\nYou can now open a browser (on your computer) and go to http://localhost:6006 to monitor your model running on a compute node in the cluster!"
  },
  {
    "objectID": "ml/hpc.html#running-several-similar-jobs",
    "href": "ml/hpc.html#running-several-similar-jobs",
    "title": "Machine learning on production clusters",
    "section": "Running several similar jobs",
    "text": "Running several similar jobs\nA number of ML tasks (e.g.¬†hyperparameter optimization) require running several instances of similar jobs. Grouping them into a single job with GLOST or GNU Parallel reduces the stress on the scheduler."
  },
  {
    "objectID": "ml/intro_hss.html",
    "href": "ml/intro_hss.html",
    "title": "Introduction to machine learning for the humanities",
    "section": "",
    "text": "We hear about it all the time, but what really is machine learning? And what about deep learning? Neural networks?? How can any of this help me with my work? And how? Which tools do I need to make use of the transformative advances happening in that field??\nThis workshop will answer these questions in a non-technical manner to give you a high level overview of a discipline that has become crucial in all fields of research."
  },
  {
    "objectID": "ml/mnist.html",
    "href": "ml/mnist.html",
    "title": "Classifying the MNIST dataset",
    "section": "",
    "text": "In this workshop, we will classify the MNIST dataset‚Äîa classic of machine learning‚Äîwith PyTorch."
  },
  {
    "objectID": "ml/mnist.html#the-mnist-dataset",
    "href": "ml/mnist.html#the-mnist-dataset",
    "title": "Classifying the MNIST dataset",
    "section": "The MNIST dataset",
    "text": "The MNIST dataset\nThe MNIST is a classic dataset commonly used for testing machine learning systems. It consists of pairs of images of handwritten digits and their corresponding labels.\nThe images are composed of 28x28 pixels of greyscale RGB codes ranging from 0 to 255 and the labels are the digits from 0 to 9 that each image represents.\nThere are 60,000 training pairs and 10,000 testing pairs.\nThe goal is to build a neural network which can learn from the training set to properly identify the handwritten digits and which will perform well when presented with the testing set that it has never seen. This is a typical case of supervised learning.\n\nNow, let‚Äôs explore the MNIST with PyTorch."
  },
  {
    "objectID": "ml/mnist.html#download-unzip-and-transform-the-data",
    "href": "ml/mnist.html#download-unzip-and-transform-the-data",
    "title": "Classifying the MNIST dataset",
    "section": "Download, unzip, and transform the data",
    "text": "Download, unzip, and transform the data\n\nWhere to store the data in the cluster\nWe will all use the same data. It would make little sense to all download it in our home directory.\nOn the Alliance clusters, a good place to store data shared amongst members of a project is in the /project file system.\nYou usually belong to /project/def-<group>, where <group> is the name of your PI. You can access it from your home directory through the symbolic link ~/projects/def-<group>.\nIn our training cluster, we are all part of the group def-sponsor00, accessible through /project/def-sponsor00 (or the hyperlink ~/projects/def-sponsor00).\nWe will thus all access the MNIST data in ~/projects/def-sponsor00/data.\n\n\nHow to obtain the data?\nThe dataset can be downloaded directly from the MNIST website, but the PyTorch package TorchVision has tools to download and transform several classic vision datasets, including the MNIST.\nhelp(torchvision.datasets.MNIST)\nHelp on class MNIST in module torchvision.datasets.mnist:\n\nclass MNIST(torchvision.datasets.vision.VisionDataset)\n\n |  MNIST(root: str, train: bool = True, \n |    transform: Optional[Callable] = None,\n |    target_transform: Optional[Callable] = None, \n |    download: bool = False) -> None\n |   \n |  Args:\n |    root (string): Root directory of dataset where \n |      MNIST/raw/train-images-idx3-ubyte and \n |      MNIST/raw/t10k-images-idx3-ubyte exists.\n |    train (bool, optional): If True, creates dataset from \n |      train-images-idx3-ubyte, otherwise from t10k-images-idx3-ubyte.\n |    download (bool, optional): If True, downloads the dataset from the \n |      internet and puts it in root directory. If dataset is already \n |      downloaded, it is not downloaded again.\n |    transform (callable, optional): A function/transform that takes in \n |      an PIL image and returns a transformed version.\n |      E.g, transforms.RandomCrop\n |    target_transform (callable, optional): A function/transform that \n |      takes in the target and transforms it.\nNote that here too, the root argument sets the location of the downloaded data and we will use /project/def-sponsor00/data/.\n\n\nPrepare the data\nFirst, let‚Äôs load the needed libraries:\n\nimport torch\nfrom torchvision import datasets, transforms\nfrom matplotlib import pyplot as plt\n\nThe MNIST dataset already consists of a training and a testing sets, so we don‚Äôt have to split the data manually. Instead, we can directly create 2 different objects with the same function (train=True selects the train set and train=False selects the test set).\nWe will transform the raw data to tensors and normalize them using the mean and standard deviation of the MNIST training set: 0.1307 and 0.3081 respectively (even though the mean and standard deviation of the test data are slightly different, it is important to normalize the test data with the values of the training data to apply the same treatment to both sets).\nSo we first need to define a transformation:\n\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.1307,), (0.3081,))\n])\n\n\n\nWe can now create our data objects\n\nTraining data\n\nRemember that train=True selects the training set of the MNIST.\n\n\ntrain_data = datasets.MNIST(\n    '~/projects/def-sponsor00/data',\n    train=True, download=True, transform=transform)\n\n\n\nTest data\n\ntrain=False selects the test set.\n\n\ntest_data = datasets.MNIST(\n    '~/projects/def-sponsor00/data',\n    train=False, transform=transform)"
  },
  {
    "objectID": "ml/mnist.html#exploring-the-data",
    "href": "ml/mnist.html#exploring-the-data",
    "title": "Classifying the MNIST dataset",
    "section": "Exploring the data",
    "text": "Exploring the data\n\nData inspection\nFirst, let‚Äôs check the size of train_data:\n\nprint(len(train_data))\n\n60000\n\n\nThat makes sense since the MNIST‚Äôs training set has 60,000 pairs. train_data has 60,000 elements and we should expect each element to be of size 2 since it is a pair. Let‚Äôs double-check with the first element:\n\nprint(len(train_data[0]))\n\n2\n\n\nSo far, so good. We can print that first pair:\n\nprint(train_data[0])\n\n(tensor([[[-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.3860, -0.1951,\n          -0.1951, -0.1951,  1.1795,  1.3068,  1.8032, -0.0933,  1.6887,\n           2.8215,  2.7197,  1.1923, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.0424,  0.0340,  0.7722,  1.5359,  1.7396,  2.7960,\n           2.7960,  2.7960,  2.7960,  2.7960,  2.4396,  1.7650,  2.7960,\n           2.6560,  2.0578,  0.3904, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n           0.1995,  2.6051,  2.7960,  2.7960,  2.7960,  2.7960,  2.7960,\n           2.7960,  2.7960,  2.7960,  2.7706,  0.7595,  0.6195,  0.6195,\n           0.2886,  0.0722, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.1951,  2.3633,  2.7960,  2.7960,  2.7960,  2.7960,  2.7960,\n           2.0960,  1.8923,  2.7197,  2.6433, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242,  0.5940,  1.5614,  0.9377,  2.7960,  2.7960,  2.1851,\n          -0.2842, -0.4242,  0.1231,  1.5359, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.2460, -0.4115,  1.5359,  2.7960,  0.7213,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242,  1.3450,  2.7960,  1.9942,\n          -0.3988, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.2842,  1.9942,  2.7960,\n           0.4668, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.0213,  2.6433,\n           2.4396,  1.6123,  0.9504, -0.4115, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.6068,\n           2.6306,  2.7960,  2.7960,  1.0904, -0.1060, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n           0.1486,  1.9432,  2.7960,  2.7960,  1.4850, -0.0806, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.2206,  0.7595,  2.7833,  2.7960,  1.9560, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242,  2.7451,  2.7960,  2.7451,  0.3904,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n           0.1613,  1.2305,  1.9051,  2.7960,  2.7960,  2.2105, -0.3988,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.0722,  1.4596,\n           2.4906,  2.7960,  2.7960,  2.7960,  2.7578,  1.8923, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.1187,  1.0268,  2.3887,  2.7960,\n           2.7960,  2.7960,  2.7960,  2.1342,  0.5686, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.1315,  0.4159,  2.2869,  2.7960,  2.7960,  2.7960,\n           2.7960,  2.0960,  0.6068, -0.3988, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.1951,\n           1.7523,  2.3633,  2.7960,  2.7960,  2.7960,  2.7960,  2.0578,\n           0.5940, -0.3097, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242,  0.2758,  1.7650,  2.4524,\n           2.7960,  2.7960,  2.7960,  2.7960,  2.6815,  1.2686, -0.2842,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242,  1.3068,  2.7960,  2.7960,\n           2.7960,  2.2742,  1.2941,  1.2559, -0.2206, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242]]]), 5)\n\n\nAnd you can see that it is a tuple with:\n\nprint(type(train_data[0]))\n\n<class 'tuple'>\n\n\nWhat is that tuple made of?\n\nprint(type(train_data[0][0]))\nprint(type(train_data[0][1]))\n\n<class 'torch.Tensor'>\n<class 'int'>\n\n\nIt is made of the tensor for the first image (remember that we transformed the images into tensors when we created the objects train_data and test_data) and the integer of the first label (which you can see is 5 when you print train_data[0][1]).\nSo since train_data[0][0] is the tensor representing the image of the first pair, let‚Äôs check its size:\n\nprint(train_data[0][0].size())\n\ntorch.Size([1, 28, 28])\n\n\nThat makes sense: a color image would have 3 layers of RGB values (so the size in the first dimension would be 3), but because the MNIST has black and white images, there is a single layer of values‚Äîthe values of each pixel on a gray scale‚Äîso the first dimension has a size of 1. The 2nd and 3rd dimensions correspond to the width and length of the image in pixels, hence 28 and 28.\n\n\nYour turn:\n\nRun the following:\nprint(train_data[0][0][0])\nprint(train_data[0][0][0][0])\nprint(train_data[0][0][0][0][0])\nAnd think about what each of them represents.\nThen explore the test_data object.\n\n\n\nPlotting an image from the data\nFor this, we will use pyplot from matplotlib.\nFirst, we select the image of the first pair and we resize it from 3 to 2 dimensions by removing its dimension of size 1 with torch.squeeze:\nimg = torch.squeeze(train_data[0][0])\nThen, we plot it with pyplot, but since we are in a cluster, instead of showing it to screen with plt.show(), we save it to file:\nplt.imshow(img, cmap='gray')\nThis is what that first image looks like:\n\nAnd indeed, it matches the first label we explored earlier (train_data[0][1]).\n\n\nPlotting an image with its pixel values\nWe can plot it with more details by showing the value of each pixel in the image. One little twist is that we need to pick a threshold value below which we print the pixel values in white otherwise they would not be visible (black on near black background). We also round the pixel values to one decimal digit so as not to clutter the result.\nimgplot = plt.figure(figsize = (12, 12))\nsub = imgplot.add_subplot(111)\nsub.imshow(img, cmap='gray')\nwidth, height = img.shape\nthresh = img.max() / 2.5\nfor x in range(width):\n    for y in range(height):\n        val = round(img[x][y].item(), 1)\n        sub.annotate(str(val), xy=(y, x),\n                     horizontalalignment='center',\n                     verticalalignment='center',\n                     color='white' if img[x][y].item() < thresh else 'black')"
  },
  {
    "objectID": "ml/mnist.html#batch-processing",
    "href": "ml/mnist.html#batch-processing",
    "title": "Classifying the MNIST dataset",
    "section": "Batch processing",
    "text": "Batch processing\nPyTorch provides the torch.utils.data.DataLoader class which combines a dataset and an optional sampler and provides an iterable (while training or testing our neural network, we will iterate over that object). It allows, among many other things, to set the batch size and shuffle the data.\nSo our last step in preparing the data is to pass it through DataLoader.\n\nCreate DataLoaders\n\nTraining data\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=20, shuffle=True)\n\n\nTest data\ntest_loader = torch.utils.data.DataLoader(\n    test_data, batch_size=20, shuffle=False)\n\n\n\nPlot a full batch of images with their labels\nNow that we have passed our data through DataLoader, it is easy to select one batch from it. Let‚Äôs plot an entire batch of images with their labels.\nFirst, we need to get one batch of training images and their labels:\ndataiter = iter(train_loader)\nbatchimg, batchlabel = dataiter.next()\nThen, we can plot them:\nbatchplot = plt.figure(figsize=(20, 5))\nfor i in torch.arange(20):\n    sub = batchplot.add_subplot(2, 10, i+1, xticks=[], yticks=[])\n    sub.imshow(torch.squeeze(batchimg[i]), cmap='gray')\n    sub.set_title(str(batchlabel[i].item()), fontsize=25)"
  },
  {
    "objectID": "ml/mnist.html#time-to-build-a-nn-to-classify-the-mnist",
    "href": "ml/mnist.html#time-to-build-a-nn-to-classify-the-mnist",
    "title": "Classifying the MNIST dataset",
    "section": "Time to build a NN to classify the MNIST",
    "text": "Time to build a NN to classify the MNIST\nLet‚Äôs build a multi-layer perceptron (MLP): the simplest neural network. It is a feed-forward (i.e.¬†no loop), fully-connected (i.e.¬†each neuron of one layer is connected to all the neurons of the adjacent layers) neural network with a single hidden layer.\n\n\nLoad packages\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torchvision import datasets, transforms\nfrom torch.optim.lr_scheduler import StepLR\nThe torch.nn.functional module contains all the functions of the torch.nn package.\nThese functions include loss functions, activation functions, pooling functions‚Ä¶\n\n\nCreate a SummaryWriter instance for TensorBoard\nwriter = SummaryWriter()\n\n\nDefine the architecture of the network\n# To build a model, create a subclass of torch.nn.Module:\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.fc1 = nn.Linear(784, 128)\n        self.fc2 = nn.Linear(128, 10)\n\n    # Method for the forward pass:\n    def forward(self, x):\n        x = torch.flatten(x, 1)\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.fc2(x)\n        output = F.log_softmax(x, dim=1)\n        return output\nPython‚Äôs class inheritance gives our subclass all the functionality of torch.nn.Module while allowing us to customize it.\n\n\nDefine a training function\ndef train(model, device, train_loader, optimizer, epoch):\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n        optimizer.zero_grad()  # reset the gradients to 0\n        output = model(data)\n        loss = F.nll_loss(output, target)  # negative log likelihood\n        writer.add_scalar(\"Loss/train\", loss, epoch)\n        loss.backward()\n        optimizer.step()\n        if batch_idx % 10 == 0:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(data), len(train_loader.dataset),\n                100. * batch_idx / len(train_loader), loss.item()))\n\n\nDefine a testing function\ndef test(model, device, test_loader):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            # Sum up batch loss:\n            test_loss += F.nll_loss(output, target, reduction='sum').item()\n            # Get the index of the max log-probability:\n            pred = output.argmax(dim=1, keepdim=True)\n            correct += pred.eq(target.view_as(pred)).sum().item()\n\n    test_loss /= len(test_loader.dataset)\n\n    # Print a summary\n    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n        test_loss, correct, len(test_loader.dataset),\n        100. * correct / len(test_loader.dataset)))\n\n\nDefine a function main() which runs our network\ndef main():\n    epochs = 1\n    torch.manual_seed(1)\n    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n\n    transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,))\n    ])\n\n    train_data = datasets.MNIST(\n        '~/projects/def-sponsor00/data',\n        train=True, download=True, transform=transform)\n\n    test_data = datasets.MNIST(\n        '~/projects/def-sponsor00/data',\n        train=False, transform=transform)\n\n    train_loader = torch.utils.data.DataLoader(train_data, batch_size=64)\n    test_loader = torch.utils.data.DataLoader(test_data, batch_size=1000)\n    model = Net().to(device)  # create instance of our network and send it to device\n    optimizer = optim.Adadelta(model.parameters(), lr=1.0)\n    scheduler = StepLR(optimizer, step_size=1, gamma=0.7)\n\n    for epoch in range(1, epochs + 1):\n        train(model, device, train_loader, optimizer, epoch)\n        test(model, device, test_loader)\n        scheduler.step()\n\n\nRun the network\nmain()\n\n\nWrite pending events to disk and close the TensorBoard\nwriter.flush()\nwriter.close()\nThe code is working. Time to actually train our model!\nJupyter is a fantastic tool. It has a major downside however: when you launch a Jupyter server, you are running a job on a compute node. If you want to play for 8 hours in Jupyter, you are requesting an 8 hour job. Now, most of the time you spend on Jupyter is spent typing, running bits and pieces of code, or doing nothing at all. If you ask for GPUs, many CPUs, and lots of RAM, all of it will remain idle almost all of the time. It is a really suboptimal use of the Alliance resources.\nIn addition, if you ask for lots of resources for a long time, you will have to wait a long time in the queue before they get allocated to you.\nLastly, you will go through your allocation quickly.\nA much better strategy is to develop and test your code (with very little data, few epochs, etc.) in an interactive job (with salloc) or in Jupyter, then, launch an sbatch job to actually train your model. This ensures that heavy duty resources such as GPU(s) are only allocated to you when you are actually needing and using them.\n\nConcrete example with our training cluster: this cluster only has 1 GPU. If you want to use it in Jupyter, you have to request it for your Jupyter session. This means that the entire time your Jupyter session is active, nobody else can use that GPU. While you let your session idle or do tasks that do not require a GPU, this is not a good use of resources."
  },
  {
    "objectID": "ml/mnist.html#lets-train-and-test-our-model",
    "href": "ml/mnist.html#lets-train-and-test-our-model",
    "title": "Classifying the MNIST dataset",
    "section": "Let‚Äôs train and test our model",
    "text": "Let‚Äôs train and test our model\n\nLog in the training cluster\nOpen a terminal and SSH to our training cluster as we saw in the first lesson.\n\n\nLoad necessary modules\nFirst, we need to load the Python and CUDA modules. This is done with the Lmod tool through the module command. Here are some key Lmod commands:\n# Get help on the module command\n$ module help\n\n# List modules that are already loaded\n$ module list\n\n# See which modules are available for a tool\n$ module avail <tool>\n\n# Load a module\n$ module load <module>[/<version>]\nHere are the modules we need:\n$ module load nixpkgs/16.09 gcc/7.3.0 cuda/10.0.130 cudnn/7.6 python/3.8.2\n\n\nInstall Python packages\nYou also need the Python packages matplotlib, torch, torchvision, and tensorboard.\nOn the Alliance clusters, you need to create a virtual environment in which you install packages with pip.\n\nDo not use Anaconda.\nWhile Anaconda is a great tool on personal computers, it is not an appropriate tool when working on the Alliance clusters: binaries are unoptimized for those clusters and library paths are inconsistent with their architecture. Anaconda installs packages in $HOME where it creates a very large number of small files. It can also create conflicts by modifying .bashrc.\n\nFor this workshop, since we all need the same packages, I already created a virtual environment that we will all use. All you have to do is to activate it with:\n$ source ~/projects/def-sponsor00/env/bin/activate\nIf you want to exit the virtual environment, you can press Ctrl-D or run:\n(env) $ deactivate\n\nFor future reference, below is how you would install packages on a real Alliance cluster (but please don‚Äôt do it in the training cluster as it is unnecessary and would only slow it down).\nCreate a virtual environment:\n$ virtualenv --no-download ~/env\nActivate the virtual environment:\n$ source ~/env/bin/activate\nUpdate pip:\n(env) $ pip install --no-index --upgrade pip\nInstall the packages you need in the virtual environment:\n(env) $ pip install --no-cache-dir --no-index matplotlib torch torchvision tensorboard\n\n\n\nWrite a Python script\nCreate a directory for this project and cd into it:\nmkdir mnist\ncd mnist\nStart a Python script with the text editor of your choice:\nnano nn.py\nIn it, copy-paste the code we played with in Jupyter, but this time have it run for 10 epochs:\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torchvision import datasets, transforms\nfrom torch.optim.lr_scheduler import StepLR\n\nwriter = SummaryWriter()\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.fc1 = nn.Linear(784, 128)\n        self.fc2 = nn.Linear(128, 10)\n\n    def forward(self, x):\n        x = torch.flatten(x, 1)\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.fc2(x)\n        output = F.log_softmax(x, dim=1)\n        return output\n\ndef train(model, device, train_loader, optimizer, epoch):\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = F.nll_loss(output, target)\n        writer.add_scalar(\"Loss/train\", loss, epoch)\n        loss.backward()\n        optimizer.step()\n        if batch_idx % 10 == 0:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(data), len(train_loader.dataset),\n                100. * batch_idx / len(train_loader), loss.item()))\n\ndef test(model, device, test_loader):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            test_loss += F.nll_loss(output, target, reduction='sum').item()\n            pred = output.argmax(dim=1, keepdim=True)\n            correct += pred.eq(target.view_as(pred)).sum().item()\n\n    test_loss /= len(test_loader.dataset)\n\n    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n        test_loss, correct, len(test_loader.dataset),\n        100. * correct / len(test_loader.dataset)))\n\ndef main():\n    epochs = 10  # don't forget to change the number of epochs\n    torch.manual_seed(1)\n    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n\n    transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,))\n    ])\n\n    train_data = datasets.MNIST(\n        '~/projects/def-sponsor00/data',\n        train=True, download=True, transform=transform)\n\n    test_data = datasets.MNIST(\n        '~/projects/def-sponsor00/data',\n        train=False, transform=transform)\n\n    train_loader = torch.utils.data.DataLoader(train_data, batch_size=64)\n    test_loader = torch.utils.data.DataLoader(test_data, batch_size=1000)\n    model = Net().to(device)\n    optimizer = optim.Adadelta(model.parameters(), lr=1.0)\n    scheduler = StepLR(optimizer, step_size=1, gamma=0.7)\n\n    for epoch in range(1, epochs + 1):\n        train(model, device, train_loader, optimizer, epoch)\n        test(model, device, test_loader)\n        scheduler.step()\n\nmain()\n\nwriter.flush()\nwriter.close()\n\n\nWrite a Slurm script\nWrite a shell script with the text editor of your choice:\nnano nn.sh\nThis is what you want in that script:\n#!/bin/bash\n#SBATCH --time=5:0\n#SBATCH --cpus-per-task=1\n#SBATCH --gres=gpu:1\n#SBATCH --mem=4G\n#SBATCH --output=%x_%j.out\n#SBATCH --error=%x_%j.err\n\npython ~/mnist/nn.py\n\n--time accepts these formats: ‚Äúmin‚Äù, ‚Äúmin:s‚Äù, ‚Äúh:min:s‚Äù, ‚Äúd-h‚Äù, ‚Äúd-h:min‚Äù & ‚Äúd-h:min:s‚Äù\n%x will get replaced by the script name & %j by the job number\n\n\n\nSubmit a job\nFinally, you need to submit your job to Slurm:\n$ sbatch ~/mnist/nn.sh\nYou can check the status of your job with:\n$ sq\n\nPD = pending\nR = running\nCG = completing (Slurm is doing the closing processes)\nNo information = your job has finished running\n\nYou can cancel it with:\n$ scancel <jobid>\nOnce your job has finished running, you can display efficiency measures with:\n$ seff <jobid>"
  },
  {
    "objectID": "ml/mnist.html#lets-explore-our-models-metrics-with-tensorboard",
    "href": "ml/mnist.html#lets-explore-our-models-metrics-with-tensorboard",
    "title": "Classifying the MNIST dataset",
    "section": "Let‚Äôs explore our model‚Äôs metrics with TensorBoard",
    "text": "Let‚Äôs explore our model‚Äôs metrics with TensorBoard\nTensorBoard is a web visualization toolkit developed by TensorFlow which can be used with PyTorch.\nBecause we have sent our model‚Äôs metrics logs to TensorBoard as part of our code, a directory called runs with those logs was created in our ~/mnist directory.\n\nLaunch TensorBoard\nTensorBoard requires too much processing power to be run on the login node. When you run long jobs, the best strategy is to launch it in the background as part of the job. This allows you to monitor your model as it is running (and cancel it if things don‚Äôt look right).\n\nExample:\n\n#!/bin/bash\n#SBATCH ...\n#SBATCH ...\n\ntensorboard --logdir=runs --host 0.0.0.0 &\npython ~/mnist/nn.py\nBecause we only have 1 GPU and are taking turns running our jobs, we need to keep our jobs very short here. So we will launch a separate job for TensorBoard. This time, we will launch an interactive job:\nsalloc --time=1:0:0 --mem=2000M\nTo launch TensorBoard, we need to activate our Python virtual environment (TensorBoard was installed by pip):\nsource ~/projects/def-sponsor00/env/bin/activate\nThen we can launch TensorBoard in the background:\ntensorboard --logdir=~/mnist/runs --host 0.0.0.0 &\nNow, we need to create a connection with SSH tunnelling between your computer and the compute note running your TensorBoard job.\n\n\nConnect to TensorBoard from your computer\nFrom a new terminal on your computer, run:\nssh -NfL localhost:6006:<hostname>:6006 userxxx@uu.c3.ca\n\nReplace <hostname> by the name of the compute node running your salloc job. You can find it by looking at your prompt (your prompt shows <username>@<hostname>).\nReplace <userxxx> by your user name.\n\nNow, you can open a browser on your computer and access TensorBoard at http://localhost:6006."
  },
  {
    "objectID": "ml/nn.html",
    "href": "ml/nn.html",
    "title": "Concepts:",
    "section": "",
    "text": "Artificial intelligence is a vast field: any system mimicking animal intelligence falls in its scope.\nMachine learning (ML) is a subfield of artificial intelligence that can be defined as computer programs whose performance at a task improves with experience.\nSince this experience comes in the form of data, ML consists of feeding vast amounts of data to algorithms to strengthen pathways.\n\n\nFrom xkcd.com\n\n\n\nCoding all the possible ways‚Äîpixel by pixel‚Äîthat a picture can represent a certain object is an impossibly large task. By feeding examples of images of that object to a neural network however, we can train it to recognize that object in images that it has never seen (without explicitly programming how it does this!).\n\n\nFrom xkcd.com"
  },
  {
    "objectID": "ml/nn.html#types-of-learning",
    "href": "ml/nn.html#types-of-learning",
    "title": "Concepts:",
    "section": "Types of learning",
    "text": "Types of learning\nThere are now more types of learning than those presented here. But these initial types are interesting because they will already be familiar to you.\n\nSupervised learning\nYou have been doing supervised machine learning for years without looking at it in the framework of machine learning:\n\nRegression is a form of supervised learning with continuous outputs\nClassification is supervised learning with discrete outputs\n\nSupervised learning uses training data in the form of example input/output \\((x_i, y_i)\\) pairs.\nGoal:\nIf \\(X\\) is the space of inputs and \\(Y\\) the space of outputs, the goal is to find a function \\(h\\) so that\nfor each \\(x_i \\in X\\):\n\n\\(h_\\theta(x_i)\\) is a predictor for the corresponding value \\(y_i\\)\n\n(\\(\\theta\\) represents the set of parameters of \\(h_\\theta\\)).\n\n‚Üí i.e.¬†we want to find the relationship between inputs and outputs.\n\n\nUnsupervised learning\nHere too, you are familiar with some forms of unsupervised learning that you weren‚Äôt thinking about in such terms:\nClustering, social network analysis, market segmentation, PCA ‚Ä¶ are all forms of unsupervised learning.\nUnsupervised learning uses unlabelled data (training set of \\(x_i\\)).\nGoal:\nFind structure within the data.\n\n\nFrom xkcd.com"
  },
  {
    "objectID": "ml/nn.html#artificial-neural-networks",
    "href": "ml/nn.html#artificial-neural-networks",
    "title": "Concepts:",
    "section": "Artificial neural networks",
    "text": "Artificial neural networks\nArtificial neural networks (ANN) are one of the machine learning models (other models include decision trees or Bayesian networks). Their potential and popularity has truly exploded in recent years and this is what we will focus on in this course.\nArtificial neural networks are a series of layered units mimicking the concept of biological neurons: inputs are received by every unit of a layer, computed, then transmitted to units of the next layer. In the process of learning, experience strengthens some connections between units and weakens others.\nIn biological networks, the information consists of action potentials (neuron membrane rapid depolarizations) propagating through the network. In artificial ones, the information consists of tensors (multidimensional arrays) of weights and biases: each unit passes a weighted sum of an input tensor with an additional‚Äîpossibly weighted‚Äîbias through an activation function before passing on the output tensor to the next layer of units.\n\n\nThe bias allows to shift the output of the activation function to the right or to the left (i.e.¬†it creates an offset).\n\nSchematic of a biological neuron:\n\n\nFrom Dhp1080, Wikipedia\n\nSchematic of an artificial neuron:\n\n\nModified from O.C. Akgun & J. Mei 2019\n\nWhile biological neurons are connected in extremely intricate patterns, artificial ones follow a layered structure. Another difference in complexity is in the number of units: the human brain has 65‚Äì90 billion neurons. ANN have much fewer units.\nNeurons in mouse cortex:\n\n\nNeurons are in green, the dark branches are blood vessels. Image by Na Ji, UC Berkeley\n\nNeural network with 2 hidden layers:\n\n\nFrom The Maverick Meerkat\n\nThe information in biological neurons is an all-or-nothing electrochemical pulse or action potential. Greater stimuli don‚Äôt produce stronger signals but increase firing frequency. In contrast, artificial neurons pass the computation of their inputs through an activation function and the output can take any of the values possible with that function.\nThreshold potential in biological neurons:\n\n\nModified from Blacktc, Wikimedia\n\nSome of the most common activation functions in artificial neurons:\n\n\nFrom Diganta Misra 2019\n\nWhich activation function to use depends on the type of problem and the available computing budget. Some early functions have fallen out of use while new ones have emerged (e.g.¬†sigmoid got replaced by ReLU which is easier to train).\n\nLearning\nThe process of learning in biological NN happens through neuron death or growth and through the creation or loss of synaptic connections between neurons. In ANN, learning happens through optimization algorithms such as gradient descent which minimize cross entropy loss functions by adjusting the weights and biases connecting each layer of neurons over many iterations (cross entropy is the difference between the predicted and the real distributions).\n\n\nFrom xkcd.com\n\n\n\nGradient descent\nThere are several gradient descent methods:\nBatch gradient descent uses all examples in each iteration and is thus slow for large datasets (the parameters are adjusted only after all the samples have been processed).\nMini-batch gradient descent is an intermediate approach: it uses mini-batch sized examples in each iteration. This allows a vectorized approach (and hence parallelization).\nThe Adam optimization algorithm is a popular variation of mini-batch gradient descent.\nStochastic gradient descent uses one example in each iteration. It is thus much faster than batch gradient descent (the parameters are adjusted after each example). But it does not allow any vectorization.\n\n\nFrom Imad Dabbura\n\n\n\n3Blue1Brown by Grant Sanderson videos\n3Blue1Brown by Grant Sanderson has a series of 4 videos on neural networks which is easy to watch, fun, and does an excellent job at introducing the functioning of a simple neural network.\n\nWhat are NN? (19 min)\n\nWatch this video beyond the acknowledgement as the function ReLU (a really important function in modern neural networks) is introduced at the very end.\n\n\n\nAs you develop your own ML models, if you find that your mathematical background is shaky, 3blue1brown also has an excellent series of videos on linear algebra and an equally great one on calculus.\n\n\n\nHow do NN learn? (21 min)\n\n\n\nWhat is backpropagation? (14 min)\n\n\nThere is one minor terminological error in this video: they call the use of mini-batches stochastic gradient descent. In fact, this is called mini-batch gradient descent. Stochastic gradient descent uses a single example at each iteration.\n\n\n\nHow does backpropagation work? (10 min)\n\n\n\n\nTypes of ANN\n\nFully connected neural networks\n\n\nFrom Glosser.ca, Wikipedia\n\nEach neuron receives inputs from every neuron of the previous layer and passes its output to every neuron of the next layer.\n\n\nConvolutional neural networks\n\n\nFrom Programming Journeys by Rensu Theart\n\nConvolutional neural networks (CNN) are used for spatially structured data (e.g.¬†in image recognition).\nImages have huge input sizes and would require a very large number of neurons in a fully connected neural net. In convolutional layers, neurons receive input from a subarea (called local receptive field) of the previous layer. This greatly reduces the number of parameters.\nOptionally, pooling (combining the outputs of neurons in a subarea) reduces the data dimensions. The stride then dictates how the subarea is moved across the image. Max-pooling is one of the forms of pooling which uses the maximum for each subarea.\n\n\nRecurrent neural networks\n\n\nFrom fdeloche, Wikipedia\n\nRecurrent neural networks (RNN) such as Long Short-Term Memory (LSTM) are used for chain structured data (e.g.¬†in speech recognition).\nThey are not feedforward networks (i.e.¬†networks for which the information moves only in the forward direction without any loop).\n\n\nTransformers\nA combination of two RNNs or sets of RNNs (the encoder and the decoder) is used in sequence to sequence models for translation or picture captioning. Such models were slow to process a lot of data.\nIn 2014 and 2015, the concept of attention (giving added weight to important words) was developed, greatly improving the ability of such models to process a lot of data.\nThis blog post by Jay Alammar‚Äîa blogger whose high-quality posts have been referenced in MIT and Stanford courses‚Äîexplains this in a high-level visual fashion.\nThe problem with recurrence is that it is not easily to parallelize (and thus to run fast on GPUs).\nIn 2017, a new model‚Äîthe transformer‚Äîwas proposed: by using only attention mechanisms and no recurrence, the transformer achieves better results in an easily parallelizable fashion.\nJay Alammar has also a blog post on the transformer. The post includes a 30 min video.\nWith the addition of transfer learning, powerful transformers emerged in the field of NLP (Natural Language Processing). Examples include BERT (Bidirectional Encoder Representations from Transformers) from Google and GPT-3 (Generative Pre-trained Transformer-3) from OpenAI.\nJay Alammar has yet another great blog post on these advanced NLP models.\n\n\n\nDeep learning\nThe first layer of a neural net is the input layer. The last one is the output layer. All the layers in-between are called hidden layers. Shallow neural networks have only one hidden layer and deep networks have two or more hidden layers. When an ANN is deep, we talk about Deep Learning (DL).\n\n\nFrom xkcd.com"
  },
  {
    "objectID": "ml/nn_building.html",
    "href": "ml/nn_building.html",
    "title": "Building a neural network",
    "section": "",
    "text": "Key to creating neural networks in PyTorch is the torch.nn package which contains the nn.Module and a forward method which returns an output from some input.\nLet‚Äôs build a neural network to classify the MNIST."
  },
  {
    "objectID": "ml/nn_building.html#load-packages",
    "href": "ml/nn_building.html#load-packages",
    "title": "Building a neural network",
    "section": "Load packages",
    "text": "Load packages\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F"
  },
  {
    "objectID": "ml/nn_building.html#define-the-architecture-of-the-network",
    "href": "ml/nn_building.html#define-the-architecture-of-the-network",
    "title": "Building a neural network",
    "section": "Define the architecture of the network",
    "text": "Define the architecture of the network\nFirst, we need to define the architecture of the network.\nThere are many types of architectures. For images, CNN are well suited.\nIn Python, you can define a subclass of an existing class with:\nclass YourSubclass(BaseClass):\n    <definition of your subclass>        \nYour subclass is derived from the base class and inherits its properties.\nPyTorch contains the class torch.nn.Module which is used as the base class when defining a neural network.\n\nclass Net(nn.Module):\n    def __init__(self):\n      super(Net, self).__init__()\n      \n      # First 2D convolutional layer, taking in 1 input channel (image),\n      # outputting 32 convolutional features.\n      # Convolution adds each pixel of an image to its neighbours,\n      # weighted by the kernel (a small matrix).\n      # Here, the kernel is square and of size 3*3\n      # Convolution helps to extract features from the input\n      # (e.g. edge detection, blurriness, sharpeness...)\n      self.conv1 = nn.Conv2d(1, 32, 3)\n      # Second 2D convolutional layer, taking in the 32 input channels,\n      # outputting 64 convolutional features, with a kernel size of 3*3\n      self.conv2 = nn.Conv2d(32, 64, 3)\n\n      # Dropouts randomly blocks a fraction of the neurons during training\n      # This is a regularization technique which prevents overfitting\n      self.dropout1 = nn.Dropout2d(0.25)\n      self.dropout2 = nn.Dropout2d(0.5)\n\n      # First fully connected layer\n      self.fc1 = nn.Linear(9216, 128)\n      # Second fully connected layer that outputs our 10 labels\n      self.fc2 = nn.Linear(128, 10)"
  },
  {
    "objectID": "ml/nn_building.html#set-the-flow-of-data-through-the-network",
    "href": "ml/nn_building.html#set-the-flow-of-data-through-the-network",
    "title": "Building a neural network",
    "section": "Set the flow of data through the network",
    "text": "Set the flow of data through the network\nThe feed-forward algorithm is defined by the forward function.\n\nclass Net(nn.Module):\n    def __init__(self):\n      super(Net, self).__init__()\n      self.conv1 = nn.Conv2d(1, 32, 3)\n      self.conv2 = nn.Conv2d(32, 64, 3)\n      self.dropout1 = nn.Dropout2d(0.25)\n      self.dropout2 = nn.Dropout2d(0.5)\n      self.fc1 = nn.Linear(9216, 128)\n      self.fc2 = nn.Linear(128, 10)\n\n    # x represents the data\n    def forward(self, x):\n      # Pass data through conv1\n      x = self.conv1(x)\n      # Use the rectified-linear activation function over x\n      x = F.relu(x)\n\n      x = self.conv2(x)\n      x = F.relu(x)\n\n      # Run max pooling over x\n      x = F.max_pool2d(x, 2)\n      # Pass data through dropout1\n      x = self.dropout1(x)\n      # Flatten x with start_dim=1\n      x = torch.flatten(x, 1)\n      # Pass data through fc1\n      x = self.fc1(x)\n      x = F.relu(x)\n      x = self.dropout2(x)\n      x = self.fc2(x)\n\n      # Apply softmax to x\n      output = F.log_softmax(x, dim=1)\n      return output\n\nLet‚Äôs create an instance of Net and print its structure:\n\nnet = Net()\nprint(net)\n\nNet(\n  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n  (dropout1): Dropout2d(p=0.25, inplace=False)\n  (dropout2): Dropout2d(p=0.5, inplace=False)\n  (fc1): Linear(in_features=9216, out_features=128, bias=True)\n  (fc2): Linear(in_features=128, out_features=10, bias=True)\n)"
  },
  {
    "objectID": "ml/nn_training.html",
    "href": "ml/nn_training.html",
    "title": "Training a model",
    "section": "",
    "text": "Before we can train a model, we need to:\n\nload the needed packages,\nget the data,\ncreate data loaders for training and testing,\ndefine a model.\n\nLet‚Äôs do this for the FashionMNIST dataset:\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets\nfrom torchvision.transforms import ToTensor, Lambda\n\ntraining_data = datasets.FashionMNIST(\n    root=\"~/projects/def-sponsor00/data/\",\n    train=True,\n    download=True,\n    transform=ToTensor(),\n    target_transform=Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n)\n\ntest_data = datasets.FashionMNIST(\n    root=\"~/projects/def-sponsor00/data/\",\n    train=False,\n    download=True,\n    transform=ToTensor(),\n    target_transform=Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n)\n\ntrain_dataloader = DataLoader(training_data, batch_size=10)\ntest_dataloader = DataLoader(test_data, batch_size=10)\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.flatten = nn.Flatten()\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(28*28, 512),\n            nn.ReLU(),\n            nn.Linear(512, 512),\n            nn.ReLU(),\n            nn.Linear(512, 10),\n        )\n\n    def forward(self, x):\n        x = self.flatten(x)\n        logits = self.linear_relu_stack(x)\n        return logits\n\nmodel = Net()"
  },
  {
    "objectID": "ml/nn_training.html#hyperparameters",
    "href": "ml/nn_training.html#hyperparameters",
    "title": "Training a model",
    "section": "Hyperparameters",
    "text": "Hyperparameters\nWhile the learning parameters of a model (weights and biases) are the values that get adjusted through training (and they will become part of the final program, along with the model architecture, once training is over), hyperparameters control the training process.\nThey include:\n\nbatch size: number of samples passed through the model before the parameters are updated,\nnumber of epochs: number iterations,\nlearning rate: size of the incremental changes to model parameters at each iteration. Smaller values yield slow learning speed, while large values may miss minima.\n\nLet‚Äôs define them here:\nlearning_rate = 1e-3\nbatch_size = 64\nepochs = 5"
  },
  {
    "objectID": "ml/nn_training.html#define-the-loss-function",
    "href": "ml/nn_training.html#define-the-loss-function",
    "title": "Training a model",
    "section": "Define the loss function",
    "text": "Define the loss function\nTo assess the predicted outputs of our model against the true values from the labels, we also need a loss function (e.g.¬†mean square error for regressions: nn.MSELoss or negative log likelihood for classification: nn.NLLLoss)\nThe machine learning literature is rich in information about various loss functions.\nHere is an example with nn.CrossEntropyLoss which combines nn.LogSoftmax and nn.NLLLoss:\nloss_fn = nn.CrossEntropyLoss()"
  },
  {
    "objectID": "ml/nn_training.html#initialize-the-optimizer",
    "href": "ml/nn_training.html#initialize-the-optimizer",
    "title": "Training a model",
    "section": "Initialize the optimizer",
    "text": "Initialize the optimizer\nThe optimization algorithm determines how the model parameters get adjusted at each iteration.\nThere are many optimizers and you need to search in the literature which one performs best for your time of model and data.\nBelow is an example with stochastic gradient descent:\noptimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n\n\nlr is the learning rate\nmomentum is a method increasing convergence rate and reducing oscillation for SDG"
  },
  {
    "objectID": "ml/nn_training.html#define-the-train-and-test-loops",
    "href": "ml/nn_training.html#define-the-train-and-test-loops",
    "title": "Training a model",
    "section": "Define the train and test loops",
    "text": "Define the train and test loops\nFinally, we need to define the train and test loops.\nThe train loop:\n\ngets a batch of training data from the DataLoader,\nresets the gradients of model parameters with optimizer.zero_grad(),\ncalculates predictions from the model for an input batch,\ncalculates the loss for that set of predictions vs.¬†the labels on the dataset,\ncalculates the backward gradients over the learning parameters (that‚Äôs the backpropagation) with loss.backward(),\nadjusts the parameters by the gradients collected in the backward pass with optimizer.step().\n\nThe test loop evaluates the model‚Äôs performance against the test data.\ndef train(dataloader, model, loss_fn, optimizer):\n    size = len(dataloader.dataset)\n    for batch, (X, y) in enumerate(dataloader):\n        # Compute prediction and loss\n        pred = model(X)\n        loss = loss_fn(pred, y)\n\n        # Backpropagation\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if batch % 100 == 0:\n            loss, current = loss.item(), batch * len(X)\n            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n\n\ndef test(dataloader, model, loss_fn):\n    size = len(dataloader.dataset)\n    num_batches = len(dataloader)\n    test_loss, correct = 0, 0\n\n    with torch.no_grad():\n        for X, y in dataloader:\n            pred = model(X)\n            test_loss += loss_fn(pred, y).item()\n            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n\n    test_loss /= num_batches\n    correct /= size\n    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
  },
  {
    "objectID": "ml/nn_training.html#train",
    "href": "ml/nn_training.html#train",
    "title": "Training a model",
    "section": "Train",
    "text": "Train\nTo train our model, we just run the loop over the epochs:\nfor t in range(epochs):\n    print(f\"Epoch {t+1}\\n-------------------------------\")\n    train(train_dataloader, model, loss_fn, optimizer)\n    test(test_dataloader, model, loss_fn)\nprint(\"Training completed\")"
  },
  {
    "objectID": "ml/pretrained_models.html",
    "href": "ml/pretrained_models.html",
    "title": "Finding pretrained models for transfer learning",
    "section": "",
    "text": "Training models from scratch requires way too much data, time, and computing power (or money) to be a practical option. This is why transfer learning has become such a common practice: by starting with models trained on related problems, you are saving time and achieving good results with little data.\nNow, where do you find such models?\nIn this workshop, we will have a look at some of the most popular pre-trained models repositories and libraries (Model Zoo, PyTorch Hub, and Hugging Face); see how you can also search models in the literature and on GitHub; and finally learn how to import models into PyTorch.\n\n\nPrerequisites:\nIf you want to follow the hands-on part of this workshop, please make sure to have an up-to-date version of PyTorch on your laptop."
  },
  {
    "objectID": "ml/resources.html",
    "href": "ml/resources.html",
    "title": "Resources",
    "section": "",
    "text": "Arxiv Sanity Preserver by Andrej Karpathy\nML papers in the computer science category on arXiv\nML papers in the stats category on arXiv\nDistill ML research online journal\n\n\n\nAdvice and sources from ML research student\n\n\n\nStack Overflow [machine-learning] tag\nStack Overflow [deep-learning] tag\nStack Overflow [supervised-learning] tag\nStack Overflow [unsupervised-learning] tag\nStack Overflow [semisupervised-learning] tag\nStack Overflow [reinforcement-learning] tag\nStack Overflow [transfer-learning] tag\nStack Overflow [machine-learning-model] tag\nStack Overflow [learning-rate] tag\nStack Overflow [bayesian-deep-learning] tag\n\n\n\ndeeplearning.ai\nfast.ai\nGoogle\n\n\n\nbenchmarks.ai\nAIBench\nkaggle\nWikipedia"
  },
  {
    "objectID": "ml/resources.html#pytorch",
    "href": "ml/resources.html#pytorch",
    "title": "Resources",
    "section": "PyTorch",
    "text": "PyTorch\n\nDocumentation\nPyTorch website\nPyTorch documentation\nPyTorch tutorials\nPyTorch online courses\nPyTorch examples\n\n\nGetting help\nPyTorch Discourse forum\nStack Overflow [pytorch] tag\nStack Overflow [pytorch-dataloader] tag\nStack Overflow [pytorch-ignite] tag\n\n\nPre-trained models\nPyTorch Hub"
  },
  {
    "objectID": "ml/resources.html#python",
    "href": "ml/resources.html#python",
    "title": "Resources",
    "section": "Python",
    "text": "Python\n\nIDE\nProject Jupyter\nList of IDEs with description\nComparison of IDEs\nEmacs Python IDE\n\n\nShell\nIPython\nbpython\nptpython\n\n\nGetting help\nStack Overflow [python] tag"
  },
  {
    "objectID": "ml/resources.html#fastai",
    "href": "ml/resources.html#fastai",
    "title": "Resources",
    "section": "fastai",
    "text": "fastai\n\nDocumentation\nManual\nTutorials\nPeer-reviewed paper\n\n\nBook\nPaperback version\nFree MOOC version of part 1 of the book\nJupyter notebooks version of the book\n\n\nGetting help\nDiscourse forum"
  },
  {
    "objectID": "ml/torchtensors.html",
    "href": "ml/torchtensors.html",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "",
    "text": "Before information can be fed to artificial neural networks (ANNs), it needs to be converted to a form ANNs can process: floating point numbers. Indeed, you don‚Äôt pass a sentence or an image through an ANN; you input numbers representing a sequence of words or pixel values.\nAll these floating point numbers need to be stored in a data structure. The most suited structure is multidimensional (to hold several layers of information) and since all data is of the same type, it is an array.\nPython already has several multidimensional array structures‚Äîthe most popular of which being NumPy‚Äôs ndarray‚Äîbut the particularities of deep learning call for special characteristics: ability to run operations on GPUs and/or in a distributed fashion, as well as the ability to keep track of computation graphs for automatic differentiation.\nThe PyTorch tensor is a Python data structure with these characteristics that can also easily be converted to/from NumPy‚Äôs ndarray and integrates well with other Python libraries such as Pandas.\nIn this webinar, suitable for users of all levels, we will have a deep look at this data structure and go much beyond a basic introduction.\nIn particular, we will:\n\nsee how tensors are stored in memory,\nlook at the metadata which allows this efficient memory storage,\ncover the basics of working with tensors (indexing, vectorized operations‚Ä¶),\nmove tensors to/from GPUs,\nconvert tensors to/from NumPy ndarrays,\nsee how tensors work in distributed frameworks,\nsee how linear algebra can be done with PyTorch tensors.\n\n\nSlides (click and wait: my reveal.js presentations are heavy and take some time to load‚Ä¶)"
  },
  {
    "objectID": "ml/torchtensors_slides.html#acknowledgements",
    "href": "ml/torchtensors_slides.html#acknowledgements",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nMany drawings in this webinar come from the book:\n\nThe section on storage is also highly inspired by it"
  },
  {
    "objectID": "ml/torchtensors_slides.html#using-tensors-locally",
    "href": "ml/torchtensors_slides.html#using-tensors-locally",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Using tensors locally",
    "text": "Using tensors locally\n\nYou need to have Python & PyTorch installed\nAdditionally, you might want to use an IDE such as elpy if you are an Emacs user, JupyterLab, etc.\n\nNote that PyTorch does not yet support Python 3.10 except in some Linux distributions or on systems where a wheel has been built For the time being, you might have to use it with Python 3.9"
  },
  {
    "objectID": "ml/torchtensors_slides.html#using-tensors-on-cc-clusters",
    "href": "ml/torchtensors_slides.html#using-tensors-on-cc-clusters",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Using tensors on CC clusters",
    "text": "Using tensors on CC clusters\nIn the cluster terminal:\navail_wheels \"torch*\" # List available wheels & compatible Python versions\nmodule avail python   # List available Python versions\nmodule load python/3.9.6             # Load a sensible Python version\nvirtualenv --no-download env         # Create a virtual env\nsource env/bin/activate              # Activate the virtual env\npip install --no-index --upgrade pip # Update pip\npip install --no-index torch         # Install PyTorch\nYou can then launch jobs with sbatch or salloc\nLeave the virtual env with the command: deactivate"
  },
  {
    "objectID": "ml/torchtensors_slides.html#outline",
    "href": "ml/torchtensors_slides.html#outline",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Outline",
    "text": "Outline\n\nWhat is a PyTorch tensor?\nMemory storage\nData type (dtype)\nBasic operations\nWorking with NumPy\nLinear algebra\nHarvesting the power of GPUs\nDistributed operations"
  },
  {
    "objectID": "ml/torchtensors_slides.html#outline-1",
    "href": "ml/torchtensors_slides.html#outline-1",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Outline",
    "text": "Outline\n\nWhat is a PyTorch tensor?\nMemory storage\nData type (dtype)\nBasic operations\nWorking with NumPy\nLinear algebra\nHarvesting the power of GPUs\nDistributed operations"
  },
  {
    "objectID": "ml/torchtensors_slides.html#ann-do-not-process-information-directly",
    "href": "ml/torchtensors_slides.html#ann-do-not-process-information-directly",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "ANN do not process information directly",
    "text": "ANN do not process information directly\n Modified from Stevens, E., Antiga, L., & Viehmann, T. (2020). Deep learning with PyTorch. Manning Publications"
  },
  {
    "objectID": "ml/torchtensors_slides.html#it-needs-to-be-converted-to-numbers",
    "href": "ml/torchtensors_slides.html#it-needs-to-be-converted-to-numbers",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "It needs to be converted to numbers",
    "text": "It needs to be converted to numbers\n Modified from Stevens, E., Antiga, L., & Viehmann, T. (2020). Deep learning with PyTorch. Manning Publications"
  },
  {
    "objectID": "ml/torchtensors_slides.html#these-numbers-must-be-stored-in-a-data-structure",
    "href": "ml/torchtensors_slides.html#these-numbers-must-be-stored-in-a-data-structure",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "These numbers must be stored in a data structure",
    "text": "These numbers must be stored in a data structure\n\nPyTorch tensors are Python objects holding multidimensional arrays\n Stevens, E., Antiga, L., & Viehmann, T. (2020). Deep learning with PyTorch. Manning Publications"
  },
  {
    "objectID": "ml/torchtensors_slides.html#why-a-new-object-when-numpy-already-exists",
    "href": "ml/torchtensors_slides.html#why-a-new-object-when-numpy-already-exists",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Why a new object when NumPy already exists?",
    "text": "Why a new object when NumPy already exists?\n\n\nCan run on accelerators (GPUs, TPUs‚Ä¶)\nKeep track of computation graphs, allowing automatic differentiation\nFuture plan for sharded tensors to run distributed computations"
  },
  {
    "objectID": "ml/torchtensors_slides.html#what-is-a-pytorch-tensor-2",
    "href": "ml/torchtensors_slides.html#what-is-a-pytorch-tensor-2",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "What is a PyTorch tensor?",
    "text": "What is a PyTorch tensor?\nPyTorch is foremost a deep learning library\nIn deep learning, the information contained in objects of interest (e.g.¬†images, texts, sounds) is converted to floating-point numbers (e.g.¬†pixel values, token values, frequencies)\nAs this information is complex, multiple dimensions are required (e.g.¬†two dimensions for the width & height of an image, plus one dimension for the RGB colour channels)\nAdditionally, items are grouped into batches to be processed together, adding yet another dimension\nMultidimensional arrays are thus particularly well suited for deep learning"
  },
  {
    "objectID": "ml/torchtensors_slides.html#what-is-a-pytorch-tensor-3",
    "href": "ml/torchtensors_slides.html#what-is-a-pytorch-tensor-3",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "What is a PyTorch tensor?",
    "text": "What is a PyTorch tensor?\nArtificial neurons perform basic computations on these tensors\nTheir number however is huge & computing efficiency is paramount\nGPUs/TPUs are particularly well suited to perform many simple operations in parallel\nThe very popular NumPy library has, at its core, a mature multidimensional array object well integrated into the scientific Python ecosystem\nBut the PyTorch tensor has additional efficiency characteristics ideal for machine learning & it can be converted to/from NumPy‚Äôs ndarray if needed"
  },
  {
    "objectID": "ml/torchtensors_slides.html#outline-2",
    "href": "ml/torchtensors_slides.html#outline-2",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Outline",
    "text": "Outline\n\nWhat is a PyTorch tensor?\nMemory storage\nData type (dtype)\nBasic operations\nWorking with NumPy\nLinear algebra\nHarvesting the power of GPUs\nDistributed operations"
  },
  {
    "objectID": "ml/torchtensors_slides.html#efficient-memory-storage",
    "href": "ml/torchtensors_slides.html#efficient-memory-storage",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Efficient memory storage",
    "text": "Efficient memory storage\nIn Python, collections (lists, tuples) are groupings of boxed Python objects\nPyTorch tensors & NumPy ndarrays are made of unboxed C numeric types\n\n\nStevens, E., Antiga, L., & Viehmann, T. (2020). Deep learning with PyTorch. Manning Publications"
  },
  {
    "objectID": "ml/torchtensors_slides.html#efficient-memory-storage-1",
    "href": "ml/torchtensors_slides.html#efficient-memory-storage-1",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Efficient memory storage",
    "text": "Efficient memory storage\nThey are usually contiguous memory blocks, but the main difference is that they are unboxed: floats will thus take 4 (32-bit) or 8 (64-bit) bytes each\nBoxed values take up more memory (memory for the pointer + memory for the primitive)\n\n\nStevens, E., Antiga, L., & Viehmann, T. (2020). Deep learning with PyTorch. Manning Publications"
  },
  {
    "objectID": "ml/torchtensors_slides.html#implementation",
    "href": "ml/torchtensors_slides.html#implementation",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Implementation",
    "text": "Implementation\nUnder the hood, the values of a PyTorch tensor are stored as a torch.Storage instance which is a one-dimensional array\n\nimport torch\nt = torch.arange(10.).view(2, 5); print(t) # Functions explained later\n\n[Out]\n\ntensor([[ 0.,  1.,  2., 3.,  4.],\n        [ 5.,  6.,  7.,  8.,  9.]])"
  },
  {
    "objectID": "ml/torchtensors_slides.html#implementation-1",
    "href": "ml/torchtensors_slides.html#implementation-1",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Implementation",
    "text": "Implementation\n\nstorage = t.storage(); print(storage)\n\n[Out]\n\n 0.0\n 1.0\n 2.0\n 3.0\n 4.0\n 5.0\n 6.0\n 7.0\n 8.0\n 9.0\n[torch.FloatStorage of size 10]"
  },
  {
    "objectID": "ml/torchtensors_slides.html#implementation-2",
    "href": "ml/torchtensors_slides.html#implementation-2",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Implementation",
    "text": "Implementation\nThe storage can be indexed\nstorage[3]\n\n[Out]\n\n3.0"
  },
  {
    "objectID": "ml/torchtensors_slides.html#implementation-3",
    "href": "ml/torchtensors_slides.html#implementation-3",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Implementation",
    "text": "Implementation\n\nstorage[3] = 10.0; print(storage)\n\n[Out]\n\n 0.0\n 1.0\n 2.0\n 10.0\n 4.0\n 5.0\n 6.0\n 7.0\n 8.0\n 9.0\n[torch.FloatStorage of size 10]"
  },
  {
    "objectID": "ml/torchtensors_slides.html#implementation-4",
    "href": "ml/torchtensors_slides.html#implementation-4",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Implementation",
    "text": "Implementation\nTo view a multidimensional array from storage, we need metadata:\n\nthe size (shape in NumPy) sets the number of elements in each dimension\nthe offset indicates where the first element of the tensor is in the storage\nthe stride establishes the increment between each element"
  },
  {
    "objectID": "ml/torchtensors_slides.html#storage-metadata",
    "href": "ml/torchtensors_slides.html#storage-metadata",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Storage metadata",
    "text": "Storage metadata\n\n\nStevens, E., Antiga, L., & Viehmann, T. (2020). Deep learning with PyTorch. Manning Publications"
  },
  {
    "objectID": "ml/torchtensors_slides.html#storage-metadata-1",
    "href": "ml/torchtensors_slides.html#storage-metadata-1",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Storage metadata",
    "text": "Storage metadata\n\nt.size()\nt.storage_offset()\nt.stride()\n\n[Out]\n\ntorch.Size([2, 5])\n0\n(5, 1)"
  },
  {
    "objectID": "ml/torchtensors_slides.html#storage-metadata-2",
    "href": "ml/torchtensors_slides.html#storage-metadata-2",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Storage metadata",
    "text": "Storage metadata"
  },
  {
    "objectID": "ml/torchtensors_slides.html#sharing-storage",
    "href": "ml/torchtensors_slides.html#sharing-storage",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Sharing storage",
    "text": "Sharing storage\nMultiple tensors can use the same storage, saving a lot of memory since the metadata is a lot lighter than a whole new array\n\n\nStevens, E., Antiga, L., & Viehmann, T. (2020). Deep learning with PyTorch. Manning Publications"
  },
  {
    "objectID": "ml/torchtensors_slides.html#transposing-in-2-dimensions",
    "href": "ml/torchtensors_slides.html#transposing-in-2-dimensions",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Transposing in 2 dimensions",
    "text": "Transposing in 2 dimensions\n\nt = torch.tensor([[3, 1, 2], [4, 1, 7]]); print(t)\nt.size()\nt.t()\nt.t().size()\n\n[Out]\n\ntensor([[3, 1, 2],\n        [4, 1, 7]])\ntorch.Size([2, 3])\ntensor([[3, 4],\n        [1, 1],\n        [2, 7]])\ntorch.Size([3, 2])"
  },
  {
    "objectID": "ml/torchtensors_slides.html#transposing-in-2-dimensions-1",
    "href": "ml/torchtensors_slides.html#transposing-in-2-dimensions-1",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Transposing in 2 dimensions",
    "text": "Transposing in 2 dimensions\n= flipping the stride elements around\n\n\nStevens, E., Antiga, L., & Viehmann, T. (2020). Deep learning with PyTorch. Manning Publications"
  },
  {
    "objectID": "ml/torchtensors_slides.html#transposing-in-higher-dimensions",
    "href": "ml/torchtensors_slides.html#transposing-in-higher-dimensions",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Transposing in higher dimensions",
    "text": "Transposing in higher dimensions\n\ntorch.t() is a shorthand for torch.transpose(0, 1):\ntorch.equal(t.t(), t.transpose(0, 1))\n\n[Out]\n\nTrue\nWhile torch.t() only works for 2D tensors, torch.transpose() can be used to transpose 2 dimensions in tensors of any number of dimensions"
  },
  {
    "objectID": "ml/torchtensors_slides.html#transposing-in-higher-dimensions-1",
    "href": "ml/torchtensors_slides.html#transposing-in-higher-dimensions-1",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Transposing in higher dimensions",
    "text": "Transposing in higher dimensions\n\nt = torch.zeros(1, 2, 3); print(t)\n\nt.size()\nt.stride()\n\n[Out]\n\ntensor([[[0., 0., 0.],\n         [0., 0., 0.]]])\n\ntorch.Size([1, 2, 3])\n(6, 3, 1)"
  },
  {
    "objectID": "ml/torchtensors_slides.html#transposing-in-higher-dimensions-2",
    "href": "ml/torchtensors_slides.html#transposing-in-higher-dimensions-2",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Transposing in higher dimensions",
    "text": "Transposing in higher dimensions\n\nt.transpose(0, 1)\n\nt.transpose(0, 1).size()\nt.transpose(0, 1).stride()\n\n[Out]\n\ntensor([[[0., 0., 0.]],\n        [[0., 0., 0.]]])\n\ntorch.Size([2, 1, 3])\n(3, 6, 1)  # Notice how transposing flipped 2 elements of the stride"
  },
  {
    "objectID": "ml/torchtensors_slides.html#transposing-in-higher-dimensions-3",
    "href": "ml/torchtensors_slides.html#transposing-in-higher-dimensions-3",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Transposing in higher dimensions",
    "text": "Transposing in higher dimensions\n\nt.transpose(0, 2)\n\nt.transpose(0, 2).size()\nt.transpose(0, 2).stride()\n\n[Out]\n\ntensor([[[0.],\n         [0.]],\n        [[0.],\n         [0.]],\n        [[0.],\n         [0.]]])\n\ntorch.Size([3, 2, 1])\n(1, 3, 6)"
  },
  {
    "objectID": "ml/torchtensors_slides.html#transposing-in-higher-dimensions-4",
    "href": "ml/torchtensors_slides.html#transposing-in-higher-dimensions-4",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Transposing in higher dimensions",
    "text": "Transposing in higher dimensions\n\nt.transpose(1, 2)\n\nt.transpose(1, 2).size()\nt.transpose(1, 2).stride()\n\n[Out]\n\ntensor([[[0., 0.],\n         [0., 0.],\n         [0., 0.]]])\n\ntorch.Size([1, 3, 2])\n(6, 1, 3)"
  },
  {
    "objectID": "ml/torchtensors_slides.html#outline-3",
    "href": "ml/torchtensors_slides.html#outline-3",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Outline",
    "text": "Outline\n\nWhat is a PyTorch tensor?\nMemory storage\nData type (dtype)\nBasic operations\nWorking with NumPy\nLinear algebra\nHarvesting the power of GPUs\nDistributed operations"
  },
  {
    "objectID": "ml/torchtensors_slides.html#default-dtype",
    "href": "ml/torchtensors_slides.html#default-dtype",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Default dtype",
    "text": "Default dtype\n\nSince PyTorch tensors were built with utmost efficiency in mind for neural networks, the default data type is 32-bit floating points\nThis is sufficient for accuracy & much faster than 64-bit floating points\n\nNote that, by contrast, NumPy ndarrays use 64-bit as their default"
  },
  {
    "objectID": "ml/torchtensors_slides.html#list-of-pytorch-tensor-dtypes",
    "href": "ml/torchtensors_slides.html#list-of-pytorch-tensor-dtypes",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "List of PyTorch tensor dtypes",
    "text": "List of PyTorch tensor dtypes\n\n\n\n\ntorch.float16 / torch.half\n\n\n‚ÄÉ‚ÄÉ\n\n\n16-bit / half-precision floating-point\n\n\n\n\ntorch.float32 / torch.float\n\n\n\n\n32-bit / single-precision floating-point\n\n\n\n\ntorch.float64 / torch.double\n\n\n\n\n64-bit / double-precision floating-point\n\n\n\n\n\n\n\n\n\n\ntorch.uint8\n\n\n\n\nunsigned 8-bit integers\n\n\n\n\ntorch.int8\n\n\n\n\nsigned 8-bit integers\n\n\n\n\ntorch.int16 / torch.short\n\n\n\n\nsigned 16-bit integers\n\n\n\n\ntorch.int32 / torch.int\n\n\n\n\nsigned 32-bit integers\n\n\n\n\ntorch.int64 / torch.long\n\n\n\n\nsigned 64-bit integers\n\n\n\n\n\n\n\n\n\n\n\n\ntorch.bool\n\n\n\n\nboolean"
  },
  {
    "objectID": "ml/torchtensors_slides.html#checking-changing-dtype",
    "href": "ml/torchtensors_slides.html#checking-changing-dtype",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Checking & changing dtype",
    "text": "Checking & changing dtype\n\nt = torch.rand(2, 3); print(t)\nt.dtype   # Remember that the default dtype for PyTorch tensors is float32\nt2 = t.type(torch.float64); print(t2) # If dtype ‚â† default, it is printed\nt2.dtype\n\n[Out]\n\ntensor([[0.8130, 0.3757, 0.7682],\n        [0.3482, 0.0516, 0.3772]])\ntorch.float32\ntensor([[0.8130, 0.3757, 0.7682],\n        [0.3482, 0.0516, 0.3772]], dtype=torch.float64)\ntorch.float64"
  },
  {
    "objectID": "ml/torchtensors_slides.html#outline-4",
    "href": "ml/torchtensors_slides.html#outline-4",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Outline",
    "text": "Outline\n\nWhat is a PyTorch tensor?\nMemory storage\nData type (dtype)\nBasic operations\nWorking with NumPy\nLinear algebra\nHarvesting the power of GPUs\nDistributed operations"
  },
  {
    "objectID": "ml/torchtensors_slides.html#creating-tensors",
    "href": "ml/torchtensors_slides.html#creating-tensors",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Creating tensors",
    "text": "Creating tensors\n\ntorch.tensor: ‚ÄÉ‚ÄÉInput individual values\ntorch.arange: ‚ÄÉ‚ÄÉSimilar to range but creates a 1D tensor\ntorch.linspace: ‚ÄÉ1D linear scale tensor\ntorch.logspace: ‚ÄÉ1D log scale tensor\ntorch.rand: ‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÇRandom numbers from a uniform distribution on [0, 1)\ntorch.randn: ‚ÄÉ‚ÄÉ‚ÄÉNumbers from the standard normal distribution\ntorch.randperm: ‚ÄÉ¬†Random permutation of integers\ntorch.empty: ‚ÄÉ‚ÄÉ‚ÄÉUninitialized tensor\ntorch.zeros: ‚ÄÉ‚ÄÉ‚ÄÉTensor filled with 0\ntorch.ones: ‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÇ¬†Tensor filled with 1\ntorch.eye: ‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÇIdentity matrix"
  },
  {
    "objectID": "ml/torchtensors_slides.html#creating-tensors-1",
    "href": "ml/torchtensors_slides.html#creating-tensors-1",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Creating tensors",
    "text": "Creating tensors\n\ntorch.manual_seed(0)  # If you want to reproduce the result\ntorch.rand(1)\n\ntorch.manual_seed(0)  # Run before each operation to get the same result\ntorch.rand(1).item()  # Extract the value from a tensor\n\n[Out]\n\ntensor([0.4963])\n\n0.49625658988952637"
  },
  {
    "objectID": "ml/torchtensors_slides.html#creating-tensors-2",
    "href": "ml/torchtensors_slides.html#creating-tensors-2",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Creating tensors",
    "text": "Creating tensors\n\ntorch.rand(1)\ntorch.rand(1, 1)\ntorch.rand(1, 1, 1)\ntorch.rand(1, 1, 1, 1)\n\n[Out]\n\ntensor([0.6984])\ntensor([[0.5675]])\ntensor([[[0.8352]]])\ntensor([[[[0.2056]]]])"
  },
  {
    "objectID": "ml/torchtensors_slides.html#creating-tensors-3",
    "href": "ml/torchtensors_slides.html#creating-tensors-3",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Creating tensors",
    "text": "Creating tensors\n\ntorch.rand(2)\ntorch.rand(2, 2, 2, 2)\n\n[Out]\n\ntensor([0.5932, 0.1123])\ntensor([[[[0.1147, 0.3168],\n          [0.6965, 0.9143]],\n         [[0.9351, 0.9412],\n          [0.5995, 0.0652]]],\n        [[[0.5460, 0.1872],\n          [0.0340, 0.9442]],\n         [[0.8802, 0.0012],\n          [0.5936, 0.4158]]]])"
  },
  {
    "objectID": "ml/torchtensors_slides.html#creating-tensors-4",
    "href": "ml/torchtensors_slides.html#creating-tensors-4",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Creating tensors",
    "text": "Creating tensors\n\ntorch.rand(2)\ntorch.rand(3)\ntorch.rand(1, 1)\ntorch.rand(1, 1, 1)\ntorch.rand(2, 6)\n\n[Out]\n\ntensor([0.7682, 0.0885])\ntensor([0.1320, 0.3074, 0.6341])\ntensor([[0.4901]])\ntensor([[[0.8964]]])\ntensor([[0.4556, 0.6323, 0.3489, 0.4017, 0.0223, 0.1689],\n        [0.2939, 0.5185, 0.6977, 0.8000, 0.1610, 0.2823]])"
  },
  {
    "objectID": "ml/torchtensors_slides.html#creating-tensors-5",
    "href": "ml/torchtensors_slides.html#creating-tensors-5",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Creating tensors",
    "text": "Creating tensors\n\ntorch.rand(2, 4, dtype=torch.float64)  # You can set dtype\ntorch.ones(2, 1, 4, 5)\n\n[Out]\n\ntensor([[0.6650, 0.7849, 0.2104, 0.6767],\n        [0.1097, 0.5238, 0.2260, 0.5582]], dtype=torch.float64)\ntensor([[[[1., 1., 1., 1., 1.],\n          [1., 1., 1., 1., 1.],\n          [1., 1., 1., 1., 1.],\n          [1., 1., 1., 1., 1.]]],\n        [[[1., 1., 1., 1., 1.],\n          [1., 1., 1., 1., 1.],\n          [1., 1., 1., 1., 1.],\n          [1., 1., 1., 1., 1.]]]])"
  },
  {
    "objectID": "ml/torchtensors_slides.html#creating-tensors-6",
    "href": "ml/torchtensors_slides.html#creating-tensors-6",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Creating tensors",
    "text": "Creating tensors\n\nt = torch.rand(2, 3); print(t)\ntorch.zeros_like(t)             # Matches the size of t\ntorch.ones_like(t)\ntorch.randn_like(t)\n\n[Out]\n\ntensor([[0.4051, 0.6394, 0.0871],\n        [0.4509, 0.5255, 0.5057]])\ntensor([[0., 0., 0.],\n        [0., 0., 0.]])\ntensor([[1., 1., 1.],\n        [1., 1., 1.]])\ntensor([[-0.3088, -0.0104,  1.0461],\n        [ 0.9233,  0.0236, -2.1217]])"
  },
  {
    "objectID": "ml/torchtensors_slides.html#creating-tensors-7",
    "href": "ml/torchtensors_slides.html#creating-tensors-7",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Creating tensors",
    "text": "Creating tensors\n\ntorch.arange(2, 10, 4)    # From 2 to 10 in increments of 4\ntorch.linspace(2, 10, 4)  # 4 elements from 2 to 10 on the linear scale\ntorch.logspace(2, 10, 4)  # Same on the log scale\ntorch.randperm(4)\ntorch.eye(3)\n\n[Out]\n\ntensor([2, 6])\ntensor([2.0000,  4.6667,  7.3333, 10.0000])\ntensor([1.0000e+02, 4.6416e+04, 2.1544e+07, 1.0000e+10])\ntensor([1, 3, 2, 0])\ntensor([[1., 0., 0.],\n        [0., 1., 0.],\n        [0., 0., 1.]])"
  },
  {
    "objectID": "ml/torchtensors_slides.html#tensor-information",
    "href": "ml/torchtensors_slides.html#tensor-information",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Tensor information",
    "text": "Tensor information\n\nt = torch.rand(2, 3); print(t)\nt.size()\nt.dim()\nt.numel()\n\n[Out]\n\ntensor([[0.5885, 0.7005, 0.1048],\n        [0.1115, 0.7526, 0.0658]])\ntorch.Size([2, 3])\n2\n6"
  },
  {
    "objectID": "ml/torchtensors_slides.html#tensor-indexing",
    "href": "ml/torchtensors_slides.html#tensor-indexing",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Tensor indexing",
    "text": "Tensor indexing\n\nx = torch.rand(3, 4)\nx[:]                 # With a range, the comma is implicit: same as x[:, ]\nx[:, 2]\nx[1, :]\nx[2, 3]\n\n[Out]\n\ntensor([[0.6575, 0.4017, 0.7391, 0.6268],\n        [0.2835, 0.0993, 0.7707, 0.1996],\n        [0.4447, 0.5684, 0.2090, 0.7724]])\ntensor([0.7391, 0.7707, 0.2090])\ntensor([0.2835, 0.0993, 0.7707, 0.1996])\ntensor(0.7724)"
  },
  {
    "objectID": "ml/torchtensors_slides.html#tensor-indexing-1",
    "href": "ml/torchtensors_slides.html#tensor-indexing-1",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Tensor indexing",
    "text": "Tensor indexing\n\nx[-1:]        # Last element (implicit comma, so all columns)\nx[-1]         # No range, no implicit comma: we are indexing \n# from a list of tensors, so the result is a one dimensional tensor\n# (Each dimension is a list of tensors of the previous dimension)\nx[-1].size()  # Same number of dimensions than x (2 dimensions)\nx[-1:].size() # We dropped one dimension\n\n[Out]\n\ntensor([[0.8168, 0.0879, 0.2642, 0.3777]])\ntensor([0.8168, 0.0879, 0.2642, 0.3777])\n\ntorch.Size([4])\ntorch.Size([1, 4])"
  },
  {
    "objectID": "ml/torchtensors_slides.html#tensor-indexing-2",
    "href": "ml/torchtensors_slides.html#tensor-indexing-2",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Tensor indexing",
    "text": "Tensor indexing\n\nx[0:1]     # Python ranges are inclusive to the left, not the right\nx[:-1]     # From start to one before last (& implicit comma)\nx[0:3:2]   # From 0th (included) to 3rd (excluded) in increment of 2\n\n[Out]\n\ntensor([[0.5873, 0.0225, 0.7234, 0.4538]])\ntensor([[0.5873, 0.0225, 0.7234, 0.4538],\n        [0.9525, 0.0111, 0.6421, 0.4647]])\ntensor([[0.5873, 0.0225, 0.7234, 0.4538],\n        [0.8168, 0.0879, 0.2642, 0.3777]])"
  },
  {
    "objectID": "ml/torchtensors_slides.html#tensor-indexing-3",
    "href": "ml/torchtensors_slides.html#tensor-indexing-3",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Tensor indexing",
    "text": "Tensor indexing\n\nx[None]          # Adds a dimension of size one as the 1st dimension\nx.size()\nx[None].size()\n\n[Out]\n\ntensor([[[0.5873, 0.0225, 0.7234, 0.4538],\n         [0.9525, 0.0111, 0.6421, 0.4647],\n         [0.8168, 0.0879, 0.2642, 0.3777]]])\ntorch.Size([3, 4])\ntorch.Size([1, 3, 4])"
  },
  {
    "objectID": "ml/torchtensors_slides.html#a-word-of-caution-about-indexing",
    "href": "ml/torchtensors_slides.html#a-word-of-caution-about-indexing",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "A word of caution about indexing",
    "text": "A word of caution about indexing\n\nWhile indexing elements of a tensor to extract some of the data as a final step of some computation is fine, you should not use indexing to run operations on tensor elements in a loop as this would be extremely inefficient\nInstead, you want to use vectorized operations"
  },
  {
    "objectID": "ml/torchtensors_slides.html#vectorized-operations",
    "href": "ml/torchtensors_slides.html#vectorized-operations",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Vectorized operations",
    "text": "Vectorized operations\n\nSince PyTorch tensors are homogeneous (i.e.¬†made of a single data type), as with NumPy‚Äôs ndarrays, operations are vectorized & thus staggeringly fast\nNumPy is mostly written in C & PyTorch in C++. With either library, when you run vectorized operations on arrays/tensors, you don‚Äôt use raw Python (slow) but compiled C/C++ code (much faster)\nHere is an excellent post explaining Python vectorization & why it makes such a big difference"
  },
  {
    "objectID": "ml/torchtensors_slides.html#vectorized-operations-comparison",
    "href": "ml/torchtensors_slides.html#vectorized-operations-comparison",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Vectorized operations: comparison",
    "text": "Vectorized operations: comparison\nRaw Python method\n# Create tensor. We use float64 here to avoid truncation errors\nt = torch.rand(10**6, dtype=torch.float64)\n# Initialize the sum\nsum = 0\n# Run loop\nfor i in range(len(t)): sum += t[i]\n# Print result\nprint(sum)\nVectorized function\nt.sum()"
  },
  {
    "objectID": "ml/torchtensors_slides.html#vectorized-operations-comparison-1",
    "href": "ml/torchtensors_slides.html#vectorized-operations-comparison-1",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Vectorized operations: comparison",
    "text": "Vectorized operations: comparison\nBoth methods give the same result\n\nThis is why we used float64: While the accuracy remains excellent with float32 if we use the PyTorch function torch.sum(), the raw Python loop gives a fairly inaccurate result\n\n\n[Out]\n\ntensor(500023.0789, dtype=torch.float64)\n\ntensor(500023.0789, dtype=torch.float64)"
  },
  {
    "objectID": "ml/torchtensors_slides.html#vectorized-operations-timing",
    "href": "ml/torchtensors_slides.html#vectorized-operations-timing",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Vectorized operations: timing",
    "text": "Vectorized operations: timing\nLet‚Äôs compare the timing with PyTorch built-in benchmark utility\n# Load utility\nimport torch.utils.benchmark as benchmark\n\n# Create a function for our loop\ndef sum_loop(t, sum):\n    for i in range(len(t)): sum += t[i]"
  },
  {
    "objectID": "ml/torchtensors_slides.html#vectorized-operations-timing-1",
    "href": "ml/torchtensors_slides.html#vectorized-operations-timing-1",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Vectorized operations: timing",
    "text": "Vectorized operations: timing\nNow we can create the timers\nt0 = benchmark.Timer(\n    stmt='sum_loop(t, sum)',\n    setup='from __main__ import sum_loop',\n    globals={'t': t, 'sum': sum})\n\nt1 = benchmark.Timer(\n    stmt='t.sum()',\n    globals={'t': t})"
  },
  {
    "objectID": "ml/torchtensors_slides.html#vectorized-operations-timing-2",
    "href": "ml/torchtensors_slides.html#vectorized-operations-timing-2",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Vectorized operations: timing",
    "text": "Vectorized operations: timing\nLet‚Äôs time 100 runs to have a reliable benchmark\nprint(t0.timeit(100))\nprint(t1.timeit(100))\n\nI ran the code on my laptop with a dedicated GPU & 32GB RAM"
  },
  {
    "objectID": "ml/torchtensors_slides.html#vectorized-operations-timing-3",
    "href": "ml/torchtensors_slides.html#vectorized-operations-timing-3",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Vectorized operations: timing",
    "text": "Vectorized operations: timing\nTiming of raw Python loop\nsum_loop(t, sum)\nsetup: from __main__ import sum_loop\n  1.37 s\n  1 measurement, 100 runs , 1 thread\nTiming of vectorized function\nt.sum()\n  191.26 us\n  1 measurement, 100 runs , 1 thread"
  },
  {
    "objectID": "ml/torchtensors_slides.html#vectorized-operations-timing-4",
    "href": "ml/torchtensors_slides.html#vectorized-operations-timing-4",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Vectorized operations: timing",
    "text": "Vectorized operations: timing\nSpeedup:\n1.37/(191.26 * 10**-6) = 7163\n\nThe vectorized function runs more than 7,000 times faster!!!"
  },
  {
    "objectID": "ml/torchtensors_slides.html#even-more-important-on-gpus",
    "href": "ml/torchtensors_slides.html#even-more-important-on-gpus",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Even more important on GPUs",
    "text": "Even more important on GPUs\nWe will talk about GPUs in detail later\nTiming of raw Python loop on GPU (actually slower on GPU!)\nsum_loop(t, sum)\nsetup: from __main__ import sum_loop\n  4.54 s\n  1 measurement, 100 runs , 1 thread\nTiming of vectorized function on GPU (here we do get a speedup)\nt.sum()\n  50.62 us\n  1 measurement, 100 runs , 1 thread"
  },
  {
    "objectID": "ml/torchtensors_slides.html#even-more-important-on-gpus-1",
    "href": "ml/torchtensors_slides.html#even-more-important-on-gpus-1",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Even more important on GPUs",
    "text": "Even more important on GPUs\nSpeedup:\n4.54/(50.62 * 10**-6) = 89688\n\nOn GPUs, it is even more important not to index repeatedly from a tensor\n\n\nOn GPUs, the vectorized function runs almost 90,000 times faster!!!"
  },
  {
    "objectID": "ml/torchtensors_slides.html#simple-mathematical-operations",
    "href": "ml/torchtensors_slides.html#simple-mathematical-operations",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Simple mathematical operations",
    "text": "Simple mathematical operations\n\nt1 = torch.arange(1, 5).view(2, 2); print(t1)\nt2 = torch.tensor([[1, 1], [0, 0]]); print(t2)\nt1 + t2 # Operation performed between elements at corresponding locations\nt1 + 1  # Operation applied to each element of the tensor\n\n[Out]\n\ntensor([[1, 2],\n        [3, 4]])\ntensor([[1, 1],\n        [0, 0]])\ntensor([[2, 3],\n        [3, 4]])\ntensor([[2, 3],\n        [4, 5]])"
  },
  {
    "objectID": "ml/torchtensors_slides.html#reduction",
    "href": "ml/torchtensors_slides.html#reduction",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Reduction",
    "text": "Reduction\n\nt = torch.ones(2, 3, 4); print(t)\nt.sum()   # Reduction over all entries\n\n[Out]\n\ntensor([[[1., 1., 1., 1.],\n         [1., 1., 1., 1.],\n         [1., 1., 1., 1.]],\n        [[1., 1., 1., 1.],\n         [1., 1., 1., 1.],\n         [1., 1., 1., 1.]]])\ntensor(24.)\n\nOther reduction functions (e.g.¬†mean) behave the same way"
  },
  {
    "objectID": "ml/torchtensors_slides.html#reduction-1",
    "href": "ml/torchtensors_slides.html#reduction-1",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Reduction",
    "text": "Reduction\n\n# Reduction over a specific dimension\nt.sum(0)  \nt.sum(1)\nt.sum(2)\n\n[Out]\n\ntensor([[2., 2., 2., 2.],\n        [2., 2., 2., 2.],\n        [2., 2., 2., 2.]])\ntensor([[3., 3., 3., 3.],\n        [3., 3., 3., 3.]])\ntensor([[4., 4., 4.],\n        [4., 4., 4.]])"
  },
  {
    "objectID": "ml/torchtensors_slides.html#reduction-2",
    "href": "ml/torchtensors_slides.html#reduction-2",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Reduction",
    "text": "Reduction\n\n# Reduction over multiple dimensions\nt.sum((0, 1))\nt.sum((0, 2))\nt.sum((1, 2))\n\n[Out]\n\ntensor([6., 6., 6., 6.])\ntensor([8., 8., 8.])\ntensor([12., 12.])"
  },
  {
    "objectID": "ml/torchtensors_slides.html#in-place-operations",
    "href": "ml/torchtensors_slides.html#in-place-operations",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "In-place operations",
    "text": "In-place operations\nWith operators post-fixed with _:\nt1 = torch.tensor([1, 2]); print(t1)\nt2 = torch.tensor([1, 1]); print(t2)\nt1.add_(t2); print(t1)\nt1.zero_(); print(t1)\n\n[Out]\n\ntensor([1, 2])\ntensor([1, 1])\ntensor([2, 3])\ntensor([0, 0])"
  },
  {
    "objectID": "ml/torchtensors_slides.html#in-place-operations-vs-reassignments",
    "href": "ml/torchtensors_slides.html#in-place-operations-vs-reassignments",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "In-place operations vs reassignments",
    "text": "In-place operations vs reassignments\n\nt1 = torch.ones(1); t1, hex(id(t1))\nt1.add_(1); t1, hex(id(t1))        # In-place operation: same address\nt1 = t1.add(1); t1, hex(id(t1))    # Reassignment: new address in memory\nt1 = t1 + 1; t1, hex(id(t1))       # Reassignment: new address in memory\n\n[Out]\n\n(tensor([1.]), '0x7fc61accc3b0')\n(tensor([2.]), '0x7fc61accc3b0')\n(tensor([3.]), '0x7fc61accc5e0')\n(tensor([4.]), '0x7fc61accc6d0')"
  },
  {
    "objectID": "ml/torchtensors_slides.html#tensor-views",
    "href": "ml/torchtensors_slides.html#tensor-views",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Tensor views",
    "text": "Tensor views\n\nt = torch.tensor([[1, 2, 3], [4, 5, 6]]); print(t)\nt.size()\nt.view(6)\nt.view(3, 2)\nt.view(3, -1) # Same: with -1, the size is inferred from other dimensions\n\n[Out]\n\ntensor([[1, 2, 3],\n        [4, 5, 6]])\ntorch.Size([2, 3])\ntensor([1, 2, 3, 4, 5, 6])\ntensor([[1, 2],\n        [3, 4],\n        [5, 6]])"
  },
  {
    "objectID": "ml/torchtensors_slides.html#note-the-difference",
    "href": "ml/torchtensors_slides.html#note-the-difference",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Note the difference",
    "text": "Note the difference\n\nt1 = torch.tensor([[1, 2, 3], [4, 5, 6]]); print(t1)\nt2 = t1.t(); print(t2)\nt3 = t1.view(3, 2); print(t3)\n\n[Out]\n\ntensor([[1, 2, 3],\n        [4, 5, 6]])\ntensor([[1, 4],\n        [2, 5],\n        [3, 6]])\ntensor([[1, 2],\n        [3, 4],\n        [5, 6]])"
  },
  {
    "objectID": "ml/torchtensors_slides.html#logical-operations",
    "href": "ml/torchtensors_slides.html#logical-operations",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Logical operations",
    "text": "Logical operations\n\nt1 = torch.randperm(5); print(t1)\nt2 = torch.randperm(5); print(t2)\nt1 > 3                            # Test each element\nt1 < t2                           # Test corresponding pairs of elements\n\n[Out]\n\ntensor([4, 1, 0, 2, 3])\ntensor([0, 4, 2, 1, 3])\ntensor([ True, False, False, False, False])\ntensor([False,  True,  True, False, False])"
  },
  {
    "objectID": "ml/torchtensors_slides.html#outline-5",
    "href": "ml/torchtensors_slides.html#outline-5",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Outline",
    "text": "Outline\n\nWhat is a PyTorch tensor?\nMemory storage\nData type (dtype)\nBasic operations\nWorking with NumPy\nLinear algebra\nHarvesting the power of GPUs\nDistributed operations"
  },
  {
    "objectID": "ml/torchtensors_slides.html#conversion-without-copy",
    "href": "ml/torchtensors_slides.html#conversion-without-copy",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Conversion without copy",
    "text": "Conversion without copy\nPyTorch tensors can be converted to NumPy ndarrays & vice-versa in a very efficient manner as both objects share the same memory\nt = torch.rand(2, 3); print(t)\nt_np = t.numpy(); print(t_np)      # From PyTorch tensor to NumPy ndarray\n\n[Out]\n\ntensor([[0.8434, 0.0876, 0.7507],\n        [0.1457, 0.3638, 0.0563]])   # PyTorch Tensor\n\n[[0.84344184 0.08764815 0.7506627 ]\n [0.14567494 0.36384273 0.05629885]] # NumPy ndarray"
  },
  {
    "objectID": "ml/torchtensors_slides.html#mind-the-different-defaults",
    "href": "ml/torchtensors_slides.html#mind-the-different-defaults",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Mind the different defaults",
    "text": "Mind the different defaults\n\nt_np.dtype\n\n[Out]\n\ndtype('float32')\n\nRemember that PyTorch tensors use 32-bit floating points by default  (because this is what you want in neural networks) \n\n\nBut NumPy defaults to 64-bit  Depending on your workflow, you might have to change dtype"
  },
  {
    "objectID": "ml/torchtensors_slides.html#from-numpy-to-pytorch",
    "href": "ml/torchtensors_slides.html#from-numpy-to-pytorch",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "From NumPy to PyTorch",
    "text": "From NumPy to PyTorch\n\nimport numpy as np\na = np.random.rand(2, 3); print(a)\na_pt = torch.from_numpy(a); print(a_pt)    # From ndarray to tensor\n\n[Out]\n\n[[0.55892276 0.06026952 0.72496545]\n [0.65659463 0.27697739 0.29141587]]\n\ntensor([[0.5589, 0.0603, 0.7250],\n        [0.6566, 0.2770, 0.2914]], dtype=torch.float64)\n\nHere again, you might have to change dtype"
  },
  {
    "objectID": "ml/torchtensors_slides.html#notes-about-conversion-without-copy",
    "href": "ml/torchtensors_slides.html#notes-about-conversion-without-copy",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Notes about conversion without copy",
    "text": "Notes about conversion without copy\nt & t_np are objects of different Python types, so, as far as Python is concerned, they have different addresses\nid(t) == id(t_np)\n\n[Out]\n\nFalse"
  },
  {
    "objectID": "ml/torchtensors_slides.html#notes-about-conversion-without-copy-1",
    "href": "ml/torchtensors_slides.html#notes-about-conversion-without-copy-1",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Notes about conversion without copy",
    "text": "Notes about conversion without copy\nHowever‚Äîthat‚Äôs quite confusing‚Äîthey share an underlying C array in memory & modifying one in-place also modifies the other\nt.zero_()\nprint(t_np)\n\n[Out]\n\ntensor([[0., 0., 0.],\n        [0., 0., 0.]])\n\n[[0. 0. 0.]\n [0. 0. 0.]]"
  },
  {
    "objectID": "ml/torchtensors_slides.html#notes-about-conversion-without-copy-2",
    "href": "ml/torchtensors_slides.html#notes-about-conversion-without-copy-2",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Notes about conversion without copy",
    "text": "Notes about conversion without copy\nLastly, as NumPy only works on CPU, to convert a PyTorch tensor allocated to the GPU, the content will have to be copied to the CPU first"
  },
  {
    "objectID": "ml/torchtensors_slides.html#outline-6",
    "href": "ml/torchtensors_slides.html#outline-6",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Outline",
    "text": "Outline\n\nWhat is a PyTorch tensor?\nMemory storage\nData type (dtype)\nBasic operations\nWorking with NumPy\nLinear algebra\nHarvesting the power of GPUs\nDistributed operations"
  },
  {
    "objectID": "ml/torchtensors_slides.html#torch.linalg-module",
    "href": "ml/torchtensors_slides.html#torch.linalg-module",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "torch.linalg module",
    "text": "torch.linalg module\n\nAll functions from numpy.linalg implemented (with accelerator & automatic differentiation support)\nSome additional functions\n\n\nRequires torch >= 1.9  Linear algebra support was less developed before the introduction of this module"
  },
  {
    "objectID": "ml/torchtensors_slides.html#system-of-linear-equations-solver",
    "href": "ml/torchtensors_slides.html#system-of-linear-equations-solver",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "System of linear equations solver",
    "text": "System of linear equations solver\nLet‚Äôs have a look at an extremely basic example:\n2x + 3y - z = 5\nx - 2y + 8z = 21\n6x + y - 3z = -1\nWe are looking for the values of x, y, & z that would satisfy this system"
  },
  {
    "objectID": "ml/torchtensors_slides.html#system-of-linear-equations-solver-1",
    "href": "ml/torchtensors_slides.html#system-of-linear-equations-solver-1",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "System of linear equations solver",
    "text": "System of linear equations solver\nWe create a 2D tensor A of size (3, 3) with the coefficients of the equations  & a 1D tensor b of size 3 with the right hand sides values of the equations\nA = torch.tensor([[2., 3., -1.], [1., -2., 8.], [6., 1., -3.]]); print(A)\nb = torch.tensor([5., 21., -1.]); print(b)\n\n[Out]\n\ntensor([[ 2.,  3., -1.],\n        [ 1., -2.,  8.],\n        [ 6.,  1., -3.]])\ntensor([ 5., 21., -1.])"
  },
  {
    "objectID": "ml/torchtensors_slides.html#system-of-linear-equations-solver-2",
    "href": "ml/torchtensors_slides.html#system-of-linear-equations-solver-2",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "System of linear equations solver",
    "text": "System of linear equations solver\nSolving this system is as simple as running the torch.linalg.solve function:\nx = torch.linalg.solve(A, b); print(x)\n\n[Out]\n\ntensor([1., 2., 3.])\nOur solution is:\nx = 1\ny = 2\nz = 3"
  },
  {
    "objectID": "ml/torchtensors_slides.html#verify-our-result",
    "href": "ml/torchtensors_slides.html#verify-our-result",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Verify our result",
    "text": "Verify our result\n\ntorch.allclose(A @ x, b)\n\n[Out]\n\nTrue"
  },
  {
    "objectID": "ml/torchtensors_slides.html#system-of-linear-equations-solver-3",
    "href": "ml/torchtensors_slides.html#system-of-linear-equations-solver-3",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "System of linear equations solver",
    "text": "System of linear equations solver\nHere is another simple example:\n# Create a square normal random matrix\nA = torch.randn(4, 4); print(A)\n# Create a tensor of right hand side values\nb = torch.randn(4); print(b)\n\n# Solve the system\nx = torch.linalg.solve(A, b); print(x)\n\n# Verify\ntorch.allclose(A @ x, b)"
  },
  {
    "objectID": "ml/torchtensors_slides.html#system-of-linear-equations-solver-4",
    "href": "ml/torchtensors_slides.html#system-of-linear-equations-solver-4",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "System of linear equations solver",
    "text": "System of linear equations solver\n\n[Out]\n\ntensor([[ 1.5091,  2.0820,  1.7067,  2.3804], # A (coefficients)\n        [-1.1256, -0.3170, -1.0925, -0.0852],\n        [ 0.3276, -0.7607, -1.5991,  0.0185],\n        [-0.7504,  0.1854,  0.6211,  0.6382]])\n\ntensor([-1.0886, -0.2666,  0.1894, -0.2190])  # b (right hand side values)\n\ntensor([ 0.1992, -0.7011,  0.2541, -0.1526])  # x (our solution)\n\nTrue                                          # Verification"
  },
  {
    "objectID": "ml/torchtensors_slides.html#with-2-multidimensional-tensors",
    "href": "ml/torchtensors_slides.html#with-2-multidimensional-tensors",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "With 2 multidimensional tensors",
    "text": "With 2 multidimensional tensors\n\nA = torch.randn(2, 3, 3)              # Must be batches of square matrices\nB = torch.randn(2, 3, 5)              # Dimensions must be compatible\nX = torch.linalg.solve(A, B); print(X)\ntorch.allclose(A @ X, B)\n\n[Out]\n\ntensor([[[-0.0545, -0.1012,  0.7863, -0.0806, -0.0191],\n         [-0.9846, -0.0137, -1.7521, -0.4579, -0.8178],\n         [-1.9142, -0.6225, -1.9239, -0.6972,  0.7011]],\n        [[ 3.2094,  0.3432, -1.6604, -0.7885,  0.0088],\n         [ 7.9852,  1.4605, -1.7037, -0.7713,  2.7319],\n         [-4.1979,  0.0849,  1.0864,  0.3098, -1.0347]]])\nTrue"
  },
  {
    "objectID": "ml/torchtensors_slides.html#matrix-inversions",
    "href": "ml/torchtensors_slides.html#matrix-inversions",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Matrix inversions",
    "text": "Matrix inversions\n\n\nIt is faster & more numerically stable to solve a system of linear equations directly than to compute the inverse matrix first\n\n\n\nLimit matrix inversions to situations where it is truly necessary"
  },
  {
    "objectID": "ml/torchtensors_slides.html#matrix-inversions-1",
    "href": "ml/torchtensors_slides.html#matrix-inversions-1",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Matrix inversions",
    "text": "Matrix inversions\n\nA = torch.rand(2, 3, 3)      # Batch of square matrices\nA_inv = torch.linalg.inv(A)  # Batch of inverse matrices\nA @ A_inv                    # Batch of identity matrices\n\n[Out]\n\ntensor([[[ 1.0000e+00, -6.0486e-07,  1.3859e-06],\n         [ 5.5627e-08,  1.0000e+00,  1.0795e-06],\n         [-1.4133e-07,  7.9992e-08,  1.0000e+00]],\n        [[ 1.0000e+00,  4.3329e-08, -3.6741e-09],\n         [-7.4627e-08,  1.0000e+00,  1.4579e-07],\n         [-6.3580e-08,  8.2354e-08,  1.0000e+00]]])"
  },
  {
    "objectID": "ml/torchtensors_slides.html#other-linear-algebra-functions",
    "href": "ml/torchtensors_slides.html#other-linear-algebra-functions",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Other linear algebra functions",
    "text": "Other linear algebra functions\ntorch.linalg contains many more functions:\n\ntorch.tensordot which generalizes matrix products\ntorch.linalg.tensorsolve which computes the solution X to the system torch.tensordot(A, X) = B\ntorch.linalg.eigvals which computes the eigenvalues of a square matrix\nand more"
  },
  {
    "objectID": "ml/torchtensors_slides.html#outline-7",
    "href": "ml/torchtensors_slides.html#outline-7",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Outline",
    "text": "Outline\n\nWhat is a PyTorch tensor?\nMemory storage\nData type (dtype)\nBasic operations\nWorking with NumPy\nLinear algebra\nHarvesting the power of GPUs\nDistributed operations"
  },
  {
    "objectID": "ml/torchtensors_slides.html#device-attribute",
    "href": "ml/torchtensors_slides.html#device-attribute",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Device attribute",
    "text": "Device attribute\nTensor data can be placed in the memory of various processor types:\n\nthe RAM of CPU\nthe RAM of a GPU with CUDA support\nthe RAM of a GPU with AMD‚Äôs ROCm support\nthe RAM of an XLA device (e.g.¬†Cloud TPU) with the torch_xla package"
  },
  {
    "objectID": "ml/torchtensors_slides.html#device-attribute-1",
    "href": "ml/torchtensors_slides.html#device-attribute-1",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Device attribute",
    "text": "Device attribute\nThe values for the device attributes are:\n\nCPU: ¬†'cpu'\nGPU (CUDA & AMD‚Äôs ROCm): ¬†'cuda'\nXLA: ¬†xm.xla_device()\n\nThis last option requires to load the torch_xla package first:\nimport torch_xla\nimport torch_xla.core.xla_model as xm"
  },
  {
    "objectID": "ml/torchtensors_slides.html#creating-a-tensor-on-a-specific-device",
    "href": "ml/torchtensors_slides.html#creating-a-tensor-on-a-specific-device",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Creating a tensor on a specific device",
    "text": "Creating a tensor on a specific device\nBy default, tensors are created on the CPU\nt1 = torch.rand(2); print(t1)\n\n[Out]\n\ntensor([0.1606, 0.9771])  # Implicit: device='cpu'\n\nPrinted tensors only display attributes with values ‚â† default values"
  },
  {
    "objectID": "ml/torchtensors_slides.html#creating-a-tensor-on-a-specific-device-1",
    "href": "ml/torchtensors_slides.html#creating-a-tensor-on-a-specific-device-1",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Creating a tensor on a specific device",
    "text": "Creating a tensor on a specific device\nYou can create a tensor on an accelerator by specifying the device attribute\nt2_gpu = torch.rand(2, device='cuda'); print(t2_gpu)\n\n[Out]\n\ntensor([0.0664, 0.7829], device='cuda:0')  # :0 means the 1st GPU"
  },
  {
    "objectID": "ml/torchtensors_slides.html#copying-a-tensor-to-a-specific-device",
    "href": "ml/torchtensors_slides.html#copying-a-tensor-to-a-specific-device",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Copying a tensor to a specific device",
    "text": "Copying a tensor to a specific device\nYou can also make copies of a tensor on other devices\n# Make a copy of t1 on the GPU\nt1_gpu = t1.to(device='cuda'); print(t1_gpu)\nt1_gpu = t1.cuda()  # Same as above written differently\n\n# Make a copy of t2_gpu on the CPU\nt2 = t2_gpu.to(device='cpu'); print(t2)\nt2 = t2_gpu.cpu()   # For the altenative form\n\n[Out]\n\ntensor([0.1606, 0.9771], device='cuda:0')\ntensor([0.0664, 0.7829]) # Implicit: device='cpu'"
  },
  {
    "objectID": "ml/torchtensors_slides.html#multiple-gpus",
    "href": "ml/torchtensors_slides.html#multiple-gpus",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Multiple GPUs",
    "text": "Multiple GPUs\nIf you have multiple GPUs, you can optionally specify which one a tensor should be created on or copied to\nt3_gpu = torch.rand(2, device='cuda:0')  # Create a tensor on 1st GPU\nt4_gpu = t1.to(device='cuda:0')          # Make a copy of t1 on 1st GPU\nt5_gpu = t1.to(device='cuda:1')          # Make a copy of t1 on 2nd GPU\n\nOr the equivalent short forms for the last two:\nt4_gpu = t1.cuda(0)\nt5_gpu = t1.cuda(1)"
  },
  {
    "objectID": "ml/torchtensors_slides.html#timing",
    "href": "ml/torchtensors_slides.html#timing",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Timing",
    "text": "Timing\nLet‚Äôs compare the timing of some matrix multiplications on CPU & GPU with PyTorch built-in benchmark utility\n# Load utility\nimport torch.utils.benchmark as benchmark\n# Define tensors on the CPU\nA = torch.randn(500, 500)\nB = torch.randn(500, 500)\n# Define tensors on the GPU\nA_gpu = torch.randn(500, 500, device='cuda')\nB_gpu = torch.randn(500, 500, device='cuda')\n\nI ran the code on my laptop with a dedicated GPU & 32GB RAM"
  },
  {
    "objectID": "ml/torchtensors_slides.html#timing-1",
    "href": "ml/torchtensors_slides.html#timing-1",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Timing",
    "text": "Timing\nLet‚Äôs time 100 runs to have a reliable benchmark\nt0 = benchmark.Timer(\n    stmt='A @ B',\n    globals={'A': A, 'B': B})\n\nt1 = benchmark.Timer(\n    stmt='A_gpu @ B_gpu',\n    globals={'A_gpu': A_gpu, 'B_gpu': B_gpu})\n\nprint(t0.timeit(100))\nprint(t1.timeit(100))"
  },
  {
    "objectID": "ml/torchtensors_slides.html#timing-2",
    "href": "ml/torchtensors_slides.html#timing-2",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Timing",
    "text": "Timing\n\n[Out]\n\nA @ B\n  2.29 ms\n  1 measurement, 100 runs , 1 thread\n\nA_gpu @ B_gpu\n  108.02 us\n  1 measurement, 100 runs , 1 thread\nSpeedup:\n(2.29 * 10**-3)/(108.02 * 10**-6) = 21\nThis computation was 21 times faster on my GPU than on CPU"
  },
  {
    "objectID": "ml/torchtensors_slides.html#timing-3",
    "href": "ml/torchtensors_slides.html#timing-3",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Timing",
    "text": "Timing\nBy replacing 500 with 5000, we get:\nA @ B\n  2.21 s\n  1 measurement, 100 runs , 1 thread\n\nA_gpu @ B_gpu\n  57.88 ms\n  1 measurement, 100 runs , 1 thread\nSpeedup:\n2.21/(57.88 * 10**-3) = 38\nThe larger the computation, the greater the benefit: now 38 times faster"
  },
  {
    "objectID": "ml/torchtensors_slides.html#outline-8",
    "href": "ml/torchtensors_slides.html#outline-8",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Outline",
    "text": "Outline\n\nWhat is a PyTorch tensor?\nMemory storage\nData type (dtype)\nBasic operations\nWorking with NumPy\nLinear algebra\nHarvesting the power of GPUs\nDistributed operations"
  },
  {
    "objectID": "ml/torchtensors_slides.html#parallel-tensor-operations",
    "href": "ml/torchtensors_slides.html#parallel-tensor-operations",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Parallel tensor operations",
    "text": "Parallel tensor operations\nPyTorch already allows for distributed training of ML models\nThe implementation of distributed tensor operations‚Äîfor instance for linear algebra‚Äîis in the work through the use of a ShardedTensor primitive that can be sharded across nodes\nSee also this issue for more comments about upcoming developments on (among other things) tensor sharding"
  },
  {
    "objectID": "ml/upscaling.html",
    "href": "ml/upscaling.html",
    "title": "Image upscaling",
    "section": "",
    "text": "Super-resolution‚Äîthe process of (re)creating high resolution images from low resolution ones‚Äîis an old field, but deep neural networks have seen a sudden surge of new and very impressive methods over the past 10 years, from SRCCN to SRGAN to Transformers.\nIn this webinar, I will give a quick overview of these methods and show how the latest state-of-the-art model‚ÄîSwinIR‚Äîperforms on a few test images. We will use PyTorch as our framework.\n\nSlides (click and wait: my reveal.js presentations are heavy and take some time to load‚Ä¶)"
  },
  {
    "objectID": "ml/upscaling_slides.html#can-be-broken-down-into-2-main-periods",
    "href": "ml/upscaling_slides.html#can-be-broken-down-into-2-main-periods",
    "title": "Super-resolution with PyTorch",
    "section": "Can be broken down into 2 main periods:",
    "text": "Can be broken down into 2 main periods:\n\n\nA rather slow history with various interpolation algorithms of increasing complexity before deep neural networks\nAn incredibly fast evolution since the advent of deep learning (DL)"
  },
  {
    "objectID": "ml/upscaling_slides.html#sr-history-pre-dl",
    "href": "ml/upscaling_slides.html#sr-history-pre-dl",
    "title": "Super-resolution with PyTorch",
    "section": "SR history Pre-DL",
    "text": "SR history Pre-DL\n\nPixel-wise interpolation prior to DL\nVarious methods ranging from simple (e.g.¬†nearest-neighbour, bicubic) to complex (e.g.¬†Gaussian process regression, iterative FIR Wiener filter) algorithms"
  },
  {
    "objectID": "ml/upscaling_slides.html#sr-history-pre-dl-1",
    "href": "ml/upscaling_slides.html#sr-history-pre-dl-1",
    "title": "Super-resolution with PyTorch",
    "section": "SR history Pre-DL",
    "text": "SR history Pre-DL\n\nNearest-neighbour interpolation\nSimplest method of interpolation\nSimply uses the value of the nearest pixel\nBicubic interpolation\nConsists of determining the 16 coefficients \\(a_{ij}\\) in:\n\\[p(x, y) = \\sum_{i=0}^3\\sum_{i=0}^3 a\\_{ij} x^i y^j\\]"
  },
  {
    "objectID": "ml/upscaling_slides.html#sr-history-with-dl",
    "href": "ml/upscaling_slides.html#sr-history-with-dl",
    "title": "Super-resolution with PyTorch",
    "section": "SR history with DL",
    "text": "SR history with DL\n\nDeep learning has seen a fast evolution marked by the successive emergence of various frameworks and architectures over the past 10 years\nSome key network architectures and frameworks:\n\nCNN\nGAN\nTransformers\n\nThese have all been applied to SR"
  },
  {
    "objectID": "ml/upscaling_slides.html#srcnn",
    "href": "ml/upscaling_slides.html#srcnn",
    "title": "Super-resolution with PyTorch",
    "section": "SRCNN",
    "text": "SRCNN\n\n\nDong, C., Loy, C. C., He, K., & Tang, X. (2015). Image super-resolution using deep convolutional networks. IEEE transactions on pattern analysis and machine intelligence, 38(2), 295-307\n\n\nGiven a low-resolution image Y, the first convolutional layer of the SRCNN extracts a set of feature maps. The second layer maps these feature maps nonlinearly to high-resolution patch representations. The last layer combines the predictions within a spatial neighbourhood to produce the final high-resolution image F(Y)"
  },
  {
    "objectID": "ml/upscaling_slides.html#srcnn-1",
    "href": "ml/upscaling_slides.html#srcnn-1",
    "title": "Super-resolution with PyTorch",
    "section": "SRCNN",
    "text": "SRCNN\nCan use sparse-coding-based methods\n\n\nDong, C., Loy, C. C., He, K., & Tang, X. (2015). Image super-resolution using deep convolutional networks. IEEE transactions on pattern analysis and machine intelligence, 38(2), 295-307"
  },
  {
    "objectID": "ml/upscaling_slides.html#srgan",
    "href": "ml/upscaling_slides.html#srgan",
    "title": "Super-resolution with PyTorch",
    "section": "SRGAN",
    "text": "SRGAN\n\nDo not provide the best PSNR, but can give more realistic results by providing more texture (less smoothing)"
  },
  {
    "objectID": "ml/upscaling_slides.html#gan",
    "href": "ml/upscaling_slides.html#gan",
    "title": "Super-resolution with PyTorch",
    "section": "GAN",
    "text": "GAN\n\n\nStevens E., Antiga L., & Viehmann T. (2020). Deep Learning with PyTorch"
  },
  {
    "objectID": "ml/upscaling_slides.html#srgan-1",
    "href": "ml/upscaling_slides.html#srgan-1",
    "title": "Super-resolution with PyTorch",
    "section": "SRGAN",
    "text": "SRGAN\n\n\nLedig, C., Theis, L., Husz√°r, F., Caballero, J., Cunningham, A., Acosta, A., ‚Ä¶ & Shi, W. (2017). Photo-realistic single image super-resolution using a generative adversarial network. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp.¬†4681-4690)"
  },
  {
    "objectID": "ml/upscaling_slides.html#srgan-2",
    "href": "ml/upscaling_slides.html#srgan-2",
    "title": "Super-resolution with PyTorch",
    "section": "SRGAN",
    "text": "SRGAN\n\nFollowed by the ESRGAN and many other flavours of SRGANs"
  },
  {
    "objectID": "ml/upscaling_slides.html#attention",
    "href": "ml/upscaling_slides.html#attention",
    "title": "Super-resolution with PyTorch",
    "section": "Attention",
    "text": "Attention\n\n\nMnih, V., Heess, N., & Graves, A. (2014). Recurrent models of visual attention. In Advances in neural information processing systems (pp.¬†2204-2212)\n\n(cited 2769 times)\n\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ‚Ä¶ & Polosukhin, I. (2017). Attention is all you need. In Advances in neural information processing systems (pp.¬†5998-6008)\n\n(cited 30999 times‚Ä¶)"
  },
  {
    "objectID": "ml/upscaling_slides.html#transformers",
    "href": "ml/upscaling_slides.html#transformers",
    "title": "Super-resolution with PyTorch",
    "section": "Transformers",
    "text": "Transformers\n\n\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ‚Ä¶ & Polosukhin, I. (2017). Attention is all you need. In Advances in neural information processing systems (pp.¬†5998-6008)"
  },
  {
    "objectID": "ml/upscaling_slides.html#transformers-1",
    "href": "ml/upscaling_slides.html#transformers-1",
    "title": "Super-resolution with PyTorch",
    "section": "Transformers",
    "text": "Transformers\n\nInitially used for NLP to replace RNN as they allow parallelization Now entering the domain of vision and others Very performant with relatively few parameters"
  },
  {
    "objectID": "ml/upscaling_slides.html#swin-transformer",
    "href": "ml/upscaling_slides.html#swin-transformer",
    "title": "Super-resolution with PyTorch",
    "section": "Swin Transformer",
    "text": "Swin Transformer\n\nThe Swin Transformer improved the use of transformers to the vision domain\nSwin = Shifted WINdows"
  },
  {
    "objectID": "ml/upscaling_slides.html#swin-transformer-1",
    "href": "ml/upscaling_slides.html#swin-transformer-1",
    "title": "Super-resolution with PyTorch",
    "section": "Swin Transformer",
    "text": "Swin Transformer\nSwin transformer (left) vs transformer as initially applied to vision (right):\n\n\nLiu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., ‚Ä¶ & Guo, B. (2021). Swin transformer: Hierarchical vision transformer using shifted windows. arXiv preprint arXiv:2103.14030"
  },
  {
    "objectID": "ml/upscaling_slides.html#swinir-1",
    "href": "ml/upscaling_slides.html#swinir-1",
    "title": "Super-resolution with PyTorch",
    "section": "SwinIR",
    "text": "SwinIR\n\n\n\nLiang, J., Cao, J., Sun, G., Zhang, K., Van Gool, L., & Timofte, R. (2021). SwinIR: Image restoration using swin transformer. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp.¬†1833-1844)"
  },
  {
    "objectID": "ml/upscaling_slides.html#training-sets-used",
    "href": "ml/upscaling_slides.html#training-sets-used",
    "title": "Super-resolution with PyTorch",
    "section": "Training sets used",
    "text": "Training sets used\n\nDIV2K, Flickr2K, and other datasets"
  },
  {
    "objectID": "ml/upscaling_slides.html#models-assessment",
    "href": "ml/upscaling_slides.html#models-assessment",
    "title": "Super-resolution with PyTorch",
    "section": "Models assessment",
    "text": "Models assessment\n\n3 metrics commonly used:\nPeak sign-to-noise ratio (PSNR) measured in dB\n\\(\\frac{\\text{Maximum possible power of signal}}{\\text{Power of noise (calculated as the mean squared error)}}\\)\nCalculated at the pixel level\nStructural similarity index measure (SSIM)\nPrediction of perceived image quality based on a ‚Äúperfect‚Äù reference image\nMean opinion score (MOS)\nMean of subjective quality ratings"
  },
  {
    "objectID": "ml/upscaling_slides.html#models-assessment-1",
    "href": "ml/upscaling_slides.html#models-assessment-1",
    "title": "Super-resolution with PyTorch",
    "section": "Models assessment",
    "text": "Models assessment\n\nPeak sign-to-noise ratio (PSNR) measured in dB\n\\[PSNR = 10\\,\\cdot\\,log_{10}\\,\\left(\\frac{MAX_I^2}{MSE}\\right)\\]\nStructural similarity index measure (SSIM)\n\\[SSIM(x,y) = \\frac{(2\\mu_x\\mu_y + c_1) + (2 \\sigma _{xy} + c_2)}\n    {(\\mu_x^2 + \\mu_y^2+c_1) (\\sigma_x^2 + \\sigma_y^2+c_2)}\\]\nMean opinion score (MOS)\n\\[MOS = \\frac{\\sum_{n=1}^N R\\_n}{N}\\]"
  },
  {
    "objectID": "ml/upscaling_slides.html#metrics-implementation",
    "href": "ml/upscaling_slides.html#metrics-implementation",
    "title": "Super-resolution with PyTorch",
    "section": "Metrics implementation",
    "text": "Metrics implementation\n\n\nImplement them yourself (using torch.log10, etc.)\nUse some library that implements them (e.g.¬†kornia)\nUse code of open source project with good implementation (e.g.¬†SwinIR)\nUse some higher level library that provides them (e.g.¬†ignite)"
  },
  {
    "objectID": "ml/upscaling_slides.html#metrics-implementation-1",
    "href": "ml/upscaling_slides.html#metrics-implementation-1",
    "title": "Super-resolution with PyTorch",
    "section": "Metrics implementation",
    "text": "Metrics implementation\n\n\nImplement them yourself (using torch.log10, etc.)\nUse some library that implements them (e.g.¬†kornia)\nUse code of open source project with good implementation (e.g.¬†SwinIR)\nUse some higher level library that provides them (e.g.¬†ignite)"
  },
  {
    "objectID": "ml/upscaling_slides.html#metrics-implementation-2",
    "href": "ml/upscaling_slides.html#metrics-implementation-2",
    "title": "Super-resolution with PyTorch",
    "section": "Metrics implementation",
    "text": "Metrics implementation\n\nimport kornia\n\npsnr_value = kornia.metrics.psnr(input, target, max_val)\nssim_value = kornia.metrics.ssim(img1, img2, window_size, max_val=1.0, eps=1e-12)\nSee the Kornia documentation for more info on kornia.metrics.psnr & kornia.metrics.ssim"
  },
  {
    "objectID": "ml/upscaling_slides.html#benchmark-datasets",
    "href": "ml/upscaling_slides.html#benchmark-datasets",
    "title": "Super-resolution with PyTorch",
    "section": "Benchmark datasets",
    "text": "Benchmark datasets\nSet5\n\nSet14\n\nBSD100 (Berkeley Segmentation Dataset)"
  },
  {
    "objectID": "ml/upscaling_slides.html#benchmark-datasets-1",
    "href": "ml/upscaling_slides.html#benchmark-datasets-1",
    "title": "Super-resolution with PyTorch",
    "section": "Benchmark datasets",
    "text": "Benchmark datasets\nSet5\n\nSet14\n\nBSD100 (Berkeley Segmentation Dataset)"
  },
  {
    "objectID": "ml/upscaling_slides.html#the-set5-dataset",
    "href": "ml/upscaling_slides.html#the-set5-dataset",
    "title": "Super-resolution with PyTorch",
    "section": "The Set5 dataset",
    "text": "The Set5 dataset\n\nA dataset consisting of 5 images which has been used for at least 18 years to assess SR methods"
  },
  {
    "objectID": "ml/upscaling_slides.html#how-to-get-the-dataset",
    "href": "ml/upscaling_slides.html#how-to-get-the-dataset",
    "title": "Super-resolution with PyTorch",
    "section": "How to get the dataset?",
    "text": "How to get the dataset?\n\nFrom the HuggingFace Datasets Hub with the HuggingFace datasets package:\nfrom datasets import load_dataset\n\nset5 = load_dataset('eugenesiow/Set5', 'bicubic_x4', split='validation')"
  },
  {
    "objectID": "ml/upscaling_slides.html#dataset-exploration",
    "href": "ml/upscaling_slides.html#dataset-exploration",
    "title": "Super-resolution with PyTorch",
    "section": "Dataset exploration",
    "text": "Dataset exploration\n\nprint(set5)\nlen(set5)\nset5[0]\nset5.shape\nset5.column_names\nset5.features\nset5.set_format('torch', columns=['hr', 'lr'])\nset5.format"
  },
  {
    "objectID": "ml/upscaling_slides.html#benchmarks",
    "href": "ml/upscaling_slides.html#benchmarks",
    "title": "Super-resolution with PyTorch",
    "section": "Benchmarks",
    "text": "Benchmarks\n\nA 2012 review of interpolation methods for SR gives the metrics for a series of interpolation methods (using other datasets)"
  },
  {
    "objectID": "ml/upscaling_slides.html#interpolation-methods",
    "href": "ml/upscaling_slides.html#interpolation-methods",
    "title": "Super-resolution with PyTorch",
    "section": "Interpolation methods",
    "text": "Interpolation methods"
  },
  {
    "objectID": "ml/upscaling_slides.html#dl-methods",
    "href": "ml/upscaling_slides.html#dl-methods",
    "title": "Super-resolution with PyTorch",
    "section": "DL methods",
    "text": "DL methods\n\nThe Papers with Code website lists available benchmarks on Set5"
  },
  {
    "objectID": "ml/upscaling_slides.html#lets-use-swinir",
    "href": "ml/upscaling_slides.html#lets-use-swinir",
    "title": "Super-resolution with PyTorch",
    "section": "Let‚Äôs use SwinIR",
    "text": "Let‚Äôs use SwinIR\n\n# Get the model\ngit clone git@github.com:JingyunLiang/SwinIR.git\ncd SwinIR\n\n# Copy our test images in the repo\ncp -r <some/path>/my_tests /testsets/my_tests\n\n# Run the model on our images\npython main_test_swinir.py --tile 400 --task real_sr --scale 4 --large_model --model_path model_zoo/swinir/003_realSR_BSRGAN_DFOWMFC_s64w8_SwinIR-L_x4_GAN.pth --folder_lq testsets/my_tests\nRan in 9 min on my machine with one GPU and 32GB of RAM"
  },
  {
    "objectID": "ml/upscaling_slides.html#results",
    "href": "ml/upscaling_slides.html#results",
    "title": "Super-resolution with PyTorch",
    "section": "Results",
    "text": "Results"
  },
  {
    "objectID": "ml/upscaling_slides.html#results-1",
    "href": "ml/upscaling_slides.html#results-1",
    "title": "Super-resolution with PyTorch",
    "section": "Results",
    "text": "Results"
  },
  {
    "objectID": "ml/upscaling_slides.html#results-2",
    "href": "ml/upscaling_slides.html#results-2",
    "title": "Super-resolution with PyTorch",
    "section": "Results",
    "text": "Results"
  },
  {
    "objectID": "ml/upscaling_slides.html#results-3",
    "href": "ml/upscaling_slides.html#results-3",
    "title": "Super-resolution with PyTorch",
    "section": "Results",
    "text": "Results"
  },
  {
    "objectID": "ml/upscaling_slides.html#results-4",
    "href": "ml/upscaling_slides.html#results-4",
    "title": "Super-resolution with PyTorch",
    "section": "Results",
    "text": "Results"
  },
  {
    "objectID": "ml/upscaling_slides.html#results-5",
    "href": "ml/upscaling_slides.html#results-5",
    "title": "Super-resolution with PyTorch",
    "section": "Results",
    "text": "Results"
  },
  {
    "objectID": "ml/upscaling_slides.html#results-6",
    "href": "ml/upscaling_slides.html#results-6",
    "title": "Super-resolution with PyTorch",
    "section": "Results",
    "text": "Results"
  },
  {
    "objectID": "ml/upscaling_slides.html#results-7",
    "href": "ml/upscaling_slides.html#results-7",
    "title": "Super-resolution with PyTorch",
    "section": "Results",
    "text": "Results"
  },
  {
    "objectID": "ml/upscaling_slides.html#results-8",
    "href": "ml/upscaling_slides.html#results-8",
    "title": "Super-resolution with PyTorch",
    "section": "Results",
    "text": "Results"
  },
  {
    "objectID": "ml/upscaling_slides.html#results-9",
    "href": "ml/upscaling_slides.html#results-9",
    "title": "Super-resolution with PyTorch",
    "section": "Results",
    "text": "Results"
  },
  {
    "objectID": "ml/upscaling_slides.html#metrics",
    "href": "ml/upscaling_slides.html#metrics",
    "title": "Super-resolution with PyTorch",
    "section": "Metrics",
    "text": "Metrics\n\nWe could use the PSNR and SSIM implementations from SwinIR, but let‚Äôs try the Kornia functions we mentioned earlier:\n\nkornia.metrics.psnr\nkornia.metrics.ssim"
  },
  {
    "objectID": "ml/upscaling_slides.html#metrics-1",
    "href": "ml/upscaling_slides.html#metrics-1",
    "title": "Super-resolution with PyTorch",
    "section": "Metrics",
    "text": "Metrics\n\nLet‚Äôs load the libraries we need:\nimport kornia\nfrom PIL import Image\nimport torch\nfrom torchvision import transforms"
  },
  {
    "objectID": "ml/upscaling_slides.html#metrics-2",
    "href": "ml/upscaling_slides.html#metrics-2",
    "title": "Super-resolution with PyTorch",
    "section": "Metrics",
    "text": "Metrics\n\nThen, we load one pair images (LR and HR):\nberlin1_lr = Image.open(\"<some/path>/lr/berlin_1945_1.jpg\")\nberlin1_hr = Image.open(\"<some/path>/hr/berlin_1945_1.png\")\n\nWe can display these images with:\nberlin1_lr.show()\nberlin1_hr.show()"
  },
  {
    "objectID": "ml/upscaling_slides.html#metrics-3",
    "href": "ml/upscaling_slides.html#metrics-3",
    "title": "Super-resolution with PyTorch",
    "section": "Metrics",
    "text": "Metrics\n\nNow, we need to resize them so that they have identical dimensions and turn them into tensors:\npreprocess = transforms.Compose([\n        transforms.Resize(256),\n        transforms.ToTensor()\n        ])\n\nberlin1_lr_t = preprocess(berlin1_lr)\nberlin1_hr_t = preprocess(berlin1_hr)"
  },
  {
    "objectID": "ml/upscaling_slides.html#metrics-4",
    "href": "ml/upscaling_slides.html#metrics-4",
    "title": "Super-resolution with PyTorch",
    "section": "Metrics",
    "text": "Metrics\n\nberlin1_lr_t.shape\nberlin1_hr_t.shape\n\n[Out]\n\ntorch.Size([3, 267, 256])\ntorch.Size([3, 267, 256])\nWe now have tensors with 3 dimensions:\n\nthe channels (RGB)\nthe height of the image (in pixels)\nthe width of the image (in pixels)"
  },
  {
    "objectID": "ml/upscaling_slides.html#metrics-5",
    "href": "ml/upscaling_slides.html#metrics-5",
    "title": "Super-resolution with PyTorch",
    "section": "Metrics",
    "text": "Metrics\n\nAs data processing is done in batch in ML, we need to add a 4th dimension: the batch size\n(It will be equal to 1 since we have a batch size of a single image)\nbatch_berlin1_lr_t = torch.unsqueeze(berlin1_lr_t, 0)\nbatch_berlin1_hr_t = torch.unsqueeze(berlin1_hr_t, 0)"
  },
  {
    "objectID": "ml/upscaling_slides.html#metrics-6",
    "href": "ml/upscaling_slides.html#metrics-6",
    "title": "Super-resolution with PyTorch",
    "section": "Metrics",
    "text": "Metrics\n\nOur new tensors are now ready:\nbatch_berlin1_lr_t.shape\nbatch_berlin1_hr_t.shape\n\n[Out]\n\ntorch.Size([1, 3, 267, 256])\ntorch.Size([1, 3, 267, 256])"
  },
  {
    "objectID": "ml/upscaling_slides.html#psnr",
    "href": "ml/upscaling_slides.html#psnr",
    "title": "Super-resolution with PyTorch",
    "section": "PSNR",
    "text": "PSNR\n\npsnr_value = kornia.metrics.psnr(batch_berlin1_lr_t, batch_berlin1_hr_t, max_val=1.0)\npsnr_value.item()\n\n[Out]\n\n33.379642486572266"
  },
  {
    "objectID": "ml/upscaling_slides.html#ssim",
    "href": "ml/upscaling_slides.html#ssim",
    "title": "Super-resolution with PyTorch",
    "section": "SSIM",
    "text": "SSIM\n\nssim_map = kornia.metrics.ssim(batch_berlin1_lr_t, batch_berlin1_hr_t, window_size=5, max_val=1.0, eps=1e-12)\nssim_map.mean().item()\n\n[Out]\n\n0.9868119359016418"
  },
  {
    "objectID": "newsletter.html",
    "href": "newsletter.html",
    "title": "Training events mailing list",
    "section": "",
    "text": "If you want to get informed about upcoming training events, please subscribe to our mailing list: \n(We will only email you about training events.)"
  },
  {
    "objectID": "r/basics.html",
    "href": "r/basics.html",
    "title": "R: the basics",
    "section": "",
    "text": "For some general documentation on R, you can run:\nhelp.start()\nTo get help on a function (e.g.¬†sum), you can run:\nhelp(sum)\nDepending on your settings, this will open a documentation for sum in a pager or in your browser."
  },
  {
    "objectID": "r/basics.html#r-settings",
    "href": "r/basics.html#r-settings",
    "title": "R: the basics",
    "section": "R settings",
    "text": "R settings\nSettings are saved in a .Rprofile file. You can edit the file directly in any text editor or from within R.\nList all options:\noptions()\nReturn the value of a particular option:\n\ngetOption(\"help_type\")\n\n[1] \"html\"\n\n\nSet an option:\noptions(help_type = \"html\")"
  },
  {
    "objectID": "r/basics.html#assignment",
    "href": "r/basics.html#assignment",
    "title": "R: the basics",
    "section": "Assignment",
    "text": "Assignment\nR can accept the equal sign (=) for assignments, but it is more idiomatic to use the assignment sign (<-) whenever you bind a name to a value and to use the equal sign everywhere else.\n\na <- 3\n\nOnce you have bound a name to a value, you can recall the value with that name:\n\na  # Note that you do not need to use a print() function in R\n\n[1] 3\n\n\nYou can remove an object from the environment by deleting its name:\n\nrm(a)\na\n\nError in eval(expr, envir, enclos): object 'a' not found\n\n\nThe garbage collector will take care of deleting the object itself from memory."
  },
  {
    "objectID": "r/basics.html#data-types-and-structures",
    "href": "r/basics.html#data-types-and-structures",
    "title": "R: the basics",
    "section": "Data types and structures",
    "text": "Data types and structures\n\n\n\nDimension\nHomogeneous\nHeterogeneous\n\n\n\n\n1 d\nAtomic vector\nList\n\n\n2 d\nMatrix\nData frame\n\n\n3 d\nArray\n\n\n\n\n\nAtomic vectors\n\nWith a single element\n\na <- 2\na\n\n[1] 2\n\ntypeof(a)\n\n[1] \"double\"\n\nstr(a)\n\n num 2\n\nlength(a)\n\n[1] 1\n\ndim(a)\n\nNULL\n\n\nThe dim attribute of a vector doesn‚Äôt exist (hence the NULL). This makes vectors different from one-dimensional arrays which have a dim of 1.\nYou might have noticed that 2 is a double (double precision floating point number, equivalent of ‚Äúfloat‚Äù in other languages). In R, this is the default, even if you don‚Äôt type 2.0. This prevents the kind of weirdness you can find in, for instance, Python.\nIn Python:\n>>> 2 == 2.0\nTrue\n>>> type(2) == type(2.0)\nFalse\n>>> type(2)\n<class 'int'>\n>>> type(2.0)\n<class 'float'>\nIn R:\n> 2 == 2.0\n[1] TRUE\n> typeof(2) == typeof(2.0)\n[1] TRUE\n> typeof(2)\n[1] \"double\"\n> typeof(2.0)\n[1] \"double\"\nIf you want to define an integer variable, you use:\n\nb <- 2L\nb\n\n[1] 2\n\ntypeof(b)\n\n[1] \"integer\"\n\nmode(b)\n\n[1] \"numeric\"\n\nstr(b)\n\n int 2\n\n\nThere are six vector types:\n\nlogical\ninteger\ndouble\ncharacter\ncomplex\nraw\n\n\n\nWith multiple elements\n\nc <- c(2, 4, 1)\nc\n\n[1] 2 4 1\n\ntypeof(c)\n\n[1] \"double\"\n\nmode(c)\n\n[1] \"numeric\"\n\nstr(c)\n\n num [1:3] 2 4 1\n\n\n\nd <- c(TRUE, TRUE, NA, FALSE)\nd\n\n[1]  TRUE  TRUE    NA FALSE\n\ntypeof(d)\n\n[1] \"logical\"\n\nstr(d)\n\n logi [1:4] TRUE TRUE NA FALSE\n\n\nNA (‚ÄúNot Available‚Äù) is a logical constant of length one. It is an indicator for a missing value.\nVectors are homogeneous, so all elements need to be of the same type.\nIf you use elements of different types, R will convert some of them to ensure that they become of the same type:\n\ne <- c(\"This is a string\", 3, \"test\")\ne\n\n[1] \"This is a string\" \"3\"                \"test\"            \n\ntypeof(e)\n\n[1] \"character\"\n\nstr(e)\n\n chr [1:3] \"This is a string\" \"3\" \"test\"\n\n\n\nf <- c(TRUE, 3, FALSE)\nf\n\n[1] 1 3 0\n\ntypeof(f)\n\n[1] \"double\"\n\nstr(f)\n\n num [1:3] 1 3 0\n\n\n\ng <- c(2L, 3, 4L)\ng\n\n[1] 2 3 4\n\ntypeof(g)\n\n[1] \"double\"\n\nstr(g)\n\n num [1:3] 2 3 4\n\n\n\nh <- c(\"string\", TRUE, 2L, 3.1)\nh\n\n[1] \"string\" \"TRUE\"   \"2\"      \"3.1\"   \n\ntypeof(h)\n\n[1] \"character\"\n\nstr(h)\n\n chr [1:4] \"string\" \"TRUE\" \"2\" \"3.1\"\n\n\nThe binary operator : is equivalent to the seq() function and generates a regular sequence of integers:\n\ni <- 1:5\ni\n\n[1] 1 2 3 4 5\n\ntypeof(i)\n\n[1] \"integer\"\n\nstr(i)\n\n int [1:5] 1 2 3 4 5\n\nidentical(2:8, seq(2, 8))\n\n[1] TRUE\n\n\n\n\n\nMatrices\n\nj <- matrix(1:12, nrow = 3, ncol = 4)\nj\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    4    7   10\n[2,]    2    5    8   11\n[3,]    3    6    9   12\n\ntypeof(j)\n\n[1] \"integer\"\n\nstr(j)\n\n int [1:3, 1:4] 1 2 3 4 5 6 7 8 9 10 ...\n\nlength(j)\n\n[1] 12\n\ndim(j)\n\n[1] 3 4\n\n\nThe default is byrow = FALSE. If you want the matrix to be filled in by row, you need to set this argument to TRUE:\n\nk <- matrix(1:12, nrow = 3, ncol = 4, byrow = TRUE)\nk\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    2    3    4\n[2,]    5    6    7    8\n[3,]    9   10   11   12\n\n\n\n\nArrays\n\nl <- array(as.double(1:24), c(3, 2, 4))\nl\n\n, , 1\n\n     [,1] [,2]\n[1,]    1    4\n[2,]    2    5\n[3,]    3    6\n\n, , 2\n\n     [,1] [,2]\n[1,]    7   10\n[2,]    8   11\n[3,]    9   12\n\n, , 3\n\n     [,1] [,2]\n[1,]   13   16\n[2,]   14   17\n[3,]   15   18\n\n, , 4\n\n     [,1] [,2]\n[1,]   19   22\n[2,]   20   23\n[3,]   21   24\n\ntypeof(l)\n\n[1] \"double\"\n\nstr(l)\n\n num [1:3, 1:2, 1:4] 1 2 3 4 5 6 7 8 9 10 ...\n\nlength(l)\n\n[1] 24\n\ndim(l)\n\n[1] 3 2 4\n\n\n\n\nLists\n\nm <- list(2, 3)\nm\n\n[[1]]\n[1] 2\n\n[[2]]\n[1] 3\n\ntypeof(m)\n\n[1] \"list\"\n\nstr(m)\n\nList of 2\n $ : num 2\n $ : num 3\n\nlength(m)\n\n[1] 2\n\ndim(m)\n\nNULL\n\n\nAs with atomic vectors, lists do not have a dim attribute. Lists are in fact a different type of vectors.\nLists can be heterogeneous:\n\nn <- list(2L, 3, c(2, 1), FALSE, \"string\")\nn\n\n[[1]]\n[1] 2\n\n[[2]]\n[1] 3\n\n[[3]]\n[1] 2 1\n\n[[4]]\n[1] FALSE\n\n[[5]]\n[1] \"string\"\n\ntypeof(n)\n\n[1] \"list\"\n\nstr(n)\n\nList of 5\n $ : int 2\n $ : num 3\n $ : num [1:2] 2 1\n $ : logi FALSE\n $ : chr \"string\"\n\nlength(n)\n\n[1] 5\n\n\n\n\nData frames\nData frames contain tabular data. Under the hood, a data frame is a list of vectors.\n\no <- data.frame(\n  country = c(\"Canada\", \"USA\", \"Mexico\"),\n  var = c(2.9, 3.1, 4.5)\n)\no\n\n  country var\n1  Canada 2.9\n2     USA 3.1\n3  Mexico 4.5\n\ntypeof(o)\n\n[1] \"list\"\n\nstr(o)\n\n'data.frame':   3 obs. of  2 variables:\n $ country: chr  \"Canada\" \"USA\" \"Mexico\"\n $ var    : num  2.9 3.1 4.5\n\nlength(o)\n\n[1] 2\n\ndim(o)\n\n[1] 3 2"
  },
  {
    "objectID": "r/basics.html#indexing",
    "href": "r/basics.html#indexing",
    "title": "R: the basics",
    "section": "Indexing",
    "text": "Indexing\nIndexing in R starts at 1.\n\na\n\n[1] 2\n\na[1]\n\n[1] 2\n\na[2]\n\n[1] NA\n\nc\n\n[1] 2 4 1\n\nc[2]\n\n[1] 4\n\nc[2:4]\n\n[1]  4  1 NA\n\nj\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    4    7   10\n[2,]    2    5    8   11\n[3,]    3    6    9   12\n\nj[2, 3]\n\n[1] 8\n\nl\n\n, , 1\n\n     [,1] [,2]\n[1,]    1    4\n[2,]    2    5\n[3,]    3    6\n\n, , 2\n\n     [,1] [,2]\n[1,]    7   10\n[2,]    8   11\n[3,]    9   12\n\n, , 3\n\n     [,1] [,2]\n[1,]   13   16\n[2,]   14   17\n[3,]   15   18\n\n, , 4\n\n     [,1] [,2]\n[1,]   19   22\n[2,]   20   23\n[3,]   21   24\n\nl[2, 1, 3]\n\n[1] 14\n\nn\n\n[[1]]\n[1] 2\n\n[[2]]\n[1] 3\n\n[[3]]\n[1] 2 1\n\n[[4]]\n[1] FALSE\n\n[[5]]\n[1] \"string\"\n\nn[3]\n\n[[1]]\n[1] 2 1\n\ntypeof(n[3])\n\n[1] \"list\"\n\nn[3][1]\n\n[[1]]\n[1] 2 1\n\nn[[3]]\n\n[1] 2 1\n\ntypeof(n[[3]])\n\n[1] \"double\"\n\nn[[3]][1]\n\n[1] 2\n\no\n\n  country var\n1  Canada 2.9\n2     USA 3.1\n3  Mexico 4.5\n\no[1]\n\n  country\n1  Canada\n2     USA\n3  Mexico\n\ntypeof(o[1])\n\n[1] \"list\"\n\nstr(o[1])\n\n'data.frame':   3 obs. of  1 variable:\n $ country: chr  \"Canada\" \"USA\" \"Mexico\"\n\no[[1]]\n\n[1] \"Canada\" \"USA\"    \"Mexico\"\n\ntypeof(o[[1]])\n\n[1] \"character\"\n\no$country\n\n[1] \"Canada\" \"USA\"    \"Mexico\"\n\ntypeof(o$country)\n\n[1] \"character\""
  },
  {
    "objectID": "r/basics.html#copy-on-modify",
    "href": "r/basics.html#copy-on-modify",
    "title": "R: the basics",
    "section": "Copy-on-modify",
    "text": "Copy-on-modify\nWhile some languages (e.g.¬†Python) do not make a copy if you modify a mutable object, R does.\nLet‚Äôs have a look at Python:\n>>> a = [1, 2, 3]\n>>> b = a\n>>> b\n[1, 2, 3]\n>>> a[0] = 4\n>>> a\n[4, 2, 3]\n>>> b\n[4, 2, 3]\nModifying a also modifies b. If you want to keep b unchanged, you need to explicitly make a copy of a.\nNow, let‚Äôs see what happens in R:\n> a <- c(1, 2, 3)\n> b <- a\n> b\n[1] 1 2 3\n> a[1] <- 4\n> a\n[1] 4 2 3\n> b\n[1] 1 2 3\nHere, the default is to create a new copy in memory when a is transformed so that b remains unchanged. This is more intuitive, but more memory intensive."
  },
  {
    "objectID": "r/basics.html#function-definition",
    "href": "r/basics.html#function-definition",
    "title": "R: the basics",
    "section": "Function definition",
    "text": "Function definition\n\ncompare <- function(x, y) {\n  x == y\n}\n\nWe can now use our function:\n\ncompare(2, 3)\n\n[1] FALSE\n\n\nNote that the result of the last statement is printed automatically:\n\ntest <- function(x, y) {\n  x\n  y\n}\ntest(2, 3)\n\n[1] 3\n\n\nIf you want to return other results, you need to explicitly use the print() function:\n\ntest <- function(x, y) {\n  print(x)\n  y\n}\ntest(2, 3)\n\n[1] 2\n\n\n[1] 3"
  },
  {
    "objectID": "r/basics.html#control-flow",
    "href": "r/basics.html#control-flow",
    "title": "R: the basics",
    "section": "Control flow",
    "text": "Control flow\n\nConditionals\n\ntest_sign <- function(x) {\n  if (x > 0) {\n    \"x is positif\"\n  } else if (x < 0) {\n    \"x is negatif\"\n  } else {\n    \"x is equal to zero\"\n  }\n}\n\n\ntest_sign(3)\n\n[1] \"x is positif\"\n\ntest_sign(-2)\n\n[1] \"x is negatif\"\n\ntest_sign(0)\n\n[1] \"x is equal to zero\"\n\n\n\n\nLoops\n\nfor (i in 1:10) {\n  print(i)\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n[1] 7\n[1] 8\n[1] 9\n[1] 10\n\n\nNotice that here we need to use the print() function."
  },
  {
    "objectID": "r/gis_mapping.html",
    "href": "r/gis_mapping.html",
    "title": "GIS mapping in R",
    "section": "",
    "text": "In this webinar, we will see how to create all sorts of GIS maps with the packages sf, tmap, raster, leaflet, ggplot2, grid (part of Base R), and mapview:\n\nsimple maps\ninset maps\nfaceted maps\nanimated maps\ninteractive maps\nraster maps\n\nFinally, we will learn how to add basemaps from OpenStreetMap and Google Maps.\n\nSlides (click and wait: my reveal.js presentations are heavy and take some time to load‚Ä¶)"
  },
  {
    "objectID": "r/gis_mapping_slides.html#types-of-spatial-data",
    "href": "r/gis_mapping_slides.html#types-of-spatial-data",
    "title": "GIS mapping in R",
    "section": "Types of spatial data",
    "text": "Types of spatial data\n\nVector data\n\nDiscrete objects\nContain: ‚ÄÇ- geometry:‚ÄÇ shape & location of the objects\n‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ- attributes:‚ÄÇ additional variables (e.g.¬†name, year, type)\nCommon file format:‚ÄÇ GeoJSON, shapefile\n\nExamples: countries, roads, rivers, towns\n\n\nRaster data\n\nContinuous phenomena or spatial fields\nCommon file formats:‚ÄÇ TIFF, GeoTIFF, NetCDF, Esri grid\n\nExamples: temperature, air quality, elevation, water depth"
  },
  {
    "objectID": "r/gis_mapping_slides.html#vector-data-1",
    "href": "r/gis_mapping_slides.html#vector-data-1",
    "title": "GIS mapping in R",
    "section": "Vector data",
    "text": "Vector data\n\nTypes\n\npoint:‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÇ¬† single set of coordinates\nmulti-point:‚ÄÉ‚ÄÉ multiple sets of coordinates\npolyline:‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÇ¬† multiple sets for which the order matters\nmulti-polyline:‚ÄÉ multiple of the above\npolygon:‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÇ¬† same as polyline but first & last sets are the same\nmulti-polygon:‚ÄÉ multiple of the above"
  },
  {
    "objectID": "r/gis_mapping_slides.html#raster-data-1",
    "href": "r/gis_mapping_slides.html#raster-data-1",
    "title": "GIS mapping in R",
    "section": "Raster data",
    "text": "Raster data\n\nGrid of equally sized rectangular cells containing values for some variables\nSize of cells = resolution\nFor computing efficiency, rasters do not have coordinates of each cell, but the bounding box & the number of rows & columns"
  },
  {
    "objectID": "r/gis_mapping_slides.html#coordinate-reference-systems-crs",
    "href": "r/gis_mapping_slides.html#coordinate-reference-systems-crs",
    "title": "GIS mapping in R",
    "section": "Coordinate Reference Systems (CRS)",
    "text": "Coordinate Reference Systems (CRS)\nA location on Earth‚Äôs surface can be identified by its coordinates & some reference system called CRS\nThe coordinates (x, y) are called longitude & latitude\nThere can be a 3rd coordinate (z) for elevation or other measurement‚Äîusually a vertical one\nAnd a 4th (m) for some other data attribute‚Äîusually a horizontal measurement\nIn 3D, longitude & latitude are expressed in angular units (e.g.¬†degrees) & the reference system needed is an angular CRS or geographic coordinate system (GCS)\nIn 2D, they are expressed in linear units (e.g.¬†meters) & the reference system needed is a planar CRS or projected coordinate system (PCS)"
  },
  {
    "objectID": "r/gis_mapping_slides.html#datums",
    "href": "r/gis_mapping_slides.html#datums",
    "title": "GIS mapping in R",
    "section": "Datums",
    "text": "Datums\nSince the Earth is not a perfect sphere, we use spheroidal models to represent its surface. Those are called geodetic datums\nSome datums are global, others local (more accurate in a particular area of the globe, but only useful there) \n\nExamples of commonly used global datums:\n\nWGS84 (World Geodesic System 1984)\nNAD83 (North American Datum of 1983)"
  },
  {
    "objectID": "r/gis_mapping_slides.html#angular-crs",
    "href": "r/gis_mapping_slides.html#angular-crs",
    "title": "GIS mapping in R",
    "section": "Angular CRS",
    "text": "Angular CRS\nAn angular CRS contains a datum, an angular unit & references such as a prime meridian (e.g.¬†the Royal Observatory, Greenwich, England)\nIn an angular CRS or GCS:\n\nLongitude (\\(\\lambda\\)) represents the angle between the prime meridian & the meridian that passes through that location\nLatitude (\\(\\phi\\)) represents the angle between the line that passes through the center of the Earth & that location & its projection on the equatorial plane\n\nLongitude & latitude are thus angular coordinates"
  },
  {
    "objectID": "r/gis_mapping_slides.html#projections",
    "href": "r/gis_mapping_slides.html#projections",
    "title": "GIS mapping in R",
    "section": "Projections",
    "text": "Projections\nTo create a two-dimensional map, you need to project this 3D angular CRS into a 2D one\nVarious projections offer different characteristics. For instance:\n\nsome respect areas (equal-area)\nsome respect the shape of geographic features (conformal)\nsome almost respect both for small areas\n\nIt is important to choose one with sensible properties for your goals\n\nExamples of projections:\n\nMercator\nUTM\nRobinson"
  },
  {
    "objectID": "r/gis_mapping_slides.html#planar-crs",
    "href": "r/gis_mapping_slides.html#planar-crs",
    "title": "GIS mapping in R",
    "section": "Planar CRS",
    "text": "Planar CRS\nA planar CRS is defined by a datum, a projection & a set of parameters such as a linear unit & the origins\nCommon planar CRS have been assigned a unique ID called EPSG code which is much more convenient to use\nIn a planar CRS, coordinates will not be in degrees anymore but in meters (or other length unit)"
  },
  {
    "objectID": "r/gis_mapping_slides.html#resources",
    "href": "r/gis_mapping_slides.html#resources",
    "title": "GIS mapping in R",
    "section": "Resources",
    "text": "Resources\n\nOpen GIS data\nFree GIS Data: list of free GIS datasets\nBooks\nGeocomputation with R by Robin Lovelace, Jakub Nowosad & Jannes Muenchow\nSpatial Data Science by Edzer Pebesma & Roger Bivand\nSpatial Data Science with R by Robert J. Hijmans\nUsing Spatial Data with R by Claudia A. Engel\nTutorial\nAn Introduction to Spatial Data Analysis and Visualisation in R by the CDRC"
  },
  {
    "objectID": "r/gis_mapping_slides.html#resources-1",
    "href": "r/gis_mapping_slides.html#resources-1",
    "title": "GIS mapping in R",
    "section": "Resources",
    "text": "Resources\n\nWebsite\nr-spatial by Edzer Pebesma, Marius Appel & Daniel N√ºst\nCRAN package list\nAnalysis of Spatial Data\nMailing list\nR Special Interest Group on using Geographical data and Mapping"
  },
  {
    "objectID": "r/gis_mapping_slides.html#data-manipulation",
    "href": "r/gis_mapping_slides.html#data-manipulation",
    "title": "GIS mapping in R",
    "section": "Data manipulation",
    "text": "Data manipulation\n\nOlder packages\n\nsp\nraster\nrgdal\nrgeos\n\nNewer generation\n\nsf: vector data\nterra: raster data (also has vector data capabilities)"
  },
  {
    "objectID": "r/gis_mapping_slides.html#mapping",
    "href": "r/gis_mapping_slides.html#mapping",
    "title": "GIS mapping in R",
    "section": "Mapping",
    "text": "Mapping\n\nStatic maps\n\nggplot2 + ggspatial\ntmap\n\nDynamic maps\n\nleaflet\nggplot2 + gganimate\nmapview\nggmap\ntmap"
  },
  {
    "objectID": "r/gis_mapping_slides.html#sf-simple-features-in-r",
    "href": "r/gis_mapping_slides.html#sf-simple-features-in-r",
    "title": "GIS mapping in R",
    "section": "sf: Simple Features in R",
    "text": "sf: Simple Features in R\nGeospatial vectors: points, lines, polygons\nSimple Features‚Äîdefined by the Open Geospatial Consortium (OGC) & formalized by ISO‚Äîis a set of standards now used by most GIS libraries\nWell-known text (WKT) is a markup language for representing vector geometry objects according to those standards\nA compact computer version also exists‚Äîwell-known binary (WKB)‚Äîused by spatial databases\nThe package sp predates Simple Features\nsf‚Äîlaunched in 2016‚Äîimplements these standards in R in the form of sf objects: data.frames (or tibbles) containing the attributes, extended by sfc objects or simple feature geometries list-columns"
  },
  {
    "objectID": "r/gis_mapping_slides.html#sf",
    "href": "r/gis_mapping_slides.html#sf",
    "title": "GIS mapping in R",
    "section": "sf",
    "text": "sf\n\nUseful links\n\nGitHub repo\nPaper\nResources\nCheatsheet\n6 vignettes: 1, 2, 3, 4, 5, 6"
  },
  {
    "objectID": "r/gis_mapping_slides.html#sf-objects",
    "href": "r/gis_mapping_slides.html#sf-objects",
    "title": "GIS mapping in R",
    "section": "sf objects",
    "text": "sf objects"
  },
  {
    "objectID": "r/gis_mapping_slides.html#sf-objects-1",
    "href": "r/gis_mapping_slides.html#sf-objects-1",
    "title": "GIS mapping in R",
    "section": "sf objects",
    "text": "sf objects"
  },
  {
    "objectID": "r/gis_mapping_slides.html#sf-objects-2",
    "href": "r/gis_mapping_slides.html#sf-objects-2",
    "title": "GIS mapping in R",
    "section": "sf objects",
    "text": "sf objects"
  },
  {
    "objectID": "r/gis_mapping_slides.html#sf-objects-3",
    "href": "r/gis_mapping_slides.html#sf-objects-3",
    "title": "GIS mapping in R",
    "section": "sf objects",
    "text": "sf objects"
  },
  {
    "objectID": "r/gis_mapping_slides.html#sf-objects-4",
    "href": "r/gis_mapping_slides.html#sf-objects-4",
    "title": "GIS mapping in R",
    "section": "sf objects",
    "text": "sf objects"
  },
  {
    "objectID": "r/gis_mapping_slides.html#sf-functions",
    "href": "r/gis_mapping_slides.html#sf-functions",
    "title": "GIS mapping in R",
    "section": "sf functions",
    "text": "sf functions\n\nMost functions start with st_ (which refers to ‚Äúspatial type‚Äù)"
  },
  {
    "objectID": "r/gis_mapping_slides.html#terra-geospatial-rasters",
    "href": "r/gis_mapping_slides.html#terra-geospatial-rasters",
    "title": "GIS mapping in R",
    "section": "terra: Geospatial rasters",
    "text": "terra: Geospatial rasters\n\nFaster and simpler replacement for the raster package by the same team\nMostly implemented in C++\nCan work with datasets too large to be loaded into memory"
  },
  {
    "objectID": "r/gis_mapping_slides.html#terra",
    "href": "r/gis_mapping_slides.html#terra",
    "title": "GIS mapping in R",
    "section": "terra",
    "text": "terra\n\nUseful links\n\nGitHub repo\nResources\nFull manual"
  },
  {
    "objectID": "r/gis_mapping_slides.html#tmap-layered-grammar-of-graphics-gis-maps",
    "href": "r/gis_mapping_slides.html#tmap-layered-grammar-of-graphics-gis-maps",
    "title": "GIS mapping in R",
    "section": "tmap: Layered grammar of graphics GIS maps",
    "text": "tmap: Layered grammar of graphics GIS maps"
  },
  {
    "objectID": "r/gis_mapping_slides.html#tmap",
    "href": "r/gis_mapping_slides.html#tmap",
    "title": "GIS mapping in R",
    "section": "tmap",
    "text": "tmap\n\nUseful links\n\nGitHub repo\nResources\n\nHelp pages and vignettes\n\n?tmap-element\nvignette(\"tmap-getstarted\")\n# All the usual help pages, e.g.:\n?tm_layout"
  },
  {
    "objectID": "r/gis_mapping_slides.html#tmap-functions",
    "href": "r/gis_mapping_slides.html#tmap-functions",
    "title": "GIS mapping in R",
    "section": "tmap functions",
    "text": "tmap functions\n\nMain functions start with tmap_\nFunctions creating map elements start with tm_"
  },
  {
    "objectID": "r/gis_mapping_slides.html#tmap-functioning",
    "href": "r/gis_mapping_slides.html#tmap-functioning",
    "title": "GIS mapping in R",
    "section": "tmap functioning",
    "text": "tmap functioning\nVery similar to ggplot2\nTypically, a map contains:\n\nOne or multiple layer(s) (the order matters as they stack on top of each other)\nSome layout (e.g.¬†customization of title, background, margins): tm_layout\nA compass: tm_compass\nA scale bar: tm_scale_bar\n\nEach layer contains:\n\nSome data: tm_shape\nHow that data will be represented: e.g.¬†tm_polygons, tm_lines, tm_raster"
  },
  {
    "objectID": "r/gis_mapping_slides.html#tmap-example",
    "href": "r/gis_mapping_slides.html#tmap-example",
    "title": "GIS mapping in R",
    "section": "tmap example",
    "text": "tmap example"
  },
  {
    "objectID": "r/gis_mapping_slides.html#tmap-example-1",
    "href": "r/gis_mapping_slides.html#tmap-example-1",
    "title": "GIS mapping in R",
    "section": "tmap example",
    "text": "tmap example"
  },
  {
    "objectID": "r/gis_mapping_slides.html#tmap-example-2",
    "href": "r/gis_mapping_slides.html#tmap-example-2",
    "title": "GIS mapping in R",
    "section": "tmap example",
    "text": "tmap example"
  },
  {
    "objectID": "r/gis_mapping_slides.html#tmap-example-3",
    "href": "r/gis_mapping_slides.html#tmap-example-3",
    "title": "GIS mapping in R",
    "section": "tmap example",
    "text": "tmap example"
  },
  {
    "objectID": "r/gis_mapping_slides.html#tmap-example-4",
    "href": "r/gis_mapping_slides.html#tmap-example-4",
    "title": "GIS mapping in R",
    "section": "tmap example",
    "text": "tmap example"
  },
  {
    "objectID": "r/gis_mapping_slides.html#tmap-example-5",
    "href": "r/gis_mapping_slides.html#tmap-example-5",
    "title": "GIS mapping in R",
    "section": "tmap example",
    "text": "tmap example"
  },
  {
    "objectID": "r/gis_mapping_slides.html#tmap-example-6",
    "href": "r/gis_mapping_slides.html#tmap-example-6",
    "title": "GIS mapping in R",
    "section": "tmap example",
    "text": "tmap example"
  },
  {
    "objectID": "r/gis_mapping_slides.html#tmap-example-7",
    "href": "r/gis_mapping_slides.html#tmap-example-7",
    "title": "GIS mapping in R",
    "section": "tmap example",
    "text": "tmap example"
  },
  {
    "objectID": "r/gis_mapping_slides.html#ggplot2-the-standard-in-r-plots",
    "href": "r/gis_mapping_slides.html#ggplot2-the-standard-in-r-plots",
    "title": "GIS mapping in R",
    "section": "ggplot2 (the standard in R plots)",
    "text": "ggplot2 (the standard in R plots)\n\nUseful links\n\nGitHub repo\nResources\nCheatsheet"
  },
  {
    "objectID": "r/gis_mapping_slides.html#ggplot2",
    "href": "r/gis_mapping_slides.html#ggplot2",
    "title": "GIS mapping in R",
    "section": "ggplot2",
    "text": "ggplot2\ngeom_sf allows to plot sf objects (i.e.¬†make maps)"
  },
  {
    "objectID": "r/gis_mapping_slides.html#data",
    "href": "r/gis_mapping_slides.html#data",
    "title": "GIS mapping in R",
    "section": "Data",
    "text": "Data\nFor this workshop, we will use:\n\nthe Alaska as well as the Western Canada & USA subsets of the Randolph Glacier Inventory version 6.01\nthe USGS time series of the named glaciers of Glacier National Park2\nthe Alaska as well as the Western Canada & USA subsets of the consensus estimate for the ice thickness distribution of all glaciers on Earth dataset3\n\nThe datasets can be downloaded as zip files from these websites\nRGI Consortium (2017). Randolph Glacier Inventory ‚Äì A Dataset of Global Glacier Outlines: Version 6.0: Technical Report, Global Land Ice Measurements from Space, Colorado, USA. Digital Media. DOI: https://doi.org/10.7265/N5-RGI-60.Fagre, D.B., McKeon, L.A., Dick, K.A. & Fountain, A.G., 2017, Glacier margin time series (1966, 1998, 2005, 2015) of the named glaciers of Glacier National Park, MT, USA: U.S. Geological Survey data release. DOI: https://doi.org/10.5066/F7P26WB1.Farinotti, Daniel, 2019, A consensus estimate for the ice thickness distribution of all glaciers on Earth - dataset, Zurich. ETH Zurich. DOI: https://doi.org/10.3929/ethz-b-000315707."
  },
  {
    "objectID": "r/gis_mapping_slides.html#packages-1",
    "href": "r/gis_mapping_slides.html#packages-1",
    "title": "GIS mapping in R",
    "section": "Packages",
    "text": "Packages\n\nPackages need to be installed before they can be loaded in a session\nPackages on CRAN can be installed with:\ninstall.packages(\"<package-name>\")\n basemaps is not on CRAN & needs to be installed from GitHub thanks to devtools:\ninstall.packages(\"devtools\")\ndevtools::install_github(\"16EAGLE/basemaps\")"
  },
  {
    "objectID": "r/gis_mapping_slides.html#packages-2",
    "href": "r/gis_mapping_slides.html#packages-2",
    "title": "GIS mapping in R",
    "section": "Packages",
    "text": "Packages\n\nWe load all the packages that we will need at the top of the script:\nlibrary(sf)                 # spatial vector data manipulation\nlibrary(tmap)               # map production & tiled web map\nlibrary(dplyr)              # non GIS specific (tabular data manipulation)\nlibrary(magrittr)           # non GIS specific (pipes)\nlibrary(purrr)              # non GIS specific (functional programming)\nlibrary(rnaturalearth)      # basemap data access functions\nlibrary(rnaturalearthdata)  # basemap data\nlibrary(mapview)            # tiled web map\nlibrary(grid)               # (part of base R) used to create inset map\nlibrary(ggplot2)            # alternative to tmap for map production\nlibrary(ggspatial)          # spatial framework for ggplot2\nlibrary(terra)              # gridded spatial data manipulation\nlibrary(ggmap)              # download basemap data\nlibrary(basemaps)           # download basemap data\nlibrary(magick)             # wrapper around ImageMagick STL\nlibrary(leaflet)            # integrate Leaflet JS in R"
  },
  {
    "objectID": "r/gis_mapping_slides.html#randolph-glacier-inventory",
    "href": "r/gis_mapping_slides.html#randolph-glacier-inventory",
    "title": "GIS mapping in R",
    "section": "Randolph Glacier Inventory",
    "text": "Randolph Glacier Inventory\n\nThis dataset contains the contour of all glaciers on Earth \nWe will focus on glaciers in Western North America \nYou can download & unzip 02_rgi60_WesternCanadaUS & 01_rgi60_Alaska from the Randolph Glacier Inventory version 6.0"
  },
  {
    "objectID": "r/gis_mapping_slides.html#reading-in-data",
    "href": "r/gis_mapping_slides.html#reading-in-data",
    "title": "GIS mapping in R",
    "section": "Reading in data",
    "text": "Reading in data\n\nData get imported & turned into sf objects with the function sf::st_read:\nak <- st_read(\"data/01_rgi60_Alaska\")\n\nMake sure to use the absolute paths or the paths relative to your working directory (which can be obtained with getwd)"
  },
  {
    "objectID": "r/gis_mapping_slides.html#reading-in-data-1",
    "href": "r/gis_mapping_slides.html#reading-in-data-1",
    "title": "GIS mapping in R",
    "section": "Reading in data",
    "text": "Reading in data\n\nak <- st_read(\"data/01_rgi60_Alaska\")\n\n[Out]\n\nReading layer `01_rgi60_Alaska' from data source `./data/01_rgi60_Alaska'\n               using driver `ESRI Shapefile'\nSimple feature collection with 27108 features and 22 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -176.1425 ymin: 52.05727 xmax: -126.8545 ymax: 69.35167\nGeodetic CRS:  WGS 84"
  },
  {
    "objectID": "r/gis_mapping_slides.html#reading-in-data-2",
    "href": "r/gis_mapping_slides.html#reading-in-data-2",
    "title": "GIS mapping in R",
    "section": "Reading in data",
    "text": "Reading in data\n\n\n\nYour turn:\n\nRead in the data for the rest of north western America (from 02_rgi60_WesternCanadaUS) and create an sf object called wes"
  },
  {
    "objectID": "r/gis_mapping_slides.html#first-look-at-the-data",
    "href": "r/gis_mapping_slides.html#first-look-at-the-data",
    "title": "GIS mapping in R",
    "section": "First look at the data",
    "text": "First look at the data\nak\n\n[Out]\n\nSimple feature collection with 27108 features and 22 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -176.1425 ymin: 52.05727 xmax: -126.8545 ymax: 69.35167\nGeodetic CRS:  WGS 84\nFirst 10 features:\n           RGIId        GLIMSId  BgnDate  EndDate    CenLon   CenLat O1Region\n1  RGI60-01.00001 G213177E63689N 20090703 -9999999 -146.8230 63.68900        1\n2  RGI60-01.00002 G213332E63404N 20090703 -9999999 -146.6680 63.40400        1\n3  RGI60-01.00003 G213920E63376N 20090703 -9999999 -146.0800 63.37600        1\n4  RGI60-01.00004 G213880E63381N 20090703 -9999999 -146.1200 63.38100        1\n5  RGI60-01.00005 G212943E63551N 20090703 -9999999 -147.0570 63.55100        1\n6  RGI60-01.00006 G213756E63571N 20090703 -9999999 -146.2440 63.57100        1\n7  RGI60-01.00007 G213771E63551N 20090703 -9999999 -146.2295 63.55085        1\n8  RGI60-01.00008 G213704E63543N 20090703 -9999999 -146.2960 63.54300        1\n9  RGI60-01.00009 G212400E63659N 20090703 -9999999 -147.6000 63.65900        1\n10 RGI60-01.00010 G212830E63513N 20090703 -9999999 -147.1700 63.51300        1\nO2Region   Area Zmin Zmax Zmed Slope Aspect  Lmax Status Connect Form\n1         2  0.360 1936 2725 2385    42    346   839      0       0    0\n2         2  0.558 1713 2144 2005    16    162  1197      0       0    0\n3         2  1.685 1609 2182 1868    18    175  2106      0       0    0\n4         2  3.681 1273 2317 1944    19    195  4175      0       0    0\n5         2  2.573 1494 2317 1914    16    181  2981      0       0    0\n6         2 10.470 1201 3547 1740    22     33 10518      0       0    0\n7         2  0.649 1918 2811 2194    23    151  1818      0       0    0\n8         2  0.200 2826 3555 3195    45     80   613      0       0    0\n9         2  1.517 1750 2514 1977    18    274  2255      0       0    0\n10        2  3.806 1280 1998 1666    17     35  3332      0       0    0\nTermType Surging Linkages Name                       geometry\n1         0       9        9 <NA> POLYGON ((-146.818 63.69081...\n2         0       9        9 <NA> POLYGON ((-146.6635 63.4076...\n3         0       9        9 <NA> POLYGON ((-146.0723 63.3834...\n4         0       9        9 <NA> POLYGON ((-146.149 63.37919...\n5         0       9        9 <NA> POLYGON ((-147.0431 63.5502...\n6         0       9        9 <NA> POLYGON ((-146.2436 63.5562...\n7         0       9        9 <NA> POLYGON ((-146.2495 63.5531...\n8         0       9        9 <NA> POLYGON ((-146.2992 63.5443...\n9         0       9        9 <NA> POLYGON ((-147.6147 63.6643...\n10        0       9        9 <NA> POLYGON ((-147.1494 63.5098..."
  },
  {
    "objectID": "r/gis_mapping_slides.html#structure-of-the-data",
    "href": "r/gis_mapping_slides.html#structure-of-the-data",
    "title": "GIS mapping in R",
    "section": "Structure of the data",
    "text": "Structure of the data\nstr(ak)\n\n[Out]\n\nClasses ‚Äòsf‚Äô and 'data.frame':  27108 obs. of  23 variables:\n$ RGIId   : chr  \"RGI60-01.00001\" \"RGI60-01.00002\" \"RGI60-01.00003\" ...\n$ GLIMSId : chr  \"G213177E63689N\" \"G213332E63404N\" \"G213920E63376N\" ...\n$ BgnDate : chr  \"20090703\" \"20090703\" \"20090703\" \"20090703\" ...\n$ EndDate : chr  \"-9999999\" \"-9999999\" \"-9999999\" \"-9999999\" ...\n$ CenLon  : num  -147 -147 -146 -146 -147 ...\n$ CenLat  : num  63.7 63.4 63.4 63.4 63.6 ...\n$ O1Region: chr  \"1\" \"1\" \"1\" \"1\" ...\n$ O2Region: chr  \"2\" \"2\" \"2\" \"2\" ...\n$ Area    : num  0.36 0.558 1.685 3.681 2.573 ...\n$ Zmin    : int  1936 1713 1609 1273 1494 1201 1918 2826 1750 1280 ...\n$ Zmax    : int  2725 2144 2182 2317 2317 3547 2811 3555 2514 1998 ...\n$ Zmed    : int  2385 2005 1868 1944 1914 1740 2194 3195 1977 1666 ...\n$ Slope   : num  42 16 18 19 16 22 23 45 18 17 ...\n$ Aspect  : int  346 162 175 195 181 33 151 80 274 35 ...\n$ Lmax    : int  839 1197 2106 4175 2981 10518 1818 613 2255 3332 ...\n$ Status  : int  0 0 0 0 0 0 0 0 0 0 ...\n$ Connect : int  0 0 0 0 0 0 0 0 0 0 ...\n$ Form    : int  0 0 0 0 0 0 0 0 0 0 ...\n$ TermType: int  0 0 0 0 0 0 0 0 0 0 ...\n$ Surging : int  9 9 9 9 9 9 9 9 9 9 ...\n$ Linkages: int  9 9 9 9 9 9 9 9 9 9 ...\n$ Name    : chr  NA NA NA NA ...\n$ geometry:sfc_POLYGON of length 27108; first list element: List of 1\n..$ : num [1:65, 1:2] -147 -147 -147 -147 -147 ...\n..- attr(*, \"class\")= chr [1:3] \"XY\" \"POLYGON\" \"sfg\"\n- attr(*, \"sf_column\")= chr \"geometry\"\n- attr(*, \"agr\")= Factor w/ 3 levels \"constant\",\"aggregate\",..: NA NA NA ...\n..- attr(*, \"names\")= chr [1:22] \"RGIId\" \"GLIMSId\" \"BgnDate\" \"EndDate\" ..."
  },
  {
    "objectID": "r/gis_mapping_slides.html#inspect-your-data",
    "href": "r/gis_mapping_slides.html#inspect-your-data",
    "title": "GIS mapping in R",
    "section": "Inspect your data",
    "text": "Inspect your data\n\n\n\nYour turn:\n\nInspect the wes object you created."
  },
  {
    "objectID": "r/gis_mapping_slides.html#glacier-national-park-dataset",
    "href": "r/gis_mapping_slides.html#glacier-national-park-dataset",
    "title": "GIS mapping in R",
    "section": "Glacier National Park dataset",
    "text": "Glacier National Park dataset\n\nThis dataset contains a time series of the retreat of 39 glaciers of Glacier National Park, MT, USA\nfor the years 1966, 1998, 2005 & 2015\nYou can download and unzip the 4 sets of files from the USGS website"
  },
  {
    "objectID": "r/gis_mapping_slides.html#read-in-and-clean-datasets",
    "href": "r/gis_mapping_slides.html#read-in-and-clean-datasets",
    "title": "GIS mapping in R",
    "section": "Read in and clean datasets",
    "text": "Read in and clean datasets\n## create a function that reads and cleans the data\nprep <- function(dir) {\n  g <- st_read(dir)\n  g %<>% rename_with(~ tolower(gsub(\"Area....\", \"area\", .x)))\n  g %<>% dplyr::select(\n    year,\n    objectid,\n    glacname,\n    area,\n    shape_leng,\n    x_coord,\n    y_coord,\n    source_sca,\n    source\n  )\n}\n\n## create a vector of dataset names\ndirs <- grep(\"data/GNPglaciers_.*\", list.dirs(), value = T)\n\n## pass each element of that vector through prep() thanks to map()\ngnp <- map(dirs, prep)\n\nWe use dplyr::select because terra also has a select function"
  },
  {
    "objectID": "r/gis_mapping_slides.html#combine-datasets-into-one-sf-object",
    "href": "r/gis_mapping_slides.html#combine-datasets-into-one-sf-object",
    "title": "GIS mapping in R",
    "section": "Combine datasets into one sf object",
    "text": "Combine datasets into one sf object\n\nCheck that the CRS are all the same:\nall(sapply(\n  list(st_crs(gnp[[1]]),\n       st_crs(gnp[[2]]),\n       st_crs(gnp[[3]]),\n       st_crs(gnp[[4]])),\n  function(x) x == st_crs(gnp[[1]])\n))\n\n[Out]\n\n[1] TRUE"
  },
  {
    "objectID": "r/gis_mapping_slides.html#combine-datasets-into-one-sf-object-1",
    "href": "r/gis_mapping_slides.html#combine-datasets-into-one-sf-object-1",
    "title": "GIS mapping in R",
    "section": "Combine datasets into one sf object",
    "text": "Combine datasets into one sf object\n\nWe can rbind the elements of our list:\ngnp <- do.call(\"rbind\", gnp)\nYou can inspect your new sf object by calling it or with str"
  },
  {
    "objectID": "r/gis_mapping_slides.html#estimate-for-ice-thickness",
    "href": "r/gis_mapping_slides.html#estimate-for-ice-thickness",
    "title": "GIS mapping in R",
    "section": "Estimate for ice thickness",
    "text": "Estimate for ice thickness\n\nThis dataset contains an estimate for the ice thickness of all glaciers on Earth\nThe nomenclature follows the Randolph Glacier Inventory\nIce thickness being a spatial field, this is raster data\nWe will use data in RGI60-02.16664_thickness.tif from the ETH Z√ºrich Research Collection which corresponds to one of the glaciers (Agassiz) of Glacier National Park"
  },
  {
    "objectID": "r/gis_mapping_slides.html#load-raster-data",
    "href": "r/gis_mapping_slides.html#load-raster-data",
    "title": "GIS mapping in R",
    "section": "Load raster data",
    "text": "Load raster data\n\nRead in data and create a SpatRaster object:\nras <- rast(\"data/RGI60-02/RGI60-02.16664_thickness.tif\")"
  },
  {
    "objectID": "r/gis_mapping_slides.html#inspect-our-spatraster-object",
    "href": "r/gis_mapping_slides.html#inspect-our-spatraster-object",
    "title": "GIS mapping in R",
    "section": "Inspect our SpatRaster object",
    "text": "Inspect our SpatRaster object\n\nras\n\n[Out]\n\nclass       : SpatRaster \ndimensions  : 93, 74, 1  (nrow, ncol, nlyr)\nresolution  : 25, 25  (x, y)\nextent      : 707362.5, 709212.5, 5422962, 5425288  (xmin, xmax, ymin, ymax)\ncoord. ref. : +proj=utm +zone=11 +datum=WGS84 +units=m +no_defs \nsource      : RGI60-02.16664_thickness.tif \nname        : RGI60-02.16664_thickness \nnlyr gives us the number of bands (a single one here). You can also run str(ras)"
  },
  {
    "objectID": "r/gis_mapping_slides.html#our-data",
    "href": "r/gis_mapping_slides.html#our-data",
    "title": "GIS mapping in R",
    "section": "Our data",
    "text": "Our data\n\nWe now have 3 sf objects & 1 SpatRaster object:\n\nak: ‚ÄÉcontour of glaciers in AK\nwes: ‚ÄÇcontour of glaciers in the rest of Western North America\ngnp: ‚ÄÇtime series of 39 glaciers in Glacier National Park, MT, USA\nras: ‚ÄÇice thickness of the Agassiz Glacier from Glacier National Park"
  },
  {
    "objectID": "r/gis_mapping_slides.html#lets-map-our-sf-object-ak",
    "href": "r/gis_mapping_slides.html#lets-map-our-sf-object-ak",
    "title": "GIS mapping in R",
    "section": "Let‚Äôs map our sf object ak",
    "text": "Let‚Äôs map our sf object ak\nAt a bare minimum, we need tm_shape with the data & some info as to how to represent that data:\ntm_shape(ak) +\n  tm_polygons()"
  },
  {
    "objectID": "r/gis_mapping_slides.html#we-need-to-label-customize-it",
    "href": "r/gis_mapping_slides.html#we-need-to-label-customize-it",
    "title": "GIS mapping in R",
    "section": "We need to label & customize it",
    "text": "We need to label & customize it\n\ntm_shape(ak) +\n  tm_polygons() +\n  tm_layout(\n    title = \"Glaciers of Alaska\",\n    title.position = c(\"center\", \"top\"),\n    title.size = 1.1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.06, 0.01, 0.09, 0.01),\n    outer.margins = 0,\n    frame.lwd = 0.2\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    size = 1.2,\n    text.size = 0.6\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 500, 1000),\n    position = c(\"right\", \"BOTTOM\")\n  )"
  },
  {
    "objectID": "r/gis_mapping_slides.html#make-a-map-of-the-wes-object",
    "href": "r/gis_mapping_slides.html#make-a-map-of-the-wes-object",
    "title": "GIS mapping in R",
    "section": "Make a map of the wes object",
    "text": "Make a map of the wes object\n\n\n\nYour turn:\n\nMake a map with the wes object you created with the data for Western North America excluding AK"
  },
  {
    "objectID": "r/gis_mapping_slides.html#now-lets-make-a-map-with-ak-wes",
    "href": "r/gis_mapping_slides.html#now-lets-make-a-map-with-ak-wes",
    "title": "GIS mapping in R",
    "section": "Now, let‚Äôs make a map with ak & wes",
    "text": "Now, let‚Äôs make a map with ak & wes\n\n\nThe Coordinate Reference Systems (CRS) must be the same\n\n\nsf has a function to retrieve the CRS of an sf object: st_crs\n\n\nst_crs(ak) == st_crs(wes)\n\n[Out]\n\n[1] TRUE\n\n\nSo we‚Äôre good (we will see later what to do if this is not the case)"
  },
  {
    "objectID": "r/gis_mapping_slides.html#our-combined-map",
    "href": "r/gis_mapping_slides.html#our-combined-map",
    "title": "GIS mapping in R",
    "section": "Our combined map",
    "text": "Our combined map\nLet‚Äôs start again with a minimum map without any layout to test things out:\ntm_shape(ak) +\n  tm_polygons() +\n  tm_shape(wes) +\n  tm_polygons()\n\n\nUh ‚Ä¶ oh ‚Ä¶"
  },
  {
    "objectID": "r/gis_mapping_slides.html#what-went-wrong",
    "href": "r/gis_mapping_slides.html#what-went-wrong",
    "title": "GIS mapping in R",
    "section": "What went wrong?",
    "text": "What went wrong?\n\nMaps are bound by ‚Äúbounding boxes‚Äù. In tmap, they are called bbox\ntmap sets the bbox the first time tm_shape is called. In our case, the bbox was thus set to the bbox of the ak object\nWe need to create a new bbox for our new map"
  },
  {
    "objectID": "r/gis_mapping_slides.html#retrieving-bounding-boxes",
    "href": "r/gis_mapping_slides.html#retrieving-bounding-boxes",
    "title": "GIS mapping in R",
    "section": "Retrieving bounding boxes",
    "text": "Retrieving bounding boxes\n\nsf has a function to retrieve the bbox of an sf object: st_bbox\nThe bbox of ak is:\nst_bbox(ak)\n\n[Out]\n\nxmin         ymin       xmax         ymax\n-176.14247   52.05727   -126.85450   69.35167"
  },
  {
    "objectID": "r/gis_mapping_slides.html#combining-bounding-boxes",
    "href": "r/gis_mapping_slides.html#combining-bounding-boxes",
    "title": "GIS mapping in R",
    "section": "Combining bounding boxes",
    "text": "Combining bounding boxes\n\nbbox objects can‚Äôt be combined directly\nHere is how we can create a new bbox encompassing both of our bboxes:\n\nFirst, we transform our bboxes to sfc objects with st_as_sfc\nThen we combine those objects into a new sfc object with st_union\nFinally, we retrieve the bbox of that object with st_bbox:\n\nnwa_bbox <- st_bbox(\n  st_union(\n    st_as_sfc(st_bbox(wes)),\n    st_as_sfc(st_bbox(ak))\n  )\n)"
  },
  {
    "objectID": "r/gis_mapping_slides.html#back-to-our-map",
    "href": "r/gis_mapping_slides.html#back-to-our-map",
    "title": "GIS mapping in R",
    "section": "Back to our map",
    "text": "Back to our map\nWe can now use our new bounding box for the map of Western North America:\ntm_shape(ak, bbox = nwa_bbox) +\n  tm_polygons() +\n  tm_shape(wes) +\n  tm_polygons() +\n  tm_layout(\n    title = \"Glaciers of Western North America\",\n    title.position = c(\"center\", \"top\"),\n    title.size = 1.1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.06, 0.01, 0.09, 0.01),\n    outer.margins = 0,\n    frame.lwd = 0.2\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    size = 1.2,\n    text.size = 0.6\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 1000, 2000),\n    position = c(\"right\", \"BOTTOM\")\n  )"
  },
  {
    "objectID": "r/gis_mapping_slides.html#lets-add-a-basemap",
    "href": "r/gis_mapping_slides.html#lets-add-a-basemap",
    "title": "GIS mapping in R",
    "section": "Let‚Äôs add a basemap",
    "text": "Let‚Äôs add a basemap\n\nWe will use data from Natural Earth, a public domain map dataset\nThere are much more fancy options, but they usually involve creating accounts (e.g.¬†with Google) to access some API\nIn addition, this dataset can be accessed direction from within R thanks to the rOpenSci packages:\n\nrnaturalearth: provides the functions\nrnaturalearthdata: provides the data"
  },
  {
    "objectID": "r/gis_mapping_slides.html#create-an-sf-object-with-statesprovinces",
    "href": "r/gis_mapping_slides.html#create-an-sf-object-with-statesprovinces",
    "title": "GIS mapping in R",
    "section": "Create an sf object with states/provinces",
    "text": "Create an sf object with states/provinces\n\nstates_all <- ne_states(\n  country = c(\"canada\", \"united states of america\"),\n  returnclass = \"sf\"\n)\n\nne_ stands for ‚ÄúNatural Earth‚Äù"
  },
  {
    "objectID": "r/gis_mapping_slides.html#select-relevant-statesprovinces",
    "href": "r/gis_mapping_slides.html#select-relevant-statesprovinces",
    "title": "GIS mapping in R",
    "section": "Select relevant states/provinces",
    "text": "Select relevant states/provinces\n\nstates <- states_all %>%\n  filter(name_en == \"Alaska\" |\n           name_en == \"British Columbia\" |\n           name_en == \"Yukon\" |\n           name_en == \"Northwest Territories\" |\n           name_en ==  \"Alberta\" |\n           name_en == \"California\" |\n           name_en == \"Washington\" |\n           name_en == \"Oregon\" |\n           name_en == \"Idaho\" |\n           name_en == \"Montana\" |\n           name_en == \"Wyoming\" |\n           name_en == \"Colorado\" |\n           name_en == \"Nevada\" |\n           name_en == \"Utah\"\n         )"
  },
  {
    "objectID": "r/gis_mapping_slides.html#add-the-basemap-to-our-map",
    "href": "r/gis_mapping_slides.html#add-the-basemap-to-our-map",
    "title": "GIS mapping in R",
    "section": "Add the basemap to our map",
    "text": "Add the basemap to our map\n\n\n\nWhat do we need to make sure of first?\n\n\n\nst_crs(states) == st_crs(ak)\n\n[Out]\n\n[1] TRUE"
  },
  {
    "objectID": "r/gis_mapping_slides.html#add-the-basemap-to-our-map-1",
    "href": "r/gis_mapping_slides.html#add-the-basemap-to-our-map-1",
    "title": "GIS mapping in R",
    "section": "Add the basemap to our map",
    "text": "Add the basemap to our map\n\nWe add the basemap as a 3rd layer\nMind the order! If you put the basemap last, it will cover your data\nOf course, we will use our nwa_bbox bounding box again\nWe will also break tm_polygons into tm_borders and tm_fill for ak and wes in order to colourise them with slightly different colours"
  },
  {
    "objectID": "r/gis_mapping_slides.html#add-the-basemap-to-our-map-2",
    "href": "r/gis_mapping_slides.html#add-the-basemap-to-our-map-2",
    "title": "GIS mapping in R",
    "section": "Add the basemap to our map",
    "text": "Add the basemap to our map\n\ntm_shape(states, bbox = nwa_bbox) +\n  tm_polygons(col = \"#f2f2f2\", lwd = 0.2) +\n  tm_shape(ak) +\n  tm_borders(col = \"#3399ff\") +\n  tm_fill(col = \"#86baff\") +\n  tm_shape(wes) +\n  tm_borders(col = \"#3399ff\") +\n  tm_fill(col = \"#86baff\") +\n  tm_layout(\n    title = \"Glaciers of Western North America\",\n    title.position = c(\"center\", \"top\"),\n    title.size = 1.1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.06, 0.01, 0.09, 0.01),\n    outer.margins = 0,\n    frame.lwd = 0.2\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    size = 1.2,\n    text.size = 0.6\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 1000, 2000),\n    position = c(\"right\", \"BOTTOM\")\n  )"
  },
  {
    "objectID": "r/gis_mapping_slides.html#tmap-styles",
    "href": "r/gis_mapping_slides.html#tmap-styles",
    "title": "GIS mapping in R",
    "section": "tmap styles",
    "text": "tmap styles\n\ntmap has a number of styles that you can try\nFor instance, to set the style to ‚Äúclassic‚Äù, run the following before making your map:\ntmap_style(\"classic\")\n\nOther options are: \n‚Äúwhite‚Äù (default), ‚Äúgray‚Äù, ‚Äúnatural‚Äù, ‚Äúcobalt‚Äù, ‚Äúcol_blind‚Äù, ‚Äúalbatross‚Äù, ‚Äúbeaver‚Äù, ‚Äúbw‚Äù, ‚Äúwatercolor‚Äù"
  },
  {
    "objectID": "r/gis_mapping_slides.html#tmap-styles-1",
    "href": "r/gis_mapping_slides.html#tmap-styles-1",
    "title": "GIS mapping in R",
    "section": "tmap styles",
    "text": "tmap styles\n\nTo return to the default, you need to run\ntmap_style(\"white\")\nor\ntmap_options_reset()\nwhich will reset every tmap option"
  },
  {
    "objectID": "r/gis_mapping_slides.html#first-lets-map-it",
    "href": "r/gis_mapping_slides.html#first-lets-map-it",
    "title": "GIS mapping in R",
    "section": "First, let‚Äôs map it",
    "text": "First, let‚Äôs map it\nLet‚Äôs use the same tm_borders and tm_fill we just used:\ntm_shape(gnp) +\n  tm_borders(col = \"#3399ff\") +\n  tm_fill(col = \"#86baff\") +\n  tm_layout(\n    title = \"Glaciers of Glacier National Park\",\n    title.position = c(\"center\", \"top\"),\n    legend.title.color = \"#fcfcfc\",\n    legend.text.size = 1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.07, 0.03, 0.07, 0.03),\n    outer.margins = 0\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    text.size = 0.7\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 10, 20),\n    position = c(\"right\", \"BOTTOM\"),\n    text.size = 1\n  )"
  },
  {
    "objectID": "r/gis_mapping_slides.html#create-an-inset-map",
    "href": "r/gis_mapping_slides.html#create-an-inset-map",
    "title": "GIS mapping in R",
    "section": "Create an inset map",
    "text": "Create an inset map\n\nAs always, first we check that the CRS are the same:\nst_crs(gnp) == st_crs(ak)\n\n[Out]\n\n[1] FALSE\n\nAH!"
  },
  {
    "objectID": "r/gis_mapping_slides.html#crs-transformation",
    "href": "r/gis_mapping_slides.html#crs-transformation",
    "title": "GIS mapping in R",
    "section": "CRS transformation",
    "text": "CRS transformation\n\nWe need to reproject gnp into the CRS of our other sf objects (e.g.¬†ak):\ngnp <- st_transform(gnp, st_crs(ak))\n\nWe can verify that the CRS are now the same:\nst_crs(gnp) == st_crs(ak)\n\n[Out]\n\n[1] TRUE"
  },
  {
    "objectID": "r/gis_mapping_slides.html#inset-maps-first-step",
    "href": "r/gis_mapping_slides.html#inset-maps-first-step",
    "title": "GIS mapping in R",
    "section": "Inset maps: first step",
    "text": "Inset maps: first step\nAdd a rectangle showing the location of the GNP map in the main North America map\nWe need to create a new sfc object from the gnp bbox so that we can add it to our previous map as a new layer:\ngnp_zone <- st_bbox(gnp) %>%\n  st_as_sfc()"
  },
  {
    "objectID": "r/gis_mapping_slides.html#inset-maps-second-step",
    "href": "r/gis_mapping_slides.html#inset-maps-second-step",
    "title": "GIS mapping in R",
    "section": "Inset maps: second step",
    "text": "Inset maps: second step\nCreate a tmap object of the main map. Of course, we need to edit the title. Also, note the presence of our new layer:lala\nmain_map <- tm_shape(states, bbox = nwa_bbox) +\n  tm_polygons(col = \"#f2f2f2\", lwd = 0.2) +\n  tm_shape(ak) +\n  tm_borders(col = \"#3399ff\") +\n  tm_fill(col = \"#86baff\") +\n  tm_shape(wes) +\n  tm_borders(col = \"#3399ff\") +\n  tm_fill(col = \"#86baff\") +\n  tm_shape(gnp_zone) +\n  tm_borders(lwd = 1.5, col = \"#ff9900\") +\n  tm_layout(\n    title = \"Glaciers of Glacier National Park\",\n    title.position = c(\"center\", \"top\"),\n    title.size = 1.1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.06, 0.01, 0.09, 0.01),\n    outer.margins = 0,\n    frame.lwd = 0.2\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    size = 1.2,\n    text.size = 0.6\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 500, 1000),\n    position = c(\"right\", \"BOTTOM\")\n  )"
  },
  {
    "objectID": "r/gis_mapping_slides.html#inset-maps-third-step",
    "href": "r/gis_mapping_slides.html#inset-maps-third-step",
    "title": "GIS mapping in R",
    "section": "Inset maps: third step",
    "text": "Inset maps: third step\nCreate a tmap object of the inset map\nWe make sure to matching colours & edit the layouts for better readability:\ninset_map <- tm_shape(gnp) +\n  tm_borders(col = \"#3399ff\") +\n  tm_fill(col = \"#86baff\") +\n  tm_layout(\n    legend.show = F,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.03, 0.03, 0.03, 0.03),\n    outer.margins = 0,\n    frame = \"#ff9900\",\n    frame.lwd = 3\n  )"
  },
  {
    "objectID": "r/gis_mapping_slides.html#inset-maps-final-step",
    "href": "r/gis_mapping_slides.html#inset-maps-final-step",
    "title": "GIS mapping in R",
    "section": "Inset maps: final step",
    "text": "Inset maps: final step\nCombine the two tmap objects\nWe print the main map & add the inset map with grid::viewport:\nmain_map\nprint(inset_map, vp = viewport(0.41, 0.26, width = 0.5, height = 0.5))"
  },
  {
    "objectID": "r/gis_mapping_slides.html#map-of-the-agassiz-glacier",
    "href": "r/gis_mapping_slides.html#map-of-the-agassiz-glacier",
    "title": "GIS mapping in R",
    "section": "Map of the Agassiz Glacier",
    "text": "Map of the Agassiz Glacier\nSelect the data points corresponding to the Agassiz Glacier:\nag <- gnp %>% filter(glacname == \"Agassiz Glacier\")"
  },
  {
    "objectID": "r/gis_mapping_slides.html#map-of-the-agassiz-glacier-1",
    "href": "r/gis_mapping_slides.html#map-of-the-agassiz-glacier-1",
    "title": "GIS mapping in R",
    "section": "Map of the Agassiz Glacier",
    "text": "Map of the Agassiz Glacier\ntm_shape(ag) +\n  tm_polygons() +\n  tm_layout(\n    title = \"Agassiz Glacier\",\n    title.position = c(\"center\", \"top\"),\n    legend.position = c(\"left\", \"bottom\"),\n    legend.title.color = \"#fcfcfc\",\n    legend.text.size = 1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.07, 0.03, 0.07, 0.03),\n    outer.margins = 0\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    text.size = 0.7\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 0.5, 1),\n    position = c(\"right\", \"BOTTOM\"),\n    text.size = 1\n  )\n\n\nNot great ‚Ä¶"
  },
  {
    "objectID": "r/gis_mapping_slides.html#map-based-on-attribute-variables",
    "href": "r/gis_mapping_slides.html#map-based-on-attribute-variables",
    "title": "GIS mapping in R",
    "section": "Map based on attribute variables",
    "text": "Map based on attribute variables\ntm_shape(ag) +\n  tm_polygons(\"year\", palette = \"Blues\") +\n  tm_layout(\n    title = \"Agassiz Glacier\",\n    title.position = c(\"center\", \"top\"),\n    legend.position = c(\"left\", \"bottom\"),\n    legend.title.color = \"#fcfcfc\",\n    legend.text.size = 1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.07, 0.03, 0.07, 0.03),\n    outer.margins = 0\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    text.size = 0.7\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 0.5, 1),\n    position = c(\"right\", \"BOTTOM\"),\n    text.size = 1\n  )"
  },
  {
    "objectID": "r/gis_mapping_slides.html#using-ggplot2-instead-of-tmap",
    "href": "r/gis_mapping_slides.html#using-ggplot2-instead-of-tmap",
    "title": "GIS mapping in R",
    "section": "Using ggplot2 instead of tmap",
    "text": "Using ggplot2 instead of tmap\nAs an alternative to tmap, ggplot2 can plot maps with the geom_sf function:\nggplot(ag) +\n  geom_sf(aes(fill = year)) +\n  scale_fill_brewer(palette = \"Blues\") +\n  labs(title = \"Agassiz Glacier\") +\n  annotation_scale(location = \"bl\", width_hint = 0.4) +\n  annotation_north_arrow(location = \"tr\", which_north = \"true\",\n                         pad_x = unit(0.75, \"in\"), pad_y = unit(0.5, \"in\"),\n                         style = north_arrow_fancy_orienteering) +\n  theme_bw() +\n  theme(plot.title = element_text(hjust = 0.5))\nThe package ggspatial adds a lot of functionality to ggplot2 for spatial data"
  },
  {
    "objectID": "r/gis_mapping_slides.html#faceted-map-of-the-retreat-of-agassiz",
    "href": "r/gis_mapping_slides.html#faceted-map-of-the-retreat-of-agassiz",
    "title": "GIS mapping in R",
    "section": "Faceted map of the retreat of Agassiz",
    "text": "Faceted map of the retreat of Agassiz\ntm_shape(ag) +\n  tm_polygons(col = \"#86baff\") +\n  tm_layout(\n    main.title = \"Agassiz Glacier\",\n    main.title.position = c(\"center\", \"top\"),\n    main.title.size = 1.2,\n    legend.position = c(\"left\", \"bottom\"),\n    legend.title.color = \"#fcfcfc\",\n    legend.text.size = 1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0, 0.03, 0, 0.03),\n    outer.margins = 0,\n    panel.label.bg.color = \"#fcfcfc\",\n    frame = F,\n    asp = 0.6\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    size = 1,\n    text.size = 0.6\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 0.5, 1),\n    position = c(\"right\", \"BOTTOM\"),\n    text.size = 0.6\n  ) +\n  tm_facets(\n    by = \"year\",\n    free.coords = F,\n    ncol = 4\n  )"
  },
  {
    "objectID": "r/gis_mapping_slides.html#animated-map-of-the-retreat-of-agassiz",
    "href": "r/gis_mapping_slides.html#animated-map-of-the-retreat-of-agassiz",
    "title": "GIS mapping in R",
    "section": "Animated map of the Retreat of Agassiz",
    "text": "Animated map of the Retreat of Agassiz\nFirst, we need to create a tmap object with facets:\nagassiz_anim <- tm_shape(ag) +\n  tm_polygons(col = \"#86baff\") +\n  tm_layout(\n    title = \"Agassiz Glacier\",\n    title.position = c(\"center\", \"top\"),\n    legend.position = c(\"left\", \"bottom\"),\n    legend.title.color = \"#fcfcfc\",\n    legend.text.size = 1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.08, 0, 0.08, 0),\n    outer.margins = 0,\n    panel.label.bg.color = \"#fcfcfc\"\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    text.size = 0.7\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 0.5, 1),\n    position = c(\"right\", \"BOTTOM\"),\n    text.size = 1\n  ) +\n  tm_facets(\n    along = \"year\",\n    free.coords = F\n  )"
  },
  {
    "objectID": "r/gis_mapping_slides.html#animated-map-of-the-retreat-of-agassiz-1",
    "href": "r/gis_mapping_slides.html#animated-map-of-the-retreat-of-agassiz-1",
    "title": "GIS mapping in R",
    "section": "Animated map of the Retreat of Agassiz",
    "text": "Animated map of the Retreat of Agassiz\nThen we can pass that object to tmap_animation:\ntmap_animation(\n  agassiz_anim,\n  filename = \"ag.gif\",\n  dpi = 300,\n  inner.margins = c(0.08, 0, 0.08, 0),\n  delay = 100\n)"
  },
  {
    "objectID": "r/gis_mapping_slides.html#map-of-ice-thickness-of-agassiz",
    "href": "r/gis_mapping_slides.html#map-of-ice-thickness-of-agassiz",
    "title": "GIS mapping in R",
    "section": "Map of ice thickness of Agassiz",
    "text": "Map of ice thickness of Agassiz\nNow, let‚Äôs map the estimated ice thickness on Agassiz Glacier\nThis time, we use tm_raster:\ntm_shape(ras) +\n  tm_raster(title = \"\") +\n  tm_layout(\n    title = \"Ice thickness (m) of Agassiz Glacier\",\n    title.position = c(\"center\", \"top\"),\n    legend.position = c(\"left\", \"bottom\"),\n    legend.bg.color = \"#ffffff\",\n    legend.text.size = 1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.07, 0.03, 0.07, 0.03),\n    outer.margins = 0\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    text.size = 0.7\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 0.5, 1),\n    position = c(\"right\", \"BOTTOM\"),\n    text.size = 1\n  )"
  },
  {
    "objectID": "r/gis_mapping_slides.html#combining-with-randolph-data",
    "href": "r/gis_mapping_slides.html#combining-with-randolph-data",
    "title": "GIS mapping in R",
    "section": "Combining with Randolph data",
    "text": "Combining with Randolph data\nAs always, we check whether the CRS are the same:\nst_crs(ag) == st_crs(ras)\n\n[Out]\n\n[1] FALSE\nWe need to reproject ag (remember that it is best to avoid reprojecting raster data):\nag %<>% st_transform(st_crs(ras))"
  },
  {
    "objectID": "r/gis_mapping_slides.html#combining-with-randolph-data-1",
    "href": "r/gis_mapping_slides.html#combining-with-randolph-data-1",
    "title": "GIS mapping in R",
    "section": "Combining with Randolph data",
    "text": "Combining with Randolph data\nThe retreat & ice thickness layers will hide each other (the order matters!)\nOne option is to use tm_borders for one of them, but we can also use transparency (alpha)\nWe also adjust the legend:\ntm_shape(ras) +\n  tm_raster(title = \"Ice (m)\") +\n  tm_shape(ag) +\n  tm_polygons(\"year\", palette = \"Blues\", alpha = 0.2, title = \"Contour\") +\n  tm_layout(\n    title = \"Ice thickness (m) and retreat of Agassiz Glacier\",\n    title.position = c(\"center\", \"top\"),\n    legend.position = c(\"left\", \"bottom\"),\n    legend.bg.color = \"#ffffff\",\n    legend.text.size = 0.7,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.07, 0.03, 0.07, 0.03),\n    outer.margins = 0\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    text.size = 0.7\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 0.5, 1),\n    position = c(\"right\", \"BOTTOM\"),\n    text.size = 1\n  )"
  },
  {
    "objectID": "r/gis_mapping_slides.html#refining-raster-maps",
    "href": "r/gis_mapping_slides.html#refining-raster-maps",
    "title": "GIS mapping in R",
    "section": "Refining raster maps",
    "text": "Refining raster maps\nLet‚Äôs go back to our ice thickness map:"
  },
  {
    "objectID": "r/gis_mapping_slides.html#basemap-with-ggmap",
    "href": "r/gis_mapping_slides.html#basemap-with-ggmap",
    "title": "GIS mapping in R",
    "section": "Basemap with ggmap",
    "text": "Basemap with ggmap\nbasemap <- get_map(\n  bbox = c(\n    left = st_bbox(ag)[1],\n    bottom = st_bbox(ag)[2],\n    right = st_bbox(ag)[3],\n    top = st_bbox(ag)[4]\n  ),\n  source = \"osm\"\n)\n\nggmap is a powerful package, but Google now requires an API key obtained through registration"
  },
  {
    "objectID": "r/gis_mapping_slides.html#basemap-with-basemaps",
    "href": "r/gis_mapping_slides.html#basemap-with-basemaps",
    "title": "GIS mapping in R",
    "section": "Basemap with basemaps",
    "text": "Basemap with basemaps\nThe package basemaps allows to download open source basemap data from several sources, but those cannot easily be combined with sf objects\nThis plots a satellite image of the Agassiz Glacier:\nbasemap_plot(ag, map_service = \"esri\", map_type = \"world_imagery\")"
  },
  {
    "objectID": "r/gis_mapping_slides.html#satellite-image-of-the-agassiz-glacier",
    "href": "r/gis_mapping_slides.html#satellite-image-of-the-agassiz-glacier",
    "title": "GIS mapping in R",
    "section": "Satellite image of the Agassiz Glacier",
    "text": "Satellite image of the Agassiz Glacier"
  },
  {
    "objectID": "r/gis_mapping_slides.html#mapview",
    "href": "r/gis_mapping_slides.html#mapview",
    "title": "GIS mapping in R",
    "section": "mapview",
    "text": "mapview\n\nmapview(gnp)"
  },
  {
    "objectID": "r/gis_mapping_slides.html#tmap-1",
    "href": "r/gis_mapping_slides.html#tmap-1",
    "title": "GIS mapping in R",
    "section": "tmap",
    "text": "tmap\n\nSo far, we have used the plot mode of tmap. There is also a view mode which allows interactive viewing in a browser through Leaflet\nChange to view mode:\ntmap_mode(\"view\")\n\nYou can also toggle between modes with ttm\n\nRe-plot the last map we plotted with tmap:\ntmap_last()"
  },
  {
    "objectID": "r/gis_mapping_slides.html#leaflet",
    "href": "r/gis_mapping_slides.html#leaflet",
    "title": "GIS mapping in R",
    "section": "leaflet",
    "text": "leaflet\n\nleaflet creates a map widget to which you add layers\nmap <- leaflet()\naddTiles(map)"
  },
  {
    "objectID": "r/gis_mapping_slides.html#resources-2",
    "href": "r/gis_mapping_slides.html#resources-2",
    "title": "GIS mapping in R",
    "section": "Resources",
    "text": "Resources\n\nHere are some resources on the topic to get started.\n\nR companion to Geographic Information Analysis\nSpatial data analysis"
  },
  {
    "objectID": "r/hpc_intro.html",
    "href": "r/hpc_intro.html",
    "title": "Introduction to high-performance research computing in R",
    "section": "",
    "text": "R is not known for its speed. However, with some code optimization, it can be used for relatively heavy computations. Additional speedup can be achieved through various parallel techniques, both with multi-threading and distributed computing.\nThis workshop will introduce you to working with R from the command line on the Alliance clusters with a focus on performance. We will discuss code profiling and benchmarking, various packages for parallelization, as well as using C++ from inside R to speed up your calculations."
  },
  {
    "objectID": "r/hpc_intro_slides.html#loading-modules-intel-vs-gcc-compilers",
    "href": "r/hpc_intro_slides.html#loading-modules-intel-vs-gcc-compilers",
    "title": "Introduction to high-performance research computing in R",
    "section": "Loading modules: Intel vs GCC compilers",
    "text": "Loading modules: Intel vs GCC compilers\nTo compile R packages, you need a C compiler.\nIn theory, you could use the proprietary Intel compiler which is loaded by default on the Alliance clusters, but it is recommended to replace it with the GCC compiler (R packages can even be compiled with Clang and LLVM, but the default GCC compiler is the best way to avoid headaches).\nIt is thus much simpler to always load a gcc module before loading an r module."
  },
  {
    "objectID": "r/hpc_intro_slides.html#loading-modules-r-module",
    "href": "r/hpc_intro_slides.html#loading-modules-r-module",
    "title": "Introduction to high-performance research computing in R",
    "section": "Loading modules: R module",
    "text": "Loading modules: R module\nTo see what versions of R are available on a cluster, run:\n$ module spider r\nTo see the dependencies of a particular version (e.g.¬†r/4.2.1), run:\n$ module spider r/4.2.1\n\nStdEnv/2020 is a required module for this version.\nOn most Alliance clusters, it is automatically loaded, so you don‚Äôt need to include it. You can double-check with module list or you can include it (before r/4.2.1) just to be sure.\n\nFinally, load your modules:\n$ module load StdEnv/2020 gcc/11.3.0 r/4.2.1"
  },
  {
    "objectID": "r/hpc_intro_slides.html#installing-r-packages",
    "href": "r/hpc_intro_slides.html#installing-r-packages",
    "title": "Introduction to high-performance research computing in R",
    "section": "Installing R packages",
    "text": "Installing R packages\nTo install a package, launch the interactive R console with:\n$ R\nIn the R console, run:\ninstall.packages(\"<package_name>\", repos=\"<url-cran-mirror>\")\n\nYou have to select a CRAN mirror from this list. Ideally, use a mirror close to the location of the cluster you are using or use https://cloud.r-project.org/.\nhttps://mirror.rcg.sfu.ca/mirror/CRAN/ is the closest mirror for Cedar.\n\n\nThe first time you install a package, R will ask you whether you want to create a personal library in your home directory. Answer yes to both questions. Your packages will now install under ~/.\n\n\nA handful of packages require additional modules to be loaded before they can be installed. This will be indicated in the error messages you will get when you try to install them."
  },
  {
    "objectID": "r/hpc_intro_slides.html#installing-r-packages-1",
    "href": "r/hpc_intro_slides.html#installing-r-packages-1",
    "title": "Introduction to high-performance research computing in R",
    "section": "Installing R packages",
    "text": "Installing R packages\nLet‚Äôs install the packages needed for this webinar:\ninstall.packages(\n  c(\"profvis\", \"bench\", \"doParallel\", \"furrr\", \"Rmpi\", \"Rcpp\"),\n  repos=\"https://mirror.rcg.sfu.ca/mirror/CRAN/\"\n)"
  },
  {
    "objectID": "r/hpc_intro_slides.html#running-r-jobs",
    "href": "r/hpc_intro_slides.html#running-r-jobs",
    "title": "Introduction to high-performance research computing in R",
    "section": "Running R jobs",
    "text": "Running R jobs\n\nWhile it is totally fine to run R on the login node when you install packages, you must start a SLURM job before any heavy computation.\n\nInteractive jobs\nTo run R interactively, you should launch an salloc session before launching R:\n$ salloc --time=1:00:00 --mem-per-cpu=3000M --cpus-per-task=4\n$ R"
  },
  {
    "objectID": "r/hpc_intro_slides.html#system.time",
    "href": "r/hpc_intro_slides.html#system.time",
    "title": "Introduction to high-performance research computing in R",
    "section": "system.time()",
    "text": "system.time()\nget info on efficiency\n.```{.r} system.time({\n})\n\n## Profiling\n\nThe first thing to do if you want to improve your code efficiency is to identify bottlenecks.\n\nWe will use the package [profvis](https://cran.r-project.org/web/packages/profvis/index.html):\n\n```{.r}\nlibrary(profvis)"
  },
  {
    "objectID": "r/hpc_intro_slides.html#benchmarking",
    "href": "r/hpc_intro_slides.html#benchmarking",
    "title": "Introduction to high-performance research computing in R",
    "section": "Benchmarking",
    "text": "Benchmarking\nWe will use the bench package described by Jim Hester in a Tidyverse blog post:\n\nlibrary(bench)\n\nbench::mark() compares the run time, memory usage, and garbage collections of equivalent codes."
  },
  {
    "objectID": "r/hpc_intro_slides.html#parallel-package-base-r",
    "href": "r/hpc_intro_slides.html#parallel-package-base-r",
    "title": "Introduction to high-performance research computing in R",
    "section": "parallel package (base R)",
    "text": "parallel package (base R)\nThe parallel package has been part of the ‚Äúbase‚Äù package group since version 2.14.0.\nThis means that it is comes with R and is loaded by default."
  },
  {
    "objectID": "r/hpc_intro_slides.html#furrr-package",
    "href": "r/hpc_intro_slides.html#furrr-package",
    "title": "Introduction to high-performance research computing in R",
    "section": "furrr package",
    "text": "furrr package\nThe furrr package xxx Davis Vaughan xxx in this Tidyverse blog post."
  },
  {
    "objectID": "r/hpc_intro_slides.html#doparallel-package",
    "href": "r/hpc_intro_slides.html#doparallel-package",
    "title": "Introduction to high-performance research computing in R",
    "section": "doParallel package",
    "text": "doParallel package\nand foreach package"
  },
  {
    "objectID": "r/hpc_intro_slides.html#boot-package",
    "href": "r/hpc_intro_slides.html#boot-package",
    "title": "Introduction to high-performance research computing in R",
    "section": "boot package",
    "text": "boot package"
  },
  {
    "objectID": "r/hpc_intro_slides.html#rmpi-package",
    "href": "r/hpc_intro_slides.html#rmpi-package",
    "title": "Introduction to high-performance research computing in R",
    "section": "Rmpi package",
    "text": "Rmpi package\nRmpi is a wrapper to MPI (Message-Passing Interface)"
  },
  {
    "objectID": "r/hpc_intro_slides.html#use-cases",
    "href": "r/hpc_intro_slides.html#use-cases",
    "title": "Introduction to high-performance research computing in R",
    "section": "Use cases",
    "text": "Use cases\n\nCode that can‚Äôt be parallelized\nLarge number of function calls\nData structures missing in R\nCreation of packages"
  },
  {
    "objectID": "r/hpc_intro_slides.html#functioning",
    "href": "r/hpc_intro_slides.html#functioning",
    "title": "Introduction to high-performance research computing in R",
    "section": "Functioning",
    "text": "Functioning"
  },
  {
    "objectID": "r/hpc_intro_slides.html#example",
    "href": "r/hpc_intro_slides.html#example",
    "title": "Introduction to high-performance research computing in R",
    "section": "Example",
    "text": "Example"
  },
  {
    "objectID": "r/intro_gis.html",
    "href": "r/intro_gis.html",
    "title": "Introduction to GIS in R",
    "section": "",
    "text": "This workshop is an introduction to GIS in R. We will learn how to import GIS data, explore it, and map it.\nIn particular, we will create maps (inset maps, faceted maps, animated maps, interactive maps, and raster maps), thanks to the packages sf, tmap, raster, leaflet, ggplot2, grid (part of Base R), and mapview.\nWe will also learn how to add basemaps from OpenStreetMap and Google Maps."
  },
  {
    "objectID": "r/intro_gis.html#getting-the-data",
    "href": "r/intro_gis.html#getting-the-data",
    "title": "Introduction to GIS in R",
    "section": "Getting the data",
    "text": "Getting the data\n\nDatasets\nFor this webinar, we will use:\n\nthe Alaska as well as the Western Canada and USA subsets of the Randolph Glacier Inventory version 6.01\nthe USGS time series of the named glaciers of Glacier National Park2 The datasets can be downloaded as zip files from these websites.\n\n1¬†RGI Consortium (2017). Randolph Glacier Inventory ‚Äì A Dataset of Global Glacier Outlines: Version 6.0: Technical Report, Global Land Ice Measurements from Space, Colorado, USA. Digital Media. DOI: https://doi.org/10.7265/N5-RGI-60.2¬†Fagre, D.B., McKeon, L.A., Dick, K.A., and Fountain, A.G., 2017, Glacier margin time series (1966, 1998, 2005, 2015) of the named glaciers of Glacier National Park, MT, USA: U.S. Geological Survey data release, https://doi.org/10.5066/F7P26WB1.\n\nBasemaps\nFor our basemaps, we will use data from:\n\nNatural Earth: this dataset can be accessed direction from within R thanks to the packages rnaturalearth (which provides the functions) and rnaturalearthdata (which provides the data)"
  },
  {
    "objectID": "r/intro_gis.html#loading-and-exploring-data",
    "href": "r/intro_gis.html#loading-and-exploring-data",
    "title": "Introduction to GIS in R",
    "section": "Loading and exploring data",
    "text": "Loading and exploring data\nFirst, let‚Äôs load the necessary packages for this webinar:\nlibrary(sf)\nlibrary(tmap)\nlibrary(dplyr)\nlibrary(magrittr)\nlibrary(purrr)\nlibrary(rnaturalearth)\nlibrary(rnaturalearthdata)\nlibrary(mapview)\nlibrary(grid) # part of base R (already installed), but needs to be explicitly loaded\nWe will start by mapping all the glaciers of Western North America thanks to:\n\nthe Alaska subset of the Randolph Glacier Inventory\nthe Western Canada and USA subset of the Randolph Glacier Inventory\n\nDownload and unzip 02_rgi60_WesternCanadaUS and 01_rgi60_Alaska from the Randolph Glacier Inventory version 6.0.\nData get imported and turned into sf objects by the function sf::st_read():\nak <- st_read(\"01_rgi60_Alaska\")\nwes <- st_read(\"02_rgi60_WesternCanadaUS\")\n\nMake sure to use the absolute paths or the proper paths relative to your working directory (which can be obtained with getwd() and modified with setwd()).\n\nYou can print and explore your new objects:\nak\nwes\n\nstr(ak)\nstr(wes)\nsf objects are data.frame-like objects with a geometry list-column as their last column. That column is itself an object of class sfc (simple feature geometry list column)."
  },
  {
    "objectID": "r/intro_gis.html#mapping-with-tmap",
    "href": "r/intro_gis.html#mapping-with-tmap",
    "title": "Introduction to GIS in R",
    "section": "Mapping with tmap",
    "text": "Mapping with tmap\ntmap follows a grammar of graphic similar to that of ggplot2: you first need to set a shape (a spatial data object) by passing an sf object to tm_shape(). Then you plot one or several layers with one of several tmap functions and you use the + sign between each element.\nTo see the available options, run:\n?tmap-element\nWe could thus plot the glaciers of Alaska with any of the options below:\ntm_shape(ak) +\n  tm_borders()\n\ntm_shape(ak) +\n  tm_fill()\n\ntm_shape(ak) +\n  tm_polygons()      # shows both borders and fill\nHere, we will use tm_polygons() which combines tm_borders() and tm_fill()."
  },
  {
    "objectID": "r/intro_gis.html#layout-elements-and-attribute-layers",
    "href": "r/intro_gis.html#layout-elements-and-attribute-layers",
    "title": "Introduction to GIS in R",
    "section": "Layout elements and attribute layers",
    "text": "Layout elements and attribute layers\nA map without title, compass, or scale bars is not very useful though. We need to add layout elements and attribute layers to the map.\nYou can loop up the many arguments of the tmap functions in the help pages to see how you can customize your maps:\n?tm_layout\n?tm_compass\n?tm_scale_bar\nLet‚Äôs now map the glaciers of Alaska:\ntm_shape(ak) +\n  tm_polygons() +\n  tm_layout(\n    title = \"Glaciers of Alaska\",\n    title.position = c(\"center\", \"top\"),\n    title.size = 1.1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.06, 0.01, 0.09, 0.01),\n    outer.margins = 0,\n    frame.lwd = 0.2\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    size = 1.2,\n    text.size = 0.6\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 500, 1000),\n    position = c(\"right\", \"BOTTOM\")\n  )"
  },
  {
    "objectID": "r/intro_gis.html#union-of-bounding-boxes",
    "href": "r/intro_gis.html#union-of-bounding-boxes",
    "title": "Introduction to GIS in R",
    "section": "Union of bounding boxes",
    "text": "Union of bounding boxes\nNow, if we want to plot all the glaciers of Western North America, we want to combine both sf objects in the same map. A map can contain multiple shapes: you only need to ‚Äúadd‚Äù a tm_shape and its element(s). Before doing so however, it is very important to ensure that they have the same coordinate reference system (CRS):\nst_crs(ak)\nst_crs(wes)\n\nst_crs(ak) == st_crs(wes)\nThey do, so we are good to go.\n\nAs with ggplot2 or GIS graphical user interfaces, the order matters since the layers stack up on top of each other.\n\ntm_shape(ak) +\n  tm_polygons() +\n  tm_shape(wes) +\n  tm_polygons()\nIf you run the code above however, you may be surprised that you are still only plotting the map of Alaska.\nThis is because each map comes with a spatial bounding box (bbox).\nst_bbox(ak)\nst_bbox(wes)\nIn the code above, the bbox is set by the first shape, i.e.¬†our entire map uses the bbox of the Alaska sf object.\nWe first need to create a new bounding box encompassing both bounding boxes:\nnwa_bbox <- st_bbox(\n  st_union(\n    st_as_sfc(st_bbox(wes)),\n    st_as_sfc(st_bbox(ak))\n  )\n)\nWe can now plot the glaciers of Western North America:\ntm_shape(ak, bbox = nwa_bbox) +\n  tm_polygons() +\n  tm_shape(wes) +\n  tm_polygons() +\n  tm_layout(\n    title = \"Glaciers of Western North America\",\n    title.position = c(\"center\", \"top\"),\n    title.size = 1.1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.06, 0.01, 0.09, 0.01),\n    outer.margins = 0,\n    frame.lwd = 0.2\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    size = 1.2,\n    text.size = 0.6\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 1000, 2000),\n    position = c(\"right\", \"BOTTOM\")\n  )"
  },
  {
    "objectID": "r/intro_gis.html#maps-based-on-an-attribute-variable",
    "href": "r/intro_gis.html#maps-based-on-an-attribute-variable",
    "title": "Introduction to GIS in R",
    "section": "Maps based on an attribute variable",
    "text": "Maps based on an attribute variable\nWhat is interesting about glacier maps is to see their evolution through time as glaciers retreat due to climate change. While the Randolph Glacier Inventory (RGI) has an amazing map in terms of spacial coverage, it doesn‚Äôt yet have much temporal data.\nTo look at glacier retreat, we will look at the USGS time series of the named glaciers of Glacier National Park3. These 4 datasets have the contour lines of 39 glaciers for the years 1966, 1998, 2005, and 2015.3¬†Fagre, D.B., McKeon, L.A., Dick, K.A., and Fountain, A.G., 2017, Glacier margin time series (1966, 1998, 2005, 2015) of the named glaciers of Glacier National Park, MT, USA: U.S. Geological Survey data release, https://doi.org/10.5066/F7P26WB1.\nWe could load and clean these datasets one by one. Copying and pasting code however is inefficient and error-prone. A better approach is to do this in a functional programming framework: create a function which does all the data loading and cleaning, then pass each element of a vector of the paths of all 4 datasets to it using purrr::map().\n‚ÄúCleaning‚Äù here consists of selecting the variables we are interested in, putting them in the same order in each dataset (they were not initially) and giving the exact same name across all datasets (there were case inconsistencies between datasets and R is case sensitive).\n# create a function that reads and cleans the data\nprep <- function(dir) {\n  g <- st_read(dir)\n  g %<>% rename_with(~ tolower(gsub(\"Area....\", \"area\", .x)))\n  g %<>% select(\n    year,\n    objectid,\n    glacname,\n    area,\n    shape_leng,\n    x_coord,\n    y_coord,\n    source_sca,\n    source\n  )\n}\n\n# create a vector of dataset names\ndirs <- grep(\"GNPglaciers_.*\", list.dirs(), value = T)\n\n# pass each element of that vector through prep() thanks to map()\ngnp <- map(dirs, prep)\nmap() returns a list, so we now have a list (gnp) of 4 elements: the 4 sf objects containing our cleaned datasets. A list is not really convenient and we will turn it into a single sf object.\nBefore doing so however, we want to make sure that they all have the same CRS:\nst_crs(gnp[[1]]) == st_crs(gnp[[2]])\nst_crs(gnp[[1]]) == st_crs(gnp[[3]])\nst_crs(gnp[[1]]) == st_crs(gnp[[4]])\nThey do, so we can turn gnp into a single sf object:\ngnp <- do.call(\"rbind\", gnp)\n\ngnp\nstr(gnp)\nWe can now map the data:\ntm_shape(gnp) +\n  tm_polygons(\"year\", palette = \"Blues\") +\n  tm_layout(\n    title = \"Glaciers of Glacier National Park\",\n    title.position = c(\"center\", \"top\"),\n    legend.title.color = \"#fcfcfc\",\n    legend.text.size = 1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.07, 0.03, 0.07, 0.03),\n    outer.margins = 0\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    text.size = 0.7\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 10, 20),\n    position = c(\"right\", \"BOTTOM\"),\n    text.size = 1\n  )\n\nI didn‚Äôt want to show the legend title and because there is no option to remove it, I set its color to that of the background."
  },
  {
    "objectID": "r/intro_gis.html#crs-transformation",
    "href": "r/intro_gis.html#crs-transformation",
    "title": "Introduction to GIS in R",
    "section": "CRS transformation",
    "text": "CRS transformation\nWouldn‚Äôt it be nice to have this map as an inset of the previous map so that we can situate it within North America?\nBefore we can do this, we need to make sure that both maps use the same CRS:\nst_crs(ak)\nst_crs(gnp)\n\nWe could use wes instead of ak since we know that both sf objects have the same CRS.\n\nThey don‚Äôt have the same CRS, so we reproject gnp by transforming its data from its current CRS to that of ak.\ngnp <- st_transform(gnp, st_crs(ak))\nst_crs(gnp)"
  },
  {
    "objectID": "r/intro_gis.html#inset-map",
    "href": "r/intro_gis.html#inset-map",
    "title": "Introduction to GIS in R",
    "section": "Inset map",
    "text": "Inset map\nNow we can create our map with an inset: the map of the Western North America glaciers (from the sf object nwa) will be our main map and the map of Glacier National Park (from the sf object gnp) will be the inset.\nIf the goal of this new map is to show the location of the gnp map within the nwa one, we need to add a rectangle showing the bounding box of gnp in the nwa map as a new layer.\nFor this, we create a new sfc_POLYGON from the bounding box of gnp:\ngnp_zone <- st_bbox(gnp) %>%\n  st_as_sfc()\nWe will use it as the following layer within the new map:\ntm_shape(gnp_zone) +\n  tm_borders(lwd = 1.5, col = \"#ff9900\")\nWe assign our new map (with an updated suitable title) to the object main_map:\nmain_map <- tm_shape(ak, bbox = nwa_bbox) +\n  tm_polygons() +\n  tm_shape(wes) +\n  tm_polygons() +\n  tm_shape(gnp_zone) +\n  tm_borders(lwd = 1.5, col = \"#ff9900\") +\n  tm_layout(\n    title = \"Glaciers of Glacier National Park\",\n    title.position = c(\"center\", \"top\"),\n    title.size = 1.1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.06, 0.01, 0.09, 0.01),\n    outer.margins = 0,\n    frame.lwd = 0.2\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    size = 1.2,\n    text.size = 0.6\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 500, 1000),\n    position = c(\"right\", \"BOTTOM\")\n  )\nNext, we will change the frame of the gnp inset to match the color of this new rectangle (to make it visually clear that this is a close-up view of that rectangle). We can also remove the title, compass and scale bar since this is an inset within a map which already have them. We assign this new map to the object inset_map:\ninset_map <- tm_shape(gnp) +\n  tm_polygons(\"year\", palette = \"Blues\") +\n  tm_layout(\n    legend.title.color = \"#fcfcfc\",\n    legend.text.size = 0.7,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.03, 0.03, 0.03, 0.03),\n    outer.margins = 0,\n    frame = \"#ff9900\",\n    frame.lwd = 3\n  )\nFinally, we combine the two maps with grid::viewport():\nmain_map\nprint(inset_map, vp = viewport(0.41, 0.26, width = 0.5, height = 0.5))"
  },
  {
    "objectID": "r/intro_gis.html#tiled-web-maps-with-leaflet",
    "href": "r/intro_gis.html#tiled-web-maps-with-leaflet",
    "title": "Introduction to GIS in R",
    "section": "Tiled web maps with Leaflet",
    "text": "Tiled web maps with Leaflet\nTiled web maps are interactive maps in a browser using web servers such as Google Maps or OpenStreetMap. Several packages allow to use Leaflet (an open-source JavaScript library for interactive maps) to create tile maps.\n\nWith mapview\nThe simplest option is to use mapview::mapview():\nmapview(gnp)\nThis will open a page in your browser in which you can pan, zoom, select/deselect data layers, and choose from a number of basemap layer options:\n CartoDB.Positron\n OpenTopoMap\n OpenStreetMap\n Esri.WorldImagery\n\n\nWith tmap\ntmap has similar capabilities.\nThe package has 2 modes:\n\nplot is the default mode for static maps that we used earlier.\nview is an interactive viewing mode using Leaflet in a browser. There, as with mapview, you can zoom in/out, select/deselect the different layers, and choose to display one of Esri.WorldGrayCanvas, OpenStreetMap, or Esri.WorldTopoMap basemaps.\n\nYou can toggle between the plot and view modes with ttm(), after which you can re-plot your last plot in the new mode with tmap_last(). You can also do both of these at once with ttmp().\nAlternatively, you can switch to either mode with tmap_mode(\"view\") and tmap_mode(\"plot\").\n\nExample:\n\nEarlier, we plotted all the glaciers of Western North America using tmap:\n\nAfter displaying this map, we could have run:\ntmap_mode(\"view\")\ntmap_last()\nAnd Leaflet would have open the following interactive map in our browser:\n\n\nAfterwards, if you want to create new static plots, don‚Äôt forget to get back to plot mode with tmap_mode(\"plot\")."
  },
  {
    "objectID": "r/intro_gis.html#mapping-a-subset-of-the-data",
    "href": "r/intro_gis.html#mapping-a-subset-of-the-data",
    "title": "Introduction to GIS in R",
    "section": "Mapping a subset of the data",
    "text": "Mapping a subset of the data\nEach glacier has 4 borders: one for each year of survey. They are however quite hard to see on such a large map.\nLet‚Äôs zoom on the Agassiz glacier:\n# select the data points corresponding to the Agassiz Glacier\nag <- g %>% filter(glacname == \"Agassiz Glacier\")\nAnd map it:\ntm_shape(ag) +\n  tm_polygons(\"year\", palette = \"Blues\") +\n  tm_layout(\n    title = \"Agassiz Glacier\",\n    title.position = c(\"center\", \"top\"),\n    legend.position = c(\"left\", \"bottom\"),\n    legend.title.color = \"#fcfcfc\",\n    legend.text.size = 1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.07, 0.03, 0.07, 0.03),\n    outer.margins = 0\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    text.size = 0.7\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 0.5, 1),\n    position = c(\"right\", \"BOTTOM\"),\n    text.size = 1\n  )\n\nNow we can clearly see the retreat of the Agassiz Glacier between 1966 and 2015."
  },
  {
    "objectID": "r/intro_gis.html#faceted-map",
    "href": "r/intro_gis.html#faceted-map",
    "title": "Introduction to GIS in R",
    "section": "Faceted map",
    "text": "Faceted map\nInstead of having all temporal data in a single map however, it can be split across facets:\ntm_shape(ag) +\n  tm_polygons(col = \"#86baff\") +\n  tm_layout(\n    main.title = \"Agassiz Glacier\",\n    main.title.position = c(\"center\", \"top\"),\n    main.title.size = 1.2,\n    legend.position = c(\"left\", \"bottom\"),\n    legend.title.color = \"#fcfcfc\",\n    legend.text.size = 1,\n    bg.color = \"#fcfcfc\",\n    # inner.margins = c(0, 0.03, 0, 0.03),\n    outer.margins = 0,\n    panel.label.bg.color = \"#fcfcfc\",\n    frame = F,\n    asp = 0.6\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    size = 1,\n    text.size = 0.6\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 0.5, 1),\n    position = c(\"right\", \"BOTTOM\"),\n    text.size = 0.6\n  ) +\n  tm_facets(\n    by = \"year\",\n    free.coords = F,\n    ncol = 4\n  )"
  },
  {
    "objectID": "r/intro_gis.html#animated-map",
    "href": "r/intro_gis.html#animated-map",
    "title": "Introduction to GIS in R",
    "section": "Animated map",
    "text": "Animated map\nThe temporal data of the Agassiz Glacier retreat can also be conveyed through an animation:\nagassiz_anim <- tm_shape(ag) +\n  tm_borders() +\n  tm_fill(col = \"#86baff\") +\n  tm_layout(\n    title = \"Agassiz Glacier\",\n    title.position = c(\"center\", \"top\"),\n    legend.position = c(\"left\", \"bottom\"),\n    legend.title.color = \"#fcfcfc\",\n    legend.text.size = 1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.08, 0, 0.08, 0),\n    outer.margins = 0\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    text.size = 0.7\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 0.5, 1),\n    position = c(\"right\", \"BOTTOM\"),\n    text.size = 1\n  ) +\n  tm_facets(\n    along = \"year\",\n    free.coords = F\n  )\n\ntmap_animation(\n  agassiz_anim,\n  filename = \"ag.gif\",\n  dpi = 300,\n  inner.margins = c(0.08, 0, 0.08, 0),\n  delay = 100\n)"
  },
  {
    "objectID": "r/intro_gis.html#additional-resources",
    "href": "r/intro_gis.html#additional-resources",
    "title": "Introduction to GIS in R",
    "section": "Additional resources",
    "text": "Additional resources\nOpen GIS data:\nFree GIS Data: list of free GIS datasets\nBooks\nGeocomputation with R by Robin Lovelace, Jakub Nowosad, and Jannes Muenchow\nSpatial Data Science by Edzer Pebesma, Roger Bivand\nSpatial Data Science with R by Robert J. Hijmans\nUsing Spatial Data with R by Claudia A. Engel\nTutorial\nAn Introduction to Spatial Data Analysis and Visualisation in R by the CDRC\nWebsite\nr-spatial by Edzer Pebesma, Marius Appel, and Daniel N√ºst\nCRAN package list\nAnalysis of Spatial Data\nMailing list\nR Special Interest Group on using Geographical data and Mapping"
  },
  {
    "objectID": "r/intro_hss.html",
    "href": "r/intro_hss.html",
    "title": "Introduction to R for the humanities",
    "section": "",
    "text": "R is a free and open-source programming language for statistical computing, modelling, and graphics, with an unbeatable collection of statistical packages. It is extremely popular in some academic fields such as statistics, biology, bioinformatics, data mining, data analysis, and linguistics.\nThis introductory course does not assume any prior knowledge: it will take you through the first steps of importing, cleaning, and visualizing your data. Along the way, we will get familiar with R data types, functions writing, and control flow."
  },
  {
    "objectID": "r/intro_hss.html#running-r",
    "href": "r/intro_hss.html#running-r",
    "title": "Introduction to R for the humanities",
    "section": "Running R",
    "text": "Running R\n\nRunning R interactively\nThere are several ways to run R.\n\nDirectly in the console (the name for the R shell):\n\n\n\nIn Jupyter with the R kernel (IRkernel package).\nIn the RStudio IDE.\nIn another IDE (e.g.¬†in Emacs with ESS).\n\nThe RStudio IDE is popular and this is what we will use today. RStudio can can be run locally, but for this course, we will use an RStudio server.\n\n\nRunning R non-interactively\nYou can also run R scripts with:\nRscript my_script.R\n\n\nAccessing our RStudio server\nTo access it:\n\nGo to the website given during the workshop,\nSign in using your username and password given during the workshop (you can ignore the OTP entry),\nChoose the following Server Options:\n\nTime: 3.0 hours\nNumber of cores: 1\nMemory: 3600 MB\nUser interface: JupyterLab\n\n\n\n\nIn JupyterLab, click on the RStudio button (big blue symbol with a white R in it).\n\n\n\nUsing RStudio\n \n\n\nfrom Posit Cheatsheets"
  },
  {
    "objectID": "r/packages.html",
    "href": "r/packages.html",
    "title": "Packages",
    "section": "",
    "text": "Packages are a set of functions and/or data that add functionality to R."
  },
  {
    "objectID": "r/packages.html#looking-for-packages",
    "href": "r/packages.html#looking-for-packages",
    "title": "Packages",
    "section": "Looking for packages",
    "text": "Looking for packages\n\nPackage finder\nYour peers and the literature"
  },
  {
    "objectID": "r/packages.html#package-documentation",
    "href": "r/packages.html#package-documentation",
    "title": "Packages",
    "section": "Package documentation",
    "text": "Package documentation\n\nList of CRAN packages\nPackage documentation"
  },
  {
    "objectID": "r/packages.html#managing-r-packages",
    "href": "r/packages.html#managing-r-packages",
    "title": "Packages",
    "section": "Managing R packages",
    "text": "Managing R packages\nR packages can be installed, updated, and removed from within R:\ninstall.packages(\"package-name\")\nremove.packages(\"package-name\")\nupdate_packages()"
  },
  {
    "objectID": "r/packages.html#loading-packages",
    "href": "r/packages.html#loading-packages",
    "title": "Packages",
    "section": "Loading packages",
    "text": "Loading packages\nTo make a package available in an R session, you load it with the library() function.\n\nExample:\n\nlibrary(readxl)\nAlternatively, you can access a function from a package without loading it with the syntax: package::function().\n\nExample:\n\nreadxl::read_excel(\"file.xlsx\")"
  },
  {
    "objectID": "r/resources.html",
    "href": "r/resources.html",
    "title": "Resources",
    "section": "",
    "text": "The main R website\nDownload page"
  },
  {
    "objectID": "r/resources.html#rstudio",
    "href": "r/resources.html#rstudio",
    "title": "Resources",
    "section": "RStudio",
    "text": "RStudio\n\nPosit site (Posit is the brand new name of the RStudio company)\nPosit cheatsheets"
  },
  {
    "objectID": "r/resources.html#documentation-as-pdf",
    "href": "r/resources.html#documentation-as-pdf",
    "title": "Resources",
    "section": "Documentation as pdf",
    "text": "Documentation as pdf\n\nContributed documentation\nIntro books"
  },
  {
    "objectID": "r/resources.html#software-carpentry-online-workshops",
    "href": "r/resources.html#software-carpentry-online-workshops",
    "title": "Resources",
    "section": "Software Carpentry online workshops",
    "text": "Software Carpentry online workshops\n\nProgramming with R\nR for Reproducible Scientific Analysis"
  },
  {
    "objectID": "r/resources.html#online-books",
    "href": "r/resources.html#online-books",
    "title": "Resources",
    "section": "Online books",
    "text": "Online books\n\nR for Data Science (heavily based on the tidyverse)\nAdvanced R (functioning of the language)\nR Packages (how to create packages)\nEfficient R programming (code optimization)"
  },
  {
    "objectID": "r/resources.html#r-research",
    "href": "r/resources.html#r-research",
    "title": "Resources",
    "section": "R research",
    "text": "R research\n\nThe R Journal"
  },
  {
    "objectID": "r/tidyverse.html",
    "href": "r/tidyverse.html",
    "title": "A little glimpse at the tidyverse",
    "section": "",
    "text": "The tidyverse is a set of packages which attempts to make R more consistent and more similar to programming languages which were developed by computer scientists rather than statisticians.\nYou can think of it as a more modern version of R.\nThe best introduction to the tidyverse is probably the book R for Data Science by Hadley Wickham and Garrett Grolemund."
  },
  {
    "objectID": "r/tidyverse.html#base-r-or-tidyverse",
    "href": "r/tidyverse.html#base-r-or-tidyverse",
    "title": "A little glimpse at the tidyverse",
    "section": "Base R or tidyverse?",
    "text": "Base R or tidyverse?\n‚ÄúBase R‚Äù refers to the use of the standard R library. The expression is often used in contrast to the tidyverse.\nThere are a many things that you can do with either base R or the tidyverse. Because the syntaxes are quite different, it almost feels like using two different languages and people tend to favour one or the other.\nWhich one should you use? Well, this is really up to you.\n\n\n\n\n\n\n\nBase R\nTidyverse\n\n\n\n\nPreferred by old-schoolers\nIncreasingly becoming the norm with newer R users\n\n\nMore stable\nMore consistent syntax and behaviour\n\n\nDoesn‚Äôt require installing and loading packages\nMore and more resources and documentation available\n\n\n\nIn truth, even though the tidyverse has many detractors amongst old R users, it is increasingly becoming the norm."
  },
  {
    "objectID": "r/tidyverse.html#a-glimpse-of-the-tidyverse",
    "href": "r/tidyverse.html#a-glimpse-of-the-tidyverse",
    "title": "A little glimpse at the tidyverse",
    "section": "A glimpse of the tidyverse",
    "text": "A glimpse of the tidyverse\nThis series of cheatsheet developed by Posit can give you a first glimpse at some of the tidyverse packages.\n\nData transformation\n\n\n\nfrom Posit Cheatsheets\n\n\n\n\nfrom Posit Cheatsheets\n\n\n\nWorking with factors\n\n\nfrom Posit Cheatsheets\n\n\n\nVisualization\n\n\n\nfrom Posit Cheatsheets\n\n\n\nFunctional programming\n\n\n\nfrom Posit Cheatsheets"
  },
  {
    "objectID": "r/webscraping.html",
    "href": "r/webscraping.html",
    "title": "Web scraping with R",
    "section": "",
    "text": "The internet is a trove of information. A lot of it is publicly available and thus suitable for use in research. Extracting that information and putting it in an organized format for analysis can, however, be extremely tedious. Web scraping tools allow to automate parts of that process and R is a popular language for the task.\nIn this workshop, we will guide you through a simple example using the package rvest."
  },
  {
    "objectID": "r/why.html",
    "href": "r/why.html",
    "title": "R: why and for whom?",
    "section": "",
    "text": "There are other high level programming languages such as Python or Julia, so when might it make sense for you to turn to R?\nHere are a number of good reasons:\n\nFree and open source\nUnequalled number of statistics and modelling packages\nIntegrated package manager\nEasy connection with fast compiled languages such as C and C++\nPowerful IDEs available (RStudio, Emacs ESS)"
  },
  {
    "objectID": "r/why.html#for-whom",
    "href": "r/why.html#for-whom",
    "title": "R: why and for whom?",
    "section": "For whom?",
    "text": "For whom?\nFor whom is R particularly well suited?\n\nFields with heavy statistics, modelling, or Bayesian analysis such as biology, linguistics, economics, or statistics\nData science using a lot of tabular data"
  },
  {
    "objectID": "r/why.html#downsides-of-r",
    "href": "r/why.html#downsides-of-r",
    "title": "R: why and for whom?",
    "section": "Downsides of R",
    "text": "Downsides of R\nOf course, R also has its downsides:\n\nInconsistent syntax full of quirks\nSlow\nLarge memory usage"
  },
  {
    "objectID": "tools/help.html",
    "href": "tools/help.html",
    "title": "So, you are stuck ‚Ä¶ now what?",
    "section": "",
    "text": "Stack Overflow, Stack Exchange, Discourse forums, and other online platforms ‚Ä¶ the internet is a treasure trove of online communities where you can find solutions to your coding problems. To have a positive experience and get the answers you need however, you have to know where to ask, how to ask, and when not to ask: if countless people are willing to give you their time for free, they usually expect that you do your part.\nIn this workshop, I will present key online sites, their functioning, and their culture; then I will go over the magic trick to get answers to your questions: knowing how to create minimum reproducible examples. I will not focus on any particular language as the principles (how to create a minimal dataset, how to deal with private data, how to create self-sufficient code, how to reproduce the problem, etc.) can apply to any language.\n\nSlides (click and wait: my reveal.js presentations are heavy and take some time to load‚Ä¶)"
  },
  {
    "objectID": "tools/help_slides.html#when-you-are-stuck-1",
    "href": "tools/help_slides.html#when-you-are-stuck-1",
    "title": "So, you are stuck ‚Ä¶ now what?",
    "section": "When you are stuck",
    "text": "When you are stuck\n\n\nFirst, look for information that is already out there\n\n\nThen, ask for help"
  },
  {
    "objectID": "tools/help_slides.html#look-for-information",
    "href": "tools/help_slides.html#look-for-information",
    "title": "So, you are stuck ‚Ä¶ now what?",
    "section": "Look for information",
    "text": "Look for information\n\n\nRead carefully any error message\nRead the documentation (local or online)\nMake sure you have up-to-date versions\nGoogle (using carefully selected keywords or the error message)\nLook for open issues & bug reports"
  },
  {
    "objectID": "tools/help_slides.html#error-messages",
    "href": "tools/help_slides.html#error-messages",
    "title": "So, you are stuck ‚Ä¶ now what?",
    "section": "Error messages",
    "text": "Error messages\n\n\nRead them!\n\nFamiliarise yourself with the error types in the languages you use\n\n\nExample: Python‚Äôs syntax errors vs exceptions\n\n\n\nWarnings ‚â† errors\n\n\nLook for bits you understand (don‚Äôt get put off by what you don‚Äôt understand)\n\n\nIdentify the locations of the errors to go investigate that part of the code"
  },
  {
    "objectID": "tools/help_slides.html#documentation",
    "href": "tools/help_slides.html#documentation",
    "title": "So, you are stuck ‚Ä¶ now what?",
    "section": "Documentation",
    "text": "Documentation\n\n\nYou need to find it\n\n\nYou need to understand it"
  },
  {
    "objectID": "tools/help_slides.html#finding-documentation",
    "href": "tools/help_slides.html#finding-documentation",
    "title": "So, you are stuck ‚Ä¶ now what?",
    "section": "Finding documentation",
    "text": "Finding documentation\n\nOnline:\nTake the time to look for the official documentation & other high quality sources for the languages & tools you use.\n\n\n\nExamples:\nPython: Reference manual, Standard library manual, Tutorial\nNumPy: Tutorial\nR: Open source book ‚ÄúR for Data Science‚Äù, Open source book ‚ÄúAdvanced R‚Äù\nJulia: Documentation\nBash: Manual\nGit: Manual, Open source book\n\n\n\nIn the program itself\n\n\nUnderstanding the documentation"
  },
  {
    "objectID": "tools/help_slides.html#up-to-date-versions",
    "href": "tools/help_slides.html#up-to-date-versions",
    "title": "So, you are stuck ‚Ä¶ now what?",
    "section": "Up-to-date versions",
    "text": "Up-to-date versions\n\n\nFirst, you need to know what needs to be updated.\n\n\nKeeping a system up to date includes updating:\n\nthe OS\nthe program\n(any potential IDE)\npackages\n\n\n\nThen, you need to update regularly."
  },
  {
    "objectID": "tools/help_slides.html#google",
    "href": "tools/help_slides.html#google",
    "title": "So, you are stuck ‚Ä¶ now what?",
    "section": "Google",
    "text": "Google\n\nGoogle‚Äôs algorithms are great at guessing what we are looking for.\n\nBut there is a frequency problem:\nSearches relating to programming-specific questions represent too small a fraction of the overall searches for results to be relevant unless you use key vocabulary.\n\n\nBe precise.\n\n\nLearn the vocabulary of your language/tool to know what to search for."
  },
  {
    "objectID": "tools/help_slides.html#open-issues-bug-reports",
    "href": "tools/help_slides.html#open-issues-bug-reports",
    "title": "So, you are stuck ‚Ä¶ now what?",
    "section": "Open issues & bug reports",
    "text": "Open issues & bug reports\n\nIf the tool you are using is open source, look for issues matching your problem in the source repository (e.g.¬†on GitHub or GitLab)."
  },
  {
    "objectID": "tools/help_slides.html#what-if-the-answer-isnt-out-there",
    "href": "tools/help_slides.html#what-if-the-answer-isnt-out-there",
    "title": "So, you are stuck ‚Ä¶ now what?",
    "section": "What if the answer isn‚Äôt out there?",
    "text": "What if the answer isn‚Äôt out there?\n\nWhen everything has failed & you have to ask for help, you need to know:\n\n\nWhere to ask\n\n\n\n\nHow to ask"
  },
  {
    "objectID": "tools/help_slides.html#where-to-ask-1",
    "href": "tools/help_slides.html#where-to-ask-1",
    "title": "So, you are stuck ‚Ä¶ now what?",
    "section": "Where to ask",
    "text": "Where to ask\n\nQ&A sites\nMostly, Stack Overflow & the Stack Exchange network.\nCo-founded in 2008 & 2009 by Jeff Atwood & Joel Spolsky. \nForums\nMostly, Discourse.\nCo-founded in 2013 by Jeff Atwood, Robin Ward & Sam Saffron.\nA few other older forums."
  },
  {
    "objectID": "tools/help_slides.html#where-to-ask-2",
    "href": "tools/help_slides.html#where-to-ask-2",
    "title": "So, you are stuck ‚Ä¶ now what?",
    "section": "Where to ask",
    "text": "Where to ask\n\nWhich one to choose is a matter of personal preference.\nPossible considerations:\n\nSome niche topics have very active communities on Discourse\nStack Overflow & some older forums can be intimidating with higher expectations for the questions quality & a more direct handling of mistakes\nFor conversations, advice, or multiple step questions, go to Discourse\nStack Overflow has over 13 million users\nStack Overflow & co have a very efficient approach"
  },
  {
    "objectID": "tools/help_slides.html#stack-overflow-co",
    "href": "tools/help_slides.html#stack-overflow-co",
    "title": "So, you are stuck ‚Ä¶ now what?",
    "section": "Stack Overflow & co",
    "text": "Stack Overflow & co\n\nPick the best site to ask your question.\nA few of the Stack Exchange network sites:\nStack Overflow: programming\nSuper User: computer hardware & software\nUnix & Linux: *nix OS TEX: TeX/LaTeX\nCross Validated: stats; data mining, collecting, analysis & visualization; ML\nData Science: focus on implementation & processes\nOpen Data\nGIS"
  },
  {
    "objectID": "tools/help_slides.html#how-to-ask-1",
    "href": "tools/help_slides.html#how-to-ask-1",
    "title": "So, you are stuck ‚Ä¶ now what?",
    "section": "How to ask",
    "text": "How to ask\n\nFamiliarize yourself with the site by reading posts\n\n\nRead the ‚ÄúTour‚Äù page (SO/SE) or take the ‚ÄúNew user tutorial‚Äù (Discourse)\n\n\nMake sure the question has not already been asked\n\n\nFormat the question properly\n\n\nGive a minimum reproducible example\n\n\nDo not share sensitive data\n\n\nShow your attempts\n\n\nAvoid cross-posting. If you really have to, make sure to cross-reference"
  },
  {
    "objectID": "tools/help_slides.html#how-to-ask-so-co",
    "href": "tools/help_slides.html#how-to-ask-so-co",
    "title": "So, you are stuck ‚Ä¶ now what?",
    "section": "How to ask: SO & co",
    "text": "How to ask: SO & co\n\nDon‚Äôt ask opinion-based questions\n\n\nDon‚Äôt ask for package, tool, or service recommendations\n\n\nDon‚Äôt ask more than one question in a single post\n\n\nCheck your spelling, grammar, punctuation, capitalized sentences, etc.\n\n\nAvoid greetings, signatures, thank-yous; keep it to the point\n\n\nAvoid apologies about being a beginner, this being your first post, the question being stupid, etc: do the best you can & skip the personal, self-judgmental & irrelevant bits"
  },
  {
    "objectID": "tools/help_slides.html#formatting-your-question",
    "href": "tools/help_slides.html#formatting-your-question",
    "title": "So, you are stuck ‚Ä¶ now what?",
    "section": "Formatting your question",
    "text": "Formatting your question\nNowadays, most sites (including Stack Overflow & Discourse) allow markdown rendering.\nSome older forums implement other markup languages (e.g.¬†BBCode).\nThe information is always easy to find. Spend the time to format your question properly. People will be much less inclined to help you if you don‚Äôt show any effort & if your question is a nightmare to read."
  },
  {
    "objectID": "tools/help_slides.html#example-of-a-typical-downvoted-question",
    "href": "tools/help_slides.html#example-of-a-typical-downvoted-question",
    "title": "So, you are stuck ‚Ä¶ now what?",
    "section": "Example of a typical downvoted question",
    "text": "Example of a typical downvoted question\n\nhowdy!!\ni am new to R sorry for a very silly question.i looked all oever the itnernwet, but i dint find\nanyanswer. i tried to use ggplot i get the error: Error in loadNamespace(i, c(lib.loc, .libPaths()),\nversionCheck = vI[[i]]) : there is no package called 'stringi'\nthank youu very much!!!!!\nmarie\n\n[Out]"
  },
  {
    "objectID": "tools/help_slides.html#same-question-fixed",
    "href": "tools/help_slides.html#same-question-fixed",
    "title": "So, you are stuck ‚Ä¶ now what?",
    "section": "Same question, fixed",
    "text": "Same question, fixed\nWhen I try to load the package `ggplot2` with:\n\n```{r}\nlibrary(ggplot2)\n```\nI get the error:\n\n> Error in loadNamespace(i, c(lib.loc, .libPaths()), versionCheck = vI[[i]]) :\nthere is no package called 'stringi'\n\nWhat am I doing wrong?"
  },
  {
    "objectID": "tools/help_slides.html#still-not-good-enough",
    "href": "tools/help_slides.html#still-not-good-enough",
    "title": "So, you are stuck ‚Ä¶ now what?",
    "section": "Still not good enough",
    "text": "Still not good enough\n\nThis question is actually a duplicate of a question asked which is itself a duplicate of another question."
  },
  {
    "objectID": "tools/help_slides.html#creating-a-minimal-reproducible-example",
    "href": "tools/help_slides.html#creating-a-minimal-reproducible-example",
    "title": "So, you are stuck ‚Ä¶ now what?",
    "section": "Creating a minimal reproducible example",
    "text": "Creating a minimal reproducible example\n\nThere are great posts on how to create a good minimal reproducible example. In particular:\nHow to create a Minimal, Reproducible Example  For R (but concepts apply to any language):\nHow to make a great R reproducible example\nWhat‚Äôs a reproducible example (reprex) and how do I do one?"
  },
  {
    "objectID": "tools/help_slides.html#creating-a-minimal-reproducible-example-1",
    "href": "tools/help_slides.html#creating-a-minimal-reproducible-example-1",
    "title": "So, you are stuck ‚Ä¶ now what?",
    "section": "Creating a minimal reproducible example",
    "text": "Creating a minimal reproducible example\n\n\nLoad all necessary packages\nLoad or create necessary data\nSimplify the data & the code as much as possible while still reproducing the problem\nUse simple variable names"
  },
  {
    "objectID": "tools/help_slides.html#data-for-your-example-your-own-data",
    "href": "tools/help_slides.html#data-for-your-example-your-own-data",
    "title": "So, you are stuck ‚Ä¶ now what?",
    "section": "Data for your example: your own data",
    "text": "Data for your example: your own data\n\nDo not upload data somewhere on the web to be downloaded.\nMake sure that the data is anonymised.\nDon‚Äôt keep more variables & more data points than are necessary to reproduce the problem.\nSimplify the variable names.\nIn R, you can use functions such as dput() to turn your reduced, anonymised data into text that is easy to copy/paste & can then be used to recreate the data."
  },
  {
    "objectID": "tools/help_slides.html#data-for-your-example-create-a-toy-dataset",
    "href": "tools/help_slides.html#data-for-your-example-create-a-toy-dataset",
    "title": "So, you are stuck ‚Ä¶ now what?",
    "section": "Data for your example: create a toy dataset",
    "text": "Data for your example: create a toy dataset\n\nYou can also create a toy dataset.\nFunctions that create random data, series, or repetitions are very useful here."
  },
  {
    "objectID": "tools/help_slides.html#data-for-your-example-pre-packaged-datasets",
    "href": "tools/help_slides.html#data-for-your-example-pre-packaged-datasets",
    "title": "So, you are stuck ‚Ä¶ now what?",
    "section": "Data for your example: pre-packaged datasets",
    "text": "Data for your example: pre-packaged datasets\n\nSome languages/packages come with pre-packaged datasets. If your code involves such languages/packages, you can make use of these datasets to create your reproducible example.\nFor example, R comes with many datasets directly available, including iris, mtcars, trees, airquality. In the R console, try: \n?iris\n?mtcars"
  },
  {
    "objectID": "tools/help_slides.html#additional-considerations",
    "href": "tools/help_slides.html#additional-considerations",
    "title": "So, you are stuck ‚Ä¶ now what?",
    "section": "Additional considerations",
    "text": "Additional considerations\n\nEven if you always find answers to your questions without having to post yourself, consider signing up to these sites:\n\nIt allows you to upvote (SO/SE) or like (Discourse) the questions & answers that help you‚Äîand why not thank in this fashion those that are making your life easier?\nIt makes you a part of these communities.\nOnce you are signed up, maybe you will start being more involved & contribute with questions & answers of your own."
  },
  {
    "objectID": "tools/help_slides.html#a-last-word",
    "href": "tools/help_slides.html#a-last-word",
    "title": "So, you are stuck ‚Ä¶ now what?",
    "section": "A last word",
    "text": "A last word\n\nWhile it takes some work to ask a good question, do not let this discourage you from posting on Stack Overflow: if you ask a good question, you will get many great answers.\nYou will learn in the process of developing your question (you may actually find the answer in that process) & you will learn from the answers.\nIt is forth the effort.\nHere is the Stack Overflow documentation on how to ask a good question.\n\n\n\n¬†Back to workshop page"
  },
  {
    "objectID": "tools/quarto.html",
    "href": "tools/quarto.html",
    "title": "Authoring scientific documents with Markdown and Quarto",
    "section": "",
    "text": "This workshop will show you how to easily create beautiful scientific documents (html, pdf, websites, books‚Ä¶)‚Äîcomplete with formatted text, dynamic code, and figures with Quarto, an open-source tool combining the powers of Jupyter or knitr with Pandoc to turn your text and code blocks into fully dynamic and formatted documents."
  },
  {
    "objectID": "tools/quarto.html#markup-and-markdown",
    "href": "tools/quarto.html#markup-and-markdown",
    "title": "Authoring scientific documents with Markdown and Quarto",
    "section": "Markup and Markdown",
    "text": "Markup and Markdown\n\nMarkup languages\nMarkup languages control the formatting of text documents. They are powerful but complex and the raw text (before it is rendered into its formatted version) is visually cluttered and hard to read.\nExamples of markup languages include LaTeX and HTML.\n\nTex (often with the macro package LaTeX) is used to create pdf.\n\n\nExample LaTeX:\n\n\\documentclass{article}\n\\title{My title}\n\\author{My name}\n\\usepackage{datetime}\n\\newdate{date}{24}{11}{2022}\n\\date{\\displaydate{date}}\n\\begin{document}\n \\maketitle\n \\section{First section}\n Some text in the first section.\n\\end{document}\n\nHTML (often with css or scss files to customize the format) is used to create webpages.\n\n\nExample HTML:\n\n<!DOCTYPE html>\n<html lang=\"en-US\">\n  <head>\n    <meta charset=\"utf-8\" />\n    <meta name=\"viewport\" content=\"width=device-width\" />\n    <title>My title</title>\n    <address class=\"author\">My name</address>\n    <input type=\"date\" value=\"2022-11-24\" />\n  </head>\n  <h1>First section</h1>\n  <body>\n    Some text in the first section.\n  </body>\n</html>\n\n\nMarkdown\nA number of minimalist markup languages intend to remove all the visual clutter and complexity to create raw texts that are readable prior to rendering. Markdown (note the pun with ‚Äúmarkup‚Äù), created in 2004, is the most popular of them. Due to its simplicity, it has become quasi-ubiquitous. Many implementations exist which add a varying number of features (as you can imagine, a very simple markup language is also fairly limited).\nMarkdown files are simply text files and they use the .md extension.\n\n\nBasic Markdown syntax\nIn its basic form, Markdown is mostly used to create webpages. Conveniently, raw HTML can be included whenever the limited markdown syntax isn‚Äôt sufficient.\nHere is an overview of the Markdown syntax supported by many applications.\n\n\nPandoc and its extended Markdown syntax\nWhile the basic syntax is good enough for HTML outputs, it is very limited for other formats.\nPandoc is a free and open-source markup format converter. Pandoc supports an extended Markdown syntax with functionality for figures, tables, callout blocks, LaTeX mathematical equations, citations, and YAML metadata blocks. In short, everything needed for the creation of scientific documents.\nSuch documents remain as readable as basic Markdown documents (thus respecting the Markdown philosophy), but they can now be rendered in sophisticated pdf, books, entire websites, Word documents, etc.\nAnd of course, as such documents remain text files, you can put them under version control with Git.\n\nPrevious example using Pandoc‚Äôs Markdown:\n\n---\ntitle: My title\nauthor: My name\ndate: 2022-11-24\n---\n# First section\nSome text in the first section."
  },
  {
    "objectID": "tools/quarto.html#literate-programming",
    "href": "tools/quarto.html#literate-programming",
    "title": "Authoring scientific documents with Markdown and Quarto",
    "section": "Literate programming",
    "text": "Literate programming\nLiterate programming is a methodology that combines snippets of code and written text. While first introduced in 1984, this approach to the creation of documents has truly exploded in popularity in recent years thanks to the development of new tools such as R Markdown and, later, Jupyter notebooks."
  },
  {
    "objectID": "tools/quarto.html#quarto",
    "href": "tools/quarto.html#quarto",
    "title": "Authoring scientific documents with Markdown and Quarto",
    "section": "Quarto",
    "text": "Quarto\n\nHow it works\nQuarto files are transformed into Pandoc‚Äôs extended Markdown by Jupyter (when used with Python or Julia) or by knitr (when used with R), then pandoc turns the Markdown document into the output of your choice.\n\nJulia and Python make use of the Jupyter engine:\n\n From Quarto documentation\n\nR uses the knitr engine:\n\n From Quarto documentation\nQuarto files use the extension .qmd.\nWhen using R, you can use Quarto directly from RStudio: if you are used to R Markdown, Quarto is the new and better R Markdown.\nWhen using Python or Julia, you can use Quarto directly from a Jupyter notebook (with .ipynb extension).\n\nUsing Quarto directly from a Jupyter notebook:\n\n From Quarto documentation\nIn this workshop, we will see the most general workflow: simply using a text editor.\n\n\n\n\n\n\nSupported languages\n\n\n\n\n\nQuarto renders highlighting in countless languages and generates dynamic output for code blocks in:\n\nPython\nR\nJulia\nObservable JS\n\nYou can render documents in a wide variety of formats:\n\nHTML\nPDF\nMS Word\nOpenOffice\nePub\nRevealjs\nPowerPoint\nBeamer\nGitHub Markdown\nCommonMark\nHugo\nDocusaurus\nMarkua\nMediaWiki\nDokuWiki\nZimWiki\nJira Wiki\nXWiki\nJATS\nJupyter\nConTeXt\nRTF\nreST\nAsciiDoc\nOrg-Mode\nMuse\nGNU\nGroff\n\nThis training website is actually built with Quarto!\n\n\n\n\n\nInstallation\n\nDownload Quarto here.\nDownload the language(s) (R, Python, or Julia) you will want to use with Quarto as well as their corresponding engine (knitr for R; Jupyter for Python and Julia):\n\nIf you want to use Quarto with R, you will need:\n\nR (download here if you don‚Äôt have R already on your system),\nthe rmarkdown package. For this, launch R and run:\n\ninstall.packages(\"rmarkdown\")\nIf you want to use it with Python, you will need:\n\nPython 3 (download here if don‚Äôt have it on your system),\nJupyterLab. For this, open a terminal and run:\n\npython3 -m pip install jupyterlab  # if you are on MacOS or Linux\npy -m pip install jupyterlab       # if you are on Windows\nFinally, if you want to use Quarto with Julia, you will need:\n\nJulia (download here if you don‚Äôt have Julia),\nthe IJulia and Revise packages. For this, launch Julia and run:\n\n] add IJulia Revise\n# <Backspace>\nusing IJulia\nnotebook()      # to install a minimal Python+Jupyter distribution\nRunning notebook() allows you to install Jupyter if you don‚Äôt already have it.\n\n\nDocument structure and syntax\n\nFront matter\nWritten in YAML. Sets the options for the document. Let‚Äôs see a few examples.\n\nHTML output:\n\n---\ntitle: \"My title\"\nauthor: \"My name\"\nformat: html\n---\n\nHTML output with a few options:\n\n---\ntitle: \"My title\"\nauthor: \"My name\"\nformat:\n  html:\n    toc: true\n    css: <my_file>.css\n---\nThe above examples would work if you don‚Äôt use any code blocks or if you use R code blocks. If you use Python or Julia however, you need to add a jupyter entry with the name of the language that should run in Jupyter.\n\nMS Word output with Python code blocks:\n\n---\ntitle: \"My title\"\nauthor: \"My name\"\nformat: docx\njupyter: python3\n---\n\nrevealjs output with some options and Julia code blocks:\n\n---\ntitle: \"Some title\"\nsubtitle: \"Some subtitle\"\ninstitute: \"Simon Fraser University\"\ndate: \"2022-11-24\"\nexecute:\n  error: true\n  echo: true\nformat:\n  revealjs:\n    theme: [default, custom.scss]\n    highlight-style: monokai\n    code-line-numbers: false\n    embed-resources: true\njupyter: julia-1.8\n---\nSee the Quarto documentation for an exhaustive list of options for all formats.\n\n\nWritten sections\nWritten sections are written in Pandoc‚Äôs extended Markdown.\n\n\nCode blocks\nIf all you want is syntax highlighting of the code blocks, use this syntax:\n```{.language}\n<some code>\n```\nIf you want syntax highlighting of the blocks and for the code to run, use instead:\n```{language}\n<some code>\n```\nIn addition, options can be added to individual code blocks:\n```{language}\n#| <some option>: <some option value>\n\n<some code>\n```\n\n\n\nRendering\nUsing Quarto is very simple: there are only two commands you need to know.\nIn a terminal, simply run either of:\nquarto render <file>.qmd     # this will render the document\nquarto preview <file>.qmd    # this will display live preview as you work on your document"
  },
  {
    "objectID": "tools/quarto.html#lets-give-this-a-try",
    "href": "tools/quarto.html#lets-give-this-a-try",
    "title": "Authoring scientific documents with Markdown and Quarto",
    "section": "Let‚Äôs give this a try",
    "text": "Let‚Äôs give this a try\nCreate a file called test.qmd with the text editor of your choice.\n\nExample:\n\nnano test.qmd\nAdd a minimal front matter with the output format.\n\nExample:\n\n---\ntitle: \"Some title\"\nformat: revealjs\n---\nThen open a new terminal, cd to the location of the file, and run the command:\nquarto preview test.qmd\nThis will open the rendered document in your browser.\nWe will play with this test.qmd file and see how it is rendered by Quarto as we go."
  },
  {
    "objectID": "tools/quarto.html#examples",
    "href": "tools/quarto.html#examples",
    "title": "Authoring scientific documents with Markdown and Quarto",
    "section": "Examples",
    "text": "Examples\nBelow are a few basic example files and their outputs.\n\nRevealjs presentation\n\n\n\n\n\n\nCode\n\n\n\n\n\n---\ntitle: \"My title\"\nauthor: \"My name\"\ninstitute: \"Simon Fraser University\"\nformat:\n  revealjs:\n    highlight-style: monokai\n    code-line-numbers: false\n    embed-resources: true\n---\n\n## First section\n\nWhen exporting to revealjs, second level sections mark the start of new slides,\nwith a slide title.\n\nThis can be changed in options.\n\n---\n\nNew slides can be started without titles this way.\n\n# There are title slides\n\n## Formatting\n\nText can be rendered *in italic* or **in bold** as well as [underlined]{.underline}.\n\nYou can use superscripts^2^, subscripts~test~, ~~strikethrough~~, and `inline code`.\n\n> This is a quote.\n\n## Columns\n\n:::: {.columns}\n\n::: {.column width=\"30%\"}\n\nYou can create columns.\n\n:::\n\n::: {.column width=\"70%\"}\n\nAnd you can set their respective width.\n\n:::\n\n::::\n\n## Lists\n\n::: {.incremental}\n\n- List can happen one line at a time\n- like\n- this\n\n:::\n\n## Lists\n\n- Or all at the same time\n- like\n- that\n\n## Ordered lists\n\n1. Item 1\n2. Item 2\n3. Item 3\n\n## Images\n\n![Example image](qmd_jupyter.png)\n\n## Tables\n\n| Col 1 | Col 2 | Col 3  |\n|------ |-------|--------|\n| a     | 1     | red    |\n| b     | 2     | orange | \n| c     | 3     | yellow |\n\n:::{.callout-note}\n\nTables can be fully customized (or you could use raw html).\n\n:::\n\n## Equations\n\n$$\n\\frac{\\partial \\mathrm C}{ \\partial \\mathrm t } + \\frac{1}{2}\\sigma^{2} \\mathrm S^{2}\n\\frac{\\partial^{2} \\mathrm C}{\\partial \\mathrm C^2}\n  + \\mathrm r \\mathrm S \\frac{\\partial \\mathrm C}{\\partial \\mathrm S}\\ =\n  \\mathrm r \\mathrm C \n$$\n\n\n\nRendered document (click on it to open it in a new tab):\n\n\n\n\n\n\n\npdf\n\n\n\n\n\n\nCode\n\n\n\n\n\n---\ntitle: \"My title\"\nauthor: \"My name\"\nformat:\n  pdf:\n    toc: true\n---\n\n# Header 1\n\nSome text.\n\n## Header 2\n\nMore text.\n\n## Formatting\n\nText can be rendered *in italic* or **in bold** as well as [underlined]{.underline}.\n\nYou can use superscripts^2^, subscripts~test~, ~~strikethrough~~, and `inline code`.\n\n> This is a quote.\n\n## Lists\n\n### Unordered\n\n- Item 1\n- Item 2\n- Item 3\n\n### Ordered\n\n1. Item 1\n2. Item 2\n3. Item 3\n\n## Images\n\n![Example image](qmd_jupyter.png)\n\n## Tables\n\n| Col 1 | Col 2 | Col 3  |\n|------ |-------|--------|\n| a     | 1     | red    |\n| b     | 2     | orange | \n| c     | 3     | yellow |\n\n:::{.callout-note}\n\nTables can be fully customized (or you could use raw html).\n\n:::\n\n## Equations\n\n$$\n\\frac{\\partial \\mathrm C}{ \\partial \\mathrm t } + \\frac{1}{2}\\sigma^{2} \\mathrm S^{2}\n\\frac{\\partial^{2} \\mathrm C}{\\partial \\mathrm C^2}\n  + \\mathrm r \\mathrm S \\frac{\\partial \\mathrm C}{\\partial \\mathrm S}\\ =\n  \\mathrm r \\mathrm C \n$$\n\n\n\nRendered document (click on it to open it in a new tab):\n\n\n\n\n\n\nIn order to export to pdf, you need a TeX distribution. You probably already have one installed on your machine, so you should first try to render or preview a document to pdf to see whether it works. If it doesn‚Äôt work, you can install the minimalist distribution TinyTex by running in your terminal:\n\nquarto install tool tinytex\n\n\nHTML with R code blocks\n\n\n\n\n\n\nCode\n\n\n\n\n\n---\ntitle: \"My title\"\nauthor: \"My name\"\ninstitute: \"Simon Fraser University\"\nformat: html\n---\n\n# Header 1\n\n## Header 2\n\nSome text.\n\n## Formatting  {#sec-formatting}\n\n::: aside\n\nNote that each header automatically creates an anchor,\nmaking it easy to link to specific sections of your documents.\n\n:::\n\nText can be rendered *in italic* or **in bold** as well as [underlined]{.underline}.\n\nYou can use superscripts^2^, subscripts~test~, ~~strikethrough~~, and `inline code`.\n\n> This is a quote.\n\n## Columns\n\n:::: {.columns}\n\n::: {.column width=\"30%\"}\n\nYou can create columns.\n\n:::\n\n::: {.column width=\"70%\"}\n\nAnd you can set their respective width.\n\n:::\n\n::::\n\n## Lists\n\n- Item 1\n- Item 2\n- Item 3\n\n## Ordered lists\n\n1. Item 1\n2. Item 2\n3. Item 3\n\n## Images\n\n![Example image](qmd_jupyter.png)\n\n## Tables\n\n| Col 1 | Col 2 | Col 3  |\n|------ |-------|--------|\n| a     | 1     | red    |\n| b     | 2     | orange | \n| c     | 3     | yellow |\n\n:::{.callout-note}\n\nTables can be fully customized (or you could use raw html).\n\n:::\n\n## Equations\n\n$$\n\\frac{\\partial \\mathrm C}{ \\partial \\mathrm t } + \\frac{1}{2}\\sigma^{2} \\mathrm S^{2}\n\\frac{\\partial^{2} \\mathrm C}{\\partial \\mathrm C^2}\n  + \\mathrm r \\mathrm S \\frac{\\partial \\mathrm C}{\\partial \\mathrm S}\\ =\n  \\mathrm r \\mathrm C \n$$\n\n## Cross-references\n\nSee @sec-formatting.\n\n*Note that you can add bibliographies, flow charts, the equivalent of HTML \"div\",\nand just so much more. Remember that this is a tiny overview.*\n\n## Let's try some code blocks now\n\n```{r}\n# This is a block that runs\n2 + 3\n```\n\n::: aside\n\nDid you notice that the content of your code blocks can be copied with a click?\nOf course, this is customizable.\n\n:::\n\n```{.r}\n# This is a block that doesn't run\n2 + 3\n```\n\n```{r}\n#| echo: false\n# And this is a block showing only the output\ndata.frame(\n  country = c(\"Canada\", \"USA\", \"Mexico\"),\n  var = c(2.9, 3.1, 4.5)\n)\n```\n\n## Plots\n\n```{r}\nplot(cars)\n```\n\n<br>\nYou can play with options to add a title:\n\n```{r}\n#| fig-cap: \"Stopping distance as a function of speed in cars\"\n\nplot(cars)\n```\n\n<br>\nYou can have more complex multi-plot layouts:\n\n```{r}\n#| layout-ncol: 2\n#| fig-cap: \n#|   - \"Stopping distance as a function of speed in cars\"\n#|   - \"Vapor pressure of mercury as a function of temperature\"\n\nplot(cars)\nplot(pressure)\n```\n\nFor those who have `ggplot2`[^1], you can try that too:\n\n```{r}\nlibrary(ggplot2)\n\nggplot(data = mpg, mapping = aes(x = displ, y = hwy)) + \n  geom_point(mapping = aes(color = class)) + \n  geom_smooth()\n```\n\n[^1]: You can install it with:\n    ```{.r}\n    install.packages(\"ggplot2\")\n    ```\n\n\n\nRendered document (click on it to open it in a new tab):\n\n\n\n\n\n\n\nBeamer with Python code blocks\nBeamer is LaTeX presentation framework: a way to create beautiful pdf slides.\n\n\n\n\n\n\nCode\n\n\n\n\n\n---\ntitle: \"Some title\"\nauthor: \"Some name\"\nformat: beamer\njupyter: python3\n---\n\n## First slide\n\nWith some content\n\n## Formatting\n\nText can be rendered *in italic* or **in bold** as well as [underlined]{.underline}.\n\nYou can use superscripts^2^, subscripts~test~, ~~strikethrough~~, and `inline code`.\n\n## Lists\n\n- Item 1\n- Item 2\n- Item 3\n\n## Ordered lists\n\n1. Item 1\n2. Item 2\n3. Item 3\n\n## Images\n\n![Example image](qmd_jupyter.png)\n\n## Tables\n\n| Col 1 | Col 2 | Col 3  |\n|------ |-------|--------|\n| a     | 1     | red    |\n| b     | 2     | orange | \n| c     | 3     | yellow |\n\n:::{.callout-note}\n\nTables can be fully customized (or you could use raw html).\n\n:::\n\n## Equations\n\n$$\n\\frac{\\partial \\mathrm C}{ \\partial \\mathrm t } + \\frac{1}{2}\\sigma^{2} \\mathrm S^{2}\n\\frac{\\partial^{2} \\mathrm C}{\\partial \\mathrm C^2}\n  + \\mathrm r \\mathrm S \\frac{\\partial \\mathrm C}{\\partial \\mathrm S}\\ =\n  \\mathrm r \\mathrm C \n$$\n\n## Some basic code block\n\n```{python}\n#| echo: true\n\n2 + 3\n```\n\n## Some plot\n\n```{python}\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Data for plotting\nt = np.arange(0.0, 2.0, 0.01)\ns = 1 + np.sin(2 * np.pi * t)\n\nfig, ax = plt.subplots()\nax.plot(t, s)\n\nax.set(xlabel='time (s)', ylabel='voltage (mV)',\n       title='Here goes the title')\nax.grid()\n\nfig.savefig(\"test.png\")\nplt.show()\n```\n\n\n\nRendered document (click on it to open it in a new tab):"
  }
]