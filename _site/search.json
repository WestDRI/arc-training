[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About this site",
    "section": "",
    "text": "This site, built with Quarto, contains Marie-H√©l√®ne Burle‚Äôs latest content.\nHer older training material can be found on the archived sites:\n\nShell, Git, R, Python, etc.\nJulia training material\nMachine learning training material\n\n\nMain WestDRI training website\nThis is the mint (‚Äúmint is not training‚Äù) WestDRI website.\nTo view all WestDRI training material, please visit the training WestDRI website.\n\n\nOther websites\nIn addition, here are a few of our websites for various training events:\n\nAutumn School 2022\nTraining Modules 2022\nTraining Modules 2021\nSummer School 2020\nCoding Fundamentals for Humanists 20221\nCoding Fundamentals for Humanists 2021\nHSS Winter Series 2023\nHSS Winter Series 2022\n\n\n\n1¬†For the Digital Humanities Summer Institute"
  },
  {
    "objectID": "bash/intro_scripting.html#what-are-unix-shells",
    "href": "bash/intro_scripting.html#what-are-unix-shells",
    "title": "Automation & scripting in bash for beginners",
    "section": "What are Unix shells?",
    "text": "What are Unix shells?\nA Unix shell is a command line interpreter: the user enters commands as text, either interactively in the command line or in a script, and the shell passes them to the operating system.\n\nBash\nBash (Bourne Again SHell), released in 1989, is part of the GNU Project and is the default Unix shell on many systems (MacOS recently changed its default to zsh).\n\n\nOther shells\nPrior to Bash, the default was the Bourne shell (sh).\nA new and popular shell (backward compatible with Bash) is zsh. It extends Bash‚Äôs capabilities.\nAnother shell in the same family is the KornShell (ksh).\nAll these shells are quite similar. The C shell (csh) however was modeled on the C programming language.\nBash is the most common shell and the one which makes the most sense to learn as a first Unix shell."
  },
  {
    "objectID": "bash/intro_scripting.html#why-use-a-shell",
    "href": "bash/intro_scripting.html#why-use-a-shell",
    "title": "Automation & scripting in bash for beginners",
    "section": "Why use a shell?",
    "text": "Why use a shell?\nWhile automating GUI operations is really difficult, it is easy to rerun a script (a file with a number of commands). Unix shells thus allow the creation of reproducible workflows and the automation of repetitive tasks.\nThey are powerful to launch tools, modify files, search text, or combine commands.\nThey also allow to work on remote machines and HPC systems."
  },
  {
    "objectID": "bash/intro_scripting.html#connecting-to-a-remote-hpc-system-via-ssh",
    "href": "bash/intro_scripting.html#connecting-to-a-remote-hpc-system-via-ssh",
    "title": "Automation & scripting in bash for beginners",
    "section": "Connecting to a remote HPC system via SSH",
    "text": "Connecting to a remote HPC system via SSH\n\nUsernames and password\nWe will give you a link to an etherpad during the workshop. Add your name next to a free username to claim it.\nWe will also give you the password for our training cluster. When prompted, enter it.\n\nNote that you will not see any character as you type the password: this is called blind typing and is a Linux safety feature. Type slowly and make sure not to make typos. It can be unsettling at first not to get any feed-back while typing.\n\n\n\nLinux and MacOS users\nLinux users: open the terminal emulator of your choice.\nMacOS users: open ‚ÄúTerminal‚Äù.\nThen type:\nssh userxx@bashworkshop.c3.ca  # Replace userxx by your username (e.g. user09)\n\n\nWindows users\nWe suggest using the free version of MobaXterm.\nMobaXterm comes with a terminal emulator and a GUI interface for SSH sessions.\nOpen MobaXterm, click on ‚ÄúSession‚Äù, then ‚ÄúSSH‚Äù, and fill in the Remote host name and your username. Here is a live demo."
  },
  {
    "objectID": "bash/intro_scripting.html#the-prompt",
    "href": "bash/intro_scripting.html#the-prompt",
    "title": "Automation & scripting in bash for beginners",
    "section": "The prompt",
    "text": "The prompt\nIn command-line interfaces, a command prompt is a sequence of characters indicating that the interpreter is ready to accept input. It can also provide some information (e.g.¬†time, error types, username and hostname, etc.)\nThe Bash prompt is customizable. By default, it often gives the username and the hostname, and it typically ends with $."
  },
  {
    "objectID": "bash/intro_scripting.html#help-on-commands",
    "href": "bash/intro_scripting.html#help-on-commands",
    "title": "Automation & scripting in bash for beginners",
    "section": "Help on commands",
    "text": "Help on commands\nMan pages:\nman <command>\n\nMan pages open in a pager (usually less).\nNavigate up/down with the space bar and the b key.\nQuit the pager with the q key.\n\nHelp pages:\n<command> --help\nInspect commands:\ncommand -V <command>"
  },
  {
    "objectID": "bash/intro_scripting.html#examples-of-commands",
    "href": "bash/intro_scripting.html#examples-of-commands",
    "title": "Automation & scripting in bash for beginners",
    "section": "Examples of commands",
    "text": "Examples of commands\n\nPrint working directory: pwd\nChange directory: cd\nPrint: echo\nPrint content of a file: cat\nList: ls\nCopy: cp\nMove or rename: mv\nCreate a new directory: mkdir\nCreate a new file: touch"
  },
  {
    "objectID": "bash/intro_scripting.html#keybindings",
    "href": "bash/intro_scripting.html#keybindings",
    "title": "Automation & scripting in bash for beginners",
    "section": "Keybindings",
    "text": "Keybindings\nClear the terminal (command clear) with C-l (this means: press the Ctrl and L keys at the same time).\nNavigate command history with C-p and C-n (or up and down arrows).\nYou can auto-complete commands by pressing the tab key."
  },
  {
    "objectID": "bash/intro_scripting.html#file-name",
    "href": "bash/intro_scripting.html#file-name",
    "title": "Automation & scripting in bash for beginners",
    "section": "File name",
    "text": "File name\nShell scripts, including Bash scripts, are usually given the extension sh (e.g.¬†my_script.sh).\nYou can store scripts anywhere, but a common practice is to store them in a ~/bin directory."
  },
  {
    "objectID": "bash/intro_scripting.html#syntax",
    "href": "bash/intro_scripting.html#syntax",
    "title": "Automation & scripting in bash for beginners",
    "section": "Syntax",
    "text": "Syntax\n\nShebang\nScripts can be written for any interpreter (e.g.¬†Bash, Python, R, etc.) The way to tell the system which one to use is to use a shebang (#!) followed by the path of the interpreter on the first line of the script.\nTo use Bash, start your scripts with:\n#!/bin/bash\nYou may also encounter this notation:\n#!/usr/bin/env bash\nIf you are curious, you can read the answers to this Stack Overflow question for the differences between the two.\n\n\nComments\nAnything to the left of # is ignored by the interpreter and is for human consumption only.\n# You can write full-line comments\n\npwd       # You can also write comments after commands"
  },
  {
    "objectID": "bash/intro_scripting.html#executing-scripts",
    "href": "bash/intro_scripting.html#executing-scripts",
    "title": "Automation & scripting in bash for beginners",
    "section": "Executing scripts",
    "text": "Executing scripts\nThere are two ways to execute a script:\nbash my_script.sh\n./my_script.sh  # The dot represents the current directory\nIn the latter case, you need to make sure that your script is executable by first running:\nchmod u+x my_script.sh  # This makes the script executable by the user (i.e. you)"
  },
  {
    "objectID": "bash/intro_scripting.html#our-first-script",
    "href": "bash/intro_scripting.html#our-first-script",
    "title": "Automation & scripting in bash for beginners",
    "section": "Our first script",
    "text": "Our first script\nOpen a text editor (e.g.¬†nano) and type:\n#!/bin/bash\n\necho \"This is our first script.\"\nSave and close the file.\n\n\nYour turn:\n\nNow run the script with one, then the other method.\nWhat does this script do?"
  },
  {
    "objectID": "bash/intro_scripting.html#declaring-variables",
    "href": "bash/intro_scripting.html#declaring-variables",
    "title": "Automation & scripting in bash for beginners",
    "section": "Declaring variables",
    "text": "Declaring variables\nYou can declare a variable (i.e.¬†a name that holds a value) with the = sign.\n!! Make sure not to put spaces around the equal sign.\nvariable=Test"
  },
  {
    "objectID": "bash/intro_scripting.html#quotes",
    "href": "bash/intro_scripting.html#quotes",
    "title": "Automation & scripting in bash for beginners",
    "section": "Quotes",
    "text": "Quotes\nLet‚Äôs experiment with quotes:\n\nvariable=This string is the value of the variable\necho $variable\n\nbash: line 1: string: command not found\n\n\nOops‚Ä¶\n\nvariable=\"This string is the value of the variable\"\necho $variable\n\nThis string is the value of the variable\n\n\n\nvariable='This string is the value of the variable'\necho $variable\n\nThis string is the value of the variable\n\n\n\nvariable='This string's the value of the variable'\necho $variable\n\nbash: -c: line 1: unexpected EOF while looking for matching `''\nbash: -c: line 3: syntax error: unexpected end of file\n\n\nOops‚Ä¶\nOne solution to this is to use double quotes:\n\nvariable=\"This string's the value of the variable\"\necho $variable\n\nThis string's the value of the variable\n\n\nAlternatively, single quotes can be escaped:\n\nvariable='This string'\"'\"'s the value of the variable'\necho $variable\n\nThis string's the value of the variable\n\n\n\nAdmittedly, this last one is a little crazy. It is the way to escape single quotes in single-quoted strings.\nThe first ' ends the first string, both \" create a double-quoted string with ' (escaped) in it, then the last ' starts the second string.\nEscaping double quotes is a lot easier and simply requires \\\"."
  },
  {
    "objectID": "bash/intro_scripting.html#expanding-a-variables-value",
    "href": "bash/intro_scripting.html#expanding-a-variables-value",
    "title": "Automation & scripting in bash for beginners",
    "section": "Expanding a variable‚Äôs value",
    "text": "Expanding a variable‚Äôs value\nTo expand a variable (to access its value), you need to prepend its name with $:\n\nvariable=Test\necho variable\n\nvariable\n\n\nMmmm‚Ä¶ not really want we want!\n\nvariable=Test\necho $variable\n\nTest\n\n\n\nvariable=Test; echo \"$variable\"\n\nTest\n\n\n!! Single quotes don‚Äôt expand variables.\n\nvariable=Test; echo '$variable'\n\n$variable"
  },
  {
    "objectID": "bash/intro_scripting.html#passing-variables-to-a-bash-script",
    "href": "bash/intro_scripting.html#passing-variables-to-a-bash-script",
    "title": "Automation & scripting in bash for beginners",
    "section": "Passing variables to a Bash script",
    "text": "Passing variables to a Bash script\nCreate a script called name.sh with the following content:\n#!/bin/bash\n\necho \"My name is $1.\"  # $1 refers to the first variable passed to the script\nYou can now pass a variable to this script with:\nbash name.sh Marie\nMy name is Marie.\nYou can pass several variables to a script. Copy name.sh to name2.sh and edit name2.sh to look like the following:\n#!/bin/bash\n\necho \"My name is $1 and I am $2 years old.\"\nbash name2.sh Marie 43\nMy name is Marie and I am 43 years old.\nYou can also pass any number of variables to a script:\n#!/bin/bash\n\necho $@\nbash script.sh argument1 argument2 argument3 argument4\nargument1 argument2 argument3 argument4"
  },
  {
    "objectID": "bash/intro_scripting.html#brace-expansion",
    "href": "bash/intro_scripting.html#brace-expansion",
    "title": "Automation & scripting in bash for beginners",
    "section": "Brace expansion",
    "text": "Brace expansion\n\necho {1..5}\n\n1 2 3 4 5\n\n\n\necho {01..10}\n\n01 02 03 04 05 06 07 08 09 10\n\n\n\necho {1..5}.txt\n\n1.txt 2.txt 3.txt 4.txt 5.txt\n\n\n\necho {r..v}\n\nr s t u v\n\n\n\necho {file1,file2}.sh\n\nfile1.sh file2.sh\n\n\n!! Make sure not to add a space after the comma.\ntouch {file1,file2}.sh\ntouch file{3..6}.sh\n\necho {list,of,strings}\n\nlist of strings"
  },
  {
    "objectID": "bash/intro_scripting.html#wildcards",
    "href": "bash/intro_scripting.html#wildcards",
    "title": "Automation & scripting in bash for beginners",
    "section": "Wildcards",
    "text": "Wildcards\nWildcards are really powerful to apply a command to all the elements having a common pattern.\nFor instance, we can delete all the files we created earlier (file1.sh, file2.sh, etc.) with a single command:\nrm file*.sh\n!! Be very careful that rm is irreversible. Deleted files do not go to the trash: they are gone."
  },
  {
    "objectID": "bash/tools1.html",
    "href": "bash/tools1.html",
    "title": "Fun tools to simplify your life in the command line",
    "section": "",
    "text": "Working in the command-line has many advantages and it is often necessary, but can it be fun?\nIn this webinar, aimed at any command-line user, I intend to demonstrate that yes, it can! by introducing three free and open source utilities which make navigating your system and your outputs a lot easier:\n\nfzf is a simple, yet extremely powerful interactive fuzzy finder allowing for incremental completion and narrowing selection of any command line output. I will show you how to build simple shell functions which harvest its power to instantly refresh your memory on your custom keybindings or aliases, navigate your command history, find and kill processes, and explore and checkout your git commits. After this, you will be able to use fzf for any number of other applications in your work in the command-line.\nautojump lets you jump anywhere you want in your directories in just a few keystrokes (no more of this painful navigation writing down long paths).\nWith the ranger file manager, you can browse (with preview!), open, copy, move, delete, etc. your files and directories in a friendly way from the command line. Added bonus: you can use fzf and autojump within ranger!\n\nWarning: too much fun in the command-line can lead to addiction and geek behaviours. Use in moderation."
  },
  {
    "objectID": "bash/tools2.html",
    "href": "bash/tools2.html",
    "title": "A few of our favourite tools",
    "section": "",
    "text": "In a previous webinar, we presented three of our favourite command line tools. Today, we will introduce other tools we find really useful in our daily workflow:\n\nlazygit: a wonderful terminal UI for Git,\nbat: a great syntax highlighter,\nripgrep: a fast alternative to grep,\nfd: a /really/ fast alternative to find,\npass: a command line password manager.\n\nAlong the way, I will use a few other neat command line tools such as hyperfine‚Äîfor sophisticated benchmarking‚Äîand diff-so-fancy‚Äîwhich makes your diffs a lot more readable.\nFor the Emacs users among you, we will finish the workshop with two Emacs utilities:\n\nTRAMP: a remote file access system,\nHelm: a ‚Äúframework for incremental completions and narrowing selections‚Äù."
  },
  {
    "objectID": "calendar.html",
    "href": "calendar.html",
    "title": "Upcoming training events",
    "section": "",
    "text": "Our training events also get posted in our main site."
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact",
    "section": "",
    "text": "Please email us at: training at westdri dot ca."
  },
  {
    "objectID": "git/collab.html",
    "href": "git/collab.html",
    "title": "Collaborating with Git and GitHub",
    "section": "",
    "text": "Git is a free and open source version control system (a software that tracks changes to your files, allowing you to revisit or revert to older versions).\nUsing Internet hosting services such as GitHub, Git is also a powerful collaboration tool.\nIn this introductory workshop you will learn the basics of working with Git on the command line."
  },
  {
    "objectID": "git/collab.html#setup",
    "href": "git/collab.html#setup",
    "title": "Collaborating with Git and GitHub",
    "section": "Setup",
    "text": "Setup\nWhen you collaborate with others using Git and GitHub, there are three possible situations:\n\nYou create a project on your machine and want others to contribute to it (1).\nYou want to contribute to a project started by others & ‚Ä¶\n‚ÄÉ‚ÄÉ‚Ä¶ you have write access to it (2).\n‚ÄÉ‚ÄÉ‚Ä¶ you do not have write access to it (3).\n\n\n(1) You start the project\n\nCreate a remote on GitHub\nThese are the steps we already saw earlier.\n\n1. Create an empty repository on GitHub\n\nGo to the GitHub website, login, and go to your home page.\nLook for the Repositories tab & click the green New button.\nEnter the name you want for your repo, without spaces.\nMake the repository public or private.\n\n\n\n2. Link empty repository to your repo\nClick on the Code green drop-down button, select SSH if you have set SSH for your GitHub account or HTTPS and copy the address.\nIn the command line, cd inside your project, and add the remote:\ngit remote add <remote-name> <remote-address>\nremote-name is a convenience name to identify that remote. You can choose any name, but since Git automatically call the remote origin when you clone a repo, it is common practice to use origin as the name for the first remote.\n\nExample (using an SSH address):\n\ngit remote add origin git@github.com:<user>/<repo>.git\n\nExample (using an HTTPS address):\n\ngit remote add origin https://github.com/<user>/<repo>.git\nIf you are working alone on this project and you only wanted to have a remote for backup, you are set.\nIf you don‚Äôt want to grant others write access to the project, and you only accept contributions through pull requests, you are also set.\nIf you want to grant your collaborators write access to the project however, you need to add them to it.\n\n\n\nInvite collaborators\n\nGo to your GitHub project page.\nClick on the Settings tab.\nClick on the Manage access section on the left-hand side (you will be prompted for your GitHub password).\nClick on the Invite a collaborator green button.\nInvite your collaborators with one of their GitHub user name, their email address, or their full name.\n\n\n\n\n(2) Write access to project\n\nClone project\ncd to location where you want your local copy, then:\ngit clone <remote-address> <local-name>\nThis sets the project as a remote to your new local copy and that remote is automatically called origin.\nWithout <local-name>, the repo will have the name of the last part of the remote address.\n\n\n\n(3) No write access to project\n\nCollaborate without write access\nIn that case, you will have to submit a pull request:\n\nFork the project on GitHub.\nClone your fork on your machine.\nAdd the initial project as a second remote & call it upstream.\nPull from upstream to update your local project.\nCreate & checkout a new branch.\nMake & commit your changes on that branch.\nPush that branch to your fork (i.e.¬†origin ‚Äî remember that you do not have write access to upstream).\nGo to the original project GitHub‚Äôs page & open a pull request."
  },
  {
    "objectID": "git/collab.html#workflow",
    "href": "git/collab.html#workflow",
    "title": "Collaborating with Git and GitHub",
    "section": "Workflow",
    "text": "Workflow\nWhen you collaborate with others using GitHub (or other equivalent service), you and others will work simultaneously on some project. How does this work?\nRemember that to upload your changes to the remote on GitHub you push to it with git push.\nIf one of your collaborators has made changes to the remote (pushing from their own machine), you won‚Äôt be able to push. Instead, you will get the following message:\nTo xxx.git\n ! [rejected]        main -> main (fetch first)\nerror: failed to push some refs to 'xxx.git'\nhint: Updates were rejected because the remote contains work that you do\nhint: not have locally. This is usually caused by another repository pushing\nhint: to the same ref. You may want to first integrate the remote changes\nhint: (e.g., 'git pull ...') before pushing again.\nhint: See the 'Note about fast-forwards' in 'git push --help' for details.\nThe solution?\nYou first have to download (git pull) their work onto your machine, merge it with yours (which will happen automatically if there are no conflicts), before you can push your work to GitHub.\nNow‚Ä¶ what if there are conflicts?"
  },
  {
    "objectID": "git/collab.html#resolving-conflicts",
    "href": "git/collab.html#resolving-conflicts",
    "title": "Collaborating with Git and GitHub",
    "section": "Resolving conflicts",
    "text": "Resolving conflicts\nGit works line by line. As long as your collaborators and you aren‚Äôt working on the same line(s) of the same file(s) at the same time, there will not be any problem. If however you modified one or more of the same line(s) of the same file(s), Git will not be able to decide which version should be kept. When you git pull their work on your machine, the automatic merging will get interrupted and Git will ask you to resolve the conflict(s) before the merge can resume. It will conveniently tell you which file(s) contain the conflict(s).\nThere are fancy tools to resolve conflicts, but you can do it in any text editor: simply open the file(s) listed by Git as having conflicts and look for the following markers:\n<<<<<<< HEAD\nThis is your version.\n=======\nThis is the alternative version of the same section of the file.\n>>>>>>> alternative version\nThese markers are added by Git to signal the areas of conflict. It is up to you to choose between the two versions (or create a third one) and remove the conflict markers. After that, you can stage the file(s) which contained the conflicts to finish the merge (and then you can commit)."
  },
  {
    "objectID": "git/contrib.html",
    "href": "git/contrib.html",
    "title": "Version control with Git & GitHub",
    "section": "",
    "text": "Git is a free and open source version control system (a software that tracks changes to your files, allowing you to revisit or revert to older versions).\nUsing Internet hosting services such as GitHub, Git is also a powerful collaboration tool.\nIn this introductory workshop you will learn the basics of working with Git on the command line."
  },
  {
    "objectID": "git/contrib.html#what-is-a-version-control-system",
    "href": "git/contrib.html#what-is-a-version-control-system",
    "title": "Version control with Git & GitHub",
    "section": "What is a version control system?",
    "text": "What is a version control system?\n\n\n\n\nfrom PhD\n\n\nWhenever we work on an important document, we intuitively realize that it is important to keep key versions (e.g.¬†the version of a manuscript that we sent to our supervisor, the revised version after we addressed their comments, the revised version after we addressed reviewer comments, etc.). We have all been there ‚Ä¶ The versions accumulate with names that are often less than helpful ‚Ä¶\n\n\n\n\n\nfrom Geek&Poke\n\n\n‚Ä¶ and soon enough, it is hell. This is a form of versioning, but a terribly messy and inefficient one. Version control systems are software that allow to handle this much more effectively."
  },
  {
    "objectID": "git/contrib.html#which-version-control-system-should-i-use",
    "href": "git/contrib.html#which-version-control-system-should-i-use",
    "title": "Version control with Git & GitHub",
    "section": "Which version control system should I use?",
    "text": "Which version control system should I use?\nIf the trends of Google searches of the existing version control systems are any indication of their popularity, we can say that Git has crushed the competition since 2010.\n\n\nNowadays, it is indeed extremely rare to come across any other version control system.\nGit is simply that good üôÇ"
  },
  {
    "objectID": "git/contrib.html#installing-git",
    "href": "git/contrib.html#installing-git",
    "title": "Version control with Git & GitHub",
    "section": "Installing Git",
    "text": "Installing Git\n\nMacOS & Linux users\nInstall Git from the official website.\n\n\nWindows users\nInstall Git for Windows. This will also install Git Bash, a Bash emulator."
  },
  {
    "objectID": "git/contrib.html#using-git",
    "href": "git/contrib.html#using-git",
    "title": "Version control with Git & GitHub",
    "section": "Using Git",
    "text": "Using Git\nWe will use Git from the command line throughout this workshop.\nMacOS users: ‚ÄÉ‚ÄÉ‚ÄÇopen Terminal.\nWindows users: ‚ÄÉ‚ÄÇopen Git Bash.\nLinux users: ‚ÄÉ‚ÄÉ‚ÄÉopen the terminal emulator of your choice."
  },
  {
    "objectID": "git/contrib.html#configuring-git",
    "href": "git/contrib.html#configuring-git",
    "title": "Version control with Git & GitHub",
    "section": "Configuring Git",
    "text": "Configuring Git\nBefore you can use Git, you need to set some basic configuration. You will do this in the terminal you just opened.\n\nUser identity\ngit config --global user.name \"<Your Name>\"\ngit config --global user.email \"<your@email>\"\n\nExample:\n\ngit config --global user.name \"John Doe\"\ngit config --global user.email \"john.doe@gmail.com\"\n\n\nText editor\ngit config --global core.editor \"<text-editor>\"\n\nExample for nano:\n\ngit config --global core.editor \"nano\"\n\n\nLine ending\n\nmacOS, Linux, or WSL\ngit config --global core.autocrlf input\n\n\nWindows\ngit config --global core.autocrlf true\n\n\n\nList settings\ngit config --list"
  },
  {
    "objectID": "git/contrib.html#internal-documentation",
    "href": "git/contrib.html#internal-documentation",
    "title": "Version control with Git & GitHub",
    "section": "Internal documentation",
    "text": "Internal documentation\n\nMan pages\ngit <command> --help\ngit help <command>\nman git-<command>\n\nExample:\n\ngit commit --help\ngit help commit\nman git-commit\n\nUseful keybindings when you are in the pager\nSPACE      scroll one screen down\nb          scroll one screen up\nq          quit\n\n\n\nCommand options\ngit <command> -h\n\nExample:\n\ngit commit -h"
  },
  {
    "objectID": "git/contrib.html#online-documentation",
    "href": "git/contrib.html#online-documentation",
    "title": "Version control with Git & GitHub",
    "section": "Online documentation",
    "text": "Online documentation\n\nOfficial Git manual\nOpen source Pro Git book\n\n\nCourses & workshops\n\nWestern Canada Research Computing Git workshops\nWestGrid Summer School 2020 Git course\nWestGrid Autumn School 2020 Git course\nSoftware Carpentry Git lesson\n\n\n\nQ & A\n\nStack Overflow [git] tag"
  },
  {
    "objectID": "git/contrib.html#mock-project",
    "href": "git/contrib.html#mock-project",
    "title": "Version control with Git & GitHub",
    "section": "Mock project",
    "text": "Mock project\nLet‚Äôs imagine that you have been working on chapter 3 of your thesis for some time, without using a version control system. We will put that chapter under version control and see how you should work from now on.\nFirst, we need to create a mock set of documents.\n\nNavigate to a suitable location\n\ncd </some/suitable/location/in/your/computer>\n\nCreate the directory at the root of chapter 3\n\nmkdir chapter3\n\nMake sure not to use any spaces in the name: Git doesn‚Äôt work well with spaces.\n\n\nCreate a number of subdirectories\n\nmkdir chapter3/src chapter3/ms chapter3/data chapter3/results\n\nCreate a mock manuscript\n\necho \"# Chapter 3\n## Introduction\nBla bla bla bla bla.\n## Methods\nBla bla bla.\" > chapter3/ms/chapter3.md\n\nGit can only version text files. If you write your papers or thesis chapter in text files (e.g.¬†markdown, LaTeX, org-mode), you will be able to put them under version control, which is really convenient. If you use a word processor, you won‚Äôt be able to.\n\n\nCreate a mock R script\n\necho \"library(ggplot2)\nlibrary(dplyr)\n\ndf <- data.frame(\n  x = (1:5),\n  y = (1:5)\n)\n\nggplot(df, aes(x, y)) + geom_point()\" > chapter3/src/chapter3.R\n\nEven if you use a word processor for your writing, your scripts (e.g.¬†in Python, R, etc.) will be written in text files. So you will always be able to put at least those files under version control."
  },
  {
    "objectID": "git/contrib.html#initializing-a-git-repository",
    "href": "git/contrib.html#initializing-a-git-repository",
    "title": "Version control with Git & GitHub",
    "section": "Initializing a Git repository",
    "text": "Initializing a Git repository\nMake sure to enter the project before initializing the repository.\ncd chapter3\nNow, you can run the command that will turn your chapter3 directory into a Git repository:\ngit init\nInitialized empty Git repository in chapter3/.git/\n\nGit is very verbose: you will often get useful feed-back after running commands.\n\nWhen you run this command, Git creates a .git repository. This is where it will store all its files.\nYou can see that this repository was created by running:\nls -a\n.\n..\n.git\ndata\nms\nresults\nsrc\n\nIf you run git init in the wrong location, you can easily fix this: simply delete the .git directory that you created!"
  },
  {
    "objectID": "git/contrib.html#creating-commits",
    "href": "git/contrib.html#creating-commits",
    "title": "Version control with Git & GitHub",
    "section": "Creating commits",
    "text": "Creating commits\nYou can think of a commit as a snapshot of a particular version of your project.\nYou should create a new commit whenever you think that your project is at a point to which you might want to go back to.\nLet‚Äôs create a first commit with the state of our chapter 3 before we do any more work to it:\ngit add .\ngit commit -m \"Initial commit\"\n[main (root-commit) 7f94f8e] Initial commit\n 2 files changed, 18 insertions(+)\n create mode 100644 ms/chapter3.md\n create mode 100644 src/chapter3.R\nTo create a commit, we first need to add the file(s) we want to add to our commit to the staging area (also called ‚Äúindex‚Äù). This is done with the command git add. To add all the files, we can use git add . (. represents the current directory).\nOnce we have added some files to the staging area, we can create a commit. But each commit has a message associated to it. One way to add this message is to use the command to create commits (git commit) with the -m flag (for ‚Äúmessage‚Äù). Here, our message is simply ‚ÄúInitial commit‚Äù.\n\nGit saves the history of a project as a series of snapshots:\n\nThose snapshots are called commits:\n\nEach commit is identified by a unique hash:\n\nEach commit contains these metadata:\n\nauthor,\ndate and time,\nthe hash of parent commit(s),\na message.\n\nAs soon as you create the first commit, a pointer called a branch is created and it points to that commit. By default, that first branch is called main:\n\nAnother pointer (HEAD) points to the branch main.\nHEAD indicates where we are in the project history.\n\n\nWe can now do some work in our chapter 3. For instance, let‚Äôs imagine that we are adding a result section to our chapter3.md file.\necho \"\n## Results\n\nWe now have a bunch of results in our markdown manuscript.\" >> ms/chapter3.md\nMake sure to use >> here and not >: >> prepends content while > replaces any existing content.\nIf this new addition is important enough to justify making a new commit (how often you commit is up to you), we can do so:\ngit add ms/chapter3.md\ngit commit -m \"Add result section to manuscript\"\n[main 451c47b] Add result section to manuscript\n 1 file changed, 4 insertions(+)\n\nAs you create more commits, the history of your project grows ‚Ä¶\n\n‚Ä¶ and the pointers HEAD and main automatically move to the last commit:\n\nFor simplicity, the diagrams can be simplified this way:"
  },
  {
    "objectID": "git/contrib.html#understanding-the-staging-area",
    "href": "git/contrib.html#understanding-the-staging-area",
    "title": "Version control with Git & GitHub",
    "section": "Understanding the staging area",
    "text": "Understanding the staging area\nNew Git users are often confused about the two-step commit process (first, you stage with git add, then you commit with git commit). This intermediate step seems, at first, totally unnecessary. In fact, it is very useful: without it, commits would always include all new changes made to a project and they would thus be very messy. The staging area allows to prepare (‚Äústage‚Äù) the next commit. This way, you only commit what you want when you want.\n\nLet‚Äôs go over a simple example:\n\nWe don‚Äôt always work linearly. Maybe you are working on a section of your manuscript when you realize by chance that there is a mistake in your script. You fix that mistake. On your next commit, it might make little sense to commit together that fix and your manuscript changes since they are not related. If your commits are random bag of changes, it will be very hard for future you to navigate your project history.\nIt is a lot better to only stage your script fix, commit it, then only stage your manuscript update, and commit this in a different commit.\nThe staging area allows you to pick and chose the changes from one or various files that constitute some coherent change to the project and that make sense to commit together."
  },
  {
    "objectID": "git/contrib.html#inspecting-changes",
    "href": "git/contrib.html#inspecting-changes",
    "title": "Version control with Git & GitHub",
    "section": "Inspecting changes",
    "text": "Inspecting changes\n\nList of modified files\nOne command you will run often when working with Git is git status:\ngit status\nOn branch main\nnothing to commit, working tree clean\nThis means that we are on the branch main (that‚Äôs the only branch in our repo at this point) and that all changes to our project have been committed.\nLet‚Äôs modify a file and see what happens:\necho \"\n## Conclusion\n\nAnd finally, the great conclusion of our paper.\" >> ms/chapter3.md\ngit status\nOn branch main\nChanges not staged for commit:\n  (use \"git add <file>...\" to update what will be committed)\n  (use \"git restore <file>...\" to discard changes in working directory)\n        modified:   ms/chapter3.md\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\nLet‚Äôs modify another file:\necho \"\na = 23\" >> src/chapter3.R\ngit status\nOn branch main\nChanges not staged for commit:\n  (use \"git add <file>...\" to update what will be committed)\n  (use \"git restore <file>...\" to discard changes in working directory)\n        modified:   ms/chapter3.md\n        modified:   src/chapter3.R\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\nMaybe we don‚Äôt want to create a commit with all those changes, so we only stage the changes made to the manuscript:\ngit add ms/chapter3.md\nThen we check the status of our repository again:\ngit status\nOn branch main\nChanges to be committed:\n  (use \"git restore --staged <file>...\" to unstage)\n        modified:   ms/chapter3.md\n\nChanges not staged for commit:\n  (use \"git add <file>...\" to update what will be committed)\n  (use \"git restore <file>...\" to discard changes in working directory)\n        modified:   src/chapter3.R\nIf we ran git commit at this point, we would create a new commit with the changes made to the manuscript.\n\n\nList of actual changes\nWhile git status gives us the list of new files and files with changes, it doesn‚Äôt allow us to see what those changes are. For this, we need a new command: git diff.\n\nDifference between the working directory and the index\ngit diff shows the difference between the working directory (our actual files) and the index (staging area):\ngit diff\ndiff --git a/src/chapter3.R b/src/chapter3.R\nindex 95f1592..2bf030d 100644\n--- a/src/chapter3.R\n+++ b/src/chapter3.R\n@@ -7,3 +7,5 @@ df <- data.frame(\n )\n\n ggplot(df, aes(x, y)) + geom_point()\n+\n+a = 23\nThis allows us to see that src/chapter3.R has a new line (a = 23) and that it is not yet staged.\n\n\nDifference between the index and your last commit\nTo see what would be committed if you ran git commit (so, to see the difference between the index and the last commit), you need to run instead:\ngit diff --cached\ndiff --git a/ms/chapter3.md b/ms/chapter3.md\nindex 9408f32..80d2c5c 100644\n--- a/ms/chapter3.md\n+++ b/ms/chapter3.md\n@@ -11,3 +11,7 @@ Bla bla bla.\n ## Results\n\n We now have a bunch of results in our markdown manuscript.\n+\n+## Conclusion\n+\n+And finally, the great conclusion of our paper.\nThis shows us the changes that we have staged but not yet committed (the changes to our manuscript).\n\n\nDifference between the working directory and your last commit\nThis means, both of the above. This can been displayed with:\ngit diff HEAD\ndiff --git a/ms/chapter3.md b/ms/chapter3.md\nindex 9408f32..80d2c5c 100644\n--- a/ms/chapter3.md\n+++ b/ms/chapter3.md\n@@ -11,3 +11,7 @@ Bla bla bla.\n ## Results\n\n We now have a bunch of results in our markdown manuscript.\n+\n+## Conclusion\n+\n+And finally, the great conclusion of our paper.\ndiff --git a/src/chapter3.R b/src/chapter3.R\nindex 95f1592..2bf030d 100644\n--- a/src/chapter3.R\n+++ b/src/chapter3.R\n@@ -7,3 +7,5 @@ df <- data.frame(\n )\n\n ggplot(df, aes(x, y)) + geom_point()\n+\n+a = 23\nNow, let‚Äôs clean up our working directory by creating two new commits:\ngit commit -m \"Add conclusion to the manuscript\"\n[main c7fc9c1] Add conclusion to the manuscript\n 1 file changed, 4 insertions(+)\ngit add .\ngit commit -m \"Define the variable a in R script\"\n[main a049a2f] Define the variable a in R script\n 1 file changed, 2 insertions(+)\nIf we look at the status of our repository now, we can see that it is clean again:\ngit status\nOn branch main\nnothing to commit, working tree clean"
  },
  {
    "objectID": "git/contrib.html#ignoring",
    "href": "git/contrib.html#ignoring",
    "title": "Version control with Git & GitHub",
    "section": "Ignoring",
    "text": "Ignoring\nNot everything should be under version control. For instance, you don‚Äôt want to put under version control non-text files or your initial data. You also shouldn‚Äôt put under version control documents that can be easily recreated such as graphs and script outputs.\nHowever, you don‚Äôt want to have such documents constantly showing up when you run git status. In order to have a clean working directory while keeping them out of version control, you can create a file called .gitignore and add to it a list of files or patterns that you want Git to disregard.\nFor instance:\necho \"/data/\n/results/\" > .gitignore\nThis creates a .gitignore file with two entries (/data/ and /results/) and from now on, any file in either of these directories will be ignored by Git.\nThe .gitignore is a file like any other file, so you‚Äôll want to stage and commit it:\ngit status\nOn branch main\nUntracked files:\n  (use \"git add <file>...\" to include in what will be committed)\n        .gitignore\n\nnothing added to commit but untracked files present (use \"git add\" to track)\ngit add .gitignore\ngit commit -m \"Add .gitignore file with data and results\"\n[main a1df8e5] Add .gitignore file with data and results\n 1 file changed, 2 insertions(+)\n create mode 100644 .gitignore\ngit status\nOn branch main\nnothing to commit, working tree clean"
  },
  {
    "objectID": "git/contrib.html#displaying-the-commit-history",
    "href": "git/contrib.html#displaying-the-commit-history",
    "title": "Version control with Git & GitHub",
    "section": "Displaying the commit history",
    "text": "Displaying the commit history\nSo far, we have created 5 commits. To display them, you use the command git log:\ngit log\ncommit a1df8e56ad45ddd514ff951f2d65e4e1d40a641c (HEAD -> main)\nAuthor: Marie-Helene Burle <xxx@xxx>\nDate:   Mon Oct 3 22:57:59 2022 -0700\n\n    Add .gitignore file with data and results\n\ncommit a049a2f6834801bf76fa3c2c191a59a3ec589d6e\nAuthor: Marie-Helene Burle <xxx@xxx>\nDate:   Mon Oct 3 21:17:23 2022 -0700\n\n    Define the variable a in R script\n\ncommit c7fc9c1743d8a40c3f72d9450b9440dca1cb5922\nAuthor: Marie-Helene Burle <xxx@xxx>\nDate:   Mon Oct 3 21:16:43 2022 -0700\n\n    Add conclusion to the manuscript\n\ncommit 451c47b386895b8b0b5bdd1a8734ef1d51f9ccc9\nAuthor: Marie-Helene Burle <xxx@xxx>\nDate:   Mon Oct 3 18:35:51 2022 -0700\n\n    Add result section to manuscript\n\ncommit 7f94f8ed631a7390a910fa13cd4954cf9e8a3061\nAuthor: Marie-Helene Burle <xxx@xxx>\nDate:   Mon Oct 3 18:19:28 2022 -0700\n\n    Initial commit\nAs you can see, commits are listed from the bottom up. You can customize the output of git log by playing with the many existing flags (you can run man git-log to get the list of all flags).\nFor instance, you can display each commit as a one-liner:\ngit log --oneline\na1df8e5 (HEAD -> main) Add .gitignore file with data and results\na049a2f Define the variable a in R script\nc7fc9c1 Add conclusion to the manuscript\n451c47b Add result section to manuscript\n7f94f8e Initial commit\nYou can display it as a graph:\ngit log --graph\n* commit a1df8e56ad45ddd514ff951f2d65e4e1d40a641c (HEAD -> main)\n| Author: Marie-Helene Burle <xxx@xxx>\n| Date:   Mon Oct 3 22:57:59 2022 -0700\n|\n|     Add .gitignore file with data and results\n|\n* commit a049a2f6834801bf76fa3c2c191a59a3ec589d6e\n| Author: Marie-Helene Burle <xxx@xxx>\n| Date:   Mon Oct 3 21:17:23 2022 -0700\n|\n|     Define the variable a in R script\n|\n* commit c7fc9c1743d8a40c3f72d9450b9440dca1cb5922\n| Author: Marie-Helene Burle <xxx@xxx>\n| Date:   Mon Oct 3 21:16:43 2022 -0700\n|\n|     Add conclusion to the manuscript\n|\n* commit 451c47b386895b8b0b5bdd1a8734ef1d51f9ccc9\n| Author: Marie-Helene Burle <xxx@xxx>\n| Date:   Mon Oct 3 18:35:51 2022 -0700\n|\n|     Add result section to manuscript\n|\n* commit 7f94f8ed631a7390a910fa13cd4954cf9e8a3061\n  Author: Marie-Helene Burle <xxx@xxx>\n  Date:   Mon Oct 3 18:19:28 2022 -0700\n\n      Initial commit\n\nHere is an example of more complex customization:\n\ngit log \\\n    --graph \\\n    --date=short \\\n    --pretty=format:'%C(cyan)%h %C(blue)%ar %C(auto)%d'`\n                   `'%C(yellow)%s%+b %C(magenta)%ae'\n* a1df8e5 88 seconds ago  (HEAD -> main)Add .gitignore file with data and results xxx@xxx\n* a049a2f 2 hours ago Define the variable a in R script xxx@xxx\n* c7fc9c1 2 hours ago Add conclusion to the manuscript xxx@xxx\n* 451c47b 4 hours ago Add result section to manuscript xxx@xxx\n* 7f94f8e 5 hours ago Initial commit xxx@xxx"
  },
  {
    "objectID": "git/contrib.html#getting-information-about-a-commit",
    "href": "git/contrib.html#getting-information-about-a-commit",
    "title": "Version control with Git & GitHub",
    "section": "Getting information about a commit",
    "text": "Getting information about a commit\ngit log is useful to get an overview of our project history, but the information we get about each commit is limited. To get additional information about a particular commit, you can use git show followed by the hash of the commit you are interested about.\nFor instance, let‚Äôs explore our second commit:\ngit show 451c47b  # Replace the hash by the hash of your second commit\ncommit 451c47b386895b8b0b5bdd1a8734ef1d51f9ccc9 (HEAD -> main)\nAuthor: Marie-Helene Burle <xxx@xxx>\nDate:   Mon Oct 3 18:35:51 2022 -0700\n\n    Add result section to manuscript\n\ndiff --git a/ms/chapter3.md b/ms/chapter3.md\nindex b88424b..9408f32 100644\n--- a/ms/chapter3.md\n+++ b/ms/chapter3.md\n@@ -7,3 +7,7 @@ Bla bla bla bla bla.\n ## Methods\n\n Bla bla bla.\n+\n+## Results\n+\n+We now have a bunch of results in our markdown manuscript.\nIn addition to displaying the commit metadata, this also displays the difference with the previous commit."
  },
  {
    "objectID": "git/contrib.html#revisiting-old-commits",
    "href": "git/contrib.html#revisiting-old-commits",
    "title": "Version control with Git & GitHub",
    "section": "Revisiting old commits",
    "text": "Revisiting old commits\nThe pointer HEAD, which normally points to the branch main which itself points to latest commit, can be moved around. By moving HEAD to any commit, you can revisit the state of your project at that particular version.\nThe command for this is git checkout followed by the hash of the commit you want to revisit.\n\nFor instance, we could revisit the first commit in our example with:\n\ngit checkout 7f94f8e  # Replace the hash by the hash of your first commit\nNote: switching to '7f94f8e'.\n\nYou are in 'detached HEAD' state. You can look around, make experimental\nchanges and commit them, and you can discard any commits you make in this\nstate without impacting any branches by switching back to a branch.\n\nIf you want to create a new branch to retain commits you create, you may\ndo so (now or later) by using -c with the switch command. Example:\n\n  git switch -c <new-branch-name>\n\nOr undo this operation with:\n\n  git switch -\n\nTurn off this advice by setting config variable advice.detachedHead to false\n\nHEAD is now at 7f94f8e Initial commit\n\nThis is the same as the command git switch --detach 7f94f8e: git switch is a command introduced a few years ago because git checkout can be used for many things in Git and it was confusing many users. git switch allows to switch from one branch to another or, with the --detach flag, to switch to a commit as is the case here.\n\nOnce you have seen what you wanted to see, you can go back to your branch main with:\ngit checkout main\nPrevious HEAD position was 7f94f8e Initial commit\nSwitched to branch 'main'\n\nThis is the same as the command git switch main.\n\nBe careful not to forget to go back to your branch main before making changes to your project. If you want to move the project to a new direction from some old commit, you need to create a new branch before doing so. When HEAD points directly to a commit (and not to a branch), this is called ‚ÄúDetached HEAD‚Äù and it is not a position from which you want to modify the project.\n\nIt is totally fine to move HEAD around and have it point directly to a commit (instead of a branch) as long as you are only looking at a version of your project and get back to a branch before doing some work:"
  },
  {
    "objectID": "git/contrib.html#branches",
    "href": "git/contrib.html#branches",
    "title": "Version control with Git & GitHub",
    "section": "Branches",
    "text": "Branches\nOne of the reasons Git has become so popular is its branch system.\nRemember that little pointer called main? That‚Äôs our main branch: the one Git creates automatically when we create our first commit.\nA branch in Git is just that: a little pointer. This makes creating branches extremely quick and cheap. But they are extremely convenient.\nInstead of checking out a commit as we just saw (which creates a detached HEAD state), we can instead create a new branch on that commit with:\ngit switch -c newbranch 7f94f8e  # Replace the hash by the hash of your first commit\nSwitched to a new branch 'newbranch'\nThis creates a new branch called newbranch on our first commit and switches HEAD to it. If you do this instead of entering a detached HEAD state, it is totally safe to make changes and create commits from there. You can easily switch HEAD back and forth between the two branches with:\ngit switch main        # Moves HEAD back to the branch main\nSwitched to branch 'main'\ngit switch newbranch\nSwitched to branch 'newbranch'\ngit status\nOn branch newbranch\nnothing to commit, working tree clean\nIf you already checked out the commit 7f94f8e with git checkout 7f94f8e, you can create the new branch newbranch on that commit and switch to it with:\ngit switch -c newbranch\nSwitched to a new branch 'newbranch'\nThose are equivalent workflows. Just don‚Äôt forget never to work from a detached HEAD state. You can look around in that state, but that‚Äôs it. Why? Because commits that are not part of a branch get automatically deleted on a regular basis when Git runs its garbage collection. So any commits you make from a detached HEAD will eventually be lost. And that‚Äôs probably not what you want.\nIn short, git switch allows you to switch HEAD from one branch to another. With the -c flag, you can create a new branch before switching to it. And by adding some starting point such as a commit, the new branch gets created on that commit rather than on the position of HEAD.\nNow, have a look at what happens if you run git log from newbranch:\ngit log\ncommit 7f94f8ed631a7390a910fa13cd4954cf9e8a3061 (HEAD -> newbranch)\nAuthor: Marie-Helene Burle <xxx@xxx>\nDate:   Mon Oct 3 18:19:28 2022 -0700\n\n    Initial commit\nHorror! It looks like all our commits except for the first one are gone!\nIn fact, they still exist, but by default, git log only shows what is on the current branch. To see all the commits that are on any branch in your project, you need to add the --all flag:\ngit log --all\ncommit 863afd650ecaeab85da2f8ed0d3c88a778754727 (main)\nAuthor: Marie-Helene Burle <xxx@xxx>\nDate:   Tue Oct 4 10:32:39 2022 -0700\n\n    Add .gitignore file with data and results\n\ncommit dc780c75c76220a39f7c89a76bebb670dad25b8e\nAuthor: Marie-Helene Burle <xxx@xxx>\nDate:   Tue Oct 4 10:32:12 2022 -0700\n\n    Define the variable a in R script\n\ncommit 5ba96b254b505f7d04f59f988a621a746a0c6896\nAuthor: Marie-Helene Burle <xxx@xxx>\nDate:   Tue Oct 4 10:28:51 2022 -0700\n\n    Add conclusion to the manuscript\n\ncommit 451c47b386895b8b0b5bdd1a8734ef1d51f9ccc9\nAuthor: Marie-Helene Burle <xxx@xxx>\nDate:   Mon Oct 3 18:35:51 2022 -0700\n\n    Add result section to manuscript\n\ncommit 7f94f8ed631a7390a910fa13cd4954cf9e8a3061 (HEAD -> newbranch)\nAuthor: Marie-Helene Burle <xxx@xxx>\nDate:   Mon Oct 3 18:19:28 2022 -0700\n\n    Initial commit\n\nIn this log, we can now see main, but that HEAD points to newbranch.\n\n\nListing branches\ngit branch\n  main\n* newbranch\nThe * shows the branch you are currently on (i.e.¬†the branch to which HEAD points to).\n\n\nComparing branches\nYou can use git diff to compare branches:\ngit diff newbranch main\ndiff --git a/.gitignore b/.gitignore\nnew file mode 100644\nindex 0000000..e85f44a\n--- /dev/null\n+++ b/.gitignore\n@@ -0,0 +1,2 @@\n+/data/\n+/results/\ndiff --git a/ms/chapter3.md b/ms/chapter3.md\nindex b88424b..80d2c5c 100644\n--- a/ms/chapter3.md\n+++ b/ms/chapter3.md\n@@ -7,3 +7,11 @@ Bla bla bla bla bla.\n ## Methods\n\n Bla bla bla.\n+\n+## Results\n+\n+We now have a bunch of results in our markdown manuscript.\n+\n+## Conclusion\n+\n+And finally, the great conclusion of our paper.\ndiff --git a/src/chapter3.R b/src/chapter3.R\nindex 95f1592..2bf030d 100644\n--- a/src/chapter3.R\n+++ b/src/chapter3.R\n@@ -7,3 +7,5 @@ df <- data.frame(\n )\n\n ggplot(df, aes(x, y)) + geom_point()\n+\n+a = 23\nThis shows all the lines that have been modified (added or deleted) between the commits both branches point to.\n\n\nMerging branches\nIf you want to merge branches, switch to the branch you want to merge into the other one, then run git merge.\nFor instance, if we want to merge newbranch onto main, we would first switch to newbranch (we are already on it, so nothing to do here), then:\ngit merge main\nUpdating 7f94f8e..863afd6\nFast-forward\n .gitignore     | 2 ++\n ms/chapter3.md | 8 ++++++++\n src/chapter3.R | 2 ++\n 3 files changed, 12 insertions(+)\n create mode 100644 .gitignore\nThis merge is called a ‚Äúfast-forward‚Äù merge because main and newbranch had not diverged. It was simply a question of having newbranch catch up to main.\nIf you run git log again, you will see that newbrach has now caught up with main:\ngit log\ncommit 863afd650ecaeab85da2f8ed0d3c88a778754727 (HEAD -> newbranch, main)\nAuthor: Marie-Helene Burle <xxx@xxx>\nDate:   Tue Oct 4 10:32:39 2022 -0700\n\n    Add .gitignore file with data and results\n\ncommit dc780c75c76220a39f7c89a76bebb670dad25b8e\nAuthor: Marie-Helene Burle <xxx@xxx>\nDate:   Tue Oct 4 10:32:12 2022 -0700\n\n    Define the variable a in R script\n\ncommit 5ba96b254b505f7d04f59f988a621a746a0c6896\nAuthor: Marie-Helene Burle <xxx@xxx>\nDate:   Tue Oct 4 10:28:51 2022 -0700\n\n    Add conclusion to the manuscript\n\ncommit 451c47b386895b8b0b5bdd1a8734ef1d51f9ccc9\nAuthor: Marie-Helene Burle <xxx@xxx>\nDate:   Mon Oct 3 18:35:51 2022 -0700\n\n    Add result section to manuscript\n\ncommit 7f94f8ed631a7390a910fa13cd4954cf9e8a3061\nAuthor: Marie-Helene Burle <xxx@xxx>\nDate:   Mon Oct 3 18:19:28 2022 -0700\n\n    Initial commit\n\nHere is a classic situation of fast-forward merge.\nInstead of working on your branch main, you create a test branch and work on it (so HEAD is on the branch test and both move along as you create commits):\n\nWhen you are happy with the changes you made on your test branch, you decide to merge main onto it.\nFirst, you switch to main:\n\nThen you do the fast-forward merge from main onto test (so main catches up to test):\n\nThen, usually, you delete the branch test as it has served its purpose (with git branch -d test). Alternatively, you can switch back to it and do the next bit of experimental work in it. This allows to keep main free of possible mishaps and bad developments (if you aren‚Äôt happy with the work you did on your test branch, you can simply delete it and Git will clean the commits that are on it but not on main during the next garbage collection.\n\n\nIf both branches have diverged (you created commits from both main and newbranch), the merge would require the creation of an additional commit called a ‚Äúmerge commit‚Äù.\n\nHere is a classic situation of merge with a commit.\n\nYou create a test branch and switch to it:\n\n\nThen you create some commits:\n\n\nNow you switch back to main:\n\nAnd you create commits from main too:\n\n\nTo merge your main branch and your test branch, a new commit is now required (note that the command is the same as in the case of a fast-forward merge: git merge. Git will create the new commit automatically. As long as there is no conflict, it is just as easy as a fast-forward merge. We will talk later about resolving conflicts).\n\nAfter which, you can delete the (now useless) test branch (with git branch -d test2):"
  },
  {
    "objectID": "git/contrib.html#what-are-remotes",
    "href": "git/contrib.html#what-are-remotes",
    "title": "Version control with Git & GitHub",
    "section": "What are remotes?",
    "text": "What are remotes?\nRemotes are copies of a project and its history.\nThey can be located anywhere, including on external drive or on the same machine as the project, although they are often on a different machine to serve as backup, or on a network (e.g.¬†internet) to serve as a syncing hub for collaborations.\nPopular online Git repository managers & hosting services:\n\nGitHub\nGitLab\nBitbucket"
  },
  {
    "objectID": "git/contrib.html#creating-a-remote-on-github",
    "href": "git/contrib.html#creating-a-remote-on-github",
    "title": "Version control with Git & GitHub",
    "section": "Creating a remote on GitHub",
    "text": "Creating a remote on GitHub\n\nCreate a free GitHub account\nSign up for a free GitHub account.\nLater on, to avoid having to type your password all the time, you should set up SSH for your account.\n\n\nCreate an empty repository on GitHub\n\nGo to the GitHub website, login, and go to your home page.\nLook for the Repositories tab & click the green New button.\nEnter the name you want for your repo, without spaces.\nMake the repository public or private.\n\n\n\nLink empty repository to your repo\nClick on the Code green drop-down button, select SSH if you have set SSH for your GitHub account or HTTPS and copy the address.\nIn the command line, cd inside your project, and add the remote:\ngit remote add <remote-name> <remote-address>\nremote-name is a convenience name to identify that remote. You can choose any name, but since Git automatically call the remote origin when you clone a repo, it is common practice to use origin as the name for the first remote.\n\nExample (using an SSH address):\n\ngit remote add origin git@github.com:<user>/<repo>.git\n\nExample (using an HTTPS address):\n\ngit remote add origin https://github.com/<user>/<repo>.git"
  },
  {
    "objectID": "git/contrib.html#getting-information-on-remotes",
    "href": "git/contrib.html#getting-information-on-remotes",
    "title": "Version control with Git & GitHub",
    "section": "Getting information on remotes",
    "text": "Getting information on remotes\nList remotes:\ngit remote\nList remotes with their addresses:\ngit remote -v\nGet more information on a remote:\ngit remote show <remote-name>\n\nExample:\n\ngit remote show origin"
  },
  {
    "objectID": "git/contrib.html#managing-remotes",
    "href": "git/contrib.html#managing-remotes",
    "title": "Version control with Git & GitHub",
    "section": "Managing remotes",
    "text": "Managing remotes\nRename a remote:\ngit remote rename <old-remote-name> <new-remote-name>\nDelete a remote:\ngit remote remove <remote-name>\nChange the address of a remote:\ngit remote set-url <remote-name> <new-url> [<old-url>]"
  },
  {
    "objectID": "git/contrib.html#getting-data-from-a-remote",
    "href": "git/contrib.html#getting-data-from-a-remote",
    "title": "Version control with Git & GitHub",
    "section": "Getting data from a remote",
    "text": "Getting data from a remote\nIf you collaborate on a project, you have to get the data added by your teammates to keep your local project up to date.\nTo download new data from a remote, you have 2 options:\n\ngit fetch\ngit pull\n\n\nFetching changes\nFetching downloads the data from a remote that you don‚Äôt already have in your local version of the project:\ngit fetch <remote-name>\nThe branches on the remote are now accessible locally as <remote-name>/<branch>. You can inspect them or you can merge them into your local branches.\n\nExample:\n\ngit fetch origin\n\n\nPulling changes\nPulling fetches the changes & merges them onto your local branches:\ngit pull <remote-name> <branch>\n\nExample:\n\ngit pull origin main\nIf your branch is already tracking a remote branch, you can omit the arguments:\ngit pull"
  },
  {
    "objectID": "git/contrib.html#pushing-to-a-remote",
    "href": "git/contrib.html#pushing-to-a-remote",
    "title": "Version control with Git & GitHub",
    "section": "Pushing to a remote",
    "text": "Pushing to a remote\nUploading data to the remote is called pushing:\ngit push <remote-name> <branch-name>\n\nExample:\n\ngit push origin main\nYou can set an upstream branch to track a local branch with the -u flag:\ngit push -u <remote-name> <branch-name>\n\nExample:\n\ngit push -u origin main\nFrom now on, all you have to run when you are on main is:\ngit push\n \n\nby jscript"
  },
  {
    "objectID": "git/contrib.html#setup",
    "href": "git/contrib.html#setup",
    "title": "Version control with Git & GitHub",
    "section": "Setup",
    "text": "Setup\nWhen you collaborate with others using Git and GitHub, there are three possible situations:\n\nYou create a project on your machine and want others to contribute to it (1).\nYou want to contribute to a project started by others & ‚Ä¶\n‚ÄÉ‚ÄÉ‚Ä¶ you have write access to it (2).\n‚ÄÉ‚ÄÉ‚Ä¶ you do not have write access to it (3).\n\n\n(1) You start the project\n\nCreate a remote on GitHub\nThese are the steps we already saw earlier.\n\n1. Create an empty repository on GitHub\n\nGo to the GitHub website, login, and go to your home page.\nLook for the Repositories tab & click the green New button.\nEnter the name you want for your repo, without spaces.\nMake the repository public or private.\n\n\n\n2. Link empty repository to your repo\nClick on the Code green drop-down button, select SSH if you have set SSH for your GitHub account or HTTPS and copy the address.\nIn the command line, cd inside your project, and add the remote:\ngit remote add <remote-name> <remote-address>\nremote-name is a convenience name to identify that remote. You can choose any name, but since Git automatically call the remote origin when you clone a repo, it is common practice to use origin as the name for the first remote.\n\nExample (using an SSH address):\n\ngit remote add origin git@github.com:<user>/<repo>.git\n\nExample (using an HTTPS address):\n\ngit remote add origin https://github.com/<user>/<repo>.git\nIf you are working alone on this project and you only wanted to have a remote for backup, you are set.\nIf you don‚Äôt want to grant others write access to the project, and you only accept contributions through pull requests, you are also set.\nIf you want to grant your collaborators write access to the project however, you need to add them to it.\n\n\n\nInvite collaborators\n\nGo to your GitHub project page.\nClick on the Settings tab.\nClick on the Manage access section on the left-hand side (you will be prompted for your GitHub password).\nClick on the Invite a collaborator green button.\nInvite your collaborators with one of their GitHub user name, their email address, or their full name.\n\n\n\n\n(2) Write access to project\n\nClone project\ncd to location where you want your local copy, then:\ngit clone <remote-address> <local-name>\nThis sets the project as a remote to your new local copy and that remote is automatically called origin.\nWithout <local-name>, the repo will have the name of the last part of the remote address.\n\n\n\n(3) No write access to project\n\nCollaborate without write access\nIn that case, you will have to submit a pull request:\n\nFork the project on GitHub.\nClone your fork on your machine.\nAdd the initial project as a second remote & call it upstream.\nPull from upstream to update your local project.\nCreate & checkout a new branch.\nMake & commit your changes on that branch.\nPush that branch to your fork (i.e.¬†origin ‚Äî remember that you do not have write access to upstream).\nGo to the original project GitHub‚Äôs page & open a pull request."
  },
  {
    "objectID": "git/contrib.html#workflow",
    "href": "git/contrib.html#workflow",
    "title": "Version control with Git & GitHub",
    "section": "Workflow",
    "text": "Workflow\nWhen you collaborate with others using GitHub (or other equivalent service), you and others will work simultaneously on some project. How does this work?\nRemember that to upload your changes to the remote on GitHub you push to it with git push.\nIf one of your collaborators has made changes to the remote (pushing from their own machine), you won‚Äôt be able to push. Instead, you will get the following message:\nTo xxx.git\n ! [rejected]        main -> main (fetch first)\nerror: failed to push some refs to 'xxx.git'\nhint: Updates were rejected because the remote contains work that you do\nhint: not have locally. This is usually caused by another repository pushing\nhint: to the same ref. You may want to first integrate the remote changes\nhint: (e.g., 'git pull ...') before pushing again.\nhint: See the 'Note about fast-forwards' in 'git push --help' for details.\nThe solution?\nYou first have to download (git pull) their work onto your machine, merge it with yours (which will happen automatically if there are no conflicts), before you can push your work to GitHub.\nNow‚Ä¶ what if there are conflicts?"
  },
  {
    "objectID": "git/contrib.html#resolving-conflicts",
    "href": "git/contrib.html#resolving-conflicts",
    "title": "Version control with Git & GitHub",
    "section": "Resolving conflicts",
    "text": "Resolving conflicts\nGit works line by line. As long as your collaborators and you aren‚Äôt working on the same line(s) of the same file(s) at the same time, there will not be any problem. If however you modified one or more of the same line(s) of the same file(s), Git will not be able to decide which version should be kept. When you git pull their work on your machine, the automatic merging will get interrupted and Git will ask you to resolve the conflict(s) before the merge can resume. It will conveniently tell you which file(s) contain the conflict(s).\nThere are fancy tools to resolve conflicts, but you can do it in any text editor: simply open the file(s) listed by Git as having conflicts and look for the following markers:\n<<<<<<< HEAD\nThis is your version.\n=======\nThis is the alternative version of the same section of the file.\n>>>>>>> alternative version\nThese markers are added by Git to signal the areas of conflict. It is up to you to choose between the two versions (or create a third one) and remove the conflict markers. After that, you can stage the file(s) which contained the conflicts to finish the merge (and then you can commit)."
  },
  {
    "objectID": "git/intro.html",
    "href": "git/intro.html",
    "title": "Version control with Git & GitHub",
    "section": "",
    "text": "Git is a free and open source version control system (a software that tracks changes to your files, allowing you to revisit or revert to older versions).\nUsing Internet hosting services such as GitHub, Git is also a powerful collaboration tool.\nIn this introductory workshop you will learn the basics of working with Git on the command line."
  },
  {
    "objectID": "git/intro.html#what-is-a-version-control-system",
    "href": "git/intro.html#what-is-a-version-control-system",
    "title": "Version control with Git & GitHub",
    "section": "What is a version control system?",
    "text": "What is a version control system?\n\n\n\n\nfrom PhD\n\n\nWhenever we work on an important document, we intuitively realize that it is important to keep key versions (e.g.¬†the version of a manuscript that we sent to our supervisor, the revised version after we addressed their comments, the revised version after we addressed reviewer comments, etc.). We have all been there ‚Ä¶ The versions accumulate with names that are often less than helpful ‚Ä¶\n\n\n\n\n\nfrom Geek&Poke\n\n\n‚Ä¶ and soon enough, it is hell. This is a form of versioning, but a terribly messy and inefficient one. Version control systems are software that allow to handle this much more effectively."
  },
  {
    "objectID": "git/intro.html#which-version-control-system-should-i-use",
    "href": "git/intro.html#which-version-control-system-should-i-use",
    "title": "Version control with Git & GitHub",
    "section": "Which version control system should I use?",
    "text": "Which version control system should I use?\nIf the trends of Google searches of the existing version control systems are any indication of their popularity, we can say that Git has crushed the competition since 2010.\n\n\nNowadays, it is indeed extremely rare to come across any other version control system.\nGit is simply that good üôÇ"
  },
  {
    "objectID": "git/intro.html#installing-git",
    "href": "git/intro.html#installing-git",
    "title": "Version control with Git & GitHub",
    "section": "Installing Git",
    "text": "Installing Git\n\nMacOS & Linux users\nInstall Git from the official website.\n\n\nWindows users\nInstall Git for Windows. This will also install Git Bash, a Bash emulator."
  },
  {
    "objectID": "git/intro.html#using-git",
    "href": "git/intro.html#using-git",
    "title": "Version control with Git & GitHub",
    "section": "Using Git",
    "text": "Using Git\nWe will use Git from the command line throughout this workshop.\nMacOS users: ‚ÄÉ‚ÄÉ‚ÄÇopen Terminal.\nWindows users: ‚ÄÉ‚ÄÇopen Git Bash.\nLinux users: ‚ÄÉ‚ÄÉ‚ÄÉopen the terminal emulator of your choice."
  },
  {
    "objectID": "git/intro.html#configuring-git",
    "href": "git/intro.html#configuring-git",
    "title": "Version control with Git & GitHub",
    "section": "Configuring Git",
    "text": "Configuring Git\nBefore you can use Git, you need to set some basic configuration. You will do this in the terminal you just opened.\n\nUser identity\ngit config --global user.name \"<Your Name>\"\ngit config --global user.email \"<your@email>\"\n\nExample:\n\ngit config --global user.name \"John Doe\"\ngit config --global user.email \"john.doe@gmail.com\"\n\n\nText editor\ngit config --global core.editor \"<text-editor>\"\n\nExample for nano:\n\ngit config --global core.editor \"nano\"\n\n\nLine ending\n\nmacOS, Linux, or WSL\ngit config --global core.autocrlf input\n\n\nWindows\ngit config --global core.autocrlf true\n\n\n\nList settings\ngit config --list"
  },
  {
    "objectID": "git/intro.html#internal-documentation",
    "href": "git/intro.html#internal-documentation",
    "title": "Version control with Git & GitHub",
    "section": "Internal documentation",
    "text": "Internal documentation\n\nMan pages\ngit <command> --help\ngit help <command>\nman git-<command>\n\nExample:\n\ngit commit --help\ngit help commit\nman git-commit\n\nUseful keybindings when you are in the pager\nSPACE      scroll one screen down\nb          scroll one screen up\nq          quit\n\n\n\nCommand options\ngit <command> -h\n\nExample:\n\ngit commit -h"
  },
  {
    "objectID": "git/intro.html#online-documentation",
    "href": "git/intro.html#online-documentation",
    "title": "Version control with Git & GitHub",
    "section": "Online documentation",
    "text": "Online documentation\n\nOfficial Git manual\nOpen source Pro Git book\n\n\nCourses & workshops\n\nWestern Canada Research Computing Git workshops\nWestGrid Summer School 2020 Git course\nWestGrid Autumn School 2020 Git course\nSoftware Carpentry Git lesson\n\n\n\nQ & A\n\nStack Overflow [git] tag"
  },
  {
    "objectID": "git/intro.html#mock-project",
    "href": "git/intro.html#mock-project",
    "title": "Version control with Git & GitHub",
    "section": "Mock project",
    "text": "Mock project\nLet‚Äôs imagine that you have been working on chapter 3 of your thesis for some time, without using a version control system. We will put that chapter under version control and see how you should work from now on.\nFirst, we need to create a mock set of documents.\n\nNavigate to a suitable location\n\ncd </some/suitable/location/in/your/computer>\n\nCreate the directory at the root of chapter 3\n\nmkdir chapter3\n\nMake sure not to use any spaces in the name: Git doesn‚Äôt work well with spaces.\n\n\nCreate a number of subdirectories\n\nmkdir chapter3/src chapter3/ms chapter3/data chapter3/results\n\nCreate a mock manuscript\n\necho \"# Chapter 3\n## Introduction\nBla bla bla bla bla.\n## Methods\nBla bla bla.\" > chapter3/ms/chapter3.md\n\nGit can only version text files. If you write your papers or thesis chapter in text files (e.g.¬†markdown, LaTeX, org-mode), you will be able to put them under version control, which is really convenient. If you use a word processor, you won‚Äôt be able to.\n\n\nCreate a mock R script\n\necho \"library(ggplot2)\nlibrary(dplyr)\n\ndf <- data.frame(\n  x = (1:5),\n  y = (1:5)\n)\n\nggplot(df, aes(x, y)) + geom_point()\" > chapter3/src/chapter3.R\n\nEven if you use a word processor for your writing, your scripts (e.g.¬†in Python, R, etc.) will be written in text files. So you will always be able to put at least those files under version control."
  },
  {
    "objectID": "git/intro.html#initializing-a-git-repository",
    "href": "git/intro.html#initializing-a-git-repository",
    "title": "Version control with Git & GitHub",
    "section": "Initializing a Git repository",
    "text": "Initializing a Git repository\nMake sure to enter the project before initializing the repository.\ncd chapter3\nNow, you can run the command that will turn your chapter3 directory into a Git repository:\ngit init\nInitialized empty Git repository in chapter3/.git/\n\nGit is very verbose: you will often get useful feed-back after running commands.\n\nWhen you run this command, Git creates a .git repository. This is where it will store all its files.\nYou can see that this repository was created by running:\nls -a\n.\n..\n.git\ndata\nms\nresults\nsrc\n\nIf you run git init in the wrong location, you can easily fix this: simply delete the .git directory that you created!"
  },
  {
    "objectID": "git/intro.html#creating-commits",
    "href": "git/intro.html#creating-commits",
    "title": "Version control with Git & GitHub",
    "section": "Creating commits",
    "text": "Creating commits\nYou can think of a commit as a snapshot of a particular version of your project.\nYou should create a new commit whenever you think that your project is at a point to which you might want to go back to.\nLet‚Äôs create a first commit with the state of our chapter 3 before we do any more work to it:\ngit add .\ngit commit -m \"Initial commit\"\n[main (root-commit) 7f94f8e] Initial commit\n 2 files changed, 18 insertions(+)\n create mode 100644 ms/chapter3.md\n create mode 100644 src/chapter3.R\nTo create a commit, we first need to add the file(s) we want to add to our commit to the staging area (also called ‚Äúindex‚Äù). This is done with the command git add. To add all the files, we can use git add . (. represents the current directory).\nOnce we have added some files to the staging area, we can create a commit. But each commit has a message associated to it. One way to add this message is to use the command to create commits (git commit) with the -m flag (for ‚Äúmessage‚Äù). Here, our message is simply ‚ÄúInitial commit‚Äù.\n\nGit saves the history of a project as a series of snapshots:\n\nThose snapshots are called commits:\n\nEach commit is identified by a unique hash:\n\nEach commit contains these metadata:\n\nauthor,\ndate and time,\nthe hash of parent commit(s),\na message.\n\nAs soon as you create the first commit, a pointer called a branch is created and it points to that commit. By default, that first branch is called main:\n\nAnother pointer (HEAD) points to the branch main.\nHEAD indicates where we are in the project history.\n\n\nWe can now do some work in our chapter 3. For instance, let‚Äôs imagine that we are adding a result section to our chapter3.md file.\necho \"\n## Results\n\nWe now have a bunch of results in our markdown manuscript.\" >> ms/chapter3.md\nMake sure to use >> here and not >: >> prepends content while > replaces any existing content.\nIf this new addition is important enough to justify making a new commit (how often you commit is up to you), we can do so:\ngit add ms/chapter3.md\ngit commit -m \"Add result section to manuscript\"\n[main 451c47b] Add result section to manuscript\n 1 file changed, 4 insertions(+)\n\nAs you create more commits, the history of your project grows ‚Ä¶\n\n‚Ä¶ and the pointers HEAD and main automatically move to the last commit:\n\nFor simplicity, the diagrams can be simplified this way:"
  },
  {
    "objectID": "git/intro.html#understanding-the-staging-area",
    "href": "git/intro.html#understanding-the-staging-area",
    "title": "Version control with Git & GitHub",
    "section": "Understanding the staging area",
    "text": "Understanding the staging area\nNew Git users are often confused about the two-step commit process (first, you stage with git add, then you commit with git commit). This intermediate step seems, at first, totally unnecessary. In fact, it is very useful: without it, commits would always include all new changes made to a project and they would thus be very messy. The staging area allows to prepare (‚Äústage‚Äù) the next commit. This way, you only commit what you want when you want.\n\nLet‚Äôs go over a simple example:\n\nWe don‚Äôt always work linearly. Maybe you are working on a section of your manuscript when you realize by chance that there is a mistake in your script. You fix that mistake. On your next commit, it might make little sense to commit together that fix and your manuscript changes since they are not related. If your commits are random bag of changes, it will be very hard for future you to navigate your project history.\nIt is a lot better to only stage your script fix, commit it, then only stage your manuscript update, and commit this in a different commit.\nThe staging area allows you to pick and chose the changes from one or various files that constitute some coherent change to the project and that make sense to commit together."
  },
  {
    "objectID": "git/intro.html#inspecting-changes",
    "href": "git/intro.html#inspecting-changes",
    "title": "Version control with Git & GitHub",
    "section": "Inspecting changes",
    "text": "Inspecting changes\n\nList of modified files\nOne command you will run often when working with Git is git status:\ngit status\nOn branch main\nnothing to commit, working tree clean\nThis means that we are on the branch main (that‚Äôs the only branch in our repo at this point) and that all changes to our project have been committed.\nLet‚Äôs modify a file and see what happens:\necho \"\n## Conclusion\n\nAnd finally, the great conclusion of our paper.\" >> ms/chapter3.md\ngit status\nOn branch main\nChanges not staged for commit:\n  (use \"git add <file>...\" to update what will be committed)\n  (use \"git restore <file>...\" to discard changes in working directory)\n        modified:   ms/chapter3.md\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\nLet‚Äôs modify another file:\necho \"\na = 23\" >> src/chapter3.R\ngit status\nOn branch main\nChanges not staged for commit:\n  (use \"git add <file>...\" to update what will be committed)\n  (use \"git restore <file>...\" to discard changes in working directory)\n        modified:   ms/chapter3.md\n        modified:   src/chapter3.R\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\nMaybe we don‚Äôt want to create a commit with all those changes, so we only stage the changes made to the manuscript:\ngit add ms/chapter3.md\nThen we check the status of our repository again:\ngit status\nOn branch main\nChanges to be committed:\n  (use \"git restore --staged <file>...\" to unstage)\n        modified:   ms/chapter3.md\n\nChanges not staged for commit:\n  (use \"git add <file>...\" to update what will be committed)\n  (use \"git restore <file>...\" to discard changes in working directory)\n        modified:   src/chapter3.R\nIf we ran git commit at this point, we would create a new commit with the changes made to the manuscript.\n\n\nList of actual changes\nWhile git status gives us the list of new files and files with changes, it doesn‚Äôt allow us to see what those changes are. For this, we need a new command: git diff.\n\nDifference between the working directory and the index\ngit diff shows the difference between the working directory (our actual files) and the index (staging area):\ngit diff\ndiff --git a/src/chapter3.R b/src/chapter3.R\nindex 95f1592..2bf030d 100644\n--- a/src/chapter3.R\n+++ b/src/chapter3.R\n@@ -7,3 +7,5 @@ df <- data.frame(\n )\n\n ggplot(df, aes(x, y)) + geom_point()\n+\n+a = 23\nThis allows us to see that src/chapter3.R has a new line (a = 23) and that it is not yet staged.\n\n\nDifference between the index and your last commit\nTo see what would be committed if you ran git commit (so, to see the difference between the index and the last commit), you need to run instead:\ngit diff --cached\ndiff --git a/ms/chapter3.md b/ms/chapter3.md\nindex 9408f32..80d2c5c 100644\n--- a/ms/chapter3.md\n+++ b/ms/chapter3.md\n@@ -11,3 +11,7 @@ Bla bla bla.\n ## Results\n\n We now have a bunch of results in our markdown manuscript.\n+\n+## Conclusion\n+\n+And finally, the great conclusion of our paper.\nThis shows us the changes that we have staged but not yet committed (the changes to our manuscript).\n\n\nDifference between the working directory and your last commit\nThis means, both of the above. This can been displayed with:\ngit diff HEAD\ndiff --git a/ms/chapter3.md b/ms/chapter3.md\nindex 9408f32..80d2c5c 100644\n--- a/ms/chapter3.md\n+++ b/ms/chapter3.md\n@@ -11,3 +11,7 @@ Bla bla bla.\n ## Results\n\n We now have a bunch of results in our markdown manuscript.\n+\n+## Conclusion\n+\n+And finally, the great conclusion of our paper.\ndiff --git a/src/chapter3.R b/src/chapter3.R\nindex 95f1592..2bf030d 100644\n--- a/src/chapter3.R\n+++ b/src/chapter3.R\n@@ -7,3 +7,5 @@ df <- data.frame(\n )\n\n ggplot(df, aes(x, y)) + geom_point()\n+\n+a = 23\nNow, let‚Äôs clean up our working directory by creating two new commits:\ngit commit -m \"Add conclusion to the manuscript\"\n[main c7fc9c1] Add conclusion to the manuscript\n 1 file changed, 4 insertions(+)\ngit add .\ngit commit -m \"Define the variable a in R script\"\n[main a049a2f] Define the variable a in R script\n 1 file changed, 2 insertions(+)\nIf we look at the status of our repository now, we can see that it is clean again:\ngit status\nOn branch main\nnothing to commit, working tree clean"
  },
  {
    "objectID": "git/intro.html#ignoring",
    "href": "git/intro.html#ignoring",
    "title": "Version control with Git & GitHub",
    "section": "Ignoring",
    "text": "Ignoring\nNot everything should be under version control. For instance, you don‚Äôt want to put under version control non-text files or your initial data. You also shouldn‚Äôt put under version control documents that can be easily recreated such as graphs and script outputs.\nHowever, you don‚Äôt want to have such documents constantly showing up when you run git status. In order to have a clean working directory while keeping them out of version control, you can create a file called .gitignore and add to it a list of files or patterns that you want Git to disregard.\nFor instance:\necho \"/data/\n/results/\" > .gitignore\nThis creates a .gitignore file with two entries (/data/ and /results/) and from now on, any file in either of these directories will be ignored by Git.\nThe .gitignore is a file like any other file, so you‚Äôll want to stage and commit it:\ngit status\nOn branch main\nUntracked files:\n  (use \"git add <file>...\" to include in what will be committed)\n        .gitignore\n\nnothing added to commit but untracked files present (use \"git add\" to track)\ngit add .gitignore\ngit commit -m \"Add .gitignore file with data and results\"\n[main a1df8e5] Add .gitignore file with data and results\n 1 file changed, 2 insertions(+)\n create mode 100644 .gitignore\ngit status\nOn branch main\nnothing to commit, working tree clean"
  },
  {
    "objectID": "git/intro.html#displaying-the-commit-history",
    "href": "git/intro.html#displaying-the-commit-history",
    "title": "Version control with Git & GitHub",
    "section": "Displaying the commit history",
    "text": "Displaying the commit history\nSo far, we have created 5 commits. To display them, you use the command git log:\ngit log\ncommit a1df8e56ad45ddd514ff951f2d65e4e1d40a641c (HEAD -> main)\nAuthor: Marie-Helene Burle <xxx@xxx>\nDate:   Mon Oct 3 22:57:59 2022 -0700\n\n    Add .gitignore file with data and results\n\ncommit a049a2f6834801bf76fa3c2c191a59a3ec589d6e\nAuthor: Marie-Helene Burle <xxx@xxx>\nDate:   Mon Oct 3 21:17:23 2022 -0700\n\n    Define the variable a in R script\n\ncommit c7fc9c1743d8a40c3f72d9450b9440dca1cb5922\nAuthor: Marie-Helene Burle <xxx@xxx>\nDate:   Mon Oct 3 21:16:43 2022 -0700\n\n    Add conclusion to the manuscript\n\ncommit 451c47b386895b8b0b5bdd1a8734ef1d51f9ccc9\nAuthor: Marie-Helene Burle <xxx@xxx>\nDate:   Mon Oct 3 18:35:51 2022 -0700\n\n    Add result section to manuscript\n\ncommit 7f94f8ed631a7390a910fa13cd4954cf9e8a3061\nAuthor: Marie-Helene Burle <xxx@xxx>\nDate:   Mon Oct 3 18:19:28 2022 -0700\n\n    Initial commit\nAs you can see, commits are listed from the bottom up. You can customize the output of git log by playing with the many existing flags (you can run man git-log to get the list of all flags).\nFor instance, you can display each commit as a one-liner:\ngit log --oneline\na1df8e5 (HEAD -> main) Add .gitignore file with data and results\na049a2f Define the variable a in R script\nc7fc9c1 Add conclusion to the manuscript\n451c47b Add result section to manuscript\n7f94f8e Initial commit\nYou can display it as a graph:\ngit log --graph\n* commit a1df8e56ad45ddd514ff951f2d65e4e1d40a641c (HEAD -> main)\n| Author: Marie-Helene Burle <xxx@xxx>\n| Date:   Mon Oct 3 22:57:59 2022 -0700\n|\n|     Add .gitignore file with data and results\n|\n* commit a049a2f6834801bf76fa3c2c191a59a3ec589d6e\n| Author: Marie-Helene Burle <xxx@xxx>\n| Date:   Mon Oct 3 21:17:23 2022 -0700\n|\n|     Define the variable a in R script\n|\n* commit c7fc9c1743d8a40c3f72d9450b9440dca1cb5922\n| Author: Marie-Helene Burle <xxx@xxx>\n| Date:   Mon Oct 3 21:16:43 2022 -0700\n|\n|     Add conclusion to the manuscript\n|\n* commit 451c47b386895b8b0b5bdd1a8734ef1d51f9ccc9\n| Author: Marie-Helene Burle <xxx@xxx>\n| Date:   Mon Oct 3 18:35:51 2022 -0700\n|\n|     Add result section to manuscript\n|\n* commit 7f94f8ed631a7390a910fa13cd4954cf9e8a3061\n  Author: Marie-Helene Burle <xxx@xxx>\n  Date:   Mon Oct 3 18:19:28 2022 -0700\n\n      Initial commit\n\nHere is an example of more complex customization:\n\ngit log \\\n    --graph \\\n    --date=short \\\n    --pretty=format:'%C(cyan)%h %C(blue)%ar %C(auto)%d'`\n                   `'%C(yellow)%s%+b %C(magenta)%ae'\n* a1df8e5 88 seconds ago  (HEAD -> main)Add .gitignore file with data and results xxx@xxx\n* a049a2f 2 hours ago Define the variable a in R script xxx@xxx\n* c7fc9c1 2 hours ago Add conclusion to the manuscript xxx@xxx\n* 451c47b 4 hours ago Add result section to manuscript xxx@xxx\n* 7f94f8e 5 hours ago Initial commit xxx@xxx"
  },
  {
    "objectID": "git/intro.html#getting-information-about-a-commit",
    "href": "git/intro.html#getting-information-about-a-commit",
    "title": "Version control with Git & GitHub",
    "section": "Getting information about a commit",
    "text": "Getting information about a commit\ngit log is useful to get an overview of our project history, but the information we get about each commit is limited. To get additional information about a particular commit, you can use git show followed by the hash of the commit you are interested about.\nFor instance, let‚Äôs explore our second commit:\ngit show 451c47b  # Replace the hash by the hash of your second commit\ncommit 451c47b386895b8b0b5bdd1a8734ef1d51f9ccc9 (HEAD -> main)\nAuthor: Marie-Helene Burle <xxx@xxx>\nDate:   Mon Oct 3 18:35:51 2022 -0700\n\n    Add result section to manuscript\n\ndiff --git a/ms/chapter3.md b/ms/chapter3.md\nindex b88424b..9408f32 100644\n--- a/ms/chapter3.md\n+++ b/ms/chapter3.md\n@@ -7,3 +7,7 @@ Bla bla bla bla bla.\n ## Methods\n\n Bla bla bla.\n+\n+## Results\n+\n+We now have a bunch of results in our markdown manuscript.\nIn addition to displaying the commit metadata, this also displays the difference with the previous commit."
  },
  {
    "objectID": "git/intro.html#revisiting-old-commits",
    "href": "git/intro.html#revisiting-old-commits",
    "title": "Version control with Git & GitHub",
    "section": "Revisiting old commits",
    "text": "Revisiting old commits\nThe pointer HEAD, which normally points to the branch main which itself points to latest commit, can be moved around. By moving HEAD to any commit, you can revisit the state of your project at that particular version.\nThe command for this is git checkout followed by the hash of the commit you want to revisit.\n\nFor instance, we could revisit the first commit in our example with:\n\ngit checkout 7f94f8e  # Replace the hash by the hash of your first commit\nNote: switching to '7f94f8e'.\n\nYou are in 'detached HEAD' state. You can look around, make experimental\nchanges and commit them, and you can discard any commits you make in this\nstate without impacting any branches by switching back to a branch.\n\nIf you want to create a new branch to retain commits you create, you may\ndo so (now or later) by using -c with the switch command. Example:\n\n  git switch -c <new-branch-name>\n\nOr undo this operation with:\n\n  git switch -\n\nTurn off this advice by setting config variable advice.detachedHead to false\n\nHEAD is now at 7f94f8e Initial commit\n\nThis is the same as the command git switch --detach 7f94f8e: git switch is a command introduced a few years ago because git checkout can be used for many things in Git and it was confusing many users. git switch allows to switch from one branch to another or, with the --detach flag, to switch to a commit as is the case here.\n\nOnce you have seen what you wanted to see, you can go back to your branch main with:\ngit checkout main\nPrevious HEAD position was 7f94f8e Initial commit\nSwitched to branch 'main'\n\nThis is the same as the command git switch main.\n\nBe careful not to forget to go back to your branch main before making changes to your project. If you want to move the project to a new direction from some old commit, you need to create a new branch before doing so. When HEAD points directly to a commit (and not to a branch), this is called ‚ÄúDetached HEAD‚Äù and it is not a position from which you want to modify the project.\n\nIt is totally fine to move HEAD around and have it point directly to a commit (instead of a branch) as long as you are only looking at a version of your project and get back to a branch before doing some work:"
  },
  {
    "objectID": "git/intro.html#branches",
    "href": "git/intro.html#branches",
    "title": "Version control with Git & GitHub",
    "section": "Branches",
    "text": "Branches\nOne of the reasons Git has become so popular is its branch system.\nRemember that little pointer called main? That‚Äôs our main branch: the one Git creates automatically when we create our first commit.\nA branch in Git is just that: a little pointer. This makes creating branches extremely quick and cheap. But they are extremely convenient.\nInstead of checking out a commit as we just saw (which creates a detached HEAD state), we can instead create a new branch on that commit with:\ngit switch -c newbranch 7f94f8e  # Replace the hash by the hash of your first commit\nSwitched to a new branch 'newbranch'\nThis creates a new branch called newbranch on our first commit and switches HEAD to it. If you do this instead of entering a detached HEAD state, it is totally safe to make changes and create commits from there. You can easily switch HEAD back and forth between the two branches with:\ngit switch main        # Moves HEAD back to the branch main\nSwitched to branch 'main'\ngit switch newbranch\nSwitched to branch 'newbranch'\ngit status\nOn branch newbranch\nnothing to commit, working tree clean\nIf you already checked out the commit 7f94f8e with git checkout 7f94f8e, you can create the new branch newbranch on that commit and switch to it with:\ngit switch -c newbranch\nSwitched to a new branch 'newbranch'\nThose are equivalent workflows. Just don‚Äôt forget never to work from a detached HEAD state. You can look around in that state, but that‚Äôs it. Why? Because commits that are not part of a branch get automatically deleted on a regular basis when Git runs its garbage collection. So any commits you make from a detached HEAD will eventually be lost. And that‚Äôs probably not what you want.\nIn short, git switch allows you to switch HEAD from one branch to another. With the -c flag, you can create a new branch before switching to it. And by adding some starting point such as a commit, the new branch gets created on that commit rather than on the position of HEAD.\nNow, have a look at what happens if you run git log from newbranch:\ngit log\ncommit 7f94f8ed631a7390a910fa13cd4954cf9e8a3061 (HEAD -> newbranch)\nAuthor: Marie-Helene Burle <xxx@xxx>\nDate:   Mon Oct 3 18:19:28 2022 -0700\n\n    Initial commit\nHorror! It looks like all our commits except for the first one are gone!\nIn fact, they still exist, but by default, git log only shows what is on the current branch. To see all the commits that are on any branch in your project, you need to add the --all flag:\ngit log --all\ncommit 863afd650ecaeab85da2f8ed0d3c88a778754727 (main)\nAuthor: Marie-Helene Burle <xxx@xxx>\nDate:   Tue Oct 4 10:32:39 2022 -0700\n\n    Add .gitignore file with data and results\n\ncommit dc780c75c76220a39f7c89a76bebb670dad25b8e\nAuthor: Marie-Helene Burle <xxx@xxx>\nDate:   Tue Oct 4 10:32:12 2022 -0700\n\n    Define the variable a in R script\n\ncommit 5ba96b254b505f7d04f59f988a621a746a0c6896\nAuthor: Marie-Helene Burle <xxx@xxx>\nDate:   Tue Oct 4 10:28:51 2022 -0700\n\n    Add conclusion to the manuscript\n\ncommit 451c47b386895b8b0b5bdd1a8734ef1d51f9ccc9\nAuthor: Marie-Helene Burle <xxx@xxx>\nDate:   Mon Oct 3 18:35:51 2022 -0700\n\n    Add result section to manuscript\n\ncommit 7f94f8ed631a7390a910fa13cd4954cf9e8a3061 (HEAD -> newbranch)\nAuthor: Marie-Helene Burle <xxx@xxx>\nDate:   Mon Oct 3 18:19:28 2022 -0700\n\n    Initial commit\n\nIn this log, we can now see main, but that HEAD points to newbranch.\n\n\nListing branches\ngit branch\n  main\n* newbranch\nThe * shows the branch you are currently on (i.e.¬†the branch to which HEAD points to).\n\n\nComparing branches\nYou can use git diff to compare branches:\ngit diff newbranch main\ndiff --git a/.gitignore b/.gitignore\nnew file mode 100644\nindex 0000000..e85f44a\n--- /dev/null\n+++ b/.gitignore\n@@ -0,0 +1,2 @@\n+/data/\n+/results/\ndiff --git a/ms/chapter3.md b/ms/chapter3.md\nindex b88424b..80d2c5c 100644\n--- a/ms/chapter3.md\n+++ b/ms/chapter3.md\n@@ -7,3 +7,11 @@ Bla bla bla bla bla.\n ## Methods\n\n Bla bla bla.\n+\n+## Results\n+\n+We now have a bunch of results in our markdown manuscript.\n+\n+## Conclusion\n+\n+And finally, the great conclusion of our paper.\ndiff --git a/src/chapter3.R b/src/chapter3.R\nindex 95f1592..2bf030d 100644\n--- a/src/chapter3.R\n+++ b/src/chapter3.R\n@@ -7,3 +7,5 @@ df <- data.frame(\n )\n\n ggplot(df, aes(x, y)) + geom_point()\n+\n+a = 23\nThis shows all the lines that have been modified (added or deleted) between the commits both branches point to.\n\n\nMerging branches\nIf you want to merge branches, switch to the branch you want to merge into the other one, then run git merge.\nFor instance, if we want to merge newbranch onto main, we would first switch to newbranch (we are already on it, so nothing to do here), then:\ngit merge main\nUpdating 7f94f8e..863afd6\nFast-forward\n .gitignore     | 2 ++\n ms/chapter3.md | 8 ++++++++\n src/chapter3.R | 2 ++\n 3 files changed, 12 insertions(+)\n create mode 100644 .gitignore\nThis merge is called a ‚Äúfast-forward‚Äù merge because main and newbranch had not diverged. It was simply a question of having newbranch catch up to main.\nIf you run git log again, you will see that newbrach has now caught up with main:\ngit log\ncommit 863afd650ecaeab85da2f8ed0d3c88a778754727 (HEAD -> newbranch, main)\nAuthor: Marie-Helene Burle <xxx@xxx>\nDate:   Tue Oct 4 10:32:39 2022 -0700\n\n    Add .gitignore file with data and results\n\ncommit dc780c75c76220a39f7c89a76bebb670dad25b8e\nAuthor: Marie-Helene Burle <xxx@xxx>\nDate:   Tue Oct 4 10:32:12 2022 -0700\n\n    Define the variable a in R script\n\ncommit 5ba96b254b505f7d04f59f988a621a746a0c6896\nAuthor: Marie-Helene Burle <xxx@xxx>\nDate:   Tue Oct 4 10:28:51 2022 -0700\n\n    Add conclusion to the manuscript\n\ncommit 451c47b386895b8b0b5bdd1a8734ef1d51f9ccc9\nAuthor: Marie-Helene Burle <xxx@xxx>\nDate:   Mon Oct 3 18:35:51 2022 -0700\n\n    Add result section to manuscript\n\ncommit 7f94f8ed631a7390a910fa13cd4954cf9e8a3061\nAuthor: Marie-Helene Burle <xxx@xxx>\nDate:   Mon Oct 3 18:19:28 2022 -0700\n\n    Initial commit\n\nHere is a classic situation of fast-forward merge.\nInstead of working on your branch main, you create a test branch and work on it (so HEAD is on the branch test and both move along as you create commits):\n\nWhen you are happy with the changes you made on your test branch, you decide to merge main onto it.\nFirst, you switch to main:\n\nThen you do the fast-forward merge from main onto test (so main catches up to test):\n\nThen, usually, you delete the branch test as it has served its purpose (with git branch -d test). Alternatively, you can switch back to it and do the next bit of experimental work in it. This allows to keep main free of possible mishaps and bad developments (if you aren‚Äôt happy with the work you did on your test branch, you can simply delete it and Git will clean the commits that are on it but not on main during the next garbage collection.\n\n\nIf both branches have diverged (you created commits from both main and newbranch), the merge would require the creation of an additional commit called a ‚Äúmerge commit‚Äù.\n\nHere is a classic situation of merge with a commit.\n\nYou create a test branch and switch to it:\n\n\nThen you create some commits:\n\n\nNow you switch back to main:\n\nAnd you create commits from main too:\n\n\nTo merge your main branch and your test branch, a new commit is now required (note that the command is the same as in the case of a fast-forward merge: git merge. Git will create the new commit automatically. As long as there is no conflict, it is just as easy as a fast-forward merge. We will talk later about resolving conflicts).\n\nAfter which, you can delete the (now useless) test branch (with git branch -d test2):"
  },
  {
    "objectID": "git/intro.html#what-are-remotes",
    "href": "git/intro.html#what-are-remotes",
    "title": "Version control with Git & GitHub",
    "section": "What are remotes?",
    "text": "What are remotes?\nRemotes are copies of a project and its history.\nThey can be located anywhere, including on external drive or on the same machine as the project, although they are often on a different machine to serve as backup, or on a network (e.g.¬†internet) to serve as a syncing hub for collaborations.\nPopular online Git repository managers & hosting services:\n\nGitHub\nGitLab\nBitbucket"
  },
  {
    "objectID": "git/intro.html#creating-a-remote-on-github",
    "href": "git/intro.html#creating-a-remote-on-github",
    "title": "Version control with Git & GitHub",
    "section": "Creating a remote on GitHub",
    "text": "Creating a remote on GitHub\n\nCreate a free GitHub account\nSign up for a free GitHub account.\nLater on, to avoid having to type your password all the time, you should set up SSH for your account.\n\n\nCreate an empty repository on GitHub\n\nGo to the GitHub website, login, and go to your home page.\nLook for the Repositories tab & click the green New button.\nEnter the name you want for your repo, without spaces.\nMake the repository public or private.\n\n\n\nLink empty repository to your repo\nClick on the Code green drop-down button, select SSH if you have set SSH for your GitHub account or HTTPS and copy the address.\nIn the command line, cd inside your project, and add the remote:\ngit remote add <remote-name> <remote-address>\nremote-name is a convenience name to identify that remote. You can choose any name, but since Git automatically call the remote origin when you clone a repo, it is common practice to use origin as the name for the first remote.\n\nExample (using an SSH address):\n\ngit remote add origin git@github.com:<user>/<repo>.git\n\nExample (using an HTTPS address):\n\ngit remote add origin https://github.com/<user>/<repo>.git"
  },
  {
    "objectID": "git/intro.html#getting-information-on-remotes",
    "href": "git/intro.html#getting-information-on-remotes",
    "title": "Version control with Git & GitHub",
    "section": "Getting information on remotes",
    "text": "Getting information on remotes\nList remotes:\ngit remote\nList remotes with their addresses:\ngit remote -v\nGet more information on a remote:\ngit remote show <remote-name>\n\nExample:\n\ngit remote show origin"
  },
  {
    "objectID": "git/intro.html#managing-remotes",
    "href": "git/intro.html#managing-remotes",
    "title": "Version control with Git & GitHub",
    "section": "Managing remotes",
    "text": "Managing remotes\nRename a remote:\ngit remote rename <old-remote-name> <new-remote-name>\nDelete a remote:\ngit remote remove <remote-name>\nChange the address of a remote:\ngit remote set-url <remote-name> <new-url> [<old-url>]"
  },
  {
    "objectID": "git/intro.html#getting-data-from-a-remote",
    "href": "git/intro.html#getting-data-from-a-remote",
    "title": "Version control with Git & GitHub",
    "section": "Getting data from a remote",
    "text": "Getting data from a remote\nIf you collaborate on a project, you have to get the data added by your teammates to keep your local project up to date.\nTo download new data from a remote, you have 2 options:\n\ngit fetch\ngit pull\n\n\nFetching changes\nFetching downloads the data from a remote that you don‚Äôt already have in your local version of the project:\ngit fetch <remote-name>\nThe branches on the remote are now accessible locally as <remote-name>/<branch>. You can inspect them or you can merge them into your local branches.\n\nExample:\n\ngit fetch origin\n\n\nPulling changes\nPulling fetches the changes & merges them onto your local branches:\ngit pull <remote-name> <branch>\n\nExample:\n\ngit pull origin main\nIf your branch is already tracking a remote branch, you can omit the arguments:\ngit pull"
  },
  {
    "objectID": "git/intro.html#pushing-to-a-remote",
    "href": "git/intro.html#pushing-to-a-remote",
    "title": "Version control with Git & GitHub",
    "section": "Pushing to a remote",
    "text": "Pushing to a remote\nUploading data to the remote is called pushing:\ngit push <remote-name> <branch-name>\n\nExample:\n\ngit push origin main\nYou can set an upstream branch to track a local branch with the -u flag:\ngit push -u <remote-name> <branch-name>\n\nExample:\n\ngit push -u origin main\nFrom now on, all you have to run when you are on main is:\ngit push\n \n\nby jscript"
  },
  {
    "objectID": "git/intro.html#setup",
    "href": "git/intro.html#setup",
    "title": "Version control with Git & GitHub",
    "section": "Setup",
    "text": "Setup\nWhen you collaborate with others using Git and GitHub, there are three possible situations:\n\nYou create a project on your machine and want others to contribute to it (1).\nYou want to contribute to a project started by others & ‚Ä¶\n‚ÄÉ‚ÄÉ‚Ä¶ you have write access to it (2).\n‚ÄÉ‚ÄÉ‚Ä¶ you do not have write access to it (3).\n\n\n(1) You start the project\n\nCreate a remote on GitHub\nThese are the steps we already saw earlier.\n\n1. Create an empty repository on GitHub\n\nGo to the GitHub website, login, and go to your home page.\nLook for the Repositories tab & click the green New button.\nEnter the name you want for your repo, without spaces.\nMake the repository public or private.\n\n\n\n2. Link empty repository to your repo\nClick on the Code green drop-down button, select SSH if you have set SSH for your GitHub account or HTTPS and copy the address.\nIn the command line, cd inside your project, and add the remote:\ngit remote add <remote-name> <remote-address>\nremote-name is a convenience name to identify that remote. You can choose any name, but since Git automatically call the remote origin when you clone a repo, it is common practice to use origin as the name for the first remote.\n\nExample (using an SSH address):\n\ngit remote add origin git@github.com:<user>/<repo>.git\n\nExample (using an HTTPS address):\n\ngit remote add origin https://github.com/<user>/<repo>.git\nIf you are working alone on this project and you only wanted to have a remote for backup, you are set.\nIf you don‚Äôt want to grant others write access to the project, and you only accept contributions through pull requests, you are also set.\nIf you want to grant your collaborators write access to the project however, you need to add them to it.\n\n\n\nInvite collaborators\n\nGo to your GitHub project page.\nClick on the Settings tab.\nClick on the Manage access section on the left-hand side (you will be prompted for your GitHub password).\nClick on the Invite a collaborator green button.\nInvite your collaborators with one of their GitHub user name, their email address, or their full name.\n\n\n\n\n(2) Write access to project\n\nClone project\ncd to location where you want your local copy, then:\ngit clone <remote-address> <local-name>\nThis sets the project as a remote to your new local copy and that remote is automatically called origin.\nWithout <local-name>, the repo will have the name of the last part of the remote address.\n\n\n\n(3) No write access to project\n\nCollaborate without write access\nIn that case, you will have to submit a pull request:\n\nFork the project on GitHub.\nClone your fork on your machine.\nAdd the initial project as a second remote & call it upstream.\nPull from upstream to update your local project.\nCreate & checkout a new branch.\nMake & commit your changes on that branch.\nPush that branch to your fork (i.e.¬†origin ‚Äî remember that you do not have write access to upstream).\nGo to the original project GitHub‚Äôs page & open a pull request."
  },
  {
    "objectID": "git/intro.html#workflow",
    "href": "git/intro.html#workflow",
    "title": "Version control with Git & GitHub",
    "section": "Workflow",
    "text": "Workflow\nWhen you collaborate with others using GitHub (or other equivalent service), you and others will work simultaneously on some project. How does this work?\nRemember that to upload your changes to the remote on GitHub you push to it with git push.\nIf one of your collaborators has made changes to the remote (pushing from their own machine), you won‚Äôt be able to push. Instead, you will get the following message:\nTo xxx.git\n ! [rejected]        main -> main (fetch first)\nerror: failed to push some refs to 'xxx.git'\nhint: Updates were rejected because the remote contains work that you do\nhint: not have locally. This is usually caused by another repository pushing\nhint: to the same ref. You may want to first integrate the remote changes\nhint: (e.g., 'git pull ...') before pushing again.\nhint: See the 'Note about fast-forwards' in 'git push --help' for details.\nThe solution?\nYou first have to download (git pull) their work onto your machine, merge it with yours (which will happen automatically if there are no conflicts), before you can push your work to GitHub.\nNow‚Ä¶ what if there are conflicts?"
  },
  {
    "objectID": "git/intro.html#resolving-conflicts",
    "href": "git/intro.html#resolving-conflicts",
    "title": "Version control with Git & GitHub",
    "section": "Resolving conflicts",
    "text": "Resolving conflicts\nGit works line by line. As long as your collaborators and you aren‚Äôt working on the same line(s) of the same file(s) at the same time, there will not be any problem. If however you modified one or more of the same line(s) of the same file(s), Git will not be able to decide which version should be kept. When you git pull their work on your machine, the automatic merging will get interrupted and Git will ask you to resolve the conflict(s) before the merge can resume. It will conveniently tell you which file(s) contain the conflict(s).\nThere are fancy tools to resolve conflicts, but you can do it in any text editor: simply open the file(s) listed by Git as having conflicts and look for the following markers:\n<<<<<<< HEAD\nThis is your version.\n=======\nThis is the alternative version of the same section of the file.\n>>>>>>> alternative version\nThese markers are added by Git to signal the areas of conflict. It is up to you to choose between the two versions (or create a third one) and remove the conflict markers. After that, you can stage the file(s) which contained the conflicts to finish the merge (and then you can commit)."
  },
  {
    "objectID": "git/practice_repo/search.html",
    "href": "git/practice_repo/search.html",
    "title": "Searching the Git history",
    "section": "",
    "text": "What is the point of creating all these commits if you are unable to make use of them because you can‚Äôt find the information you need in them?\nIn this workshop, we will learn how to search your files at any of their versions and search your commits logs.\nBy the end of the workshop, you should be able to retrieve anything you need from your versioned history.\n\n\nPrerequisites:\nThis special Git topic is suitable for people who already use Git.\nYou don‚Äôt need to be an expert, but you will need to bring a project under version control with several commits and we expect that you are able to run basic Git commands in the command line.\n\n\n\n\n\ngit clone git@github.com:prosoitos/git_workshop_collab.git\ncd git_workshop_collab\n\npwd\n\n/home/marie/parvus/prog/arc_training/git/practice_repo\n\n\n\ngit status\n\nOn branch main\nYour branch is up to date with 'origin/main'.\n\nnothing to commit, working tree clean\n\n\n\ngit log --all --graph --oneline -n 30\n\n* e3cfb2e Update gitignore with Quarto files\n* 15fdec6 Update README.org\n* 15d4ee9 change values training\n* 06efa34 add lots of code\n* 1457143 remove stupid line\n* 711e1dc add real py content to test_manual.py\n* 90016aa adding new python file\n*   2c0f612 Merge branch 'main' of github.com:prosoitos/git_workshop_collab\n|\\  \n| *   6f7d03d Merge branch 'main' of https://github.com/prosoitos/git_workshop_collab into main\n| |\\  \n| * \\   3c53269 Merge branch 'main' of https://github.com/prosoitos/git_workshop_collab into main\n| |\\ \\  \n| * \\ \\   eef5b78 Merge branch 'main' of https://github.com/prosoitos/git_workshop_collab into main\n| |\\ \\ \\  \n| * | | | a55ca0d new comment add just as test\n* | | | |   dedc94f Merge branch 'main' of github.com:prosoitos/git_workshop_collab\n|\\ \\ \\ \\ \\  \n| | |_|_|/  \n| |/| | |   \n| * | | |   b861a65 Merge branch 'main' of https://github.com/prosoitos/git_workshop_collab\n| |\\ \\ \\ \\  \n| | | |_|/  \n| | |/| |   \n| | * | |   35e8d5a Merge branch 'main' of github.com:prosoitos/git_workshop_collab\n| | |\\ \\ \\  \n| | | | |/  \n| | | |/|   \n| | * | | 9750bf1 New file\n| | | |/  \n| | |/|   \n| * | | 334aeaf 2022-Feb-17 14:24\n* | | | 0899037 Add 2 variables in Julia script\n| |_|/  \n|/| |   \n* | | 39542fe Create 2 new scripts\n| |/  \n|/|   \n* | 9ffabcd Comment line\n|/  \n| * ab22e00 added text to names.txt\n|/  \n*   5330352 Merge branch 'master' of github.com:prosoitos/git_workshop_collab\n|\\  \n| * ea74e46 Add test_mk.txt\n* | 1919b96 Add sep23.txt file\n|/  \n* f9f6999 Commit from @rcassani\n* 0e88506 modified hc.tct\n*   ad2e3b4 Merge branch 'master' of https://github.com/prosoitos/git_workshop_collab\n|\\  \n| * 60beaa9 Adding a text file\n| *   126ee6c Merge branch 'master' of https://github.com/prosoitos/git_workshop_collab\n| |\\  \n| * | 7e1135e 2021-Aug-04 15:10"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "WestDRI",
    "section": "",
    "text": "Git\nVersion control with Git and collaboration with GitHub/GitLab\n\n\n\n\nR\nResearch computing in R\n\n\n\n\nJulia\nResearch computing in the Julia programming language\n\n\n\n\nPython\nResearch computing in Python\n\n\n\n\n\n\nMachine learning\nDeep learning with the PyTorch framework\n\n\n\n\nBash\nBash/Zsh scripting, Unix commands, and useful CLI utilities\n\n\n\n\nResearch tools\nOpen source tools for computing and publishing\n\n\n\n¬†\n\n\n\n\n\nMain WestDRI website:\nThis site contains content by Marie-H√©l√®ne Burle. To view all training material, please visit WestDRI‚Äôs main website."
  },
  {
    "objectID": "julia/firstdab.html",
    "href": "julia/firstdab.html",
    "title": "First dab at Julia",
    "section": "",
    "text": "Julia is fast: just-in-time (JIT) compilation and multiple dispatch bring efficiency to interactivity. People often say that using Julia feels like running R or python with a speed almost comparable to that of C.\nBut Julia also comes with parallel computing and multi-threading capabilities.\nIn this webinar, after a quickly presentation of some of the key features of Julia‚Äôs beautifully concise syntax, I will dive into using Julia for HPC."
  },
  {
    "objectID": "julia/flux.html",
    "href": "julia/flux.html",
    "title": "Machine learning in Julia with Flux",
    "section": "",
    "text": "This webinar, aimed at users with no experience in machine learning, is an introduction to the basic concepts of neural networks, followed by a simple example‚Äîthe classic classification of the MNIST database of handwritten digits‚Äîusing the Julia package Flux."
  },
  {
    "objectID": "julia/intro_hpc.html",
    "href": "julia/intro_hpc.html",
    "title": "Introduction to high performance research computing in Julia",
    "section": "",
    "text": "Why would I want to learn a new language? I already know R/python.\n\nR and python are interpreted languages: the code is executed directly, without prior-compilation. This is extremely convenient: it is what allows you to run code in an interactive shell. The price to pay is low performance: R and python are simply not good at handling large amounts of data. To overcome this limitation, users often turn to C or C++ for the most computation-intensive parts of their analyses. These are compiled‚Äîand extremely efficient‚Äîlanguages, but the need to use multiple languages and the non-interactive nature of compiled languages make this approach tedious.\nJulia uses just-in-time (JIT) compilation: the code is compiled at run time. This combines the interactive advantage of interpreted languages with the efficiency of compiled ones. Basically, it feels like running R or python, while it is almost as fast as C. This makes Julia particularly well suited for big data analyses, machine learning, or heavy modelling.\nIn addition, multiple dispatch (generic functions with multiple methods depending on the types of all the arguments) is at the very core of Julia. This is extremly convenient, cutting on conditionals and repetitions, and allowing for easy extensibility without having to rewrite code.\nFinally, Julia shines by its extremely clean and concise syntax. This last feature makes it easy to learn and really enjoyable to use.\nIn this workshop, which does not require any prior experience in Julia (experience in another language‚Äîe.g.¬†R or python‚Äîwould be best), we will go over the basics of Julia‚Äôs syntax and package system; then we will push the performance aspect further by looking at how Julia can make use of clusters for large scale parallel computing."
  },
  {
    "objectID": "julia/intro_hpc.html#background",
    "href": "julia/intro_hpc.html#background",
    "title": "Introduction to high performance research computing in Julia",
    "section": "Background",
    "text": "Background\n\nBrief history\nStarted in 2009 by Jeff Bezanson, Stefan Karpinski, Viral B. Shah, and Alan Edelman, the general-purpose programming language Julia was launched in 2012 as free and open source software. Version 1.0 was released in 2018.\nRust developer Graydon Hoare wrote an interesting post which places Julia in a historical context of programming languages.\n\n\nWhy another language?\n\nJIT\nComputer languages mostly fall into two categories: compiled languages and interpreted languages.\n\nCompiled languages\nCompiled languages require two steps:\n\nin a first step the code you write in a human-readable format (the source code, usually in plain text) gets compiled into machine code\nit is then this machine code that is used to process your data\n\nSo you write a script, compile it, then use it.\n\nBecause machine code is a lot easier to process by computers, compiled languages are fast. The two step process however makes prototyping new code less practical, these languages are hard to learn, and debugging compilation errors can be challenging.\n\nExamples of compiled languages include C, C++, Fortran, Go, and Haskell.\n\n\n\nInterpreted languages\nInterpreted languages are executed directly which has many advantages such as dynamic typing and direct feed-back from the code and they are easy to learn, but this comes at the cost of efficiency. The source code can facultatively be bytecompiled into non human-readable, more compact, lower level bytecode which is read by the interpreter more efficiently.\n\n\nExamples of interpreted languages include R, Python, Perl, and JavaScript.\n\n\n\nJIT compiled languages\nJulia uses just-in-time compilation or JIT based on LLVM: the source code is compiled at run time. This combines the flexibility of interpretation with the speed of compilation, bringing speed to an interactive language. It also allows for dynamic recompilation, continuous weighing of gains and costs of the compilation of parts of the code, and other on the fly optimizations.\nOf course, there are costs here too. They come in the form of overhead time to compile code the first time it is run and increased memory usage.\n\n\n\nMultiple dispatch\nIn languages with multiple dispatch, functions apply different methods at run time based on the type of the operands. This brings great type stability and improves speed.\nJulia is extremely flexible: type declaration is not required. Out of convenience, you can forego the feature if you want. Specifying types however will greatly optimize your code.\nHere is a good post on type stability, multiple dispatch, and Julia efficiency."
  },
  {
    "objectID": "julia/intro_hpc.html#resources",
    "href": "julia/intro_hpc.html#resources",
    "title": "Introduction to high performance research computing in Julia",
    "section": "Resources",
    "text": "Resources\n\nDocumentation\n\nOfficial Julia website\nOfficial Julia manual\nOnline training material\nThe Julia YouTube channel\nThe Julia Wikibook\nA blog aggregator for Julia\n\n\n\nGetting help\n\nDiscourse forum\n[julia] tag on Stack Overflow\nSlack team (you need to agree to the community code of conduct at slackinvite.julialang.org to receive an invitation)\n#julialang hashtag on Twitter\nSubreddit\nGitter channel\n#julia IRC channel on Freenode\n\n\n\nInterface\nCopying and pasting code from a script to the Julia REPL works, but there are nicer ways to integrate the two.\nHere are a few:\n\nVS Code extension\nJulia for Visual Studio Code has become the main Julia IDE.\n\n\nEmacs\n\nthrough the julia-emacs and julia-repl packages\nthrough the ESS package\nthrough the Emacs IPython Notebook package if you want to access Jupyter notebooks in Emacs\n\n\n\nVim\nThrough the julia-vim package.\n\n\nJupyter\nProject Jupyter allows to create interactive programming documents through its web-based JupyterLab environment and its Jupyter Notebook.\n\n\nPluto\nThe Julia package Juno is a reactive notebook for Julia.\n\n\nQuarto\nQuarto builds interactive documents with code and runs Julia through Jupyter."
  },
  {
    "objectID": "julia/intro_hpc.html#repl-keybindings",
    "href": "julia/intro_hpc.html#repl-keybindings",
    "title": "Introduction to high performance research computing in Julia",
    "section": "REPL keybindings",
    "text": "REPL keybindings\nIn the REPL, you can use standard command line keybindings:\nC-c     cancel\nC-d     quit\nC-l     clear console\n\nC-u     kill from the start of line\nC-k     kill until the end of line\n\nC-a     go to start of line\nC-e     go to end of line\n\nC-f     move forward one character\nC-b     move backward one character\n\nM-f     move forward one word\nM-b     move backward one word\n\nC-d     delete forward one character\nC-h     delete backward one character\n\nM-d     delete forward one word\nM-Backspace delete backward one word\n\nC-p     previous command\nC-n     next command\n\nC-r     backward search\nC-s     forward search\nIn addition, there are 4 REPL modes:\njulia> ‚ÄÉ‚ÄÉ The main mode in which you will be running your code.\nhelp?> ‚ÄÉ‚ÄÉ A mode to easily access documentation.\nshell> ‚ÄÉ‚ÄÉ A mode in which you can run bash commands from within Julia.\n(env) pkg> ¬† A mode to easily perform actions on packages with Julia package manager.\n(env is the name of your current project environment.\nProject environments are similar to Python‚Äôs virtual environments and allow you, for instance, to have different package versions for different projects. By default, it is the current Julia version. So what you will see is (v1.3) pkg>).\nEnter the various modes by typing ?, ;, and ]. Go back to the regular mode with the Backspace key."
  },
  {
    "objectID": "julia/intro_hpc.html#startup-options",
    "href": "julia/intro_hpc.html#startup-options",
    "title": "Introduction to high performance research computing in Julia",
    "section": "Startup options",
    "text": "Startup options\nYou can configure Julia by creating the file ~/.julia/config/startup.jl."
  },
  {
    "objectID": "julia/intro_hpc.html#packages",
    "href": "julia/intro_hpc.html#packages",
    "title": "Introduction to high performance research computing in Julia",
    "section": "Packages",
    "text": "Packages\n\nStandard library\nJulia comes with a collection of packages. In Linux, they are in /usr/share/julia/stdlib/vx.x.\nHere is the list:\nBase64\nCRC32c\nDates\nDelimitedFiles\nDistributed\nFileWatching\nFuture\nInteractiveUtils\nLibdl\nLibGit2\nLinearAlgebra\nLogging\nMarkdown\nMmap\nPkg\nPrintf\nProfile\nRandom\nREPL\nSerialization\nSHA\nSharedArrays\nSockets\nSparseArrays\nStatistics\nSuiteSparse\nTest\nUnicode\nUUIDs\n\n\nInstalling additional packages\nYou can install additional packages.\nThese go to your personal library in ~/.julia (this is also where your REPL history is saved).\nAll registered packages are on GitHub and can easily be searched here.\nThe GitHub star system allows you to easily judge the popularity of a package and to see whether it is under current development.\nIn addition to these, there are unregistered packages and you can build your own.\n\n\nYour turn:\n\nTry to find a list of popular plotting packages.\n\nYou can manage your personal library easily in package mode with the commands:\n(env) pkg> add <package>        # install <package>\n(env) pkg> rm <package>         # uninstall <package>\n(env) pkg> up <package>         # upgrade <package>\n\n(env) pkg> st                   # check which packages are installed\n(env) pkg> up                   # upgrade all packages\n\n\nYour turn:\n\nCheck your list of packages; install the packages Plots, GR, Distributions, StatsPlots, and UnicodePlot; then check that list again.\n\n\n\nYour turn:\n\nNow go explore your ~/.julia. If you don‚Äôt find it, make sure that your file explorer allows you to see hidden files.\n\n\n\nLoading packages\nWhether a package from the standard library or one you installed, before you can use a package you need to load it. This has to be done at each new Julia session so the code to load packages should be part of your scripts.\nThis is done with the using command (e.g.¬†using Plots)."
  },
  {
    "objectID": "julia/intro_hpc.html#finding-documentation",
    "href": "julia/intro_hpc.html#finding-documentation",
    "title": "Introduction to high performance research computing in Julia",
    "section": "Finding documentation",
    "text": "Finding documentation\nAs we already saw, you can type ? to enter the help mode.\nTo print the list of functions containing a certain word in their description, you can use apropos().\n\nExample:\n\n> apropos(\"truncate\")"
  },
  {
    "objectID": "julia/intro_hpc.html#lets-try-a-few-commands",
    "href": "julia/intro_hpc.html#lets-try-a-few-commands",
    "title": "Introduction to high performance research computing in Julia",
    "section": "Let‚Äôs try a few commands",
    "text": "Let‚Äôs try a few commands\n> versioninfo()\n> VERSION\n\n> x = 10\n> x\n> x = 2;\n> x\n> y = x;\n> y\n> ans\n> ans + 3\n\n> a, b, c = 1, 2, 3\n> b\n\n> 3 + 2\n> +(3, 2)\n\n> a = 3\n> 2a\n> a += 7\n> a\n\n> 2\\8\n\n> a = [1 2; 3 4]\n> b = a\n> a[1, 1] = 0\n> b\n\n> [1, 2, 3, 4]\n> [1 2; 3 4]\n> [1 2 3 4]\n> [1 2 3 4]'\n> collect(1:4)\n> collect(1:1:4)\n> 1:4\n> a = 1:4\n> collect(a)\n\n> [1, 2, 3] .* [1, 2, 3]\n\n> 4//8\n> 8//1\n> 1//2 + 3//4\n\n> a = true\n> b = false\n> a + b\n\n\nYour turn:\n\nWhat does ; at the end of a command do?\nWhat is surprising about 2a?\nWhat does += do?\nWhat does .+do?\n\n> a = [3, 1, 2]\n\n> sort(a)\n> println(a)\n\n> sort!(a)\n> println(a)\n\n\nYour turn:\n\nWhat does ! at the end of a function name do?"
  },
  {
    "objectID": "julia/intro_hpc.html#sourcing-a-file",
    "href": "julia/intro_hpc.html#sourcing-a-file",
    "title": "Introduction to high performance research computing in Julia",
    "section": "Sourcing a file",
    "text": "Sourcing a file\nTo source a Julia script within Julia, use the function include().\n\nExample:\n\n> include(\"/path/to/file.jl\")"
  },
  {
    "objectID": "julia/intro_hpc.html#comments",
    "href": "julia/intro_hpc.html#comments",
    "title": "Introduction to high performance research computing in Julia",
    "section": "Comments",
    "text": "Comments\n> # Single line comment\n\n> #=\n  Comments can\n  also contain\n  multiple lines\n  =#\n\n> x = 2;          # And they can be added at the end of lines"
  },
  {
    "objectID": "julia/intro_hpc.html#a-few-fun-quirks",
    "href": "julia/intro_hpc.html#a-few-fun-quirks",
    "title": "Introduction to high performance research computing in Julia",
    "section": "A few fun quirks",
    "text": "A few fun quirks\n> \\omega          # Press TAB\n> \\sum            # Press TAB\n> \\sqrt           # Press TAB\n> \\in             # Press TAB\n> \\: phone:       # (No space after colon. I added it to prevent parsing) Press TAB\n\n> pi\n> Base.MathConstants.golden"
  },
  {
    "objectID": "julia/intro_hpc.html#data-types",
    "href": "julia/intro_hpc.html#data-types",
    "title": "Introduction to high performance research computing in Julia",
    "section": "Data types",
    "text": "Data types\n> typeof(2)\n> typeof(2.0)\n> typeof(\"hello\")\n> typeof(true)"
  },
  {
    "objectID": "julia/intro_hpc.html#indexing",
    "href": "julia/intro_hpc.html#indexing",
    "title": "Introduction to high performance research computing in Julia",
    "section": "Indexing",
    "text": "Indexing\nIndexing is done with square brackets. As in R and unlike in C++ or Python, Julia starts indexing at 1, not at 0.\n> a = [1 2; 3 4]\n> a[1, 1]\n> a[1, :]\n\n\nYour turn:\n\nHow can I get the second column?\nHow can I get the tuple (2, 4)? (a tuple is a list of elements)"
  },
  {
    "objectID": "julia/intro_hpc.html#for-loops",
    "href": "julia/intro_hpc.html#for-loops",
    "title": "Introduction to high performance research computing in Julia",
    "section": "For loops",
    "text": "For loops\n> for i in 1:10\n      println(i)\n  end\n\n\n> for i in 1:3, j in 1:2\n      println(i * j)\n  end"
  },
  {
    "objectID": "julia/intro_hpc.html#predicates-and-conditionals",
    "href": "julia/intro_hpc.html#predicates-and-conditionals",
    "title": "Introduction to high performance research computing in Julia",
    "section": "Predicates and conditionals",
    "text": "Predicates and conditionals\n> a = 2\n> b = 2.0\n\n> if a == b\n      println(\"It's true\")\n  else\n      println(\"It's false\")\n  end\n\n# This can be written in a terse format\n# predicate ? if true : if false\n> a == b ? println(\"It's true\") : println(\"It's false\")\n\n> if a === b\n      println(\"It's true\")\n  else\n      println(\"It's false\")\n  end\n\n\nYour turn:\n\nWhat is the difference between == and ===?\n\nPredicates can be built with many other operators and functions.\n\nExample:\n\n> occursin(\"that\", \"this and that\")\n> 4 < 3\n> a != b\n> 2 in 1:3\n> 3 <= 4 && 4 > 5\n> 3 <= 4 || 4 > 5"
  },
  {
    "objectID": "julia/intro_hpc.html#functions",
    "href": "julia/intro_hpc.html#functions",
    "title": "Introduction to high performance research computing in Julia",
    "section": "Functions",
    "text": "Functions\n> function addTwo(a)\n      a + 2\n  end\n\n> addTwo(3)\n\n# This can be written in a terse format\n> addtwo = a -> a + 2\n\n# With default arguments\n> function addSomethingOrTwo(a, b = 2)\n      a + b\n  end\n\n> addSomethingOrTwo(3)\n> addSomethingOrTwo(3, 4)"
  },
  {
    "objectID": "julia/intro_hpc.html#plotting",
    "href": "julia/intro_hpc.html#plotting",
    "title": "Introduction to high performance research computing in Julia",
    "section": "Plotting",
    "text": "Plotting\nIt can be convenient to plot directly in the REPL (for instance when using SSH).\n> using UnicodePlots\n> histogram(randn(1000), nbins=40)\nMost of the time however, you will want to make nicer looking graphs. There are many options to plot in Julia.\n\nExample:\n\n# Will take a while when run for the first time as the packages need to compile\n> using Plots, Distributions, StatsPlots\n\n# Using the GR framework as backend\n> gr()\n\n> x = 1:10; y = rand(10, 2);\n> p1 = histogram(randn(1000), nbins=40)\n> p2 = plot(Normal(0, 1))\n> p3 = scatter(x, y)\n> p4 = plot(x, y)\n> plot(p1, p2, p3, p4)"
  },
  {
    "objectID": "julia/intro_hpc.html#multi-threading",
    "href": "julia/intro_hpc.html#multi-threading",
    "title": "Introduction to high performance research computing in Julia",
    "section": "Multi-threading",
    "text": "Multi-threading\nJulia, which was built with efficiency in mind, aimed from the start to have parallel programming abilities. These however came gradually: first, there were coroutines, which is not parallel programming, but allows independent executions of elements of code; then there was a macro allowing for loops to run on several cores, but this would not work on nested loops and it did not integrate with the coroutines or I/O. It is only in the current (1.3) version, released a few months ago, that true multi-threading capabilities were born. Now is thus a very exciting time for Julia. This is all very new (this feature is still considered in testing mode) and it is likely that things will get even better in the coming months/years, for instance with the development of multi-threading capabilities for the compiler.\nWhat is great about Julia‚Äôs new task parallelism is that it is incredibly easy to use: no need to write low-level code as with MPI to set where tasks are run. Everything is automatic.\nTo use Julia with multiple threads, we need to set the JULIA_NUM_THREADS environment variable.\nThis can be done by running (in the terminal, not in Julia):\n$ export JULIA_NUM_THREADS=n      # n is the number of threads we want to use\nOr by launching Julia with (again, in the terminal):\n$ JULIA_NUM_THREADS=n julia\nFirst, we need to know how many threads we actually have on our machine.\nThere are many Linux tools for this, but here are two particularly convenient options:\n# To get the total number of available processes\n$ nproc\n\n# To have more information (# of sockets, cores per socket, and threads per core)\n$ lscpu | grep -E '(S|s)ocket|Thread|^CPU\\(s\\)'\nSince I have 4 available processes (2 cores with 2 threads each), I can launch Julia on 4 threads:\n$ JULIA_NUM_THREADS=4 julia\nThis can also be done from within the Juno IDE.\nTo see how many threads we are using, as well as the ID of the current thread, you can run:\n> Threads.nthreads()\n> Threads.threadid()"
  },
  {
    "objectID": "julia/intro_hpc.html#for-loops-on-multiple-threads",
    "href": "julia/intro_hpc.html#for-loops-on-multiple-threads",
    "title": "Introduction to high performance research computing in Julia",
    "section": "For loops on multiple threads",
    "text": "For loops on multiple threads\n\n\nYour turn:\n\nLaunch Julia on 1 thread and run the function below. Then run Julia on the maximum number of threads you have on your machine and run the same function.\n\n> Threads.@threads for i = 1:10\n      println(\"i = $i on thread $(Threads.threadid())\")\n  end\nUtilities such as htop allow you to visualize the working threads."
  },
  {
    "objectID": "julia/intro_hpc.html#generalization-of-multi-threading",
    "href": "julia/intro_hpc.html#generalization-of-multi-threading",
    "title": "Introduction to high performance research computing in Julia",
    "section": "Generalization of multi-threading",
    "text": "Generalization of multi-threading\nLet‚Äôs consider the example presented in a Julia blog post in July 2019.\nBoth scripts sort a one dimensional array of 20,000,000 floats between 0 and 1, one with parallelism and one without.\nScript 1, without parallelism: sort.jl.\n# Create one dimensional array of 20,000,000 floats between 0 and 1\n> a = rand(20000000);\n\n# Use the MergeSort algorithm of the sort function\n# (in the standard Julia Base library)\n> b = copy(a); @time sort!(b, alg = MergeSort);\n\n# Let's run the function a second time to remove the effect\n# of the initial compilation\n> b = copy(a); @time sort!(b, alg = MergeSort);\nScript 2, with parallelism: psort.jl.\n> import Base.Threads.@spawn\n\n# The psort function is the same as the MergeSort algorithm\n# of the Base sort function with the addition of\n# the @spawn macro on one of the recursive calls\n\n# Sort the elements of `v` in place, from indices `lo` to `hi` inclusive\n> function psort!(v, lo::Int=1, hi::Int = length(v))\n      if lo >= hi                       # 1 or 0 elements: nothing to do\n          return v\n      end\n\n      if hi - lo < 100000               # Below some cutoff, run in serial\n          sort!(view(v, lo:hi), alg = MergeSort)\n          return v\n      end\n\n      mid = (lo + hi) >>> 1             # Find the midpoint\n\n      half = @spawn psort!(v, lo, mid)  # Task to sort the lower half: will run\n      psort!(v, mid + 1, hi)            # in parallel with the current call sorting\n      # the upper half\n      wait(half)                        # Wait for the lower half to finish\n\n      temp = v[lo:mid]                  # Workspace for merging\n\n      i, k, j = 1, lo, mid + 1          # Merge the two sorted sub-arrays\n      @inbounds while k < j <= hi\n          if v[j] < temp[i]\n              v[k] = v[j]\n              j += 1\n          else\n              v[k] = temp[i]\n              i += 1\n          end\n          k += 1\n      end\n      @inbounds while k < j\n          v[k] = temp[i]\n          k += 1\n          i += 1\n      end\n\n      return v\n  end\n\n> a = rand(20000000);\n\n# Now, let's use our function\n> b = copy(a); @time psort!(b);\n\n# And running it a second time to remove\n# the effect of the initial compilation\n> b = copy(a); @time psort!(b);\nNow, we can test both scripts with one or multiple threads:\n# Single thread, non-parallel script\n$ julia /path/to/sort.jl\n\n    2.234024 seconds (111.88 k allocations: 82.489 MiB, 0.21% gc time)\n    2.158333 seconds (11 allocations: 76.294 MiB, 0.51% gc time)\n    # Note the lower time for the 2nd run due to pre-compilation\n\n# Single thread, parallel script\n$ julia /path/to/psort.jl\n\n    2.748138 seconds (336.77 k allocations: 703.200 MiB, 2.24% gc time)\n    2.438032 seconds (3.58 k allocations: 686.932 MiB, 0.27% gc time)\n    # Even longer time: normal, there was more to run (import package, read function)\n\n# 2 threads, non-parallel script\n$ JULIA_NUM_THREADS=2 julia /path/to/sort.jl\n\n    2.233720 seconds (111.87 k allocations: 82.145 MiB, 0.21% gc time)\n    2.155232 seconds (11 allocations: 76.294 MiB, 0.54% gc time)\n    # Remarkably similar to the single thread:\n    # the addition of a thread did not change anything\n\n# 2 threads, parallel script\n$ JULIA_NUM_THREADS=2 julia /path/to/psort.jl\n\n    1.773643 seconds (336.99 k allocations: 703.171 MiB, 4.08% gc time)\n    1.460539 seconds (3.79 k allocations: 686.935 MiB, 0.47% gc time)\n    # 33% faster. Not twice as fast as one could have hoped since processes\n    # have to wait for each other. But that's a good improvement.\n\n# 4 threads, non-parallel script\n$ JULIA_NUM_THREADS=4 julia /path/to/sort.jl\n\n    2.231717 seconds (111.87 k allocations: 82.145 MiB, 0.21% gc time)\n    2.153509 seconds (11 allocations: 76.294 MiB, 0.53% gc time)\n    # Again: same result as the single thread\n\n# 4 threads, parallel script\n$ JULIA_NUM_THREADS=4 julia /path/to/psort.jl\n\n    1.291714 seconds (336.98 k allocations: 703.171 MiB, 3.48% gc time)\n    1.194282 seconds (3.78 k allocations: 686.935 MiB, 5.19% gc time)\n    # Even though we only split our code in 2 tasks,\n    # there is still an improvement over the 2 thread run"
  },
  {
    "objectID": "julia/intro_hpc.html#distributed-computing",
    "href": "julia/intro_hpc.html#distributed-computing",
    "title": "Introduction to high performance research computing in Julia",
    "section": "Distributed computing",
    "text": "Distributed computing"
  },
  {
    "objectID": "julia/intro_hpc.html#logging-in-to-the-cluster",
    "href": "julia/intro_hpc.html#logging-in-to-the-cluster",
    "title": "Introduction to high performance research computing in Julia",
    "section": "Logging in to the cluster",
    "text": "Logging in to the cluster\nOpen a terminal emulator.\n/Windows users, launch MobaXTerm./\n/MacOS users, launch Terminal./\n/Linux users, launch xterm or the terminal emulator of your choice./\n$ ssh userxxx@cassiopeia.c3.ca\n\n# enter password\nYou are now in our training cluster."
  },
  {
    "objectID": "julia/intro_hpc.html#accessing-julia",
    "href": "julia/intro_hpc.html#accessing-julia",
    "title": "Introduction to high performance research computing in Julia",
    "section": "Accessing Julia",
    "text": "Accessing Julia\nThis is done with the Lmod tool through the module command. You can find the full documentation here and below are the subcommands you will need:\n# get help on the module command\n$ module help\n$ module --help\n$ module -h\n\n# list modules that are already loaded\n$ module list\n\n# see which modules are available for Julia\n$ module spider julia\n\n# see how to load julia 1.3\n$ module spider julia/1.3.0\n\n# load julia 1.3 with the required gcc module first\n# (the order is important)\n$ module load gcc/7.3.0 julia/1.3.0\n\n# you can see that we now have Julia loaded\n$ module list"
  },
  {
    "objectID": "julia/intro_hpc.html#copying-files-to-the-cluster",
    "href": "julia/intro_hpc.html#copying-files-to-the-cluster",
    "title": "Introduction to high performance research computing in Julia",
    "section": "Copying files to the cluster",
    "text": "Copying files to the cluster\nWe will create a julia_workshop directory in ~/scratch, then copy our julia script in it.\n$ mkdir ~/scratch/julia_job\nOpen a new terminal window and from your local terminal (make sure that you are not on the remote terminal by looking at the bash prompt) run:\n$ scp /local/path/to/sort.jl userxxx@cassiopeia.c3.ca:scratch/julia_job\n$ scp /local/path/to/psort.jl userxxx@cassiopeia.c3.ca:scratch/julia_job\n\n# enter password"
  },
  {
    "objectID": "julia/intro_hpc.html#job-scripts",
    "href": "julia/intro_hpc.html#job-scripts",
    "title": "Introduction to high performance research computing in Julia",
    "section": "Job scripts",
    "text": "Job scripts\nWe will not run an interactive session with Julia on the cluster: we already have julia scripts ready to run. All we need to do is to write job scripts to submit to Slurm, the job scheduler used by the Alliance clusters.\nWe will create 2 scripts: one to run Julia on one core and one on as many cores as are available.\n\n\nYour turn:\n\nHow many processors are there on our training cluster?\n\nNote that here too, we could run Julia with multiple threads by running:\n$ JULIA_NUM_THREADS=2 julia\nOnce in Julia, you can double check that Julia does indeed have access to 2 threads by running:\n> Threads.nthreads()\nSave your job scripts in the files ~/scratch/julia_job/job_julia1c.sh and job_julia2c.sh for one and two cores respectively.\nHere is what our single core Slurm script looks like:\n#!/bin/bash\n#SBATCH --job-name=julia1c          # job name\n#SBATCH --time=00:01:00             # max walltime 1 min\n#SBATCH --cpus-per-task=1               # number of cores\n#SBATCH --mem=1000                  # max memory (default unit is megabytes)\n#SBATCH --output=julia1c%j.out      # file name for the output\n#SBATCH --error=julia1c%j.err       # file name for errors\n# %j gets replaced with the job number\n\necho Running NON parallel script on $SLURM_CPUS_PER_TASK core\nJULIA_NUM_THREADS=$SLURM_CPUS_PER_TASK julia sort.jl\necho Running parallel script on $SLURM_CPUS_PER_TASK core\nJULIA_NUM_THREADS=$SLURM_CPUS_PER_TASK julia psort.jl\n\n\nYour turn:\n\nWrite the script for 2 cores.\n\nNow, we can submit our jobs to the cluster:\n$ cd ~/scratch/julia_job\n$ sbatch job_julia1c.sh\n$ sbatch job_julia2c.sh\nAnd we can check their status with:\n$ sq\nPD stands for pending and R for running."
  },
  {
    "objectID": "julia/makie.html",
    "href": "julia/makie.html",
    "title": "Makie",
    "section": "",
    "text": "There are several popular data visualization libraries for the Julia programming language (e.g.¬†Plots, Gadfly, VegaLite, Makie). They vary in their precompilation time, time to first plot, layout capabilities, ability to handle 3D data, ease of use, and syntax style. In this landscape, Makie focuses on high performance, fancy layouts, and extensibility.\nMakie comes with multiple backends. In this workshop, we will cover:\n\nGLMakie (ideal for interactive 2D and 3D plotting)\nWGLMakie (an equivalent that runs within browsers)\nCairoMakie (best for high-quality vector graphics)\n\nWe will also see how to run Makie in the Alliance clusters.\n\nSlides (click and wait: my reveal.js presentations are heavy and take some time to load‚Ä¶)"
  },
  {
    "objectID": "julia/makie_slides.html#plotting-in-julia",
    "href": "julia/makie_slides.html#plotting-in-julia",
    "title": "Makie",
    "section": "Plotting in Julia",
    "text": "Plotting in Julia\n\nMany options:\n\nPlots.jl: high-level API for working with different back-ends (GR, Pyplot, Plotly‚Ä¶)\nPyPlot.jl: Julia interface to Matplotlib‚Äôs matplotlib.pyplot\nPlotlyJS.jl: Julia interface to plotly.js\nPlotlyLight.jl: the fastest plotting option in Julia by far, but limited features\nGadfly.jl: following the grammar of graphics popularized by Hadley Wickham in R\nVegaLite.jl: grammar of interactive graphics\nPGFPlotsX.jl: Julia interface to the PGFPlots LaTeX package\nUnicodePlots.jl: plots in the terminal üôÇ\n\n\n\n\nMakie.jl: powerful plotting ecosystem: animation, 3D, GPU optimization"
  },
  {
    "objectID": "julia/makie_slides.html#makie-ecosystem",
    "href": "julia/makie_slides.html#makie-ecosystem",
    "title": "Makie",
    "section": "Makie ecosystem",
    "text": "Makie ecosystem\n\n\nMain package:\n\nMakie: plots functionalities. Backend needed to render plots into images or vector graphics\n\n\n\n\n\nBackends:\n\nCairoMakie: vector graphics or high-quality 2D plots. Creates, but does not display plots (you need an IDE that does or you can use ElectronDisplay.jl)\nGLMakie: based on OpenGL; 3D rendering and interactivity in GLFW window (no vector graphics)\nWGLMakie: web version of GLMakie (plots rendered in a browser instead of a window)"
  },
  {
    "objectID": "julia/makie_slides.html#extensions",
    "href": "julia/makie_slides.html#extensions",
    "title": "Makie",
    "section": "Extensions",
    "text": "Extensions\n\nGeoMakie.jl add geographical plotting utilities to Makie\nAlgebraOfGraphics.jl turns plotting into a simple algebra of building blocks\nGraphMakie.jl to create network graphs"
  },
  {
    "objectID": "julia/makie_slides.html#cheatsheet-2d",
    "href": "julia/makie_slides.html#cheatsheet-2d",
    "title": "Makie",
    "section": "Cheatsheet 2D",
    "text": "Cheatsheet 2D\n\n\nFrom: Storopoli, Huijzer and Alonso (2021). Julia Data Science. https://juliadatascience.io. ISBN: 97984898"
  },
  {
    "objectID": "julia/makie_slides.html#cheatsheet-3d",
    "href": "julia/makie_slides.html#cheatsheet-3d",
    "title": "Makie",
    "section": "Cheatsheet 3D",
    "text": "Cheatsheet 3D\n\n\nFrom: Storopoli, Huijzer and Alonso (2021). Julia Data Science. https://juliadatascience.io. ISBN: 97984898"
  },
  {
    "objectID": "julia/makie_slides.html#resources",
    "href": "julia/makie_slides.html#resources",
    "title": "Makie",
    "section": "Resources",
    "text": "Resources\n\nOfficial documentation\nJulia Data Science book, chapter 5\nMany examples in the project Beautiful Makie"
  },
  {
    "objectID": "julia/makie_slides.html#troubleshooting",
    "href": "julia/makie_slides.html#troubleshooting",
    "title": "Makie",
    "section": "Troubleshooting",
    "text": "Troubleshooting\nInstalling GLMakie can be challenging. This page may lead you towards solutions\nCairoMakie and WGLMakie should install without issues"
  },
  {
    "objectID": "julia/makie_slides.html#figure",
    "href": "julia/makie_slides.html#figure",
    "title": "Makie",
    "section": "Figure",
    "text": "Figure\nLoad the package\nHere, we are using CairoMakie\n\nusing CairoMakie                        # no need to import Makie itself\n\n\n\n Create a Figure (container object)\n\nfig = Figure()\n\n\n\n\n\n\n\n \n\ntypeof(fig)\n\nFigure"
  },
  {
    "objectID": "julia/makie_slides.html#axis",
    "href": "julia/makie_slides.html#axis",
    "title": "Makie",
    "section": "Axis",
    "text": "Axis\n\n\nThen, you can create an Axis\n\nax = Axis(Figure()[1, 1])\n\nAxis with 0 plots:\n\n\n\n\n\n\n\ntypeof(ax)\n\nAxis"
  },
  {
    "objectID": "julia/makie_slides.html#plot",
    "href": "julia/makie_slides.html#plot",
    "title": "Makie",
    "section": "Plot",
    "text": "Plot\nFinally, we can add a plot\n\nfig = Figure()\nax = Axis(fig[1, 1])\nx = LinRange(-10, 10, 20)\ny = x\nscatter!(ax, x, y)  # Functions with ! transform their arguments\nfig"
  },
  {
    "objectID": "julia/makie_slides.html#d",
    "href": "julia/makie_slides.html#d",
    "title": "Makie",
    "section": "2D",
    "text": "2D\n\n\nusing CairoMakie\nusing StatsBase, LinearAlgebra\nusing Interpolations, OnlineStats\nusing Distributions\nCairoMakie.activate!(type = \"png\")\n\nfunction eq_hist(matrix; nbins = 256 * 256)\n    h_eq = fit(Histogram, vec(matrix), nbins = nbins)\n    h_eq = normalize(h_eq, mode = :density)\n    cdf = cumsum(h_eq.weights)\n    cdf = cdf / cdf[end]\n    edg = h_eq.edges[1]\n    interp_linear = LinearInterpolation(edg, [cdf..., cdf[end]])\n    out = reshape(interp_linear(vec(matrix)), size(matrix))\n    return out\nend\n\nfunction getcounts!(h, fn; n = 100)\n    for _ in 1:n\n        vals = eigvals(fn())\n        x0 = real.(vals)\n        y0 = imag.(vals)\n        fit!(h, zip(x0,y0))\n    end\nend\n\nm(;a=10rand()-5, b=10rand()-5) = [0 0 0 a; -1 -1 1 0; b 0 0 0; -1 -1 -1 -1]\n\nh = HeatMap(range(-3.5,3.5,length=1200), range(-3.5,3.5, length=1200))\ngetcounts!(h, m; n=2_000_000)\n\nwith_theme(theme_black()) do\n    fig = Figure(figure_padding=0,resolution=(600,600))\n    ax = Axis(fig[1,1]; aspect = DataAspect())\n    heatmap!(ax,-3.5..3.5, -3.5..3.5, eq_hist(h.counts); colormap = :bone_1)\n    hidedecorations!(ax)\n    hidespines!(ax)\n    fig\nend"
  },
  {
    "objectID": "julia/makie_slides.html#d-output",
    "href": "julia/makie_slides.html#d-output",
    "title": "Makie",
    "section": "2D",
    "text": "2D"
  },
  {
    "objectID": "julia/makie_slides.html#d-1",
    "href": "julia/makie_slides.html#d-1",
    "title": "Makie",
    "section": "3D",
    "text": "3D\n\n\nusing GLMakie, Random\nGLMakie.activate!()\n\nRandom.seed!(13)\nx = -6:0.5:6\ny = -6:0.5:6\nz = 6exp.( -(x.^2 .+ y' .^ 2)./4)\n\nbox = Rect3(Point3f(-0.5), Vec3f(1))\nn = 100\ng(x) = x^(1/10)\nalphas = [g(x) for x in range(0,1,length=n)]\ncmap_alpha = resample_cmap(:linear_worb_100_25_c53_n256, n, alpha = alphas)\n\nwith_theme(theme_dark()) do\n    fig, ax, = meshscatter(x, y, z;\n                           marker=box,\n                           markersize = 0.5,\n                           color = vec(z),\n                           colormap = cmap_alpha,\n                           colorrange = (0,6),\n                           axis = (;\n                                   type = Axis3,\n                                   aspect = :data,\n                                   azimuth = 7.3,\n                                   elevation = 0.189,\n            perspectiveness = 0.5),\n        figure = (;\n            resolution =(1200,800)))\n    meshscatter!(ax, x .+ 7, y, z./2;\n        markersize = 0.25,\n        color = vec(z./2),\n        colormap = cmap_alpha,\n        colorrange = (0, 6),\n        ambient = Vec3f(0.85, 0.85, 0.85),\n        backlight = 1.5f0)\n    xlims!(-5.5,10)\n    ylims!(-5.5,5.5)\n    hidedecorations!(ax; grid = false)\n    hidespines!(ax)\n    fig\nend"
  },
  {
    "objectID": "julia/makie_slides.html#d-1-output",
    "href": "julia/makie_slides.html#d-1-output",
    "title": "Makie",
    "section": "3D",
    "text": "3D"
  },
  {
    "objectID": "julia/makie_slides.html#cairomakie",
    "href": "julia/makie_slides.html#cairomakie",
    "title": "Makie",
    "section": "CairoMakie",
    "text": "CairoMakie\nCairoMakie will run without problem on the Alliance clusters\nIt is not designed for interactivity, so saving to file is what makes the most sense \n\nExample:\n\nsave(\"graph.png\", fig)\n\nRemember however that CairoMakie is 2D only (for now)"
  },
  {
    "objectID": "julia/makie_slides.html#glmakie",
    "href": "julia/makie_slides.html#glmakie",
    "title": "Makie",
    "section": "GLMakie",
    "text": "GLMakie\n\nGLMakie relies on GLFW to create windows with OpenGL\nGLFW doesn‚Äôt support creating contexts without an associated window\nThe dependency GLFW.jl will thus not install in the clusters‚Äîeven with X11 forwarding‚Äîunless you use VDI nodes, VNC, or Virtual GL"
  },
  {
    "objectID": "julia/makie_slides.html#wglmakie",
    "href": "julia/makie_slides.html#wglmakie",
    "title": "Makie",
    "section": "WGLMakie",
    "text": "WGLMakie\n\nYou can setup a server with JSServe.jl as per the documentation\nHowever, this method is intended at creating interactive widget, e.g.¬†for a website\nWhile this is really cool, it isn‚Äôt optimized for performance\nThere might also be a way to create an SSH tunnel to your local browser, although there is no documentation on this\nBest probably is to save to file"
  },
  {
    "objectID": "julia/makie_slides.html#conclusion-about-the-makie-ecosystem-on-production-clusters",
    "href": "julia/makie_slides.html#conclusion-about-the-makie-ecosystem-on-production-clusters",
    "title": "Makie",
    "section": "Conclusion about the Makie ecosystem on production clusters",
    "text": "Conclusion about the Makie ecosystem on production clusters\n\n2D plots: use CairoMakie and save to file\n3D plots: use WGLMakie and save to file"
  },
  {
    "objectID": "julia/tabular.html",
    "href": "julia/tabular.html",
    "title": "Working with tabular data:",
    "section": "",
    "text": "Requirements:\n1 - The current Julia stable release\nInstallation instructions can be found here.\n2 - The packages: CSV, DataFrames, TimeSeries, Plots\nPackages can be installed with ] add <package>.\n3 - Covid-19 data from the Johns Hopkins University CSSE repository\nClone (git clone <repo url>) or download and unzip the repository."
  },
  {
    "objectID": "julia/tabular.html#load-the-data",
    "href": "julia/tabular.html#load-the-data",
    "title": "Working with tabular data:",
    "section": "Load the data",
    "text": "Load the data\nIf you did not clone or download and unzip the Covid-19 data repository in your working directory, adapt the path consequently.\n#= create a variable with the path we are interested in;\nthis makes the code below a bit shorter =#\ndir = \"COVID-19/csse_covid_19_data/csse_covid_19_time_series\"\n\n# create a list of the full paths of all the files in dir\nlist = joinpath.(relpath(dir), readdir(dir))\n\n#= read in the 3 csv files with confirmed, dead, and recovered numbers\ncorresponding to the first set of data (until March 22, 2020) =#\ndat = DataFrame.(CSV.File.(list[collect(2:4)]))\nWe now have a one-dimensional array of 3 DataFrames called dat."
  },
  {
    "objectID": "julia/tabular.html#transform-data-into-long-format",
    "href": "julia/tabular.html#transform-data-into-long-format",
    "title": "Working with tabular data:",
    "section": "Transform data into long format",
    "text": "Transform data into long format\n# rename some variables to easier names\nDataFrames.rename!.(dat, Dict.(1 => Symbol(\"province\"),\n                               2 => Symbol(\"country\")))\n\n# create a one-dimensional array of strings\nvar = [\"total\", \"dead\", \"recovered\"]\n\n#= transform the data into long format in a vectorized fashion\nusing both our one-dimensional arrays of 3 elements =#\ndatlong = map((x, y) -> stack(x, Not(collect(1:4)),\n                              variable_name = Symbol(\"date\"),\n                              value_name = Symbol(\"$y\")),\n              dat, var)\nWe now have a one-dimensional array of 3 DataFrames in long format called datlong.\n# join all elements of this array into a single DataFrame\nall = join(datlong[1], datlong[2], datlong[3],\n           on = [:date, :country, :province, :Lat, :Long])\n\n# get rid of \"Lat\" and \"Long\" and re-order the columns\nselect!(all, [4, 3, 1, 2, 7, 8])\n\n#= turn the year from 2 digits to 4 digits using regular expression\n(in a vectorised fashion by braodcasting with the dot notation);\nthen turn these values into strings, and finally into dates =#\nall.date = Date.(replace.(string.(all[:, 3]),\n                          r\"(.*)(..)$\" => s\"\\g<1>20\\2\"), \"m/dd/yy\");\n\n#= replace the missing values by the string \"NA\"\n(these are not real missing values, but rather non applicable ones) =#\nreplace!(all.province, missing => \"NA\");\nWe now have a single DataFrame called all, in long format, with the variables confirmed, dead, recovered, and ill.\nCalculate the number of currently ill individuals (again, in a vectorized fashion, by broadcasting with the dot notation):\nall.current = all.total .- all.dead .- all.recovered;"
  },
  {
    "objectID": "julia/tabular.html#world-summary",
    "href": "julia/tabular.html#world-summary",
    "title": "Working with tabular data:",
    "section": "World summary",
    "text": "World summary\nTo make a single plot with world totals of confirmed, dead, recovered, and ill cases, we want the sums of these variables for each day. We do this by grouping the data by date:\nworld = by(all, :date,\n           total = :total => sum,\n           dead = :dead => sum,\n           recovered = :recovered => sum,\n           current = :current => sum)\nNow we can plot our new variable world.\nAs our data is a time series, we need to transform it to a TimeArray thanks to the TimeArray() function from the TimeSeries package.\nplot(TimeArray(world, timestamp = :date),\n     title = \"World\",\n     legend = :outertopright,\n     widen = :false)\n Data until March 22, 2020"
  },
  {
    "objectID": "julia/tabular.html#countriesprovinces-summaries",
    "href": "julia/tabular.html#countriesprovinces-summaries",
    "title": "Working with tabular data:",
    "section": "Countries/provinces summaries",
    "text": "Countries/provinces summaries\nNow, we want to group the data by country:\ncountries = groupby(all, :country)\nWe also need to know how the authors of the dataset decided to label the various countries and their subregions.\nFor example, if you want to see what the data looks like for France, Canada, and India, you can run:\ncountries[findall(x -> \"France\" in x, keys(countries))]\ncountries[findall(x -> \"Canada\" in x, keys(countries))]\ncountries[findall(x -> \"India\" in x, keys(countries))]\nThen you need to subset the data for the countries or provinces you are interested in.\nHere are some examples:\n# countries for which there are data for several provinces\ncanada = all[all[:, :country] .== \"Canada\", :]\nus = all[all[:, :country] .== \"US\", :]\nchina = all[all[:, :country] .== \"China\", :]\n\n# countries with no province data\nskorea = all[all[:, :country] .== \"Korea, South\", :]\ntaiwan = all[all[:, :country] .== \"Taiwan*\", :]\nsingapore = all[all[:, :country] .== \"Singapore\", :]\nitaly = all[all[:, :country] .== \"Italy\", :]\nspain = all[all[:, :country] .== \"Spain\", :]\n\n#= countries wich have subregions spread widely in the world;\nhere, I took the arbitrary decision to only look at the main subregions =#\nfrance = all[all[:, :province] .== \"France\", :]\nuk = all[all[:, :province] .== \"United Kingdom\", :]\n\n# provinces\nbc = all[all[:, :province] .== \"British Columbia\", :]\nny = all[all[:, :province] .== \"New York\", :]\nCalculate the totals for Canada, US, and China which all have data for subregions:\ncanada, us, china = by.([canada, us, china], :date,\n                        total = :total => sum,\n                        dead = :dead => sum,\n                        recovered = :recovered => sum,\n                        current = :current => sum)\nloclist1 = [canada, us, china]\nloctitles1 = [\"Canada\", \"US\", \"China\"]\n\npcanada, pus, pchina =\n    map((x, y) -> plot(TimeArray(x, timestamp = :date),\n                       title = \"$y\", legend = :outertopright,\n                       widen = :false, dpi = :300),\n        loclist1, loctitles1)\nloclist2 = [france, bc, ny, taiwan, skorea, singapore, spain, italy, uk]\nloctitles2 = [\"France\", \"BC\", \"NY\", \"Taiwan\", \"South Korea\",\n              \"Singapore\", \"Spain\", \"Italy\", \"UK\"]\n\npfrance, pbc, pny, ptaiwan, pskorea,\npsingapore, pspain, pitaly, puk =\n    map((x, y) -> plot(TimeArray(select(x, Not([:country, :province])),\n                                 timestamp = :date),\n                       title = \"$y\", legend = :outertopright,\n                       widen = :false, dpi = :300),\n        loclist2, loctitles2)\nNow, let‚Äôs plot a few countries/provinces:\n\nNorth America\nplot(pcanada, pbc, pus, pny,\n     legend = false, titlefontsize = 7, tickfontsize = 6)\n Data until March 22, 2020\n\n\nAsia\nplot(pchina, ptaiwan, pskorea, psingapore,\n     legend = false, titlefontsize = 7, tickfontsize = 6)\n Data until March 22, 2020\n\n\nEurope\nplot(pfrance, pspain, pitaly, puk,\n     legend = false, titlefontsize = 7, tickfontsize = 6)\n Data until March 22, 2020"
  },
  {
    "objectID": "julia/tabular.html#summary-graphs",
    "href": "julia/tabular.html#summary-graphs",
    "title": "Working with tabular data:",
    "section": "Summary graphs",
    "text": "Summary graphs\n\n\nYour turn:\n\nWrite the code to create an up-to-date graph for the world using the files: time_series_covid19_confirmed_global.csv and time_series_covid19_deaths_global.csv.\n\nHere is the result:\n Data until March 25, 2020\n\n\nYour turn:\n\nCreate up-to-date graphs for the countries and/or provinces of your choice.\n\nHere are a few possible results:\n Data until March 25, 2020"
  },
  {
    "objectID": "julia/tabular.html#countries-comparison",
    "href": "julia/tabular.html#countries-comparison",
    "title": "Working with tabular data:",
    "section": "Countries comparison",
    "text": "Countries comparison\nOur side by side graphs don‚Äôt make comparisons very easy since they vary greatly in their axes scales.\nOf course, we could constrain them to have the same axes, but then, why not plot multiple countries or provinces in the same graph?\ncanada[!, :loc] .= \"Canada\";\nchina[!, :loc] .= \"China\";\n\nall = join(all, canada, china, on = [:date, :total, :dead, :loc],\n           kind = :outer)\n\nconfirmed = unstack(all[:, collect(3:5)], :loc, :total)\n\nconf_sel = select(confirmed,\n                  [:date, :Italy, :Spain, :China, :Iran,\n                   :France, :US, Symbol(\"South Korea\"), :Canada])\n\nplot(TimeArray(conf_sel, timestamp = :date),\n     title = \"Confirmed across a few countries\",\n     legend = :outertopright, widen = :false)\n Data until March 25, 2020\n\n\nYour turn:\n\nWrite the code to make a similar graph with the number of deaths in a few countries of your choice.\n\nHere is a possible result:\n Data until March 25, 2020"
  },
  {
    "objectID": "ml/audio_dataloader.html",
    "href": "ml/audio_dataloader.html",
    "title": "Creating an audio DataLoader",
    "section": "",
    "text": "import torch\nimport torchaudio\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "ml/audio_dataloader.html#where-to-store-this-data-in-the-cluster",
    "href": "ml/audio_dataloader.html#where-to-store-this-data-in-the-cluster",
    "title": "Creating an audio DataLoader",
    "section": "Where to store this data in the cluster",
    "text": "Where to store this data in the cluster\nWe will all use the same data. It would make little sense to all download it in our home directory.\n\nIn the Alliance clusters, a good place to store data shared amongst members of a project is in the /project file system.\nYou usually belong to /project/def-<group>, where <group> is the name of your PI. You can access it from your home directory through the symbolic link ~/projects/def-<group>.\n\nIn our training cluster, we are all part of the group def-sponsor00, accessible through /project/def-sponsor00 (or the hyperlink ~/projects/def-sponsor00).\nWe will thus use ~/projects/def-sponsor00/data as the root argument for torchaudio.datasets.yesno):\nyesno_data = torchaudio.datasets.YESNO(\n    '~/projects/def-sponsor00/data/',\n    download=True)"
  },
  {
    "objectID": "ml/autograd.html",
    "href": "ml/autograd.html",
    "title": "Automatic differentiation",
    "section": "",
    "text": "Imagine how hard it would be to write the chain rules of neural networks (with so many derivatives!) in backpropagation manually.\nPyTorch has automatic differentiation abilities‚Äîmeaning that it can track all the operations conducted on tensors and do the backprop for you‚Äîthanks to its package torch.autograd.\nLet‚Äôs have a first look at it."
  },
  {
    "objectID": "ml/autograd.html#the-grad_fun-attribute",
    "href": "ml/autograd.html#the-grad_fun-attribute",
    "title": "Automatic differentiation",
    "section": "The grad_fun attribute",
    "text": "The grad_fun attribute\nWhenever a tensor is created by an operation involving a tracked tensor, it has a grad_fun attribute:\nx = torch.ones(2, 4, requires_grad=True)\nprint(x)\ny = x + 1\nprint(y)\nprint(y.grad_fn)"
  },
  {
    "objectID": "ml/autograd.html#judicious-tracking",
    "href": "ml/autograd.html#judicious-tracking",
    "title": "Automatic differentiation",
    "section": "Judicious tracking",
    "text": "Judicious tracking\nYou don‚Äôt want to track more than is necessary. There are multiple ways to avoid tracking what you don‚Äôt want to.\nYou can simply stop tracking computations on a tensor with the method detach:\nx = torch.rand(4, 3, requires_grad=True)\nprint(x)\nprint(x.detach_())\nYou can change its requires_grad flag:\nx = torch.rand(4, 3, requires_grad=True)\nprint(x)\nprint(x.requires_grad_(False))\nAlternatively, you can wrap any code you don‚Äôt want to track with with torch.no_grad():\nwith torch.no_grad():\n    <some code>"
  },
  {
    "objectID": "ml/autograd.html#manual-derivative-calculation",
    "href": "ml/autograd.html#manual-derivative-calculation",
    "title": "Automatic differentiation",
    "section": "Manual derivative calculation",
    "text": "Manual derivative calculation\nThe formula for this first derivative, with the loss function we used, is:\n\\[\\text{gradient}_\\text{predicted}=2(\\text{predicted} - \\text{real})\\]\nThere is no point in adding this operation to predicted‚Äôs computation history, so we will exclude it with with torch.no_grad():\nwith torch.no_grad():\n    manual_gradient_predicted = 2.0 * (predicted - real)\n\nprint(manual_gradient_predicted)"
  },
  {
    "objectID": "ml/autograd.html#automatic-derivative-calculation",
    "href": "ml/autograd.html#automatic-derivative-calculation",
    "title": "Automatic differentiation",
    "section": "Automatic derivative calculation",
    "text": "Automatic derivative calculation\nNow, with torch.autograd:\nloss.backward()\nSince we tracked computations on predicted, we can calculate its gradient with:\nauto_gradient_predicted = predicted.grad\nprint(auto_gradient_predicted)"
  },
  {
    "objectID": "ml/autograd.html#comparison",
    "href": "ml/autograd.html#comparison",
    "title": "Automatic differentiation",
    "section": "Comparison",
    "text": "Comparison\nThe result is the same, as can be tested with:\nprint(manual_gradient_predicted.eq(auto_gradient_predicted).all())\nThe calculation of this first derivative of backpropagation was simple enough. But to propagate all the derivatives calculations backward through the chain rule would quickly turn into a deep calculus problem.\nWith torch.autograd, calculating the gradients of all the other elements of the network is as simple as calling them with the attribute grad once the function torch.Tensor.backward() has been run."
  },
  {
    "objectID": "ml/checkpoints.html",
    "href": "ml/checkpoints.html",
    "title": "Saving/loading models and checkpointing",
    "section": "",
    "text": "Saving models\nYou can save a model by serializing its internal state dictionary. The state dictionary is a Python dictionary that contains the parameters of your model.\ntorch.save(model.state_dict(), \"model.pth\")\n\n\nLoading models\nTo recreate your model, you first need to recreate its structure:\nmodel = Net()\nThen you can load the state dictionary containing the parameters values into it:\nmodel.load_state_dict(torch.load(\"model.pth\"))\n\n\nCreate a checkpoint\ntorch.save({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'loss': loss,\n            ...\n            }, PATH)\n\n\nResume training from a checkpoint\nmodel = TheModelClass(*args, **kwargs)\noptimizer = TheOptimizerClass(*args, **kwargs)\n\ncheckpoint = torch.load(PATH)\nmodel.load_state_dict(checkpoint['model_state_dict'])\noptimizer.load_state_dict(checkpoint['optimizer_state_dict'])\nepoch = checkpoint['epoch']\nloss = checkpoint['loss']\n\nmodel.train()"
  },
  {
    "objectID": "ml/choosing_frameworks.html",
    "href": "ml/choosing_frameworks.html",
    "title": "Which framework to choose?",
    "section": "",
    "text": "With the growing popularity of machine learning, many frameworks have appeared in various languages. One of the questions you might be facing is: which tool should I choose?\nThe main focus here is on the downsides of proprietary tools.\n\n\nPoints worth considering\nThere are a several points you might want to consider in making that choice. For instance, what tools do people use in your field? (what tools are used in the papers you read?) What tools are your colleagues and collaborators using?\nLooking a bit further into the future, whether you are considering staying in academia or working in industry could also influence your choice. If the former, paying attention to the literature is key, if the latter, it may be good to have a look at the trends in job postings.\nSome frameworks offer collections of already-made toolkits. They are thus easy to start using and do not require a lot of programming experience. On the other hand, they may feel like black boxes and they are not very customizable. Scikit-learn and Keras‚Äîusually run on top of TensorFlow‚Äîfall in that category. Lower level tools allow you full control and tuning of your models, but can come with a steeper learning curve.\nPyTorch, developed by Facebook‚Äôs AI Research lab, has seen a huge increase in popularity in research in recent years due to its highly pythonic syntax, very convenient tensors, just-in-time (JIT) compilation, dynamic computation graphs, and because it is free and open-source.\nSeveral libraries are now adding a higher level on top of PyTorch: fastai, which we will use in this course, PyTorch Lightning, and PyTorch Ignite. fastai, in addition to the convenience of being able to write a model in a few lines of code, allows to dive as low as you choose into the PyTorch code, thus making it unconstrained by the optional ease of use. It also adds countless functionality. The downside of using this added layer is that it can make it less straightforward to install on a machine or to tweak and customize.\nThe most popular machine learning library currently remains TensorFlow, developed by the Google Brain Team. While it has a Python API, its syntax can be more obscure.\nJulia‚Äôs syntax is well suited for the implementation of mathematical models, GPU kernels can be written directly in Julia, and Julia‚Äôs speed is attractive in computation hungry fields. So Julia has also seen the development of many ML packages such as Flux or Knet. The user base of Julia remains quite small however.\nMy main motivation in writing this section however is to raise awareness about one question that should really be considered: whether the tool you decide to learn and use in your research is open-source or proprietary.\n\n\nProprietary tools: a word of caution\nAs a student, it is tempting to have the following perspective:\n\nMy university pays for this very expensive license. I have free access to this very expensive tool. It would be foolish not to make use of it while I can!\n\nWhen there are no equivalent or better open-source tools, that might be true. But when superior open-source tools exist, these university licenses are more of a trap than a gift.\nHere are some of the reasons you should be wary of proprietary tools:\n\nResearchers who do not have access to the tool cannot reproduce your methods\nLarge Canadian universities may offer a license for the tool, but grad students in other countries, independent researchers, researchers in small organizations, etc. may not have access to a free license (or tens of thousands of dollars to pay for it).\n\n\nOnce you graduate, you may not have access to the tool anymore\nOnce you leave your Canadian institution, you may become one of those researchers who do not have access to that tool. This means that you will not be able to re-run your thesis analyses, re-use your methods, and apply the skills you learnt. The time you spent learning that expensive tool you could play with for free may feel a lot less like a gift then.\n\n\nYour university may stop paying for a license\nAs commercial tools fall behind successful open-source ones, some universities may decide to stop purchasing a license. It happened during my years at SFU with an expensive and clunky citation manager which, after having been promoted for years by the library through countless workshops, got abandoned in favour of a much better free and open-source one.\n\n\nYou may get locked-in\nProprietary tools often come with proprietary formats and, depending on the tool, it may be painful (or impossible) to convert your work to another format. When that happens, you are locked-in.\n\n\nProprietary tools are often black boxes\nIt is often impossible to see the source code of proprietary software.\n\n\nLong-term access\nIt is often very difficult to have access to old versions of proprietary tools (and this can be necessary to reproduce old studies). When companies disappear, the tools they produced usually disappear with them. open-source tools, particularly those who have their code under version control in repositories such as GitHub, remain fully accessible (including all stages of development), and if they get abandoned, their maintenance can be taken over or restarted by others.\n\n\nThe licenses you have access to may be limiting and a cause of headache\nFor instance, the Alliance does not have an unlimited number of MATLAB licenses. Since these licenses come with complex rules (one license needed for each node, additional licenses for each toolbox, additional licenses for newer tools, etc.), it can quickly become a nightmare to navigate through it all. You may want to have a look at some of the comments in this thread.\n\n\nProprietary tools fall behind popular open-source tools\nEven large teams of software engineers cannot compete against an active community of researchers developing open-source tools. When open-source tools become really popular, the number of users contributing to their development vastly outnumbers what any company can provide. The testing, licensing, and production of proprietary tools are also too slow to keep up with quickly evolving fields of research. (Of course, open-source tools which do not take off and remain absolutely obscure do not see the benefit of a vast community.)\n\n\nProprietary tools often fail to address specialized edge cases needed in research\nIt is not commercially sound to develop cutting edge capabilities so specialized in a narrow subfield that they can only target a minuscule number of customers. But this is often what research needs. With open-source tools, researchers can develop the capabilities that fit their very specific needs. So while commercial tools are good and reliable for large audiences, they are often not the best in research. This explains the success of R over tools such as SASS or Stata in the past decade.\n\n\n\nConclusion\nAll that said, sometimes you don‚Äôt have a choice over the tool to use for your research as this may be dictated by the culture in your field or by your supervisor. But if you are free to choose and if superior or equal open-source alternatives exist and are popular, do not fall in the trap of thinking that because your university and the Alliance pay for a license, you should make use of it. It may be free for you‚Äîfor now‚Äîbut it can have hidden costs."
  },
  {
    "objectID": "ml/concept.html",
    "href": "ml/concept.html",
    "title": "Overarching concept of deep learning",
    "section": "",
    "text": "Neural networks learn by adjusting their parameters automatically in an iterative manner. This is derived from Arthur Samuel‚Äôs concept.\nIt is important to get a good understanding of this process, so let‚Äôs go over it step by step.\n\n\nDecide on an architecture\n\nThe architecture won‚Äôt change during training. This is set. The type of architecture you choose (e.g.¬†CNN, Transformer, etc.) depends on the type of data you have (e.g.¬†vision, textual, etc.). The depth and breadth of your network depend on the amount of data and computing resource you have.\n\n\nSet some initial parameters\n\nYou can initialize them randomly or get much better ones through transfer learning.\nWhile the parameters are also part of the model, those will change during training.\n\n\nGet some labelled data\n\nWhen we say that we need a lot of data for machine learning, we mean ‚Äúlots of labelled data‚Äù as this is what gets used for training models.\n\n\nMake sure to keep some data for testing\n\nThose data won‚Äôt be used for training the model. Often people keep around 20% of their data for testing.\n\n\nTrain data and parameters are passed through the architecture\n\nThe train data are the inputs and the process of calculating the outputs is the forward pass.\n\n\nThe outputs of the model are predictions\n\n\n\nCompare those predictions to the train labels\n\nSince our data was labelled, we know what the true outputs are.\n\n\nCalculate train loss\n\nThe deviation of our predictions from the true outputs gives us a measure of training loss.\n\n\nParameters adjustement\n\nThe parameters get automatically adjusted to reduce the training loss through the mechanism of backpropagation.\nThis is the actual training part.\nThis process is repeated many times. Training models is pretty much a giant for loop.\n\n\nFrom model to program\n\nRemember that the model architecture is fixed, but that the parameters change at each iteration of the training process.\nWhile the labelled data are key to training, what we are really interested in is the combination of architecture + final parameters.\n\nWhen the training is over, the parameters become fixed. Which means that our model now behaves like a classic program.\n\n\n\nEvaluating the model\n\nWe can now use the testing set (which was never used to train the model) to evaluate our model: if we pass the test inputs through our program, we get some predictions that we can compare to the test labels (which are the true outputs).\nThis gives us the test loss: a measure of how well our model performs.\n\n\nUse the model\n\nNow that we have a program, we can use it on unlabelled inputs to get what people ultimately want: unknown outputs. This is when we put our model to actual use to solve some problem."
  },
  {
    "objectID": "ml/fastai.html",
    "href": "ml/fastai.html",
    "title": "fastai",
    "section": "",
    "text": "fastai is a deep learning library which provides practitioners with high-level components that can quickly and easily provide state-of-the-art results in standard deep learning domains. This webinar will take a closer look at the features and functionality of fastai."
  },
  {
    "objectID": "ml/flux.html",
    "href": "ml/flux.html",
    "title": "Machine learning in Julia with Flux",
    "section": "",
    "text": "Find this webinar in the Julia section."
  },
  {
    "objectID": "ml/high_level_frameworks.html",
    "href": "ml/high_level_frameworks.html",
    "title": "High-level frameworks for PyTorch",
    "section": "",
    "text": "Several popular higher-level frameworks are built on top of PyTorch and make the code easier to write and run:\n\nignite is developed by the PyTorch project. It is the easiest to combine with raw PyTorch, but not the easiest to use.\nPyTorch-lightning is probably the most popular. It makes some operations (e.g.¬†using multiple GPUs) very easy, but some of the Lightning products are not free.\nfastai adds several interesting concepts to PyTorch borrowed from other languages, but debugging can be challenging.\nCatalyst\n\nThe following tag trends on Stack Overflow might give an idea of the popularity of these frameworks over time (catalyst doesn‚Äôt have any Stack Overflow tag):\n\nIf this data is to be believed, ignite never really took off (it also has a lower number of stars on GitHub), fast-ai was extremely popular when it came out, but its usage is going down, and PyTorch-lightning is currently the most popular.\n\nShould you use one (and which one)?\nLearning raw PyTorch is probably the best option for research. PyTorch is stable and here to stay. Higher-level frameworks may rise and drop in popularity and today‚Äôs popular one may see little usage tomorrow.\nRaw PyTorch is also the most flexible, the closest to the actual computations happening in your model, and probably the easiest to debug.\nDepending on your deep learning trajectory, you might find some of these tools useful though:\n\nIf you work in industry, you might want or need to get results quickly\nSome operations (e.g.¬†parallel execution on multiple GPUs) can be tricky in raw PyTorch, while being extremely streamlined when using e.g.¬†PyTorch-lightning\nEven in research, it might make sense to spend more time thinking about the structure of your model and the functioning of a network instead of getting bogged down in writing code\n\n\nBefore moving to any of these tools, it is probably a good idea to get a good knowledge of raw PyTorch: use these tools to simplify your workflow, not cloud your understanding of what your code is doing."
  },
  {
    "objectID": "ml/hpc.html",
    "href": "ml/hpc.html",
    "title": "Machine learning on production clusters",
    "section": "",
    "text": "This lesson is a summary of relevant information while using Python in an HPC context for deep learning.\nWhen you ssh into one of the Alliance clusters, you log into the login node.\nEverybody using a cluster uses that node to enter the cluster. Do not run anything computationally intensive on this node or you would make the entire cluster very slow for everyone. To run your code, you need to start an interactive job or submit a batch job to Slurm (the job scheduler used by the Alliance clusters)."
  },
  {
    "objectID": "ml/hpc.html#copy-files-tofrom-the-cluster",
    "href": "ml/hpc.html#copy-files-tofrom-the-cluster",
    "title": "Machine learning on production clusters",
    "section": "Copy files to/from the cluster",
    "text": "Copy files to/from the cluster\n\nFew files\nIf you need to copy files to or from the cluster, you can use scp from your local machine.\n\nCopy file from your computer to the cluster\n[local]$ scp </local/path/to/file> <user>@<hostname>:<path/in/cluster>\n\nExpressions between the < and > signs need to be replaced by the relevant information (without those signs).\n\n\n\nCopy file from the cluster to your computer\n[local]$ scp <user>@<hostname>:<cluster/path/to/file> </local/path>\n\n\n\nLarge amount of data\nUse Globus for large data transfers.\n\nThe Alliance is starting to store classic ML datasets on its clusters. So if your research uses a common dataset, it may be worth inquiring whether it might be available before downloading a copy."
  },
  {
    "objectID": "ml/hpc.html#large-collections-of-files",
    "href": "ml/hpc.html#large-collections-of-files",
    "title": "Machine learning on production clusters",
    "section": "Large collections of files",
    "text": "Large collections of files\nThe Alliance clusters are optimized for very large files and are slowed by large collections of small files. Datasets with many small files need to be turned into single-file archives with tar. Failing to do so will affect performance not just for you, but for all users of the cluster.\n$ tar cf <data>.tar <path/to/dataset/directory>/*\n\n\nIf you want to also compress the files, replace tar cf with tar czf\nAs a modern alternative to tar, you can use Dar"
  },
  {
    "objectID": "ml/hpc.html#job-script",
    "href": "ml/hpc.html#job-script",
    "title": "Machine learning on production clusters",
    "section": "Job script",
    "text": "Job script\n\nHere is an example script:\n\n#!/bin/bash\n#SBATCH --job-name=<name>*            # job name\n#SBATCH --account=def-<account>\n#SBATCH --time=<time>                 # max walltime in D-HH:MM or HH:MM:SS\n#SBATCH --cpus-per-task=<number>      # number of cores\n#SBATCH --gres=gpu:<type>:<number>    # type and number of GPU(s) per node\n#SBATCH --mem=<mem>                   # max memory (default unit is MB) per node\n#SBATCH --output=%x_%j.out*       # file name for the output\n#SBATCH --error=%x_%j.err*        # file name for errors\n#SBATCH --mail-user=<email_address>*\n#SBATCH --mail-type=ALL*\n\n# Load modules\n# (Do not use this in our workshop since we aren't using GPUs)\n# (Note: loading the Python module is not necessary\n# when you activate a Python virtual environment)\n# module load cudacore/.10.1.243 cuda/10 cudnn/7.6.5\n\n# Create a variable with the directory for your ML project\nSOURCEDIR=~/<path/project/dir>\n\n# Activate your Python virtual environment\nsource ~/env/bin/activate\n\n# Transfer and extract data to a compute node\nmkdir $SLURM_TMPDIR/data\ntar xf ~/projects/def-<user>/<data>.tar -C $SLURM_TMPDIR/data\n\n# Run your Python script on the data\npython $SOURCEDIR/<script>.py $SLURM_TMPDIR/data\n\n\n%x will get replaced by the script name and %j by the job number\nIf you compressed your data with tar czf, you need to extract it with tar xzf\nSBATCH options marked with a * are optional\nThere are various other options for email notifications\n\n\nYou may wonder why we transferred data to a compute node. This makes any I/O operation involving your data a lot faster, so it will speed up your code. Here is how this works:\nFirst, we create a temporary data directory in $SLURM_TMPDIR:\n$ mkdir $SLURM_TMPDIR/data\n\nThe variable $SLURM_TMPDIR is created by Slurm on the compute node where a job is running. Its path is /localscratch/<user>.<jobid>.0. Anything in it gets deleted when the job is done.\n\nThen we extract the data into it:\n$ tar xf ~/projects/def-<user>/<data>.tar -C $SLURM_TMPDIR/data\nIf your data is not in a tar file, you can simply copy it to the compute node running your job:\n$ cp -r ~/projects/def-<user>/<data> $SLURM_TMPDIR/data"
  },
  {
    "objectID": "ml/hpc.html#job-handling",
    "href": "ml/hpc.html#job-handling",
    "title": "Machine learning on production clusters",
    "section": "Job handling",
    "text": "Job handling\n\nSubmit a job\n$ cd </dir/containing/job>\n$ sbatch <jobscript>.sh\n\n\nCheck the status of your job(s)\n$ sq\n\nPD = pending\nR = running\nCG = completing (Slurm is doing the closing processes)\nNo information = your job has finished running\n\n\n\nCancel a job\n$ scancel <jobid>\n\n\nDisplay efficiency measures of a completed job\n$ seff <jobid>"
  },
  {
    "objectID": "ml/hpc.html#gpu-types",
    "href": "ml/hpc.html#gpu-types",
    "title": "Machine learning on production clusters",
    "section": "GPU types",
    "text": "GPU types\nSeveral Alliance clusters have GPUs. Their numbers and types differ:\n From the Alliance Wiki\nThe default is 12G P100, but you can request another type with SBATCH --gres=gpu:<type>:<number> (example: --gres=gpu:p100l:1 to request a 16G P100 on Cedar). Please refer to the Alliance Wiki for more details."
  },
  {
    "objectID": "ml/hpc.html#number-of-gpus",
    "href": "ml/hpc.html#number-of-gpus",
    "title": "Machine learning on production clusters",
    "section": "Number of GPU(s)",
    "text": "Number of GPU(s)\nTry running your model on a single GPU first.\nIt is very likely that you do not need more than one GPU. Asking for more than you need will greatly increase your waiting time until your job is run. The lesson on distributed computing with PyTorch gives a few information as to when you might benefit from using several GPUs and provides some links to more resources. We will also offer workshops on distributed ML in the future. In any event, you should test your model before asking for several GPUs."
  },
  {
    "objectID": "ml/hpc.html#cpugpu-ratio",
    "href": "ml/hpc.html#cpugpu-ratio",
    "title": "Machine learning on production clusters",
    "section": "CPU/GPU ratio",
    "text": "CPU/GPU ratio\nHere are the Alliance recommendations:\nB√©luga:\nNo more than 10 CPU per GPU.\nCedar:\nP100 GPU: no more than 6 CPU per GPU.\nV100 GPU: no more than 8 CPU per GPU.\nGraham:\nNo more than 16 CPU per GPU."
  },
  {
    "objectID": "ml/hpc.html#activate-your-python-virtual-environment",
    "href": "ml/hpc.html#activate-your-python-virtual-environment",
    "title": "Machine learning on production clusters",
    "section": "Activate your Python virtual environment",
    "text": "Activate your Python virtual environment\n$ source ~/env/bin/activate"
  },
  {
    "objectID": "ml/hpc.html#start-an-interactive-job",
    "href": "ml/hpc.html#start-an-interactive-job",
    "title": "Machine learning on production clusters",
    "section": "Start an interactive job",
    "text": "Start an interactive job\n\nExample:\n\n$ salloc --account=def-<account> --gres=gpu:1 --cpus-per-task=6 --mem=32000 --time=0:30:0"
  },
  {
    "objectID": "ml/hpc.html#prepare-the-data",
    "href": "ml/hpc.html#prepare-the-data",
    "title": "Machine learning on production clusters",
    "section": "Prepare the data",
    "text": "Prepare the data\nCreate a temporary data directory in $SLURM_TMPDIR:\n(env) $ mkdir $SLURM_TMPDIR/data\n\nThe variable $SLURM_TMPDIR is created by Slurm on the compute node where a job is running. Its path is /localscratch/<user>.<jobid>.0. Anything in it gets deleted when the job is done.\n\nExtract the data into it:\n(env) $ tar xf ~/projects/def-<user>/<data>.tar -C $SLURM_TMPDIR/data"
  },
  {
    "objectID": "ml/hpc.html#try-to-run-your-code",
    "href": "ml/hpc.html#try-to-run-your-code",
    "title": "Machine learning on production clusters",
    "section": "Try to run your code",
    "text": "Try to run your code\nPlay in Python to test your code:\n(env) $ python\n>>> import torch\n>>> ...\nTo exit the virtual environment, run:\n(env) $ deactivate"
  },
  {
    "objectID": "ml/hpc.html#launch-tensorboard",
    "href": "ml/hpc.html#launch-tensorboard",
    "title": "Machine learning on production clusters",
    "section": "Launch TensorBoard",
    "text": "Launch TensorBoard\nFirst, you need to launch TensorBoard in the background (with a trailing &) before running your Python script. To do so, ad to your sbatch script:\ntensorboard --logdir=/tmp/<your log dir> --host 0.0.0.0 &\n\nExample:\n\n#!/bin/bash\n#SBATCH ...\n...\n\ntensorboard --logdir=/tmp/<your log dir> --host 0.0.0.0 &\npython $SOURCEDIR/<script>.py $SLURM_TMPDIR/data"
  },
  {
    "objectID": "ml/hpc.html#create-a-connection-between-the-compute-node-and-your-computer",
    "href": "ml/hpc.html#create-a-connection-between-the-compute-node-and-your-computer",
    "title": "Machine learning on production clusters",
    "section": "Create a connection between the compute node and your computer",
    "text": "Create a connection between the compute node and your computer\nOnce the job is running, you need to create a connection between the compute node running TensorBoard and your computer.\nFirst, you need to find the hostname of the compute node running the Tensorboard server. This is the value under NODELIST for your job when you run:\n$ sq\nThen, from your computer, enter this ssh command:\n[local]$ ssh -N -f -L localhost:6006:<node hostname>:6006 <user>@<cluster>.computecanada.ca\n\nReplace <node hostname> by the compute node hostname you just identified, <user> by your user name, and <cluster> by the name of the Alliance cluster hostname‚Äîe.g.¬†beluga, cedar, graham."
  },
  {
    "objectID": "ml/hpc.html#access-tensorboard",
    "href": "ml/hpc.html#access-tensorboard",
    "title": "Machine learning on production clusters",
    "section": "Access TensorBoard",
    "text": "Access TensorBoard\nYou can now open a browser (on your computer) and go to http://localhost:6006 to monitor your model running on a compute node in the cluster!"
  },
  {
    "objectID": "ml/intro_hss.html",
    "href": "ml/intro_hss.html",
    "title": "Introduction to machine learning for the humanities",
    "section": "",
    "text": "We hear about it all the time, but what really is machine learning? And what about deep learning? Neural networks?? How can any of this help me with my work? And how? Which tools do I need to make use of the transformative advances happening in that field??\nThis workshop will answer these questions in a non-technical manner to give you a high level overview of a discipline that has become crucial in all fields of research."
  },
  {
    "objectID": "ml/mnist.html",
    "href": "ml/mnist.html",
    "title": "Classifying the MNIST dataset",
    "section": "",
    "text": "In this workshop, we will classify the MNIST dataset‚Äîa classic of machine learning‚Äîwith PyTorch."
  },
  {
    "objectID": "ml/mnist.html#where-to-store-the-data-in-the-cluster",
    "href": "ml/mnist.html#where-to-store-the-data-in-the-cluster",
    "title": "Classifying the MNIST dataset",
    "section": "Where to store the data in the cluster",
    "text": "Where to store the data in the cluster\nWe will all use the same data. It would make little sense to all download it in our home directory.\nOn the Alliance clusters, a good place to store data shared amongst members of a project is in the /project file system.\nYou usually belong to /project/def-<group>, where <group> is the name of your PI. You can access it from your home directory through the symbolic link ~/projects/def-<group>.\nIn our training cluster, we are all part of the group def-sponsor00, accessible through /project/def-sponsor00 (or the hyperlink ~/projects/def-sponsor00).\nWe will thus all access the MNIST data in ~/projects/def-sponsor00/data."
  },
  {
    "objectID": "ml/mnist.html#how-to-obtain-the-data",
    "href": "ml/mnist.html#how-to-obtain-the-data",
    "title": "Classifying the MNIST dataset",
    "section": "How to obtain the data?",
    "text": "How to obtain the data?\nThe dataset can be downloaded directly from the MNIST website, but the PyTorch package TorchVision has tools to download and transform several classic vision datasets, including the MNIST.\nhelp(torchvision.datasets.MNIST)\nHelp on class MNIST in module torchvision.datasets.mnist:\n\nclass MNIST(torchvision.datasets.vision.VisionDataset)\n\n |  MNIST(root: str, train: bool = True, \n |    transform: Optional[Callable] = None,\n |    target_transform: Optional[Callable] = None, \n |    download: bool = False) -> None\n |   \n |  Args:\n |    root (string): Root directory of dataset where \n |      MNIST/raw/train-images-idx3-ubyte and \n |      MNIST/raw/t10k-images-idx3-ubyte exists.\n |    train (bool, optional): If True, creates dataset from \n |      train-images-idx3-ubyte, otherwise from t10k-images-idx3-ubyte.\n |    download (bool, optional): If True, downloads the dataset from the \n |      internet and puts it in root directory. If dataset is already \n |      downloaded, it is not downloaded again.\n |    transform (callable, optional): A function/transform that takes in \n |      an PIL image and returns a transformed version.\n |      E.g, transforms.RandomCrop\n |    target_transform (callable, optional): A function/transform that \n |      takes in the target and transforms it.\nNote that here too, the root argument sets the location of the downloaded data and we will use /project/def-sponsor00/data/."
  },
  {
    "objectID": "ml/mnist.html#prepare-the-data",
    "href": "ml/mnist.html#prepare-the-data",
    "title": "Classifying the MNIST dataset",
    "section": "Prepare the data",
    "text": "Prepare the data\nFirst, let‚Äôs load the needed libraries:\n\nimport torch\nfrom torchvision import datasets, transforms\nfrom matplotlib import pyplot as plt\n\nThe MNIST dataset already consists of a training and a testing sets, so we don‚Äôt have to split the data manually. Instead, we can directly create 2 different objects with the same function (train=True selects the train set and train=False selects the test set).\nWe will transform the raw data to tensors and normalize them using the mean and standard deviation of the MNIST training set: 0.1307 and 0.3081 respectively (even though the mean and standard deviation of the test data are slightly different, it is important to normalize the test data with the values of the training data to apply the same treatment to both sets).\nSo we first need to define a transformation:\n\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.1307,), (0.3081,))\n])"
  },
  {
    "objectID": "ml/mnist.html#we-can-now-create-our-data-objects",
    "href": "ml/mnist.html#we-can-now-create-our-data-objects",
    "title": "Classifying the MNIST dataset",
    "section": "We can now create our data objects",
    "text": "We can now create our data objects\n\nTraining data\n\nRemember that train=True selects the training set of the MNIST.\n\n\ntrain_data = datasets.MNIST(\n    '~/projects/def-sponsor00/data',\n    train=True, download=True, transform=transform)\n\n\n\nTest data\n\ntrain=False selects the test set.\n\n\ntest_data = datasets.MNIST(\n    '~/projects/def-sponsor00/data',\n    train=False, transform=transform)"
  },
  {
    "objectID": "ml/mnist.html#data-inspection",
    "href": "ml/mnist.html#data-inspection",
    "title": "Classifying the MNIST dataset",
    "section": "Data inspection",
    "text": "Data inspection\nFirst, let‚Äôs check the size of train_data:\n\nprint(len(train_data))\n\n60000\n\n\nThat makes sense since the MNIST‚Äôs training set has 60,000 pairs. train_data has 60,000 elements and we should expect each element to be of size 2 since it is a pair. Let‚Äôs double-check with the first element:\n\nprint(len(train_data[0]))\n\n2\n\n\nSo far, so good. We can print that first pair:\n\nprint(train_data[0])\n\n(tensor([[[-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.3860, -0.1951,\n          -0.1951, -0.1951,  1.1795,  1.3068,  1.8032, -0.0933,  1.6887,\n           2.8215,  2.7197,  1.1923, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.0424,  0.0340,  0.7722,  1.5359,  1.7396,  2.7960,\n           2.7960,  2.7960,  2.7960,  2.7960,  2.4396,  1.7650,  2.7960,\n           2.6560,  2.0578,  0.3904, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n           0.1995,  2.6051,  2.7960,  2.7960,  2.7960,  2.7960,  2.7960,\n           2.7960,  2.7960,  2.7960,  2.7706,  0.7595,  0.6195,  0.6195,\n           0.2886,  0.0722, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.1951,  2.3633,  2.7960,  2.7960,  2.7960,  2.7960,  2.7960,\n           2.0960,  1.8923,  2.7197,  2.6433, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242,  0.5940,  1.5614,  0.9377,  2.7960,  2.7960,  2.1851,\n          -0.2842, -0.4242,  0.1231,  1.5359, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.2460, -0.4115,  1.5359,  2.7960,  0.7213,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242,  1.3450,  2.7960,  1.9942,\n          -0.3988, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.2842,  1.9942,  2.7960,\n           0.4668, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.0213,  2.6433,\n           2.4396,  1.6123,  0.9504, -0.4115, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.6068,\n           2.6306,  2.7960,  2.7960,  1.0904, -0.1060, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n           0.1486,  1.9432,  2.7960,  2.7960,  1.4850, -0.0806, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.2206,  0.7595,  2.7833,  2.7960,  1.9560, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242,  2.7451,  2.7960,  2.7451,  0.3904,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n           0.1613,  1.2305,  1.9051,  2.7960,  2.7960,  2.2105, -0.3988,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.0722,  1.4596,\n           2.4906,  2.7960,  2.7960,  2.7960,  2.7578,  1.8923, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.1187,  1.0268,  2.3887,  2.7960,\n           2.7960,  2.7960,  2.7960,  2.1342,  0.5686, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.1315,  0.4159,  2.2869,  2.7960,  2.7960,  2.7960,\n           2.7960,  2.0960,  0.6068, -0.3988, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.1951,\n           1.7523,  2.3633,  2.7960,  2.7960,  2.7960,  2.7960,  2.0578,\n           0.5940, -0.3097, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242,  0.2758,  1.7650,  2.4524,\n           2.7960,  2.7960,  2.7960,  2.7960,  2.6815,  1.2686, -0.2842,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242,  1.3068,  2.7960,  2.7960,\n           2.7960,  2.2742,  1.2941,  1.2559, -0.2206, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242]]]), 5)\n\n\nAnd you can see that it is a tuple with:\n\nprint(type(train_data[0]))\n\n<class 'tuple'>\n\n\nWhat is that tuple made of?\n\nprint(type(train_data[0][0]))\nprint(type(train_data[0][1]))\n\n<class 'torch.Tensor'>\n<class 'int'>\n\n\nIt is made of the tensor for the first image (remember that we transformed the images into tensors when we created the objects train_data and test_data) and the integer of the first label (which you can see is 5 when you print train_data[0][1]).\nSo since train_data[0][0] is the tensor representing the image of the first pair, let‚Äôs check its size:\n\nprint(train_data[0][0].size())\n\ntorch.Size([1, 28, 28])\n\n\nThat makes sense: a color image would have 3 layers of RGB values (so the size in the first dimension would be 3), but because the MNIST has black and white images, there is a single layer of values‚Äîthe values of each pixel on a gray scale‚Äîso the first dimension has a size of 1. The 2nd and 3rd dimensions correspond to the width and length of the image in pixels, hence 28 and 28.\n\n\nYour turn:\n\nRun the following:\nprint(train_data[0][0][0])\nprint(train_data[0][0][0][0])\nprint(train_data[0][0][0][0][0])\nAnd think about what each of them represents.\nThen explore the test_data object."
  },
  {
    "objectID": "ml/mnist.html#plotting-an-image-from-the-data",
    "href": "ml/mnist.html#plotting-an-image-from-the-data",
    "title": "Classifying the MNIST dataset",
    "section": "Plotting an image from the data",
    "text": "Plotting an image from the data\nFor this, we will use pyplot from matplotlib.\nFirst, we select the image of the first pair and we resize it from 3 to 2 dimensions by removing its dimension of size 1 with torch.squeeze:\nimg = torch.squeeze(train_data[0][0])\nThen, we plot it with pyplot, but since we are in a cluster, instead of showing it to screen with plt.show(), we save it to file:\nplt.imshow(img, cmap='gray')\nThis is what that first image looks like:\n\nAnd indeed, it matches the first label we explored earlier (train_data[0][1])."
  },
  {
    "objectID": "ml/mnist.html#plotting-an-image-with-its-pixel-values",
    "href": "ml/mnist.html#plotting-an-image-with-its-pixel-values",
    "title": "Classifying the MNIST dataset",
    "section": "Plotting an image with its pixel values",
    "text": "Plotting an image with its pixel values\nWe can plot it with more details by showing the value of each pixel in the image. One little twist is that we need to pick a threshold value below which we print the pixel values in white otherwise they would not be visible (black on near black background). We also round the pixel values to one decimal digit so as not to clutter the result.\nimgplot = plt.figure(figsize = (12, 12))\nsub = imgplot.add_subplot(111)\nsub.imshow(img, cmap='gray')\nwidth, height = img.shape\nthresh = img.max() / 2.5\nfor x in range(width):\n    for y in range(height):\n        val = round(img[x][y].item(), 1)\n        sub.annotate(str(val), xy=(y, x),\n                     horizontalalignment='center',\n                     verticalalignment='center',\n                     color='white' if img[x][y].item() < thresh else 'black')"
  },
  {
    "objectID": "ml/mnist.html#create-dataloaders",
    "href": "ml/mnist.html#create-dataloaders",
    "title": "Classifying the MNIST dataset",
    "section": "Create DataLoaders",
    "text": "Create DataLoaders\n\nTraining data\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=20, shuffle=True)\n\n\nTest data\ntest_loader = torch.utils.data.DataLoader(\n    test_data, batch_size=20, shuffle=False)"
  },
  {
    "objectID": "ml/mnist.html#plot-a-full-batch-of-images-with-their-labels",
    "href": "ml/mnist.html#plot-a-full-batch-of-images-with-their-labels",
    "title": "Classifying the MNIST dataset",
    "section": "Plot a full batch of images with their labels",
    "text": "Plot a full batch of images with their labels\nNow that we have passed our data through DataLoader, it is easy to select one batch from it. Let‚Äôs plot an entire batch of images with their labels.\nFirst, we need to get one batch of training images and their labels:\ndataiter = iter(train_loader)\nbatchimg, batchlabel = dataiter.next()\nThen, we can plot them:\nbatchplot = plt.figure(figsize=(20, 5))\nfor i in torch.arange(20):\n    sub = batchplot.add_subplot(2, 10, i+1, xticks=[], yticks=[])\n    sub.imshow(torch.squeeze(batchimg[i]), cmap='gray')\n    sub.set_title(str(batchlabel[i].item()), fontsize=25)"
  },
  {
    "objectID": "ml/mnist.html#load-packages",
    "href": "ml/mnist.html#load-packages",
    "title": "Classifying the MNIST dataset",
    "section": "Load packages",
    "text": "Load packages\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torchvision import datasets, transforms\nfrom torch.optim.lr_scheduler import StepLR\nThe torch.nn.functional module contains all the functions of the torch.nn package.\nThese functions include loss functions, activation functions, pooling functions‚Ä¶"
  },
  {
    "objectID": "ml/mnist.html#create-a-summarywriter-instance-for-tensorboard",
    "href": "ml/mnist.html#create-a-summarywriter-instance-for-tensorboard",
    "title": "Classifying the MNIST dataset",
    "section": "Create a SummaryWriter instance for TensorBoard",
    "text": "Create a SummaryWriter instance for TensorBoard\nwriter = SummaryWriter()"
  },
  {
    "objectID": "ml/mnist.html#define-the-architecture-of-the-network",
    "href": "ml/mnist.html#define-the-architecture-of-the-network",
    "title": "Classifying the MNIST dataset",
    "section": "Define the architecture of the network",
    "text": "Define the architecture of the network\n# To build a model, create a subclass of torch.nn.Module:\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.fc1 = nn.Linear(784, 128)\n        self.fc2 = nn.Linear(128, 10)\n\n    # Method for the forward pass:\n    def forward(self, x):\n        x = torch.flatten(x, 1)\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.fc2(x)\n        output = F.log_softmax(x, dim=1)\n        return output\nPython‚Äôs class inheritance gives our subclass all the functionality of torch.nn.Module while allowing us to customize it."
  },
  {
    "objectID": "ml/mnist.html#define-a-training-function",
    "href": "ml/mnist.html#define-a-training-function",
    "title": "Classifying the MNIST dataset",
    "section": "Define a training function",
    "text": "Define a training function\ndef train(model, device, train_loader, optimizer, epoch):\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n        optimizer.zero_grad()  # reset the gradients to 0\n        output = model(data)\n        loss = F.nll_loss(output, target)  # negative log likelihood\n        writer.add_scalar(\"Loss/train\", loss, epoch)\n        loss.backward()\n        optimizer.step()\n        if batch_idx % 10 == 0:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(data), len(train_loader.dataset),\n                100. * batch_idx / len(train_loader), loss.item()))"
  },
  {
    "objectID": "ml/mnist.html#define-a-testing-function",
    "href": "ml/mnist.html#define-a-testing-function",
    "title": "Classifying the MNIST dataset",
    "section": "Define a testing function",
    "text": "Define a testing function\ndef test(model, device, test_loader):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            # Sum up batch loss:\n            test_loss += F.nll_loss(output, target, reduction='sum').item()\n            # Get the index of the max log-probability:\n            pred = output.argmax(dim=1, keepdim=True)\n            correct += pred.eq(target.view_as(pred)).sum().item()\n\n    test_loss /= len(test_loader.dataset)\n\n    # Print a summary\n    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n        test_loss, correct, len(test_loader.dataset),\n        100. * correct / len(test_loader.dataset)))"
  },
  {
    "objectID": "ml/mnist.html#define-a-function-main-which-runs-our-network",
    "href": "ml/mnist.html#define-a-function-main-which-runs-our-network",
    "title": "Classifying the MNIST dataset",
    "section": "Define a function main() which runs our network",
    "text": "Define a function main() which runs our network\ndef main():\n    epochs = 1\n    torch.manual_seed(1)\n    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n\n    transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,))\n    ])\n\n    train_data = datasets.MNIST(\n        '~/projects/def-sponsor00/data',\n        train=True, download=True, transform=transform)\n\n    test_data = datasets.MNIST(\n        '~/projects/def-sponsor00/data',\n        train=False, transform=transform)\n\n    train_loader = torch.utils.data.DataLoader(train_data, batch_size=64)\n    test_loader = torch.utils.data.DataLoader(test_data, batch_size=1000)\n    model = Net().to(device)  # create instance of our network and send it to device\n    optimizer = optim.Adadelta(model.parameters(), lr=1.0)\n    scheduler = StepLR(optimizer, step_size=1, gamma=0.7)\n\n    for epoch in range(1, epochs + 1):\n        train(model, device, train_loader, optimizer, epoch)\n        test(model, device, test_loader)\n        scheduler.step()"
  },
  {
    "objectID": "ml/mnist.html#run-the-network",
    "href": "ml/mnist.html#run-the-network",
    "title": "Classifying the MNIST dataset",
    "section": "Run the network",
    "text": "Run the network\nmain()"
  },
  {
    "objectID": "ml/mnist.html#write-pending-events-to-disk-and-close-the-tensorboard",
    "href": "ml/mnist.html#write-pending-events-to-disk-and-close-the-tensorboard",
    "title": "Classifying the MNIST dataset",
    "section": "Write pending events to disk and close the TensorBoard",
    "text": "Write pending events to disk and close the TensorBoard\nwriter.flush()\nwriter.close()\nThe code is working. Time to actually train our model!\nJupyter is a fantastic tool. It has a major downside however: when you launch a Jupyter server, you are running a job on a compute node. If you want to play for 8 hours in Jupyter, you are requesting an 8 hour job. Now, most of the time you spend on Jupyter is spent typing, running bits and pieces of code, or doing nothing at all. If you ask for GPUs, many CPUs, and lots of RAM, all of it will remain idle almost all of the time. It is a really suboptimal use of the Alliance resources.\nIn addition, if you ask for lots of resources for a long time, you will have to wait a long time in the queue before they get allocated to you.\nLastly, you will go through your allocation quickly.\nA much better strategy is to develop and test your code (with very little data, few epochs, etc.) in an interactive job (with salloc) or in Jupyter, then, launch an sbatch job to actually train your model. This ensures that heavy duty resources such as GPU(s) are only allocated to you when you are actually needing and using them.\n\nConcrete example with our training cluster: this cluster only has 1 GPU. If you want to use it in Jupyter, you have to request it for your Jupyter session. This means that the entire time your Jupyter session is active, nobody else can use that GPU. While you let your session idle or do tasks that do not require a GPU, this is not a good use of resources."
  },
  {
    "objectID": "ml/mnist.html#log-in-the-training-cluster",
    "href": "ml/mnist.html#log-in-the-training-cluster",
    "title": "Classifying the MNIST dataset",
    "section": "Log in the training cluster",
    "text": "Log in the training cluster\nOpen a terminal and SSH to our training cluster as we saw in the first lesson."
  },
  {
    "objectID": "ml/mnist.html#load-necessary-modules",
    "href": "ml/mnist.html#load-necessary-modules",
    "title": "Classifying the MNIST dataset",
    "section": "Load necessary modules",
    "text": "Load necessary modules\nFirst, we need to load the Python and CUDA modules. This is done with the Lmod tool through the module command. Here are some key Lmod commands:\n# Get help on the module command\n$ module help\n\n# List modules that are already loaded\n$ module list\n\n# See which modules are available for a tool\n$ module avail <tool>\n\n# Load a module\n$ module load <module>[/<version>]\nHere are the modules we need:\n$ module load nixpkgs/16.09 gcc/7.3.0 cuda/10.0.130 cudnn/7.6 python/3.8.2"
  },
  {
    "objectID": "ml/mnist.html#install-python-packages",
    "href": "ml/mnist.html#install-python-packages",
    "title": "Classifying the MNIST dataset",
    "section": "Install Python packages",
    "text": "Install Python packages\nYou also need the Python packages matplotlib, torch, torchvision, and tensorboard.\nOn the Alliance clusters, you need to create a virtual environment in which you install packages with pip.\n\nDo not use Anaconda.\nWhile Anaconda is a great tool on personal computers, it is not an appropriate tool when working on the Alliance clusters: binaries are unoptimized for those clusters and library paths are inconsistent with their architecture. Anaconda installs packages in $HOME where it creates a very large number of small files. It can also create conflicts by modifying .bashrc.\n\nFor this workshop, since we all need the same packages, I already created a virtual environment that we will all use. All you have to do is to activate it with:\n$ source ~/projects/def-sponsor00/env/bin/activate\nIf you want to exit the virtual environment, you can press Ctrl-D or run:\n(env) $ deactivate\n\nFor future reference, below is how you would install packages on a real Alliance cluster (but please don‚Äôt do it in the training cluster as it is unnecessary and would only slow it down).\nCreate a virtual environment:\n$ virtualenv --no-download ~/env\nActivate the virtual environment:\n$ source ~/env/bin/activate\nUpdate pip:\n(env) $ pip install --no-index --upgrade pip\nInstall the packages you need in the virtual environment:\n(env) $ pip install --no-cache-dir --no-index matplotlib torch torchvision tensorboard"
  },
  {
    "objectID": "ml/mnist.html#write-a-python-script",
    "href": "ml/mnist.html#write-a-python-script",
    "title": "Classifying the MNIST dataset",
    "section": "Write a Python script",
    "text": "Write a Python script\nCreate a directory for this project and cd into it:\nmkdir mnist\ncd mnist\nStart a Python script with the text editor of your choice:\nnano nn.py\nIn it, copy-paste the code we played with in Jupyter, but this time have it run for 10 epochs:\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torchvision import datasets, transforms\nfrom torch.optim.lr_scheduler import StepLR\n\nwriter = SummaryWriter()\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.fc1 = nn.Linear(784, 128)\n        self.fc2 = nn.Linear(128, 10)\n\n    def forward(self, x):\n        x = torch.flatten(x, 1)\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.fc2(x)\n        output = F.log_softmax(x, dim=1)\n        return output\n\ndef train(model, device, train_loader, optimizer, epoch):\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = F.nll_loss(output, target)\n        writer.add_scalar(\"Loss/train\", loss, epoch)\n        loss.backward()\n        optimizer.step()\n        if batch_idx % 10 == 0:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(data), len(train_loader.dataset),\n                100. * batch_idx / len(train_loader), loss.item()))\n\ndef test(model, device, test_loader):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            test_loss += F.nll_loss(output, target, reduction='sum').item()\n            pred = output.argmax(dim=1, keepdim=True)\n            correct += pred.eq(target.view_as(pred)).sum().item()\n\n    test_loss /= len(test_loader.dataset)\n\n    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n        test_loss, correct, len(test_loader.dataset),\n        100. * correct / len(test_loader.dataset)))\n\ndef main():\n    epochs = 10  # don't forget to change the number of epochs\n    torch.manual_seed(1)\n    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n\n    transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,))\n    ])\n\n    train_data = datasets.MNIST(\n        '~/projects/def-sponsor00/data',\n        train=True, download=True, transform=transform)\n\n    test_data = datasets.MNIST(\n        '~/projects/def-sponsor00/data',\n        train=False, transform=transform)\n\n    train_loader = torch.utils.data.DataLoader(train_data, batch_size=64)\n    test_loader = torch.utils.data.DataLoader(test_data, batch_size=1000)\n    model = Net().to(device)\n    optimizer = optim.Adadelta(model.parameters(), lr=1.0)\n    scheduler = StepLR(optimizer, step_size=1, gamma=0.7)\n\n    for epoch in range(1, epochs + 1):\n        train(model, device, train_loader, optimizer, epoch)\n        test(model, device, test_loader)\n        scheduler.step()\n\nmain()\n\nwriter.flush()\nwriter.close()"
  },
  {
    "objectID": "ml/mnist.html#write-a-slurm-script",
    "href": "ml/mnist.html#write-a-slurm-script",
    "title": "Classifying the MNIST dataset",
    "section": "Write a Slurm script",
    "text": "Write a Slurm script\nWrite a shell script with the text editor of your choice:\nnano nn.sh\nThis is what you want in that script:\n#!/bin/bash\n#SBATCH --time=5:0\n#SBATCH --cpus-per-task=1\n#SBATCH --gres=gpu:1\n#SBATCH --mem=4G\n#SBATCH --output=%x_%j.out\n#SBATCH --error=%x_%j.err\n\npython ~/mnist/nn.py\n\n--time accepts these formats: ‚Äúmin‚Äù, ‚Äúmin:s‚Äù, ‚Äúh:min:s‚Äù, ‚Äúd-h‚Äù, ‚Äúd-h:min‚Äù & ‚Äúd-h:min:s‚Äù\n%x will get replaced by the script name & %j by the job number"
  },
  {
    "objectID": "ml/mnist.html#submit-a-job",
    "href": "ml/mnist.html#submit-a-job",
    "title": "Classifying the MNIST dataset",
    "section": "Submit a job",
    "text": "Submit a job\nFinally, you need to submit your job to Slurm:\n$ sbatch ~/mnist/nn.sh\nYou can check the status of your job with:\n$ sq\n\nPD = pending\nR = running\nCG = completing (Slurm is doing the closing processes)\nNo information = your job has finished running\n\nYou can cancel it with:\n$ scancel <jobid>\nOnce your job has finished running, you can display efficiency measures with:\n$ seff <jobid>"
  },
  {
    "objectID": "ml/mnist.html#launch-tensorboard",
    "href": "ml/mnist.html#launch-tensorboard",
    "title": "Classifying the MNIST dataset",
    "section": "Launch TensorBoard",
    "text": "Launch TensorBoard\nTensorBoard requires too much processing power to be run on the login node. When you run long jobs, the best strategy is to launch it in the background as part of the job. This allows you to monitor your model as it is running (and cancel it if things don‚Äôt look right).\n\nExample:\n\n#!/bin/bash\n#SBATCH ...\n#SBATCH ...\n\ntensorboard --logdir=runs --host 0.0.0.0 &\npython ~/mnist/nn.py\nBecause we only have 1 GPU and are taking turns running our jobs, we need to keep our jobs very short here. So we will launch a separate job for TensorBoard. This time, we will launch an interactive job:\nsalloc --time=1:0:0 --mem=2000M\nTo launch TensorBoard, we need to activate our Python virtual environment (TensorBoard was installed by pip):\nsource ~/projects/def-sponsor00/env/bin/activate\nThen we can launch TensorBoard in the background:\ntensorboard --logdir=~/mnist/runs --host 0.0.0.0 &\nNow, we need to create a connection with SSH tunnelling between your computer and the compute note running your TensorBoard job."
  },
  {
    "objectID": "ml/mnist.html#connect-to-tensorboard-from-your-computer",
    "href": "ml/mnist.html#connect-to-tensorboard-from-your-computer",
    "title": "Classifying the MNIST dataset",
    "section": "Connect to TensorBoard from your computer",
    "text": "Connect to TensorBoard from your computer\nFrom a new terminal on your computer, run:\nssh -NfL localhost:6006:<hostname>:6006 userxxx@uu.c3.ca\n\nReplace <hostname> by the name of the compute node running your salloc job. You can find it by looking at your prompt (your prompt shows <username>@<hostname>).\nReplace <userxxx> by your user name.\n\nNow, you can open a browser on your computer and access TensorBoard at http://localhost:6006."
  },
  {
    "objectID": "ml/nn.html",
    "href": "ml/nn.html",
    "title": "Concepts:",
    "section": "",
    "text": "Artificial intelligence is a vast field: any system mimicking animal intelligence falls in its scope.\nMachine learning (ML) is a subfield of artificial intelligence that can be defined as computer programs whose performance at a task improves with experience.\nSince this experience comes in the form of data, ML consists of feeding vast amounts of data to algorithms to strengthen pathways.\n\n\nFrom xkcd.com\n\n\n\nCoding all the possible ways‚Äîpixel by pixel‚Äîthat a picture can represent a certain object is an impossibly large task. By feeding examples of images of that object to a neural network however, we can train it to recognize that object in images that it has never seen (without explicitly programming how it does this!).\n\n\nFrom xkcd.com"
  },
  {
    "objectID": "ml/nn.html#supervised-learning",
    "href": "ml/nn.html#supervised-learning",
    "title": "Concepts:",
    "section": "Supervised learning",
    "text": "Supervised learning\nYou have been doing supervised machine learning for years without looking at it in the framework of machine learning:\n\nRegression is a form of supervised learning with continuous outputs\nClassification is supervised learning with discrete outputs\n\nSupervised learning uses training data in the form of example input/output \\((x_i, y_i)\\) pairs.\nGoal:\nIf \\(X\\) is the space of inputs and \\(Y\\) the space of outputs, the goal is to find a function \\(h\\) so that\nfor each \\(x_i \\in X\\):\n\n\\(h_\\theta(x_i)\\) is a predictor for the corresponding value \\(y_i\\)\n\n(\\(\\theta\\) represents the set of parameters of \\(h_\\theta\\)).\n\n‚Üí i.e.¬†we want to find the relationship between inputs and outputs."
  },
  {
    "objectID": "ml/nn.html#unsupervised-learning",
    "href": "ml/nn.html#unsupervised-learning",
    "title": "Concepts:",
    "section": "Unsupervised learning",
    "text": "Unsupervised learning\nHere too, you are familiar with some forms of unsupervised learning that you weren‚Äôt thinking about in such terms:\nClustering, social network analysis, market segmentation, PCA ‚Ä¶ are all forms of unsupervised learning.\nUnsupervised learning uses unlabelled data (training set of \\(x_i\\)).\nGoal:\nFind structure within the data.\n\n\nFrom xkcd.com"
  },
  {
    "objectID": "ml/nn.html#types-of-ann",
    "href": "ml/nn.html#types-of-ann",
    "title": "Concepts:",
    "section": "Types of ANN",
    "text": "Types of ANN\n\nFully connected neural networks\n\n\nFrom Glosser.ca, Wikipedia\n\nEach neuron receives inputs from every neuron of the previous layer and passes its output to every neuron of the next layer.\n\n\nConvolutional neural networks\n\n\nFrom Programming Journeys by Rensu Theart\n\nConvolutional neural networks (CNN) are used for spatially structured data (e.g.¬†in image recognition).\nImages have huge input sizes and would require a very large number of neurons in a fully connected neural net. In convolutional layers, neurons receive input from a subarea (called local receptive field) of the previous layer. This greatly reduces the number of parameters.\nOptionally, pooling (combining the outputs of neurons in a subarea) reduces the data dimensions. The stride then dictates how the subarea is moved across the image. Max-pooling is one of the forms of pooling which uses the maximum for each subarea.\n\n\nRecurrent neural networks\n\n\nFrom fdeloche, Wikipedia\n\nRecurrent neural networks (RNN) such as Long Short-Term Memory (LSTM) are used for chain structured data (e.g.¬†in speech recognition).\nThey are not feedforward networks (i.e.¬†networks for which the information moves only in the forward direction without any loop).\n\n\nTransformers\nA combination of two RNNs or sets of RNNs (the encoder and the decoder) is used in sequence to sequence models for translation or picture captioning. Such models were slow to process a lot of data.\nIn 2014 and 2015, the concept of attention (giving added weight to important words) was developed, greatly improving the ability of such models to process a lot of data.\nThis blog post by Jay Alammar‚Äîa blogger whose high-quality posts have been referenced in MIT and Stanford courses‚Äîexplains this in a high-level visual fashion.\nThe problem with recurrence is that it is not easily to parallelize (and thus to run fast on GPUs).\nIn 2017, a new model‚Äîthe transformer‚Äîwas proposed: by using only attention mechanisms and no recurrence, the transformer achieves better results in an easily parallelizable fashion.\nJay Alammar has also a blog post on the transformer. The post includes a 30 min video.\nWith the addition of transfer learning, powerful transformers emerged in the field of NLP (Natural Language Processing). Examples include BERT (Bidirectional Encoder Representations from Transformers) from Google and GPT-3 (Generative Pre-trained Transformer-3) from OpenAI.\nJay Alammar has yet another great blog post on these advanced NLP models."
  },
  {
    "objectID": "ml/nn.html#deep-learning",
    "href": "ml/nn.html#deep-learning",
    "title": "Concepts:",
    "section": "Deep learning",
    "text": "Deep learning\nThe first layer of a neural net is the input layer. The last one is the output layer. All the layers in-between are called hidden layers. Shallow neural networks have only one hidden layer and deep networks have two or more hidden layers. When an ANN is deep, we talk about Deep Learning (DL).\n\n\nFrom xkcd.com"
  },
  {
    "objectID": "ml/nn_building.html",
    "href": "ml/nn_building.html",
    "title": "Building a neural network",
    "section": "",
    "text": "Key to creating neural networks in PyTorch is the torch.nn package which contains the nn.Module and a forward method which returns an output from some input.\nLet‚Äôs build a neural network to classify the MNIST.\n\n\nLoad packages\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\n\nDefine the architecture of the network\nFirst, we need to define the architecture of the network.\nThere are many types of architectures. For images, CNN are well suited.\nIn Python, you can define a subclass of an existing class with:\nclass YourSubclass(BaseClass):\n    <definition of your subclass>        \nYour subclass is derived from the base class and inherits its properties.\nPyTorch contains the class torch.nn.Module which is used as the base class when defining a neural network.\n\nclass Net(nn.Module):\n    def __init__(self):\n      super(Net, self).__init__()\n      \n      # First 2D convolutional layer, taking in 1 input channel (image),\n      # outputting 32 convolutional features.\n      # Convolution adds each pixel of an image to its neighbours,\n      # weighted by the kernel (a small matrix).\n      # Here, the kernel is square and of size 3*3\n      # Convolution helps to extract features from the input\n      # (e.g. edge detection, blurriness, sharpeness...)\n      self.conv1 = nn.Conv2d(1, 32, 3)\n      # Second 2D convolutional layer, taking in the 32 input channels,\n      # outputting 64 convolutional features, with a kernel size of 3*3\n      self.conv2 = nn.Conv2d(32, 64, 3)\n\n      # Dropouts randomly blocks a fraction of the neurons during training\n      # This is a regularization technique which prevents overfitting\n      self.dropout1 = nn.Dropout2d(0.25)\n      self.dropout2 = nn.Dropout2d(0.5)\n\n      # First fully connected layer\n      self.fc1 = nn.Linear(9216, 128)\n      # Second fully connected layer that outputs our 10 labels\n      self.fc2 = nn.Linear(128, 10)\n\n\n\nSet the flow of data through the network\nThe feed-forward algorithm is defined by the forward function.\n\nclass Net(nn.Module):\n    def __init__(self):\n      super(Net, self).__init__()\n      self.conv1 = nn.Conv2d(1, 32, 3)\n      self.conv2 = nn.Conv2d(32, 64, 3)\n      self.dropout1 = nn.Dropout2d(0.25)\n      self.dropout2 = nn.Dropout2d(0.5)\n      self.fc1 = nn.Linear(9216, 128)\n      self.fc2 = nn.Linear(128, 10)\n\n    # x represents the data\n    def forward(self, x):\n      # Pass data through conv1\n      x = self.conv1(x)\n      # Use the rectified-linear activation function over x\n      x = F.relu(x)\n\n      x = self.conv2(x)\n      x = F.relu(x)\n\n      # Run max pooling over x\n      x = F.max_pool2d(x, 2)\n      # Pass data through dropout1\n      x = self.dropout1(x)\n      # Flatten x with start_dim=1\n      x = torch.flatten(x, 1)\n      # Pass data through fc1\n      x = self.fc1(x)\n      x = F.relu(x)\n      x = self.dropout2(x)\n      x = self.fc2(x)\n\n      # Apply softmax to x\n      output = F.log_softmax(x, dim=1)\n      return output\n\nLet‚Äôs create an instance of Net and print its structure:\n\nnet = Net()\nprint(net)\n\nNet(\n  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n  (dropout1): Dropout2d(p=0.25, inplace=False)\n  (dropout2): Dropout2d(p=0.5, inplace=False)\n  (fc1): Linear(in_features=9216, out_features=128, bias=True)\n  (fc2): Linear(in_features=128, out_features=10, bias=True)\n)"
  },
  {
    "objectID": "ml/nn_training.html",
    "href": "ml/nn_training.html",
    "title": "Training a model",
    "section": "",
    "text": "Prerequisites\nBefore we can train a model, we need to:\n\nload the needed packages,\nget the data,\ncreate data loaders for training and testing,\ndefine a model.\n\nLet‚Äôs do this for the FashionMNIST dataset:\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets\nfrom torchvision.transforms import ToTensor, Lambda\n\ntraining_data = datasets.FashionMNIST(\n    root=\"~/projects/def-sponsor00/data/\",\n    train=True,\n    download=True,\n    transform=ToTensor(),\n    target_transform=Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n)\n\ntest_data = datasets.FashionMNIST(\n    root=\"~/projects/def-sponsor00/data/\",\n    train=False,\n    download=True,\n    transform=ToTensor(),\n    target_transform=Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n)\n\ntrain_dataloader = DataLoader(training_data, batch_size=10)\ntest_dataloader = DataLoader(test_data, batch_size=10)\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.flatten = nn.Flatten()\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(28*28, 512),\n            nn.ReLU(),\n            nn.Linear(512, 512),\n            nn.ReLU(),\n            nn.Linear(512, 10),\n        )\n\n    def forward(self, x):\n        x = self.flatten(x)\n        logits = self.linear_relu_stack(x)\n        return logits\n\nmodel = Net()\n\n\nHyperparameters\nWhile the learning parameters of a model (weights and biases) are the values that get adjusted through training (and they will become part of the final program, along with the model architecture, once training is over), hyperparameters control the training process.\nThey include:\n\nbatch size: number of samples passed through the model before the parameters are updated,\nnumber of epochs: number iterations,\nlearning rate: size of the incremental changes to model parameters at each iteration. Smaller values yield slow learning speed, while large values may miss minima.\n\nLet‚Äôs define them here:\nlearning_rate = 1e-3\nbatch_size = 64\nepochs = 5\n\n\nDefine the loss function\nTo assess the predicted outputs of our model against the true values from the labels, we also need a loss function (e.g.¬†mean square error for regressions: nn.MSELoss or negative log likelihood for classification: nn.NLLLoss)\nThe machine learning literature is rich in information about various loss functions.\nHere is an example with nn.CrossEntropyLoss which combines nn.LogSoftmax and nn.NLLLoss:\nloss_fn = nn.CrossEntropyLoss()\n\n\nInitialize the optimizer\nThe optimization algorithm determines how the model parameters get adjusted at each iteration.\nThere are many optimizers and you need to search in the literature which one performs best for your time of model and data.\nBelow is an example with stochastic gradient descent:\noptimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n\n\nlr is the learning rate\nmomentum is a method increasing convergence rate and reducing oscillation for SDG\n\n\n\n\nDefine the train and test loops\nFinally, we need to define the train and test loops.\nThe train loop:\n\ngets a batch of training data from the DataLoader,\nresets the gradients of model parameters with optimizer.zero_grad(),\ncalculates predictions from the model for an input batch,\ncalculates the loss for that set of predictions vs.¬†the labels on the dataset,\ncalculates the backward gradients over the learning parameters (that‚Äôs the backpropagation) with loss.backward(),\nadjusts the parameters by the gradients collected in the backward pass with optimizer.step().\n\nThe test loop evaluates the model‚Äôs performance against the test data.\ndef train(dataloader, model, loss_fn, optimizer):\n    size = len(dataloader.dataset)\n    for batch, (X, y) in enumerate(dataloader):\n        # Compute prediction and loss\n        pred = model(X)\n        loss = loss_fn(pred, y)\n\n        # Backpropagation\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if batch % 100 == 0:\n            loss, current = loss.item(), batch * len(X)\n            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n\n\ndef test(dataloader, model, loss_fn):\n    size = len(dataloader.dataset)\n    num_batches = len(dataloader)\n    test_loss, correct = 0, 0\n\n    with torch.no_grad():\n        for X, y in dataloader:\n            pred = model(X)\n            test_loss += loss_fn(pred, y).item()\n            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n\n    test_loss /= num_batches\n    correct /= size\n    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n\n\nTrain\nTo train our model, we just run the loop over the epochs:\nfor t in range(epochs):\n    print(f\"Epoch {t+1}\\n-------------------------------\")\n    train(train_dataloader, model, loss_fn, optimizer)\n    test(test_dataloader, model, loss_fn)\nprint(\"Training completed\")"
  },
  {
    "objectID": "ml/pretrained_models.html",
    "href": "ml/pretrained_models.html",
    "title": "Finding pretrained models for transfer learning",
    "section": "",
    "text": "Training models from scratch requires way too much data, time, and computing power (or money) to be a practical option. This is why transfer learning has become such a common practice: by starting with models trained on related problems, you are saving time and achieving good results with little data.\nNow, where do you find such models?\nIn this workshop, we will have a look at some of the most popular pre-trained models repositories and libraries (Model Zoo, PyTorch Hub, and Hugging Face); see how you can also search models in the literature and on GitHub; and finally learn how to import models into PyTorch.\n\n\nPrerequisites:\nIf you want to follow the hands-on part of this workshop, please make sure to have an up-to-date version of PyTorch on your laptop."
  },
  {
    "objectID": "ml/resources.html",
    "href": "ml/resources.html",
    "title": "Resources",
    "section": "",
    "text": "Arxiv Sanity Preserver by Andrej Karpathy\nML papers in the computer science category on arXiv\nML papers in the stats category on arXiv\nDistill ML research online journal\n\n\n\nAdvice and sources from ML research student\n\n\n\nStack Overflow [machine-learning] tag\nStack Overflow [deep-learning] tag\nStack Overflow [supervised-learning] tag\nStack Overflow [unsupervised-learning] tag\nStack Overflow [semisupervised-learning] tag\nStack Overflow [reinforcement-learning] tag\nStack Overflow [transfer-learning] tag\nStack Overflow [machine-learning-model] tag\nStack Overflow [learning-rate] tag\nStack Overflow [bayesian-deep-learning] tag\n\n\n\ndeeplearning.ai\nfast.ai\nGoogle\n\n\n\nbenchmarks.ai\nAIBench\nkaggle\nWikipedia"
  },
  {
    "objectID": "ml/resources.html#documentation",
    "href": "ml/resources.html#documentation",
    "title": "Resources",
    "section": "Documentation",
    "text": "Documentation\nPyTorch website\nPyTorch documentation\nPyTorch tutorials\nPyTorch online courses\nPyTorch examples"
  },
  {
    "objectID": "ml/resources.html#getting-help-1",
    "href": "ml/resources.html#getting-help-1",
    "title": "Resources",
    "section": "Getting help",
    "text": "Getting help\nPyTorch Discourse forum\nStack Overflow [pytorch] tag\nStack Overflow [pytorch-dataloader] tag\nStack Overflow [pytorch-ignite] tag"
  },
  {
    "objectID": "ml/resources.html#pre-trained-models",
    "href": "ml/resources.html#pre-trained-models",
    "title": "Resources",
    "section": "Pre-trained models",
    "text": "Pre-trained models\nPyTorch Hub"
  },
  {
    "objectID": "ml/resources.html#ide",
    "href": "ml/resources.html#ide",
    "title": "Resources",
    "section": "IDE",
    "text": "IDE\nProject Jupyter\nList of IDEs with description\nComparison of IDEs\nEmacs Python IDE"
  },
  {
    "objectID": "ml/resources.html#shell",
    "href": "ml/resources.html#shell",
    "title": "Resources",
    "section": "Shell",
    "text": "Shell\nIPython\nbpython\nptpython"
  },
  {
    "objectID": "ml/resources.html#getting-help-2",
    "href": "ml/resources.html#getting-help-2",
    "title": "Resources",
    "section": "Getting help",
    "text": "Getting help\nStack Overflow [python] tag"
  },
  {
    "objectID": "ml/resources.html#documentation-1",
    "href": "ml/resources.html#documentation-1",
    "title": "Resources",
    "section": "Documentation",
    "text": "Documentation\nManual\nTutorials\nPeer-reviewed paper"
  },
  {
    "objectID": "ml/resources.html#book",
    "href": "ml/resources.html#book",
    "title": "Resources",
    "section": "Book",
    "text": "Book\nPaperback version\nFree MOOC version of part 1 of the book\nJupyter notebooks version of the book"
  },
  {
    "objectID": "ml/resources.html#getting-help-3",
    "href": "ml/resources.html#getting-help-3",
    "title": "Resources",
    "section": "Getting help",
    "text": "Getting help\nDiscourse forum"
  },
  {
    "objectID": "ml/torchtensors.html",
    "href": "ml/torchtensors.html",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "",
    "text": "Before information can be fed to artificial neural networks (ANNs), it needs to be converted to a form ANNs can process: floating point numbers. Indeed, you don‚Äôt pass a sentence or an image through an ANN; you input numbers representing a sequence of words or pixel values.\nAll these floating point numbers need to be stored in a data structure. The most suited structure is multidimensional (to hold several layers of information) and since all data is of the same type, it is an array.\nPython already has several multidimensional array structures‚Äîthe most popular of which being NumPy‚Äôs ndarray‚Äîbut the particularities of deep learning call for special characteristics: ability to run operations on GPUs and/or in a distributed fashion, as well as the ability to keep track of computation graphs for automatic differentiation.\nThe PyTorch tensor is a Python data structure with these characteristics that can also easily be converted to/from NumPy‚Äôs ndarray and integrates well with other Python libraries such as Pandas.\nIn this webinar, suitable for users of all levels, we will have a deep look at this data structure and go much beyond a basic introduction.\nIn particular, we will:\n\nsee how tensors are stored in memory,\nlook at the metadata which allows this efficient memory storage,\ncover the basics of working with tensors (indexing, vectorized operations‚Ä¶),\nmove tensors to/from GPUs,\nconvert tensors to/from NumPy ndarrays,\nsee how tensors work in distributed frameworks,\nsee how linear algebra can be done with PyTorch tensors.\n\n\nSlides (click and wait: my reveal.js presentations are heavy and take some time to load‚Ä¶)"
  },
  {
    "objectID": "ml/torchtensors_slides.html#acknowledgements",
    "href": "ml/torchtensors_slides.html#acknowledgements",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nMany drawings in this webinar come from the book:\n\nThe section on storage is also highly inspired by it"
  },
  {
    "objectID": "ml/torchtensors_slides.html#using-tensors-locally",
    "href": "ml/torchtensors_slides.html#using-tensors-locally",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Using tensors locally",
    "text": "Using tensors locally\n\nYou need to have Python & PyTorch installed\nAdditionally, you might want to use an IDE such as elpy if you are an Emacs user, JupyterLab, etc.\n\nNote that PyTorch does not yet support Python 3.10 except in some Linux distributions or on systems where a wheel has been built For the time being, you might have to use it with Python 3.9"
  },
  {
    "objectID": "ml/torchtensors_slides.html#using-tensors-on-cc-clusters",
    "href": "ml/torchtensors_slides.html#using-tensors-on-cc-clusters",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Using tensors on CC clusters",
    "text": "Using tensors on CC clusters\nIn the cluster terminal:\navail_wheels \"torch*\" # List available wheels & compatible Python versions\nmodule avail python   # List available Python versions\nmodule load python/3.9.6             # Load a sensible Python version\nvirtualenv --no-download env         # Create a virtual env\nsource env/bin/activate              # Activate the virtual env\npip install --no-index --upgrade pip # Update pip\npip install --no-index torch         # Install PyTorch\nYou can then launch jobs with sbatch or salloc\nLeave the virtual env with the command: deactivate"
  },
  {
    "objectID": "ml/torchtensors_slides.html#outline",
    "href": "ml/torchtensors_slides.html#outline",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Outline",
    "text": "Outline\n\nWhat is a PyTorch tensor?\nMemory storage\nData type (dtype)\nBasic operations\nWorking with NumPy\nLinear algebra\nHarvesting the power of GPUs\nDistributed operations"
  },
  {
    "objectID": "ml/torchtensors_slides.html#outline-1",
    "href": "ml/torchtensors_slides.html#outline-1",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Outline",
    "text": "Outline\n\nWhat is a PyTorch tensor?\nMemory storage\nData type (dtype)\nBasic operations\nWorking with NumPy\nLinear algebra\nHarvesting the power of GPUs\nDistributed operations"
  },
  {
    "objectID": "ml/torchtensors_slides.html#ann-do-not-process-information-directly",
    "href": "ml/torchtensors_slides.html#ann-do-not-process-information-directly",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "ANN do not process information directly",
    "text": "ANN do not process information directly\n Modified from Stevens, E., Antiga, L., & Viehmann, T. (2020). Deep learning with PyTorch. Manning Publications"
  },
  {
    "objectID": "ml/torchtensors_slides.html#it-needs-to-be-converted-to-numbers",
    "href": "ml/torchtensors_slides.html#it-needs-to-be-converted-to-numbers",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "It needs to be converted to numbers",
    "text": "It needs to be converted to numbers\n Modified from Stevens, E., Antiga, L., & Viehmann, T. (2020). Deep learning with PyTorch. Manning Publications"
  },
  {
    "objectID": "ml/torchtensors_slides.html#these-numbers-must-be-stored-in-a-data-structure",
    "href": "ml/torchtensors_slides.html#these-numbers-must-be-stored-in-a-data-structure",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "These numbers must be stored in a data structure",
    "text": "These numbers must be stored in a data structure\n\nPyTorch tensors are Python objects holding multidimensional arrays\n Stevens, E., Antiga, L., & Viehmann, T. (2020). Deep learning with PyTorch. Manning Publications"
  },
  {
    "objectID": "ml/torchtensors_slides.html#why-a-new-object-when-numpy-already-exists",
    "href": "ml/torchtensors_slides.html#why-a-new-object-when-numpy-already-exists",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Why a new object when NumPy already exists?",
    "text": "Why a new object when NumPy already exists?\n\n\nCan run on accelerators (GPUs, TPUs‚Ä¶)\nKeep track of computation graphs, allowing automatic differentiation\nFuture plan for sharded tensors to run distributed computations"
  },
  {
    "objectID": "ml/torchtensors_slides.html#what-is-a-pytorch-tensor-2",
    "href": "ml/torchtensors_slides.html#what-is-a-pytorch-tensor-2",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "What is a PyTorch tensor?",
    "text": "What is a PyTorch tensor?\nPyTorch is foremost a deep learning library\nIn deep learning, the information contained in objects of interest (e.g.¬†images, texts, sounds) is converted to floating-point numbers (e.g.¬†pixel values, token values, frequencies)\nAs this information is complex, multiple dimensions are required (e.g.¬†two dimensions for the width & height of an image, plus one dimension for the RGB colour channels)\nAdditionally, items are grouped into batches to be processed together, adding yet another dimension\nMultidimensional arrays are thus particularly well suited for deep learning"
  },
  {
    "objectID": "ml/torchtensors_slides.html#what-is-a-pytorch-tensor-3",
    "href": "ml/torchtensors_slides.html#what-is-a-pytorch-tensor-3",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "What is a PyTorch tensor?",
    "text": "What is a PyTorch tensor?\nArtificial neurons perform basic computations on these tensors\nTheir number however is huge & computing efficiency is paramount\nGPUs/TPUs are particularly well suited to perform many simple operations in parallel\nThe very popular NumPy library has, at its core, a mature multidimensional array object well integrated into the scientific Python ecosystem\nBut the PyTorch tensor has additional efficiency characteristics ideal for machine learning & it can be converted to/from NumPy‚Äôs ndarray if needed"
  },
  {
    "objectID": "ml/torchtensors_slides.html#outline-2",
    "href": "ml/torchtensors_slides.html#outline-2",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Outline",
    "text": "Outline\n\nWhat is a PyTorch tensor?\nMemory storage\nData type (dtype)\nBasic operations\nWorking with NumPy\nLinear algebra\nHarvesting the power of GPUs\nDistributed operations"
  },
  {
    "objectID": "ml/torchtensors_slides.html#efficient-memory-storage",
    "href": "ml/torchtensors_slides.html#efficient-memory-storage",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Efficient memory storage",
    "text": "Efficient memory storage\nIn Python, collections (lists, tuples) are groupings of boxed Python objects\nPyTorch tensors & NumPy ndarrays are made of unboxed C numeric types\n\n\nStevens, E., Antiga, L., & Viehmann, T. (2020). Deep learning with PyTorch. Manning Publications"
  },
  {
    "objectID": "ml/torchtensors_slides.html#efficient-memory-storage-1",
    "href": "ml/torchtensors_slides.html#efficient-memory-storage-1",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Efficient memory storage",
    "text": "Efficient memory storage\nThey are usually contiguous memory blocks, but the main difference is that they are unboxed: floats will thus take 4 (32-bit) or 8 (64-bit) bytes each\nBoxed values take up more memory (memory for the pointer + memory for the primitive)\n\n\nStevens, E., Antiga, L., & Viehmann, T. (2020). Deep learning with PyTorch. Manning Publications"
  },
  {
    "objectID": "ml/torchtensors_slides.html#implementation",
    "href": "ml/torchtensors_slides.html#implementation",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Implementation",
    "text": "Implementation\nUnder the hood, the values of a PyTorch tensor are stored as a torch.Storage instance which is a one-dimensional array\n\nimport torch\nt = torch.arange(10.).view(2, 5); print(t) # Functions explained later\n\n[Out]\n\ntensor([[ 0.,  1.,  2., 3.,  4.],\n        [ 5.,  6.,  7.,  8.,  9.]])"
  },
  {
    "objectID": "ml/torchtensors_slides.html#implementation-1",
    "href": "ml/torchtensors_slides.html#implementation-1",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Implementation",
    "text": "Implementation\n\nstorage = t.storage(); print(storage)\n\n[Out]\n\n 0.0\n 1.0\n 2.0\n 3.0\n 4.0\n 5.0\n 6.0\n 7.0\n 8.0\n 9.0\n[torch.FloatStorage of size 10]"
  },
  {
    "objectID": "ml/torchtensors_slides.html#implementation-2",
    "href": "ml/torchtensors_slides.html#implementation-2",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Implementation",
    "text": "Implementation\nThe storage can be indexed\nstorage[3]\n\n[Out]\n\n3.0"
  },
  {
    "objectID": "ml/torchtensors_slides.html#implementation-3",
    "href": "ml/torchtensors_slides.html#implementation-3",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Implementation",
    "text": "Implementation\n\nstorage[3] = 10.0; print(storage)\n\n[Out]\n\n 0.0\n 1.0\n 2.0\n 10.0\n 4.0\n 5.0\n 6.0\n 7.0\n 8.0\n 9.0\n[torch.FloatStorage of size 10]"
  },
  {
    "objectID": "ml/torchtensors_slides.html#implementation-4",
    "href": "ml/torchtensors_slides.html#implementation-4",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Implementation",
    "text": "Implementation\nTo view a multidimensional array from storage, we need metadata:\n\nthe size (shape in NumPy) sets the number of elements in each dimension\nthe offset indicates where the first element of the tensor is in the storage\nthe stride establishes the increment between each element"
  },
  {
    "objectID": "ml/torchtensors_slides.html#storage-metadata",
    "href": "ml/torchtensors_slides.html#storage-metadata",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Storage metadata",
    "text": "Storage metadata\n\n\nStevens, E., Antiga, L., & Viehmann, T. (2020). Deep learning with PyTorch. Manning Publications"
  },
  {
    "objectID": "ml/torchtensors_slides.html#storage-metadata-1",
    "href": "ml/torchtensors_slides.html#storage-metadata-1",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Storage metadata",
    "text": "Storage metadata\n\nt.size()\nt.storage_offset()\nt.stride()\n\n[Out]\n\ntorch.Size([2, 5])\n0\n(5, 1)"
  },
  {
    "objectID": "ml/torchtensors_slides.html#storage-metadata-2",
    "href": "ml/torchtensors_slides.html#storage-metadata-2",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Storage metadata",
    "text": "Storage metadata"
  },
  {
    "objectID": "ml/torchtensors_slides.html#sharing-storage",
    "href": "ml/torchtensors_slides.html#sharing-storage",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Sharing storage",
    "text": "Sharing storage\nMultiple tensors can use the same storage, saving a lot of memory since the metadata is a lot lighter than a whole new array\n\n\nStevens, E., Antiga, L., & Viehmann, T. (2020). Deep learning with PyTorch. Manning Publications"
  },
  {
    "objectID": "ml/torchtensors_slides.html#transposing-in-2-dimensions",
    "href": "ml/torchtensors_slides.html#transposing-in-2-dimensions",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Transposing in 2 dimensions",
    "text": "Transposing in 2 dimensions\n\nt = torch.tensor([[3, 1, 2], [4, 1, 7]]); print(t)\nt.size()\nt.t()\nt.t().size()\n\n[Out]\n\ntensor([[3, 1, 2],\n        [4, 1, 7]])\ntorch.Size([2, 3])\ntensor([[3, 4],\n        [1, 1],\n        [2, 7]])\ntorch.Size([3, 2])"
  },
  {
    "objectID": "ml/torchtensors_slides.html#transposing-in-2-dimensions-1",
    "href": "ml/torchtensors_slides.html#transposing-in-2-dimensions-1",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Transposing in 2 dimensions",
    "text": "Transposing in 2 dimensions\n= flipping the stride elements around\n\n\nStevens, E., Antiga, L., & Viehmann, T. (2020). Deep learning with PyTorch. Manning Publications"
  },
  {
    "objectID": "ml/torchtensors_slides.html#transposing-in-higher-dimensions",
    "href": "ml/torchtensors_slides.html#transposing-in-higher-dimensions",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Transposing in higher dimensions",
    "text": "Transposing in higher dimensions\n\ntorch.t() is a shorthand for torch.transpose(0, 1):\ntorch.equal(t.t(), t.transpose(0, 1))\n\n[Out]\n\nTrue\nWhile torch.t() only works for 2D tensors, torch.transpose() can be used to transpose 2 dimensions in tensors of any number of dimensions"
  },
  {
    "objectID": "ml/torchtensors_slides.html#transposing-in-higher-dimensions-1",
    "href": "ml/torchtensors_slides.html#transposing-in-higher-dimensions-1",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Transposing in higher dimensions",
    "text": "Transposing in higher dimensions\n\nt = torch.zeros(1, 2, 3); print(t)\n\nt.size()\nt.stride()\n\n[Out]\n\ntensor([[[0., 0., 0.],\n         [0., 0., 0.]]])\n\ntorch.Size([1, 2, 3])\n(6, 3, 1)"
  },
  {
    "objectID": "ml/torchtensors_slides.html#transposing-in-higher-dimensions-2",
    "href": "ml/torchtensors_slides.html#transposing-in-higher-dimensions-2",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Transposing in higher dimensions",
    "text": "Transposing in higher dimensions\n\nt.transpose(0, 1)\n\nt.transpose(0, 1).size()\nt.transpose(0, 1).stride()\n\n[Out]\n\ntensor([[[0., 0., 0.]],\n        [[0., 0., 0.]]])\n\ntorch.Size([2, 1, 3])\n(3, 6, 1)  # Notice how transposing flipped 2 elements of the stride"
  },
  {
    "objectID": "ml/torchtensors_slides.html#transposing-in-higher-dimensions-3",
    "href": "ml/torchtensors_slides.html#transposing-in-higher-dimensions-3",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Transposing in higher dimensions",
    "text": "Transposing in higher dimensions\n\nt.transpose(0, 2)\n\nt.transpose(0, 2).size()\nt.transpose(0, 2).stride()\n\n[Out]\n\ntensor([[[0.],\n         [0.]],\n        [[0.],\n         [0.]],\n        [[0.],\n         [0.]]])\n\ntorch.Size([3, 2, 1])\n(1, 3, 6)"
  },
  {
    "objectID": "ml/torchtensors_slides.html#transposing-in-higher-dimensions-4",
    "href": "ml/torchtensors_slides.html#transposing-in-higher-dimensions-4",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Transposing in higher dimensions",
    "text": "Transposing in higher dimensions\n\nt.transpose(1, 2)\n\nt.transpose(1, 2).size()\nt.transpose(1, 2).stride()\n\n[Out]\n\ntensor([[[0., 0.],\n         [0., 0.],\n         [0., 0.]]])\n\ntorch.Size([1, 3, 2])\n(6, 1, 3)"
  },
  {
    "objectID": "ml/torchtensors_slides.html#outline-3",
    "href": "ml/torchtensors_slides.html#outline-3",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Outline",
    "text": "Outline\n\nWhat is a PyTorch tensor?\nMemory storage\nData type (dtype)\nBasic operations\nWorking with NumPy\nLinear algebra\nHarvesting the power of GPUs\nDistributed operations"
  },
  {
    "objectID": "ml/torchtensors_slides.html#default-dtype",
    "href": "ml/torchtensors_slides.html#default-dtype",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Default dtype",
    "text": "Default dtype\n\nSince PyTorch tensors were built with utmost efficiency in mind for neural networks, the default data type is 32-bit floating points\nThis is sufficient for accuracy & much faster than 64-bit floating points\n\nNote that, by contrast, NumPy ndarrays use 64-bit as their default"
  },
  {
    "objectID": "ml/torchtensors_slides.html#list-of-pytorch-tensor-dtypes",
    "href": "ml/torchtensors_slides.html#list-of-pytorch-tensor-dtypes",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "List of PyTorch tensor dtypes",
    "text": "List of PyTorch tensor dtypes\n\n\n\n\ntorch.float16 / torch.half\n\n\n‚ÄÉ‚ÄÉ\n\n\n16-bit / half-precision floating-point\n\n\n\n\ntorch.float32 / torch.float\n\n\n\n\n32-bit / single-precision floating-point\n\n\n\n\ntorch.float64 / torch.double\n\n\n\n\n64-bit / double-precision floating-point\n\n\n\n\n\n\n\n\n\n\ntorch.uint8\n\n\n\n\nunsigned 8-bit integers\n\n\n\n\ntorch.int8\n\n\n\n\nsigned 8-bit integers\n\n\n\n\ntorch.int16 / torch.short\n\n\n\n\nsigned 16-bit integers\n\n\n\n\ntorch.int32 / torch.int\n\n\n\n\nsigned 32-bit integers\n\n\n\n\ntorch.int64 / torch.long\n\n\n\n\nsigned 64-bit integers\n\n\n\n\n\n\n\n\n\n\n\n\ntorch.bool\n\n\n\n\nboolean"
  },
  {
    "objectID": "ml/torchtensors_slides.html#checking-changing-dtype",
    "href": "ml/torchtensors_slides.html#checking-changing-dtype",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Checking & changing dtype",
    "text": "Checking & changing dtype\n\nt = torch.rand(2, 3); print(t)\nt.dtype   # Remember that the default dtype for PyTorch tensors is float32\nt2 = t.type(torch.float64); print(t2) # If dtype ‚â† default, it is printed\nt2.dtype\n\n[Out]\n\ntensor([[0.8130, 0.3757, 0.7682],\n        [0.3482, 0.0516, 0.3772]])\ntorch.float32\ntensor([[0.8130, 0.3757, 0.7682],\n        [0.3482, 0.0516, 0.3772]], dtype=torch.float64)\ntorch.float64"
  },
  {
    "objectID": "ml/torchtensors_slides.html#outline-4",
    "href": "ml/torchtensors_slides.html#outline-4",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Outline",
    "text": "Outline\n\nWhat is a PyTorch tensor?\nMemory storage\nData type (dtype)\nBasic operations\nWorking with NumPy\nLinear algebra\nHarvesting the power of GPUs\nDistributed operations"
  },
  {
    "objectID": "ml/torchtensors_slides.html#creating-tensors",
    "href": "ml/torchtensors_slides.html#creating-tensors",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Creating tensors",
    "text": "Creating tensors\n\ntorch.tensor: ‚ÄÉ‚ÄÉInput individual values\ntorch.arange: ‚ÄÉ‚ÄÉSimilar to range but creates a 1D tensor\ntorch.linspace: ‚ÄÉ1D linear scale tensor\ntorch.logspace: ‚ÄÉ1D log scale tensor\ntorch.rand: ‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÇRandom numbers from a uniform distribution on [0, 1)\ntorch.randn: ‚ÄÉ‚ÄÉ‚ÄÉNumbers from the standard normal distribution\ntorch.randperm: ‚ÄÉ¬†Random permutation of integers\ntorch.empty: ‚ÄÉ‚ÄÉ‚ÄÉUninitialized tensor\ntorch.zeros: ‚ÄÉ‚ÄÉ‚ÄÉTensor filled with 0\ntorch.ones: ‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÇ¬†Tensor filled with 1\ntorch.eye: ‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÇIdentity matrix"
  },
  {
    "objectID": "ml/torchtensors_slides.html#creating-tensors-1",
    "href": "ml/torchtensors_slides.html#creating-tensors-1",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Creating tensors",
    "text": "Creating tensors\n\ntorch.manual_seed(0)  # If you want to reproduce the result\ntorch.rand(1)\n\ntorch.manual_seed(0)  # Run before each operation to get the same result\ntorch.rand(1).item()  # Extract the value from a tensor\n\n[Out]\n\ntensor([0.4963])\n\n0.49625658988952637"
  },
  {
    "objectID": "ml/torchtensors_slides.html#creating-tensors-2",
    "href": "ml/torchtensors_slides.html#creating-tensors-2",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Creating tensors",
    "text": "Creating tensors\n\ntorch.rand(1)\ntorch.rand(1, 1)\ntorch.rand(1, 1, 1)\ntorch.rand(1, 1, 1, 1)\n\n[Out]\n\ntensor([0.6984])\ntensor([[0.5675]])\ntensor([[[0.8352]]])\ntensor([[[[0.2056]]]])"
  },
  {
    "objectID": "ml/torchtensors_slides.html#creating-tensors-3",
    "href": "ml/torchtensors_slides.html#creating-tensors-3",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Creating tensors",
    "text": "Creating tensors\n\ntorch.rand(2)\ntorch.rand(2, 2, 2, 2)\n\n[Out]\n\ntensor([0.5932, 0.1123])\ntensor([[[[0.1147, 0.3168],\n          [0.6965, 0.9143]],\n         [[0.9351, 0.9412],\n          [0.5995, 0.0652]]],\n        [[[0.5460, 0.1872],\n          [0.0340, 0.9442]],\n         [[0.8802, 0.0012],\n          [0.5936, 0.4158]]]])"
  },
  {
    "objectID": "ml/torchtensors_slides.html#creating-tensors-4",
    "href": "ml/torchtensors_slides.html#creating-tensors-4",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Creating tensors",
    "text": "Creating tensors\n\ntorch.rand(2)\ntorch.rand(3)\ntorch.rand(1, 1)\ntorch.rand(1, 1, 1)\ntorch.rand(2, 6)\n\n[Out]\n\ntensor([0.7682, 0.0885])\ntensor([0.1320, 0.3074, 0.6341])\ntensor([[0.4901]])\ntensor([[[0.8964]]])\ntensor([[0.4556, 0.6323, 0.3489, 0.4017, 0.0223, 0.1689],\n        [0.2939, 0.5185, 0.6977, 0.8000, 0.1610, 0.2823]])"
  },
  {
    "objectID": "ml/torchtensors_slides.html#creating-tensors-5",
    "href": "ml/torchtensors_slides.html#creating-tensors-5",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Creating tensors",
    "text": "Creating tensors\n\ntorch.rand(2, 4, dtype=torch.float64)  # You can set dtype\ntorch.ones(2, 1, 4, 5)\n\n[Out]\n\ntensor([[0.6650, 0.7849, 0.2104, 0.6767],\n        [0.1097, 0.5238, 0.2260, 0.5582]], dtype=torch.float64)\ntensor([[[[1., 1., 1., 1., 1.],\n          [1., 1., 1., 1., 1.],\n          [1., 1., 1., 1., 1.],\n          [1., 1., 1., 1., 1.]]],\n        [[[1., 1., 1., 1., 1.],\n          [1., 1., 1., 1., 1.],\n          [1., 1., 1., 1., 1.],\n          [1., 1., 1., 1., 1.]]]])"
  },
  {
    "objectID": "ml/torchtensors_slides.html#creating-tensors-6",
    "href": "ml/torchtensors_slides.html#creating-tensors-6",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Creating tensors",
    "text": "Creating tensors\n\nt = torch.rand(2, 3); print(t)\ntorch.zeros_like(t)             # Matches the size of t\ntorch.ones_like(t)\ntorch.randn_like(t)\n\n[Out]\n\ntensor([[0.4051, 0.6394, 0.0871],\n        [0.4509, 0.5255, 0.5057]])\ntensor([[0., 0., 0.],\n        [0., 0., 0.]])\ntensor([[1., 1., 1.],\n        [1., 1., 1.]])\ntensor([[-0.3088, -0.0104,  1.0461],\n        [ 0.9233,  0.0236, -2.1217]])"
  },
  {
    "objectID": "ml/torchtensors_slides.html#creating-tensors-7",
    "href": "ml/torchtensors_slides.html#creating-tensors-7",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Creating tensors",
    "text": "Creating tensors\n\ntorch.arange(2, 10, 4)    # From 2 to 10 in increments of 4\ntorch.linspace(2, 10, 4)  # 4 elements from 2 to 10 on the linear scale\ntorch.logspace(2, 10, 4)  # Same on the log scale\ntorch.randperm(4)\ntorch.eye(3)\n\n[Out]\n\ntensor([2, 6])\ntensor([2.0000,  4.6667,  7.3333, 10.0000])\ntensor([1.0000e+02, 4.6416e+04, 2.1544e+07, 1.0000e+10])\ntensor([1, 3, 2, 0])\ntensor([[1., 0., 0.],\n        [0., 1., 0.],\n        [0., 0., 1.]])"
  },
  {
    "objectID": "ml/torchtensors_slides.html#tensor-information",
    "href": "ml/torchtensors_slides.html#tensor-information",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Tensor information",
    "text": "Tensor information\n\nt = torch.rand(2, 3); print(t)\nt.size()\nt.dim()\nt.numel()\n\n[Out]\n\ntensor([[0.5885, 0.7005, 0.1048],\n        [0.1115, 0.7526, 0.0658]])\ntorch.Size([2, 3])\n2\n6"
  },
  {
    "objectID": "ml/torchtensors_slides.html#tensor-indexing",
    "href": "ml/torchtensors_slides.html#tensor-indexing",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Tensor indexing",
    "text": "Tensor indexing\n\nx = torch.rand(3, 4)\nx[:]                 # With a range, the comma is implicit: same as x[:, ]\nx[:, 2]\nx[1, :]\nx[2, 3]\n\n[Out]\n\ntensor([[0.6575, 0.4017, 0.7391, 0.6268],\n        [0.2835, 0.0993, 0.7707, 0.1996],\n        [0.4447, 0.5684, 0.2090, 0.7724]])\ntensor([0.7391, 0.7707, 0.2090])\ntensor([0.2835, 0.0993, 0.7707, 0.1996])\ntensor(0.7724)"
  },
  {
    "objectID": "ml/torchtensors_slides.html#tensor-indexing-1",
    "href": "ml/torchtensors_slides.html#tensor-indexing-1",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Tensor indexing",
    "text": "Tensor indexing\n\nx[-1:]        # Last element (implicit comma, so all columns)\nx[-1]         # No range, no implicit comma: we are indexing \n# from a list of tensors, so the result is a one dimensional tensor\n# (Each dimension is a list of tensors of the previous dimension)\nx[-1].size()  # Same number of dimensions than x (2 dimensions)\nx[-1:].size() # We dropped one dimension\n\n[Out]\n\ntensor([[0.8168, 0.0879, 0.2642, 0.3777]])\ntensor([0.8168, 0.0879, 0.2642, 0.3777])\n\ntorch.Size([4])\ntorch.Size([1, 4])"
  },
  {
    "objectID": "ml/torchtensors_slides.html#tensor-indexing-2",
    "href": "ml/torchtensors_slides.html#tensor-indexing-2",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Tensor indexing",
    "text": "Tensor indexing\n\nx[0:1]     # Python ranges are inclusive to the left, not the right\nx[:-1]     # From start to one before last (& implicit comma)\nx[0:3:2]   # From 0th (included) to 3rd (excluded) in increment of 2\n\n[Out]\n\ntensor([[0.5873, 0.0225, 0.7234, 0.4538]])\ntensor([[0.5873, 0.0225, 0.7234, 0.4538],\n        [0.9525, 0.0111, 0.6421, 0.4647]])\ntensor([[0.5873, 0.0225, 0.7234, 0.4538],\n        [0.8168, 0.0879, 0.2642, 0.3777]])"
  },
  {
    "objectID": "ml/torchtensors_slides.html#tensor-indexing-3",
    "href": "ml/torchtensors_slides.html#tensor-indexing-3",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Tensor indexing",
    "text": "Tensor indexing\n\nx[None]          # Adds a dimension of size one as the 1st dimension\nx.size()\nx[None].size()\n\n[Out]\n\ntensor([[[0.5873, 0.0225, 0.7234, 0.4538],\n         [0.9525, 0.0111, 0.6421, 0.4647],\n         [0.8168, 0.0879, 0.2642, 0.3777]]])\ntorch.Size([3, 4])\ntorch.Size([1, 3, 4])"
  },
  {
    "objectID": "ml/torchtensors_slides.html#a-word-of-caution-about-indexing",
    "href": "ml/torchtensors_slides.html#a-word-of-caution-about-indexing",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "A word of caution about indexing",
    "text": "A word of caution about indexing\n\nWhile indexing elements of a tensor to extract some of the data as a final step of some computation is fine, you should not use indexing to run operations on tensor elements in a loop as this would be extremely inefficient\nInstead, you want to use vectorized operations"
  },
  {
    "objectID": "ml/torchtensors_slides.html#vectorized-operations",
    "href": "ml/torchtensors_slides.html#vectorized-operations",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Vectorized operations",
    "text": "Vectorized operations\n\nSince PyTorch tensors are homogeneous (i.e.¬†made of a single data type), as with NumPy‚Äôs ndarrays, operations are vectorized & thus staggeringly fast\nNumPy is mostly written in C & PyTorch in C++. With either library, when you run vectorized operations on arrays/tensors, you don‚Äôt use raw Python (slow) but compiled C/C++ code (much faster)\nHere is an excellent post explaining Python vectorization & why it makes such a big difference"
  },
  {
    "objectID": "ml/torchtensors_slides.html#vectorized-operations-comparison",
    "href": "ml/torchtensors_slides.html#vectorized-operations-comparison",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Vectorized operations: comparison",
    "text": "Vectorized operations: comparison\nRaw Python method\n# Create tensor. We use float64 here to avoid truncation errors\nt = torch.rand(10**6, dtype=torch.float64)\n# Initialize the sum\nsum = 0\n# Run loop\nfor i in range(len(t)): sum += t[i]\n# Print result\nprint(sum)\nVectorized function\nt.sum()"
  },
  {
    "objectID": "ml/torchtensors_slides.html#vectorized-operations-comparison-1",
    "href": "ml/torchtensors_slides.html#vectorized-operations-comparison-1",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Vectorized operations: comparison",
    "text": "Vectorized operations: comparison\nBoth methods give the same result\n\nThis is why we used float64: While the accuracy remains excellent with float32 if we use the PyTorch function torch.sum(), the raw Python loop gives a fairly inaccurate result\n\n\n[Out]\n\ntensor(500023.0789, dtype=torch.float64)\n\ntensor(500023.0789, dtype=torch.float64)"
  },
  {
    "objectID": "ml/torchtensors_slides.html#vectorized-operations-timing",
    "href": "ml/torchtensors_slides.html#vectorized-operations-timing",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Vectorized operations: timing",
    "text": "Vectorized operations: timing\nLet‚Äôs compare the timing with PyTorch built-in benchmark utility\n# Load utility\nimport torch.utils.benchmark as benchmark\n\n# Create a function for our loop\ndef sum_loop(t, sum):\n    for i in range(len(t)): sum += t[i]"
  },
  {
    "objectID": "ml/torchtensors_slides.html#vectorized-operations-timing-1",
    "href": "ml/torchtensors_slides.html#vectorized-operations-timing-1",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Vectorized operations: timing",
    "text": "Vectorized operations: timing\nNow we can create the timers\nt0 = benchmark.Timer(\n    stmt='sum_loop(t, sum)',\n    setup='from __main__ import sum_loop',\n    globals={'t': t, 'sum': sum})\n\nt1 = benchmark.Timer(\n    stmt='t.sum()',\n    globals={'t': t})"
  },
  {
    "objectID": "ml/torchtensors_slides.html#vectorized-operations-timing-2",
    "href": "ml/torchtensors_slides.html#vectorized-operations-timing-2",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Vectorized operations: timing",
    "text": "Vectorized operations: timing\nLet‚Äôs time 100 runs to have a reliable benchmark\nprint(t0.timeit(100))\nprint(t1.timeit(100))\n\nI ran the code on my laptop with a dedicated GPU & 32GB RAM"
  },
  {
    "objectID": "ml/torchtensors_slides.html#vectorized-operations-timing-3",
    "href": "ml/torchtensors_slides.html#vectorized-operations-timing-3",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Vectorized operations: timing",
    "text": "Vectorized operations: timing\nTiming of raw Python loop\nsum_loop(t, sum)\nsetup: from __main__ import sum_loop\n  1.37 s\n  1 measurement, 100 runs , 1 thread\nTiming of vectorized function\nt.sum()\n  191.26 us\n  1 measurement, 100 runs , 1 thread"
  },
  {
    "objectID": "ml/torchtensors_slides.html#vectorized-operations-timing-4",
    "href": "ml/torchtensors_slides.html#vectorized-operations-timing-4",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Vectorized operations: timing",
    "text": "Vectorized operations: timing\nSpeedup:\n1.37/(191.26 * 10**-6) = 7163\n\nThe vectorized function runs more than 7,000 times faster!!!"
  },
  {
    "objectID": "ml/torchtensors_slides.html#even-more-important-on-gpus",
    "href": "ml/torchtensors_slides.html#even-more-important-on-gpus",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Even more important on GPUs",
    "text": "Even more important on GPUs\nWe will talk about GPUs in detail later\nTiming of raw Python loop on GPU (actually slower on GPU!)\nsum_loop(t, sum)\nsetup: from __main__ import sum_loop\n  4.54 s\n  1 measurement, 100 runs , 1 thread\nTiming of vectorized function on GPU (here we do get a speedup)\nt.sum()\n  50.62 us\n  1 measurement, 100 runs , 1 thread"
  },
  {
    "objectID": "ml/torchtensors_slides.html#even-more-important-on-gpus-1",
    "href": "ml/torchtensors_slides.html#even-more-important-on-gpus-1",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Even more important on GPUs",
    "text": "Even more important on GPUs\nSpeedup:\n4.54/(50.62 * 10**-6) = 89688\n\nOn GPUs, it is even more important not to index repeatedly from a tensor\n\n\nOn GPUs, the vectorized function runs almost 90,000 times faster!!!"
  },
  {
    "objectID": "ml/torchtensors_slides.html#simple-mathematical-operations",
    "href": "ml/torchtensors_slides.html#simple-mathematical-operations",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Simple mathematical operations",
    "text": "Simple mathematical operations\n\nt1 = torch.arange(1, 5).view(2, 2); print(t1)\nt2 = torch.tensor([[1, 1], [0, 0]]); print(t2)\nt1 + t2 # Operation performed between elements at corresponding locations\nt1 + 1  # Operation applied to each element of the tensor\n\n[Out]\n\ntensor([[1, 2],\n        [3, 4]])\ntensor([[1, 1],\n        [0, 0]])\ntensor([[2, 3],\n        [3, 4]])\ntensor([[2, 3],\n        [4, 5]])"
  },
  {
    "objectID": "ml/torchtensors_slides.html#reduction",
    "href": "ml/torchtensors_slides.html#reduction",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Reduction",
    "text": "Reduction\n\nt = torch.ones(2, 3, 4); print(t)\nt.sum()   # Reduction over all entries\n\n[Out]\n\ntensor([[[1., 1., 1., 1.],\n         [1., 1., 1., 1.],\n         [1., 1., 1., 1.]],\n        [[1., 1., 1., 1.],\n         [1., 1., 1., 1.],\n         [1., 1., 1., 1.]]])\ntensor(24.)\n\nOther reduction functions (e.g.¬†mean) behave the same way"
  },
  {
    "objectID": "ml/torchtensors_slides.html#reduction-1",
    "href": "ml/torchtensors_slides.html#reduction-1",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Reduction",
    "text": "Reduction\n\n# Reduction over a specific dimension\nt.sum(0)  \nt.sum(1)\nt.sum(2)\n\n[Out]\n\ntensor([[2., 2., 2., 2.],\n        [2., 2., 2., 2.],\n        [2., 2., 2., 2.]])\ntensor([[3., 3., 3., 3.],\n        [3., 3., 3., 3.]])\ntensor([[4., 4., 4.],\n        [4., 4., 4.]])"
  },
  {
    "objectID": "ml/torchtensors_slides.html#reduction-2",
    "href": "ml/torchtensors_slides.html#reduction-2",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Reduction",
    "text": "Reduction\n\n# Reduction over multiple dimensions\nt.sum((0, 1))\nt.sum((0, 2))\nt.sum((1, 2))\n\n[Out]\n\ntensor([6., 6., 6., 6.])\ntensor([8., 8., 8.])\ntensor([12., 12.])"
  },
  {
    "objectID": "ml/torchtensors_slides.html#in-place-operations",
    "href": "ml/torchtensors_slides.html#in-place-operations",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "In-place operations",
    "text": "In-place operations\nWith operators post-fixed with _:\nt1 = torch.tensor([1, 2]); print(t1)\nt2 = torch.tensor([1, 1]); print(t2)\nt1.add_(t2); print(t1)\nt1.zero_(); print(t1)\n\n[Out]\n\ntensor([1, 2])\ntensor([1, 1])\ntensor([2, 3])\ntensor([0, 0])"
  },
  {
    "objectID": "ml/torchtensors_slides.html#in-place-operations-vs-reassignments",
    "href": "ml/torchtensors_slides.html#in-place-operations-vs-reassignments",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "In-place operations vs reassignments",
    "text": "In-place operations vs reassignments\n\nt1 = torch.ones(1); t1, hex(id(t1))\nt1.add_(1); t1, hex(id(t1))        # In-place operation: same address\nt1 = t1.add(1); t1, hex(id(t1))    # Reassignment: new address in memory\nt1 = t1 + 1; t1, hex(id(t1))       # Reassignment: new address in memory\n\n[Out]\n\n(tensor([1.]), '0x7fc61accc3b0')\n(tensor([2.]), '0x7fc61accc3b0')\n(tensor([3.]), '0x7fc61accc5e0')\n(tensor([4.]), '0x7fc61accc6d0')"
  },
  {
    "objectID": "ml/torchtensors_slides.html#tensor-views",
    "href": "ml/torchtensors_slides.html#tensor-views",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Tensor views",
    "text": "Tensor views\n\nt = torch.tensor([[1, 2, 3], [4, 5, 6]]); print(t)\nt.size()\nt.view(6)\nt.view(3, 2)\nt.view(3, -1) # Same: with -1, the size is inferred from other dimensions\n\n[Out]\n\ntensor([[1, 2, 3],\n        [4, 5, 6]])\ntorch.Size([2, 3])\ntensor([1, 2, 3, 4, 5, 6])\ntensor([[1, 2],\n        [3, 4],\n        [5, 6]])"
  },
  {
    "objectID": "ml/torchtensors_slides.html#note-the-difference",
    "href": "ml/torchtensors_slides.html#note-the-difference",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Note the difference",
    "text": "Note the difference\n\nt1 = torch.tensor([[1, 2, 3], [4, 5, 6]]); print(t1)\nt2 = t1.t(); print(t2)\nt3 = t1.view(3, 2); print(t3)\n\n[Out]\n\ntensor([[1, 2, 3],\n        [4, 5, 6]])\ntensor([[1, 4],\n        [2, 5],\n        [3, 6]])\ntensor([[1, 2],\n        [3, 4],\n        [5, 6]])"
  },
  {
    "objectID": "ml/torchtensors_slides.html#logical-operations",
    "href": "ml/torchtensors_slides.html#logical-operations",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Logical operations",
    "text": "Logical operations\n\nt1 = torch.randperm(5); print(t1)\nt2 = torch.randperm(5); print(t2)\nt1 > 3                            # Test each element\nt1 < t2                           # Test corresponding pairs of elements\n\n[Out]\n\ntensor([4, 1, 0, 2, 3])\ntensor([0, 4, 2, 1, 3])\ntensor([ True, False, False, False, False])\ntensor([False,  True,  True, False, False])"
  },
  {
    "objectID": "ml/torchtensors_slides.html#outline-5",
    "href": "ml/torchtensors_slides.html#outline-5",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Outline",
    "text": "Outline\n\nWhat is a PyTorch tensor?\nMemory storage\nData type (dtype)\nBasic operations\nWorking with NumPy\nLinear algebra\nHarvesting the power of GPUs\nDistributed operations"
  },
  {
    "objectID": "ml/torchtensors_slides.html#conversion-without-copy",
    "href": "ml/torchtensors_slides.html#conversion-without-copy",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Conversion without copy",
    "text": "Conversion without copy\nPyTorch tensors can be converted to NumPy ndarrays & vice-versa in a very efficient manner as both objects share the same memory\nt = torch.rand(2, 3); print(t)\nt_np = t.numpy(); print(t_np)      # From PyTorch tensor to NumPy ndarray\n\n[Out]\n\ntensor([[0.8434, 0.0876, 0.7507],\n        [0.1457, 0.3638, 0.0563]])   # PyTorch Tensor\n\n[[0.84344184 0.08764815 0.7506627 ]\n [0.14567494 0.36384273 0.05629885]] # NumPy ndarray"
  },
  {
    "objectID": "ml/torchtensors_slides.html#mind-the-different-defaults",
    "href": "ml/torchtensors_slides.html#mind-the-different-defaults",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Mind the different defaults",
    "text": "Mind the different defaults\n\nt_np.dtype\n\n[Out]\n\ndtype('float32')\n\nRemember that PyTorch tensors use 32-bit floating points by default  (because this is what you want in neural networks) \n\n\nBut NumPy defaults to 64-bit  Depending on your workflow, you might have to change dtype"
  },
  {
    "objectID": "ml/torchtensors_slides.html#from-numpy-to-pytorch",
    "href": "ml/torchtensors_slides.html#from-numpy-to-pytorch",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "From NumPy to PyTorch",
    "text": "From NumPy to PyTorch\n\nimport numpy as np\na = np.random.rand(2, 3); print(a)\na_pt = torch.from_numpy(a); print(a_pt)    # From ndarray to tensor\n\n[Out]\n\n[[0.55892276 0.06026952 0.72496545]\n [0.65659463 0.27697739 0.29141587]]\n\ntensor([[0.5589, 0.0603, 0.7250],\n        [0.6566, 0.2770, 0.2914]], dtype=torch.float64)\n\nHere again, you might have to change dtype"
  },
  {
    "objectID": "ml/torchtensors_slides.html#notes-about-conversion-without-copy",
    "href": "ml/torchtensors_slides.html#notes-about-conversion-without-copy",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Notes about conversion without copy",
    "text": "Notes about conversion without copy\nt & t_np are objects of different Python types, so, as far as Python is concerned, they have different addresses\nid(t) == id(t_np)\n\n[Out]\n\nFalse"
  },
  {
    "objectID": "ml/torchtensors_slides.html#notes-about-conversion-without-copy-1",
    "href": "ml/torchtensors_slides.html#notes-about-conversion-without-copy-1",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Notes about conversion without copy",
    "text": "Notes about conversion without copy\nHowever‚Äîthat‚Äôs quite confusing‚Äîthey share an underlying C array in memory & modifying one in-place also modifies the other\nt.zero_()\nprint(t_np)\n\n[Out]\n\ntensor([[0., 0., 0.],\n        [0., 0., 0.]])\n\n[[0. 0. 0.]\n [0. 0. 0.]]"
  },
  {
    "objectID": "ml/torchtensors_slides.html#notes-about-conversion-without-copy-2",
    "href": "ml/torchtensors_slides.html#notes-about-conversion-without-copy-2",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Notes about conversion without copy",
    "text": "Notes about conversion without copy\nLastly, as NumPy only works on CPU, to convert a PyTorch tensor allocated to the GPU, the content will have to be copied to the CPU first"
  },
  {
    "objectID": "ml/torchtensors_slides.html#outline-6",
    "href": "ml/torchtensors_slides.html#outline-6",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Outline",
    "text": "Outline\n\nWhat is a PyTorch tensor?\nMemory storage\nData type (dtype)\nBasic operations\nWorking with NumPy\nLinear algebra\nHarvesting the power of GPUs\nDistributed operations"
  },
  {
    "objectID": "ml/torchtensors_slides.html#torch.linalg-module",
    "href": "ml/torchtensors_slides.html#torch.linalg-module",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "torch.linalg module",
    "text": "torch.linalg module\n\nAll functions from numpy.linalg implemented (with accelerator & automatic differentiation support)\nSome additional functions\n\n\nRequires torch >= 1.9  Linear algebra support was less developed before the introduction of this module"
  },
  {
    "objectID": "ml/torchtensors_slides.html#system-of-linear-equations-solver",
    "href": "ml/torchtensors_slides.html#system-of-linear-equations-solver",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "System of linear equations solver",
    "text": "System of linear equations solver\nLet‚Äôs have a look at an extremely basic example:\n2x + 3y - z = 5\nx - 2y + 8z = 21\n6x + y - 3z = -1\nWe are looking for the values of x, y, & z that would satisfy this system"
  },
  {
    "objectID": "ml/torchtensors_slides.html#system-of-linear-equations-solver-1",
    "href": "ml/torchtensors_slides.html#system-of-linear-equations-solver-1",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "System of linear equations solver",
    "text": "System of linear equations solver\nWe create a 2D tensor A of size (3, 3) with the coefficients of the equations  & a 1D tensor b of size 3 with the right hand sides values of the equations\nA = torch.tensor([[2., 3., -1.], [1., -2., 8.], [6., 1., -3.]]); print(A)\nb = torch.tensor([5., 21., -1.]); print(b)\n\n[Out]\n\ntensor([[ 2.,  3., -1.],\n        [ 1., -2.,  8.],\n        [ 6.,  1., -3.]])\ntensor([ 5., 21., -1.])"
  },
  {
    "objectID": "ml/torchtensors_slides.html#system-of-linear-equations-solver-2",
    "href": "ml/torchtensors_slides.html#system-of-linear-equations-solver-2",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "System of linear equations solver",
    "text": "System of linear equations solver\nSolving this system is as simple as running the torch.linalg.solve function:\nx = torch.linalg.solve(A, b); print(x)\n\n[Out]\n\ntensor([1., 2., 3.])\nOur solution is:\nx = 1\ny = 2\nz = 3"
  },
  {
    "objectID": "ml/torchtensors_slides.html#verify-our-result",
    "href": "ml/torchtensors_slides.html#verify-our-result",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Verify our result",
    "text": "Verify our result\n\ntorch.allclose(A @ x, b)\n\n[Out]\n\nTrue"
  },
  {
    "objectID": "ml/torchtensors_slides.html#system-of-linear-equations-solver-3",
    "href": "ml/torchtensors_slides.html#system-of-linear-equations-solver-3",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "System of linear equations solver",
    "text": "System of linear equations solver\nHere is another simple example:\n# Create a square normal random matrix\nA = torch.randn(4, 4); print(A)\n# Create a tensor of right hand side values\nb = torch.randn(4); print(b)\n\n# Solve the system\nx = torch.linalg.solve(A, b); print(x)\n\n# Verify\ntorch.allclose(A @ x, b)"
  },
  {
    "objectID": "ml/torchtensors_slides.html#system-of-linear-equations-solver-4",
    "href": "ml/torchtensors_slides.html#system-of-linear-equations-solver-4",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "System of linear equations solver",
    "text": "System of linear equations solver\n\n[Out]\n\ntensor([[ 1.5091,  2.0820,  1.7067,  2.3804], # A (coefficients)\n        [-1.1256, -0.3170, -1.0925, -0.0852],\n        [ 0.3276, -0.7607, -1.5991,  0.0185],\n        [-0.7504,  0.1854,  0.6211,  0.6382]])\n\ntensor([-1.0886, -0.2666,  0.1894, -0.2190])  # b (right hand side values)\n\ntensor([ 0.1992, -0.7011,  0.2541, -0.1526])  # x (our solution)\n\nTrue                                          # Verification"
  },
  {
    "objectID": "ml/torchtensors_slides.html#with-2-multidimensional-tensors",
    "href": "ml/torchtensors_slides.html#with-2-multidimensional-tensors",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "With 2 multidimensional tensors",
    "text": "With 2 multidimensional tensors\n\nA = torch.randn(2, 3, 3)              # Must be batches of square matrices\nB = torch.randn(2, 3, 5)              # Dimensions must be compatible\nX = torch.linalg.solve(A, B); print(X)\ntorch.allclose(A @ X, B)\n\n[Out]\n\ntensor([[[-0.0545, -0.1012,  0.7863, -0.0806, -0.0191],\n         [-0.9846, -0.0137, -1.7521, -0.4579, -0.8178],\n         [-1.9142, -0.6225, -1.9239, -0.6972,  0.7011]],\n        [[ 3.2094,  0.3432, -1.6604, -0.7885,  0.0088],\n         [ 7.9852,  1.4605, -1.7037, -0.7713,  2.7319],\n         [-4.1979,  0.0849,  1.0864,  0.3098, -1.0347]]])\nTrue"
  },
  {
    "objectID": "ml/torchtensors_slides.html#matrix-inversions",
    "href": "ml/torchtensors_slides.html#matrix-inversions",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Matrix inversions",
    "text": "Matrix inversions\n\n\nIt is faster & more numerically stable to solve a system of linear equations directly than to compute the inverse matrix first\n\n\n\nLimit matrix inversions to situations where it is truly necessary"
  },
  {
    "objectID": "ml/torchtensors_slides.html#matrix-inversions-1",
    "href": "ml/torchtensors_slides.html#matrix-inversions-1",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Matrix inversions",
    "text": "Matrix inversions\n\nA = torch.rand(2, 3, 3)      # Batch of square matrices\nA_inv = torch.linalg.inv(A)  # Batch of inverse matrices\nA @ A_inv                    # Batch of identity matrices\n\n[Out]\n\ntensor([[[ 1.0000e+00, -6.0486e-07,  1.3859e-06],\n         [ 5.5627e-08,  1.0000e+00,  1.0795e-06],\n         [-1.4133e-07,  7.9992e-08,  1.0000e+00]],\n        [[ 1.0000e+00,  4.3329e-08, -3.6741e-09],\n         [-7.4627e-08,  1.0000e+00,  1.4579e-07],\n         [-6.3580e-08,  8.2354e-08,  1.0000e+00]]])"
  },
  {
    "objectID": "ml/torchtensors_slides.html#other-linear-algebra-functions",
    "href": "ml/torchtensors_slides.html#other-linear-algebra-functions",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Other linear algebra functions",
    "text": "Other linear algebra functions\ntorch.linalg contains many more functions:\n\ntorch.tensordot which generalizes matrix products\ntorch.linalg.tensorsolve which computes the solution X to the system torch.tensordot(A, X) = B\ntorch.linalg.eigvals which computes the eigenvalues of a square matrix\nand more"
  },
  {
    "objectID": "ml/torchtensors_slides.html#outline-7",
    "href": "ml/torchtensors_slides.html#outline-7",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Outline",
    "text": "Outline\n\nWhat is a PyTorch tensor?\nMemory storage\nData type (dtype)\nBasic operations\nWorking with NumPy\nLinear algebra\nHarvesting the power of GPUs\nDistributed operations"
  },
  {
    "objectID": "ml/torchtensors_slides.html#device-attribute",
    "href": "ml/torchtensors_slides.html#device-attribute",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Device attribute",
    "text": "Device attribute\nTensor data can be placed in the memory of various processor types:\n\nthe RAM of CPU\nthe RAM of a GPU with CUDA support\nthe RAM of a GPU with AMD‚Äôs ROCm support\nthe RAM of an XLA device (e.g.¬†Cloud TPU) with the torch_xla package"
  },
  {
    "objectID": "ml/torchtensors_slides.html#device-attribute-1",
    "href": "ml/torchtensors_slides.html#device-attribute-1",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Device attribute",
    "text": "Device attribute\nThe values for the device attributes are:\n\nCPU: ¬†'cpu'\nGPU (CUDA & AMD‚Äôs ROCm): ¬†'cuda'\nXLA: ¬†xm.xla_device()\n\nThis last option requires to load the torch_xla package first:\nimport torch_xla\nimport torch_xla.core.xla_model as xm"
  },
  {
    "objectID": "ml/torchtensors_slides.html#creating-a-tensor-on-a-specific-device",
    "href": "ml/torchtensors_slides.html#creating-a-tensor-on-a-specific-device",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Creating a tensor on a specific device",
    "text": "Creating a tensor on a specific device\nBy default, tensors are created on the CPU\nt1 = torch.rand(2); print(t1)\n\n[Out]\n\ntensor([0.1606, 0.9771])  # Implicit: device='cpu'\n\nPrinted tensors only display attributes with values ‚â† default values"
  },
  {
    "objectID": "ml/torchtensors_slides.html#creating-a-tensor-on-a-specific-device-1",
    "href": "ml/torchtensors_slides.html#creating-a-tensor-on-a-specific-device-1",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Creating a tensor on a specific device",
    "text": "Creating a tensor on a specific device\nYou can create a tensor on an accelerator by specifying the device attribute\nt2_gpu = torch.rand(2, device='cuda'); print(t2_gpu)\n\n[Out]\n\ntensor([0.0664, 0.7829], device='cuda:0')  # :0 means the 1st GPU"
  },
  {
    "objectID": "ml/torchtensors_slides.html#copying-a-tensor-to-a-specific-device",
    "href": "ml/torchtensors_slides.html#copying-a-tensor-to-a-specific-device",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Copying a tensor to a specific device",
    "text": "Copying a tensor to a specific device\nYou can also make copies of a tensor on other devices\n# Make a copy of t1 on the GPU\nt1_gpu = t1.to(device='cuda'); print(t1_gpu)\nt1_gpu = t1.cuda()  # Same as above written differently\n\n# Make a copy of t2_gpu on the CPU\nt2 = t2_gpu.to(device='cpu'); print(t2)\nt2 = t2_gpu.cpu()   # For the altenative form\n\n[Out]\n\ntensor([0.1606, 0.9771], device='cuda:0')\ntensor([0.0664, 0.7829]) # Implicit: device='cpu'"
  },
  {
    "objectID": "ml/torchtensors_slides.html#multiple-gpus",
    "href": "ml/torchtensors_slides.html#multiple-gpus",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Multiple GPUs",
    "text": "Multiple GPUs\nIf you have multiple GPUs, you can optionally specify which one a tensor should be created on or copied to\nt3_gpu = torch.rand(2, device='cuda:0')  # Create a tensor on 1st GPU\nt4_gpu = t1.to(device='cuda:0')          # Make a copy of t1 on 1st GPU\nt5_gpu = t1.to(device='cuda:1')          # Make a copy of t1 on 2nd GPU\n\nOr the equivalent short forms for the last two:\nt4_gpu = t1.cuda(0)\nt5_gpu = t1.cuda(1)"
  },
  {
    "objectID": "ml/torchtensors_slides.html#timing",
    "href": "ml/torchtensors_slides.html#timing",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Timing",
    "text": "Timing\nLet‚Äôs compare the timing of some matrix multiplications on CPU & GPU with PyTorch built-in benchmark utility\n# Load utility\nimport torch.utils.benchmark as benchmark\n# Define tensors on the CPU\nA = torch.randn(500, 500)\nB = torch.randn(500, 500)\n# Define tensors on the GPU\nA_gpu = torch.randn(500, 500, device='cuda')\nB_gpu = torch.randn(500, 500, device='cuda')\n\nI ran the code on my laptop with a dedicated GPU & 32GB RAM"
  },
  {
    "objectID": "ml/torchtensors_slides.html#timing-1",
    "href": "ml/torchtensors_slides.html#timing-1",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Timing",
    "text": "Timing\nLet‚Äôs time 100 runs to have a reliable benchmark\nt0 = benchmark.Timer(\n    stmt='A @ B',\n    globals={'A': A, 'B': B})\n\nt1 = benchmark.Timer(\n    stmt='A_gpu @ B_gpu',\n    globals={'A_gpu': A_gpu, 'B_gpu': B_gpu})\n\nprint(t0.timeit(100))\nprint(t1.timeit(100))"
  },
  {
    "objectID": "ml/torchtensors_slides.html#timing-2",
    "href": "ml/torchtensors_slides.html#timing-2",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Timing",
    "text": "Timing\n\n[Out]\n\nA @ B\n  2.29 ms\n  1 measurement, 100 runs , 1 thread\n\nA_gpu @ B_gpu\n  108.02 us\n  1 measurement, 100 runs , 1 thread\nSpeedup:\n(2.29 * 10**-3)/(108.02 * 10**-6) = 21\nThis computation was 21 times faster on my GPU than on CPU"
  },
  {
    "objectID": "ml/torchtensors_slides.html#timing-3",
    "href": "ml/torchtensors_slides.html#timing-3",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Timing",
    "text": "Timing\nBy replacing 500 with 5000, we get:\nA @ B\n  2.21 s\n  1 measurement, 100 runs , 1 thread\n\nA_gpu @ B_gpu\n  57.88 ms\n  1 measurement, 100 runs , 1 thread\nSpeedup:\n2.21/(57.88 * 10**-3) = 38\nThe larger the computation, the greater the benefit: now 38 times faster"
  },
  {
    "objectID": "ml/torchtensors_slides.html#outline-8",
    "href": "ml/torchtensors_slides.html#outline-8",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Outline",
    "text": "Outline\n\nWhat is a PyTorch tensor?\nMemory storage\nData type (dtype)\nBasic operations\nWorking with NumPy\nLinear algebra\nHarvesting the power of GPUs\nDistributed operations"
  },
  {
    "objectID": "ml/torchtensors_slides.html#parallel-tensor-operations",
    "href": "ml/torchtensors_slides.html#parallel-tensor-operations",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "Parallel tensor operations",
    "text": "Parallel tensor operations\nPyTorch already allows for distributed training of ML models\nThe implementation of distributed tensor operations‚Äîfor instance for linear algebra‚Äîis in the work through the use of a ShardedTensor primitive that can be sharded across nodes\nSee also this issue for more comments about upcoming developments on (among other things) tensor sharding"
  },
  {
    "objectID": "ml/upscaling.html",
    "href": "ml/upscaling.html",
    "title": "Image upscaling",
    "section": "",
    "text": "Super-resolution‚Äîthe process of (re)creating high resolution images from low resolution ones‚Äîis an old field, but deep neural networks have seen a sudden surge of new and very impressive methods over the past 10 years, from SRCCN to SRGAN to Transformers.\nIn this webinar, I will give a quick overview of these methods and show how the latest state-of-the-art model‚ÄîSwinIR‚Äîperforms on a few test images. We will use PyTorch as our framework.\n\nSlides (click and wait: my reveal.js presentations are heavy and take some time to load‚Ä¶)"
  },
  {
    "objectID": "ml/upscaling_slides.html#can-be-broken-down-into-2-main-periods",
    "href": "ml/upscaling_slides.html#can-be-broken-down-into-2-main-periods",
    "title": "Super-resolution with PyTorch",
    "section": "Can be broken down into 2 main periods:",
    "text": "Can be broken down into 2 main periods:\n\n\nA rather slow history with various interpolation algorithms of increasing complexity before deep neural networks\nAn incredibly fast evolution since the advent of deep learning (DL)"
  },
  {
    "objectID": "ml/upscaling_slides.html#sr-history-pre-dl",
    "href": "ml/upscaling_slides.html#sr-history-pre-dl",
    "title": "Super-resolution with PyTorch",
    "section": "SR history Pre-DL",
    "text": "SR history Pre-DL\n\nPixel-wise interpolation prior to DL\nVarious methods ranging from simple (e.g.¬†nearest-neighbour, bicubic) to complex (e.g.¬†Gaussian process regression, iterative FIR Wiener filter) algorithms"
  },
  {
    "objectID": "ml/upscaling_slides.html#sr-history-pre-dl-1",
    "href": "ml/upscaling_slides.html#sr-history-pre-dl-1",
    "title": "Super-resolution with PyTorch",
    "section": "SR history Pre-DL",
    "text": "SR history Pre-DL\n\nNearest-neighbour interpolation\nSimplest method of interpolation\nSimply uses the value of the nearest pixel\nBicubic interpolation\nConsists of determining the 16 coefficients \\(a_{ij}\\) in:\n\\[p(x, y) = \\sum_{i=0}^3\\sum_{i=0}^3 a\\_{ij} x^i y^j\\]"
  },
  {
    "objectID": "ml/upscaling_slides.html#sr-history-with-dl",
    "href": "ml/upscaling_slides.html#sr-history-with-dl",
    "title": "Super-resolution with PyTorch",
    "section": "SR history with DL",
    "text": "SR history with DL\n\nDeep learning has seen a fast evolution marked by the successive emergence of various frameworks and architectures over the past 10 years\nSome key network architectures and frameworks:\n\nCNN\nGAN\nTransformers\n\nThese have all been applied to SR"
  },
  {
    "objectID": "ml/upscaling_slides.html#srcnn",
    "href": "ml/upscaling_slides.html#srcnn",
    "title": "Super-resolution with PyTorch",
    "section": "SRCNN",
    "text": "SRCNN\n\n\nDong, C., Loy, C. C., He, K., & Tang, X. (2015). Image super-resolution using deep convolutional networks. IEEE transactions on pattern analysis and machine intelligence, 38(2), 295-307\n\n\nGiven a low-resolution image Y, the first convolutional layer of the SRCNN extracts a set of feature maps. The second layer maps these feature maps nonlinearly to high-resolution patch representations. The last layer combines the predictions within a spatial neighbourhood to produce the final high-resolution image F(Y)"
  },
  {
    "objectID": "ml/upscaling_slides.html#srcnn-1",
    "href": "ml/upscaling_slides.html#srcnn-1",
    "title": "Super-resolution with PyTorch",
    "section": "SRCNN",
    "text": "SRCNN\nCan use sparse-coding-based methods\n\n\nDong, C., Loy, C. C., He, K., & Tang, X. (2015). Image super-resolution using deep convolutional networks. IEEE transactions on pattern analysis and machine intelligence, 38(2), 295-307"
  },
  {
    "objectID": "ml/upscaling_slides.html#srgan",
    "href": "ml/upscaling_slides.html#srgan",
    "title": "Super-resolution with PyTorch",
    "section": "SRGAN",
    "text": "SRGAN\n\nDo not provide the best PSNR, but can give more realistic results by providing more texture (less smoothing)"
  },
  {
    "objectID": "ml/upscaling_slides.html#gan",
    "href": "ml/upscaling_slides.html#gan",
    "title": "Super-resolution with PyTorch",
    "section": "GAN",
    "text": "GAN\n\n\nStevens E., Antiga L., & Viehmann T. (2020). Deep Learning with PyTorch"
  },
  {
    "objectID": "ml/upscaling_slides.html#srgan-1",
    "href": "ml/upscaling_slides.html#srgan-1",
    "title": "Super-resolution with PyTorch",
    "section": "SRGAN",
    "text": "SRGAN\n\n\nLedig, C., Theis, L., Husz√°r, F., Caballero, J., Cunningham, A., Acosta, A., ‚Ä¶ & Shi, W. (2017). Photo-realistic single image super-resolution using a generative adversarial network. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp.¬†4681-4690)"
  },
  {
    "objectID": "ml/upscaling_slides.html#srgan-2",
    "href": "ml/upscaling_slides.html#srgan-2",
    "title": "Super-resolution with PyTorch",
    "section": "SRGAN",
    "text": "SRGAN\n\nFollowed by the ESRGAN and many other flavours of SRGANs"
  },
  {
    "objectID": "ml/upscaling_slides.html#attention",
    "href": "ml/upscaling_slides.html#attention",
    "title": "Super-resolution with PyTorch",
    "section": "Attention",
    "text": "Attention\n\n\nMnih, V., Heess, N., & Graves, A. (2014). Recurrent models of visual attention. In Advances in neural information processing systems (pp.¬†2204-2212)\n\n(cited 2769 times)\n\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ‚Ä¶ & Polosukhin, I. (2017). Attention is all you need. In Advances in neural information processing systems (pp.¬†5998-6008)\n\n(cited 30999 times‚Ä¶)"
  },
  {
    "objectID": "ml/upscaling_slides.html#transformers",
    "href": "ml/upscaling_slides.html#transformers",
    "title": "Super-resolution with PyTorch",
    "section": "Transformers",
    "text": "Transformers\n\n\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ‚Ä¶ & Polosukhin, I. (2017). Attention is all you need. In Advances in neural information processing systems (pp.¬†5998-6008)"
  },
  {
    "objectID": "ml/upscaling_slides.html#transformers-1",
    "href": "ml/upscaling_slides.html#transformers-1",
    "title": "Super-resolution with PyTorch",
    "section": "Transformers",
    "text": "Transformers\n\nInitially used for NLP to replace RNN as they allow parallelization Now entering the domain of vision and others Very performant with relatively few parameters"
  },
  {
    "objectID": "ml/upscaling_slides.html#swin-transformer",
    "href": "ml/upscaling_slides.html#swin-transformer",
    "title": "Super-resolution with PyTorch",
    "section": "Swin Transformer",
    "text": "Swin Transformer\n\nThe Swin Transformer improved the use of transformers to the vision domain\nSwin = Shifted WINdows"
  },
  {
    "objectID": "ml/upscaling_slides.html#swin-transformer-1",
    "href": "ml/upscaling_slides.html#swin-transformer-1",
    "title": "Super-resolution with PyTorch",
    "section": "Swin Transformer",
    "text": "Swin Transformer\nSwin transformer (left) vs transformer as initially applied to vision (right):\n\n\nLiu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., ‚Ä¶ & Guo, B. (2021). Swin transformer: Hierarchical vision transformer using shifted windows. arXiv preprint arXiv:2103.14030"
  },
  {
    "objectID": "ml/upscaling_slides.html#swinir-1",
    "href": "ml/upscaling_slides.html#swinir-1",
    "title": "Super-resolution with PyTorch",
    "section": "SwinIR",
    "text": "SwinIR\n\n\n\nLiang, J., Cao, J., Sun, G., Zhang, K., Van Gool, L., & Timofte, R. (2021). SwinIR: Image restoration using swin transformer. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp.¬†1833-1844)"
  },
  {
    "objectID": "ml/upscaling_slides.html#training-sets-used",
    "href": "ml/upscaling_slides.html#training-sets-used",
    "title": "Super-resolution with PyTorch",
    "section": "Training sets used",
    "text": "Training sets used\n\nDIV2K, Flickr2K, and other datasets"
  },
  {
    "objectID": "ml/upscaling_slides.html#models-assessment",
    "href": "ml/upscaling_slides.html#models-assessment",
    "title": "Super-resolution with PyTorch",
    "section": "Models assessment",
    "text": "Models assessment\n\n3 metrics commonly used:\nPeak sign-to-noise ratio (PSNR) measured in dB\n\\(\\frac{\\text{Maximum possible power of signal}}{\\text{Power of noise (calculated as the mean squared error)}}\\)\nCalculated at the pixel level\nStructural similarity index measure (SSIM)\nPrediction of perceived image quality based on a ‚Äúperfect‚Äù reference image\nMean opinion score (MOS)\nMean of subjective quality ratings"
  },
  {
    "objectID": "ml/upscaling_slides.html#models-assessment-1",
    "href": "ml/upscaling_slides.html#models-assessment-1",
    "title": "Super-resolution with PyTorch",
    "section": "Models assessment",
    "text": "Models assessment\n\nPeak sign-to-noise ratio (PSNR) measured in dB\n\\[PSNR = 10\\,\\cdot\\,log_{10}\\,\\left(\\frac{MAX_I^2}{MSE}\\right)\\]\nStructural similarity index measure (SSIM)\n\\[SSIM(x,y) = \\frac{(2\\mu_x\\mu_y + c_1) + (2 \\sigma _{xy} + c_2)}\n    {(\\mu_x^2 + \\mu_y^2+c_1) (\\sigma_x^2 + \\sigma_y^2+c_2)}\\]\nMean opinion score (MOS)\n\\[MOS = \\frac{\\sum_{n=1}^N R\\_n}{N}\\]"
  },
  {
    "objectID": "ml/upscaling_slides.html#metrics-implementation",
    "href": "ml/upscaling_slides.html#metrics-implementation",
    "title": "Super-resolution with PyTorch",
    "section": "Metrics implementation",
    "text": "Metrics implementation\n\n\nImplement them yourself (using torch.log10, etc.)\nUse some library that implements them (e.g.¬†kornia)\nUse code of open source project with good implementation (e.g.¬†SwinIR)\nUse some higher level library that provides them (e.g.¬†ignite)"
  },
  {
    "objectID": "ml/upscaling_slides.html#metrics-implementation-1",
    "href": "ml/upscaling_slides.html#metrics-implementation-1",
    "title": "Super-resolution with PyTorch",
    "section": "Metrics implementation",
    "text": "Metrics implementation\n\n\nImplement them yourself (using torch.log10, etc.)\nUse some library that implements them (e.g.¬†kornia)\nUse code of open source project with good implementation (e.g.¬†SwinIR)\nUse some higher level library that provides them (e.g.¬†ignite)"
  },
  {
    "objectID": "ml/upscaling_slides.html#metrics-implementation-2",
    "href": "ml/upscaling_slides.html#metrics-implementation-2",
    "title": "Super-resolution with PyTorch",
    "section": "Metrics implementation",
    "text": "Metrics implementation\n\nimport kornia\n\npsnr_value = kornia.metrics.psnr(input, target, max_val)\nssim_value = kornia.metrics.ssim(img1, img2, window_size, max_val=1.0, eps=1e-12)\nSee the Kornia documentation for more info on kornia.metrics.psnr & kornia.metrics.ssim"
  },
  {
    "objectID": "ml/upscaling_slides.html#benchmark-datasets",
    "href": "ml/upscaling_slides.html#benchmark-datasets",
    "title": "Super-resolution with PyTorch",
    "section": "Benchmark datasets",
    "text": "Benchmark datasets\nSet5\n\nSet14\n\nBSD100 (Berkeley Segmentation Dataset)"
  },
  {
    "objectID": "ml/upscaling_slides.html#benchmark-datasets-1",
    "href": "ml/upscaling_slides.html#benchmark-datasets-1",
    "title": "Super-resolution with PyTorch",
    "section": "Benchmark datasets",
    "text": "Benchmark datasets\nSet5\n\nSet14\n\nBSD100 (Berkeley Segmentation Dataset)"
  },
  {
    "objectID": "ml/upscaling_slides.html#the-set5-dataset",
    "href": "ml/upscaling_slides.html#the-set5-dataset",
    "title": "Super-resolution with PyTorch",
    "section": "The Set5 dataset",
    "text": "The Set5 dataset\n\nA dataset consisting of 5 images which has been used for at least 18 years to assess SR methods"
  },
  {
    "objectID": "ml/upscaling_slides.html#how-to-get-the-dataset",
    "href": "ml/upscaling_slides.html#how-to-get-the-dataset",
    "title": "Super-resolution with PyTorch",
    "section": "How to get the dataset?",
    "text": "How to get the dataset?\n\nFrom the HuggingFace Datasets Hub with the HuggingFace datasets package:\nfrom datasets import load_dataset\n\nset5 = load_dataset('eugenesiow/Set5', 'bicubic_x4', split='validation')"
  },
  {
    "objectID": "ml/upscaling_slides.html#dataset-exploration",
    "href": "ml/upscaling_slides.html#dataset-exploration",
    "title": "Super-resolution with PyTorch",
    "section": "Dataset exploration",
    "text": "Dataset exploration\n\nprint(set5)\nlen(set5)\nset5[0]\nset5.shape\nset5.column_names\nset5.features\nset5.set_format('torch', columns=['hr', 'lr'])\nset5.format"
  },
  {
    "objectID": "ml/upscaling_slides.html#benchmarks",
    "href": "ml/upscaling_slides.html#benchmarks",
    "title": "Super-resolution with PyTorch",
    "section": "Benchmarks",
    "text": "Benchmarks\n\nA 2012 review of interpolation methods for SR gives the metrics for a series of interpolation methods (using other datasets)"
  },
  {
    "objectID": "ml/upscaling_slides.html#interpolation-methods",
    "href": "ml/upscaling_slides.html#interpolation-methods",
    "title": "Super-resolution with PyTorch",
    "section": "Interpolation methods",
    "text": "Interpolation methods"
  },
  {
    "objectID": "ml/upscaling_slides.html#dl-methods",
    "href": "ml/upscaling_slides.html#dl-methods",
    "title": "Super-resolution with PyTorch",
    "section": "DL methods",
    "text": "DL methods\n\nThe Papers with Code website lists available benchmarks on Set5"
  },
  {
    "objectID": "ml/upscaling_slides.html#lets-use-swinir",
    "href": "ml/upscaling_slides.html#lets-use-swinir",
    "title": "Super-resolution with PyTorch",
    "section": "Let‚Äôs use SwinIR",
    "text": "Let‚Äôs use SwinIR\n\n# Get the model\ngit clone git@github.com:JingyunLiang/SwinIR.git\ncd SwinIR\n\n# Copy our test images in the repo\ncp -r <some/path>/my_tests /testsets/my_tests\n\n# Run the model on our images\npython main_test_swinir.py --tile 400 --task real_sr --scale 4 --large_model --model_path model_zoo/swinir/003_realSR_BSRGAN_DFOWMFC_s64w8_SwinIR-L_x4_GAN.pth --folder_lq testsets/my_tests\nRan in 9 min on my machine with one GPU and 32GB of RAM"
  },
  {
    "objectID": "ml/upscaling_slides.html#results",
    "href": "ml/upscaling_slides.html#results",
    "title": "Super-resolution with PyTorch",
    "section": "Results",
    "text": "Results"
  },
  {
    "objectID": "ml/upscaling_slides.html#results-1",
    "href": "ml/upscaling_slides.html#results-1",
    "title": "Super-resolution with PyTorch",
    "section": "Results",
    "text": "Results"
  },
  {
    "objectID": "ml/upscaling_slides.html#results-2",
    "href": "ml/upscaling_slides.html#results-2",
    "title": "Super-resolution with PyTorch",
    "section": "Results",
    "text": "Results"
  },
  {
    "objectID": "ml/upscaling_slides.html#results-3",
    "href": "ml/upscaling_slides.html#results-3",
    "title": "Super-resolution with PyTorch",
    "section": "Results",
    "text": "Results"
  },
  {
    "objectID": "ml/upscaling_slides.html#results-4",
    "href": "ml/upscaling_slides.html#results-4",
    "title": "Super-resolution with PyTorch",
    "section": "Results",
    "text": "Results"
  },
  {
    "objectID": "ml/upscaling_slides.html#results-5",
    "href": "ml/upscaling_slides.html#results-5",
    "title": "Super-resolution with PyTorch",
    "section": "Results",
    "text": "Results"
  },
  {
    "objectID": "ml/upscaling_slides.html#results-6",
    "href": "ml/upscaling_slides.html#results-6",
    "title": "Super-resolution with PyTorch",
    "section": "Results",
    "text": "Results"
  },
  {
    "objectID": "ml/upscaling_slides.html#results-7",
    "href": "ml/upscaling_slides.html#results-7",
    "title": "Super-resolution with PyTorch",
    "section": "Results",
    "text": "Results"
  },
  {
    "objectID": "ml/upscaling_slides.html#results-8",
    "href": "ml/upscaling_slides.html#results-8",
    "title": "Super-resolution with PyTorch",
    "section": "Results",
    "text": "Results"
  },
  {
    "objectID": "ml/upscaling_slides.html#results-9",
    "href": "ml/upscaling_slides.html#results-9",
    "title": "Super-resolution with PyTorch",
    "section": "Results",
    "text": "Results"
  },
  {
    "objectID": "ml/upscaling_slides.html#metrics",
    "href": "ml/upscaling_slides.html#metrics",
    "title": "Super-resolution with PyTorch",
    "section": "Metrics",
    "text": "Metrics\n\nWe could use the PSNR and SSIM implementations from SwinIR, but let‚Äôs try the Kornia functions we mentioned earlier:\n\nkornia.metrics.psnr\nkornia.metrics.ssim"
  },
  {
    "objectID": "ml/upscaling_slides.html#metrics-1",
    "href": "ml/upscaling_slides.html#metrics-1",
    "title": "Super-resolution with PyTorch",
    "section": "Metrics",
    "text": "Metrics\n\nLet‚Äôs load the libraries we need:\nimport kornia\nfrom PIL import Image\nimport torch\nfrom torchvision import transforms"
  },
  {
    "objectID": "ml/upscaling_slides.html#metrics-2",
    "href": "ml/upscaling_slides.html#metrics-2",
    "title": "Super-resolution with PyTorch",
    "section": "Metrics",
    "text": "Metrics\n\nThen, we load one pair images (LR and HR):\nberlin1_lr = Image.open(\"<some/path>/lr/berlin_1945_1.jpg\")\nberlin1_hr = Image.open(\"<some/path>/hr/berlin_1945_1.png\")\n\nWe can display these images with:\nberlin1_lr.show()\nberlin1_hr.show()"
  },
  {
    "objectID": "ml/upscaling_slides.html#metrics-3",
    "href": "ml/upscaling_slides.html#metrics-3",
    "title": "Super-resolution with PyTorch",
    "section": "Metrics",
    "text": "Metrics\n\nNow, we need to resize them so that they have identical dimensions and turn them into tensors:\npreprocess = transforms.Compose([\n        transforms.Resize(256),\n        transforms.ToTensor()\n        ])\n\nberlin1_lr_t = preprocess(berlin1_lr)\nberlin1_hr_t = preprocess(berlin1_hr)"
  },
  {
    "objectID": "ml/upscaling_slides.html#metrics-4",
    "href": "ml/upscaling_slides.html#metrics-4",
    "title": "Super-resolution with PyTorch",
    "section": "Metrics",
    "text": "Metrics\n\nberlin1_lr_t.shape\nberlin1_hr_t.shape\n\n[Out]\n\ntorch.Size([3, 267, 256])\ntorch.Size([3, 267, 256])\nWe now have tensors with 3 dimensions:\n\nthe channels (RGB)\nthe height of the image (in pixels)\nthe width of the image (in pixels)"
  },
  {
    "objectID": "ml/upscaling_slides.html#metrics-5",
    "href": "ml/upscaling_slides.html#metrics-5",
    "title": "Super-resolution with PyTorch",
    "section": "Metrics",
    "text": "Metrics\n\nAs data processing is done in batch in ML, we need to add a 4th dimension: the batch size\n(It will be equal to 1 since we have a batch size of a single image)\nbatch_berlin1_lr_t = torch.unsqueeze(berlin1_lr_t, 0)\nbatch_berlin1_hr_t = torch.unsqueeze(berlin1_hr_t, 0)"
  },
  {
    "objectID": "ml/upscaling_slides.html#metrics-6",
    "href": "ml/upscaling_slides.html#metrics-6",
    "title": "Super-resolution with PyTorch",
    "section": "Metrics",
    "text": "Metrics\n\nOur new tensors are now ready:\nbatch_berlin1_lr_t.shape\nbatch_berlin1_hr_t.shape\n\n[Out]\n\ntorch.Size([1, 3, 267, 256])\ntorch.Size([1, 3, 267, 256])"
  },
  {
    "objectID": "ml/upscaling_slides.html#psnr",
    "href": "ml/upscaling_slides.html#psnr",
    "title": "Super-resolution with PyTorch",
    "section": "PSNR",
    "text": "PSNR\n\npsnr_value = kornia.metrics.psnr(batch_berlin1_lr_t, batch_berlin1_hr_t, max_val=1.0)\npsnr_value.item()\n\n[Out]\n\n33.379642486572266"
  },
  {
    "objectID": "ml/upscaling_slides.html#ssim",
    "href": "ml/upscaling_slides.html#ssim",
    "title": "Super-resolution with PyTorch",
    "section": "SSIM",
    "text": "SSIM\n\nssim_map = kornia.metrics.ssim(batch_berlin1_lr_t, batch_berlin1_hr_t, window_size=5, max_val=1.0, eps=1e-12)\nssim_map.mean().item()\n\n[Out]\n\n0.9868119359016418"
  },
  {
    "objectID": "newsletter.html",
    "href": "newsletter.html",
    "title": "Training events mailing list",
    "section": "",
    "text": "If you want to get informed about upcoming training events, please subscribe to our mailing list: \n(We will only email you about training events.)"
  },
  {
    "objectID": "r/basics.html",
    "href": "r/basics.html",
    "title": "R: the basics",
    "section": "",
    "text": "For some general documentation on R, you can run:\nhelp.start()\nTo get help on a function (e.g.¬†sum), you can run:\nhelp(sum)\nDepending on your settings, this will open a documentation for sum in a pager or in your browser."
  },
  {
    "objectID": "r/basics.html#atomic-vectors",
    "href": "r/basics.html#atomic-vectors",
    "title": "R: the basics",
    "section": "Atomic vectors",
    "text": "Atomic vectors\n\nWith a single element\n\na <- 2\na\n\n[1] 2\n\ntypeof(a)\n\n[1] \"double\"\n\nstr(a)\n\n num 2\n\nlength(a)\n\n[1] 1\n\ndim(a)\n\nNULL\n\n\nThe dim attribute of a vector doesn‚Äôt exist (hence the NULL). This makes vectors different from one-dimensional arrays which have a dim of 1.\nYou might have noticed that 2 is a double (double precision floating point number, equivalent of ‚Äúfloat‚Äù in other languages). In R, this is the default, even if you don‚Äôt type 2.0. This prevents the kind of weirdness you can find in, for instance, Python.\nIn Python:\n>>> 2 == 2.0\nTrue\n>>> type(2) == type(2.0)\nFalse\n>>> type(2)\n<class 'int'>\n>>> type(2.0)\n<class 'float'>\nIn R:\n> 2 == 2.0\n[1] TRUE\n> typeof(2) == typeof(2.0)\n[1] TRUE\n> typeof(2)\n[1] \"double\"\n> typeof(2.0)\n[1] \"double\"\nIf you want to define an integer variable, you use:\n\nb <- 2L\nb\n\n[1] 2\n\ntypeof(b)\n\n[1] \"integer\"\n\nmode(b)\n\n[1] \"numeric\"\n\nstr(b)\n\n int 2\n\n\nThere are six vector types:\n\nlogical\ninteger\ndouble\ncharacter\ncomplex\nraw\n\n\n\nWith multiple elements\n\nc <- c(2, 4, 1)\nc\n\n[1] 2 4 1\n\ntypeof(c)\n\n[1] \"double\"\n\nmode(c)\n\n[1] \"numeric\"\n\nstr(c)\n\n num [1:3] 2 4 1\n\n\n\nd <- c(TRUE, TRUE, NA, FALSE)\nd\n\n[1]  TRUE  TRUE    NA FALSE\n\ntypeof(d)\n\n[1] \"logical\"\n\nstr(d)\n\n logi [1:4] TRUE TRUE NA FALSE\n\n\nNA (‚ÄúNot Available‚Äù) is a logical constant of length one. It is an indicator for a missing value.\nVectors are homogeneous, so all elements need to be of the same type.\nIf you use elements of different types, R will convert some of them to ensure that they become of the same type:\n\ne <- c(\"This is a string\", 3, \"test\")\ne\n\n[1] \"This is a string\" \"3\"                \"test\"            \n\ntypeof(e)\n\n[1] \"character\"\n\nstr(e)\n\n chr [1:3] \"This is a string\" \"3\" \"test\"\n\n\n\nf <- c(TRUE, 3, FALSE)\nf\n\n[1] 1 3 0\n\ntypeof(f)\n\n[1] \"double\"\n\nstr(f)\n\n num [1:3] 1 3 0\n\n\n\ng <- c(2L, 3, 4L)\ng\n\n[1] 2 3 4\n\ntypeof(g)\n\n[1] \"double\"\n\nstr(g)\n\n num [1:3] 2 3 4\n\n\n\nh <- c(\"string\", TRUE, 2L, 3.1)\nh\n\n[1] \"string\" \"TRUE\"   \"2\"      \"3.1\"   \n\ntypeof(h)\n\n[1] \"character\"\n\nstr(h)\n\n chr [1:4] \"string\" \"TRUE\" \"2\" \"3.1\"\n\n\nThe binary operator : is equivalent to the seq() function and generates a regular sequence of integers:\n\ni <- 1:5\ni\n\n[1] 1 2 3 4 5\n\ntypeof(i)\n\n[1] \"integer\"\n\nstr(i)\n\n int [1:5] 1 2 3 4 5\n\nidentical(2:8, seq(2, 8))\n\n[1] TRUE"
  },
  {
    "objectID": "r/basics.html#matrices",
    "href": "r/basics.html#matrices",
    "title": "R: the basics",
    "section": "Matrices",
    "text": "Matrices\n\nj <- matrix(1:12, nrow = 3, ncol = 4)\nj\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    4    7   10\n[2,]    2    5    8   11\n[3,]    3    6    9   12\n\ntypeof(j)\n\n[1] \"integer\"\n\nstr(j)\n\n int [1:3, 1:4] 1 2 3 4 5 6 7 8 9 10 ...\n\nlength(j)\n\n[1] 12\n\ndim(j)\n\n[1] 3 4\n\n\nThe default is byrow = FALSE. If you want the matrix to be filled in by row, you need to set this argument to TRUE:\n\nk <- matrix(1:12, nrow = 3, ncol = 4, byrow = TRUE)\nk\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    2    3    4\n[2,]    5    6    7    8\n[3,]    9   10   11   12"
  },
  {
    "objectID": "r/basics.html#arrays",
    "href": "r/basics.html#arrays",
    "title": "R: the basics",
    "section": "Arrays",
    "text": "Arrays\n\nl <- array(as.double(1:24), c(3, 2, 4))\nl\n\n, , 1\n\n     [,1] [,2]\n[1,]    1    4\n[2,]    2    5\n[3,]    3    6\n\n, , 2\n\n     [,1] [,2]\n[1,]    7   10\n[2,]    8   11\n[3,]    9   12\n\n, , 3\n\n     [,1] [,2]\n[1,]   13   16\n[2,]   14   17\n[3,]   15   18\n\n, , 4\n\n     [,1] [,2]\n[1,]   19   22\n[2,]   20   23\n[3,]   21   24\n\ntypeof(l)\n\n[1] \"double\"\n\nstr(l)\n\n num [1:3, 1:2, 1:4] 1 2 3 4 5 6 7 8 9 10 ...\n\nlength(l)\n\n[1] 24\n\ndim(l)\n\n[1] 3 2 4"
  },
  {
    "objectID": "r/basics.html#lists",
    "href": "r/basics.html#lists",
    "title": "R: the basics",
    "section": "Lists",
    "text": "Lists\n\nm <- list(2, 3)\nm\n\n[[1]]\n[1] 2\n\n[[2]]\n[1] 3\n\ntypeof(m)\n\n[1] \"list\"\n\nstr(m)\n\nList of 2\n $ : num 2\n $ : num 3\n\nlength(m)\n\n[1] 2\n\ndim(m)\n\nNULL\n\n\nAs with atomic vectors, lists do not have a dim attribute. Lists are in fact a different type of vectors.\nLists can be heterogeneous:\n\nn <- list(2L, 3, c(2, 1), FALSE, \"string\")\nn\n\n[[1]]\n[1] 2\n\n[[2]]\n[1] 3\n\n[[3]]\n[1] 2 1\n\n[[4]]\n[1] FALSE\n\n[[5]]\n[1] \"string\"\n\ntypeof(n)\n\n[1] \"list\"\n\nstr(n)\n\nList of 5\n $ : int 2\n $ : num 3\n $ : num [1:2] 2 1\n $ : logi FALSE\n $ : chr \"string\"\n\nlength(n)\n\n[1] 5"
  },
  {
    "objectID": "r/basics.html#data-frames",
    "href": "r/basics.html#data-frames",
    "title": "R: the basics",
    "section": "Data frames",
    "text": "Data frames\nData frames contain tabular data. Under the hood, a data frame is a list of vectors.\n\no <- data.frame(\n  country = c(\"Canada\", \"USA\", \"Mexico\"),\n  var = c(2.9, 3.1, 4.5)\n)\no\n\n  country var\n1  Canada 2.9\n2     USA 3.1\n3  Mexico 4.5\n\ntypeof(o)\n\n[1] \"list\"\n\nstr(o)\n\n'data.frame':   3 obs. of  2 variables:\n $ country: chr  \"Canada\" \"USA\" \"Mexico\"\n $ var    : num  2.9 3.1 4.5\n\nlength(o)\n\n[1] 2\n\ndim(o)\n\n[1] 3 2"
  },
  {
    "objectID": "r/basics.html#conditionals",
    "href": "r/basics.html#conditionals",
    "title": "R: the basics",
    "section": "Conditionals",
    "text": "Conditionals\n\ntest_sign <- function(x) {\n  if (x > 0) {\n    \"x is positif\"\n  } else if (x < 0) {\n    \"x is negatif\"\n  } else {\n    \"x is equal to zero\"\n  }\n}\n\n\ntest_sign(3)\n\n[1] \"x is positif\"\n\ntest_sign(-2)\n\n[1] \"x is negatif\"\n\ntest_sign(0)\n\n[1] \"x is equal to zero\""
  },
  {
    "objectID": "r/basics.html#loops",
    "href": "r/basics.html#loops",
    "title": "R: the basics",
    "section": "Loops",
    "text": "Loops\n\nfor (i in 1:10) {\n  print(i)\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n[1] 7\n[1] 8\n[1] 9\n[1] 10\n\n\nNotice that here we need to use the print() function."
  },
  {
    "objectID": "r/gis_mapping.html",
    "href": "r/gis_mapping.html",
    "title": "GIS mapping in R",
    "section": "",
    "text": "In this webinar, we will see how to create all sorts of GIS maps with the packages sf, tmap, raster, leaflet, ggplot2, grid (part of Base R), and mapview:\n\nsimple maps\ninset maps\nfaceted maps\nanimated maps\ninteractive maps\nraster maps\n\nFinally, we will learn how to add basemaps from OpenStreetMap and Google Maps.\n\nSlides (click and wait: my reveal.js presentations are heavy and take some time to load‚Ä¶)"
  },
  {
    "objectID": "r/gis_mapping_slides.html#types-of-spatial-data",
    "href": "r/gis_mapping_slides.html#types-of-spatial-data",
    "title": "GIS mapping in R",
    "section": "Types of spatial data",
    "text": "Types of spatial data\n\nVector data\n\nDiscrete objects\nContain: ‚ÄÇ- geometry:‚ÄÇ shape & location of the objects\n‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ- attributes:‚ÄÇ additional variables (e.g.¬†name, year, type)\nCommon file format:‚ÄÇ GeoJSON, shapefile\n\nExamples: countries, roads, rivers, towns\n\n\nRaster data\n\nContinuous phenomena or spatial fields\nCommon file formats:‚ÄÇ TIFF, GeoTIFF, NetCDF, Esri grid\n\nExamples: temperature, air quality, elevation, water depth"
  },
  {
    "objectID": "r/gis_mapping_slides.html#vector-data-1",
    "href": "r/gis_mapping_slides.html#vector-data-1",
    "title": "GIS mapping in R",
    "section": "Vector data",
    "text": "Vector data\n\nTypes\n\npoint:‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÇ¬† single set of coordinates\nmulti-point:‚ÄÉ‚ÄÉ multiple sets of coordinates\npolyline:‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÇ¬† multiple sets for which the order matters\nmulti-polyline:‚ÄÉ multiple of the above\npolygon:‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÇ¬† same as polyline but first & last sets are the same\nmulti-polygon:‚ÄÉ multiple of the above"
  },
  {
    "objectID": "r/gis_mapping_slides.html#raster-data-1",
    "href": "r/gis_mapping_slides.html#raster-data-1",
    "title": "GIS mapping in R",
    "section": "Raster data",
    "text": "Raster data\n\nGrid of equally sized rectangular cells containing values for some variables\nSize of cells = resolution\nFor computing efficiency, rasters do not have coordinates of each cell, but the bounding box & the number of rows & columns"
  },
  {
    "objectID": "r/gis_mapping_slides.html#coordinate-reference-systems-crs",
    "href": "r/gis_mapping_slides.html#coordinate-reference-systems-crs",
    "title": "GIS mapping in R",
    "section": "Coordinate Reference Systems (CRS)",
    "text": "Coordinate Reference Systems (CRS)\nA location on Earth‚Äôs surface can be identified by its coordinates & some reference system called CRS\nThe coordinates (x, y) are called longitude & latitude\nThere can be a 3rd coordinate (z) for elevation or other measurement‚Äîusually a vertical one\nAnd a 4th (m) for some other data attribute‚Äîusually a horizontal measurement\nIn 3D, longitude & latitude are expressed in angular units (e.g.¬†degrees) & the reference system needed is an angular CRS or geographic coordinate system (GCS)\nIn 2D, they are expressed in linear units (e.g.¬†meters) & the reference system needed is a planar CRS or projected coordinate system (PCS)"
  },
  {
    "objectID": "r/gis_mapping_slides.html#datums",
    "href": "r/gis_mapping_slides.html#datums",
    "title": "GIS mapping in R",
    "section": "Datums",
    "text": "Datums\nSince the Earth is not a perfect sphere, we use spheroidal models to represent its surface. Those are called geodetic datums\nSome datums are global, others local (more accurate in a particular area of the globe, but only useful there) \n\nExamples of commonly used global datums:\n\nWGS84 (World Geodesic System 1984)\nNAD83 (North American Datum of 1983)"
  },
  {
    "objectID": "r/gis_mapping_slides.html#angular-crs",
    "href": "r/gis_mapping_slides.html#angular-crs",
    "title": "GIS mapping in R",
    "section": "Angular CRS",
    "text": "Angular CRS\nAn angular CRS contains a datum, an angular unit & references such as a prime meridian (e.g.¬†the Royal Observatory, Greenwich, England)\nIn an angular CRS or GCS:\n\nLongitude (\\(\\lambda\\)) represents the angle between the prime meridian & the meridian that passes through that location\nLatitude (\\(\\phi\\)) represents the angle between the line that passes through the center of the Earth & that location & its projection on the equatorial plane\n\nLongitude & latitude are thus angular coordinates"
  },
  {
    "objectID": "r/gis_mapping_slides.html#projections",
    "href": "r/gis_mapping_slides.html#projections",
    "title": "GIS mapping in R",
    "section": "Projections",
    "text": "Projections\nTo create a two-dimensional map, you need to project this 3D angular CRS into a 2D one\nVarious projections offer different characteristics. For instance:\n\nsome respect areas (equal-area)\nsome respect the shape of geographic features (conformal)\nsome almost respect both for small areas\n\nIt is important to choose one with sensible properties for your goals\n\nExamples of projections:\n\nMercator\nUTM\nRobinson"
  },
  {
    "objectID": "r/gis_mapping_slides.html#planar-crs",
    "href": "r/gis_mapping_slides.html#planar-crs",
    "title": "GIS mapping in R",
    "section": "Planar CRS",
    "text": "Planar CRS\nA planar CRS is defined by a datum, a projection & a set of parameters such as a linear unit & the origins\nCommon planar CRS have been assigned a unique ID called EPSG code which is much more convenient to use\nIn a planar CRS, coordinates will not be in degrees anymore but in meters (or other length unit)"
  },
  {
    "objectID": "r/gis_mapping_slides.html#resources",
    "href": "r/gis_mapping_slides.html#resources",
    "title": "GIS mapping in R",
    "section": "Resources",
    "text": "Resources\n\nOpen GIS data\nFree GIS Data: list of free GIS datasets\nBooks\nGeocomputation with R by Robin Lovelace, Jakub Nowosad & Jannes Muenchow\nSpatial Data Science by Edzer Pebesma & Roger Bivand\nSpatial Data Science with R by Robert J. Hijmans\nUsing Spatial Data with R by Claudia A. Engel\nTutorial\nAn Introduction to Spatial Data Analysis and Visualisation in R by the CDRC"
  },
  {
    "objectID": "r/gis_mapping_slides.html#resources-1",
    "href": "r/gis_mapping_slides.html#resources-1",
    "title": "GIS mapping in R",
    "section": "Resources",
    "text": "Resources\n\nWebsite\nr-spatial by Edzer Pebesma, Marius Appel & Daniel N√ºst\nCRAN package list\nAnalysis of Spatial Data\nMailing list\nR Special Interest Group on using Geographical data and Mapping"
  },
  {
    "objectID": "r/gis_mapping_slides.html#data-manipulation",
    "href": "r/gis_mapping_slides.html#data-manipulation",
    "title": "GIS mapping in R",
    "section": "Data manipulation",
    "text": "Data manipulation\n\nOlder packages\n\nsp\nraster\nrgdal\nrgeos\n\nNewer generation\n\nsf: vector data\nterra: raster data (also has vector data capabilities)"
  },
  {
    "objectID": "r/gis_mapping_slides.html#mapping",
    "href": "r/gis_mapping_slides.html#mapping",
    "title": "GIS mapping in R",
    "section": "Mapping",
    "text": "Mapping\n\nStatic maps\n\nggplot2 + ggspatial\ntmap\n\nDynamic maps\n\nleaflet\nggplot2 + gganimate\nmapview\nggmap\ntmap"
  },
  {
    "objectID": "r/gis_mapping_slides.html#sf-simple-features-in-r",
    "href": "r/gis_mapping_slides.html#sf-simple-features-in-r",
    "title": "GIS mapping in R",
    "section": "sf: Simple Features in R",
    "text": "sf: Simple Features in R\nGeospatial vectors: points, lines, polygons\nSimple Features‚Äîdefined by the Open Geospatial Consortium (OGC) & formalized by ISO‚Äîis a set of standards now used by most GIS libraries\nWell-known text (WKT) is a markup language for representing vector geometry objects according to those standards\nA compact computer version also exists‚Äîwell-known binary (WKB)‚Äîused by spatial databases\nThe package sp predates Simple Features\nsf‚Äîlaunched in 2016‚Äîimplements these standards in R in the form of sf objects: data.frames (or tibbles) containing the attributes, extended by sfc objects or simple feature geometries list-columns"
  },
  {
    "objectID": "r/gis_mapping_slides.html#sf",
    "href": "r/gis_mapping_slides.html#sf",
    "title": "GIS mapping in R",
    "section": "sf",
    "text": "sf\n\nUseful links\n\nGitHub repo\nPaper\nResources\nCheatsheet\n6 vignettes: 1, 2, 3, 4, 5, 6"
  },
  {
    "objectID": "r/gis_mapping_slides.html#sf-objects",
    "href": "r/gis_mapping_slides.html#sf-objects",
    "title": "GIS mapping in R",
    "section": "sf objects",
    "text": "sf objects"
  },
  {
    "objectID": "r/gis_mapping_slides.html#sf-objects-1",
    "href": "r/gis_mapping_slides.html#sf-objects-1",
    "title": "GIS mapping in R",
    "section": "sf objects",
    "text": "sf objects"
  },
  {
    "objectID": "r/gis_mapping_slides.html#sf-objects-2",
    "href": "r/gis_mapping_slides.html#sf-objects-2",
    "title": "GIS mapping in R",
    "section": "sf objects",
    "text": "sf objects"
  },
  {
    "objectID": "r/gis_mapping_slides.html#sf-objects-3",
    "href": "r/gis_mapping_slides.html#sf-objects-3",
    "title": "GIS mapping in R",
    "section": "sf objects",
    "text": "sf objects"
  },
  {
    "objectID": "r/gis_mapping_slides.html#sf-objects-4",
    "href": "r/gis_mapping_slides.html#sf-objects-4",
    "title": "GIS mapping in R",
    "section": "sf objects",
    "text": "sf objects"
  },
  {
    "objectID": "r/gis_mapping_slides.html#sf-functions",
    "href": "r/gis_mapping_slides.html#sf-functions",
    "title": "GIS mapping in R",
    "section": "sf functions",
    "text": "sf functions\n\nMost functions start with st_ (which refers to ‚Äúspatial type‚Äù)"
  },
  {
    "objectID": "r/gis_mapping_slides.html#terra-geospatial-rasters",
    "href": "r/gis_mapping_slides.html#terra-geospatial-rasters",
    "title": "GIS mapping in R",
    "section": "terra: Geospatial rasters",
    "text": "terra: Geospatial rasters\n\nFaster and simpler replacement for the raster package by the same team\nMostly implemented in C++\nCan work with datasets too large to be loaded into memory"
  },
  {
    "objectID": "r/gis_mapping_slides.html#terra",
    "href": "r/gis_mapping_slides.html#terra",
    "title": "GIS mapping in R",
    "section": "terra",
    "text": "terra\n\nUseful links\n\nGitHub repo\nResources\nFull manual"
  },
  {
    "objectID": "r/gis_mapping_slides.html#tmap-layered-grammar-of-graphics-gis-maps",
    "href": "r/gis_mapping_slides.html#tmap-layered-grammar-of-graphics-gis-maps",
    "title": "GIS mapping in R",
    "section": "tmap: Layered grammar of graphics GIS maps",
    "text": "tmap: Layered grammar of graphics GIS maps"
  },
  {
    "objectID": "r/gis_mapping_slides.html#tmap",
    "href": "r/gis_mapping_slides.html#tmap",
    "title": "GIS mapping in R",
    "section": "tmap",
    "text": "tmap\n\nUseful links\n\nGitHub repo\nResources\n\nHelp pages and vignettes\n\n?tmap-element\nvignette(\"tmap-getstarted\")\n# All the usual help pages, e.g.:\n?tm_layout"
  },
  {
    "objectID": "r/gis_mapping_slides.html#tmap-functions",
    "href": "r/gis_mapping_slides.html#tmap-functions",
    "title": "GIS mapping in R",
    "section": "tmap functions",
    "text": "tmap functions\n\nMain functions start with tmap_\nFunctions creating map elements start with tm_"
  },
  {
    "objectID": "r/gis_mapping_slides.html#tmap-functioning",
    "href": "r/gis_mapping_slides.html#tmap-functioning",
    "title": "GIS mapping in R",
    "section": "tmap functioning",
    "text": "tmap functioning\nVery similar to ggplot2\nTypically, a map contains:\n\nOne or multiple layer(s) (the order matters as they stack on top of each other)\nSome layout (e.g.¬†customization of title, background, margins): tm_layout\nA compass: tm_compass\nA scale bar: tm_scale_bar\n\nEach layer contains:\n\nSome data: tm_shape\nHow that data will be represented: e.g.¬†tm_polygons, tm_lines, tm_raster"
  },
  {
    "objectID": "r/gis_mapping_slides.html#tmap-example",
    "href": "r/gis_mapping_slides.html#tmap-example",
    "title": "GIS mapping in R",
    "section": "tmap example",
    "text": "tmap example"
  },
  {
    "objectID": "r/gis_mapping_slides.html#tmap-example-1",
    "href": "r/gis_mapping_slides.html#tmap-example-1",
    "title": "GIS mapping in R",
    "section": "tmap example",
    "text": "tmap example"
  },
  {
    "objectID": "r/gis_mapping_slides.html#tmap-example-2",
    "href": "r/gis_mapping_slides.html#tmap-example-2",
    "title": "GIS mapping in R",
    "section": "tmap example",
    "text": "tmap example"
  },
  {
    "objectID": "r/gis_mapping_slides.html#tmap-example-3",
    "href": "r/gis_mapping_slides.html#tmap-example-3",
    "title": "GIS mapping in R",
    "section": "tmap example",
    "text": "tmap example"
  },
  {
    "objectID": "r/gis_mapping_slides.html#tmap-example-4",
    "href": "r/gis_mapping_slides.html#tmap-example-4",
    "title": "GIS mapping in R",
    "section": "tmap example",
    "text": "tmap example"
  },
  {
    "objectID": "r/gis_mapping_slides.html#tmap-example-5",
    "href": "r/gis_mapping_slides.html#tmap-example-5",
    "title": "GIS mapping in R",
    "section": "tmap example",
    "text": "tmap example"
  },
  {
    "objectID": "r/gis_mapping_slides.html#tmap-example-6",
    "href": "r/gis_mapping_slides.html#tmap-example-6",
    "title": "GIS mapping in R",
    "section": "tmap example",
    "text": "tmap example"
  },
  {
    "objectID": "r/gis_mapping_slides.html#tmap-example-7",
    "href": "r/gis_mapping_slides.html#tmap-example-7",
    "title": "GIS mapping in R",
    "section": "tmap example",
    "text": "tmap example"
  },
  {
    "objectID": "r/gis_mapping_slides.html#ggplot2-the-standard-in-r-plots",
    "href": "r/gis_mapping_slides.html#ggplot2-the-standard-in-r-plots",
    "title": "GIS mapping in R",
    "section": "ggplot2 (the standard in R plots)",
    "text": "ggplot2 (the standard in R plots)\n\nUseful links\n\nGitHub repo\nResources\nCheatsheet"
  },
  {
    "objectID": "r/gis_mapping_slides.html#ggplot2",
    "href": "r/gis_mapping_slides.html#ggplot2",
    "title": "GIS mapping in R",
    "section": "ggplot2",
    "text": "ggplot2\ngeom_sf allows to plot sf objects (i.e.¬†make maps)"
  },
  {
    "objectID": "r/gis_mapping_slides.html#data",
    "href": "r/gis_mapping_slides.html#data",
    "title": "GIS mapping in R",
    "section": "Data",
    "text": "Data\nFor this workshop, we will use:\n\nthe Alaska as well as the Western Canada & USA subsets of the Randolph Glacier Inventory version 6.01\nthe USGS time series of the named glaciers of Glacier National Park2\nthe Alaska as well as the Western Canada & USA subsets of the consensus estimate for the ice thickness distribution of all glaciers on Earth dataset3\n\nThe datasets can be downloaded as zip files from these websites\nRGI Consortium (2017). Randolph Glacier Inventory ‚Äì A Dataset of Global Glacier Outlines: Version 6.0: Technical Report, Global Land Ice Measurements from Space, Colorado, USA. Digital Media. DOI: https://doi.org/10.7265/N5-RGI-60.Fagre, D.B., McKeon, L.A., Dick, K.A. & Fountain, A.G., 2017, Glacier margin time series (1966, 1998, 2005, 2015) of the named glaciers of Glacier National Park, MT, USA: U.S. Geological Survey data release. DOI: https://doi.org/10.5066/F7P26WB1.Farinotti, Daniel, 2019, A consensus estimate for the ice thickness distribution of all glaciers on Earth - dataset, Zurich. ETH Zurich. DOI: https://doi.org/10.3929/ethz-b-000315707."
  },
  {
    "objectID": "r/gis_mapping_slides.html#packages-1",
    "href": "r/gis_mapping_slides.html#packages-1",
    "title": "GIS mapping in R",
    "section": "Packages",
    "text": "Packages\n\nPackages need to be installed before they can be loaded in a session\nPackages on CRAN can be installed with:\ninstall.packages(\"<package-name>\")\n basemaps is not on CRAN & needs to be installed from GitHub thanks to devtools:\ninstall.packages(\"devtools\")\ndevtools::install_github(\"16EAGLE/basemaps\")"
  },
  {
    "objectID": "r/gis_mapping_slides.html#packages-2",
    "href": "r/gis_mapping_slides.html#packages-2",
    "title": "GIS mapping in R",
    "section": "Packages",
    "text": "Packages\n\nWe load all the packages that we will need at the top of the script:\nlibrary(sf)                 # spatial vector data manipulation\nlibrary(tmap)               # map production & tiled web map\nlibrary(dplyr)              # non GIS specific (tabular data manipulation)\nlibrary(magrittr)           # non GIS specific (pipes)\nlibrary(purrr)              # non GIS specific (functional programming)\nlibrary(rnaturalearth)      # basemap data access functions\nlibrary(rnaturalearthdata)  # basemap data\nlibrary(mapview)            # tiled web map\nlibrary(grid)               # (part of base R) used to create inset map\nlibrary(ggplot2)            # alternative to tmap for map production\nlibrary(ggspatial)          # spatial framework for ggplot2\nlibrary(terra)              # gridded spatial data manipulation\nlibrary(ggmap)              # download basemap data\nlibrary(basemaps)           # download basemap data\nlibrary(magick)             # wrapper around ImageMagick STL\nlibrary(leaflet)            # integrate Leaflet JS in R"
  },
  {
    "objectID": "r/gis_mapping_slides.html#randolph-glacier-inventory",
    "href": "r/gis_mapping_slides.html#randolph-glacier-inventory",
    "title": "GIS mapping in R",
    "section": "Randolph Glacier Inventory",
    "text": "Randolph Glacier Inventory\n\nThis dataset contains the contour of all glaciers on Earth \nWe will focus on glaciers in Western North America \nYou can download & unzip 02_rgi60_WesternCanadaUS & 01_rgi60_Alaska from the Randolph Glacier Inventory version 6.0"
  },
  {
    "objectID": "r/gis_mapping_slides.html#reading-in-data",
    "href": "r/gis_mapping_slides.html#reading-in-data",
    "title": "GIS mapping in R",
    "section": "Reading in data",
    "text": "Reading in data\n\nData get imported & turned into sf objects with the function sf::st_read:\nak <- st_read(\"data/01_rgi60_Alaska\")\n\nMake sure to use the absolute paths or the paths relative to your working directory (which can be obtained with getwd)"
  },
  {
    "objectID": "r/gis_mapping_slides.html#reading-in-data-1",
    "href": "r/gis_mapping_slides.html#reading-in-data-1",
    "title": "GIS mapping in R",
    "section": "Reading in data",
    "text": "Reading in data\n\nak <- st_read(\"data/01_rgi60_Alaska\")\n\n[Out]\n\nReading layer `01_rgi60_Alaska' from data source `./data/01_rgi60_Alaska'\n               using driver `ESRI Shapefile'\nSimple feature collection with 27108 features and 22 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -176.1425 ymin: 52.05727 xmax: -126.8545 ymax: 69.35167\nGeodetic CRS:  WGS 84"
  },
  {
    "objectID": "r/gis_mapping_slides.html#reading-in-data-2",
    "href": "r/gis_mapping_slides.html#reading-in-data-2",
    "title": "GIS mapping in R",
    "section": "Reading in data",
    "text": "Reading in data\n\n\n\nYour turn:\n\nRead in the data for the rest of north western America (from 02_rgi60_WesternCanadaUS) and create an sf object called wes"
  },
  {
    "objectID": "r/gis_mapping_slides.html#first-look-at-the-data",
    "href": "r/gis_mapping_slides.html#first-look-at-the-data",
    "title": "GIS mapping in R",
    "section": "First look at the data",
    "text": "First look at the data\nak\n\n[Out]\n\nSimple feature collection with 27108 features and 22 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -176.1425 ymin: 52.05727 xmax: -126.8545 ymax: 69.35167\nGeodetic CRS:  WGS 84\nFirst 10 features:\n           RGIId        GLIMSId  BgnDate  EndDate    CenLon   CenLat O1Region\n1  RGI60-01.00001 G213177E63689N 20090703 -9999999 -146.8230 63.68900        1\n2  RGI60-01.00002 G213332E63404N 20090703 -9999999 -146.6680 63.40400        1\n3  RGI60-01.00003 G213920E63376N 20090703 -9999999 -146.0800 63.37600        1\n4  RGI60-01.00004 G213880E63381N 20090703 -9999999 -146.1200 63.38100        1\n5  RGI60-01.00005 G212943E63551N 20090703 -9999999 -147.0570 63.55100        1\n6  RGI60-01.00006 G213756E63571N 20090703 -9999999 -146.2440 63.57100        1\n7  RGI60-01.00007 G213771E63551N 20090703 -9999999 -146.2295 63.55085        1\n8  RGI60-01.00008 G213704E63543N 20090703 -9999999 -146.2960 63.54300        1\n9  RGI60-01.00009 G212400E63659N 20090703 -9999999 -147.6000 63.65900        1\n10 RGI60-01.00010 G212830E63513N 20090703 -9999999 -147.1700 63.51300        1\nO2Region   Area Zmin Zmax Zmed Slope Aspect  Lmax Status Connect Form\n1         2  0.360 1936 2725 2385    42    346   839      0       0    0\n2         2  0.558 1713 2144 2005    16    162  1197      0       0    0\n3         2  1.685 1609 2182 1868    18    175  2106      0       0    0\n4         2  3.681 1273 2317 1944    19    195  4175      0       0    0\n5         2  2.573 1494 2317 1914    16    181  2981      0       0    0\n6         2 10.470 1201 3547 1740    22     33 10518      0       0    0\n7         2  0.649 1918 2811 2194    23    151  1818      0       0    0\n8         2  0.200 2826 3555 3195    45     80   613      0       0    0\n9         2  1.517 1750 2514 1977    18    274  2255      0       0    0\n10        2  3.806 1280 1998 1666    17     35  3332      0       0    0\nTermType Surging Linkages Name                       geometry\n1         0       9        9 <NA> POLYGON ((-146.818 63.69081...\n2         0       9        9 <NA> POLYGON ((-146.6635 63.4076...\n3         0       9        9 <NA> POLYGON ((-146.0723 63.3834...\n4         0       9        9 <NA> POLYGON ((-146.149 63.37919...\n5         0       9        9 <NA> POLYGON ((-147.0431 63.5502...\n6         0       9        9 <NA> POLYGON ((-146.2436 63.5562...\n7         0       9        9 <NA> POLYGON ((-146.2495 63.5531...\n8         0       9        9 <NA> POLYGON ((-146.2992 63.5443...\n9         0       9        9 <NA> POLYGON ((-147.6147 63.6643...\n10        0       9        9 <NA> POLYGON ((-147.1494 63.5098..."
  },
  {
    "objectID": "r/gis_mapping_slides.html#structure-of-the-data",
    "href": "r/gis_mapping_slides.html#structure-of-the-data",
    "title": "GIS mapping in R",
    "section": "Structure of the data",
    "text": "Structure of the data\nstr(ak)\n\n[Out]\n\nClasses ‚Äòsf‚Äô and 'data.frame':  27108 obs. of  23 variables:\n$ RGIId   : chr  \"RGI60-01.00001\" \"RGI60-01.00002\" \"RGI60-01.00003\" ...\n$ GLIMSId : chr  \"G213177E63689N\" \"G213332E63404N\" \"G213920E63376N\" ...\n$ BgnDate : chr  \"20090703\" \"20090703\" \"20090703\" \"20090703\" ...\n$ EndDate : chr  \"-9999999\" \"-9999999\" \"-9999999\" \"-9999999\" ...\n$ CenLon  : num  -147 -147 -146 -146 -147 ...\n$ CenLat  : num  63.7 63.4 63.4 63.4 63.6 ...\n$ O1Region: chr  \"1\" \"1\" \"1\" \"1\" ...\n$ O2Region: chr  \"2\" \"2\" \"2\" \"2\" ...\n$ Area    : num  0.36 0.558 1.685 3.681 2.573 ...\n$ Zmin    : int  1936 1713 1609 1273 1494 1201 1918 2826 1750 1280 ...\n$ Zmax    : int  2725 2144 2182 2317 2317 3547 2811 3555 2514 1998 ...\n$ Zmed    : int  2385 2005 1868 1944 1914 1740 2194 3195 1977 1666 ...\n$ Slope   : num  42 16 18 19 16 22 23 45 18 17 ...\n$ Aspect  : int  346 162 175 195 181 33 151 80 274 35 ...\n$ Lmax    : int  839 1197 2106 4175 2981 10518 1818 613 2255 3332 ...\n$ Status  : int  0 0 0 0 0 0 0 0 0 0 ...\n$ Connect : int  0 0 0 0 0 0 0 0 0 0 ...\n$ Form    : int  0 0 0 0 0 0 0 0 0 0 ...\n$ TermType: int  0 0 0 0 0 0 0 0 0 0 ...\n$ Surging : int  9 9 9 9 9 9 9 9 9 9 ...\n$ Linkages: int  9 9 9 9 9 9 9 9 9 9 ...\n$ Name    : chr  NA NA NA NA ...\n$ geometry:sfc_POLYGON of length 27108; first list element: List of 1\n..$ : num [1:65, 1:2] -147 -147 -147 -147 -147 ...\n..- attr(*, \"class\")= chr [1:3] \"XY\" \"POLYGON\" \"sfg\"\n- attr(*, \"sf_column\")= chr \"geometry\"\n- attr(*, \"agr\")= Factor w/ 3 levels \"constant\",\"aggregate\",..: NA NA NA ...\n..- attr(*, \"names\")= chr [1:22] \"RGIId\" \"GLIMSId\" \"BgnDate\" \"EndDate\" ..."
  },
  {
    "objectID": "r/gis_mapping_slides.html#inspect-your-data",
    "href": "r/gis_mapping_slides.html#inspect-your-data",
    "title": "GIS mapping in R",
    "section": "Inspect your data",
    "text": "Inspect your data\n\n\n\nYour turn:\n\nInspect the wes object you created."
  },
  {
    "objectID": "r/gis_mapping_slides.html#glacier-national-park-dataset",
    "href": "r/gis_mapping_slides.html#glacier-national-park-dataset",
    "title": "GIS mapping in R",
    "section": "Glacier National Park dataset",
    "text": "Glacier National Park dataset\n\nThis dataset contains a time series of the retreat of 39 glaciers of Glacier National Park, MT, USA\nfor the years 1966, 1998, 2005 & 2015\nYou can download and unzip the 4 sets of files from the USGS website"
  },
  {
    "objectID": "r/gis_mapping_slides.html#read-in-and-clean-datasets",
    "href": "r/gis_mapping_slides.html#read-in-and-clean-datasets",
    "title": "GIS mapping in R",
    "section": "Read in and clean datasets",
    "text": "Read in and clean datasets\n## create a function that reads and cleans the data\nprep <- function(dir) {\n  g <- st_read(dir)\n  g %<>% rename_with(~ tolower(gsub(\"Area....\", \"area\", .x)))\n  g %<>% dplyr::select(\n    year,\n    objectid,\n    glacname,\n    area,\n    shape_leng,\n    x_coord,\n    y_coord,\n    source_sca,\n    source\n  )\n}\n\n## create a vector of dataset names\ndirs <- grep(\"data/GNPglaciers_.*\", list.dirs(), value = T)\n\n## pass each element of that vector through prep() thanks to map()\ngnp <- map(dirs, prep)\n\nWe use dplyr::select because terra also has a select function"
  },
  {
    "objectID": "r/gis_mapping_slides.html#combine-datasets-into-one-sf-object",
    "href": "r/gis_mapping_slides.html#combine-datasets-into-one-sf-object",
    "title": "GIS mapping in R",
    "section": "Combine datasets into one sf object",
    "text": "Combine datasets into one sf object\n\nCheck that the CRS are all the same:\nall(sapply(\n  list(st_crs(gnp[[1]]),\n       st_crs(gnp[[2]]),\n       st_crs(gnp[[3]]),\n       st_crs(gnp[[4]])),\n  function(x) x == st_crs(gnp[[1]])\n))\n\n[Out]\n\n[1] TRUE"
  },
  {
    "objectID": "r/gis_mapping_slides.html#combine-datasets-into-one-sf-object-1",
    "href": "r/gis_mapping_slides.html#combine-datasets-into-one-sf-object-1",
    "title": "GIS mapping in R",
    "section": "Combine datasets into one sf object",
    "text": "Combine datasets into one sf object\n\nWe can rbind the elements of our list:\ngnp <- do.call(\"rbind\", gnp)\nYou can inspect your new sf object by calling it or with str"
  },
  {
    "objectID": "r/gis_mapping_slides.html#estimate-for-ice-thickness",
    "href": "r/gis_mapping_slides.html#estimate-for-ice-thickness",
    "title": "GIS mapping in R",
    "section": "Estimate for ice thickness",
    "text": "Estimate for ice thickness\n\nThis dataset contains an estimate for the ice thickness of all glaciers on Earth\nThe nomenclature follows the Randolph Glacier Inventory\nIce thickness being a spatial field, this is raster data\nWe will use data in RGI60-02.16664_thickness.tif from the ETH Z√ºrich Research Collection which corresponds to one of the glaciers (Agassiz) of Glacier National Park"
  },
  {
    "objectID": "r/gis_mapping_slides.html#load-raster-data",
    "href": "r/gis_mapping_slides.html#load-raster-data",
    "title": "GIS mapping in R",
    "section": "Load raster data",
    "text": "Load raster data\n\nRead in data and create a SpatRaster object:\nras <- rast(\"data/RGI60-02/RGI60-02.16664_thickness.tif\")"
  },
  {
    "objectID": "r/gis_mapping_slides.html#inspect-our-spatraster-object",
    "href": "r/gis_mapping_slides.html#inspect-our-spatraster-object",
    "title": "GIS mapping in R",
    "section": "Inspect our SpatRaster object",
    "text": "Inspect our SpatRaster object\n\nras\n\n[Out]\n\nclass       : SpatRaster \ndimensions  : 93, 74, 1  (nrow, ncol, nlyr)\nresolution  : 25, 25  (x, y)\nextent      : 707362.5, 709212.5, 5422962, 5425288  (xmin, xmax, ymin, ymax)\ncoord. ref. : +proj=utm +zone=11 +datum=WGS84 +units=m +no_defs \nsource      : RGI60-02.16664_thickness.tif \nname        : RGI60-02.16664_thickness \nnlyr gives us the number of bands (a single one here). You can also run str(ras)"
  },
  {
    "objectID": "r/gis_mapping_slides.html#our-data",
    "href": "r/gis_mapping_slides.html#our-data",
    "title": "GIS mapping in R",
    "section": "Our data",
    "text": "Our data\n\nWe now have 3 sf objects & 1 SpatRaster object:\n\nak: ‚ÄÉcontour of glaciers in AK\nwes: ‚ÄÇcontour of glaciers in the rest of Western North America\ngnp: ‚ÄÇtime series of 39 glaciers in Glacier National Park, MT, USA\nras: ‚ÄÇice thickness of the Agassiz Glacier from Glacier National Park"
  },
  {
    "objectID": "r/gis_mapping_slides.html#lets-map-our-sf-object-ak",
    "href": "r/gis_mapping_slides.html#lets-map-our-sf-object-ak",
    "title": "GIS mapping in R",
    "section": "Let‚Äôs map our sf object ak",
    "text": "Let‚Äôs map our sf object ak\nAt a bare minimum, we need tm_shape with the data & some info as to how to represent that data:\ntm_shape(ak) +\n  tm_polygons()"
  },
  {
    "objectID": "r/gis_mapping_slides.html#we-need-to-label-customize-it",
    "href": "r/gis_mapping_slides.html#we-need-to-label-customize-it",
    "title": "GIS mapping in R",
    "section": "We need to label & customize it",
    "text": "We need to label & customize it\n\ntm_shape(ak) +\n  tm_polygons() +\n  tm_layout(\n    title = \"Glaciers of Alaska\",\n    title.position = c(\"center\", \"top\"),\n    title.size = 1.1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.06, 0.01, 0.09, 0.01),\n    outer.margins = 0,\n    frame.lwd = 0.2\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    size = 1.2,\n    text.size = 0.6\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 500, 1000),\n    position = c(\"right\", \"BOTTOM\")\n  )"
  },
  {
    "objectID": "r/gis_mapping_slides.html#make-a-map-of-the-wes-object",
    "href": "r/gis_mapping_slides.html#make-a-map-of-the-wes-object",
    "title": "GIS mapping in R",
    "section": "Make a map of the wes object",
    "text": "Make a map of the wes object\n\n\n\nYour turn:\n\nMake a map with the wes object you created with the data for Western North America excluding AK"
  },
  {
    "objectID": "r/gis_mapping_slides.html#now-lets-make-a-map-with-ak-wes",
    "href": "r/gis_mapping_slides.html#now-lets-make-a-map-with-ak-wes",
    "title": "GIS mapping in R",
    "section": "Now, let‚Äôs make a map with ak & wes",
    "text": "Now, let‚Äôs make a map with ak & wes\n\n\nThe Coordinate Reference Systems (CRS) must be the same\n\n\nsf has a function to retrieve the CRS of an sf object: st_crs\n\n\nst_crs(ak) == st_crs(wes)\n\n[Out]\n\n[1] TRUE\n\n\nSo we‚Äôre good (we will see later what to do if this is not the case)"
  },
  {
    "objectID": "r/gis_mapping_slides.html#our-combined-map",
    "href": "r/gis_mapping_slides.html#our-combined-map",
    "title": "GIS mapping in R",
    "section": "Our combined map",
    "text": "Our combined map\nLet‚Äôs start again with a minimum map without any layout to test things out:\ntm_shape(ak) +\n  tm_polygons() +\n  tm_shape(wes) +\n  tm_polygons()\n\n\nUh ‚Ä¶ oh ‚Ä¶"
  },
  {
    "objectID": "r/gis_mapping_slides.html#what-went-wrong",
    "href": "r/gis_mapping_slides.html#what-went-wrong",
    "title": "GIS mapping in R",
    "section": "What went wrong?",
    "text": "What went wrong?\n\nMaps are bound by ‚Äúbounding boxes‚Äù. In tmap, they are called bbox\ntmap sets the bbox the first time tm_shape is called. In our case, the bbox was thus set to the bbox of the ak object\nWe need to create a new bbox for our new map"
  },
  {
    "objectID": "r/gis_mapping_slides.html#retrieving-bounding-boxes",
    "href": "r/gis_mapping_slides.html#retrieving-bounding-boxes",
    "title": "GIS mapping in R",
    "section": "Retrieving bounding boxes",
    "text": "Retrieving bounding boxes\n\nsf has a function to retrieve the bbox of an sf object: st_bbox\nThe bbox of ak is:\nst_bbox(ak)\n\n[Out]\n\nxmin         ymin       xmax         ymax\n-176.14247   52.05727   -126.85450   69.35167"
  },
  {
    "objectID": "r/gis_mapping_slides.html#combining-bounding-boxes",
    "href": "r/gis_mapping_slides.html#combining-bounding-boxes",
    "title": "GIS mapping in R",
    "section": "Combining bounding boxes",
    "text": "Combining bounding boxes\n\nbbox objects can‚Äôt be combined directly\nHere is how we can create a new bbox encompassing both of our bboxes:\n\nFirst, we transform our bboxes to sfc objects with st_as_sfc\nThen we combine those objects into a new sfc object with st_union\nFinally, we retrieve the bbox of that object with st_bbox:\n\nnwa_bbox <- st_bbox(\n  st_union(\n    st_as_sfc(st_bbox(wes)),\n    st_as_sfc(st_bbox(ak))\n  )\n)"
  },
  {
    "objectID": "r/gis_mapping_slides.html#back-to-our-map",
    "href": "r/gis_mapping_slides.html#back-to-our-map",
    "title": "GIS mapping in R",
    "section": "Back to our map",
    "text": "Back to our map\nWe can now use our new bounding box for the map of Western North America:\ntm_shape(ak, bbox = nwa_bbox) +\n  tm_polygons() +\n  tm_shape(wes) +\n  tm_polygons() +\n  tm_layout(\n    title = \"Glaciers of Western North America\",\n    title.position = c(\"center\", \"top\"),\n    title.size = 1.1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.06, 0.01, 0.09, 0.01),\n    outer.margins = 0,\n    frame.lwd = 0.2\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    size = 1.2,\n    text.size = 0.6\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 1000, 2000),\n    position = c(\"right\", \"BOTTOM\")\n  )"
  },
  {
    "objectID": "r/gis_mapping_slides.html#lets-add-a-basemap",
    "href": "r/gis_mapping_slides.html#lets-add-a-basemap",
    "title": "GIS mapping in R",
    "section": "Let‚Äôs add a basemap",
    "text": "Let‚Äôs add a basemap\n\nWe will use data from Natural Earth, a public domain map dataset\nThere are much more fancy options, but they usually involve creating accounts (e.g.¬†with Google) to access some API\nIn addition, this dataset can be accessed direction from within R thanks to the rOpenSci packages:\n\nrnaturalearth: provides the functions\nrnaturalearthdata: provides the data"
  },
  {
    "objectID": "r/gis_mapping_slides.html#create-an-sf-object-with-statesprovinces",
    "href": "r/gis_mapping_slides.html#create-an-sf-object-with-statesprovinces",
    "title": "GIS mapping in R",
    "section": "Create an sf object with states/provinces",
    "text": "Create an sf object with states/provinces\n\nstates_all <- ne_states(\n  country = c(\"canada\", \"united states of america\"),\n  returnclass = \"sf\"\n)\n\nne_ stands for ‚ÄúNatural Earth‚Äù"
  },
  {
    "objectID": "r/gis_mapping_slides.html#select-relevant-statesprovinces",
    "href": "r/gis_mapping_slides.html#select-relevant-statesprovinces",
    "title": "GIS mapping in R",
    "section": "Select relevant states/provinces",
    "text": "Select relevant states/provinces\n\nstates <- states_all %>%\n  filter(name_en == \"Alaska\" |\n           name_en == \"British Columbia\" |\n           name_en == \"Yukon\" |\n           name_en == \"Northwest Territories\" |\n           name_en ==  \"Alberta\" |\n           name_en == \"California\" |\n           name_en == \"Washington\" |\n           name_en == \"Oregon\" |\n           name_en == \"Idaho\" |\n           name_en == \"Montana\" |\n           name_en == \"Wyoming\" |\n           name_en == \"Colorado\" |\n           name_en == \"Nevada\" |\n           name_en == \"Utah\"\n         )"
  },
  {
    "objectID": "r/gis_mapping_slides.html#add-the-basemap-to-our-map",
    "href": "r/gis_mapping_slides.html#add-the-basemap-to-our-map",
    "title": "GIS mapping in R",
    "section": "Add the basemap to our map",
    "text": "Add the basemap to our map\n\n\n\nWhat do we need to make sure of first?\n\n\n\nst_crs(states) == st_crs(ak)\n\n[Out]\n\n[1] TRUE"
  },
  {
    "objectID": "r/gis_mapping_slides.html#add-the-basemap-to-our-map-1",
    "href": "r/gis_mapping_slides.html#add-the-basemap-to-our-map-1",
    "title": "GIS mapping in R",
    "section": "Add the basemap to our map",
    "text": "Add the basemap to our map\n\nWe add the basemap as a 3rd layer\nMind the order! If you put the basemap last, it will cover your data\nOf course, we will use our nwa_bbox bounding box again\nWe will also break tm_polygons into tm_borders and tm_fill for ak and wes in order to colourise them with slightly different colours"
  },
  {
    "objectID": "r/gis_mapping_slides.html#add-the-basemap-to-our-map-2",
    "href": "r/gis_mapping_slides.html#add-the-basemap-to-our-map-2",
    "title": "GIS mapping in R",
    "section": "Add the basemap to our map",
    "text": "Add the basemap to our map\n\ntm_shape(states, bbox = nwa_bbox) +\n  tm_polygons(col = \"#f2f2f2\", lwd = 0.2) +\n  tm_shape(ak) +\n  tm_borders(col = \"#3399ff\") +\n  tm_fill(col = \"#86baff\") +\n  tm_shape(wes) +\n  tm_borders(col = \"#3399ff\") +\n  tm_fill(col = \"#86baff\") +\n  tm_layout(\n    title = \"Glaciers of Western North America\",\n    title.position = c(\"center\", \"top\"),\n    title.size = 1.1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.06, 0.01, 0.09, 0.01),\n    outer.margins = 0,\n    frame.lwd = 0.2\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    size = 1.2,\n    text.size = 0.6\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 1000, 2000),\n    position = c(\"right\", \"BOTTOM\")\n  )"
  },
  {
    "objectID": "r/gis_mapping_slides.html#tmap-styles",
    "href": "r/gis_mapping_slides.html#tmap-styles",
    "title": "GIS mapping in R",
    "section": "tmap styles",
    "text": "tmap styles\n\ntmap has a number of styles that you can try\nFor instance, to set the style to ‚Äúclassic‚Äù, run the following before making your map:\ntmap_style(\"classic\")\n\nOther options are: \n‚Äúwhite‚Äù (default), ‚Äúgray‚Äù, ‚Äúnatural‚Äù, ‚Äúcobalt‚Äù, ‚Äúcol_blind‚Äù, ‚Äúalbatross‚Äù, ‚Äúbeaver‚Äù, ‚Äúbw‚Äù, ‚Äúwatercolor‚Äù"
  },
  {
    "objectID": "r/gis_mapping_slides.html#tmap-styles-1",
    "href": "r/gis_mapping_slides.html#tmap-styles-1",
    "title": "GIS mapping in R",
    "section": "tmap styles",
    "text": "tmap styles\n\nTo return to the default, you need to run\ntmap_style(\"white\")\nor\ntmap_options_reset()\nwhich will reset every tmap option"
  },
  {
    "objectID": "r/gis_mapping_slides.html#first-lets-map-it",
    "href": "r/gis_mapping_slides.html#first-lets-map-it",
    "title": "GIS mapping in R",
    "section": "First, let‚Äôs map it",
    "text": "First, let‚Äôs map it\nLet‚Äôs use the same tm_borders and tm_fill we just used:\ntm_shape(gnp) +\n  tm_borders(col = \"#3399ff\") +\n  tm_fill(col = \"#86baff\") +\n  tm_layout(\n    title = \"Glaciers of Glacier National Park\",\n    title.position = c(\"center\", \"top\"),\n    legend.title.color = \"#fcfcfc\",\n    legend.text.size = 1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.07, 0.03, 0.07, 0.03),\n    outer.margins = 0\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    text.size = 0.7\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 10, 20),\n    position = c(\"right\", \"BOTTOM\"),\n    text.size = 1\n  )"
  },
  {
    "objectID": "r/gis_mapping_slides.html#create-an-inset-map",
    "href": "r/gis_mapping_slides.html#create-an-inset-map",
    "title": "GIS mapping in R",
    "section": "Create an inset map",
    "text": "Create an inset map\n\nAs always, first we check that the CRS are the same:\nst_crs(gnp) == st_crs(ak)\n\n[Out]\n\n[1] FALSE\n\nAH!"
  },
  {
    "objectID": "r/gis_mapping_slides.html#crs-transformation",
    "href": "r/gis_mapping_slides.html#crs-transformation",
    "title": "GIS mapping in R",
    "section": "CRS transformation",
    "text": "CRS transformation\n\nWe need to reproject gnp into the CRS of our other sf objects (e.g.¬†ak):\ngnp <- st_transform(gnp, st_crs(ak))\n\nWe can verify that the CRS are now the same:\nst_crs(gnp) == st_crs(ak)\n\n[Out]\n\n[1] TRUE"
  },
  {
    "objectID": "r/gis_mapping_slides.html#inset-maps-first-step",
    "href": "r/gis_mapping_slides.html#inset-maps-first-step",
    "title": "GIS mapping in R",
    "section": "Inset maps: first step",
    "text": "Inset maps: first step\nAdd a rectangle showing the location of the GNP map in the main North America map\nWe need to create a new sfc object from the gnp bbox so that we can add it to our previous map as a new layer:\ngnp_zone <- st_bbox(gnp) %>%\n  st_as_sfc()"
  },
  {
    "objectID": "r/gis_mapping_slides.html#inset-maps-second-step",
    "href": "r/gis_mapping_slides.html#inset-maps-second-step",
    "title": "GIS mapping in R",
    "section": "Inset maps: second step",
    "text": "Inset maps: second step\nCreate a tmap object of the main map. Of course, we need to edit the title. Also, note the presence of our new layer:lala\nmain_map <- tm_shape(states, bbox = nwa_bbox) +\n  tm_polygons(col = \"#f2f2f2\", lwd = 0.2) +\n  tm_shape(ak) +\n  tm_borders(col = \"#3399ff\") +\n  tm_fill(col = \"#86baff\") +\n  tm_shape(wes) +\n  tm_borders(col = \"#3399ff\") +\n  tm_fill(col = \"#86baff\") +\n  tm_shape(gnp_zone) +\n  tm_borders(lwd = 1.5, col = \"#ff9900\") +\n  tm_layout(\n    title = \"Glaciers of Glacier National Park\",\n    title.position = c(\"center\", \"top\"),\n    title.size = 1.1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.06, 0.01, 0.09, 0.01),\n    outer.margins = 0,\n    frame.lwd = 0.2\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    size = 1.2,\n    text.size = 0.6\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 500, 1000),\n    position = c(\"right\", \"BOTTOM\")\n  )"
  },
  {
    "objectID": "r/gis_mapping_slides.html#inset-maps-third-step",
    "href": "r/gis_mapping_slides.html#inset-maps-third-step",
    "title": "GIS mapping in R",
    "section": "Inset maps: third step",
    "text": "Inset maps: third step\nCreate a tmap object of the inset map\nWe make sure to matching colours & edit the layouts for better readability:\ninset_map <- tm_shape(gnp) +\n  tm_borders(col = \"#3399ff\") +\n  tm_fill(col = \"#86baff\") +\n  tm_layout(\n    legend.show = F,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.03, 0.03, 0.03, 0.03),\n    outer.margins = 0,\n    frame = \"#ff9900\",\n    frame.lwd = 3\n  )"
  },
  {
    "objectID": "r/gis_mapping_slides.html#inset-maps-final-step",
    "href": "r/gis_mapping_slides.html#inset-maps-final-step",
    "title": "GIS mapping in R",
    "section": "Inset maps: final step",
    "text": "Inset maps: final step\nCombine the two tmap objects\nWe print the main map & add the inset map with grid::viewport:\nmain_map\nprint(inset_map, vp = viewport(0.41, 0.26, width = 0.5, height = 0.5))"
  },
  {
    "objectID": "r/gis_mapping_slides.html#map-of-the-agassiz-glacier",
    "href": "r/gis_mapping_slides.html#map-of-the-agassiz-glacier",
    "title": "GIS mapping in R",
    "section": "Map of the Agassiz Glacier",
    "text": "Map of the Agassiz Glacier\nSelect the data points corresponding to the Agassiz Glacier:\nag <- gnp %>% filter(glacname == \"Agassiz Glacier\")"
  },
  {
    "objectID": "r/gis_mapping_slides.html#map-of-the-agassiz-glacier-1",
    "href": "r/gis_mapping_slides.html#map-of-the-agassiz-glacier-1",
    "title": "GIS mapping in R",
    "section": "Map of the Agassiz Glacier",
    "text": "Map of the Agassiz Glacier\ntm_shape(ag) +\n  tm_polygons() +\n  tm_layout(\n    title = \"Agassiz Glacier\",\n    title.position = c(\"center\", \"top\"),\n    legend.position = c(\"left\", \"bottom\"),\n    legend.title.color = \"#fcfcfc\",\n    legend.text.size = 1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.07, 0.03, 0.07, 0.03),\n    outer.margins = 0\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    text.size = 0.7\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 0.5, 1),\n    position = c(\"right\", \"BOTTOM\"),\n    text.size = 1\n  )\n\n\nNot great ‚Ä¶"
  },
  {
    "objectID": "r/gis_mapping_slides.html#map-based-on-attribute-variables",
    "href": "r/gis_mapping_slides.html#map-based-on-attribute-variables",
    "title": "GIS mapping in R",
    "section": "Map based on attribute variables",
    "text": "Map based on attribute variables\ntm_shape(ag) +\n  tm_polygons(\"year\", palette = \"Blues\") +\n  tm_layout(\n    title = \"Agassiz Glacier\",\n    title.position = c(\"center\", \"top\"),\n    legend.position = c(\"left\", \"bottom\"),\n    legend.title.color = \"#fcfcfc\",\n    legend.text.size = 1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.07, 0.03, 0.07, 0.03),\n    outer.margins = 0\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    text.size = 0.7\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 0.5, 1),\n    position = c(\"right\", \"BOTTOM\"),\n    text.size = 1\n  )"
  },
  {
    "objectID": "r/gis_mapping_slides.html#using-ggplot2-instead-of-tmap",
    "href": "r/gis_mapping_slides.html#using-ggplot2-instead-of-tmap",
    "title": "GIS mapping in R",
    "section": "Using ggplot2 instead of tmap",
    "text": "Using ggplot2 instead of tmap\nAs an alternative to tmap, ggplot2 can plot maps with the geom_sf function:\nggplot(ag) +\n  geom_sf(aes(fill = year)) +\n  scale_fill_brewer(palette = \"Blues\") +\n  labs(title = \"Agassiz Glacier\") +\n  annotation_scale(location = \"bl\", width_hint = 0.4) +\n  annotation_north_arrow(location = \"tr\", which_north = \"true\",\n                         pad_x = unit(0.75, \"in\"), pad_y = unit(0.5, \"in\"),\n                         style = north_arrow_fancy_orienteering) +\n  theme_bw() +\n  theme(plot.title = element_text(hjust = 0.5))\nThe package ggspatial adds a lot of functionality to ggplot2 for spatial data"
  },
  {
    "objectID": "r/gis_mapping_slides.html#faceted-map-of-the-retreat-of-agassiz",
    "href": "r/gis_mapping_slides.html#faceted-map-of-the-retreat-of-agassiz",
    "title": "GIS mapping in R",
    "section": "Faceted map of the retreat of Agassiz",
    "text": "Faceted map of the retreat of Agassiz\ntm_shape(ag) +\n  tm_polygons(col = \"#86baff\") +\n  tm_layout(\n    main.title = \"Agassiz Glacier\",\n    main.title.position = c(\"center\", \"top\"),\n    main.title.size = 1.2,\n    legend.position = c(\"left\", \"bottom\"),\n    legend.title.color = \"#fcfcfc\",\n    legend.text.size = 1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0, 0.03, 0, 0.03),\n    outer.margins = 0,\n    panel.label.bg.color = \"#fcfcfc\",\n    frame = F,\n    asp = 0.6\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    size = 1,\n    text.size = 0.6\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 0.5, 1),\n    position = c(\"right\", \"BOTTOM\"),\n    text.size = 0.6\n  ) +\n  tm_facets(\n    by = \"year\",\n    free.coords = F,\n    ncol = 4\n  )"
  },
  {
    "objectID": "r/gis_mapping_slides.html#animated-map-of-the-retreat-of-agassiz",
    "href": "r/gis_mapping_slides.html#animated-map-of-the-retreat-of-agassiz",
    "title": "GIS mapping in R",
    "section": "Animated map of the Retreat of Agassiz",
    "text": "Animated map of the Retreat of Agassiz\nFirst, we need to create a tmap object with facets:\nagassiz_anim <- tm_shape(ag) +\n  tm_polygons(col = \"#86baff\") +\n  tm_layout(\n    title = \"Agassiz Glacier\",\n    title.position = c(\"center\", \"top\"),\n    legend.position = c(\"left\", \"bottom\"),\n    legend.title.color = \"#fcfcfc\",\n    legend.text.size = 1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.08, 0, 0.08, 0),\n    outer.margins = 0,\n    panel.label.bg.color = \"#fcfcfc\"\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    text.size = 0.7\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 0.5, 1),\n    position = c(\"right\", \"BOTTOM\"),\n    text.size = 1\n  ) +\n  tm_facets(\n    along = \"year\",\n    free.coords = F\n  )"
  },
  {
    "objectID": "r/gis_mapping_slides.html#animated-map-of-the-retreat-of-agassiz-1",
    "href": "r/gis_mapping_slides.html#animated-map-of-the-retreat-of-agassiz-1",
    "title": "GIS mapping in R",
    "section": "Animated map of the Retreat of Agassiz",
    "text": "Animated map of the Retreat of Agassiz\nThen we can pass that object to tmap_animation:\ntmap_animation(\n  agassiz_anim,\n  filename = \"ag.gif\",\n  dpi = 300,\n  inner.margins = c(0.08, 0, 0.08, 0),\n  delay = 100\n)"
  },
  {
    "objectID": "r/gis_mapping_slides.html#map-of-ice-thickness-of-agassiz",
    "href": "r/gis_mapping_slides.html#map-of-ice-thickness-of-agassiz",
    "title": "GIS mapping in R",
    "section": "Map of ice thickness of Agassiz",
    "text": "Map of ice thickness of Agassiz\nNow, let‚Äôs map the estimated ice thickness on Agassiz Glacier\nThis time, we use tm_raster:\ntm_shape(ras) +\n  tm_raster(title = \"\") +\n  tm_layout(\n    title = \"Ice thickness (m) of Agassiz Glacier\",\n    title.position = c(\"center\", \"top\"),\n    legend.position = c(\"left\", \"bottom\"),\n    legend.bg.color = \"#ffffff\",\n    legend.text.size = 1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.07, 0.03, 0.07, 0.03),\n    outer.margins = 0\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    text.size = 0.7\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 0.5, 1),\n    position = c(\"right\", \"BOTTOM\"),\n    text.size = 1\n  )"
  },
  {
    "objectID": "r/gis_mapping_slides.html#combining-with-randolph-data",
    "href": "r/gis_mapping_slides.html#combining-with-randolph-data",
    "title": "GIS mapping in R",
    "section": "Combining with Randolph data",
    "text": "Combining with Randolph data\nAs always, we check whether the CRS are the same:\nst_crs(ag) == st_crs(ras)\n\n[Out]\n\n[1] FALSE\nWe need to reproject ag (remember that it is best to avoid reprojecting raster data):\nag %<>% st_transform(st_crs(ras))"
  },
  {
    "objectID": "r/gis_mapping_slides.html#combining-with-randolph-data-1",
    "href": "r/gis_mapping_slides.html#combining-with-randolph-data-1",
    "title": "GIS mapping in R",
    "section": "Combining with Randolph data",
    "text": "Combining with Randolph data\nThe retreat & ice thickness layers will hide each other (the order matters!)\nOne option is to use tm_borders for one of them, but we can also use transparency (alpha)\nWe also adjust the legend:\ntm_shape(ras) +\n  tm_raster(title = \"Ice (m)\") +\n  tm_shape(ag) +\n  tm_polygons(\"year\", palette = \"Blues\", alpha = 0.2, title = \"Contour\") +\n  tm_layout(\n    title = \"Ice thickness (m) and retreat of Agassiz Glacier\",\n    title.position = c(\"center\", \"top\"),\n    legend.position = c(\"left\", \"bottom\"),\n    legend.bg.color = \"#ffffff\",\n    legend.text.size = 0.7,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.07, 0.03, 0.07, 0.03),\n    outer.margins = 0\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    text.size = 0.7\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 0.5, 1),\n    position = c(\"right\", \"BOTTOM\"),\n    text.size = 1\n  )"
  },
  {
    "objectID": "r/gis_mapping_slides.html#refining-raster-maps",
    "href": "r/gis_mapping_slides.html#refining-raster-maps",
    "title": "GIS mapping in R",
    "section": "Refining raster maps",
    "text": "Refining raster maps\nLet‚Äôs go back to our ice thickness map:"
  },
  {
    "objectID": "r/gis_mapping_slides.html#basemap-with-ggmap",
    "href": "r/gis_mapping_slides.html#basemap-with-ggmap",
    "title": "GIS mapping in R",
    "section": "Basemap with ggmap",
    "text": "Basemap with ggmap\nbasemap <- get_map(\n  bbox = c(\n    left = st_bbox(ag)[1],\n    bottom = st_bbox(ag)[2],\n    right = st_bbox(ag)[3],\n    top = st_bbox(ag)[4]\n  ),\n  source = \"osm\"\n)\n\nggmap is a powerful package, but Google now requires an API key obtained through registration"
  },
  {
    "objectID": "r/gis_mapping_slides.html#basemap-with-basemaps",
    "href": "r/gis_mapping_slides.html#basemap-with-basemaps",
    "title": "GIS mapping in R",
    "section": "Basemap with basemaps",
    "text": "Basemap with basemaps\nThe package basemaps allows to download open source basemap data from several sources, but those cannot easily be combined with sf objects\nThis plots a satellite image of the Agassiz Glacier:\nbasemap_plot(ag, map_service = \"esri\", map_type = \"world_imagery\")"
  },
  {
    "objectID": "r/gis_mapping_slides.html#satellite-image-of-the-agassiz-glacier",
    "href": "r/gis_mapping_slides.html#satellite-image-of-the-agassiz-glacier",
    "title": "GIS mapping in R",
    "section": "Satellite image of the Agassiz Glacier",
    "text": "Satellite image of the Agassiz Glacier"
  },
  {
    "objectID": "r/gis_mapping_slides.html#mapview",
    "href": "r/gis_mapping_slides.html#mapview",
    "title": "GIS mapping in R",
    "section": "mapview",
    "text": "mapview\n\nmapview(gnp)"
  },
  {
    "objectID": "r/gis_mapping_slides.html#tmap-1",
    "href": "r/gis_mapping_slides.html#tmap-1",
    "title": "GIS mapping in R",
    "section": "tmap",
    "text": "tmap\n\nSo far, we have used the plot mode of tmap. There is also a view mode which allows interactive viewing in a browser through Leaflet\nChange to view mode:\ntmap_mode(\"view\")\n\nYou can also toggle between modes with ttm\n\nRe-plot the last map we plotted with tmap:\ntmap_last()"
  },
  {
    "objectID": "r/gis_mapping_slides.html#leaflet",
    "href": "r/gis_mapping_slides.html#leaflet",
    "title": "GIS mapping in R",
    "section": "leaflet",
    "text": "leaflet\n\nleaflet creates a map widget to which you add layers\nmap <- leaflet()\naddTiles(map)"
  },
  {
    "objectID": "r/gis_mapping_slides.html#resources-2",
    "href": "r/gis_mapping_slides.html#resources-2",
    "title": "GIS mapping in R",
    "section": "Resources",
    "text": "Resources\n\nHere are some resources on the topic to get started.\n\nR companion to Geographic Information Analysis\nSpatial data analysis"
  },
  {
    "objectID": "r/hpc_intro.html",
    "href": "r/hpc_intro.html",
    "title": "Introduction to high-performance research computing in R",
    "section": "",
    "text": "This talk is an introduction to heavy computations in R.\nIt will cover:\n\nrunning R on the Alliance clusters,\nbenchmarking R code,\nprofiling R code,\nan introduction to parallel R,\nan introduction to using C++ through the Rcpp package."
  },
  {
    "objectID": "r/hpc_intro_slides.html#loading-the-r-module",
    "href": "r/hpc_intro_slides.html#loading-the-r-module",
    "title": "Introduction to high-performance research computing in R",
    "section": "Loading the R module",
    "text": "Loading the R module\nIn this webinar, I am using a virtual training cluster mimicking the Alliance clusters. The only differences reside in the loading of modules and I will make clear what they are.\nTo see what versions of R are available on a cluster, run:\n$ module spider r\nFor this webinar, I will load version 4.2.1.\nOn the Alliance clusters, StdEnv/2020 is automatically loaded, so you don‚Äôt have to include it in the line below (although it doesn‚Äôt hurt to have it). On our training cluster, this is the first module I need to run.\n$ module load StdEnv/2020 gcc/11.3.0 r/4.2.1\n\nNote that, in theory, you could run R using the proprietary Intel module which is loaded by default on the Alliance clusters, but it is recommended to replace it by the gcc module to compile R packages as some packages will need tweaking before they can be compiled by the Intel compiler (R packages can even be compiled with clang and LLVM, but the default GCC compiler is the best way to avoid headaches).\nIt is thus much simpler to always load the gcc module before loading the R module."
  },
  {
    "objectID": "r/hpc_intro_slides.html#installing-r-packages",
    "href": "r/hpc_intro_slides.html#installing-r-packages",
    "title": "Introduction to high-performance research computing in R",
    "section": "Installing R packages",
    "text": "Installing R packages\nTo install a package, launch the interactive R console with:\n$ R\nThen, in the R console, run:\ninstall.packages('<package_name>', repos='https://mirror.rcg.sfu.ca/mirror/CRAN/')  # Best CRAN mirror for Cedar\n{{}} You have to select a CRAN mirror from this list. Ideally, use a mirror close to the location of the cluster you are using or use https://cloud.r-project.org/. {{}}\n{{}} The first time you install a package, R will ask you whether you want to create a personal library in your home directory. Answer yes to both questions. All your packages will now install under ~/. {{}}\n{{}} A handful of packages require additional modules to be loaded before they can be installed. This will be indicated in the error messages you will get when you try to install them. {{}}"
  },
  {
    "objectID": "r/hpc_intro_slides.html#running-r-jobs",
    "href": "r/hpc_intro_slides.html#running-r-jobs",
    "title": "Introduction to high-performance research computing in R",
    "section": "Running R jobs",
    "text": "Running R jobs\nWhile it is totally fine to run R on the login node when you install packages, you must start a SLURM job before any heavy computation.\nInteractive jobs\nTo run R interactively, you should launch an salloc session before launching R:\n$ salloc --time=1:00:00 --mem-per-cpu=3000M --cpus-per-task=4\n$ R\nScripts\nTo run an R script called <your_script>.R, first, you need to write a job file.\n{{}} Example of sbatch job file called <your_job>.sh (note that R scripts are run with the command Rscript): {{}}\n#!/bin/bash\n#SBATCH --account=def-<your_account>\n#SBATCH --time=15\n#SBATCH --mem-per-cpu=3000M\n#SBATCH --cpus-per-task=4\n#SBATCH --job-name=\"<your_job>\"\nmodule load StdEnv/2020 gcc/11.3.0 r/4.2.1\nRscript <your_script>.R\nThen launch your job with:\n$ sbatch <your_job>.sh\nYou can monitor your job with sq."
  },
  {
    "objectID": "r/hpc_intro_slides.html#benchmarking",
    "href": "r/hpc_intro_slides.html#benchmarking",
    "title": "Introduction to high-performance research computing in R",
    "section": "Benchmarking",
    "text": "Benchmarking"
  },
  {
    "objectID": "r/hpc_intro_slides.html#profiling",
    "href": "r/hpc_intro_slides.html#profiling",
    "title": "Introduction to high-performance research computing in R",
    "section": "Profiling",
    "text": "Profiling"
  },
  {
    "objectID": "r/hpc_intro_slides.html#parallel-r",
    "href": "r/hpc_intro_slides.html#parallel-r",
    "title": "Introduction to high-performance research computing in R",
    "section": "Parallel R",
    "text": "Parallel R\nhttps://towardsdatascience.com/getting-started-with-parallel-programming-in-r-d5f801d43745 https://nceas.github.io/oss-lessons/parallel-computing-in-r/parallel-computing-in-r.html https://yxue-me.com/post/2019-05-12-a-glossary-of-parallel-computing-packages-in-r-2019/ https://subscription.packtpub.com/book/big-data-and-business-intelligence/9781784394004/1/ch01lvl1sec09/the-r-parallel-package https://www.r-bloggers.com/2017/08/implementing-parallel-processing-in-r/ https://www.stat.umn.edu/geyer/3701/notes/parallel.html https://www.stat.umn.edu/geyer/8054/notes/parallel.html https://blog.esciencecenter.nl/parallel-r-in-a-nutshell-4391d45b5461 https://dept.stat.lsa.umich.edu/~jerrick/courses/stat701/notes/parallel.html https://bookdown.org/rdpeng/rprogdatascience/parallel-computation.html https://docs.alliancecan.ca/wiki/R#doParallel_and_foreach https://www.r-bloggers.com/2016/01/strategies-to-speedup-r-code/ https://www.datacamp.com/tutorial/r-tutorial-apply-family https://bookdown.org/rdpeng/rprogdatascience/profiling-r-code.html\nStart large (4GB) on a test script. Then: While the script is running, check how much memory is used in real time by typing: sstat yourjobID.batch ‚Äìformat=‚ÄúJobID,MaxRSS‚Äù Or When the script is done running, check how much was used by typing: sacct -o MaxRSS -j yourjobID If you check the slurm.out file and you‚Äôre getting ‚Äúoom-kill‚Äù errors, you need to request more memory If you‚Äôre using less than you asked for, it‚Äôs beneficial to reduce the memory in ‚Äìmem or ‚Äìmem-per-cpu (this way your job will get scheduled sooner) Resubmit your job with your new estimate.\nIndependent repeats of computations (e.g.¬†bootstrapping)\nNo communication needed between computations.\nParallel package\n\nStart up M ‚Äòworker‚Äô processes, and do any initialization needed on the workers.\nSend any data required for each task to the workers.\nSplit the task into M roughly equally-sized chunks, and send the chunks (including the R code needed) to the workers.\nWait for all the workers to complete their tasks, and ask them for their results.\nRepeat steps (b‚Äìd) for any further tasks.\nShut down the worker processes.\n\n\n\n\n¬†Back to workshop page"
  },
  {
    "objectID": "r/intro_gis.html",
    "href": "r/intro_gis.html",
    "title": "Introduction to GIS in R",
    "section": "",
    "text": "This workshop is an introduction to GIS in R. We will learn how to import GIS data, explore it, and map it.\nIn particular, we will create maps (inset maps, faceted maps, animated maps, interactive maps, and raster maps), thanks to the packages sf, tmap, raster, leaflet, ggplot2, grid (part of Base R), and mapview.\nWe will also learn how to add basemaps from OpenStreetMap and Google Maps."
  },
  {
    "objectID": "r/intro_gis.html#datasets",
    "href": "r/intro_gis.html#datasets",
    "title": "Introduction to GIS in R",
    "section": "Datasets",
    "text": "Datasets\nFor this webinar, we will use:\n\nthe Alaska as well as the Western Canada and USA subsets of the Randolph Glacier Inventory version 6.01\nthe USGS time series of the named glaciers of Glacier National Park2 The datasets can be downloaded as zip files from these websites.\n\n1¬†RGI Consortium (2017). Randolph Glacier Inventory ‚Äì A Dataset of Global Glacier Outlines: Version 6.0: Technical Report, Global Land Ice Measurements from Space, Colorado, USA. Digital Media. DOI: https://doi.org/10.7265/N5-RGI-60.2¬†Fagre, D.B., McKeon, L.A., Dick, K.A., and Fountain, A.G., 2017, Glacier margin time series (1966, 1998, 2005, 2015) of the named glaciers of Glacier National Park, MT, USA: U.S. Geological Survey data release, https://doi.org/10.5066/F7P26WB1."
  },
  {
    "objectID": "r/intro_gis.html#basemaps",
    "href": "r/intro_gis.html#basemaps",
    "title": "Introduction to GIS in R",
    "section": "Basemaps",
    "text": "Basemaps\nFor our basemaps, we will use data from:\n\nNatural Earth: this dataset can be accessed direction from within R thanks to the packages rnaturalearth (which provides the functions) and rnaturalearthdata (which provides the data)"
  },
  {
    "objectID": "r/intro_gis.html#with-mapview",
    "href": "r/intro_gis.html#with-mapview",
    "title": "Introduction to GIS in R",
    "section": "With mapview",
    "text": "With mapview\nThe simplest option is to use mapview::mapview():\nmapview(gnp)\nThis will open a page in your browser in which you can pan, zoom, select/deselect data layers, and choose from a number of basemap layer options:\n CartoDB.Positron\n OpenTopoMap\n OpenStreetMap\n Esri.WorldImagery"
  },
  {
    "objectID": "r/intro_gis.html#with-tmap",
    "href": "r/intro_gis.html#with-tmap",
    "title": "Introduction to GIS in R",
    "section": "With tmap",
    "text": "With tmap\ntmap has similar capabilities.\nThe package has 2 modes:\n\nplot is the default mode for static maps that we used earlier.\nview is an interactive viewing mode using Leaflet in a browser. There, as with mapview, you can zoom in/out, select/deselect the different layers, and choose to display one of Esri.WorldGrayCanvas, OpenStreetMap, or Esri.WorldTopoMap basemaps.\n\nYou can toggle between the plot and view modes with ttm(), after which you can re-plot your last plot in the new mode with tmap_last(). You can also do both of these at once with ttmp().\nAlternatively, you can switch to either mode with tmap_mode(\"view\") and tmap_mode(\"plot\").\n\nExample:\n\nEarlier, we plotted all the glaciers of Western North America using tmap:\n\nAfter displaying this map, we could have run:\ntmap_mode(\"view\")\ntmap_last()\nAnd Leaflet would have open the following interactive map in our browser:\n\n\nAfterwards, if you want to create new static plots, don‚Äôt forget to get back to plot mode with tmap_mode(\"plot\")."
  },
  {
    "objectID": "r/intro_hss.html",
    "href": "r/intro_hss.html",
    "title": "Introduction to R for the humanities",
    "section": "",
    "text": "R is a free and open-source programming language for statistical computing, modelling, and graphics, with an unbeatable collection of statistical packages. It is extremely popular in some academic fields such as statistics, biology, bioinformatics, data mining, data analysis, and linguistics.\nThis introductory course does not assume any prior knowledge: it will take you through the first steps of importing, cleaning, and visualizing your data. Along the way, we will get familiar with R data types, functions writing, and control flow."
  },
  {
    "objectID": "r/webscraping.html",
    "href": "r/webscraping.html",
    "title": "Web scraping with R",
    "section": "",
    "text": "The internet is a trove of information. A lot of it is publicly available and thus suitable for use in research. Extracting that information and putting it in an organized format for analysis can, however, be extremely tedious. Web scraping tools allow to automate parts of that process and R is a popular language for the task.\nIn this workshop, we will guide you through a simple example using the package rvest."
  },
  {
    "objectID": "tools/help.html",
    "href": "tools/help.html",
    "title": "So, you are stuck ‚Ä¶ now what?",
    "section": "",
    "text": "Stack Overflow, Stack Exchange, Discourse forums, and other online platforms ‚Ä¶ the internet is a treasure trove of online communities where you can find solutions to your coding problems. To have a positive experience and get the answers you need however, you have to know where to ask, how to ask, and when not to ask: if countless people are willing to give you their time for free, they usually expect that you do your part.\nIn this workshop, I will present key online sites, their functioning, and their culture; then I will go over the magic trick to get answers to your questions: knowing how to create minimum reproducible examples. I will not focus on any particular language as the principles (how to create a minimal dataset, how to deal with private data, how to create self-sufficient code, how to reproduce the problem, etc.) can apply to any language.\n\nSlides (click and wait: my reveal.js presentations are heavy and take some time to load‚Ä¶)"
  },
  {
    "objectID": "tools/help_slides.html#when-you-are-stuck-1",
    "href": "tools/help_slides.html#when-you-are-stuck-1",
    "title": "So, you are stuck ‚Ä¶ now what?",
    "section": "When you are stuck",
    "text": "When you are stuck\n\n\nFirst, look for information that is already out there\n\n\nThen, ask for help"
  },
  {
    "objectID": "tools/help_slides.html#look-for-information",
    "href": "tools/help_slides.html#look-for-information",
    "title": "So, you are stuck ‚Ä¶ now what?",
    "section": "Look for information",
    "text": "Look for information\n\n\nRead carefully any error message\nRead the documentation (local or online)\nMake sure you have up-to-date versions\nGoogle (using carefully selected keywords or the error message)\nLook for open issues & bug reports"
  },
  {
    "objectID": "tools/help_slides.html#error-messages",
    "href": "tools/help_slides.html#error-messages",
    "title": "So, you are stuck ‚Ä¶ now what?",
    "section": "Error messages",
    "text": "Error messages\n\n\nRead them!\n\nFamiliarise yourself with the error types in the languages you use\n\n\nExample: Python‚Äôs syntax errors vs exceptions\n\n\n\nWarnings ‚â† errors\n\n\nLook for bits you understand (don‚Äôt get put off by what you don‚Äôt understand)\n\n\nIdentify the locations of the errors to go investigate that part of the code"
  },
  {
    "objectID": "tools/help_slides.html#documentation",
    "href": "tools/help_slides.html#documentation",
    "title": "So, you are stuck ‚Ä¶ now what?",
    "section": "Documentation",
    "text": "Documentation\n\n\nYou need to find it\n\n\nYou need to understand it"
  },
  {
    "objectID": "tools/help_slides.html#finding-documentation",
    "href": "tools/help_slides.html#finding-documentation",
    "title": "So, you are stuck ‚Ä¶ now what?",
    "section": "Finding documentation",
    "text": "Finding documentation\n\nOnline:\nTake the time to look for the official documentation & other high quality sources for the languages & tools you use.\n\n\n\nExamples:\nPython: Reference manual, Standard library manual, Tutorial\nNumPy: Tutorial\nR: Open source book ‚ÄúR for Data Science‚Äù, Open source book ‚ÄúAdvanced R‚Äù\nJulia: Documentation\nBash: Manual\nGit: Manual, Open source book\n\n\n\nIn the program itself\n\n\nUnderstanding the documentation"
  },
  {
    "objectID": "tools/help_slides.html#up-to-date-versions",
    "href": "tools/help_slides.html#up-to-date-versions",
    "title": "So, you are stuck ‚Ä¶ now what?",
    "section": "Up-to-date versions",
    "text": "Up-to-date versions\n\n\nFirst, you need to know what needs to be updated.\n\n\nKeeping a system up to date includes updating:\n\nthe OS\nthe program\n(any potential IDE)\npackages\n\n\n\nThen, you need to update regularly."
  },
  {
    "objectID": "tools/help_slides.html#google",
    "href": "tools/help_slides.html#google",
    "title": "So, you are stuck ‚Ä¶ now what?",
    "section": "Google",
    "text": "Google\n\nGoogle‚Äôs algorithms are great at guessing what we are looking for.\n\nBut there is a frequency problem:\nSearches relating to programming-specific questions represent too small a fraction of the overall searches for results to be relevant unless you use key vocabulary.\n\n\nBe precise.\n\n\nLearn the vocabulary of your language/tool to know what to search for."
  },
  {
    "objectID": "tools/help_slides.html#open-issues-bug-reports",
    "href": "tools/help_slides.html#open-issues-bug-reports",
    "title": "So, you are stuck ‚Ä¶ now what?",
    "section": "Open issues & bug reports",
    "text": "Open issues & bug reports\n\nIf the tool you are using is open source, look for issues matching your problem in the source repository (e.g.¬†on GitHub or GitLab)."
  },
  {
    "objectID": "tools/help_slides.html#what-if-the-answer-isnt-out-there",
    "href": "tools/help_slides.html#what-if-the-answer-isnt-out-there",
    "title": "So, you are stuck ‚Ä¶ now what?",
    "section": "What if the answer isn‚Äôt out there?",
    "text": "What if the answer isn‚Äôt out there?\n\nWhen everything has failed & you have to ask for help, you need to know:\n\n\nWhere to ask\n\n\n\n\nHow to ask"
  },
  {
    "objectID": "tools/help_slides.html#where-to-ask-1",
    "href": "tools/help_slides.html#where-to-ask-1",
    "title": "So, you are stuck ‚Ä¶ now what?",
    "section": "Where to ask",
    "text": "Where to ask\n\nQ&A sites\nMostly, Stack Overflow & the Stack Exchange network.\nCo-founded in 2008 & 2009 by Jeff Atwood & Joel Spolsky. \nForums\nMostly, Discourse.\nCo-founded in 2013 by Jeff Atwood, Robin Ward & Sam Saffron.\nA few other older forums."
  },
  {
    "objectID": "tools/help_slides.html#where-to-ask-2",
    "href": "tools/help_slides.html#where-to-ask-2",
    "title": "So, you are stuck ‚Ä¶ now what?",
    "section": "Where to ask",
    "text": "Where to ask\n\nWhich one to choose is a matter of personal preference.\nPossible considerations:\n\nSome niche topics have very active communities on Discourse\nStack Overflow & some older forums can be intimidating with higher expectations for the questions quality & a more direct handling of mistakes\nFor conversations, advice, or multiple step questions, go to Discourse\nStack Overflow has over 13 million users\nStack Overflow & co have a very efficient approach"
  },
  {
    "objectID": "tools/help_slides.html#stack-overflow-co",
    "href": "tools/help_slides.html#stack-overflow-co",
    "title": "So, you are stuck ‚Ä¶ now what?",
    "section": "Stack Overflow & co",
    "text": "Stack Overflow & co\n\nPick the best site to ask your question.\nA few of the Stack Exchange network sites:\nStack Overflow: programming\nSuper User: computer hardware & software\nUnix & Linux: *nix OS TEX: TeX/LaTeX\nCross Validated: stats; data mining, collecting, analysis & visualization; ML\nData Science: focus on implementation & processes\nOpen Data\nGIS"
  },
  {
    "objectID": "tools/help_slides.html#how-to-ask-1",
    "href": "tools/help_slides.html#how-to-ask-1",
    "title": "So, you are stuck ‚Ä¶ now what?",
    "section": "How to ask",
    "text": "How to ask\n\nFamiliarize yourself with the site by reading posts\n\n\nRead the ‚ÄúTour‚Äù page (SO/SE) or take the ‚ÄúNew user tutorial‚Äù (Discourse)\n\n\nMake sure the question has not already been asked\n\n\nFormat the question properly\n\n\nGive a minimum reproducible example\n\n\nDo not share sensitive data\n\n\nShow your attempts\n\n\nAvoid cross-posting. If you really have to, make sure to cross-reference"
  },
  {
    "objectID": "tools/help_slides.html#how-to-ask-so-co",
    "href": "tools/help_slides.html#how-to-ask-so-co",
    "title": "So, you are stuck ‚Ä¶ now what?",
    "section": "How to ask: SO & co",
    "text": "How to ask: SO & co\n\nDon‚Äôt ask opinion-based questions\n\n\nDon‚Äôt ask for package, tool, or service recommendations\n\n\nDon‚Äôt ask more than one question in a single post\n\n\nCheck your spelling, grammar, punctuation, capitalized sentences, etc.\n\n\nAvoid greetings, signatures, thank-yous; keep it to the point\n\n\nAvoid apologies about being a beginner, this being your first post, the question being stupid, etc: do the best you can & skip the personal, self-judgmental & irrelevant bits"
  },
  {
    "objectID": "tools/help_slides.html#formatting-your-question",
    "href": "tools/help_slides.html#formatting-your-question",
    "title": "So, you are stuck ‚Ä¶ now what?",
    "section": "Formatting your question",
    "text": "Formatting your question\nNowadays, most sites (including Stack Overflow & Discourse) allow markdown rendering.\nSome older forums implement other markup languages (e.g.¬†BBCode).\nThe information is always easy to find. Spend the time to format your question properly. People will be much less inclined to help you if you don‚Äôt show any effort & if your question is a nightmare to read."
  },
  {
    "objectID": "tools/help_slides.html#example-of-a-typical-downvoted-question",
    "href": "tools/help_slides.html#example-of-a-typical-downvoted-question",
    "title": "So, you are stuck ‚Ä¶ now what?",
    "section": "Example of a typical downvoted question",
    "text": "Example of a typical downvoted question\n\nhowdy!!\ni am new to R sorry for a very silly question.i looked all oever the itnernwet, but i dint find\nanyanswer. i tried to use ggplot i get the error: Error in loadNamespace(i, c(lib.loc, .libPaths()),\nversionCheck = vI[[i]]) : there is no package called 'stringi'\nthank youu very much!!!!!\nmarie\n\n[Out]"
  },
  {
    "objectID": "tools/help_slides.html#same-question-fixed",
    "href": "tools/help_slides.html#same-question-fixed",
    "title": "So, you are stuck ‚Ä¶ now what?",
    "section": "Same question, fixed",
    "text": "Same question, fixed\nWhen I try to load the package `ggplot2` with:\n\n```{r}\nlibrary(ggplot2)\n```\nI get the error:\n\n> Error in loadNamespace(i, c(lib.loc, .libPaths()), versionCheck = vI[[i]]) :\nthere is no package called 'stringi'\n\nWhat am I doing wrong?"
  },
  {
    "objectID": "tools/help_slides.html#still-not-good-enough",
    "href": "tools/help_slides.html#still-not-good-enough",
    "title": "So, you are stuck ‚Ä¶ now what?",
    "section": "Still not good enough",
    "text": "Still not good enough\n\nThis question is actually a duplicate of a question asked which is itself a duplicate of another question."
  },
  {
    "objectID": "tools/help_slides.html#creating-a-minimal-reproducible-example",
    "href": "tools/help_slides.html#creating-a-minimal-reproducible-example",
    "title": "So, you are stuck ‚Ä¶ now what?",
    "section": "Creating a minimal reproducible example",
    "text": "Creating a minimal reproducible example\n\nThere are great posts on how to create a good minimal reproducible example. In particular:\nHow to create a Minimal, Reproducible Example  For R (but concepts apply to any language):\nHow to make a great R reproducible example\nWhat‚Äôs a reproducible example (reprex) and how do I do one?"
  },
  {
    "objectID": "tools/help_slides.html#creating-a-minimal-reproducible-example-1",
    "href": "tools/help_slides.html#creating-a-minimal-reproducible-example-1",
    "title": "So, you are stuck ‚Ä¶ now what?",
    "section": "Creating a minimal reproducible example",
    "text": "Creating a minimal reproducible example\n\n\nLoad all necessary packages\nLoad or create necessary data\nSimplify the data & the code as much as possible while still reproducing the problem\nUse simple variable names"
  },
  {
    "objectID": "tools/help_slides.html#data-for-your-example-your-own-data",
    "href": "tools/help_slides.html#data-for-your-example-your-own-data",
    "title": "So, you are stuck ‚Ä¶ now what?",
    "section": "Data for your example: your own data",
    "text": "Data for your example: your own data\n\nDo not upload data somewhere on the web to be downloaded.\nMake sure that the data is anonymised.\nDon‚Äôt keep more variables & more data points than are necessary to reproduce the problem.\nSimplify the variable names.\nIn R, you can use functions such as dput() to turn your reduced, anonymised data into text that is easy to copy/paste & can then be used to recreate the data."
  },
  {
    "objectID": "tools/help_slides.html#data-for-your-example-create-a-toy-dataset",
    "href": "tools/help_slides.html#data-for-your-example-create-a-toy-dataset",
    "title": "So, you are stuck ‚Ä¶ now what?",
    "section": "Data for your example: create a toy dataset",
    "text": "Data for your example: create a toy dataset\n\nYou can also create a toy dataset.\nFunctions that create random data, series, or repetitions are very useful here."
  },
  {
    "objectID": "tools/help_slides.html#data-for-your-example-pre-packaged-datasets",
    "href": "tools/help_slides.html#data-for-your-example-pre-packaged-datasets",
    "title": "So, you are stuck ‚Ä¶ now what?",
    "section": "Data for your example: pre-packaged datasets",
    "text": "Data for your example: pre-packaged datasets\n\nSome languages/packages come with pre-packaged datasets. If your code involves such languages/packages, you can make use of these datasets to create your reproducible example.\nFor example, R comes with many datasets directly available, including iris, mtcars, trees, airquality. In the R console, try: \n?iris\n?mtcars"
  },
  {
    "objectID": "tools/help_slides.html#additional-considerations",
    "href": "tools/help_slides.html#additional-considerations",
    "title": "So, you are stuck ‚Ä¶ now what?",
    "section": "Additional considerations",
    "text": "Additional considerations\n\nEven if you always find answers to your questions without having to post yourself, consider signing up to these sites:\n\nIt allows you to upvote (SO/SE) or like (Discourse) the questions & answers that help you‚Äîand why not thank in this fashion those that are making your life easier?\nIt makes you a part of these communities.\nOnce you are signed up, maybe you will start being more involved & contribute with questions & answers of your own."
  },
  {
    "objectID": "tools/help_slides.html#a-last-word",
    "href": "tools/help_slides.html#a-last-word",
    "title": "So, you are stuck ‚Ä¶ now what?",
    "section": "A last word",
    "text": "A last word\n\nWhile it takes some work to ask a good question, do not let this discourage you from posting on Stack Overflow: if you ask a good question, you will get many great answers.\nYou will learn in the process of developing your question (you may actually find the answer in that process) & you will learn from the answers.\nIt is forth the effort.\nHere is the Stack Overflow documentation on how to ask a good question.\n\n\n\n¬†Back to workshop page"
  },
  {
    "objectID": "tools/quarto.html",
    "href": "tools/quarto.html",
    "title": "Authoring scientific documents with Markdown and Quarto",
    "section": "",
    "text": "This workshop will show you how to easily create beautiful scientific documents (html, pdf, websites, books‚Ä¶)‚Äîcomplete with formatted text, dynamic code, and figures with Quarto, an open-source tool combining the powers of Jupyter or knitr with Pandoc to turn your text and code blocks into fully dynamic and formatted documents."
  },
  {
    "objectID": "tools/quarto.html#markup-languages",
    "href": "tools/quarto.html#markup-languages",
    "title": "Authoring scientific documents with Markdown and Quarto",
    "section": "Markup languages",
    "text": "Markup languages\nMarkup languages control the formatting of text documents. They are powerful but complex and the raw text (before it is rendered into its formatted version) is visually cluttered and hard to read.\nExamples of markup languages include LaTeX and HTML.\n\nTex (often with the macro package LaTeX) is used to create pdf.\n\n\nExample LaTeX:\n\n\\documentclass{article}\n\\title{My title}\n\\author{My name}\n\\usepackage{datetime}\n\\newdate{date}{24}{11}{2022}\n\\date{\\displaydate{date}}\n\\begin{document}\n \\maketitle\n \\section{First section}\n Some text in the first section.\n\\end{document}\n\nHTML (often with css or scss files to customize the format) is used to create webpages.\n\n\nExample HTML:\n\n<!DOCTYPE html>\n<html lang=\"en-US\">\n  <head>\n    <meta charset=\"utf-8\" />\n    <meta name=\"viewport\" content=\"width=device-width\" />\n    <title>My title</title>\n    <address class=\"author\">My name</address>\n    <input type=\"date\" value=\"2022-11-24\" />\n  </head>\n  <h1>First section</h1>\n  <body>\n    Some text in the first section.\n  </body>\n</html>"
  },
  {
    "objectID": "tools/quarto.html#markdown",
    "href": "tools/quarto.html#markdown",
    "title": "Authoring scientific documents with Markdown and Quarto",
    "section": "Markdown",
    "text": "Markdown\nA number of minimalist markup languages intend to remove all the visual clutter and complexity to create raw texts that are readable prior to rendering. Markdown (note the pun with ‚Äúmarkup‚Äù), created in 2004, is the most popular of them. Due to its simplicity, it has become quasi-ubiquitous. Many implementations exist which add a varying number of features (as you can imagine, a very simple markup language is also fairly limited).\nMarkdown files are simply text files and they use the .md extension."
  },
  {
    "objectID": "tools/quarto.html#basic-markdown-syntax",
    "href": "tools/quarto.html#basic-markdown-syntax",
    "title": "Authoring scientific documents with Markdown and Quarto",
    "section": "Basic Markdown syntax",
    "text": "Basic Markdown syntax\nIn its basic form, Markdown is mostly used to create webpages. Conveniently, raw HTML can be included whenever the limited markdown syntax isn‚Äôt sufficient.\nHere is an overview of the Markdown syntax supported by many applications."
  },
  {
    "objectID": "tools/quarto.html#pandoc-and-its-extended-markdown-syntax",
    "href": "tools/quarto.html#pandoc-and-its-extended-markdown-syntax",
    "title": "Authoring scientific documents with Markdown and Quarto",
    "section": "Pandoc and its extended Markdown syntax",
    "text": "Pandoc and its extended Markdown syntax\nWhile the basic syntax is good enough for HTML outputs, it is very limited for other formats.\nPandoc is a free and open-source markup format converter. Pandoc supports an extended Markdown syntax with functionality for figures, tables, callout blocks, LaTeX mathematical equations, citations, and YAML metadata blocks. In short, everything needed for the creation of scientific documents.\nSuch documents remain as readable as basic Markdown documents (thus respecting the Markdown philosophy), but they can now be rendered in sophisticated pdf, books, entire websites, Word documents, etc.\nAnd of course, as such documents remain text files, you can put them under version control with Git.\n\nPrevious example using Pandoc‚Äôs Markdown:\n\n---\ntitle: My title\nauthor: My name\ndate: 2022-11-24\n---\n# First section\nSome text in the first section."
  },
  {
    "objectID": "tools/quarto.html#how-it-works",
    "href": "tools/quarto.html#how-it-works",
    "title": "Authoring scientific documents with Markdown and Quarto",
    "section": "How it works",
    "text": "How it works\nQuarto files are transformed into Pandoc‚Äôs extended Markdown by Jupyter (when used with Python or Julia) or by knitr (when used with R), then pandoc turns the Markdown document into the output of your choice.\n\nJulia and Python make use of the Jupyter engine:\n\n From Quarto documentation\n\nR uses the knitr engine:\n\n From Quarto documentation\nQuarto files use the extension .qmd.\nWhen using R, you can use Quarto directly from RStudio: if you are used to R Markdown, Quarto is the new and better R Markdown.\nWhen using Python or Julia, you can use Quarto directly from a Jupyter notebook (with .ipynb extension).\n\nUsing Quarto directly from a Jupyter notebook:\n\n From Quarto documentation\nIn this workshop, we will see the most general workflow: simply using a text editor.\n\n\n\n\n\n\nSupported languages\n\n\n\n\n\nQuarto renders highlighting in countless languages and generates dynamic output for code blocks in:\n\nPython\nR\nJulia\nObservable JS\n\nYou can render documents in a wide variety of formats:\n\nHTML\nPDF\nMS Word\nOpenOffice\nePub\nRevealjs\nPowerPoint\nBeamer\nGitHub Markdown\nCommonMark\nHugo\nDocusaurus\nMarkua\nMediaWiki\nDokuWiki\nZimWiki\nJira Wiki\nXWiki\nJATS\nJupyter\nConTeXt\nRTF\nreST\nAsciiDoc\nOrg-Mode\nMuse\nGNU\nGroff\n\nThis training website is actually built with Quarto!"
  },
  {
    "objectID": "tools/quarto.html#installation",
    "href": "tools/quarto.html#installation",
    "title": "Authoring scientific documents with Markdown and Quarto",
    "section": "Installation",
    "text": "Installation\n\nDownload Quarto here.\nDownload the language(s) (R, Python, or Julia) you will want to use with Quarto as well as their corresponding engine (knitr for R; Jupyter for Python and Julia):\n\nIf you want to use Quarto with R, you will need:\n\nR (download here if you don‚Äôt have R already on your system),\nthe rmarkdown package. For this, launch R and run:\n\ninstall.packages(\"rmarkdown\")\nIf you want to use it with Python, you will need:\n\nPython 3 (download here if don‚Äôt have it on your system),\nJupyterLab. For this, open a terminal and run:\n\npython3 -m pip install jupyterlab  # if you are on MacOS or Linux\npy -m pip install jupyterlab       # if you are on Windows\nFinally, if you want to use Quarto with Julia, you will need:\n\nJulia (download here if you don‚Äôt have Julia),\nthe IJulia and Revise packages. For this, launch Julia and run:\n\n] add IJulia Revise\n# <Backspace>\nusing IJulia\nnotebook()      # to install a minimal Python+Jupyter distribution\nRunning notebook() allows you to install Jupyter if you don‚Äôt already have it."
  },
  {
    "objectID": "tools/quarto.html#document-structure-and-syntax",
    "href": "tools/quarto.html#document-structure-and-syntax",
    "title": "Authoring scientific documents with Markdown and Quarto",
    "section": "Document structure and syntax",
    "text": "Document structure and syntax\n\nFront matter\nWritten in YAML. Sets the options for the document. Let‚Äôs see a few examples.\n\nHTML output:\n\n---\ntitle: \"My title\"\nauthor: \"My name\"\nformat: html\n---\n\nHTML output with a few options:\n\n---\ntitle: \"My title\"\nauthor: \"My name\"\nformat:\n  html:\n    toc: true\n    css: <my_file>.css\n---\nThe above examples would work if you don‚Äôt use any code blocks or if you use R code blocks. If you use Python or Julia however, you need to add a jupyter entry with the name of the language that should run in Jupyter.\n\nMS Word output with Python code blocks:\n\n---\ntitle: \"My title\"\nauthor: \"My name\"\nformat: docx\njupyter: python3\n---\n\nrevealjs output with some options and Julia code blocks:\n\n---\ntitle: \"Some title\"\nsubtitle: \"Some subtitle\"\ninstitute: \"Simon Fraser University\"\ndate: \"2022-11-24\"\nexecute:\n  error: true\n  echo: true\nformat:\n  revealjs:\n    theme: [default, custom.scss]\n    highlight-style: monokai\n    code-line-numbers: false\n    embed-resources: true\njupyter: julia-1.8\n---\nSee the Quarto documentation for an exhaustive list of options for all formats.\n\n\nWritten sections\nWritten sections are written in Pandoc‚Äôs extended Markdown.\n\n\nCode blocks\nIf all you want is syntax highlighting of the code blocks, use this syntax:\n```{.language}\n<some code>\n```\nIf you want syntax highlighting of the blocks and for the code to run, use instead:\n```{language}\n<some code>\n```\nIn addition, options can be added to individual code blocks:\n```{language}\n#| <some option>: <some option value>\n\n<some code>\n```"
  },
  {
    "objectID": "tools/quarto.html#rendering",
    "href": "tools/quarto.html#rendering",
    "title": "Authoring scientific documents with Markdown and Quarto",
    "section": "Rendering",
    "text": "Rendering\nUsing Quarto is very simple: there are only two commands you need to know.\nIn a terminal, simply run either of:\nquarto render <file>.qmd     # this will render the document\nquarto preview <file>.qmd    # this will display live preview as you work on your document"
  },
  {
    "objectID": "tools/quarto.html#revealjs-presentation",
    "href": "tools/quarto.html#revealjs-presentation",
    "title": "Authoring scientific documents with Markdown and Quarto",
    "section": "Revealjs presentation",
    "text": "Revealjs presentation\n\n\n\n\n\n\nCode\n\n\n\n\n\n---\ntitle: \"My title\"\nauthor: \"My name\"\ninstitute: \"Simon Fraser University\"\nformat:\n  revealjs:\n    highlight-style: monokai\n    code-line-numbers: false\n    embed-resources: true\n---\n\n## First section\n\nWhen exporting to revealjs, second level sections mark the start of new slides,\nwith a slide title.\n\nThis can be changed in options.\n\n---\n\nNew slides can be started without titles this way.\n\n# There are title slides\n\n## Formatting\n\nText can be rendered *in italic* or **in bold** as well as [underlined]{.underline}.\n\nYou can use superscripts^2^, subscripts~test~, ~~strikethrough~~, and `inline code`.\n\n> This is a quote.\n\n## Columns\n\n:::: {.columns}\n\n::: {.column width=\"30%\"}\n\nYou can create columns.\n\n:::\n\n::: {.column width=\"70%\"}\n\nAnd you can set their respective width.\n\n:::\n\n::::\n\n## Lists\n\n::: {.incremental}\n\n- List can happen one line at a time\n- like\n- this\n\n:::\n\n## Lists\n\n- Or all at the same time\n- like\n- that\n\n## Ordered lists\n\n1. Item 1\n2. Item 2\n3. Item 3\n\n## Images\n\n![Example image](qmd_jupyter.png)\n\n## Tables\n\n| Col 1 | Col 2 | Col 3  |\n|------ |-------|--------|\n| a     | 1     | red    |\n| b     | 2     | orange | \n| c     | 3     | yellow |\n\n:::{.callout-note}\n\nTables can be fully customized (or you could use raw html).\n\n:::\n\n## Equations\n\n$$\n\\frac{\\partial \\mathrm C}{ \\partial \\mathrm t } + \\frac{1}{2}\\sigma^{2} \\mathrm S^{2}\n\\frac{\\partial^{2} \\mathrm C}{\\partial \\mathrm C^2}\n  + \\mathrm r \\mathrm S \\frac{\\partial \\mathrm C}{\\partial \\mathrm S}\\ =\n  \\mathrm r \\mathrm C \n$$\n\n\n\nRendered document (click on it to open it in a new tab):"
  },
  {
    "objectID": "tools/quarto.html#pdf",
    "href": "tools/quarto.html#pdf",
    "title": "Authoring scientific documents with Markdown and Quarto",
    "section": "pdf",
    "text": "pdf\n\n\n\n\n\n\nCode\n\n\n\n\n\n---\ntitle: \"My title\"\nauthor: \"My name\"\nformat:\n  pdf:\n    toc: true\n---\n\n# Header 1\n\nSome text.\n\n## Header 2\n\nMore text.\n\n## Formatting\n\nText can be rendered *in italic* or **in bold** as well as [underlined]{.underline}.\n\nYou can use superscripts^2^, subscripts~test~, ~~strikethrough~~, and `inline code`.\n\n> This is a quote.\n\n## Lists\n\n### Unordered\n\n- Item 1\n- Item 2\n- Item 3\n\n### Ordered\n\n1. Item 1\n2. Item 2\n3. Item 3\n\n## Images\n\n![Example image](qmd_jupyter.png)\n\n## Tables\n\n| Col 1 | Col 2 | Col 3  |\n|------ |-------|--------|\n| a     | 1     | red    |\n| b     | 2     | orange | \n| c     | 3     | yellow |\n\n:::{.callout-note}\n\nTables can be fully customized (or you could use raw html).\n\n:::\n\n## Equations\n\n$$\n\\frac{\\partial \\mathrm C}{ \\partial \\mathrm t } + \\frac{1}{2}\\sigma^{2} \\mathrm S^{2}\n\\frac{\\partial^{2} \\mathrm C}{\\partial \\mathrm C^2}\n  + \\mathrm r \\mathrm S \\frac{\\partial \\mathrm C}{\\partial \\mathrm S}\\ =\n  \\mathrm r \\mathrm C \n$$\n\n\n\nRendered document (click on it to open it in a new tab):\n\n\n\n\n\n\nIn order to export to pdf, you need a TeX distribution. You probably already have one installed on your machine, so you should first try to render or preview a document to pdf to see whether it works. If it doesn‚Äôt work, you can install the minimalist distribution TinyTex by running in your terminal:\n\nquarto install tool tinytex"
  },
  {
    "objectID": "tools/quarto.html#html-with-r-code-blocks",
    "href": "tools/quarto.html#html-with-r-code-blocks",
    "title": "Authoring scientific documents with Markdown and Quarto",
    "section": "HTML with R code blocks",
    "text": "HTML with R code blocks\n\n\n\n\n\n\nCode\n\n\n\n\n\n---\ntitle: \"My title\"\nauthor: \"My name\"\ninstitute: \"Simon Fraser University\"\nformat: html\n---\n\n# Header 1\n\n## Header 2\n\nSome text.\n\n## Formatting  {#sec-formatting}\n\n::: aside\n\nNote that each header automatically creates an anchor,\nmaking it easy to link to specific sections of your documents.\n\n:::\n\nText can be rendered *in italic* or **in bold** as well as [underlined]{.underline}.\n\nYou can use superscripts^2^, subscripts~test~, ~~strikethrough~~, and `inline code`.\n\n> This is a quote.\n\n## Columns\n\n:::: {.columns}\n\n::: {.column width=\"30%\"}\n\nYou can create columns.\n\n:::\n\n::: {.column width=\"70%\"}\n\nAnd you can set their respective width.\n\n:::\n\n::::\n\n## Lists\n\n- Item 1\n- Item 2\n- Item 3\n\n## Ordered lists\n\n1. Item 1\n2. Item 2\n3. Item 3\n\n## Images\n\n![Example image](qmd_jupyter.png)\n\n## Tables\n\n| Col 1 | Col 2 | Col 3  |\n|------ |-------|--------|\n| a     | 1     | red    |\n| b     | 2     | orange | \n| c     | 3     | yellow |\n\n:::{.callout-note}\n\nTables can be fully customized (or you could use raw html).\n\n:::\n\n## Equations\n\n$$\n\\frac{\\partial \\mathrm C}{ \\partial \\mathrm t } + \\frac{1}{2}\\sigma^{2} \\mathrm S^{2}\n\\frac{\\partial^{2} \\mathrm C}{\\partial \\mathrm C^2}\n  + \\mathrm r \\mathrm S \\frac{\\partial \\mathrm C}{\\partial \\mathrm S}\\ =\n  \\mathrm r \\mathrm C \n$$\n\n## Cross-references\n\nSee @sec-formatting.\n\n*Note that you can add bibliographies, flow charts, the equivalent of HTML \"div\",\nand just so much more. Remember that this is a tiny overview.*\n\n## Let's try some code blocks now\n\n```{r}\n# This is a block that runs\n2 + 3\n```\n\n::: aside\n\nDid you notice that the content of your code blocks can be copied with a click?\nOf course, this is customizable.\n\n:::\n\n```{.r}\n# This is a block that doesn't run\n2 + 3\n```\n\n```{r}\n#| echo: false\n# And this is a block showing only the output\ndata.frame(\n  country = c(\"Canada\", \"USA\", \"Mexico\"),\n  var = c(2.9, 3.1, 4.5)\n)\n```\n\n## Plots\n\n```{r}\nplot(cars)\n```\n\n<br>\nYou can play with options to add a title:\n\n```{r}\n#| fig-cap: \"Stopping distance as a function of speed in cars\"\n\nplot(cars)\n```\n\n<br>\nYou can have more complex multi-plot layouts:\n\n```{r}\n#| layout-ncol: 2\n#| fig-cap: \n#|   - \"Stopping distance as a function of speed in cars\"\n#|   - \"Vapor pressure of mercury as a function of temperature\"\n\nplot(cars)\nplot(pressure)\n```\n\nFor those who have `ggplot2`[^1], you can try that too:\n\n```{r}\nlibrary(ggplot2)\n\nggplot(data = mpg, mapping = aes(x = displ, y = hwy)) + \n  geom_point(mapping = aes(color = class)) + \n  geom_smooth()\n```\n\n[^1]: You can install it with:\n    ```{.r}\n    install.packages(\"ggplot2\")\n    ```\n\n\n\nRendered document (click on it to open it in a new tab):"
  },
  {
    "objectID": "tools/quarto.html#beamer-with-python-code-blocks",
    "href": "tools/quarto.html#beamer-with-python-code-blocks",
    "title": "Authoring scientific documents with Markdown and Quarto",
    "section": "Beamer with Python code blocks",
    "text": "Beamer with Python code blocks\nBeamer is LaTeX presentation framework: a way to create beautiful pdf slides.\n\n\n\n\n\n\nCode\n\n\n\n\n\n---\ntitle: \"Some title\"\nauthor: \"Some name\"\nformat: beamer\njupyter: python3\n---\n\n## First slide\n\nWith some content\n\n## Formatting\n\nText can be rendered *in italic* or **in bold** as well as [underlined]{.underline}.\n\nYou can use superscripts^2^, subscripts~test~, ~~strikethrough~~, and `inline code`.\n\n## Lists\n\n- Item 1\n- Item 2\n- Item 3\n\n## Ordered lists\n\n1. Item 1\n2. Item 2\n3. Item 3\n\n## Images\n\n![Example image](qmd_jupyter.png)\n\n## Tables\n\n| Col 1 | Col 2 | Col 3  |\n|------ |-------|--------|\n| a     | 1     | red    |\n| b     | 2     | orange | \n| c     | 3     | yellow |\n\n:::{.callout-note}\n\nTables can be fully customized (or you could use raw html).\n\n:::\n\n## Equations\n\n$$\n\\frac{\\partial \\mathrm C}{ \\partial \\mathrm t } + \\frac{1}{2}\\sigma^{2} \\mathrm S^{2}\n\\frac{\\partial^{2} \\mathrm C}{\\partial \\mathrm C^2}\n  + \\mathrm r \\mathrm S \\frac{\\partial \\mathrm C}{\\partial \\mathrm S}\\ =\n  \\mathrm r \\mathrm C \n$$\n\n## Some basic code block\n\n```{python}\n#| echo: true\n\n2 + 3\n```\n\n## Some plot\n\n```{python}\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Data for plotting\nt = np.arange(0.0, 2.0, 0.01)\ns = 1 + np.sin(2 * np.pi * t)\n\nfig, ax = plt.subplots()\nax.plot(t, s)\n\nax.set(xlabel='time (s)', ylabel='voltage (mV)',\n       title='Here goes the title')\nax.grid()\n\nfig.savefig(\"test.png\")\nplt.show()\n```\n\n\n\nRendered document (click on it to open it in a new tab):"
  }
]