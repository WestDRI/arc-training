[
  {
    "objectID": "tools/quarto_webinar_slides.html#markup-languages",
    "href": "tools/quarto_webinar_slides.html#markup-languages",
    "title": "The new R Markdown:",
    "section": "Markup languages",
    "text": "Markup languages\n\nControl the formatting of text documents\nPowerful but the unrendered text is visually cluttered and hard to read"
  },
  {
    "objectID": "tools/quarto_webinar_slides.html#markup-languages-1",
    "href": "tools/quarto_webinar_slides.html#markup-languages-1",
    "title": "The new R Markdown:",
    "section": "Markup languages",
    "text": "Markup languages\n\nControl the formatting of text documents\nPowerful but the unrendered text is visually cluttered and hard to read\n\n\nExample: Tex—often with macro package LaTeX—to create pdfs\n\n\\documentclass{article}\n\\title{My title}\n\\author{My name}\n\\usepackage{datetime}\n\\newdate{date}{24}{11}{2022}\n\\date{\\displaydate{date}}\n\\begin{document}\n \\maketitle\n \\section{First section}\n Some text in the first section.\n\\end{document}"
  },
  {
    "objectID": "tools/quarto_webinar_slides.html#markup-languages-2",
    "href": "tools/quarto_webinar_slides.html#markup-languages-2",
    "title": "The new R Markdown:",
    "section": "Markup languages",
    "text": "Markup languages\n\nControl the formatting of text documents\nPowerful but the unrendered text is visually cluttered and hard to read\n\n\nExample: HTML—often with css/scss files—to create webpages\n\n&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en-US\"&gt;\n  &lt;head&gt;\n    &lt;meta charset=\"utf-8\" /&gt;\n    &lt;meta name=\"viewport\" content=\"width=device-width\" /&gt;\n    &lt;title&gt;My title&lt;/title&gt;\n    &lt;address class=\"author\"&gt;My name&lt;/address&gt;\n    &lt;input type=\"date\" value=\"2022-11-24\" /&gt;\n  &lt;/head&gt;\n  &lt;h1&gt;First section&lt;/h1&gt;\n  &lt;body&gt;\n    Some text in the first section.\n  &lt;/body&gt;\n&lt;/html&gt;"
  },
  {
    "objectID": "tools/quarto_webinar_slides.html#markdown",
    "href": "tools/quarto_webinar_slides.html#markdown",
    "title": "The new R Markdown:",
    "section": "Markdown",
    "text": "Markdown\n\nRemoves the visual clutter and makes texts readable prior to rendering\nCreated in 2004\nBy now quasi-ubiquitous\nInitially created for webpages\nRaw HTML can be inserted when easy syntax falls short\n\n\nPandoc’s extended Markdown\nPandoc (free and open-source markup formats converter) supports an extended Markdown syntax with functionality for figures, tables, callout blocks, LaTeX equations, citations…\nRemains as readable as basic Markdown, but can be rendered in any format (pdf, books, entire websites, Word documents…)"
  },
  {
    "objectID": "tools/quarto_webinar_slides.html#markdown-1",
    "href": "tools/quarto_webinar_slides.html#markdown-1",
    "title": "The new R Markdown:",
    "section": "Markdown",
    "text": "Markdown\n\nRemoves the visual clutter and makes texts readable prior to rendering\nCreated in 2004\nBy now quasi-ubiquitous\nInitially created for webpages\nRaw HTML can be inserted when easy syntax falls short\n\n\nPrevious example using Pandoc’s Markdown:\n\n---\ntitle: My title\nauthor: My name\ndate: 2022-11-24\n---\n# First section\nSome text in the first section."
  },
  {
    "objectID": "tools/quarto_webinar_slides.html#how-it-works",
    "href": "tools/quarto_webinar_slides.html#how-it-works",
    "title": "The new R Markdown:",
    "section": "How it works",
    "text": "How it works\nCode blocks are executed by Jupyter (Python or Julia) or knitr (R), then pandoc renders the document into any format\n\nJulia/Python:\n From Quarto documentation\nR:\n From Quarto documentation"
  },
  {
    "objectID": "tools/quarto_webinar_slides.html#how-it-works-1",
    "href": "tools/quarto_webinar_slides.html#how-it-works-1",
    "title": "The new R Markdown:",
    "section": "How it works",
    "text": "How it works\nCode blocks are executed by Jupyter (Python or Julia) or knitr (R), then pandoc renders the document into any format\nCan be used from .qmd text files or directly from RStudio or Jupyter notebooks."
  },
  {
    "objectID": "tools/quarto_webinar_slides.html#supported-languages",
    "href": "tools/quarto_webinar_slides.html#supported-languages",
    "title": "The new R Markdown:",
    "section": "Supported languages",
    "text": "Supported languages\nSyntax highlighting in pretty much any language\n\nExecutable code blocks in Python, R, Julia, Observable JS\n\n\nOutput formats\n- HTML\n- PDF\n- MS Word\n- OpenOffice\n- ePub\n- Revealjs\n- PowerPoint\n- Beamer\n- GitHub Markdown\n- CommonMark\n- Hugo\n- Docusaurus\n- Markua\n- MediaWiki\n- DokuWiki\n- ZimWiki\n- Jira Wiki\n- XWiki\n- JATS\n- Jupyter\n- ConTeXt\n- RTF\n- reST\n- AsciiDoc\n- Org-Mode\n- Muse\n- GNU\n- Groff"
  },
  {
    "objectID": "tools/quarto_webinar_slides.html#document-structure-syntax-front-matter",
    "href": "tools/quarto_webinar_slides.html#document-structure-syntax-front-matter",
    "title": "The new R Markdown:",
    "section": "Document structure & syntax: front matter",
    "text": "Document structure & syntax: front matter\nWritten in YAML\nSets the options for the document. Let’s see a few examples.\n\n\nCan be very basic:\n\n---\ntitle: \"My title\"\nauthor: \"My name\"\nformat: html\n---"
  },
  {
    "objectID": "tools/quarto_webinar_slides.html#document-structure-syntax-front-matter-1",
    "href": "tools/quarto_webinar_slides.html#document-structure-syntax-front-matter-1",
    "title": "The new R Markdown:",
    "section": "Document structure & syntax: front matter",
    "text": "Document structure & syntax: front matter\nWritten in YAML\nSets the options for the document. Let’s see a few examples.\n\nOr more sophisticated:\n\n---\ntitle: \"Some title\"\nsubtitle: \"Some subtitle\"\ninstitute: \"Simon Fraser University\"\ndate: \"2022-11-24\"\nexecute:\n  error: true\n  echo: true\nformat:\n  revealjs:\n    theme: [default, custom.scss]\n    highlight-style: monokai\n    code-line-numbers: false\n    embed-resources: true\n---"
  },
  {
    "objectID": "tools/quarto_webinar_slides.html#document-structure-syntax-text",
    "href": "tools/quarto_webinar_slides.html#document-structure-syntax-text",
    "title": "The new R Markdown:",
    "section": "Document structure & syntax: text",
    "text": "Document structure & syntax: text\nWritten in Pandoc’s extended Markdown"
  },
  {
    "objectID": "tools/quarto_webinar_slides.html#document-structure-syntax-code-blocks",
    "href": "tools/quarto_webinar_slides.html#document-structure-syntax-code-blocks",
    "title": "The new R Markdown:",
    "section": "Document structure & syntax: code blocks",
    "text": "Document structure & syntax: code blocks\nSyntax highlighting only:\n{{.language}} code\n\nSyntax highlighting and code execution:\n```{language}\ncode\n```\n\n\nOptions can be added to individual blocks:\n```{language}\n#| option: value\n\ncode\n```"
  },
  {
    "objectID": "tools/quarto_webinar_slides.html#rendering",
    "href": "tools/quarto_webinar_slides.html#rendering",
    "title": "The new R Markdown:",
    "section": "Rendering",
    "text": "Rendering\nTwo commands:\nquarto render file.qmd     # Renders the document\nquarto preview file.qmd    # Displays a live preview"
  },
  {
    "objectID": "tools/quarto_webinar_slides.html#some-advantages-of-quarto",
    "href": "tools/quarto_webinar_slides.html#some-advantages-of-quarto",
    "title": "The new R Markdown:",
    "section": "Some advantages of Quarto",
    "text": "Some advantages of Quarto\nGeneral considerations\n\nExtremely well documented\nSolid team behind the work\nFree and open source\nUses only well established and well tested tools"
  },
  {
    "objectID": "tools/quarto_webinar_slides.html#some-advantages-of-quarto-1",
    "href": "tools/quarto_webinar_slides.html#some-advantages-of-quarto-1",
    "title": "The new R Markdown:",
    "section": "Some advantages of Quarto",
    "text": "Some advantages of Quarto\nWebpages/websites\n\nFast, easy, and clean\nSites work on screens of any size out of the box (uses Bootstrap 5)\nCan be customized with CSS/SCSS, but good out of the box\nCode blocks can have a copy button\nGreat search functionality\nSite/pages can be hosted anywhere easily"
  },
  {
    "objectID": "tools/quarto_webinar_slides.html#some-advantages-of-quarto-2",
    "href": "tools/quarto_webinar_slides.html#some-advantages-of-quarto-2",
    "title": "The new R Markdown:",
    "section": "Some advantages of Quarto",
    "text": "Some advantages of Quarto\nAdvantages of code execution\n\nPeople can see code outputs without running code\nForces to test every bit of code\nNo need for a complex system linking code scripts with publishing documents"
  },
  {
    "objectID": "tools/quarto_webinar_slides.html#resources",
    "href": "tools/quarto_webinar_slides.html#resources",
    "title": "The new R Markdown:",
    "section": "Resources",
    "text": "Resources\nOfficial sites\nWebsite\nRepo\nDocumentation index\nInstallation\nYou can find information in the Quarto documentation or in our previous workshop on Quarto\nBasic examples\nYou can find several examples in our previous workshop on Quarto"
  },
  {
    "objectID": "tools/quarto_staff2staff_slides.html#markup-languages",
    "href": "tools/quarto_staff2staff_slides.html#markup-languages",
    "title": "Quarto as a great teaching tool",
    "section": "Markup languages",
    "text": "Markup languages\n\nControl the formatting of text documents\nPowerful but the unrendered text is visually cluttered and hard to read"
  },
  {
    "objectID": "tools/quarto_staff2staff_slides.html#markup-languages-1",
    "href": "tools/quarto_staff2staff_slides.html#markup-languages-1",
    "title": "Quarto as a great teaching tool",
    "section": "Markup languages",
    "text": "Markup languages\n\nControl the formatting of text documents\nPowerful but the unrendered text is visually cluttered and hard to read\n\n\nExample: Tex—often with macro package LaTeX—to create pdfs\n\n\\documentclass{article}\n\\title{My title}\n\\author{My name}\n\\usepackage{datetime}\n\\newdate{date}{24}{11}{2022}\n\\date{\\displaydate{date}}\n\\begin{document}\n \\maketitle\n \\section{First section}\n Some text in the first section.\n\\end{document}"
  },
  {
    "objectID": "tools/quarto_staff2staff_slides.html#markup-languages-2",
    "href": "tools/quarto_staff2staff_slides.html#markup-languages-2",
    "title": "Quarto as a great teaching tool",
    "section": "Markup languages",
    "text": "Markup languages\n\nControl the formatting of text documents\nPowerful but the unrendered text is visually cluttered and hard to read\n\n\nExample: HTML—often with css/scss files—to create webpages\n\n&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en-US\"&gt;\n  &lt;head&gt;\n    &lt;meta charset=\"utf-8\" /&gt;\n    &lt;meta name=\"viewport\" content=\"width=device-width\" /&gt;\n    &lt;title&gt;My title&lt;/title&gt;\n    &lt;address class=\"author\"&gt;My name&lt;/address&gt;\n    &lt;input type=\"date\" value=\"2022-11-24\" /&gt;\n  &lt;/head&gt;\n  &lt;h1&gt;First section&lt;/h1&gt;\n  &lt;body&gt;\n    Some text in the first section.\n  &lt;/body&gt;\n&lt;/html&gt;"
  },
  {
    "objectID": "tools/quarto_staff2staff_slides.html#markdown",
    "href": "tools/quarto_staff2staff_slides.html#markdown",
    "title": "Quarto as a great teaching tool",
    "section": "Markdown",
    "text": "Markdown\n\nRemoves the visual clutter and makes texts readable prior to rendering\nCreated in 2004\nBy now quasi-ubiquitous\nInitially created for webpages\nRaw HTML can be inserted when easy syntax falls short\n\n\nPandoc’s extended Markdown\nPandoc (free and open-source markup formats converter) supports an extended Markdown syntax with functionality for figures, tables, callout blocks, LaTeX equations, citations…\nRemains as readable as basic Markdown, but can be rendered in any format (pdf, books, entire websites, Word documents…)"
  },
  {
    "objectID": "tools/quarto_staff2staff_slides.html#markdown-1",
    "href": "tools/quarto_staff2staff_slides.html#markdown-1",
    "title": "Quarto as a great teaching tool",
    "section": "Markdown",
    "text": "Markdown\n\nRemoves the visual clutter and makes texts readable prior to rendering\nCreated in 2004\nBy now quasi-ubiquitous\nInitially created for webpages\nRaw HTML can be inserted when easy syntax falls short\n\n\nPrevious example using Pandoc’s Markdown:\n\n---\ntitle: My title\nauthor: My name\ndate: 2022-11-24\n---\n# First section\nSome text in the first section."
  },
  {
    "objectID": "tools/quarto_staff2staff_slides.html#how-it-works",
    "href": "tools/quarto_staff2staff_slides.html#how-it-works",
    "title": "Quarto as a great teaching tool",
    "section": "How it works",
    "text": "How it works\nCode blocks are executed by Jupyter (Python or Julia) or knitr (R), then pandoc renders the document into any format\n\nJulia/Python:\n From Quarto documentation\nR:\n From Quarto documentation"
  },
  {
    "objectID": "tools/quarto_staff2staff_slides.html#how-it-works-1",
    "href": "tools/quarto_staff2staff_slides.html#how-it-works-1",
    "title": "Quarto as a great teaching tool",
    "section": "How it works",
    "text": "How it works\nCode blocks are executed by Jupyter (Python or Julia) or knitr (R), then pandoc renders the document into any format\nCan be used from .qmd text files or directly from RStudio or Jupyter notebooks."
  },
  {
    "objectID": "tools/quarto_staff2staff_slides.html#supported-languages",
    "href": "tools/quarto_staff2staff_slides.html#supported-languages",
    "title": "Quarto as a great teaching tool",
    "section": "Supported languages",
    "text": "Supported languages\nSyntax highlighting in pretty much any language\n\nExecutable code blocks in Python, R, Julia, Observable JS\n\n\nOutput formats\n- HTML\n- PDF\n- MS Word\n- OpenOffice\n- ePub\n- Revealjs\n- PowerPoint\n- Beamer\n- GitHub Markdown\n- CommonMark\n- Hugo\n- Docusaurus\n- Markua\n- MediaWiki\n- DokuWiki\n- ZimWiki\n- Jira Wiki\n- XWiki\n- JATS\n- Jupyter\n- ConTeXt\n- RTF\n- reST\n- AsciiDoc\n- Org-Mode\n- Muse\n- GNU\n- Groff"
  },
  {
    "objectID": "tools/quarto_staff2staff_slides.html#document-structure-syntax-front-matter",
    "href": "tools/quarto_staff2staff_slides.html#document-structure-syntax-front-matter",
    "title": "Quarto as a great teaching tool",
    "section": "Document structure & syntax: front matter",
    "text": "Document structure & syntax: front matter\nWritten in YAML\nSets the options for the document. Let’s see a few examples.\n\n\nCan be very basic:\n\n---\ntitle: \"My title\"\nauthor: \"My name\"\nformat: html\n---"
  },
  {
    "objectID": "tools/quarto_staff2staff_slides.html#document-structure-syntax-front-matter-1",
    "href": "tools/quarto_staff2staff_slides.html#document-structure-syntax-front-matter-1",
    "title": "Quarto as a great teaching tool",
    "section": "Document structure & syntax: front matter",
    "text": "Document structure & syntax: front matter\nWritten in YAML\nSets the options for the document. Let’s see a few examples.\n\nOr more sophisticated:\n\n---\ntitle: \"Some title\"\nsubtitle: \"Some subtitle\"\ninstitute: \"Simon Fraser University\"\ndate: \"2022-11-24\"\nexecute:\n  error: true\n  echo: true\nformat:\n  revealjs:\n    theme: [default, custom.scss]\n    highlight-style: monokai\n    code-line-numbers: false\n    embed-resources: true\n---"
  },
  {
    "objectID": "tools/quarto_staff2staff_slides.html#document-structure-syntax-text",
    "href": "tools/quarto_staff2staff_slides.html#document-structure-syntax-text",
    "title": "Quarto as a great teaching tool",
    "section": "Document structure & syntax: text",
    "text": "Document structure & syntax: text\nWritten in Pandoc’s extended Markdown"
  },
  {
    "objectID": "tools/quarto_staff2staff_slides.html#document-structure-syntax-code-blocks",
    "href": "tools/quarto_staff2staff_slides.html#document-structure-syntax-code-blocks",
    "title": "Quarto as a great teaching tool",
    "section": "Document structure & syntax: code blocks",
    "text": "Document structure & syntax: code blocks\nSyntax highlighting only:\n{{.language}} code\n\nSyntax highlighting and code execution:\n```{language}\ncode\n```\n\n\nOptions can be added to individual blocks:\n```{language}\n#| option: value\n\ncode\n```"
  },
  {
    "objectID": "tools/quarto_staff2staff_slides.html#rendering",
    "href": "tools/quarto_staff2staff_slides.html#rendering",
    "title": "Quarto as a great teaching tool",
    "section": "Rendering",
    "text": "Rendering\nTwo commands:\nquarto render file.qmd     # Renders the document\nquarto preview file.qmd  # Displays a live preview"
  },
  {
    "objectID": "tools/quarto_staff2staff_slides.html#general-considerations",
    "href": "tools/quarto_staff2staff_slides.html#general-considerations",
    "title": "Quarto as a great teaching tool",
    "section": "General considerations",
    "text": "General considerations\n\nExtremely well documented\nSolid team behind the work\nFree and open source\nUses only well established and well tested tools"
  },
  {
    "objectID": "tools/quarto_staff2staff_slides.html#webpageswebsites",
    "href": "tools/quarto_staff2staff_slides.html#webpageswebsites",
    "title": "Quarto as a great teaching tool",
    "section": "Webpages/websites",
    "text": "Webpages/websites\n\nFast, easy, and clean\nSites work on screens of any size out of the box (uses Bootstrap 5)\nCan be customized with CSS/SCSS, but good out of the box\nCode blocks can have a little copy button\nSite/pages can be hosted anywhere easily"
  },
  {
    "objectID": "tools/quarto_staff2staff_slides.html#advantages-of-code-execution",
    "href": "tools/quarto_staff2staff_slides.html#advantages-of-code-execution",
    "title": "Quarto as a great teaching tool",
    "section": "Advantages of code execution",
    "text": "Advantages of code execution\n\nPeople can see the output without running the code\nForces to test every bit of code\nIf the code broke when giving an old workshop, prevents the embarrassment of discovering it in the middle of a live demo\nNo need for a complex system linking code scripts with teaching documents"
  },
  {
    "objectID": "tools/index.html",
    "href": "tools/index.html",
    "title": "Research tools",
    "section": "",
    "text": "Workshops\nWorkshops on various computing tools\n\n\n\n\nWebinars\n60 min webinars on various research tools"
  },
  {
    "objectID": "tools/help.html",
    "href": "tools/help.html",
    "title": "So, you are stuck … now what?",
    "section": "",
    "text": "Stack Overflow, Stack Exchange, Discourse forums, and other online platforms … the internet is a treasure trove of online communities where you can find solutions to your coding problems. To have a positive experience and get the answers you need however, you have to know where to ask, how to ask, and when not to ask: if countless people are willing to give you their time for free, they usually expect that you do your part.\nIn this webinar, I will present key online sites, their functioning, and their culture; then I will go over the magic trick to get answers to your questions: knowing how to create minimum reproducible examples. I will not focus on any particular language as the principles (how to create a minimal dataset, how to deal with private data, how to create self-sufficient code, how to reproduce the problem, etc.) can apply to any language.\n\nSlides (Click and wait: this reveal.js presentation may take a little time to load.)"
  },
  {
    "objectID": "tools/emacs_prog_ide.html",
    "href": "tools/emacs_prog_ide.html",
    "title": "Emacs as a programming IDE",
    "section": "",
    "text": "Once upon a time (not that long ago), powerful text editors such as Vim and Emacs were the only nice interfaces to work with code. Nowadays, there are countless sleek and more GUI-oriented tools such as VS Code, RStudio, or JupyterLab that provide amazing IDEs, without the learning curve.\nSo why would one still use Emacs as a programming IDE?\nWhat does that even look like?\nIn this webinar, I will show some of the many reasons why I can’t let go of Emacs, then show how it can be used as a programming IDE for Python, R, and Julia.\n\nSlides (Click and wait: this reveal.js presentation may take a little time to load.)"
  },
  {
    "objectID": "talks/2023_driconnect_slides.html#topics",
    "href": "talks/2023_driconnect_slides.html#topics",
    "title": "The instruments for advanced research computing are here, but are researchers ready?",
    "section": "Topics",
    "text": "Topics\n\nUnix shell\nHPC\nVersion control with Git/DataLad\nScientific programming in R/Python/Julia\nParallel computing in R/Julia/Chapel\nDeep learning with PyTorch\nScientific visualization\nContainers/Alliance clouds/VMs\nWebscraping in R/Python\nGIS in R\nScientific publishing with Quarto"
  },
  {
    "objectID": "talks/2023_driconnect_slides.html#fast",
    "href": "talks/2023_driconnect_slides.html#fast",
    "title": "The instruments for advanced research computing are here, but are researchers ready?",
    "section": "Fast",
    "text": "Fast\n\n\n\n\n\n\n Back to talk page"
  },
  {
    "objectID": "r/ws_webscraping.html",
    "href": "r/ws_webscraping.html",
    "title": "Web scraping with R",
    "section": "",
    "text": "The internet is a trove of information. A lot of it is publicly available and thus suitable for use in research. Extracting that information and putting it in an organized format for analysis can, however, be extremely tedious. Web scraping tools allow to automate parts of that process and R is a popular language for the task.\nIn this workshop, we will guide you through a simple example using the package rvest."
  },
  {
    "objectID": "r/ws_webscraping.html#html-and-css",
    "href": "r/ws_webscraping.html#html-and-css",
    "title": "Web scraping with R",
    "section": "HTML and CSS",
    "text": "HTML and CSS\nHyperText Markup Language (HTML) is the standard markup language for websites: it encodes the information related to the formatting and structure of webpages. Additionally, some of the customization can be stored in Cascading Style Sheets (CSS) files.\nHTML uses tags of the form:\n&lt;some_tag&gt;Your content&lt;/some_tag&gt;\nSome tags have attributes:\n&lt;some_tag attribute_name=\"attribute value\"&gt;Your content&lt;/some_tag&gt;\n\nExamples:\n\nSite structure:\n\n&lt;h2&gt;This is a heading of level 2&lt;/h2&gt;\n&lt;p&gt;This is a paragraph&lt;/p&gt;\n\nFormatting:\n\n&lt;b&gt;This is bold&lt;/b&gt;\n&lt;a href=\"https://some.url\"&gt;This is the text for a link&lt;/a&gt;"
  },
  {
    "objectID": "r/ws_webscraping.html#web-scrapping",
    "href": "r/ws_webscraping.html#web-scrapping",
    "title": "Web scraping with R",
    "section": "Web scrapping",
    "text": "Web scrapping\nWeb scraping is a general term for a set of tools which allow for the extraction of data from the web automatically.\nWhile most of the data on the internet is publicly available, it is illegal to scrape some sites and you should always look into the policy of a site before attempting to scrape it. Some sites will also block you if you submit too many requests in a short amount of time, so if you plan on scraping sites at a fairly large scale, you should look into the polite package which will help you scrape responsibly."
  },
  {
    "objectID": "r/ws_webscraping.html#example-for-this-workshop",
    "href": "r/ws_webscraping.html#example-for-this-workshop",
    "title": "Web scraping with R",
    "section": "Example for this workshop",
    "text": "Example for this workshop\nWe will use a website from the University of Tennessee containing a database of PhD theses from that university.\nOur goal is to scrape data from this site to produce a dataframe with the date, major, and advisor for each dissertation.\n\nWe will only do this for the first page which contains the links to the 100 most recent theses. If you really wanted to gather all the data, you would have to do this for all pages."
  },
  {
    "objectID": "r/ws_webscraping.html#lets-look-at-the-sites",
    "href": "r/ws_webscraping.html#lets-look-at-the-sites",
    "title": "Web scraping with R",
    "section": "Let’s look at the sites",
    "text": "Let’s look at the sites\nFirst of all, let’s have a close look at the websites we want to scrape to think carefully about what we want to do. Before starting to write code, it is always a good idea to think about what you are trying to achieve with your code.\nTo create a dataframe with the data for all the dissertations on that first page, we need to do two things:\n\nStep 1: from the dissertations database first page, we want to scrape the list of URLs for the dissertation pages.\nStep 2: once we have the URLs, we want to scrape those pages too to get the date, major, and advisor for each dissertation."
  },
  {
    "objectID": "r/ws_webscraping.html#package",
    "href": "r/ws_webscraping.html#package",
    "title": "Web scraping with R",
    "section": "Package",
    "text": "Package\nTo do all this, we will use the package rvest, part of the tidyverse (a modern set of R packages). It is a package influenced by the popular Python package Beautiful Soup and it makes scraping websites with R really easy.\nLet’s load it:\n\nlibrary(rvest)"
  },
  {
    "objectID": "r/ws_webscraping.html#read-in-html-from-main-site",
    "href": "r/ws_webscraping.html#read-in-html-from-main-site",
    "title": "Web scraping with R",
    "section": "Read in HTML from main site",
    "text": "Read in HTML from main site\nAs mentioned above, our site is the database of PhD dissertations from the University of Tennessee.\nLet’s create a character vector with the URL:\n\nurl &lt;- \"https://trace.tennessee.edu/utk_graddiss/index.html\"\n\nFirst, we read in the html data from that page:\n\nhtml &lt;- read_html(url)\n\nLet’s have a look at the raw data:\n\nhtml\n\n{html_document}\n&lt;html lang=\"en\"&gt;\n[1] &lt;head&gt;\\n&lt;meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8 ...\n[2] &lt;body&gt;\\n&lt;!-- FILE /srv/sequoia/main/data/trace.tennessee.edu/assets/heade ..."
  },
  {
    "objectID": "r/ws_webscraping.html#test-run",
    "href": "r/ws_webscraping.html#test-run",
    "title": "Web scraping with R",
    "section": "Test run",
    "text": "Test run\n\nIdentify the relevant HTML markers\nThe html code for this webpage contains the data we are interested in, but it is mixed in with a lot of HTML formatting and data we don’t care about. We need to extract the data relevant to us and turn it into a workable format.\nThe first step is to find the HTML markers that contain our data. One option is to use a web inspector or—even easier—the SelectorGadget, a JavaScript bookmarklet built by Andrew Cantino.\nTo use this tool, go to the SelectorGadget website and drag the link of the bookmarklet to your bookmarks bar.\nNow, go to the dissertations database first page and click on the bookmarklet in your bookmarks bar. You will see a floating box at the bottom of your screen. As you move your mouse across the screen, an orange rectangle appears around each element over which you pass.\nClick on one of the dissertation links: now, there is an a appearing in the box at the bottom as well as the number of elements selected. The selected elements are highlighted in yellow. Those elements are links (in HTML, a tags define hyperlinks).\nAs you can see, all the links we want are selected. However, there are many other links we don’t want that are also highlighted. In fact, all links in the document are selected. We need to remove the categories of links that we don’t want. To do this, hover above any of the links we don’t want. You will see a red rectangle around it. Click on it: now all similar links are gone. You might have to do this a few times until only the relevant links (i.e. those that lead to the dissertation information pages) remain highlighted.\nAs there are 100 such links per page, the count of selected elements in the bottom floating box should be down to 100.\nIn the main section of the floating box, you can now see: .article-listing a. This means that the data we want are under the HTML elements .article-listing a (the class .article-listing and the tag a).\n\n\nExtract test URL\nIt is a good idea to test things out on a single element before doing a massive batch scraping of a site, so let’s test our method for the first dissertation.\nTo start, we need to extract the first URL. The function html_element() from the package rvest extracts the first element matching some character. Let’s pass to this function our html object and the character \".article-listing a\" and assign the result to an object that we will call test:\n\ntest &lt;- html %&gt;% html_element(\".article-listing a\")\n\nError in dyn.load(file, DLLpath = DLLpath, ...): unable to load shared object '/home/marie/R/lib/stringi/libs/stringi.so':\n  libicui18n.so.72: cannot open shared object file: No such file or directory\n\n\n\n%&gt;% is a pipe from the magrittr tidyverse package. It passes the output from the left-hand side expression as the first argument of the right-hand side expression. We could have written this as:\ntest &lt;- html_element(html, \".article-listing a\")\n\nOur new object is a list:\n\ntypeof(test)\n\nError in eval(expr, envir, enclos): object 'test' not found\n\n\nLet’s print it:\n\ntest\n\nError in eval(expr, envir, enclos): object 'test' not found\n\n\nThe URL is in there, so we successfully extracted the correct element, but we need to do more cleaning.\na is one of the HTML tags that have an attribute (href) as you can see when you print test. It is actually the value of that attribute that we want. To extract an attribute value, we use the function html_attr():\n\nurl_test &lt;- test %&gt;% html_attr(\"href\")\n\nError in eval(expr, envir, enclos): object 'test' not found\n\nurl_test\n\nError in eval(expr, envir, enclos): object 'url_test' not found\n\n\nThis is our URL.\n\nstr(url_test)\n\nError in eval(expr, envir, enclos): object 'url_test' not found\n\n\nIt is saved in a character vector, which is perfect.\n\nInstead of creating the intermediate objects html and test, we could have chained the functions:\n\nurl_test &lt;- read_html(url) %&gt;%\n  html_element(\".article-listing a\") %&gt;%\n  html_attr(\"href\")\n\nError in dyn.load(file, DLLpath = DLLpath, ...): unable to load shared object '/home/marie/R/lib/stringi/libs/stringi.so':\n  libicui18n.so.72: cannot open shared object file: No such file or directory\n\n\n\n\n\nRead in HTML data for test URL\nNow that we have the URL for the first dissertation information page, we want to extract the date, major, and advisor for that dissertation.\nWe just saw that url_test is a character vector representing a URL. We know how to deal with this.\nThe first thing to do—as we did earlier with the database site—is to read in the html data. Let’s assign it to a new object that we will call html_test:\n\nhtml_test &lt;- read_html(url_test)\n\nError in eval(expr, envir, enclos): object 'url_test' not found\n\nhtml_test\n\nError in eval(expr, envir, enclos): object 'html_test' not found\n\n\n\n\nGet data for test URL\nNow, we want to extract the publication date. Thanks to the SelectorGadget, following the method we saw earlier, we can see that we now need the element marked by #publication_date p.\nWe start by extracting the data as we did earlier by passing our object html_test and the character \"#publication_date p\" to html_element().\nWhile earlier we wanted the value of a tag attribute (i.e. part of the metadata), here we want the actual text (i.e. part of the actual content). To extract text from a snippet of HTML, we pass it to html_text2().\nLet’s run both operations at once to save the creation of an intermediate object:\n\ndate_test &lt;- html_test %&gt;%\n  html_element(\"#publication_date p\") %&gt;%\n  html_text2()\n\nError in eval(expr, envir, enclos): object 'html_test' not found\n\n\n\nNote the difference with what we did earlier to extract the URL: if we had used html_text2() then we would have gotten the text part of the link (\"The Novel Chlorination of Zirconium Metal and Its Application to a Recycling Protocol for Zircaloy Cladding from Spent Nuclear Fuel Rods\") rather than the URL (\"https://trace.tennessee.edu/utk_graddiss/7600\").\n\nLet’s verify that our date object indeed contains the date:\n\ndate_test\n\nError in eval(expr, envir, enclos): object 'date_test' not found\n\n\nWe also want the major for this thesis. The SelectorGadget allows us to find that this time, it is the #department p element that we need. Let’s extract it in the same fashion:\n\nmajor_test &lt;- html_test %&gt;%\n  html_element(\"#department p\") %&gt;%\n  html_text2()\n\nError in eval(expr, envir, enclos): object 'html_test' not found\n\nmajor_test\n\nError in eval(expr, envir, enclos): object 'major_test' not found\n\n\nAnd for the advisor, we need the #advisor1 p element:\n\nadvisor_test &lt;- html_test %&gt;%\n  html_element(\"#advisor1 p\") %&gt;%\n  html_text2()\n\nError in eval(expr, envir, enclos): object 'html_test' not found\n\nadvisor_test\n\nError in eval(expr, envir, enclos): object 'advisor_test' not found\n\n\n\n\nYour turn:\n\nTry using the SelectorGadget to identify the element necessary to extract the abstract of this dissertation.\nNow, write the code to extract it and make sure you actually get what you want.\n\nWe now have the date, major, and advisor for the first dissertation. We can create a matrix by passing them as arguments to cbind():\n\nresult_test &lt;- cbind(date_test, major_test, advisor_test)\n\nError in eval(expr, envir, enclos): object 'date_test' not found\n\nresult_test\n\nError in eval(expr, envir, enclos): object 'result_test' not found"
  },
  {
    "objectID": "r/ws_webscraping.html#full-run",
    "href": "r/ws_webscraping.html#full-run",
    "title": "Web scraping with R",
    "section": "Full run",
    "text": "Full run\n\nExtract all URLs\nNow that we have tested our code on the first dissertation, we can apply it on all 100 dissertations of the first page of the database.\nInstead of using html_element(), this time we will use html_elements() which extracts all matching elements (instead of just the first one):\n\ndat &lt;- html %&gt;% html_elements(\".article-listing a\")\n\nError in dyn.load(file, DLLpath = DLLpath, ...): unable to load shared object '/home/marie/R/lib/stringi/libs/stringi.so':\n  libicui18n.so.72: cannot open shared object file: No such file or directory\n\ndat\n\nError in eval(expr, envir, enclos): object 'dat' not found\n\n\n\ntypeof(dat)\n\nError in eval(expr, envir, enclos): object 'dat' not found\n\nlength(dat)\n\nError in eval(expr, envir, enclos): object 'dat' not found\n\ntypeof(dat[[1]])\n\nError in eval(expr, envir, enclos): object 'dat' not found\n\n\nWe now have a list of lists.\nAs we did for a single URL in the test run, we now want to extract all the URLs. We will do this using a loop.\nBefore running for loops, it is important to initialize empty loops. It is much more efficient than growing the result at each iteration.\nSo let’s initialize an empty list that we call list_urls of the appropriate size:\n\nlist_urls &lt;- vector(\"list\", length(dat))\n\nError in eval(expr, envir, enclos): object 'dat' not found\n\n\nNow we can run a loop to fill in our list:\n\nfor (i in seq_along(dat)) {\n  list_urls[[i]] &lt;- dat[[i]] %&gt;% html_attr(\"href\")\n}\n\nError in eval(expr, envir, enclos): object 'dat' not found\n\n\nLet’s print again the first element of list_urls to make sure all looks good:\n\nlist_urls[[1]]\n\nError in eval(expr, envir, enclos): object 'list_urls' not found\n\n\nWe now have a list of URLs (in the form of character vectors) as we wanted.\n\n\nExtract data from each page\nWe will now extract the data (date, major, and advisor) for all URLs in our list.\nAgain, before running a for loop, we need to allocate memory first by creating an empty container (here a list):\n\nlist_data &lt;- vector(\"list\", length(list_urls))\n\nError in eval(expr, envir, enclos): object 'list_urls' not found\n\n\nWe move the code we tested for a single URL inside a loop and we add one result to the list_data list at each iteration until we have all 100 dissertation sites scraped. Because there are quite a few of us running the code at the same time, we don’t want the site to block our request. To play safe, we will add a little delay (0.1 second) at each iteration (many sites will block requests if they are too frequent):\n\nfor (i in seq_along(list_urls)) {\n  html &lt;- read_html(list_urls[[i]])\n  date &lt;- html %&gt;%\n    html_element(\"#publication_date p\") %&gt;%\n    html_text2()\n  major &lt;- html %&gt;%\n    html_element(\"#department p\") %&gt;%\n    html_text2()\n  advisor &lt;- html %&gt;%\n    html_element(\"#advisor1 p\") %&gt;%\n    html_text2()\n  Sys.sleep(0.1)  # Add a little delay\n  list_data[[i]] &lt;- cbind(date, major, advisor)\n}\n\nError in eval(expr, envir, enclos): object 'list_urls' not found\n\n\nLet’s make sure all looks good by printing the first element of list_data:\n\nlist_data[[1]]\n\nError in eval(expr, envir, enclos): object 'list_data' not found"
  },
  {
    "objectID": "r/ws_webscraping.html#store-results-in-dataframe",
    "href": "r/ws_webscraping.html#store-results-in-dataframe",
    "title": "Web scraping with R",
    "section": "Store results in DataFrame",
    "text": "Store results in DataFrame\nWe can turn this big list into a dataframe:\n\nresult &lt;- do.call(rbind.data.frame, list_data)\n\nError in eval(expr, envir, enclos): object 'list_data' not found\n\n\nresult is a long dataframe, so we will only print the first few elements:\n\nhead(result)\n\nError in eval(expr, envir, enclos): object 'result' not found\n\n\nIf you like the tidyverse, you can turn it into a tibble:\n\nresult &lt;- result %&gt;% tibble::as_tibble()\n\nError in eval(expr, envir, enclos): object 'result' not found\n\n\n\nThe notation tibble::as_tibble() means that we are using the function as_tibble() from the package tibble. A tibble is the tidyverse version of a dataframe. One advantage is that it will only print the first 10 rows by default instead of printing the whole dataframe, so you don’t have to use head() when printing long dataframes:\n\nresult\n\nError in eval(expr, envir, enclos): object 'result' not found\n\n\n\nWe can capitalize the headers:\n\nnames(result) &lt;- c(\"Date\", \"Major\", \"Advisor\")\n\nError: object 'result' not found\n\n\nThis is what our final result looks like:\n\nresult\n\nError in eval(expr, envir, enclos): object 'result' not found"
  },
  {
    "objectID": "r/ws_webscraping.html#save-results-to-file",
    "href": "r/ws_webscraping.html#save-results-to-file",
    "title": "Web scraping with R",
    "section": "Save results to file",
    "text": "Save results to file\nAs a final step, we will save our data to a CSV file:\nwrite.csv(result, \"dissertations_data.csv\", row.names = FALSE)"
  },
  {
    "objectID": "r/ws_webscraping.html#functions-recap",
    "href": "r/ws_webscraping.html#functions-recap",
    "title": "Web scraping with R",
    "section": "Functions recap",
    "text": "Functions recap\nBelow is a recapitulation of the rvest functions we have used today:\n\n\n\nFunctions\nUsage\n\n\n\n\nread_html()\nRead in HTML from URL\n\n\nhtml_element()\nExtract first matching element\n\n\nhtml_elements()\nExtract all matching elements\n\n\nhtml_attr()\nExtract the value of an attribute\n\n\nhtml_text2()\nExtract text"
  },
  {
    "objectID": "r/ws_webscraping.html#recording",
    "href": "r/ws_webscraping.html#recording",
    "title": "Web scraping with R",
    "section": "Recording",
    "text": "Recording\n\nVideo of this workshop for the Digital Research Alliance of Canada HSS Winter Series 2023:"
  },
  {
    "objectID": "r/ws_gis_intro.html",
    "href": "r/ws_gis_intro.html",
    "title": "Introduction to GIS with R",
    "section": "",
    "text": "This workshop is an introduction to GIS in R. We will learn how to import GIS data, explore it, and map it.\nIn particular, we will create maps (inset maps, faceted maps, animated maps, interactive maps, and raster maps), thanks to the packages sf, tmap, raster, leaflet, ggplot2, grid (part of Base R), and mapview.\nWe will also learn how to add basemaps from OpenStreetMap and Google Maps."
  },
  {
    "objectID": "r/ws_gis_intro.html#getting-the-data",
    "href": "r/ws_gis_intro.html#getting-the-data",
    "title": "Introduction to GIS with R",
    "section": "Getting the data",
    "text": "Getting the data\n\nDatasets\nFor this webinar, we will use:\n\nthe Alaska as well as the Western Canada and USA subsets of the Randolph Glacier Inventory version 6.01\nthe USGS time series of the named glaciers of Glacier National Park2 The datasets can be downloaded as zip files from these websites.\n\n1 RGI Consortium (2017). Randolph Glacier Inventory – A Dataset of Global Glacier Outlines: Version 6.0: Technical Report, Global Land Ice Measurements from Space, Colorado, USA. Digital Media. DOI: https://doi.org/10.7265/N5-RGI-60.2 Fagre, D.B., McKeon, L.A., Dick, K.A., and Fountain, A.G., 2017, Glacier margin time series (1966, 1998, 2005, 2015) of the named glaciers of Glacier National Park, MT, USA: U.S. Geological Survey data release, https://doi.org/10.5066/F7P26WB1.\n\nBasemaps\nFor our basemaps, we will use data from:\n\nNatural Earth: this dataset can be accessed direction from within R thanks to the packages rnaturalearth (which provides the functions) and rnaturalearthdata (which provides the data)"
  },
  {
    "objectID": "r/ws_gis_intro.html#loading-and-exploring-data",
    "href": "r/ws_gis_intro.html#loading-and-exploring-data",
    "title": "Introduction to GIS with R",
    "section": "Loading and exploring data",
    "text": "Loading and exploring data\nFirst, let’s load the necessary packages for this webinar:\nlibrary(sf)\nlibrary(tmap)\nlibrary(dplyr)\nlibrary(magrittr)\nlibrary(purrr)\nlibrary(rnaturalearth)\nlibrary(rnaturalearthdata)\nlibrary(mapview)\nlibrary(grid) # part of base R (already installed), but needs to be explicitly loaded\nWe will start by mapping all the glaciers of Western North America thanks to:\n\nthe Alaska subset of the Randolph Glacier Inventory\nthe Western Canada and USA subset of the Randolph Glacier Inventory\n\nDownload and unzip 02_rgi60_WesternCanadaUS and 01_rgi60_Alaska from the Randolph Glacier Inventory version 6.0.\nData get imported and turned into sf objects by the function sf::st_read():\nak &lt;- st_read(\"01_rgi60_Alaska\")\nwes &lt;- st_read(\"02_rgi60_WesternCanadaUS\")\n\nMake sure to use the absolute paths or the proper paths relative to your working directory (which can be obtained with getwd() and modified with setwd()).\n\nYou can print and explore your new objects:\nak\nwes\n\nstr(ak)\nstr(wes)\nsf objects are data.frame-like objects with a geometry list-column as their last column. That column is itself an object of class sfc (simple feature geometry list column)."
  },
  {
    "objectID": "r/ws_gis_intro.html#mapping-with-tmap",
    "href": "r/ws_gis_intro.html#mapping-with-tmap",
    "title": "Introduction to GIS with R",
    "section": "Mapping with tmap",
    "text": "Mapping with tmap\ntmap follows a grammar of graphic similar to that of ggplot2: you first need to set a shape (a spatial data object) by passing an sf object to tm_shape(). Then you plot one or several layers with one of several tmap functions and you use the + sign between each element.\nTo see the available options, run:\n?tmap-element\nWe could thus plot the glaciers of Alaska with any of the options below:\ntm_shape(ak) +\n  tm_borders()\n\ntm_shape(ak) +\n  tm_fill()\n\ntm_shape(ak) +\n  tm_polygons()      # shows both borders and fill\nHere, we will use tm_polygons() which combines tm_borders() and tm_fill()."
  },
  {
    "objectID": "r/ws_gis_intro.html#layout-elements-and-attribute-layers",
    "href": "r/ws_gis_intro.html#layout-elements-and-attribute-layers",
    "title": "Introduction to GIS with R",
    "section": "Layout elements and attribute layers",
    "text": "Layout elements and attribute layers\nA map without title, compass, or scale bars is not very useful though. We need to add layout elements and attribute layers to the map.\nYou can loop up the many arguments of the tmap functions in the help pages to see how you can customize your maps:\n?tm_layout\n?tm_compass\n?tm_scale_bar\nLet’s now map the glaciers of Alaska:\ntm_shape(ak) +\n  tm_polygons() +\n  tm_layout(\n    title = \"Glaciers of Alaska\",\n    title.position = c(\"center\", \"top\"),\n    title.size = 1.1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.06, 0.01, 0.09, 0.01),\n    outer.margins = 0,\n    frame.lwd = 0.2\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    size = 1.2,\n    text.size = 0.6\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 500, 1000),\n    position = c(\"right\", \"BOTTOM\")\n  )"
  },
  {
    "objectID": "r/ws_gis_intro.html#union-of-bounding-boxes",
    "href": "r/ws_gis_intro.html#union-of-bounding-boxes",
    "title": "Introduction to GIS with R",
    "section": "Union of bounding boxes",
    "text": "Union of bounding boxes\nNow, if we want to plot all the glaciers of Western North America, we want to combine both sf objects in the same map. A map can contain multiple shapes: you only need to “add” a tm_shape and its element(s). Before doing so however, it is very important to ensure that they have the same coordinate reference system (CRS):\nst_crs(ak)\nst_crs(wes)\n\nst_crs(ak) == st_crs(wes)\nThey do, so we are good to go.\n\nAs with ggplot2 or GIS graphical user interfaces, the order matters since the layers stack up on top of each other.\n\ntm_shape(ak) +\n  tm_polygons() +\n  tm_shape(wes) +\n  tm_polygons()\nIf you run the code above however, you may be surprised that you are still only plotting the map of Alaska.\nThis is because each map comes with a spatial bounding box (bbox).\nst_bbox(ak)\nst_bbox(wes)\nIn the code above, the bbox is set by the first shape, i.e. our entire map uses the bbox of the Alaska sf object.\nWe first need to create a new bounding box encompassing both bounding boxes:\nnwa_bbox &lt;- st_bbox(\n  st_union(\n    st_as_sfc(st_bbox(wes)),\n    st_as_sfc(st_bbox(ak))\n  )\n)\nWe can now plot the glaciers of Western North America:\ntm_shape(ak, bbox = nwa_bbox) +\n  tm_polygons() +\n  tm_shape(wes) +\n  tm_polygons() +\n  tm_layout(\n    title = \"Glaciers of Western North America\",\n    title.position = c(\"center\", \"top\"),\n    title.size = 1.1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.06, 0.01, 0.09, 0.01),\n    outer.margins = 0,\n    frame.lwd = 0.2\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    size = 1.2,\n    text.size = 0.6\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 1000, 2000),\n    position = c(\"right\", \"BOTTOM\")\n  )"
  },
  {
    "objectID": "r/ws_gis_intro.html#maps-based-on-an-attribute-variable",
    "href": "r/ws_gis_intro.html#maps-based-on-an-attribute-variable",
    "title": "Introduction to GIS with R",
    "section": "Maps based on an attribute variable",
    "text": "Maps based on an attribute variable\nWhat is interesting about glacier maps is to see their evolution through time as glaciers retreat due to climate change. While the Randolph Glacier Inventory (RGI) has an amazing map in terms of spacial coverage, it doesn’t yet have much temporal data.\nTo look at glacier retreat, we will look at the USGS time series of the named glaciers of Glacier National Park3. These 4 datasets have the contour lines of 39 glaciers for the years 1966, 1998, 2005, and 2015.3 Fagre, D.B., McKeon, L.A., Dick, K.A., and Fountain, A.G., 2017, Glacier margin time series (1966, 1998, 2005, 2015) of the named glaciers of Glacier National Park, MT, USA: U.S. Geological Survey data release, https://doi.org/10.5066/F7P26WB1.\nWe could load and clean these datasets one by one. Copying and pasting code however is inefficient and error-prone. A better approach is to do this in a functional programming framework: create a function which does all the data loading and cleaning, then pass each element of a vector of the paths of all 4 datasets to it using purrr::map().\n“Cleaning” here consists of selecting the variables we are interested in, putting them in the same order in each dataset (they were not initially) and giving the exact same name across all datasets (there were case inconsistencies between datasets and R is case sensitive).\n# create a function that reads and cleans the data\nprep &lt;- function(dir) {\n  g &lt;- st_read(dir)\n  g %&lt;&gt;% rename_with(~ tolower(gsub(\"Area....\", \"area\", .x)))\n  g %&lt;&gt;% select(\n    year,\n    objectid,\n    glacname,\n    area,\n    shape_leng,\n    x_coord,\n    y_coord,\n    source_sca,\n    source\n  )\n}\n\n# create a vector of dataset names\ndirs &lt;- grep(\"GNPglaciers_.*\", list.dirs(), value = T)\n\n# pass each element of that vector through prep() thanks to map()\ngnp &lt;- map(dirs, prep)\nmap() returns a list, so we now have a list (gnp) of 4 elements: the 4 sf objects containing our cleaned datasets. A list is not really convenient and we will turn it into a single sf object.\nBefore doing so however, we want to make sure that they all have the same CRS:\nst_crs(gnp[[1]]) == st_crs(gnp[[2]])\nst_crs(gnp[[1]]) == st_crs(gnp[[3]])\nst_crs(gnp[[1]]) == st_crs(gnp[[4]])\nThey do, so we can turn gnp into a single sf object:\ngnp &lt;- do.call(\"rbind\", gnp)\n\ngnp\nstr(gnp)\nWe can now map the data:\ntm_shape(gnp) +\n  tm_polygons(\"year\", palette = \"Blues\") +\n  tm_layout(\n    title = \"Glaciers of Glacier National Park\",\n    title.position = c(\"center\", \"top\"),\n    legend.title.color = \"#fcfcfc\",\n    legend.text.size = 1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.07, 0.03, 0.07, 0.03),\n    outer.margins = 0\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    text.size = 0.7\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 10, 20),\n    position = c(\"right\", \"BOTTOM\"),\n    text.size = 1\n  )\n\nI didn’t want to show the legend title and because there is no option to remove it, I set its color to that of the background."
  },
  {
    "objectID": "r/ws_gis_intro.html#crs-transformation",
    "href": "r/ws_gis_intro.html#crs-transformation",
    "title": "Introduction to GIS with R",
    "section": "CRS transformation",
    "text": "CRS transformation\nWouldn’t it be nice to have this map as an inset of the previous map so that we can situate it within North America?\nBefore we can do this, we need to make sure that both maps use the same CRS:\nst_crs(ak)\nst_crs(gnp)\n\nWe could use wes instead of ak since we know that both sf objects have the same CRS.\n\nThey don’t have the same CRS, so we reproject gnp by transforming its data from its current CRS to that of ak.\ngnp &lt;- st_transform(gnp, st_crs(ak))\nst_crs(gnp)"
  },
  {
    "objectID": "r/ws_gis_intro.html#inset-map",
    "href": "r/ws_gis_intro.html#inset-map",
    "title": "Introduction to GIS with R",
    "section": "Inset map",
    "text": "Inset map\nNow we can create our map with an inset: the map of the Western North America glaciers (from the sf object nwa) will be our main map and the map of Glacier National Park (from the sf object gnp) will be the inset.\nIf the goal of this new map is to show the location of the gnp map within the nwa one, we need to add a rectangle showing the bounding box of gnp in the nwa map as a new layer.\nFor this, we create a new sfc_POLYGON from the bounding box of gnp:\ngnp_zone &lt;- st_bbox(gnp) %&gt;%\n  st_as_sfc()\nWe will use it as the following layer within the new map:\ntm_shape(gnp_zone) +\n  tm_borders(lwd = 1.5, col = \"#ff9900\")\nWe assign our new map (with an updated suitable title) to the object main_map:\nmain_map &lt;- tm_shape(ak, bbox = nwa_bbox) +\n  tm_polygons() +\n  tm_shape(wes) +\n  tm_polygons() +\n  tm_shape(gnp_zone) +\n  tm_borders(lwd = 1.5, col = \"#ff9900\") +\n  tm_layout(\n    title = \"Glaciers of Glacier National Park\",\n    title.position = c(\"center\", \"top\"),\n    title.size = 1.1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.06, 0.01, 0.09, 0.01),\n    outer.margins = 0,\n    frame.lwd = 0.2\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    size = 1.2,\n    text.size = 0.6\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 500, 1000),\n    position = c(\"right\", \"BOTTOM\")\n  )\nNext, we will change the frame of the gnp inset to match the color of this new rectangle (to make it visually clear that this is a close-up view of that rectangle). We can also remove the title, compass and scale bar since this is an inset within a map which already have them. We assign this new map to the object inset_map:\ninset_map &lt;- tm_shape(gnp) +\n  tm_polygons(\"year\", palette = \"Blues\") +\n  tm_layout(\n    legend.title.color = \"#fcfcfc\",\n    legend.text.size = 0.7,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.03, 0.03, 0.03, 0.03),\n    outer.margins = 0,\n    frame = \"#ff9900\",\n    frame.lwd = 3\n  )\nFinally, we combine the two maps with grid::viewport():\nmain_map\nprint(inset_map, vp = viewport(0.41, 0.26, width = 0.5, height = 0.5))"
  },
  {
    "objectID": "r/ws_gis_intro.html#tiled-web-maps-with-leaflet",
    "href": "r/ws_gis_intro.html#tiled-web-maps-with-leaflet",
    "title": "Introduction to GIS with R",
    "section": "Tiled web maps with Leaflet",
    "text": "Tiled web maps with Leaflet\nTiled web maps are interactive maps in a browser using web servers such as Google Maps or OpenStreetMap. Several packages allow to use Leaflet (an open-source JavaScript library for interactive maps) to create tile maps.\n\nWith mapview\nThe simplest option is to use mapview::mapview():\nmapview(gnp)\nThis will open a page in your browser in which you can pan, zoom, select/deselect data layers, and choose from a number of basemap layer options:\n CartoDB.Positron\n OpenTopoMap\n OpenStreetMap\n Esri.WorldImagery\n\n\nWith tmap\ntmap has similar capabilities.\nThe package has 2 modes:\n\nplot is the default mode for static maps that we used earlier.\nview is an interactive viewing mode using Leaflet in a browser. There, as with mapview, you can zoom in/out, select/deselect the different layers, and choose to display one of Esri.WorldGrayCanvas, OpenStreetMap, or Esri.WorldTopoMap basemaps.\n\nYou can toggle between the plot and view modes with ttm(), after which you can re-plot your last plot in the new mode with tmap_last(). You can also do both of these at once with ttmp().\nAlternatively, you can switch to either mode with tmap_mode(\"view\") and tmap_mode(\"plot\").\n\nExample:\n\nEarlier, we plotted all the glaciers of Western North America using tmap:\n\nAfter displaying this map, we could have run:\ntmap_mode(\"view\")\ntmap_last()\nAnd Leaflet would have open the following interactive map in our browser:\n\n\nAfterwards, if you want to create new static plots, don’t forget to get back to plot mode with tmap_mode(\"plot\")."
  },
  {
    "objectID": "r/ws_gis_intro.html#mapping-a-subset-of-the-data",
    "href": "r/ws_gis_intro.html#mapping-a-subset-of-the-data",
    "title": "Introduction to GIS with R",
    "section": "Mapping a subset of the data",
    "text": "Mapping a subset of the data\nEach glacier has 4 borders: one for each year of survey. They are however quite hard to see on such a large map.\nLet’s zoom on the Agassiz glacier:\n# select the data points corresponding to the Agassiz Glacier\nag &lt;- g %&gt;% filter(glacname == \"Agassiz Glacier\")\nAnd map it:\ntm_shape(ag) +\n  tm_polygons(\"year\", palette = \"Blues\") +\n  tm_layout(\n    title = \"Agassiz Glacier\",\n    title.position = c(\"center\", \"top\"),\n    legend.position = c(\"left\", \"bottom\"),\n    legend.title.color = \"#fcfcfc\",\n    legend.text.size = 1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.07, 0.03, 0.07, 0.03),\n    outer.margins = 0\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    text.size = 0.7\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 0.5, 1),\n    position = c(\"right\", \"BOTTOM\"),\n    text.size = 1\n  )\n\nNow we can clearly see the retreat of the Agassiz Glacier between 1966 and 2015."
  },
  {
    "objectID": "r/ws_gis_intro.html#faceted-map",
    "href": "r/ws_gis_intro.html#faceted-map",
    "title": "Introduction to GIS with R",
    "section": "Faceted map",
    "text": "Faceted map\nInstead of having all temporal data in a single map however, it can be split across facets:\ntm_shape(ag) +\n  tm_polygons(col = \"#86baff\") +\n  tm_layout(\n    main.title = \"Agassiz Glacier\",\n    main.title.position = c(\"center\", \"top\"),\n    main.title.size = 1.2,\n    legend.position = c(\"left\", \"bottom\"),\n    legend.title.color = \"#fcfcfc\",\n    legend.text.size = 1,\n    bg.color = \"#fcfcfc\",\n    # inner.margins = c(0, 0.03, 0, 0.03),\n    outer.margins = 0,\n    panel.label.bg.color = \"#fcfcfc\",\n    frame = F,\n    asp = 0.6\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    size = 1,\n    text.size = 0.6\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 0.5, 1),\n    position = c(\"right\", \"BOTTOM\"),\n    text.size = 0.6\n  ) +\n  tm_facets(\n    by = \"year\",\n    free.coords = F,\n    ncol = 4\n  )"
  },
  {
    "objectID": "r/ws_gis_intro.html#animated-map",
    "href": "r/ws_gis_intro.html#animated-map",
    "title": "Introduction to GIS with R",
    "section": "Animated map",
    "text": "Animated map\nThe temporal data of the Agassiz Glacier retreat can also be conveyed through an animation:\nagassiz_anim &lt;- tm_shape(ag) +\n  tm_borders() +\n  tm_fill(col = \"#86baff\") +\n  tm_layout(\n    title = \"Agassiz Glacier\",\n    title.position = c(\"center\", \"top\"),\n    legend.position = c(\"left\", \"bottom\"),\n    legend.title.color = \"#fcfcfc\",\n    legend.text.size = 1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.08, 0, 0.08, 0),\n    outer.margins = 0\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    text.size = 0.7\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 0.5, 1),\n    position = c(\"right\", \"BOTTOM\"),\n    text.size = 1\n  ) +\n  tm_facets(\n    along = \"year\",\n    free.coords = F\n  )\n\ntmap_animation(\n  agassiz_anim,\n  filename = \"ag.gif\",\n  dpi = 300,\n  inner.margins = c(0.08, 0, 0.08, 0),\n  delay = 100\n)"
  },
  {
    "objectID": "r/ws_gis_intro.html#additional-resources",
    "href": "r/ws_gis_intro.html#additional-resources",
    "title": "Introduction to GIS with R",
    "section": "Additional resources",
    "text": "Additional resources\nOpen GIS data:\nFree GIS Data: list of free GIS datasets\nBooks\nGeocomputation with R by Robin Lovelace, Jakub Nowosad, and Jannes Muenchow\nSpatial Data Science by Edzer Pebesma, Roger Bivand\nSpatial Data Science with R by Robert J. Hijmans\nUsing Spatial Data with R by Claudia A. Engel\nTutorial\nAn Introduction to Spatial Data Analysis and Visualisation in R by the CDRC\nWebsite\nr-spatial by Edzer Pebesma, Marius Appel, and Daniel Nüst\nCRAN package list\nAnalysis of Spatial Data\nMailing list\nR Special Interest Group on using Geographical data and Mapping"
  },
  {
    "objectID": "r/wb_hpc.html",
    "href": "r/wb_hpc.html",
    "title": "High-performance research computing in R",
    "section": "",
    "text": "R is not famous for its speed. With code optimization and parallelization, it can however be used for heavy computations.\nThis webinar will introduce you to working with R from the command line on the Alliance clusters with a focus on performance. We will discuss code benchmarking and various parallelization techniques.\n\nSlides (Click and wait: this reveal.js presentation may take a little time to load.)"
  },
  {
    "objectID": "r/wb_gis_mapping.html",
    "href": "r/wb_gis_mapping.html",
    "title": "GIS mapping with R",
    "section": "",
    "text": "In this webinar, we will see how to create all sorts of GIS maps with the packages sf, tmap, raster, leaflet, ggplot2, grid (part of Base R), and mapview:\n\nsimple maps\ninset maps\nfaceted maps\nanimated maps\ninteractive maps\nraster maps\n\nFinally, we will learn how to add basemaps from OpenStreetMap and Google Maps.\nSlides (Click and wait: this reveal.js presentation may take a little time to load.)"
  },
  {
    "objectID": "r/wb_gis_mapping.html#gis-concepts",
    "href": "r/wb_gis_mapping.html#gis-concepts",
    "title": "GIS mapping with R",
    "section": "GIS concepts",
    "text": "GIS concepts\n\nTypes of spatial data\n\nVector data\nVector data represent discrete objects.\nThey contain:\n\na geometry: the shape and location of the objects,\nattributes: additional variables (e.g. name, year, type).\n\nCommon file formats include GeoJSON and shapefile.\n\nExamples: countries, roads, rivers, towns.\n\n\n\nRaster data\nRaster data represent continuous phenomena or spatial fields.\nCommon file formats include TIFF, GeoTIFF, NetCDF, and Esri grid.\n\nExamples: temperature, air quality, elevation, water depth.\n\n\n\n\nVector data\nVector data come in multiple types:\n\npoint:     single set of coordinates,\nmulti-point:   multiple sets of coordinates,\npolyline:    multiple sets for which the order matters,\nmulti-polyline:  multiple of the above,\npolygon:    same as polyline but first and last sets are the same,\nmulti-polygon:  multiple of the above.\n\n\n\nRaster data\nGrid of equally sized rectangular cells containing values for some variables.\nSize of cells = resolution.\nFor computing efficiency, rasters do not have coordinates of each cell, but the bounding box and the number of rows and columns.\n\n\nCoordinate Reference Systems (CRS)\nA location on Earth’s surface can be identified by its coordinates and some reference system called CRS.\nThe coordinates (x, y) are called longitude and latitude.\nThere can be a 3rd coordinate (z) for elevation or other measurement—usually a vertical one.\nAnd a 4th (m) for some other data attribute—usually a horizontal measurement.\nIn 3D, longitude and latitude are expressed in angular units (e.g. degrees) and the reference system needed is an angular CRS or geographic coordinate system (GCS).\nIn 2D, they are expressed in linear units (e.g. meters) and the reference system needed is a planar CRS or projected coordinate system (PCS).\n\n\nDatums\nSince the Earth is not a perfect sphere, we use spheroidal models to represent its surface. Those are called geodetic datums.\nSome datums are global, others local (more accurate in a particular area of the globe, but only useful there).\n\nExamples of commonly used global datums:\n\nWGS84 (World Geodesic System 1984),\nNAD83 (North American Datum of 1983).\n\n\n\n\nAngular CRS\nAn angular CRS contains a datum, an angular unit and references such as a prime meridian (e.g. the Royal Observatory, Greenwich, England).\nIn an angular CRS or GCS:\n\nLongitude (\\(\\lambda\\)) represents the angle between the prime meridian and the meridian that passes through that location.\nLatitude (\\(\\phi\\)) represents the angle between the line that passes through the center of the Earth and that location and its projection on the equatorial plane.\n\nLongitude and latitude are thus angular coordinates.\n\n\nProjections\nTo create a two-dimensional map, you need to project this 3D angular CRS into a 2D one.\nVarious projections offer different characteristics. For instance:\n\nsome respect areas (equal-area),\nsome respect the shape of geographic features (conformal),\nsome almost respect both for small areas.\n\nIt is important to choose one with sensible properties for your goals.\n\nExamples of projections:\n\nMercator,\nUTM,\nRobinson.\n\n\n\n\nPlanar CRS\nA planar CRS is defined by a datum, a projection and a set of parameters such as a linear unit and the origins.\nCommon planar CRS have been assigned a unique ID called EPSG code which is much more convenient to use.\nIn a planar CRS, coordinates will not be in degrees anymore but in meters (or other length unit).\n\n\nProjecting into a new CRS\nYou can change the projection of your data.\nVector data won’t suffer any loss of precision, but raster data will.\n→ best to try to avoid reprojecting rasters: if you want to combine various datasets which have different projections, reproject vector data instead."
  },
  {
    "objectID": "r/wb_gis_mapping.html#gis-in-r",
    "href": "r/wb_gis_mapping.html#gis-in-r",
    "title": "GIS mapping with R",
    "section": "GIS in R",
    "text": "GIS in R\n\nResources\n\nOpen GIS data\nFree GIS Data: list of free GIS datasets.\n\n\nBooks\nGeocomputation with R by Robin Lovelace, Jakub Nowosad and Jannes Muenchow.\nSpatial Data Science by Edzer Pebesma and Roger Bivand.\nSpatial Data Science with R by Robert J. Hijmans.\nUsing Spatial Data with R by Claudia A. Engel.\n\n\nTutorial\nAn Introduction to Spatial Data Analysis and Visualisation in R by the CDRC.\n\n\n\nResources\n\nWebsite\nr-spatial by Edzer Pebesma, Marius Appel and Daniel Nüst.\n\n\nCRAN package list\nAnalysis of Spatial Data.\n\n\nMailing list\nR Special Interest Group on using Geographical data and Mapping."
  },
  {
    "objectID": "r/wb_gis_mapping.html#packages",
    "href": "r/wb_gis_mapping.html#packages",
    "title": "GIS mapping with R",
    "section": "Packages",
    "text": "Packages\nThere is now a rich ecosystem of GIS packages in R1.1 Bivand, R.S. Progress in the R ecosystem for representing and handling spatial data. J Geogr Syst (2020). https://doi.org/10.1007/s10109-020-00336-0.\n\nData manipulation\n\nOlder packages\n\nsp\nraster\nrgdal\nrgeos\n\n\n\nNewer generation\n\nsf:    vector data,\nterra:  raster data (also has vector data capabilities).\n\n\n\n\nMapping\n\nStatic maps\n\nggplot2 + ggspatial\ntmap\n\n\n\nDynamic maps\n\nleaflet\nggplot2 + gganimate\nmapview\nggmap\ntmap\n\n\n\n\nsf: Simple Features in R\nGeospatial vectors: points, lines, polygons.\nSimple Features—defined by the Open Geospatial Consortium (OGC) and formalized by ISO—is a set of standards now used by most GIS libraries.\nWell-known text (WKT) is a markup language for representing vector geometry objects according to those standards.\nA compact computer version also exists—well-known binary (WKB)—used by spatial databases.\nThe package sp predates Simple Features.\nsf—launched in 2016—implements these standards in R in the form of sf objects: data.frames (or tibbles) containing the attributes, extended by sfc objects or simple feature geometries list-columns.\n\n\nsf\nSome useful links:\n\nGitHub repo,\nPaper,\nResources,\nCheatsheet,\n6 vignettes: 1, 2, 3, 4, 5, 6.\n\nAnd the cheatsheet:\nxxxxx\n\n\n\n\n\n\n\nsf objects\n\n\n\n\n\n\n\nsf functions\nMost functions start with st_ (which refers to “spatial type”).\n\n\nterra: Geospatial rasters\nFaster and simpler replacement for the raster package by the same team.\nMostly implemented in C++.\nCan work with datasets too large to be loaded into memory.\n\n\nterra\nSome useful links:\n\nGitHub repo,\nResources,\nFull manual.\n\n\n\ntmap: Layered grammar of graphics GIS maps\nSome useful links:\n\nGitHub repo,\nResources.\n\n\nHelp pages and vignettes\n?tmap-element\nvignette(\"tmap-getstarted\")\n# All the usual help pages, e.g.:\n?tm_layout\n\n\ntmap functions\nMain functions start with tmap_\nFunctions creating map elements start with tm_\n\n\ntmap functioning\nVery similar to ggplot2\nTypically, a map contains:\n\nOne or multiple layer(s) (the order matters as they stack on top of each other)\nSome layout (e.g. customization of title, background, margins): tm_layout\nA compass: tm_compass\nA scale bar: tm_scale_bar\n\nEach layer contains:\n\nSome data: tm_shape\nHow that data will be represented: e.g. tm_polygons, tm_lines, tm_raster\n\n\n\ntmap example\n\n\n\n\n\n\n\n\n\n\n\nggplot2 (the standard in R plots)\nSome useful links:\n\nGitHub repo,\nResources,\nCheatsheet.\n\n\n\ngeom_sf allows to plot sf objects (i.e. make maps)."
  },
  {
    "objectID": "r/wb_gis_mapping.html#example-glaciers-melt-in-north-america",
    "href": "r/wb_gis_mapping.html#example-glaciers-melt-in-north-america",
    "title": "GIS mapping with R",
    "section": "Example: Glaciers melt in North America",
    "text": "Example: Glaciers melt in North America\n\nData\nFor this webinar, we will use:\n\nthe Alaska as well as the Western Canada and USA subsets of the Randolph Glacier Inventory version 6.02,\nthe USGS time series of the named glaciers of Glacier National Park3,\nthe Alaska as well as the Western Canada and USA subsets of the consensus estimate for the ice thickness distribution of all glaciers on Earth dataset4.\n\n2 RGI Consortium (2017). Randolph Glacier Inventory – A Dataset of Global Glacier Outlines: Version 6.0: Technical Report, Global Land Ice Measurements from Space, Colorado, USA. Digital Media. DOI: https://doi.org/10.7265/N5-RGI-60.3 Fagre, D.B., McKeon, L.A., Dick, K.A. and Fountain, A.G., 2017, Glacier margin time series (1966, 1998, 2005, 2015) of the named glaciers of Glacier National Park, MT, USA: U.S. Geological Survey data release. DOI: https://doi.org/10.5066/F7P26WB1.4 Farinotti, Daniel, 2019, A consensus estimate for the ice thickness distribution of all glaciers on Earth - dataset, Zurich. ETH Zurich. DOI: https://doi.org/10.3929/ethz-b-000315707.The datasets can be downloaded as zip files from these websites\n\n\nPackages\nPackages need to be installed before they can be loaded in a session.\nPackages on CRAN can be installed with:\ninstall.packages(\"&lt;package-name&gt;\")\nbasemaps is not on CRAN and needs to be installed from GitHub thanks to devtools:\ninstall.packages(\"devtools\")\ndevtools::install_github(\"16EAGLE/basemaps\")\nWe load all the packages that we will need at the top of the script:\n\nlibrary(sf)                 # spatial vector data manipulation\n\nLinking to GEOS 3.12.0, GDAL 3.7.2, PROJ 9.2.1; sf_use_s2() is TRUE\n\nlibrary(tmap)               # map production and tiled web map\n\nThe legacy packages maptools, rgdal, and rgeos, underpinning the sp package,\nwhich was just loaded, will retire in October 2023.\nPlease refer to R-spatial evolution reports for details, especially\nhttps://r-spatial.org/r/2023/05/15/evolution4.html.\nIt may be desirable to make the sf package available;\npackage maintainers should consider adding sf to Suggests:.\nThe sp package is now running under evolution status 2\n     (status 2 uses the sf package in place of rgdal)\n\n\nBreaking News: tmap 3.x is retiring. Please test v4, e.g. with\nremotes::install_github('r-tmap/tmap')\n\nlibrary(dplyr)              # non GIS specific (tabular data manipulation)\nlibrary(magrittr)           # non GIS specific (pipes)\nlibrary(purrr)              # non GIS specific (functional programming)\nlibrary(rnaturalearth)      # basemap data access functions\n\nSupport for Spatial objects (`sp`) will be deprecated in {rnaturalearth} and will be removed in a future release of the package. Please use `sf` objects with {rnaturalearth}. For example: `ne_download(returnclass = 'sf')`\n\nlibrary(rnaturalearthdata)  # basemap data\n\n\nAttaching package: 'rnaturalearthdata'\n\n\nThe following object is masked from 'package:rnaturalearth':\n\n    countries110\n\nlibrary(mapview)            # tiled web map\nlibrary(grid)               # (part of base R) used to create inset map\nlibrary(ggplot2)            # alternative to tmap for map production\nlibrary(ggspatial)          # spatial framework for ggplot2\nlibrary(terra)              # gridded spatial data manipulation\n\nterra 1.7.46\n\n\n\nAttaching package: 'terra'\n\n\nThe following object is masked from 'package:grid':\n\n    depth\n\n\nThe following objects are masked from 'package:magrittr':\n\n    extract, inset\n\nlibrary(ggmap)              # download basemap data\n\nError: package or namespace load failed for 'ggmap' in dyn.load(file, DLLpath = DLLpath, ...):\n unable to load shared object '/home/marie/R/lib/stringi/libs/stringi.so':\n  libicui18n.so.72: cannot open shared object file: No such file or directory\n\nlibrary(basemaps)           # download basemap data\nlibrary(magick)             # wrapper around ImageMagick STL\n\nLinking to ImageMagick 7.1.1.18\nEnabled features: cairo, fontconfig, freetype, fftw, heic, lcms, raw, rsvg, webp, x11\nDisabled features: ghostscript, pango\n\n\nUsing 16 threads\n\nlibrary(leaflet)            # integrate Leaflet JS in R"
  },
  {
    "objectID": "r/wb_gis_mapping.html#reading-and-preparing-data",
    "href": "r/wb_gis_mapping.html#reading-and-preparing-data",
    "title": "GIS mapping with R",
    "section": "Reading and preparing data",
    "text": "Reading and preparing data\n\nRandolph Glacier Inventory\nThis dataset contains the contour of all glaciers on Earth.\nWe will focus on glaciers in Western North America.\nYou can download and unzip 02_rgi60_WesternCanadaUS and 01_rgi60_Alaska from the Randolph Glacier Inventory version 6.0.\n\n\nReading in data\nData get imported and turned into sf objects with the function sf::st_read:\nak &lt;- st_read(\"data/01_rgi60_Alaska\")\n\nMake sure to use the absolute paths or the paths relative to your working directory (which can be obtained with getwd).\n\nak &lt;- st_read(\"data/01_rgi60_Alaska\")\n\n[Out]\n\nReading layer `01_rgi60_Alaska' from data source `./data/01_rgi60_Alaska'\n               using driver `ESRI Shapefile'\nSimple feature collection with 27108 features and 22 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -176.1425 ymin: 52.05727 xmax: -126.8545 ymax: 69.35167\nGeodetic CRS:  WGS 84\n\n\nYour turn:\n\nRead in the data for the rest of north western America (from 02_rgi60_WesternCanadaUS) and create an sf object called wes.\n\n\n\nFirst look at the data\nak\n\n[Out]\n\nSimple feature collection with 27108 features and 22 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -176.1425 ymin: 52.05727 xmax: -126.8545 ymax: 69.35167\nGeodetic CRS:  WGS 84\nFirst 10 features:\n           RGIId        GLIMSId  BgnDate  EndDate    CenLon   CenLat O1Region\n1  RGI60-01.00001 G213177E63689N 20090703 -9999999 -146.8230 63.68900        1\n2  RGI60-01.00002 G213332E63404N 20090703 -9999999 -146.6680 63.40400        1\n3  RGI60-01.00003 G213920E63376N 20090703 -9999999 -146.0800 63.37600        1\n4  RGI60-01.00004 G213880E63381N 20090703 -9999999 -146.1200 63.38100        1\n5  RGI60-01.00005 G212943E63551N 20090703 -9999999 -147.0570 63.55100        1\n6  RGI60-01.00006 G213756E63571N 20090703 -9999999 -146.2440 63.57100        1\n7  RGI60-01.00007 G213771E63551N 20090703 -9999999 -146.2295 63.55085        1\n8  RGI60-01.00008 G213704E63543N 20090703 -9999999 -146.2960 63.54300        1\n9  RGI60-01.00009 G212400E63659N 20090703 -9999999 -147.6000 63.65900        1\n10 RGI60-01.00010 G212830E63513N 20090703 -9999999 -147.1700 63.51300        1\nO2Region   Area Zmin Zmax Zmed Slope Aspect  Lmax Status Connect Form\n1         2  0.360 1936 2725 2385    42    346   839      0       0    0\n2         2  0.558 1713 2144 2005    16    162  1197      0       0    0\n3         2  1.685 1609 2182 1868    18    175  2106      0       0    0\n4         2  3.681 1273 2317 1944    19    195  4175      0       0    0\n5         2  2.573 1494 2317 1914    16    181  2981      0       0    0\n6         2 10.470 1201 3547 1740    22     33 10518      0       0    0\n7         2  0.649 1918 2811 2194    23    151  1818      0       0    0\n8         2  0.200 2826 3555 3195    45     80   613      0       0    0\n9         2  1.517 1750 2514 1977    18    274  2255      0       0    0\n10        2  3.806 1280 1998 1666    17     35  3332      0       0    0\nTermType Surging Linkages Name                       geometry\n1         0       9        9 &lt;NA&gt; POLYGON ((-146.818 63.69081...\n2         0       9        9 &lt;NA&gt; POLYGON ((-146.6635 63.4076...\n3         0       9        9 &lt;NA&gt; POLYGON ((-146.0723 63.3834...\n4         0       9        9 &lt;NA&gt; POLYGON ((-146.149 63.37919...\n5         0       9        9 &lt;NA&gt; POLYGON ((-147.0431 63.5502...\n6         0       9        9 &lt;NA&gt; POLYGON ((-146.2436 63.5562...\n7         0       9        9 &lt;NA&gt; POLYGON ((-146.2495 63.5531...\n8         0       9        9 &lt;NA&gt; POLYGON ((-146.2992 63.5443...\n9         0       9        9 &lt;NA&gt; POLYGON ((-147.6147 63.6643...\n10        0       9        9 &lt;NA&gt; POLYGON ((-147.1494 63.5098...\n\n\nStructure of the data\nstr(ak)\n\n[Out]\n\nClasses ‘sf’ and 'data.frame':  27108 obs. of  23 variables:\n$ RGIId   : chr  \"RGI60-01.00001\" \"RGI60-01.00002\" \"RGI60-01.00003\" ...\n$ GLIMSId : chr  \"G213177E63689N\" \"G213332E63404N\" \"G213920E63376N\" ...\n$ BgnDate : chr  \"20090703\" \"20090703\" \"20090703\" \"20090703\" ...\n$ EndDate : chr  \"-9999999\" \"-9999999\" \"-9999999\" \"-9999999\" ...\n$ CenLon  : num  -147 -147 -146 -146 -147 ...\n$ CenLat  : num  63.7 63.4 63.4 63.4 63.6 ...\n$ O1Region: chr  \"1\" \"1\" \"1\" \"1\" ...\n$ O2Region: chr  \"2\" \"2\" \"2\" \"2\" ...\n$ Area    : num  0.36 0.558 1.685 3.681 2.573 ...\n$ Zmin    : int  1936 1713 1609 1273 1494 1201 1918 2826 1750 1280 ...\n$ Zmax    : int  2725 2144 2182 2317 2317 3547 2811 3555 2514 1998 ...\n$ Zmed    : int  2385 2005 1868 1944 1914 1740 2194 3195 1977 1666 ...\n$ Slope   : num  42 16 18 19 16 22 23 45 18 17 ...\n$ Aspect  : int  346 162 175 195 181 33 151 80 274 35 ...\n$ Lmax    : int  839 1197 2106 4175 2981 10518 1818 613 2255 3332 ...\n$ Status  : int  0 0 0 0 0 0 0 0 0 0 ...\n$ Connect : int  0 0 0 0 0 0 0 0 0 0 ...\n$ Form    : int  0 0 0 0 0 0 0 0 0 0 ...\n$ TermType: int  0 0 0 0 0 0 0 0 0 0 ...\n$ Surging : int  9 9 9 9 9 9 9 9 9 9 ...\n$ Linkages: int  9 9 9 9 9 9 9 9 9 9 ...\n$ Name    : chr  NA NA NA NA ...\n$ geometry:sfc_POLYGON of length 27108; first list element: List of 1\n..$ : num [1:65, 1:2] -147 -147 -147 -147 -147 ...\n..- attr(*, \"class\")= chr [1:3] \"XY\" \"POLYGON\" \"sfg\"\n- attr(*, \"sf_column\")= chr \"geometry\"\n- attr(*, \"agr\")= Factor w/ 3 levels \"constant\",\"aggregate\",..: NA NA NA ...\n..- attr(*, \"names\")= chr [1:22] \"RGIId\" \"GLIMSId\" \"BgnDate\" \"EndDate\" ...\n\n\nInspect your data\n\n\nYour turn:\n\nInspect the wes object you created.\n\n\n\nGlacier National Park dataset\nThis dataset contains a time series of the retreat of 39 glaciers of Glacier National Park, MT, USA for the years 1966, 1998, 2005 and 2015.\nYou can download and unzip the 4 sets of files from the USGS website.\n\n\nRead in and clean datasets\nCreate a function that reads and cleans the data:\nprep &lt;- function(dir) {\n  g &lt;- st_read(dir)\n  g %&lt;&gt;% rename_with(~ tolower(gsub(\"Area....\", \"area\", .x)))\n  g %&lt;&gt;% dplyr::select(\n    year,\n    objectid,\n    glacname,\n    area,\n    shape_leng,\n    x_coord,\n    y_coord,\n    source_sca,\n    source\n  )\n}\nCreate a vector of dataset names:\ndirs &lt;- grep(\"data/GNPglaciers_.*\", list.dirs(), value = T)\nPass each element of that vector through prep() thanks to map():\ngnp &lt;- map(dirs, prep)\n\nWe use dplyr::select because terra also has a select function.\n\n\n\nCombine datasets into one sf object\nCheck that the CRS are all the same:\nall(sapply(\n  list(st_crs(gnp[[1]]),\n       st_crs(gnp[[2]]),\n       st_crs(gnp[[3]]),\n       st_crs(gnp[[4]])),\n  function(x) x == st_crs(gnp[[1]])\n))\n\n[Out]\n\n[1] TRUE\nWe can rbind the elements of our list:\ngnp &lt;- do.call(\"rbind\", gnp)\nYou can inspect your new sf object by calling it or with str.\n\n\nEstimate for ice thickness\nThis dataset contains an estimate for the ice thickness of all glaciers on Earth.\nThe nomenclature follows the Randolph Glacier Inventory.\nIce thickness being a spatial field, this is raster data.\nWe will use data in RGI60-02.16664_thickness.tif from the ETH Zürich Research Collection which corresponds to one of the glaciers (Agassiz) of Glacier National Park.\n\n\nLoad raster data\nRead in data and create a SpatRaster object:\nras &lt;- rast(\"data/RGI60-02/RGI60-02.16664_thickness.tif\")\n\n\nInspect our SpatRaster object\nras\n\n[Out]\n\nclass       : SpatRaster \ndimensions  : 93, 74, 1  (nrow, ncol, nlyr)\nresolution  : 25, 25  (x, y)\nextent      : 707362.5, 709212.5, 5422962, 5425288  (xmin, xmax, ymin, ymax)\ncoord. ref. : +proj=utm +zone=11 +datum=WGS84 +units=m +no_defs \nsource      : RGI60-02.16664_thickness.tif \nname        : RGI60-02.16664_thickness \nnlyr gives us the number of bands (a single one here). You can also run str(ras).\n\n\nOur data\nWe now have 3 sf objects and 1 SpatRaster object:\n\nak: contour of glaciers in AK,\nwes: contour of glaciers in the rest of Western North America,\ngnp: time series of 39 glaciers in Glacier National Park, MT, USA,\nras: ice thickness of the Agassiz Glacier from Glacier National Park."
  },
  {
    "objectID": "r/wb_gis_mapping.html#making-maps",
    "href": "r/wb_gis_mapping.html#making-maps",
    "title": "GIS mapping with R",
    "section": "Making maps",
    "text": "Making maps\n\nLet’s map our sf object ak\nAt a bare minimum, we need tm_shape with the data and some info as to how to represent that data:\ntm_shape(ak) +\n  tm_polygons()\n\n\n\nWe need to label and customize it\ntm_shape(ak) +\n  tm_polygons() +\n  tm_layout(\n    title = \"Glaciers of Alaska\",\n    title.position = c(\"center\", \"top\"),\n    title.size = 1.1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.06, 0.01, 0.09, 0.01),\n    outer.margins = 0,\n    frame.lwd = 0.2\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    size = 1.2,\n    text.size = 0.6\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 500, 1000),\n    position = c(\"right\", \"BOTTOM\")\n  )\n\n\n\nMake a map of the wes object\n\n\nYour turn:\n\nMake a map with the wes object you created with the data for Western North America excluding AK.\n\n\n\n\nNow, let’s make a map with ak and wes\nThe Coordinate Reference Systems (CRS) must be the same.\nsf has a function to retrieve the CRS of an sf object: st_crs.\nst_crs(ak) == st_crs(wes)\n\n[Out]\n\n[1] TRUE\nSo we’re good (we will see later what to do if this is not the case).\n\n\nOur combined map\nLet’s start again with a minimum map without any layout to test things out:\ntm_shape(ak) +\n  tm_polygons() +\n  tm_shape(wes) +\n  tm_polygons()\n\nUh … oh …\n\n\nWhat went wrong?\nMaps are bound by “bounding boxes”. In tmap, they are called bbox.\ntmap sets the bbox the first time tm_shape is called. In our case, the bbox was thus set to the bbox of the ak object.\nWe need to create a new bbox for our new map.\n\n\nRetrieving bounding boxes\nsf has a function to retrieve the bbox of an sf object: st_bbox\nThe bbox of ak is:\nst_bbox(ak)\n\n[Out]\n\nxmin         ymin       xmax         ymax\n-176.14247   52.05727   -126.85450   69.35167\n\n\nCombining bounding boxes\nbbox objects can’t be combined directly.\nHere is how we can create a new bbox encompassing both of our bboxes:\n\nfirst, we transform our bboxes to sfc objects with st_as_sfc,\nthen we combine those objects into a new sfc object with st_union,\nfinally, we retrieve the bbox of that object with st_bbox:\n\nnwa_bbox &lt;- st_bbox(\n  st_union(\n    st_as_sfc(st_bbox(wes)),\n    st_as_sfc(st_bbox(ak))\n  )\n)\n\n\nBack to our map\nWe can now use our new bounding box for the map of Western North America:\ntm_shape(ak, bbox = nwa_bbox) +\n  tm_polygons() +\n  tm_shape(wes) +\n  tm_polygons() +\n  tm_layout(\n    title = \"Glaciers of Western North America\",\n    title.position = c(\"center\", \"top\"),\n    title.size = 1.1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.06, 0.01, 0.09, 0.01),\n    outer.margins = 0,\n    frame.lwd = 0.2\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    size = 1.2,\n    text.size = 0.6\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 1000, 2000),\n    position = c(\"right\", \"BOTTOM\")\n  )\n\n\n\nLet’s add a basemap\nWe will use data from Natural Earth, a public domain map dataset.\nThere are much more fancy options, but they usually involve creating accounts (e.g. with Google) to access some API.\nIn addition, this dataset can be accessed direction from within R thanks to the rOpenSci packages:\n\nrnaturalearth: provides the functions,\nrnaturalearthdata: provides the data.\n\n\n\nCreate an sf object with states/provinces\nstates_all &lt;- ne_states(\n  country = c(\"canada\", \"united states of america\"),\n  returnclass = \"sf\"\n)\n\nne_ stands for “Natural Earth”.\n\n\n\nSelect relevant states/provinces\nstates &lt;- states_all %&gt;%\n  filter(name_en == \"Alaska\" |\n           name_en == \"British Columbia\" |\n           name_en == \"Yukon\" |\n           name_en == \"Northwest Territories\" |\n           name_en ==  \"Alberta\" |\n           name_en == \"California\" |\n           name_en == \"Washington\" |\n           name_en == \"Oregon\" |\n           name_en == \"Idaho\" |\n           name_en == \"Montana\" |\n           name_en == \"Wyoming\" |\n           name_en == \"Colorado\" |\n           name_en == \"Nevada\" |\n           name_en == \"Utah\"\n         )\n\n\nAdd the basemap to our map\n\nWhat do we need to make sure of first?\n\nst_crs(states) == st_crs(ak)\n\n[Out]\n\n[1] TRUE\nWe add the basemap as a 3rd layer.\nMind the order! If you put the basemap last, it will cover your data.\nOf course, we will use our nwa_bbox bounding box again.\nWe will also break tm_polygons into tm_borders and tm_fill for ak and wes in order to colourise them with slightly different colours:\ntm_shape(states, bbox = nwa_bbox) +\n  tm_polygons(col = \"#f2f2f2\", lwd = 0.2) +\n  tm_shape(ak) +\n  tm_borders(col = \"#3399ff\") +\n  tm_fill(col = \"#86baff\") +\n  tm_shape(wes) +\n  tm_borders(col = \"#3399ff\") +\n  tm_fill(col = \"#86baff\") +\n  tm_layout(\n    title = \"Glaciers of Western North America\",\n    title.position = c(\"center\", \"top\"),\n    title.size = 1.1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.06, 0.01, 0.09, 0.01),\n    outer.margins = 0,\n    frame.lwd = 0.2\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    size = 1.2,\n    text.size = 0.6\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 1000, 2000),\n    position = c(\"right\", \"BOTTOM\")\n  )\n\n\n\ntmap styles\ntmap has a number of styles that you can try.\nFor instance, to set the style to “classic”, run the following before making your map:\ntmap_style(\"classic\")\n\nOther options are:\n“white” (default), “gray”, “natural”, “cobalt”, “col_blind”, “albatross”, “beaver”, “bw”, and “watercolor”.\n\n\nTo return to the default, you need to run:\ntmap_style(\"white\")\nor:\ntmap_options_reset()\nwhich will reset every tmap option."
  },
  {
    "objectID": "r/wb_gis_mapping.html#inset-maps",
    "href": "r/wb_gis_mapping.html#inset-maps",
    "title": "GIS mapping with R",
    "section": "Inset maps",
    "text": "Inset maps\nNow, how can we combine this with our gnp object?\nWe could add it as an inset of our Western North America map.\n\nFirst, let’s map it\nLet’s use the same tm_borders and tm_fill we just used:\ntm_shape(gnp) +\n  tm_borders(col = \"#3399ff\") +\n  tm_fill(col = \"#86baff\") +\n  tm_layout(\n    title = \"Glaciers of Glacier National Park\",\n    title.position = c(\"center\", \"top\"),\n    legend.title.color = \"#fcfcfc\",\n    legend.text.size = 1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.07, 0.03, 0.07, 0.03),\n    outer.margins = 0\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    text.size = 0.7\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 10, 20),\n    position = c(\"right\", \"BOTTOM\"),\n    text.size = 1\n  )\n\n\n\nCreate an inset map\nAs always, first we check that the CRS are the same:\nst_crs(gnp) == st_crs(ak)\n\n[Out]\n\n[1] FALSE\nAH!\n\n\nCRS transformation\nWe need to reproject gnp into the CRS of our other sf objects (e.g. ak):\ngnp &lt;- st_transform(gnp, st_crs(ak))\nWe can verify that the CRS are now the same:\nst_crs(gnp) == st_crs(ak)\n\n[Out]\n\n[1] TRUE\n\n\nInset maps: first step\nAdd a rectangle showing the location of the GNP map in the main North America map.\nWe need to create a new sfc object from the gnp bbox so that we can add it to our previous map as a new layer:\ngnp_zone &lt;- st_bbox(gnp) %&gt;%\n  st_as_sfc()\n\n\nInset maps: second step\nCreate a tmap object of the main map. Of course, we need to edit the title. Also, note the presence of our new layer:\nmain_map &lt;- tm_shape(states, bbox = nwa_bbox) +\n  tm_polygons(col = \"#f2f2f2\", lwd = 0.2) +\n  tm_shape(ak) +\n  tm_borders(col = \"#3399ff\") +\n  tm_fill(col = \"#86baff\") +\n  tm_shape(wes) +\n  tm_borders(col = \"#3399ff\") +\n  tm_fill(col = \"#86baff\") +\n  tm_shape(gnp_zone) +\n  tm_borders(lwd = 1.5, col = \"#ff9900\") +\n  tm_layout(\n    title = \"Glaciers of Glacier National Park\",\n    title.position = c(\"center\", \"top\"),\n    title.size = 1.1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.06, 0.01, 0.09, 0.01),\n    outer.margins = 0,\n    frame.lwd = 0.2\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    size = 1.2,\n    text.size = 0.6\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 500, 1000),\n    position = c(\"right\", \"BOTTOM\")\n  )\n\n\nInset maps: third step\nCreate a tmap object of the inset map.\nWe make sure to matching colours and edit the layouts for better readability:\ninset_map &lt;- tm_shape(gnp) +\n  tm_borders(col = \"#3399ff\") +\n  tm_fill(col = \"#86baff\") +\n  tm_layout(\n    legend.show = F,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.03, 0.03, 0.03, 0.03),\n    outer.margins = 0,\n    frame = \"#ff9900\",\n    frame.lwd = 3\n  )\n\n\nInset maps: final step\nCombine the two tmap objects.\nWe print the main map and add the inset map with grid::viewport:\nmain_map\nprint(inset_map, vp = viewport(0.41, 0.26, width = 0.5, height = 0.5))"
  },
  {
    "objectID": "r/wb_gis_mapping.html#mapping-a-subset-of-the-data",
    "href": "r/wb_gis_mapping.html#mapping-a-subset-of-the-data",
    "title": "GIS mapping with R",
    "section": "Mapping a subset of the data",
    "text": "Mapping a subset of the data\nTo see the retreat of the ice, we need to zoom in.\nLet’s focus on a single glacier: Agassiz Glacier.\n\nMap of the Agassiz Glacier\nSelect the data points corresponding to the Agassiz Glacier:\nag &lt;- gnp %&gt;% filter(glacname == \"Agassiz Glacier\")\ntm_shape(ag) +\n  tm_polygons() +\n  tm_layout(\n    title = \"Agassiz Glacier\",\n    title.position = c(\"center\", \"top\"),\n    legend.position = c(\"left\", \"bottom\"),\n    legend.title.color = \"#fcfcfc\",\n    legend.text.size = 1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.07, 0.03, 0.07, 0.03),\n    outer.margins = 0\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    text.size = 0.7\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 0.5, 1),\n    position = c(\"right\", \"BOTTOM\"),\n    text.size = 1\n  )\n\nNot great …\n\n\nMap based on attribute variables\ntm_shape(ag) +\n  tm_polygons(\"year\", palette = \"Blues\") +\n  tm_layout(\n    title = \"Agassiz Glacier\",\n    title.position = c(\"center\", \"top\"),\n    legend.position = c(\"left\", \"bottom\"),\n    legend.title.color = \"#fcfcfc\",\n    legend.text.size = 1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.07, 0.03, 0.07, 0.03),\n    outer.margins = 0\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    text.size = 0.7\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 0.5, 1),\n    position = c(\"right\", \"BOTTOM\"),\n    text.size = 1\n  )\n\n\n\nUsing ggplot2 instead of tmap\nAs an alternative to tmap, ggplot2 can plot maps with the geom_sf function:\nggplot(ag) +\n  geom_sf(aes(fill = year)) +\n  scale_fill_brewer(palette = \"Blues\") +\n  labs(title = \"Agassiz Glacier\") +\n  annotation_scale(location = \"bl\", width_hint = 0.4) +\n  annotation_north_arrow(location = \"tr\", which_north = \"true\",\n                         pad_x = unit(0.75, \"in\"), pad_y = unit(0.5, \"in\"),\n                         style = north_arrow_fancy_orienteering) +\n  theme_bw() +\n  theme(plot.title = element_text(hjust = 0.5))\nThe package ggspatial adds a lot of functionality to ggplot2 for spatial data."
  },
  {
    "objectID": "r/wb_gis_mapping.html#faceted-maps",
    "href": "r/wb_gis_mapping.html#faceted-maps",
    "title": "GIS mapping with R",
    "section": "Faceted maps",
    "text": "Faceted maps\n\nFaceted map of the retreat of Agassiz\ntm_shape(ag) +\n  tm_polygons(col = \"#86baff\") +\n  tm_layout(\n    main.title = \"Agassiz Glacier\",\n    main.title.position = c(\"center\", \"top\"),\n    main.title.size = 1.2,\n    legend.position = c(\"left\", \"bottom\"),\n    legend.title.color = \"#fcfcfc\",\n    legend.text.size = 1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0, 0.03, 0, 0.03),\n    outer.margins = 0,\n    panel.label.bg.color = \"#fcfcfc\",\n    frame = F,\n    asp = 0.6\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    size = 1,\n    text.size = 0.6\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 0.5, 1),\n    position = c(\"right\", \"BOTTOM\"),\n    text.size = 0.6\n  ) +\n  tm_facets(\n    by = \"year\",\n    free.coords = F,\n    ncol = 4\n  )"
  },
  {
    "objectID": "r/wb_gis_mapping.html#animated-maps",
    "href": "r/wb_gis_mapping.html#animated-maps",
    "title": "GIS mapping with R",
    "section": "Animated maps",
    "text": "Animated maps\n\nAnimated map of the Retreat of Agassiz\nFirst, we need to create a tmap object with facets:\nagassiz_anim &lt;- tm_shape(ag) +\n  tm_polygons(col = \"#86baff\") +\n  tm_layout(\n    title = \"Agassiz Glacier\",\n    title.position = c(\"center\", \"top\"),\n    legend.position = c(\"left\", \"bottom\"),\n    legend.title.color = \"#fcfcfc\",\n    legend.text.size = 1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.08, 0, 0.08, 0),\n    outer.margins = 0,\n    panel.label.bg.color = \"#fcfcfc\"\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    text.size = 0.7\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 0.5, 1),\n    position = c(\"right\", \"BOTTOM\"),\n    text.size = 1\n  ) +\n  tm_facets(\n    along = \"year\",\n    free.coords = F\n  )\nThen we can pass that object to tmap_animation:\ntmap_animation(\n  agassiz_anim,\n  filename = \"ag.gif\",\n  dpi = 300,\n  inner.margins = c(0.08, 0, 0.08, 0),\n  delay = 100\n)\n\n\n\nMap of ice thickness of Agassiz\nNow, let’s map the estimated ice thickness on Agassiz Glacier.\nThis time, we use tm_raster:\ntm_shape(ras) +\n  tm_raster(title = \"\") +\n  tm_layout(\n    title = \"Ice thickness (m) of Agassiz Glacier\",\n    title.position = c(\"center\", \"top\"),\n    legend.position = c(\"left\", \"bottom\"),\n    legend.bg.color = \"#ffffff\",\n    legend.text.size = 1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.07, 0.03, 0.07, 0.03),\n    outer.margins = 0\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    text.size = 0.7\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 0.5, 1),\n    position = c(\"right\", \"BOTTOM\"),\n    text.size = 1\n  )\n\n\n\nCombining with Randolph data\nAs always, we check whether the CRS are the same:\nst_crs(ag) == st_crs(ras)\n\n[Out]\n\n[1] FALSE\nWe need to reproject ag (remember that it is best to avoid reprojecting raster data):\nag %&lt;&gt;% st_transform(st_crs(ras))\nThe retreat and ice thickness layers will hide each other (the order matters!). One option is to use tm_borders for one of them, but we can also use transparency (alpha). We also adjust the legend:\ntm_shape(ras) +\n  tm_raster(title = \"Ice (m)\") +\n  tm_shape(ag) +\n  tm_polygons(\"year\", palette = \"Blues\", alpha = 0.2, title = \"Contour\") +\n  tm_layout(\n    title = \"Ice thickness (m) and retreat of Agassiz Glacier\",\n    title.position = c(\"center\", \"top\"),\n    legend.position = c(\"left\", \"bottom\"),\n    legend.bg.color = \"#ffffff\",\n    legend.text.size = 0.7,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.07, 0.03, 0.07, 0.03),\n    outer.margins = 0\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    text.size = 0.7\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 0.5, 1),\n    position = c(\"right\", \"BOTTOM\"),\n    text.size = 1\n  )\n\n\n\nRefining raster maps\nLet’s go back to our ice thickness map:\n\nWe can change the palette to blue with tm_raster(palette = \"Blues\"):\n\n\nWe can create a more suitable interval scale\nFirst, let’s see what the maximum value is:\nglobal(ras, \"max\")\n\n[Out]\n\nmax\nRGI60-02.16664_thickness 70.10873\nThen we can set the breaks with tm_raster(breaks = seq(0, 80, 5))\nWe also need to tweak the layout, legend, etc.:\ntm_shape(ras) +\n  tm_raster(title = \"\", palette = \"Blues\", breaks = seq(0, 80, 5)) +\n  tm_layout(\n    title = \"Ice thickness (m) of Agassiz Glacier\",\n    title.position = c(\"center\", \"top\"),\n    legend.position = c(\"left\", \"bottom\"),\n    legend.bg.color = \"#ffffff\",\n    legend.text.size = 0.7,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.07, 0.03, 0.07, 0.03),\n    outer.margins = 0\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    text.size = 0.7\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 0.5, 1),\n    position = c(\"right\", \"BOTTOM\"),\n    text.size = 1\n  )\n\nOr we can use a continuous colour scheme with tm_raster(style = \"cont\"):"
  },
  {
    "objectID": "r/wb_gis_mapping.html#basemaps",
    "href": "r/wb_gis_mapping.html#basemaps",
    "title": "GIS mapping with R",
    "section": "Basemaps",
    "text": "Basemaps\n\nBasemap with ggmap\nbasemap &lt;- get_map(\n  bbox = c(\n    left = st_bbox(ag)[1],\n    bottom = st_bbox(ag)[2],\n    right = st_bbox(ag)[3],\n    top = st_bbox(ag)[4]\n  ),\n  source = \"osm\"\n)\n\nggmap is a powerful package, but Google now requires an API key obtained through registration\n\n\n\nBasemap with basemaps\nThe package basemaps allows to download open source basemap data from several sources, but those cannot easily be combined with sf objects\nThis plots a satellite image of the Agassiz Glacier:\nbasemap_plot(ag, map_service = \"esri\", map_type = \"world_imagery\")\n\n\nSatellite image of the Agassiz Glacier"
  },
  {
    "objectID": "r/wb_gis_mapping.html#tiled-web-maps-with-leaflet-js",
    "href": "r/wb_gis_mapping.html#tiled-web-maps-with-leaflet-js",
    "title": "GIS mapping with R",
    "section": "Tiled web maps with Leaflet JS",
    "text": "Tiled web maps with Leaflet JS\n\nmapview\nmapview(gnp)\n\n\n\n\n\nCartoDB.Positron\n\n\n\n\n\nOpenStreetMap\n\n\n\n\n\n\nOpenTopoMap\n\n\n\n\n\nEsri.WorldImagery\n\n\n\n\n\n\ntmap\nSo far, we have used the plot mode of tmap. There is also a view mode which allows interactive viewing in a browser through Leaflet\nChange to view mode:\ntmap_mode(\"view\")\n\nYou can also toggle between modes with ttm\n\nRe-plot the last map we plotted with tmap:\ntmap_last()\n\n\nleaflet\nleaflet creates a map widget to which you add layers\nmap &lt;- leaflet()\naddTiles(map)"
  },
  {
    "objectID": "r/wb_gis_mapping.html#spatial-data-analysis",
    "href": "r/wb_gis_mapping.html#spatial-data-analysis",
    "title": "GIS mapping with R",
    "section": "Spatial data analysis",
    "text": "Spatial data analysis\n\nResources\nHere are some resources on the topic to get started.\n\nR companion to Geographic Information Analysis\nSpatial data analysis"
  },
  {
    "objectID": "r/wb_gis_mapping.html#image-credits",
    "href": "r/wb_gis_mapping.html#image-credits",
    "title": "GIS mapping with R",
    "section": "Image credits",
    "text": "Image credits\nSzűcs Róbert, Grasshopper Geography"
  },
  {
    "objectID": "r/top_hpc.html",
    "href": "r/top_hpc.html",
    "title": "High-performance R",
    "section": "",
    "text": "R is not famous for its speed. With code optimization and parallelization, it can however be used for heavy computations.\nThis course will introduce you to working with R from the command line on the Alliance clusters with a focus on performance. We will discuss code profiling and benchmarking, various parallelization techniques, as well as using C++ from inside R to speed up calculations.\nA basic knowledge of R will be useful for this course.\n\n Start course ➤"
  },
  {
    "objectID": "r/intro_tidyverse.html",
    "href": "r/intro_tidyverse.html",
    "title": "Introduction to the tidyverse",
    "section": "",
    "text": "The tidyverse is a set of packages which attempts to make R more consistent and more similar to programming languages which were developed by computer scientists rather than statisticians.\nYou can think of it as a more modern version of R."
  },
  {
    "objectID": "r/intro_tidyverse.html#base-r-or-tidyverse",
    "href": "r/intro_tidyverse.html#base-r-or-tidyverse",
    "title": "Introduction to the tidyverse",
    "section": "Base R or tidyverse?",
    "text": "Base R or tidyverse?\n“Base R” refers to the use of the standard R library. The expression is often used in contrast to the tidyverse.\nThere are a many things that you can do with either base R or the tidyverse. Because the syntaxes are quite different, it almost feels like using two different languages and people tend to favour one or the other.\nWhich one you should use is really up to you.\n\n\n\n\n\n\n\nBase R\nTidyverse\n\n\n\n\nPreferred by old-schoolers\nIncreasingly becoming the norm with newer R users\n\n\nMore stable\nMore consistent syntax and behaviour\n\n\nDoesn’t require installing and loading packages\nMore and more resources and documentation available\n\n\n\nIn truth, even though the tidyverse has many detractors amongst old R users, it is increasingly becoming the norm."
  },
  {
    "objectID": "r/intro_tidyverse.html#a-glimpse-of-the-tidyverse",
    "href": "r/intro_tidyverse.html#a-glimpse-of-the-tidyverse",
    "title": "Introduction to the tidyverse",
    "section": "A glimpse of the tidyverse",
    "text": "A glimpse of the tidyverse\nThe best introduction to the tidyverse is probably the book R for Data Science by Hadley Wickham and Garrett Grolemund.\nPosit (the company formerly known as RStudio Inc. behind the tidyverse) developed a series of useful cheatsheets. Below are links to the ones you are the most likely to use as you get started with R.\n\nData import\nThe first thing you often need to do is to import your data into R. This is done with readr.\n\n\n\n\n\n\n\nfrom Posit Cheatsheets\n\n\n\nData transformation\nYou then often need to transformation your data into the right format. This is done with the packages dplyr and tidyr.\n\n\n\n\n\n\n\nfrom Posit Cheatsheets\n\n\n\n\n\n\n\n\nfrom Posit Cheatsheets\n\n\n\nVisualization\nVisualization in the tidyverse is done with the ggplot2 package which we will explore in the next section.\n\n\n\n\n\n\n\nfrom Posit Cheatsheets\n\n\n\nWorking with factors\nThe package forcats offers the tidyverse approach to working with factors.\n\n\n\n\n\n\nfrom Posit Cheatsheets\n\n\n\nWorking with strings\nstringr is for strings.\n\n\n\n\n\n\n\nfrom Posit Cheatsheets\n\n\n\nWorking with dates\nlubridate will help you deal with dates.\n\n\n\n\n\n\n\nfrom Posit Cheatsheets\n\n\n\nFunctional programming\nFinally, purrr is the tidyverse equivalent to the apply functions in base R: a way to run functions on functions.\n\n\n\n\n\n\n\nfrom Posit Cheatsheets"
  },
  {
    "objectID": "r/intro_resources.html",
    "href": "r/intro_resources.html",
    "title": "Resources",
    "section": "",
    "text": "The R community is dynamic and offers a lot of online resources, from IDEs to Q&A, to workshops, books, or publications.\nThis section provides a selection of useful sites.\n\n\nMain sites\n\nR website\nComprehensive R Archive Network (CRAN): R versions and packages\n\n\n\nPosit and RStudio IDE\n\nPosit website (Posit was formerly called RStudio Inc.)\nPosit cheatsheets\n\n\n\nForums and Q&A\n\nStack Overflow [r] tag wiki\nStack Overflow [r] tag questions\nPosit Discourse\n\n\n\nDocumentation as pdf\n\nContributed documentation\nIntro books\n\n\n\nSoftware Carpentry online workshops\n\nProgramming with R\nR for Reproducible Scientific Analysis\nData analysis using R in the digital humanities\n\n\n\nOnline books\n\nR for Data Science (heavily based on the tidyverse)\nR Packages (how to create packages)\nR Programming for Data Science\nMastering Software Development in R\n\n\n\nR research\n\nThe R Journal"
  },
  {
    "objectID": "r/intro_plotting.html",
    "href": "r/intro_plotting.html",
    "title": "Plotting",
    "section": "",
    "text": "This section focuses on plotting in R with the package ggplot2 from the tidyverse."
  },
  {
    "objectID": "r/intro_plotting.html#the-data",
    "href": "r/intro_plotting.html#the-data",
    "title": "Plotting",
    "section": "The data",
    "text": "The data\nR comes with a number of datasets. You can get a list by running data(). The ggplot2 package provides additional ones. We will use the mpg dataset from ggplot2.\nTo access the data, let’s load the package:\n\nlibrary(ggplot2)\n\nHere is what that dataset looks like:\n\nmpg\n\n# A tibble: 234 × 11\n   manufacturer model      displ  year   cyl trans      drv     cty   hwy fl   \n   &lt;chr&gt;        &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;      &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;\n 1 audi         a4           1.8  1999     4 auto(l5)   f        18    29 p    \n 2 audi         a4           1.8  1999     4 manual(m5) f        21    29 p    \n 3 audi         a4           2    2008     4 manual(m6) f        20    31 p    \n 4 audi         a4           2    2008     4 auto(av)   f        21    30 p    \n 5 audi         a4           2.8  1999     6 auto(l5)   f        16    26 p    \n 6 audi         a4           2.8  1999     6 manual(m5) f        18    26 p    \n 7 audi         a4           3.1  2008     6 auto(av)   f        18    27 p    \n 8 audi         a4 quattro   1.8  1999     4 manual(m5) 4        18    26 p    \n 9 audi         a4 quattro   1.8  1999     4 auto(l5)   4        16    25 p    \n10 audi         a4 quattro   2    2008     4 manual(m6) 4        20    28 p    \n   class  \n   &lt;chr&gt;  \n 1 compact\n 2 compact\n 3 compact\n 4 compact\n 5 compact\n 6 compact\n 7 compact\n 8 compact\n 9 compact\n10 compact\n# ℹ 224 more rows\n\n\n?mpg will give you information on the variables. In particular:\n\ndispl contains data on engine displacement (a measure of engine size and thus power) in litres (L).\nhwy contains data on fuel economy while driving on highways in miles per gallon (mpg).\ndrv represents the type of drive train (front-wheel drive, rear wheel drive, 4WD).\nclass represents the type of car.\n\nWe are interested in the relationship between engine size and fuel economy and see how the type of drive train and/or the type of car might affect this relationship."
  },
  {
    "objectID": "r/intro_plotting.html#base-r-plotting",
    "href": "r/intro_plotting.html#base-r-plotting",
    "title": "Plotting",
    "section": "Base R plotting",
    "text": "Base R plotting\nR contains built-in plotting capability thanks to the plot() function.\nA basic version of our plot would be:\n\nplot(\n  mpg$displ,\n  mpg$hwy,\n  main = \"Fuel consumption per engine size on highways\",\n  xlab = \"Engine size (L)\",\n  ylab = \"Fuel economy (mpg) on highways\"\n)"
  },
  {
    "objectID": "r/intro_plotting.html#grammar-of-graphics",
    "href": "r/intro_plotting.html#grammar-of-graphics",
    "title": "Plotting",
    "section": "Grammar of graphics",
    "text": "Grammar of graphics\nLeland Wilkinson developed the concept of grammar of graphics in his 2005 book The Grammar of Graphics. By breaking down statistical graphs into components following a set of rules, any plot can be described and constructed in a rigorous fashion.\nThis was further refined by Hadley Wickham in his 2010 article A Layered Grammar of Graphics and implemented in the package ggplot2 (that’s what the 2 “g” stand for in “ggplot”).\nggplot2 has become the dominant graphing package in R. Let’s see how to construct a plot with this package."
  },
  {
    "objectID": "r/intro_plotting.html#plotting-with-ggplot2",
    "href": "r/intro_plotting.html#plotting-with-ggplot2",
    "title": "Plotting",
    "section": "Plotting with ggplot2",
    "text": "Plotting with ggplot2\n\nYou can find the ggplot2 cheatsheet here.\n\n\nThe Canvas\nThe first component is the data:\n\nggplot(data = mpg)\n\n\n\n\n\nThis can be simplified into ggplot(mpg).\n\nThe second component sets the way variables are mapped on the axes. This is done with the aes() (aesthetics) function:\n\nggplot(data = mpg, mapping = aes(x = displ, y = hwy))\n\n\n\n\n\nThis can be simplified into ggplot(mpg, aes(x = displ, y = hwy)).\n\n\n\nGeometric representations of the data\nOnto this canvas, we can add “geoms” (geometrical objects) representing the data. The type of “geom” defines the type of representation (e.g. boxplot, histogram, bar chart).\nTo represent the data as a scatterplot, we use the geom_point() function:\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point()\n\n\n\n\nWe can colour-code the points in the scatterplot based on the drv variable, showing the lower fuel efficiency of 4WD vehicles:\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = drv))\n\n\n\n\nOr we can colour-code them based on the class variable:\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = class))\n\n\n\n\nMultiple “geoms” can be added on top of each other. For instance, we can add a smoothed conditional means function that aids at seeing patterns in the data with geom_smooth():\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = class)) +\n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nThanks to the colour-coding of the types of car, we can see that the cluster of points in the top right corner all belong to the same type: 2 seaters. Those are outliers with high power, yet high few efficiency due to their smaller size.\nThe default smoothing function uses the LOESS (locally estimated scatterplot smoothing) method, which is a nonlinear regression. But maybe a linear model would actually show the general trend better. We can change the method by passing it as an argument to geom_smooth():\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = class)) +\n  geom_smooth(method = lm)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nOf course, we could apply the smoothing function to each class instead of the entire data. It creates a busy plot but shows that the downward trend remains true within each type of car:\n\nggplot(mpg, aes(x = displ, y = hwy, color = class)) +\n  geom_point(aes(color = class)) +\n  geom_smooth(method = lm)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nOther arguments to geom_smooth() can set the line width, color, or whether or not the standard error (se) is shown:\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = class)) +\n  geom_smooth(\n    method = lm,\n    se = FALSE,\n    color = \"#999999\",\n    linewidth = 0.5\n  )\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\nColour scales\nIf we want to change the colour scale, we add another layer for this:\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = class)) +\n  scale_color_brewer(palette = \"Dark2\") +\n  geom_smooth(\n    method = lm,\n    se = FALSE,\n    color = \"#999999\",\n    linewidth = 0.5\n  )\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nscale_color_brewer(), based on color brewer 2.0, is one of many methods to change the color scale. Here is the list of available scales for this particular method:\n\n\n\nLabels\nWe can keep on adding layers. For instance, the labs() function allows to set title, subtitle, captions, tags, axes labels, etc.\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = class)) +\n  scale_color_brewer(palette = \"Dark2\") +\n  geom_smooth(\n    method = lm,\n    se = FALSE,\n    color = \"#999999\",\n    linewidth = 0.5\n  ) +\n  labs(\n    title = \"Fuel consumption per engine size on highways\",\n    x = \"Engine size (L)\",\n    y = \"Fuel economy (mpg) on highways\",\n    color = \"Type of car\",\n    caption = \"EPA data from https://fueleconomy.gov/\"\n  )\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\nThemes\nAnother optional layer sets one of several preset themes.\nEdward Tufte developed, amongst others, the principle of data-ink ratio which emphasizes that ink should be used primarily where it communicates meaningful messages. It is indeed common to see charts where more ink is used in labels or background than in the actual representation of the data.\nThe default ggplot2 theme could be criticized as not following this principle. Let’s change it:\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = class)) +\n  scale_color_brewer(palette = \"Dark2\") +\n  geom_smooth(\n    method = lm,\n    se = FALSE,\n    color = \"#999999\",\n    linewidth = 0.5\n  ) +\n  labs(\n    title = \"Fuel consumption per engine size on highways\",\n    x = \"Engine size (L)\",\n    y = \"Fuel economy (mpg) on highways\",\n    color = \"Type of car\",\n    caption = \"EPA data from https://fueleconomy.gov/\"\n  ) +\n  theme_classic()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nThe theme() function allows to tweak the theme in any number of ways. For instance, what if we don’t like the default position of the title and we’d rather have it centered?\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = class)) +\n  scale_color_brewer(palette = \"Dark2\") +\n  geom_smooth(\n    method = lm,\n    se = FALSE,\n    color = \"#999999\",\n    linewidth = 0.5\n  ) +\n  labs(\n    title = \"Fuel consumption per engine size on highways\",\n    x = \"Engine size (L)\",\n    y = \"Fuel economy (mpg) on highways\",\n    color = \"Type of car\",\n    caption = \"EPA data from https://fueleconomy.gov/\"\n  ) +\n  theme_classic() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nMany things can be changed thanks to the theme() function. For instance, we can move the legend to give more space to the actual graph:\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = class)) +\n  scale_color_brewer(palette = \"Dark2\") +\n  geom_smooth(\n    method = lm,\n    se = FALSE,\n    color = \"#999999\",\n    linewidth = 0.5\n  ) +\n  labs(\n    title = \"Fuel consumption per engine size on highways\",\n    x = \"Engine size (L)\",\n    y = \"Fuel economy (mpg) on highways\",\n    color = \"Type of car\",\n    caption = \"EPA data from https://fueleconomy.gov/\"\n  ) +\n  theme_classic() +\n  theme(plot.title = element_text(hjust = 0.5), legend.position = \"bottom\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nAs you could see, ggplot2 works by adding a number of layers on top of each other, all following a standard set of rules, or “grammar”. This way, a vast array of graphs can be created by organizing simple components."
  },
  {
    "objectID": "r/intro_plotting.html#ggplot2-extensions",
    "href": "r/intro_plotting.html#ggplot2-extensions",
    "title": "Plotting",
    "section": "ggplot2 extensions",
    "text": "ggplot2 extensions\nThanks to its vast popularity, ggplot2 has seen a proliferation of packages extending its capabilities.\n\nCombining plots\nFor instance the patchwork package allows to easily combine multiple plots on the same frame.\nLet’s add a second plot next to our plot. To add plots side by side, we simply add them to each other. We also make a few changes to the labels to improve the plots integration:\n\nlibrary(patchwork)\n\nggplot(mpg, aes(x = displ, y = hwy)) +        # First plot\n  geom_point(aes(color = class)) +\n  scale_color_brewer(palette = \"Dark2\") +\n  geom_smooth(\n    method = lm,\n    se = FALSE,\n    color = \"#999999\",\n    linewidth = 0.5\n  ) +\n  labs(\n    x = \"Engine size (L)\",\n    y = \"Fuel economy (mpg) on highways\",\n    color = \"Type of car\"\n  ) +\n  theme_classic() +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    legend.position = c(0.7, 0.75),           # Better legend position\n    legend.background = element_rect(         # Add a frame to the legend\n      linewidth = 0.1,\n      linetype = \"solid\",\n      colour = \"black\"\n    )\n  ) +\n  ggplot(mpg, aes(x = displ, y = hwy)) +      # Second plot\n  geom_point(aes(color = drv)) +\n  scale_color_brewer(palette = \"Dark2\") +\n  labs(\n    x = \"Engine size (L)\",\n    y = element_blank(),                      # Remove redundant label\n    color = \"Type of drive train\",\n    caption = \"EPA data from https://fueleconomy.gov/\"\n  ) +\n  theme_classic() +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    legend.position = c(0.7, 0.87),\n    legend.background = element_rect(\n      linewidth = 0.1,\n      linetype = \"solid\",\n      colour = \"black\"\n    )\n  )\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\nExtensions list\nAnother popular extension is the gganimate package which allows to create data animations.\nA full list of extensions for ggplot2 is shown below (here is the website):"
  },
  {
    "objectID": "r/intro_indexing.html",
    "href": "r/intro_indexing.html",
    "title": "Indexing",
    "section": "",
    "text": "This section covers indexing from the various data structures."
  },
  {
    "objectID": "r/intro_indexing.html#indexing-atomic-vectors",
    "href": "r/intro_indexing.html#indexing-atomic-vectors",
    "title": "Indexing",
    "section": "Indexing atomic vectors",
    "text": "Indexing atomic vectors\n\nHere is an example with an atomic vector of size one:\n\nIndexing in R starts at 1 and is done with square brackets next to the element to index:\n\nx &lt;- 2\nx\n\n[1] 2\n\nx[1]\n\n[1] 2\n\n\nWhat happens if we index out of range?\n\nx[2]\n\n[1] NA\n\n\n\nExample for an atomic vector with multiple elements:\n\n\nx &lt;- c(2, 4, 1)\nx\n\n[1] 2 4 1\n\nx[2]\n\n[1] 4\n\nx[2:4]\n\n[1]  4  1 NA\n\n\n\nModifying mutable objects\nIndexing also allows to modify some of the values of mutable objects:\n\nx\n\n[1] 2 4 1\n\nx[2] &lt;- 0\nx\n\n[1] 2 0 1\n\n\n\n\nCopy-on-modify\nNot all languages behave the same when you assign the same mutable object to several variables, then modify one of them.\n\nIn Python: no copy-on-modify\n\nDon’t try to run this code in R. This is for information only.\n\n\n\nPython\n\na = [1, 2, 3]\nb = a\nb\n\n[1, 2, 3]\n\n\nPython\n\na[0] = 4           # In Python, indexing starts at 0\na\n\n[4, 2, 3]\n\n\nPython\n\nb\n\n[4, 2, 3]\nModifying a also modifies b: this is because no copy is made when you modify a. If you want to keep b unchanged, you need to assign an explicit copy of a to it with b = copy.copy(a).\n\n\nIn R: copy-on-modify\n\na &lt;- c(1, 2, 3)\nb &lt;- a\nb\n\n[1] 1 2 3\n\na[1] &lt;- 4          # In R, indexing starts at 1\na\n\n[1] 4 2 3\n\nb\n\n[1] 1 2 3\n\n\nHere, the default is to create a new copy in memory when a is transformed so that b remains unchanged."
  },
  {
    "objectID": "r/intro_indexing.html#indexing-matrices-and-arrays",
    "href": "r/intro_indexing.html#indexing-matrices-and-arrays",
    "title": "Indexing",
    "section": "Indexing matrices and arrays",
    "text": "Indexing matrices and arrays\n\nx &lt;- matrix(1:12, nrow = 3, ncol = 4)\nx\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    4    7   10\n[2,]    2    5    8   11\n[3,]    3    6    9   12\n\nx[2, 3]\n\n[1] 8\n\nx &lt;- array(as.double(1:24), c(3, 2, 4))\nx\n\n, , 1\n\n     [,1] [,2]\n[1,]    1    4\n[2,]    2    5\n[3,]    3    6\n\n, , 2\n\n     [,1] [,2]\n[1,]    7   10\n[2,]    8   11\n[3,]    9   12\n\n, , 3\n\n     [,1] [,2]\n[1,]   13   16\n[2,]   14   17\n[3,]   15   18\n\n, , 4\n\n     [,1] [,2]\n[1,]   19   22\n[2,]   20   23\n[3,]   21   24\n\nx[2, 1, 3]\n\n[1] 14"
  },
  {
    "objectID": "r/intro_indexing.html#indexing-lists",
    "href": "r/intro_indexing.html#indexing-lists",
    "title": "Indexing",
    "section": "Indexing lists",
    "text": "Indexing lists\n\nx &lt;- list(2L, 3:8, c(2, 1), FALSE, \"string\")\nx\n\n[[1]]\n[1] 2\n\n[[2]]\n[1] 3 4 5 6 7 8\n\n[[3]]\n[1] 2 1\n\n[[4]]\n[1] FALSE\n\n[[5]]\n[1] \"string\"\n\n\nIndexing a list returns a list:\n\nx[3]\n\n[[1]]\n[1] 2 1\n\ntypeof(x[3])\n\n[1] \"list\"\n\n\nTo extract elements of a list, double square brackets are required:\n\nx[[3]]\n\n[1] 2 1\n\ntypeof(x[[3]])\n\n[1] \"double\"\n\n\n\n\nYour turn:\n\nTry to extract the number 7 from this list."
  },
  {
    "objectID": "r/intro_indexing.html#indexing-data-frames",
    "href": "r/intro_indexing.html#indexing-data-frames",
    "title": "Indexing",
    "section": "Indexing data frames",
    "text": "Indexing data frames\n\nx &lt;- data.frame(\n  country = c(\"Canada\", \"USA\", \"Mexico\"),\n  var = c(2.9, 3.1, 4.5)\n)\nx\n\n  country var\n1  Canada 2.9\n2     USA 3.1\n3  Mexico 4.5\n\n\nIndexing dataframes can be done by using indices, as we saw for matrices:\n\nx[2, 1]\n\n[1] \"USA\"\n\n\nIt can also be done using column names thanks to the $ symbol (a column is a vector, so indexing from a column is the same as indexing from a vector):\n\nx$country[2]\n\n[1] \"USA\"\n\n\nA data frame is actually a list of vectors representing the various columns:\n\ntypeof(x)\n\n[1] \"list\"\n\n\nIndexing a column can thus also be done by indexing the element of the list with double square brackets (although this is a slower method).\nWe get the same result with:\n\nx[[1]][2]\n\n[1] \"USA\""
  },
  {
    "objectID": "r/intro_data_structure.html",
    "href": "r/intro_data_structure.html",
    "title": "Data types and structures",
    "section": "",
    "text": "This section covers the various data types and structures available in R."
  },
  {
    "objectID": "r/intro_data_structure.html#summary-of-structures",
    "href": "r/intro_data_structure.html#summary-of-structures",
    "title": "Data types and structures",
    "section": "Summary of structures",
    "text": "Summary of structures\n\n\n\nDimension\nHomogeneous\nHeterogeneous\n\n\n\n\n1 d\nAtomic vector\nList\n\n\n2 d\nMatrix\nData frame\n\n\n3 d\nArray"
  },
  {
    "objectID": "r/intro_data_structure.html#atomic-vectors",
    "href": "r/intro_data_structure.html#atomic-vectors",
    "title": "Data types and structures",
    "section": "Atomic vectors",
    "text": "Atomic vectors\n\nWith a single element\n\na &lt;- 2\na\n\n[1] 2\n\ntypeof(a)\n\n[1] \"double\"\n\nstr(a)\n\n num 2\n\nlength(a)\n\n[1] 1\n\ndim(a)\n\nNULL\n\n\nThe dim attribute of a vector doesn’t exist (hence the NULL). This makes vectors different from one-dimensional arrays which have a dim of 1.\nYou might have noticed that 2 is a double (double precision floating point number, equivalent of “float” in other languages). In R, this is the default, even if you don’t type 2.0. This prevents the kind of weirdness you can find in, for instance, Python.\nIn Python:\n&gt;&gt;&gt; 2 == 2.0\nTrue\n&gt;&gt;&gt; type(2) == type(2.0)\nFalse\n&gt;&gt;&gt; type(2)\n&lt;class 'int'&gt;\n&gt;&gt;&gt; type(2.0)\n&lt;class 'float'&gt;\nIn R:\n&gt; 2 == 2.0\n[1] TRUE\n&gt; typeof(2) == typeof(2.0)\n[1] TRUE\n&gt; typeof(2)\n[1] \"double\"\n&gt; typeof(2.0)\n[1] \"double\"\nIf you want to define an integer variable, you use:\n\nb &lt;- 2L\nb\n\n[1] 2\n\ntypeof(b)\n\n[1] \"integer\"\n\nmode(b)\n\n[1] \"numeric\"\n\nstr(b)\n\n int 2\n\n\nThere are six vector types:\n\nlogical\ninteger\ndouble\ncharacter\ncomplex\nraw\n\n\n\nWith multiple elements\n\nc &lt;- c(2, 4, 1)\nc\n\n[1] 2 4 1\n\ntypeof(c)\n\n[1] \"double\"\n\nmode(c)\n\n[1] \"numeric\"\n\nstr(c)\n\n num [1:3] 2 4 1\n\n\n\nd &lt;- c(TRUE, TRUE, NA, FALSE)\nd\n\n[1]  TRUE  TRUE    NA FALSE\n\ntypeof(d)\n\n[1] \"logical\"\n\nstr(d)\n\n logi [1:4] TRUE TRUE NA FALSE\n\n\n\nNA (“Not Available”) is a logical constant of length one. It is an indicator for a missing value.\n\nVectors are homogeneous, so all elements need to be of the same type.\nIf you use elements of different types, R will convert some of them to ensure that they become of the same type:\n\ne &lt;- c(\"This is a string\", 3, \"test\")\ne\n\n[1] \"This is a string\" \"3\"                \"test\"            \n\ntypeof(e)\n\n[1] \"character\"\n\nstr(e)\n\n chr [1:3] \"This is a string\" \"3\" \"test\"\n\n\n\nf &lt;- c(TRUE, 3, FALSE)\nf\n\n[1] 1 3 0\n\ntypeof(f)\n\n[1] \"double\"\n\nstr(f)\n\n num [1:3] 1 3 0\n\n\n\ng &lt;- c(2L, 3, 4L)\ng\n\n[1] 2 3 4\n\ntypeof(g)\n\n[1] \"double\"\n\nstr(g)\n\n num [1:3] 2 3 4\n\n\n\nh &lt;- c(\"string\", TRUE, 2L, 3.1)\nh\n\n[1] \"string\" \"TRUE\"   \"2\"      \"3.1\"   \n\ntypeof(h)\n\n[1] \"character\"\n\nstr(h)\n\n chr [1:4] \"string\" \"TRUE\" \"2\" \"3.1\"\n\n\nThe binary operator : is equivalent to the seq() function and generates a regular sequence of integers:\n\ni &lt;- 1:5\ni\n\n[1] 1 2 3 4 5\n\ntypeof(i)\n\n[1] \"integer\"\n\nstr(i)\n\n int [1:5] 1 2 3 4 5\n\nidentical(2:8, seq(2, 8))\n\n[1] TRUE"
  },
  {
    "objectID": "r/intro_data_structure.html#matrices",
    "href": "r/intro_data_structure.html#matrices",
    "title": "Data types and structures",
    "section": "Matrices",
    "text": "Matrices\n\nj &lt;- matrix(1:12, nrow = 3, ncol = 4)\nj\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    4    7   10\n[2,]    2    5    8   11\n[3,]    3    6    9   12\n\ntypeof(j)\n\n[1] \"integer\"\n\nstr(j)\n\n int [1:3, 1:4] 1 2 3 4 5 6 7 8 9 10 ...\n\nlength(j)\n\n[1] 12\n\ndim(j)\n\n[1] 3 4\n\n\nThe default is byrow = FALSE. If you want the matrix to be filled in by row, you need to set this argument to TRUE:\n\nk &lt;- matrix(1:12, nrow = 3, ncol = 4, byrow = TRUE)\nk\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    2    3    4\n[2,]    5    6    7    8\n[3,]    9   10   11   12"
  },
  {
    "objectID": "r/intro_data_structure.html#arrays",
    "href": "r/intro_data_structure.html#arrays",
    "title": "Data types and structures",
    "section": "Arrays",
    "text": "Arrays\n\nl &lt;- array(as.double(1:24), c(3, 2, 4))\nl\n\n, , 1\n\n     [,1] [,2]\n[1,]    1    4\n[2,]    2    5\n[3,]    3    6\n\n, , 2\n\n     [,1] [,2]\n[1,]    7   10\n[2,]    8   11\n[3,]    9   12\n\n, , 3\n\n     [,1] [,2]\n[1,]   13   16\n[2,]   14   17\n[3,]   15   18\n\n, , 4\n\n     [,1] [,2]\n[1,]   19   22\n[2,]   20   23\n[3,]   21   24\n\ntypeof(l)\n\n[1] \"double\"\n\nstr(l)\n\n num [1:3, 1:2, 1:4] 1 2 3 4 5 6 7 8 9 10 ...\n\nlength(l)\n\n[1] 24\n\ndim(l)\n\n[1] 3 2 4"
  },
  {
    "objectID": "r/intro_data_structure.html#lists",
    "href": "r/intro_data_structure.html#lists",
    "title": "Data types and structures",
    "section": "Lists",
    "text": "Lists\n\nm &lt;- list(2, 3)\nm\n\n[[1]]\n[1] 2\n\n[[2]]\n[1] 3\n\ntypeof(m)\n\n[1] \"list\"\n\nstr(m)\n\nList of 2\n $ : num 2\n $ : num 3\n\nlength(m)\n\n[1] 2\n\ndim(m)\n\nNULL\n\n\nAs with atomic vectors, lists do not have a dim attribute. Lists are in fact a different type of vectors.\nLists can be heterogeneous:\n\nn &lt;- list(2L, 3, c(2, 1), FALSE, \"string\")\nn\n\n[[1]]\n[1] 2\n\n[[2]]\n[1] 3\n\n[[3]]\n[1] 2 1\n\n[[4]]\n[1] FALSE\n\n[[5]]\n[1] \"string\"\n\ntypeof(n)\n\n[1] \"list\"\n\nstr(n)\n\nList of 5\n $ : int 2\n $ : num 3\n $ : num [1:2] 2 1\n $ : logi FALSE\n $ : chr \"string\"\n\nlength(n)\n\n[1] 5"
  },
  {
    "objectID": "r/intro_data_structure.html#data-frames",
    "href": "r/intro_data_structure.html#data-frames",
    "title": "Data types and structures",
    "section": "Data frames",
    "text": "Data frames\nData frames contain tabular data. Under the hood, a data frame is a list of vectors.\n\no &lt;- data.frame(\n  country = c(\"Canada\", \"USA\", \"Mexico\"),\n  var = c(2.9, 3.1, 4.5)\n)\no\n\n  country var\n1  Canada 2.9\n2     USA 3.1\n3  Mexico 4.5\n\ntypeof(o)\n\n[1] \"list\"\n\nstr(o)\n\n'data.frame':   3 obs. of  2 variables:\n $ country: chr  \"Canada\" \"USA\" \"Mexico\"\n $ var    : num  2.9 3.1 4.5\n\nlength(o)\n\n[1] 2\n\ndim(o)\n\n[1] 3 2"
  },
  {
    "objectID": "r/intro_basics.html",
    "href": "r/intro_basics.html",
    "title": "First steps in R",
    "section": "",
    "text": "In this section, we take our first few steps in R: we will access the R documentation, see how to set R options, and talk about a few concepts."
  },
  {
    "objectID": "r/intro_basics.html#help-and-documentation",
    "href": "r/intro_basics.html#help-and-documentation",
    "title": "First steps in R",
    "section": "Help and documentation",
    "text": "Help and documentation\nFor some general documentation on R, you can run:\nhelp.start()\nTo get help on a function (e.g. sum), you can run:\nhelp(sum)\nDepending on your settings, this will open a documentation for sum in a pager or in your browser."
  },
  {
    "objectID": "r/intro_basics.html#r-settings",
    "href": "r/intro_basics.html#r-settings",
    "title": "First steps in R",
    "section": "R settings",
    "text": "R settings\nSettings are saved in a .Rprofile file. You can edit the file directly in any text editor or from within R.\nList all options:\noptions()\nReturn the value of a particular option:\n\ngetOption(\"help_type\")\n\n[1] \"html\"\n\n\nSet an option:\noptions(help_type = \"html\")"
  },
  {
    "objectID": "r/intro_basics.html#assignment",
    "href": "r/intro_basics.html#assignment",
    "title": "First steps in R",
    "section": "Assignment",
    "text": "Assignment\nR can accept the equal sign (=) for assignments, but it is more idiomatic to use the assignment sign (&lt;-) whenever you bind a name to a value and to use the equal sign everywhere else.\n\na &lt;- 3\n\nOnce you have bound a name to a value, you can recall the value with that name:\n\na  # Note that you do not need to use a print() function in R\n\n[1] 3\n\n\nYou can remove an object from the environment by deleting its name:\n\nrm(a)\n\nLet’s confirm that a doesn’t exist anymore in the environment:\n\na\n\nError in eval(expr, envir, enclos): object 'a' not found\n\n\nThe garbage collector will take care of deleting the object itself from memory."
  },
  {
    "objectID": "r/intro_basics.html#comments",
    "href": "r/intro_basics.html#comments",
    "title": "First steps in R",
    "section": "Comments",
    "text": "Comments\nAnything to the left of # is a comment and is ignored by R:\n\n# This is an inline comment\n\na &lt;- 3  # This is also a comment"
  },
  {
    "objectID": "r/hpc_run.html",
    "href": "r/hpc_run.html",
    "title": "SSH login",
    "section": "",
    "text": "This section will show you how to access our temporary remote cluster through SSH."
  },
  {
    "objectID": "r/hpc_run.html#why-not-use-rstudio-server",
    "href": "r/hpc_run.html#why-not-use-rstudio-server",
    "title": "SSH login",
    "section": "Why not use RStudio server?",
    "text": "Why not use RStudio server?\nIn our introduction to R, we used an RStudio server running on a remote cluster. In this course, we will log in a similar remote supercomputer using Secure Shell, then run R scripts from the command line.\nWhy are we not making use of the interactivity of R which is an interpreted language and why are we not using the added comfort of an IDE? The short answer is: resource efficiency.\nOnce you have developed your code in an interactive fashion in the IDE of your choice using small hardware resources on a sample of your data, running scripts allows you to only request large resources when you need them (i.e. when your code is running). This prevents heavy resources from sitting idle when not in use, as would happen in an interactive session while you type, think, etc. It will save you money on commercial clusters and waiting time on the Alliance clusters.\nThis course being about high-performance R, let’s learn to use it through scripts."
  },
  {
    "objectID": "r/hpc_run.html#logging-in-the-temporary-cluster-through-ssh",
    "href": "r/hpc_run.html#logging-in-the-temporary-cluster-through-ssh",
    "title": "SSH login",
    "section": "Logging in the temporary cluster through SSH",
    "text": "Logging in the temporary cluster through SSH\nYou do not need to install anything on your machine for this course as we will provide access to a temporary remote cluster.\n\nA username, hostname, and password will be given to you during the workshop.\n\n\nNote that this temporary cluster will only be available for the duration of this course.\n\n\nOpen a terminal emulator\nWindows users:  Install the free version of MobaXTerm and launch it.\nMacOS users:   Launch Terminal.\nLinux users:     Open the terminal emulator of your choice.\n\n\nAccess the cluster through secure shell\n\nWindows users\nFollow the first 18% of this demo.\nFor “Remote host”, use the hostname we gave you.\nSelect the box “Specify username” and provide your username.\n\nNote that the password is entered through blind typing, meaning that you will not see anything happening as you type it. This is a Linux feature. While it is a little disturbing at first, do know that it is working. Make sure to type it slowly to avoid typos, then press the “enter” key on your keyboard.\n\n\n\nMacOS and Linux users\nIn the terminal, run:\nssh &lt;username&gt;@&lt;hostname&gt;\n\nReplace the username and hostname by their values. For instance:\nssh user021@somecluster.c3.ca\n\nYou will be asked a question, answer “Yes”.\nWhen prompted, type the password.\n\nNote that the password is entered through blind typing, meaning that you will not see anything happening as you type it. This is a Linux feature. While it is a little disturbing at first, do know that it is working. Make sure to type it slowly to avoid typos, then press the “enter” key on your keyboard.\n\n\n\n\nTroubleshooting\nProblems logging in are almost always due to typos. If you cannot log in, retry slowly, entering your password carefully."
  },
  {
    "objectID": "r/hpc_rcpp.html",
    "href": "r/hpc_rcpp.html",
    "title": "Writing C++ in R with Rcpp",
    "section": "",
    "text": "Sometimes, parallelization is not an option, either because the code is hard to parallelize or because of lack of hardware. In such cases, one way to increase speed is to replace slow R code with C++. The package Rcpp makes this particularly easy by creating mappings between both languages."
  },
  {
    "objectID": "r/hpc_rcpp.html#back-to-fibonacci",
    "href": "r/hpc_rcpp.html#back-to-fibonacci",
    "title": "Writing C++ in R with Rcpp",
    "section": "Back to Fibonacci",
    "text": "Back to Fibonacci\nDo you remember the Fibonacci numbers? Here was a naive implementation in R:\n\nfib &lt;- function(n) {\n  if(n == 0) return(0)\n  if(n == 1) return(1)\n  Recall(n - 1) + Recall(n - 2)\n}\n\nThis function gives the nth number in the sequence.\n\nExample:\n\n\nfib(30)\n\n[1] 832040"
  },
  {
    "objectID": "r/hpc_rcpp.html#rcpp",
    "href": "r/hpc_rcpp.html#rcpp",
    "title": "Writing C++ in R with Rcpp",
    "section": "Rcpp",
    "text": "Rcpp\nLet’s translate this function in C++ within R!\nFirst we need to load the Rcpp package:\n\nlibrary(Rcpp)\n\nWe then use the function cppFunction() to assign to an R function a function written in C++:\n\nfibRcpp &lt;- cppFunction( '\nint fibonacci(const int x) {\n   if (x == 0) return(0);\n   if (x == 1) return(1);\n   return (fibonacci(x - 1)) + fibonacci(x - 2);\n}\n' )\n\nWe can call our function as any R function:\nfibRcpp(30)\n[1] 832040\nWe can compare both functions:\nlibrary(bench)\n\nn &lt;- 30\nmark(fib(n), fibRcpp(n))\n# A tibble: 2 × 13\n  expression      min   median `itr/sec` mem_alloc `gc/sec` n_itr  n_gc\n  &lt;bch:expr&gt; &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;\n1 fib(n)        1.66s    1.66s     0.601    44.7KB     22.8     1    38\n2 fibRcpp(n)   1.08ms   1.08ms   901.       2.49KB      0     451     0\n  total_time result    memory                 time            \n    &lt;bch:tm&gt; &lt;list&gt;    &lt;list&gt;                 &lt;list&gt;          \n1      1.66s &lt;dbl [1]&gt; &lt;Rprofmem [6,778 × 3]&gt; &lt;bench_tm [1]&gt;  \n2   500.37ms &lt;int [1]&gt; &lt;Rprofmem [1 × 3]&gt;     &lt;bench_tm [451]&gt;\n  gc                \n  &lt;list&gt;            \n1 &lt;tibble [1 × 3]&gt;  \n2 &lt;tibble [451 × 3]&gt;\nWarning message:\nSome expressions had a GC in every iteration; so filtering is disabled.\nThe speedup is 1,537, which is amazing.\nIn this particular example, we saw that memoisation gives an even more incredible speedup (35,000!), but while memoisation will only work in very specific situations (e.g. recursive function calls), using C++ code is a general method to provide speedup. It is particularly useful when:\n\nthere are large numbers of function calls (R is particularly slow with function calls),\nyou need data structures that are missing in R,\nyou want to create efficient packages (fast R packages are written in C++ and many use Rcpp).\n\n\nIn this example, we declared the C++ function directly in R. It is possible to use source files instead."
  },
  {
    "objectID": "r/hpc_partition.html",
    "href": "r/hpc_partition.html",
    "title": "Partitioning data with multidplyr",
    "section": "",
    "text": "The package multidplyr provides simple techniques to partition data across a set of workers on the same node."
  },
  {
    "objectID": "r/hpc_partition.html#data-partitioning-for-memory",
    "href": "r/hpc_partition.html#data-partitioning-for-memory",
    "title": "Partitioning data with multidplyr",
    "section": "Data partitioning for memory",
    "text": "Data partitioning for memory\n\nCase example\nWhat if we have an even bigger dataset?\nThe randomForest() function has limitations:\n\nIt is a memory hog.\nIt doesn’t run if your data frame has too many rows.\n\nIf you try to run:\n\n\nbigger.R\n\nlibrary(randomForest)\n\nbigger_iris &lt;- iris[rep(seq_len(nrow(iris)), each = 1e3), ]\nrownames(bigger_iris) &lt;- NULL\n\nset.seed(123)\nrf &lt;- randomForest(Species ~ ., data = bigger_iris)\n\nrf\n\non a single core, you will get:\nrandomForest 4.7-1.1\nType rfNews() to see new features/changes/bug fixes.\n/var/spool/slurmd/job00016/slurm_script: line 5: 74451 Killed                  Rscript data_partition.R\nslurmstepd: error: Detected 1 oom-kill event(s) in StepId=16.batch. Some of your processes may have been killed by the cgroup out-of-memory handler.\nYou have ran out of memory.\nReducing the number of trees won’t help as the problem comes from the size of the data frame.\nSimilarly, using foreach and doFuture as we did previously won’t help either because that spreads the number of trees on various cores, but again, the problem doesn’t come from the number of trees, but for the size of the dataset.\n\nWith plan(multisession), you would get:\nCluster with multisession\nError in unserialize(node$con) :\n  MultisessionFuture (doFuture2-3) failed to receive message results from cluster RichSOCKnode #3 (PID 445273 on localhost ‘localhost’). The reason reported was ‘error reading from connection’. Post-mortem diagnostic: No process exists with this PID, i.e. the localhost worker is no longer alive. The total size of the 3 globals exported is 5.15 MiB. There are three globals: ‘big_iris’ (5.15 MiB of class ‘list’), ‘...future.seeds_ii’ (160 bytes of class ‘list’) and ‘...future.x_ii’ (112 bytes of class ‘list’)\nAnd with plan(multicore):\nCluster with multicore\nError: Failed to retrieve the result of MulticoreFuture (doFuture2-2) from the forked worker (on localhost; PID 444769). Post-mortem diagnostic: No process exists with this PID, i.e. the forked localhost worker is no longer alive. The total size of the 3 globals exported is 5.15 MiB. There are three globals: ‘big_iris’ (5.15 MiB of class ‘list’), ‘...future.seeds_ii’ (160 bytes of class ‘list’) and ‘...future.x_ii’ (112 bytes of class ‘list’)\nIn addition: Warning message:\nIn mccollect(jobs = jobs, wait = TRUE) :\n  1 parallel job did not deliver a result\n\nYou can even try spreading the trees on multiple nodes, but things will fail as well, without any error message.\nOf course, you could always try on a different machine—one with more memory. I used my machine which has more memory than this training cluster and it worked.\nBut then, what if big_iris is even bigger? Say, if we have this for instance:\nbigger_iris &lt;- iris[rep(seq_len(nrow(iris)), each = 1e4), ]\nThen no amount of memory will save you and you will get errors similar to this:\nError in randomForest.default(m, y, ...) : \n  long vectors (argument 28) are not supported in .C\nThat’s because randomForest() does not accept datasets with too many rows.\n\nThe bottom line is that there are situation in which the data is just too big. In such cases, you want to look at data parallelism: instead of splitting your code into tasks that can run in parallel as we did previously, you split the data into chunks and run the code in parallel on those chunks.\n\n\nOf course, you could also simply run the code on a subset of your data. In many situation, reducing your data by sampling it properly will be good enough. But there are situations in which you want to use a huge dataset.\n\nYou could split the data manually and run the code on each chunk, but it would be tedious and very lengthy. And to run the code on all the chunks in parallel, you could implement that yourself. There is a much simpler option provided by the multidplyr package.\n\n\nUsing multidplyr\nTo see what happens as we use multidplyr, let’s first run the code in an interactive session on one node with 4 cores:\n# Launch the interactive job\nsalloc --time=50 --mem-per-cpu=7500M --cpus-per-task=4\n\n# Then launch R\nR\nFirst, we load the packages that are running in the main session:\n\nlibrary(multidplyr)\nlibrary(dplyr, warn.conflicts = FALSE)\n\n\nWe load dplyr for the do() function.\nNotice that we aren’t loading the randomForest package yet: that’s because we will use it on workers, not in the main session.\n\nThen we need to create a cluster of workers. Let’s use 4 workers that we will run on a full node:\n\ncl &lt;- new_cluster(4)\ncl\n\n4 session cluster [....]\n\n\nNow we can load the randomForest package on each worker:\ncluster_library(cl, \"randomForest\")\nrandomForest 4.7-1.1\nType rfNews() to see new features/changes/bug fixes.\n\nAttaching package: ‘randomForest’\n\nThe following object is masked from ‘package:dplyr’:\n\n    combine\nOf course, we need to generate our big dataset:\n\nbigger_iris &lt;- iris[rep(seq_len(nrow(iris)), each = 1e3), ]\nrownames(bigger_iris) &lt;- NULL\n\nThen we create a partitioned data frame on the workers with the partition() function. The function will try to split the data as heavenly as possible among workers.\nIf you group observations by some variable (with dplyr::group_by()) beforehand, multidplyr will ensure that all data points in a group end up on the same worker. This is very convenient in a lot of cases, but is not relevant here. Without grouping observations first, it is unclear how partition() chooses which observation goes to which worker. In our data, we have all the setosa observations first, then all the versicolor, and finally all the virginica. We want to make sure that the randomForest() function runs on a sample of all 3 species. We will thus randomly shuffle the data before partitioning it (when we were parallelizing by splitting the trees, we didn’t have to worry about that since each subset of trees was running on the entire dataset):\n\n# Shuffle the rows of the data frame randomly\nset.seed(11)\nbigger_iris_shuffled &lt;- bigger_iris[sample(nrow(bigger_iris)), ]\n\n# You can check that they are shuffled\nhead(bigger_iris_shuffled)\n\n       Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n65570           6.7         3.1          4.4         1.4 versicolor\n19004           5.1         3.8          1.5         0.3     setosa\n73612           6.1         2.8          4.7         1.2 versicolor\n28886           5.2         3.4          1.4         0.2     setosa\n121310          5.6         2.8          4.9         2.0  virginica\n21667           5.1         3.7          1.5         0.4     setosa\n\n\n# Create the partitioned data frame\nsplit_iris &lt;- partition(bigger_iris_shuffled, cl)\nsplit_iris\nSource: party_df [150,000 x 5]\nShards: 4 [37,500--37,500 rows]\n\n# A data frame: 150,000 × 5\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n         &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;\n1          6.7         3.1          4.4         1.4 versicolor\n2          5.6         2.8          4.9         2   virginica\n3          6.4         2.8          5.6         2.2 virginica\n4          5.6         2.5          3.9         1.1 versicolor\n5          4.7         3.2          1.6         0.2 setosa\n6          6.7         3            5           1.7 versicolor\n# ℹ 149,994 more rows\n# ℹ Use `print(n = ...)` to see more rows\nIf we want the code to be reproducible, we should set the seed on each worker:\ncluster_send(cl, set.seed(123))\n\nRun cluster_send() to send code to each worker when you aren’t interested in any result (as is the case here) and cluster_call() if you want a computation to be executed on each worker and a result to be returned.\n\nNow we can run the randomForest() function on each worker:\nsplit_rfs &lt;- split_iris %&gt;%\n  do(rf = randomForest(Species ~ ., data = .))\nsplit_rfs is a partitioned data frame containing the results from each worker (the intermediate randomForest models):\nsplit_rfs\nSource: party_df [4 x 1]\nShards: 4 [1--1 rows]\n\n# A data frame: 4 × 1\n  rf\n  &lt;list&gt;\n1 &lt;rndmFrs.&gt;\n2 &lt;rndmFrs.&gt;\n3 &lt;rndmFrs.&gt;\n4 &lt;rndmFrs.&gt;\nNow we need to bring the partitioned results in the main process:\nrfs &lt;- split_rfs %&gt;% collect()\nrfs is a data frame with a single column called rf:\nrfs\n# A tibble: 4 × 1\n  rf\n  &lt;list&gt;\n1 &lt;rndmFrs.&gt;\n2 &lt;rndmFrs.&gt;\n3 &lt;rndmFrs.&gt;\n4 &lt;rndmFrs.&gt;\nWhich means that rfs$rf is a list:\ntypeof(rfs$rf)\n[1] \"list\"\nEach element of this list is a randomForest object (the 4 intermediate models created by the 4 workers):\nrfs$rf\n[[1]]\n\nCall:\n randomForest(formula = Species ~ ., data = .)\n               Type of random forest: classification\n                     Number of trees: 500\nNo. of variables tried at each split: 2\n\n        OOB estimate of  error rate: 0%\nConfusion matrix:\n           setosa versicolor virginica class.error\nsetosa      12500          0         0           0\nversicolor      0      12500         0           0\nvirginica       0          0     12500           0\n\n[[2]]\n\nCall:\n randomForest(formula = Species ~ ., data = .)\n               Type of random forest: classification\n                     Number of trees: 500\nNo. of variables tried at each split: 2\n\n        OOB estimate of  error rate: 0%\nConfusion matrix:\n           setosa versicolor virginica class.error\nsetosa      12500          0         0           0\nversicolor      0      12500         0           0\nvirginica       0          0     12500           0\n\n[[3]]\n\nCall:\n randomForest(formula = Species ~ ., data = .)\n               Type of random forest: classification\n                     Number of trees: 500\nNo. of variables tried at each split: 2\n\n        OOB estimate of  error rate: 0%\nConfusion matrix:\n           setosa versicolor virginica class.error\nsetosa      12500          0         0           0\nversicolor      0      12500         0           0\nvirginica       0          0     12500           0\n\n[[4]]\n\nCall:\n randomForest(formula = Species ~ ., data = .)\n               Type of random forest: classification\n                     Number of trees: 500\nNo. of variables tried at each split: 2\n\n        OOB estimate of  error rate: 0%\nConfusion matrix:\n           setosa versicolor virginica class.error\nsetosa      12500          0         0           0\nversicolor      0      12500         0           0\nvirginica       0          0     12500           0\n\nIf you don’t need to explore the intermediate objects, you can combine the commands as:\nrfs &lt;- split_iris %&gt;%\n  do(rf = randomForest(Species ~ ., data = .)) %&gt;%\n  collect()\n\nFinally, we need to combine the 4 randomForest models into a single one. This can be done with the combine() function from the randomForest package (the same function we already used in our foreach expressions):\nrf_all &lt;- do.call(combine, rfs$rf)\n\nBe careful that randomForest and dplyr both have a combine() function. The one we want here is the one from the randomForest package. To avoid all conflict and confusion, you can use randomForest::combine(). combine() is ok if you make sure to load dplyr before randomForest since latest loaded functions overwrite earlier loaded ones.\n\nWhy are we using do.call()? If we use:\ncombine(rfs$rf)\nWe get the silly message:\nError in combine(rfs$rf) :\n  Argument must be a list of randomForest objects\nThat is because randomForest::combine() expects a list of randomForest objects, but cannot accept an object of type list.\nHere is our final randomForest model:\nrf_all\nCall:\n randomForest(formula = Species ~ ., data = .)\n               Type of random forest: classification\n                     Number of trees: 2000\nNo. of variables tried at each split: 2\nThis is it: by splitting our data frame on 4 cores, we could run the code and create a randomForest model using whole of the data.\nWe can test our model:\nnew_data &lt;- data.frame(\n  Sepal.Length = c(5.3, 4.6, 6.5),\n  Sepal.Width = c(3.1, 3.9, 2.5),\n  Petal.Length = c(1.5, 1.5, 5.0),\n  Petal.Width = c(0.2, 0.1, 2.1)\n)\n\npredict(rf_all, new_data)\n        1         2         3\n   setosa    setosa virginica\nLevels: setosa versicolor virginica\nRunning this in an interactive session was useful to see what happens, but the way you would actually do this is by writing a script (let’s call it partition.R):\n\n\npartition.R\n\nlibrary(multidplyr)\nlibrary(dplyr, warn.conflicts = FALSE)\n\n# Create cluster of workers\ncl &lt;- new_cluster(4)\n\n# Load randomForest on each worker\ncluster_library(cl, \"randomForest\")\n\n# Create our big data frame\nbigger_iris &lt;- iris[rep(seq_len(nrow(iris)), each = 1e3), ]\nrownames(bigger_iris) &lt;- NULL\n\n# Create a partitioned data frame on the workers\nsplit_iris &lt;- partition(bigger_iris, cl)\n\n# Set the seed on each worker\ncluster_send(cl, set.seed(123))\n\n# Run the randomForest() function on each worker\nrfs &lt;- split_iris %&gt;%\n  do(rf = randomForest(Species ~ ., data = .)) %&gt;%\n  collect()\n\n# Combine the randomForest models into one\nrf_all &lt;- do.call(combine, rfs$rf)\n\nAnd run it with a Bash partition.sh script:\n\n\npartition.sh\n\n#!/bin/bash\n#SBATCH --time=10\n#SBATCH --mem-per-cpu=7500M\n#SBATCH --cpus-per-task=4\n\nRscript partition.R\n\n\n\nConclusion\nmultidplyr allowed us to split our data frame across multiple workers on one node and this solved the memory issue we had with our large dataset."
  },
  {
    "objectID": "r/hpc_partition.html#data-partitioning-for-speed",
    "href": "r/hpc_partition.html#data-partitioning-for-speed",
    "title": "Partitioning data with multidplyr",
    "section": "Data partitioning for speed",
    "text": "Data partitioning for speed\nBeside the memory advantage, are we getting any speedup from data parallelization? i.e. how does this code compare with the parallelization we did as regard the number of trees with foreach and doFuture?\nWe want to make sure to compare the same things. So we go back to our smaller big_iris and we up the number of trees back to 2000.\nWe will compare it with the plans multisession and multicore that we performed earlier. The minimum and median times for these two options for shared memory parallelism were of 2.72s and 3.15s respectively.\n\n\npartition_bench.R\n\nlibrary(multidplyr)\nlibrary(dplyr, warn.conflicts = FALSE)\nlibrary(bench)\n\ncl &lt;- new_cluster(4)\ncluster_library(cl, \"randomForest\")\n\nbig_iris &lt;- iris[rep(seq_len(nrow(iris)), each = 1e2), ]\nrownames(big_iris) &lt;- NULL\n\ncluster_send(cl, set.seed(123))\n\npart_rf &lt;- function(data, cluster) {\n  split_data &lt;- partition(data, cluster)\n  rfs &lt;- split_data %&gt;%\n    do(rf = randomForest(Species ~ ., data = ., ntree = 2000)) %&gt;%\n    collect()\n  do.call(combine, rfs$rf)\n}\n\nmark(rf_all &lt;- part_rf(big_iris, cl))\n\n\n\npartition_bench.sh\n\n#!/bin/bash\n#SBATCH --time=10\n#SBATCH --mem-per-cpu=7500M\n#SBATCH --cpus-per-task=4\n\nRscript partition_bench.R\n\nsbatch partition_bench.sh\nrandomForest 4.7-1.1\nType rfNews() to see new features/changes/bug fixes.\n\nAttaching package: ‘randomForest’\n\nThe following object is masked from ‘package:dplyr’:\n\n    combine\n\n# A tibble: 1 × 13\n  expression      min median `itr/sec` mem_alloc `gc/sec` n_itr  n_gc total_time\n  &lt;bch:expr&gt;    &lt;bch&gt; &lt;bch:&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;   &lt;bch:tm&gt;\n1 rf_all &lt;- pa… 2.48s  2.48s     0.403        NA     2.02     1     5      2.48s\n# ℹ 4 more variables: result &lt;list&gt;, memory &lt;list&gt;, time &lt;list&gt;, gc &lt;list&gt;\nWarning message:\nSome expressions had a GC in every iteration; so filtering is disabled.\n\nWhat about distributed memory?\nCan multidplyr run in distributed memory? There is nothing on this in the documentation, so I tried it.\nI upped the number of workers to 8 and ran the code on 2 nodes with 4 cores per node and got no speedup. I also created a dataset 10 times bigger (with each = 1e4), which creates an OOM on 4 cores one a single node and tried it on 11 nodes with 4 cores (10 to match the 10 times size increase, plus one to play safe). This didn’t solve the OOM issue. I tried various other tests, all with no success.\nIn conclusion, it seems that multidply’s way of creating a cluster of workers doesn’t have a mechanism to spread them across nodes and that the package thus does not allow to split data across nodes.\nIn cases where your data is so big that it doesn’t fit in the memory of a single node, it doesn’t seem that any R package currently allow to split the data automatically for you.\n\n\nConclusion\nAs we could see, we got similar results: in this case, it is the same to spread the number of trees running on the full data on 4 cores (as we did with foreach and doFuture or to run all the trees on the data spread on 4 cores.\nThe difference being that foreach and doFuture allowed us to spread the trees across nodes while multidplyr does not allow this for the data."
  },
  {
    "objectID": "r/hpc_partition.html#direct-data-loading",
    "href": "r/hpc_partition.html#direct-data-loading",
    "title": "Partitioning data with multidplyr",
    "section": "Direct data loading",
    "text": "Direct data loading\nThe method we used is very convenient, but it involves copying the data to the workers. If you want to save some memory, you can load the split data directly to the workers.\nFor this, first, split your data into several files and have all those files (and only those files) in a directory.\nThen, you can run:\nlibrary(multidplyr)\nlibrary(dplyr)\nlibrary(vroom)\n\n# Create the cluster of workers\ncl &lt;- new_cluster(4)\n\n# Create a character vector with the list of data files\nfiles &lt;- dir(\"/path/to/data/directory\", full.names = TRUE)\n\n# Split up the vector amongst the workers\ncluster_assign_partition(cl, files = files)\n\n# Create a data frame called split_iris on each worker\ncluster_send(cl, split_iris &lt;- vroom(files))\n\n# Create the partitioned data frame from the workers' data frames\nsplit_iris &lt;- party_df(cl, \"split_iris\")\nFrom here on, you can work as we did earlier."
  },
  {
    "objectID": "r/hpc_parallel_r.html",
    "href": "r/hpc_parallel_r.html",
    "title": "Running R code in parallel",
    "section": "",
    "text": "The parallel package has been part of the base package group since R version 2.14.0.\nThis means that it is comes with R, however it needs to be loaded in a session before its content can be accessed:\nlibrary(parallel)\nMost parallel approaches in R build on this package.\nAll other packages mentioned in this lesson are external packages and need to be installed with install.packages()."
  },
  {
    "objectID": "r/hpc_parallel_r.html#base-r-parallel-package",
    "href": "r/hpc_parallel_r.html#base-r-parallel-package",
    "title": "Running R code in parallel",
    "section": "",
    "text": "The parallel package has been part of the base package group since R version 2.14.0.\nThis means that it is comes with R, however it needs to be loaded in a session before its content can be accessed:\nlibrary(parallel)\nMost parallel approaches in R build on this package.\nAll other packages mentioned in this lesson are external packages and need to be installed with install.packages()."
  },
  {
    "objectID": "r/hpc_parallel_r.html#parallelly-package",
    "href": "r/hpc_parallel_r.html#parallelly-package",
    "title": "Running R code in parallel",
    "section": "parallelly package",
    "text": "parallelly package\nThe parallelly package—part of the futureverse suite of packages developed by Henrik Bengtsson—adds functionality to the parallel package."
  },
  {
    "objectID": "r/hpc_memory.html",
    "href": "r/hpc_memory.html",
    "title": "Memory management",
    "section": "",
    "text": "Memory can be a limiting factor and releasing it when not needed can be critical to avoid out of memory states. On the other hand, memoisation is an optimization technique which consists of caching the results of heavy computations for re-use.\nMemory and speed are thus linked in a trade-off."
  },
  {
    "objectID": "r/hpc_memory.html#releasing-memory",
    "href": "r/hpc_memory.html#releasing-memory",
    "title": "Memory management",
    "section": "Releasing memory",
    "text": "Releasing memory\nIt is best to avoid creating very large intermediate objects that take space in memory unnecessarily.\n\nOne option is to use nested functions or functions chained with pipes.\nAnother option is to create the intermediate objects within the local environment of a function as they will automatically be deleted as soon as the function has finished running.\n\nLet’s go over a basic example: let’s extract the sepal width variable from the iris dataset (one of the datasets that come packaged with R), take the natural logarithm of the values, and round them to one decimal place.\nFirst, let’s delete all objects inside our environment to make our little test as clean as possible:\n\nrm(list=ls())\nls()\n\ncharacter(0)\n\n\nNow, we could perform our task this way:\n\nsepalwidth &lt;- iris$Sepal.Width\nsepalwidth_ln &lt;- log(sepalwidth)\nround(sepalwidth_ln, 1)\n\n  [1] 1.3 1.1 1.2 1.1 1.3 1.4 1.2 1.2 1.1 1.1 1.3 1.2 1.1 1.1 1.4 1.5 1.4 1.3\n [19] 1.3 1.3 1.2 1.3 1.3 1.2 1.2 1.1 1.2 1.3 1.2 1.2 1.1 1.2 1.4 1.4 1.1 1.2\n [37] 1.3 1.3 1.1 1.2 1.3 0.8 1.2 1.3 1.3 1.1 1.3 1.2 1.3 1.2 1.2 1.2 1.1 0.8\n [55] 1.0 1.0 1.2 0.9 1.1 1.0 0.7 1.1 0.8 1.1 1.1 1.1 1.1 1.0 0.8 0.9 1.2 1.0\n [73] 0.9 1.0 1.1 1.1 1.0 1.1 1.1 1.0 0.9 0.9 1.0 1.0 1.1 1.2 1.1 0.8 1.1 0.9\n [91] 1.0 1.1 1.0 0.8 1.0 1.1 1.1 1.1 0.9 1.0 1.2 1.0 1.1 1.1 1.1 1.1 0.9 1.1\n[109] 0.9 1.3 1.2 1.0 1.1 0.9 1.0 1.2 1.1 1.3 1.0 0.8 1.2 1.0 1.0 1.0 1.2 1.2\n[127] 1.0 1.1 1.0 1.1 1.0 1.3 1.0 1.0 1.0 1.1 1.2 1.1 1.1 1.1 1.1 1.1 1.0 1.2\n[145] 1.2 1.1 0.9 1.1 1.2 1.1\n\n\nBut this creates the unnecessary intermediate variables sepalwidth and sepalwidth_ln which get stored in memory:\n\nls()\n\n[1] \"sepalwidth\"    \"sepalwidth_ln\"\n\n\nFor very large objects, this is not ideal.\nLet’s clear objects in our environment again:\n\nrm(list=ls())\nls()\n\ncharacter(0)\n\n\nA better option is to use nested functions:\n\nround(log(iris$Sepal.Width), 1)\n\n  [1] 1.3 1.1 1.2 1.1 1.3 1.4 1.2 1.2 1.1 1.1 1.3 1.2 1.1 1.1 1.4 1.5 1.4 1.3\n [19] 1.3 1.3 1.2 1.3 1.3 1.2 1.2 1.1 1.2 1.3 1.2 1.2 1.1 1.2 1.4 1.4 1.1 1.2\n [37] 1.3 1.3 1.1 1.2 1.3 0.8 1.2 1.3 1.3 1.1 1.3 1.2 1.3 1.2 1.2 1.2 1.1 0.8\n [55] 1.0 1.0 1.2 0.9 1.1 1.0 0.7 1.1 0.8 1.1 1.1 1.1 1.1 1.0 0.8 0.9 1.2 1.0\n [73] 0.9 1.0 1.1 1.1 1.0 1.1 1.1 1.0 0.9 0.9 1.0 1.0 1.1 1.2 1.1 0.8 1.1 0.9\n [91] 1.0 1.1 1.0 0.8 1.0 1.1 1.1 1.1 0.9 1.0 1.2 1.0 1.1 1.1 1.1 1.1 0.9 1.1\n[109] 0.9 1.3 1.2 1.0 1.1 0.9 1.0 1.2 1.1 1.3 1.0 0.8 1.2 1.0 1.0 1.0 1.2 1.2\n[127] 1.0 1.1 1.0 1.1 1.0 1.3 1.0 1.0 1.0 1.1 1.2 1.1 1.1 1.1 1.1 1.1 1.0 1.2\n[145] 1.2 1.1 0.9 1.1 1.2 1.1\n\n\nAn equivalent option is to chain functions:\n\niris$Sepal.Width |&gt; log() |&gt; round(1)\n\n  [1] 1.3 1.1 1.2 1.1 1.3 1.4 1.2 1.2 1.1 1.1 1.3 1.2 1.1 1.1 1.4 1.5 1.4 1.3\n [19] 1.3 1.3 1.2 1.3 1.3 1.2 1.2 1.1 1.2 1.3 1.2 1.2 1.1 1.2 1.4 1.4 1.1 1.2\n [37] 1.3 1.3 1.1 1.2 1.3 0.8 1.2 1.3 1.3 1.1 1.3 1.2 1.3 1.2 1.2 1.2 1.1 0.8\n [55] 1.0 1.0 1.2 0.9 1.1 1.0 0.7 1.1 0.8 1.1 1.1 1.1 1.1 1.0 0.8 0.9 1.2 1.0\n [73] 0.9 1.0 1.1 1.1 1.0 1.1 1.1 1.0 0.9 0.9 1.0 1.0 1.1 1.2 1.1 0.8 1.1 0.9\n [91] 1.0 1.1 1.0 0.8 1.0 1.1 1.1 1.1 0.9 1.0 1.2 1.0 1.1 1.1 1.1 1.1 0.9 1.1\n[109] 0.9 1.3 1.2 1.0 1.1 0.9 1.0 1.2 1.1 1.3 1.0 0.8 1.2 1.0 1.0 1.0 1.2 1.2\n[127] 1.0 1.1 1.0 1.1 1.0 1.3 1.0 1.0 1.0 1.1 1.2 1.1 1.1 1.1 1.1 1.1 1.0 1.2\n[145] 1.2 1.1 0.9 1.1 1.2 1.1\n\n\nAnother option is to create the intermediate variables in the local environment of a function:\n\nget_sepalwidth &lt;- function(dataset) {\n  sepalwidth &lt;- dataset$Sepal.Width\n  sepalwidth_ln &lt;- log(sepalwidth)\n  round(sepalwidth_ln, 1)\n}\n\nget_sepalwidth(iris)\n\n  [1] 1.3 1.1 1.2 1.1 1.3 1.4 1.2 1.2 1.1 1.1 1.3 1.2 1.1 1.1 1.4 1.5 1.4 1.3\n [19] 1.3 1.3 1.2 1.3 1.3 1.2 1.2 1.1 1.2 1.3 1.2 1.2 1.1 1.2 1.4 1.4 1.1 1.2\n [37] 1.3 1.3 1.1 1.2 1.3 0.8 1.2 1.3 1.3 1.1 1.3 1.2 1.3 1.2 1.2 1.2 1.1 0.8\n [55] 1.0 1.0 1.2 0.9 1.1 1.0 0.7 1.1 0.8 1.1 1.1 1.1 1.1 1.0 0.8 0.9 1.2 1.0\n [73] 0.9 1.0 1.1 1.1 1.0 1.1 1.1 1.0 0.9 0.9 1.0 1.0 1.1 1.2 1.1 0.8 1.1 0.9\n [91] 1.0 1.1 1.0 0.8 1.0 1.1 1.1 1.1 0.9 1.0 1.2 1.0 1.1 1.1 1.1 1.1 0.9 1.1\n[109] 0.9 1.3 1.2 1.0 1.1 0.9 1.0 1.2 1.1 1.3 1.0 0.8 1.2 1.0 1.0 1.0 1.2 1.2\n[127] 1.0 1.1 1.0 1.1 1.0 1.3 1.0 1.0 1.0 1.1 1.2 1.1 1.1 1.1 1.1 1.1 1.0 1.2\n[145] 1.2 1.1 0.9 1.1 1.2 1.1\n\n\nNone of these options left intermediate variables in our environment:\n\nls()\n\n[1] \"get_sepalwidth\"\n\n\nNote that in the case of a very large function, it might still be beneficial to run rm() inside the function to clear the memory for other processes coming next within that function. But this is a pretty rare case.\nIf you really have to create large intermediate objects in the global environment, make sure to delete them as soon as you don’t need them anymore (e.g. rm(sepalwidth, sepalwidth_ln)).\n\nrm() deletes the names of variables (the pointers to objects in memory). But as soon as all the pointers to an object in memory are deleted, the garbage collector clears its value and releases the memory it used."
  },
  {
    "objectID": "r/hpc_memory.html#caching",
    "href": "r/hpc_memory.html#caching",
    "title": "Memory management",
    "section": "Caching",
    "text": "Caching\nMemoisation is a technique by which some results are cached to avoid re-calculating them. This is convenient in a variety of settings (e.g. to reduce calls to an API, to avoid repeating heavy computations). In particular, it improves the efficiency of recursive function calls dramatically.\nLet’s consider the calculation of the Fibonacci numbers as an example. Those numbers form a sequence starting with 0 and 11, after which each number is the sum of the previous two.1 Alternative versions have the sequence start with 1, 1 or with 1, 2.\nHere is a function that would return the nth Fibonacci number2:2 There are more efficient ways to calculate the Fibonacci numbers, but this inefficient function is a great example to show the advantage of memoisation.\nfib &lt;- function(n) {\n  if(n == 0) {\n    return(0)\n  } else if(n == 1) {\n    return(1)\n  } else {\n    Recall(n - 1) + Recall(n - 2)\n  }\n}\nIt can be written more tersely as:\n\nfib &lt;- function(n) {\n  if(n == 0) return(0)\n  if(n == 1) return(1)\n  Recall(n - 1) + Recall(n - 2)\n}\n\n\nRecall() is a placeholder for the name of the recursive function. We could have used fib() instead, but Recall() is more robust as it allows for function renaming.\n\nMemoisation is very useful here because, for each Fibonacci number, we need to calculate the two preceding Fibonacci numbers and to calculate each of those we need to calculate the two Fibonacci numbers preceding that one and to calculate… etc. That is a large number of calculations, but, thanks to caching, we don’t have to calculate any one of them more than once.\nThe packages R.cache and memoise both allow for memoisation with an incredibly simple syntax.\nApplying the latter to our function gives us:\n\nlibrary(memoise)\n\nfibmem &lt;- memoise(\n  function(n) {\n    if(n == 0) return(0)\n    if(n == 1) return(1)\n    Recall(n - 1) + Recall(n - 2)\n  }\n)\n\nWe can do some benchmarking to see the speedup for the 30th Fibonacci number:\n\nlibrary(bench)\n\nn &lt;- 30\nmark(fib(n), fibmem(n))\n\nWarning: Some expressions had a GC in every iteration; so filtering is\ndisabled.\n\n\n# A tibble: 2 × 6\n  expression      min   median `itr/sec` mem_alloc `gc/sec`\n  &lt;bch:expr&gt; &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt;\n1 fib(n)        1.55s    1.55s     0.643      33KB     25.1\n2 fibmem(n)    37.7µs  39.52µs 23598.       68.4KB     18.9\n\n\nThe speedup is over 35,000!"
  },
  {
    "objectID": "r/hpc_foreach.html",
    "href": "r/hpc_foreach.html",
    "title": "foreach and doFuture",
    "section": "",
    "text": "One of the options to parallelize code with the future package is to use foreach with doFuture. In this section, we will go over an example with the random forest algorithm."
  },
  {
    "objectID": "r/hpc_foreach.html#our-example-code-random-forest",
    "href": "r/hpc_foreach.html#our-example-code-random-forest",
    "title": "foreach and doFuture",
    "section": "Our example code: random forest",
    "text": "Our example code: random forest\n\nOn the iris dataset\nRandom forest is a commonly used ensemble learning technique for classification and regression. The idea is to combine the results from many decision trees on bootstrap samples of the dataset to improve the predictive accuracy and control over-fitting. The algorithm used was developed by Tin Kam Ho, then improved by Leo Breiman and Adele Cutler. An implementation in R is provided by the randomForest() function from the randomForest package. Let’s use it to classify the iris dataset that comes packaged with R.\nFirst, let’s have a look at the dataset:\n\n# Structure of the dataset\nstr(iris)\n\n'data.frame':   150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\n# Dimensions of the dataset\ndim(iris)\n\n[1] 150   5\n\n# First 6 data points\nhead(iris)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\n# The 3 species (3 levels of the factor)\nlevels(iris$Species)\n\n[1] \"setosa\"     \"versicolor\" \"virginica\" \n\n\nThe goal is to create a random forest model (let’s call it rf) that can classify an iris flower in one of the 3 species based on the 4 measurements of its sepals and petals.\n\nlibrary(randomForest)\n\nset.seed(123)\nrf &lt;- randomForest(Species ~ ., data=iris)\n\n\nOur response variable (Species) is a factor, so classification is assumed.\nThe . on the right side of the formula represents all other variables (so we are using all variables, except for the response variable Species of course, as feature variables).\n\n\nrf\n\n\nCall:\n randomForest(formula = Species ~ ., data = iris) \n               Type of random forest: classification\n                     Number of trees: 500\nNo. of variables tried at each split: 2\n\n        OOB estimate of  error rate: 4.67%\nConfusion matrix:\n           setosa versicolor virginica class.error\nsetosa         50          0         0        0.00\nversicolor      0         47         3        0.06\nvirginica       0          4        46        0.08\n\n\nAs can be seen by the confusion matrix, our model performs well.\nWe can use it on new data to make predictions. Let’s try with some made-up data:\n\nnew_data &lt;- data.frame(\n  Sepal.Length = c(5.3, 4.6, 6.5),\n  Sepal.Width = c(3.1, 3.9, 2.5),\n  Petal.Length = c(1.5, 1.5, 5.0),\n  Petal.Width = c(0.2, 0.1, 2.1)\n)\n\nnew_data\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width\n1          5.3         3.1          1.5         0.2\n2          4.6         3.9          1.5         0.1\n3          6.5         2.5          5.0         2.1\n\npredict(rf, new_data)\n\n        1         2         3 \n   setosa    setosa virginica \nLevels: setosa versicolor virginica\n\n\n\n\nLet’s make it big\nNow, the iris dataset only has 150 observations and we used the default number of trees (500) of the randomForest() function, so things ran fast. Often, random forests are run on large datasets. Let’s artificially increase the iris dataset and use more trees to create a situation in which parallelization would make sense.\nOne easy way is to replicate each row 100 times (and we can then delete the row names that get created by this operation):\n\nbig_iris &lt;- iris[rep(seq_len(nrow(iris)), each = 1e2), ]\nrownames(big_iris) &lt;- NULL\n\n\ndim(big_iris)\n\n[1] 15000     5\n\n\nAnd then we can run randomForest() on this dataset and 2000 trees."
  },
  {
    "objectID": "r/hpc_foreach.html#hidden-parallelism-check",
    "href": "r/hpc_foreach.html#hidden-parallelism-check",
    "title": "foreach and doFuture",
    "section": "Hidden parallelism check",
    "text": "Hidden parallelism check\nBefore parallelizing your code, remember to check whether the package you are using is already doing any parallelization under the hood (after all, maybe the randomForest package runs things in parallel. We don’t know).\nOne way to do this is to test the package on your local machine and, while some sample code is running, to open htop and see how many cores are used.\nWhy do this on your local machine? because on the cluster, if you launch htop while your batch job is running, you will be looking at processes running on the login node while your code is running on compute node(s). So this will not help you. You could salloc on the/one of the compute node(s) running your job and run htop there, but in production clusters, compute nodes are large and you will see all the processes from all the other users using that compute node. So this test is just easier done locally.\nOn my machine I ran:\nlibrary(randomForest)\n\nbig_iris &lt;- iris[rep(seq_len(nrow(iris)), each = 1e2), ]\nrownames(big_iris) &lt;- NULL\n\nset.seed(123)\nrf &lt;- randomForest(Species ~ ., data=big_iris, ntree=2000)\nAnd I could confirm that the function does not run in parallel.\nSo let’s parallelize this code."
  },
  {
    "objectID": "r/hpc_foreach.html#the-foreach-package",
    "href": "r/hpc_foreach.html#the-foreach-package",
    "title": "foreach and doFuture",
    "section": "The foreach package",
    "text": "The foreach package\nThe foreach package provides a construct for repeated executions, i.e. it can replace for loops, while loops, repeat loops, and functional programming code written with the *apply functions or the purrr package. The foreach vignette gives many examples."
  },
  {
    "objectID": "r/hpc_foreach.html#the-dofuture-package",
    "href": "r/hpc_foreach.html#the-dofuture-package",
    "title": "foreach and doFuture",
    "section": "The doFuture package",
    "text": "The doFuture package\nThe most useful part of foreach is that it allows for easily parallelization with countless backends: doFuture, doMC, doMPI, doParallel, doRedis, doRNG, doSNOW, and doAzureParallel.\nThe doFuture package is the most modern of these backends. It allows to evaluate foreach expressions across the evaluation strategies of the future package very easily. All you have to do is to register it as a backend, declare the evaluation strategy of futures of your choice, make sure to generate parallel-safe random numbers for reproducibility (if your code uses randomness), and replace %do% with %dofuture%."
  },
  {
    "objectID": "r/hpc_foreach.html#benchmarks",
    "href": "r/hpc_foreach.html#benchmarks",
    "title": "foreach and doFuture",
    "section": "Benchmarks",
    "text": "Benchmarks\nWe will run and benchmark all versions of our code by submitting batch jobs to Slurm.\n\nInitial code\nFirst, let’s benchmark the initial (non parallel, not using foreach) code. We need to create an R script. Let’s call it reference.R (I will use Emacs, but you can use the nano text editor with the command nano to write the script if you want):\n\n\nreference.R\n\nlibrary(randomForest)\nlibrary(bench)\n\nbig_iris &lt;- iris[rep(seq_len(nrow(iris)), each = 1e2), ]\nrownames(big_iris) &lt;- NULL\n\ncat(\"\\nBenchmarking results:\\n\\n\")\n\nset.seed(123)\nmark(rf &lt;- randomForest(Species ~ ., data=big_iris, ntree=2000))\n\nThen we need to create a Bash script for Slurm. Let’s call it reference.sh:\n\n\nreference.sh\n\n#!/bin/bash\n#SBATCH --time=5\n#SBATCH --mem-per-cpu=7500M\n\nRscript reference.R\n\n\nYou can see the full list of sbatch options here.\n\nAnd now we submit the job with:\nsbatch reference.sh\nYou can monitor your job with sq. The result will be written to a file called slurm-xx.out with xx being the number of the job that just ran. To see the result, we can simply print the content of that file to screen with cat (you can run ls to see the list of files in the current directory). Make sure that your job has finished running before printing the result (otherwise you might get a partial output which can be confusing).\ncat slurm-xx.out    # Replace xx by the job number\nrandomForest 4.7-1.1\nType rfNews() to see new features/changes/bug fixes.\n\nBenchmarking results:\n\n# A tibble: 1 × 13\n  expression      min median `itr/sec` mem_alloc `gc/sec` n_itr  n_gc total_time\n  &lt;bch:expr&gt;    &lt;bch&gt; &lt;bch:&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;   &lt;bch:tm&gt;\n1 rf &lt;- random… 6.33s  6.33s     0.158        NA    0.474     1     3      6.33s\n# ℹ 4 more variables: result &lt;list&gt;, memory &lt;list&gt;, time &lt;list&gt;, gc &lt;list&gt;\nWarning message:\nSome expressions had a GC in every iteration; so filtering is disabled.\n\n\nforeach expression\nNow, let’s try the foreach version:\n\n\nforeach.R\n\nlibrary(foreach)\nlibrary(randomForest)\nlibrary(bench)\n\nbig_iris &lt;- iris[rep(seq_len(nrow(iris)), each = 1e2), ]\nrownames(big_iris) &lt;- NULL\n\ncat(\"\\nBenchmarking results:\\n\\n\")\n\nset.seed(123)\nmark(\n  rf &lt;- foreach(ntree = rep(250, 8), .combine = combine) %do%\n    randomForest(Species ~ ., data=big_iris, ntree=ntree)\n)\n\n\n\nforeach.sh\n\n#!/bin/bash\n#SBATCH --time=5\n#SBATCH --mem-per-cpu=7500M\n\nRscript foreach.R\n\nsbatch foreach.sh\nrandomForest 4.7-1.1\nType rfNews() to see new features/changes/bug fixes.\n\nBenchmarking results:\n\n# A tibble: 1 × 13\n  expression      min median `itr/sec` mem_alloc `gc/sec` n_itr  n_gc total_time\n  &lt;bch:expr&gt;    &lt;bch&gt; &lt;bch:&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;   &lt;bch:tm&gt;\n1 rf &lt;- foreac… 7.04s  7.04s     0.142        NA     4.55     1    32      7.04s\n# ℹ 4 more variables: result &lt;list&gt;, memory &lt;list&gt;, time &lt;list&gt;, gc &lt;list&gt;\nWarning message:\nSome expressions had a GC in every iteration; so filtering is disabled.\n\nThe foreach expression is slower than the standard expression (it is always the case: foreach slows things down before this overhead gets offset by parallelization).\n\n\n\nPlan sequential\nYou might wonder why the sequential evaluation strategy exists (i.e. why go through all the trouble of writing your code with foreach and doFuture to then run it without parallelism?).\nThere are many reasons:\n\nIt can be very useful for debugging.\nIt makes it easy to switch the futures execution strategy back and forth for different sections of the code (maybe you don’t want to run everything in parallel).\nIt allows other people to run the same code on their different hardware without changing it (if they don’t have the resources to run things in parallel, they only have to change the execution strategy).\n\nTo turn the code into a parallelizable version with doFuture, we replace %do% with %dofuture%.\nHere, we also need to use the option .options.future = list(seed = TRUE): whenever your parallel code rely on a random process, it isn’t enough to use set.seed() to ensure reproducibility, you also need to generate parallel-safe random numbers. In random forest, each tree is trained on a random subset of the data and random variables are selected for splitting at each node. The option .options.future = list(seed = TRUE) pregenerates the random seeds using L’Ecuyer-CMRG RNG streams1.1 L’Ecuyer, P. (1999). Good parameters and implementations for combined multiple recursive random number generators. Operations Research, 47, 159–164.\nThis is the parallelizable foreach code, but run sequentially:\n\n\nsequential.R\n\nlibrary(doFuture)    # Also loads foreach and future\nlibrary(randomForest)\nlibrary(bench)\n\nregisterDoFuture()   # Set the parallel backend\nplan(sequential)     # Set the evaluation strategy\n\nbig_iris &lt;- iris[rep(seq_len(nrow(iris)), each = 1e2), ]\nrownames(big_iris) &lt;- NULL\n\ncat(\"\\nBenchmarking results:\\n\\n\")\n\nset.seed(123)\nmark(\n  rf &lt;- foreach(\n    ntree = rep(250, 8),\n    .options.future = list(seed = TRUE),\n    .combine = combine\n  ) %dofuture%\n    randomForest(Species ~ ., data=big_iris, ntree=ntree)\n)\n\n\n\nsequential.sh\n\n#!/bin/bash\n#SBATCH --time=5\n#SBATCH --mem-per-cpu=7500M\n\nRscript sequential.R\n\nsbatch sequential.sh\nLoading required package: foreach\nLoading required package: future\nrandomForest 4.7-1.1\nType rfNews() to see new features/changes/bug fixes.\n\nBenchmarking results:\n\n# A tibble: 1 × 13\n  expression      min median `itr/sec` mem_alloc `gc/sec` n_itr  n_gc total_time\n  &lt;bch:expr&gt;    &lt;bch&gt; &lt;bch:&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;   &lt;bch:tm&gt;\n1 rf &lt;- foreac… 8.39s  8.39s     0.119        NA     3.81     1    32      8.39s\n# ℹ 4 more variables: result &lt;list&gt;, memory &lt;list&gt;, time &lt;list&gt;, gc &lt;list&gt;\nWarning message:\nSome expressions had a GC in every iteration; so filtering is disabled.\n\nEach time we add unnecessary complexity in the code, things run a little slower.\n\n\n\nMulti-processing in shared memory\nNow, it is time to parallelize. First, we will use multiple cores on a single node (shared-memory parallelism).\n\nNumber of cores\nThe future package provides the availableCores() function to detect the number of available cores. We will run it as part of our script as a check on our available hardware.\nThe cluster for this course is made of 20 nodes with 4 CPUs each. We want to test shared memory parallelism, so our job needs to stay within one node. We can thus ask for a maximum of 4 CPUs and we want to ensure that we aren’t getting them on different nodes. Let’s go with that maximum of 4 cores.\n\n\nMultisession\nShared memory multi-processing can be run with plan(multisession) that will spawn new R sessions in the background to evaluate futures.\n\n\nmultisession.R\n\nlibrary(doFuture)\nlibrary(randomForest)\nlibrary(bench)\n\n# Check number of cores:\ncat(\"\\nWe have\", availableCores(), \"cores.\\n\\n\")\n\nregisterDoFuture()   # Set the parallel backend\nplan(multisession)   # Set the evaluation strategy\n\nbig_iris &lt;- iris[rep(seq_len(nrow(iris)), each = 1e2), ]\nrownames(big_iris) &lt;- NULL\n\ncat(\"\\nBenchmarking results:\\n\\n\")\n\nset.seed(123)\nmark(\n  rf &lt;- foreach(\n    ntree = rep(250, 8),\n    .options.future = list(seed = TRUE),\n    .combine = combine\n  ) %dofuture%\n    randomForest(Species ~ ., data=big_iris, ntree=ntree)\n)\n\n\n\nmultisession.sh\n\n#!/bin/bash\n#SBATCH --time=5\n#SBATCH --mem-per-cpu=7500M\n#SBATCH --cpus-per-task=4\n\nRscript multisession.R\n\nsbatch multisession.sh\nLoading required package: foreach\nLoading required package: future\nrandomForest 4.7-1.1\nType rfNews() to see new features/changes/bug fixes.\n\nWe have 4 cores.\n\nBenchmarking results:\n\n# A tibble: 1 × 13\n  expression      min median `itr/sec` mem_alloc `gc/sec` n_itr  n_gc total_time\n  &lt;bch:expr&gt;    &lt;bch&gt; &lt;bch:&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;   &lt;bch:tm&gt;\n1 rf &lt;- foreac… 2.72s  2.72s     0.368        NA     2.21     1     6      2.72s\n# ℹ 4 more variables: result &lt;list&gt;, memory &lt;list&gt;, time &lt;list&gt;, gc &lt;list&gt;\nWarning message:\nSome expressions had a GC in every iteration; so filtering is disabled.\nSpeedup: 3.1.\n\nNot too bad, considering that the ideal speedup, without any overhead, would be 4.\n\n\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=4\ncould be used instead of:\n#SBATCH --cpus-per-task=4\nWhat matters is to have 4 cores running on the same node to be in a shared memory parallelism scenario.\n\n\n\nMulticore\nShared memory multi-processing can also be run with plan(multicore) (except on Windows) that will fork the current R process to evaluate futures.\n\n\nmulticore.R\n\nlibrary(doFuture)\nlibrary(randomForest)\nlibrary(bench)\n\n# Check number of cores:\ncat(\"\\nWe have\", availableCores(), \"cores.\\n\\n\")\n\nregisterDoFuture()   # Set the parallel backend\nplan(multicore)      # Set the evaluation strategy\n\nbig_iris &lt;- iris[rep(seq_len(nrow(iris)), each = 1e2), ]\nrownames(big_iris) &lt;- NULL\n\ncat(\"\\nBenchmarking results:\\n\\n\")\n\nset.seed(123)\nmark(\n  rf &lt;- foreach(\n    ntree = rep(250, 8),\n    .options.future = list(seed = TRUE),\n    .combine = combine\n  ) %dofuture%\n    randomForest(Species ~ ., data=big_iris, ntree=ntree)\n)\n\n\n\nmulticore.sh\n\n#!/bin/bash\n#SBATCH --time=5\n#SBATCH --mem-per-cpu=7500M\n#SBATCH --cpus-per-task=4\n\nRscript multicore.R\n\nsbatch multicore.sh\nLoading required package: foreach\nLoading required package: future\nrandomForest 4.7-1.1\nType rfNews() to see new features/changes/bug fixes.\n\nWe have 4 cores.\n\nBenchmarking results:\n\n# A tibble: 1 × 13\n  expression      min median `itr/sec` mem_alloc `gc/sec` n_itr  n_gc total_time\n  &lt;bch:expr&gt;    &lt;bch&gt; &lt;bch:&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;   &lt;bch:tm&gt;\n1 rf &lt;- foreac… 3.15s  3.15s     0.318        NA     13.7     1    43      3.15s\n# ℹ 4 more variables: result &lt;list&gt;, memory &lt;list&gt;, time &lt;list&gt;, gc &lt;list&gt;\nWarning message:\nSome expressions had a GC in every iteration; so filtering is disabled.\nSpeedup: 2.7.\n\nWhile in theory we should get a similar speedup, we are getting a lower one here.\n\n\n\n\nMulti-processing in distributed memory\nLet’s run our distributed parallel code using 8 cores across 2 nodes.\nWe need to create a cluster of workers. We do this by creating a character vector with the names of the nodes our tasks are running on and passing it to the makeCluster() function from the parallel package (included in R):\n# Create a character vector with the nodes names\nhosts &lt;- system(\"srun hostname -s\", intern = T)\n\n# Create the cluster of workers\ncl &lt;- parallel::makeCluster(hosts)\nWe can verify that we did get 8 tasks by accessing the SLURM_NTASKS environment variable from within R:\nas.numeric(Sys.getenv(\"SLURM_NTASKS\"))\nHere is the R script:\n\n\ndistributed.R\n\nlibrary(doFuture)\nlibrary(randomForest)\nlibrary(bench)\n\n# Check number of tasks:\ncat(\"\\nWe have\", as.numeric(Sys.getenv(\"SLURM_NTASKS\")), \"tasks.\\n\\n\")\n\n# Create a character vector with the nodes names\nhosts &lt;- system(\"srun hostname -s\", intern = T)\n\n# Look at the location of our tasks:\ncat(\"\\nOur tasks are running on the following nodes: \", hosts)\n\n# Create the cluster of workers\ncl &lt;- parallel::makeCluster(hosts)\n\nregisterDoFuture()           # Set the parallel backend\nplan(cluster, workers = cl)  # Set the evaluation strategy\n\nbig_iris &lt;- iris[rep(seq_len(nrow(iris)), each = 1e2), ]\nrownames(big_iris) &lt;- NULL\n\ncat(\"\\nBenchmarking results:\\n\\n\")\n\nset.seed(123)\nmark(\n  rf &lt;- foreach(\n    ntree = rep(250, 8),\n    .options.future = list(seed = TRUE),\n    .combine = combine\n  ) %dofuture%\n    randomForest(Species ~ ., data=big_iris, ntree=ntree)\n)\n\n\nThe cluster of workers can be stopped with:\nparallel::stopCluster(cl)\nHere, this is not necessary since our job stops running as soon as the execution is complete, but in other systems, this will prevent you from monopolizing hardware or paying unnecessarily.\n\nAnd now we need to ask Slurm for 8 tasks on 2 nodes:\n\n\ndistributed.sh\n\n#!/bin/bash\n#SBATCH --time=5\n#SBATCH --mem-per-cpu=7500M\n#SBATCH --ntasks-per-node=4\n#SBATCH --nodes=2\n\nRscript distributed.R\n\nsbatch distributed.sh\nLoading required package: foreach\nLoading required package: future\nrandomForest 4.7-1.1\nType rfNews() to see new features/changes/bug fixes.\n\nWe have 8 tasks.\n\nOur tasks are running on the following nodes: \"node1\" \"node1\" \"node1\" \"node1\" \"node2\" \"node2\" \"node2\" \"node2\"\n\nBenchmarking results:\n\n# A tibble: 1 × 13\n  expression      min median `itr/sec` mem_alloc `gc/sec` n_itr  n_gc total_time\n  &lt;bch:expr&gt;    &lt;bch&gt; &lt;bch:&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;   &lt;bch:tm&gt;\n1 rf &lt;- foreac…  1.6s   1.6s     0.624        NA     3.12     1     5       1.6s\n# ℹ 4 more variables: result &lt;list&gt;, memory &lt;list&gt;, time &lt;list&gt;, gc &lt;list&gt;\nWarning message:\nSome expressions had a GC in every iteration; so filtering is disabled.\nSpeedup: 5.2.\n\nThe overhead is larger in distributed parallelism due to message passing between nodes. We are further from the ideal speedup of 8, but we still got a speedup larger than what we could have obtained with shared-memory parallelism.\n\n\n#SBATCH --ntasks=8\ncould be used instead of:\n#SBATCH --ntasks-per-node=4\n#SBATCH --nodes=2\nHowever the latter is slightly better because it allows us to use 2 full nodes instead of having tasks running on any number of nodes. However, it also means that we might have to wait longer for our job to run as it is more restrictive."
  },
  {
    "objectID": "r/hpc_clusters.html",
    "href": "r/hpc_clusters.html",
    "title": "R on HPC clusters",
    "section": "",
    "text": "In this section, you will learn how to use R on an Alliance cluster: load modules, install packages, and run jobs."
  },
  {
    "objectID": "r/hpc_clusters.html#modules",
    "href": "r/hpc_clusters.html#modules",
    "title": "R on HPC clusters",
    "section": "Modules",
    "text": "Modules\nOn the Alliance clusters, a number of utilities are available right away (e.g. Bash utilities, git, tmux, various text editors). Before you can use more specialized software however, you have to load the module corresponding to the version of your choice as well as any potential dependencies.\n\nThe cluster setup for this course has everything loaded, so this step is not necessary today, but it is very important to learn it.\n\n\nR\nFirst, of course, we need an R module.\nTo see which versions of R are available on a cluster, run:\nmodule spider r\nTo see the dependencies of a particular version (e.g. r/4.3.1), run:\nmodule spider r/4.3.1\nThis shows us that we need StdEnv/2020 to load r/4.3.1.\n\n\nC compiler\nIf you plan on installing any R package, you will also need a C compiler.\nIn theory, one could use the proprietary Intel compiler which is loaded by default on the Alliance clusters, but it is recommended to replace it with the GCC compiler (R packages can be compiled by any C compiler—also including Clang and LLVM—but the default GCC compiler is the best way to avoid headaches).\n\n\nYour turn:\n\n\nHow can you check which gcc versions are available on our training cluster?\nWhat are the dependencies required by gcc/11.3.0?\n\n\n\n\nLoading the modules\nOnce you know which modules you need, you can load them. The order is important: the dependencies (here StdEnv/2020) must be listed before the modules which depend on them.\nmodule load StdEnv/2020 gcc/11.3.0 r/4.3.1"
  },
  {
    "objectID": "r/hpc_clusters.html#installing-r-packages",
    "href": "r/hpc_clusters.html#installing-r-packages",
    "title": "R on HPC clusters",
    "section": "Installing R packages",
    "text": "Installing R packages\n\nFor this course, all packages have already been installed in a communal library. You thus don’t have to install anything.\n\nTo install a package, launch the interactive R console with:\nR\nIn the R console, run:\ninstall.packages(\"&lt;package_name&gt;\", repos=\"&lt;url-cran-mirror&gt;\")\n\nrepos argument: chose a CRAN mirror close to the location of your cluster or use https://cloud.r-project.org/.\n\n\nThe first time you install a package, R will ask you whether you want to create a personal library in your home directory. Answer yes to both questions. Your packages will now install under ~/.\n\n\nSome packages require additional modules to be loaded before they can be installed. Other packages need additional R packages as dependencies. In either case, you will get explicit error messages. Adding the argument dependencies = T helps in the second case, but you will still have to add packages manually from time to time.\n\nTo leave the R console, press &lt;Ctrl+D&gt;."
  },
  {
    "objectID": "r/hpc_clusters.html#running-r-jobs",
    "href": "r/hpc_clusters.html#running-r-jobs",
    "title": "R on HPC clusters",
    "section": "Running R jobs",
    "text": "Running R jobs\nThere are two types of jobs that can be launched on an Alliance cluster: interactive jobs and batch jobs. We will practice both and discuss their respective merits and when to use which.\nFor this course, I purposefully built a rather small cluster (10 nodes with 4 CPUs and 15GB each) to give a tangible illustration of the constraints of resource sharing.\n\nInteractive jobs\n\nWhile it is fine to run R on the login node when you install packages, you must start a SLURM job before any heavy computation.\n\nTo run R interactively, you should launch an salloc session.\n\nExample to launch an interactive job on a single CPU with 3500MB of memory for 2h:\n\nsalloc --time=2:00:00 --mem-per-cpu=3500M\nThis takes you to a compute node where you can now launch R to run computations:\nR\n\nThis however leads to the same inefficient use of resources as happens when running an RStudio server: all the resources that you requested are blocked for you while your job is running, whether you are making use of them (running heavy computations) or not (thinking, typing code, running computations that use only a fraction of the requested resources).\nInteractive jobs are thus best kept to develop code.\n\n\n\nScripts\nTo run an R script called &lt;your_script&gt;.R, you first need to write a job script:\n\nExample to run a script on 4 CPUs with 3500MB per CPU for 15min:\n\n\n&lt;your_job&gt;.sh\n\n#!/bin/bash\n#SBATCH --account=def-&lt;your_account&gt;\n#SBATCH --time=15\n#SBATCH --mem-per-cpu=3500M\n#SBATCH --cpus-per-task=4\n#SBATCH --job-name=\"&lt;your_job&gt;\"\nmodule load StdEnv/2020 gcc/11.3.0 r/4.3.1\nRscript &lt;your_script&gt;.R\n\n\n\nNote that R scripts are run with the command Rscript (not R).\n\nThen launch your job with:\nsbatch &lt;your_job&gt;.sh\nYou can monitor your job with sq (an alias for squeue -u $USER $@).\n\nBatch jobs are the best approach to run parallel computations, particularly when they require a lot of hardware.\nIt will save you lots of waiting time (Alliance clusters) or money (commercial clusters)."
  },
  {
    "objectID": "python/top_intro.html",
    "href": "python/top_intro.html",
    "title": "Getting started with Python",
    "section": "",
    "text": "This introductory course in Python does not assume any prior programming experience.\n\n Start course ➤"
  },
  {
    "objectID": "python/resources.html",
    "href": "python/resources.html",
    "title": "Resources",
    "section": "",
    "text": "Python page\n\nThere are many books on Python, several of which can be accessed online for free, either directly, or through your university."
  },
  {
    "objectID": "python/resources.html#alliance-wiki",
    "href": "python/resources.html#alliance-wiki",
    "title": "Resources",
    "section": "",
    "text": "Python page\n\nThere are many books on Python, several of which can be accessed online for free, either directly, or through your university."
  },
  {
    "objectID": "python/resources.html#books-by-oreilly",
    "href": "python/resources.html#books-by-oreilly",
    "title": "Resources",
    "section": "Books by O’Reilly",
    "text": "Books by O’Reilly\n\nThink Python, 2nd Edition, by Allen B. Downey\nPython Pocket Reference, 5th Edition, by Mark Lutz\nIntroducing Python, by Bill Lubanovic\nPython in a Nutshell, 3rd Edition, by Alex Martelli, Anna Ravenscroft, and Steve Holden\nLearning Python, 5th Edition, by Mark Lutz\nPython Cookbook, 3rd Edition, by David Beazley and Brian K. Jones\nThe Hitchhiker’s Guide to Python, by Kenneth Reitz and Tanya Schlusser\nFluent Python, by Luciano Ramalho\nHigh Performance Python, by Micha Gorelick and Ian Ozsvald\nWeb Scraping with Python, by Ryan Mitchell\nPython Data Science Handbook, by Jake VanderPlas\nPython for Data Analysis, by Wes McKinney\nFoundations for Analytics with Python, by Clinton W. Brownley\nData Wrangling with Python, by Jacquiline Kazil and Katharine Jarmul\nData Visualization with Python and Javascript, by Kyran Dale\nNatural Language Processing with Python, by Steven Bird and Ewan Klein\nThoughtful Machine Learning with Python, by Matthew Kirk\nPython for Finance, by Yves Hilpisch"
  },
  {
    "objectID": "python/resources.html#books-by-no-starch-press",
    "href": "python/resources.html#books-by-no-starch-press",
    "title": "Resources",
    "section": "Books by No Starch Press",
    "text": "Books by No Starch Press\n\nAutomate the Boring Stuff with Python, by Al Sweigart\nPython Crash Course, by Eric Matthews\nPython Playground, by Mahesh Venkitachalam\nDoing Math with Python, by Amit Saha\nInvent Your Own Computer Games with Python, by Al Sweigart"
  },
  {
    "objectID": "python/resources.html#other-books",
    "href": "python/resources.html#other-books",
    "title": "Resources",
    "section": "Other books",
    "text": "Other books\n\nPython Machine Learning, by Sebastian Raschka\nPractical Programming: An Introduction to Computer Science Using Python 3, by Paul Gries, Jennifer Campbell, and Jason Montojo\nPython for Dummies, by Stef Maruch and Aahz Maruch\nPython Essential Reference, 4th Edition, by David Beazley\nHead First Python, by Paul Barry\nPython for Data Science for Dummies, by John Paul Mueller and Luca Massaron\nBeginning Programming with Python for Dummies, by John Paul Mueller\nPython for Everybody, by Charles Severance"
  },
  {
    "objectID": "python/packages.html",
    "href": "python/packages.html",
    "title": "Modules, packages, and libraries",
    "section": "",
    "text": "“Modules” are Python files containing reusable code (e.g. functions, constants, utilities).\n“Packages” are collections of modules.\n“Libraries”, technically, are collections of packages, although “packages” and “libraries” are often used loosely and interchangeably in Python."
  },
  {
    "objectID": "python/packages.html#definitions",
    "href": "python/packages.html#definitions",
    "title": "Modules, packages, and libraries",
    "section": "",
    "text": "“Modules” are Python files containing reusable code (e.g. functions, constants, utilities).\n“Packages” are collections of modules.\n“Libraries”, technically, are collections of packages, although “packages” and “libraries” are often used loosely and interchangeably in Python."
  },
  {
    "objectID": "python/packages.html#installing-external-packages",
    "href": "python/packages.html#installing-external-packages",
    "title": "Modules, packages, and libraries",
    "section": "Installing external packages",
    "text": "Installing external packages\nYou can install external packages containing additional functions, constants, and utilities to extend the capabilities of Python.\nThe Python Package Index is a public repository of open source packages contributed by users.\nInstallation of packages can be done via pip:\npip install --no-index --upgrade pip\npython -m pip install &lt;package&gt;\nOn your local machine, particularly if you are on Windows and want to install a complex software stack, conda can makes things easy by installing from the Anaconda Distribution. This is however never what you want to do when using the Alliance clusters.\nOn the clusters, you always want to:\n\ncreate a virtual environment,\ninstall packages in it with pip."
  },
  {
    "objectID": "python/packages.html#virtual-environments",
    "href": "python/packages.html#virtual-environments",
    "title": "Modules, packages, and libraries",
    "section": "Virtual environments",
    "text": "Virtual environments\nInstead of installing packages system wide or for your user, you can create a semi-isolated Python environment in which you install the packages needed for a particular project. This makes reproducibility and collaboration easier. It also helps handle dependency conflicts."
  },
  {
    "objectID": "python/packages.html#installing-packages-on-the-clusters",
    "href": "python/packages.html#installing-packages-on-the-clusters",
    "title": "Modules, packages, and libraries",
    "section": "Installing packages on the clusters",
    "text": "Installing packages on the clusters\nOn the Alliance clusters, you should install packages inside a virtual environment."
  },
  {
    "objectID": "python/index.html",
    "href": "python/index.html",
    "title": "Python",
    "section": "",
    "text": "Getting started with Python\nAn introductory course to programming in Python\n\n\n\n \n\n\n\n\n \n\n\n \n\n\n\n\n\nWorkshops\nWorkshops on various Python topics"
  },
  {
    "objectID": "python/control_flow.html",
    "href": "python/control_flow.html",
    "title": "Control flow",
    "section": "",
    "text": "Control flow statements alter the linear execution of code, allowing for one or another section of code to be executed, or for one section of code to be executed multiple times."
  },
  {
    "objectID": "python/control_flow.html#conditionals",
    "href": "python/control_flow.html#conditionals",
    "title": "Control flow",
    "section": "Conditionals",
    "text": "Conditionals\nConditionals dictate the flow of information based on predicates (statements that return True or False).\n\nExample predicates:\n\n4 &lt; 3\n2 == 4\n2 != 4\n2 in range(5)\n2 not in range(5)\n3 &lt;= 4 and 4 &gt; 5\n3 &lt;= 4 and 4 &gt; 5 and 3 != 2\n3 &lt;= 4 or 4 &gt; 5\n\nIf statements\nIn the simplest case, we have:\nif &lt;predicate&gt;:\n    &lt;some action&gt;\nThis translates to:\n\nIf &lt;predicate&gt; evaluates to True, the body of the if statement gets evaluated (&lt;some action&gt; is run),\nIf &lt;predicate&gt; evaluates to False, nothing happens.\n\n\nExamples:\n\n\nx = 3\nif x &gt;= 0:\n    print(x, 'is positive')\n\n3 is positive\n\n\n\nx = -3\nif x &gt;= 0:\n    print(x, 'is positive')\n\n\nNothing gets returned since the predicate returned False.\n\n\n\nIf else statements\nLet’s add an else statement so that our code also returns something when the predicate evaluates to False:\nif &lt;predicate&gt;:\n    &lt;some action&gt;\nelse:\n    &lt;some other action&gt;\n\nExample:\n\n\nx = -3\nif x &gt;= 0:\n    print(x, 'is positive')\nelse:\n    print(x, 'is negative')\n\n-3 is negative\n\n\n\n\nIf elif else\nWe can make this even more complex with:\nif &lt;predicate1&gt;:\n    &lt;some action&gt;\nelif &lt;predicate2&gt;:\n    &lt;some other action&gt;    \nelse:\n    &lt;yet some other action&gt;\n\nExample:\n\n\nx = -3\nif x &gt; 0:\n    print(x, 'is positive')\nelif x &lt; 0:\n    print(x, 'is negative')\nelse:\n    print(x, 'is zero')\n\n-3 is negative"
  },
  {
    "objectID": "python/control_flow.html#loops",
    "href": "python/control_flow.html#loops",
    "title": "Control flow",
    "section": "Loops",
    "text": "Loops\n\nFor loops\nFor loops run a set of instructions for each element of an iterable.\nAn iterable is any Python object cable of returning the items it contains one at a time.\n\nExamples of iterables:\n\nrange(5)\n'a string is an iterable'\n[2, 'word', 4.0]\nFor loops follow the syntax:\nfor &lt;iterable&gt;:\n    &lt;some action&gt;\n\nExample:\n\n\nfor i in range(5):\n    print(i)\n\n0\n1\n2\n3\n4\n\n\n\n\nYour turn:\n\nRemember that the indentation matters in Python.\nWhat do you think that this will print?\nfor i in range(5):\n    print(i)\nprint(i)\n\nStrings are iterables too, so this works:\n\nfor i in 'a string is an iterable':\n    print(i)\n\na\n \ns\nt\nr\ni\nn\ng\n \ni\ns\n \na\nn\n \ni\nt\ne\nr\na\nb\nl\ne\n\n\nTo iterate over multiple iterables at the same time, a convenient option is to use the function zip which creates an iterator of tuples:\n\nfor i, j in zip([1, 2, 3, 4], [3, 4, 5, 6]):\n    print(i + j)\n\n4\n6\n8\n10\n\n\n\n\nWhile loops\nWhile loops run as long as a predicate remains true. They follow the syntax:\nwhile &lt;predicate&gt;:\n    &lt;some action&gt;\n\nExample:\n\n\ni = 0\nwhile i &lt;= 10:\n    print(i)\n    i += 1\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10"
  },
  {
    "objectID": "python/basics.html",
    "href": "python/basics.html",
    "title": "Python: the basics",
    "section": "",
    "text": "Python is a hugely popular interpreted language with a simple, easily readable syntax and a large collection of external packages.\nIt was created by Dutch programmer Guido van Rossum in the 80s, with a launch in 1989. Since the start of the PYPL PopularitY of Programming Language index (based on the number of tutorial searches on Google) in 2004, its popularity has grown steadily, reaching the number one position in 2018 where it still is (as of January 2023)."
  },
  {
    "objectID": "python/basics.html#about-python",
    "href": "python/basics.html#about-python",
    "title": "Python: the basics",
    "section": "",
    "text": "Python is a hugely popular interpreted language with a simple, easily readable syntax and a large collection of external packages.\nIt was created by Dutch programmer Guido van Rossum in the 80s, with a launch in 1989. Since the start of the PYPL PopularitY of Programming Language index (based on the number of tutorial searches on Google) in 2004, its popularity has grown steadily, reaching the number one position in 2018 where it still is (as of January 2023)."
  },
  {
    "objectID": "python/basics.html#the-standard-library",
    "href": "python/basics.html#the-standard-library",
    "title": "Python: the basics",
    "section": "The standard library",
    "text": "The standard library\nPython comes with a standard library. As soon as you launch the program, you can access part of the standard library such as the built-in functions and built-in constants:\n\nExample:\n\n\ntype(3)    # type is a built-in function\n\nint\n\n\nMost of the standard library however is held in several thematic modules. Each module contains additional functions, constants, and facilities. Before you can use them, you need to load them into your session.\n\nExample: the os module\nThe os module contains the function getcwd returning the path of the current working directory as a string.\nThis function cannot be used directly:\n\ngetcwd()\n\nNameError: name 'getcwd' is not defined\n\n\nIn order to access it, you have several options:\n\nLoad the module, then access the function as a method of the module:\n\n\nimport os\nos.getcwd()\n\n'/home/marie/parvus/prog/mint/python'\n\n\n\nYou can create an alias for the module:\n\nimport os as o\no.getcwd()\n\n'/home/marie/parvus/prog/mint/python'\n\n\nWhile it is a little silly for a module with such a short name, it is very convenient with modules of longer names.\n\n\nImport the function directly:\n\n\nfrom os import getcwd\ngetcwd()\n\n'/home/marie/parvus/prog/mint/python'"
  },
  {
    "objectID": "python/basics.html#help-and-documentation",
    "href": "python/basics.html#help-and-documentation",
    "title": "Python: the basics",
    "section": "Help and documentation",
    "text": "Help and documentation\n\nModule\nYou can get help on a module thanks to the help function, but only after you have loaded that module into your session:\nimport os\nhelp(os)\nHelp on module os:\n\nNAME\n    os - OS routines for NT or Posix depending on what system we're on.\n\nMODULE REFERENCE\n    https://docs.python.org/3.10/library/os.html\n\n    The following documentation is automatically generated from the Python\n    source files.  It may be incomplete, incorrect or include features that\n    are considered implementation detail and may vary between Python\n    implementations.  When in doubt, consult the module reference at the\n    location listed above.\n    \n... \n\n\nFunctions\nYou can also access the internal Python documentation on a function with help:\n\nhelp(max)\n\nHelp on built-in function max in module builtins:\n\nmax(...)\n    max(iterable, *[, default=obj, key=func]) -&gt; value\n    max(arg1, arg2, *args, *[, key=func]) -&gt; value\n    \n    With a single iterable argument, return its biggest item. The\n    default keyword-only argument specifies an object to return if\n    the provided iterable is empty.\n    With two or more arguments, return the largest argument.\n\n\n\n\nIn Jupyter, you can also use ?max or max?.\n\nAlternatively, you can print the __doc__ method of the function:\n\nprint(max.__doc__)\n\nmax(iterable, *[, default=obj, key=func]) -&gt; value\nmax(arg1, arg2, *args, *[, key=func]) -&gt; value\n\nWith a single iterable argument, return its biggest item. The\ndefault keyword-only argument specifies an object to return if\nthe provided iterable is empty.\nWith two or more arguments, return the largest argument.\n\n\n\n\nMethods of object types\nSome methods belong to specific objects types (e.g. lists have a method called append).\nIn those cases, help(&lt;method&gt;) won’t work.\n\nExample:\n\n\nhelp(append)\n\nNameError: name 'append' is not defined\n\n\nWhat you need to run instead is help(&lt;object&gt;.&lt;method&gt;).\n\nExample:\n\n\nhelp(list.append)\n\nHelp on method_descriptor:\n\nappend(self, object, /)\n    Append object to the end of the list."
  },
  {
    "objectID": "python/basics.html#syntax",
    "href": "python/basics.html#syntax",
    "title": "Python: the basics",
    "section": "Syntax",
    "text": "Syntax\nCommands are usually written one per line, but you can write multiple commands on the same line with the separator ;:\n\na = 2.0; a\n\n2.0\n\n\nTabs or 4 spaces (the number of spaces can be customized in many IDEs) have a syntactic meaning in Python and are not just for human readability:\n\n# Incorrect code\nfor i in [1, 2]:\nprint(i)\n\nIndentationError: expected an indented block after 'for' statement on line 2 (1993980772.py, line 3)\n\n\n\n# Correct code\nfor i in [1, 2]:\n    print(i)\n\n1\n2\n\n\n\nIDEs and good text editors can indent the code automatically.\n\nComments (snippets of text for human consumption and ignored by Python) are marked by #:\n\n# This is a full-line comment\n\na         # This is an inline comment\n\n2.0\n\n\nPEP 8—the style guide for Python code—suggests a maximum of 72 characters per line for comments. Try to keep comments to the point and spread them over multiple lines if they are too long."
  },
  {
    "objectID": "python/basics.html#creating-and-deleting-objects",
    "href": "python/basics.html#creating-and-deleting-objects",
    "title": "Python: the basics",
    "section": "Creating and deleting objects",
    "text": "Creating and deleting objects\n\nAssignment\nThe assignment statement = binds a name (a reference) and a value to create an object (variable, data structure, function, or method).\n\nFor instance, we can bind the name a and the value 1 to create the variable a:\n\n\na = 1\n\nYou can define multiple objects at once (here variables), assigning them the same value:\n\na = b = 10\nprint(a, b)\n\n10 10\n\n\n… or different values:\n\na, b = 1, 2\nprint(a, b)\n\n1 2\n\n\n\n\nYour turn:\n\n\na = 1\nb = a\na = 2\n\nWhat do you think the value of b is now?\n\n\n\nChoosing names\nWhile I am using a and b a lot in this workshop (since the code has no other purpose than to demo the language itself), in your scripts you should use meaningful names (e.g. survival, age, year, species, temperature). It will make reading the code this much easier.\nMake sure not to use the names of built-in functions or built-in constants.\n\n\nDeleting objects\nDeletion of the names can be done with the del statement:\n\nvar = 3\nvar\n\n3\n\n\n\ndel var\nvar\n\nNameError: name 'var' is not defined\n\n\nThe Python garbage collector automatically removes values with no names bound to them from memory."
  },
  {
    "objectID": "python/basics.html#data-types",
    "href": "python/basics.html#data-types",
    "title": "Python: the basics",
    "section": "Data types",
    "text": "Data types\nPython comes with multiple built-in types.\n\nExamples (non exhaustive):\n\n\ntype(1), type(1.0), type('1'), type(3+2j), type(True)\n\n(int, float, str, complex, bool)\n\n\n\nint = integer\nfloat = floating point number\ncomplex = complex number\nstr = string\nbool = Boolean\n\nPython is dynamically-typed: names do not have types, but they are bound to typed values and they can be bound over time to values of different types.\n\nvar = 2.3\ntype1 = type(var)\nvar = \"A string.\"\ntype2 = type(var)\n\ntype1, type2\n\n(float, str)\n\n\nYou can also convert the type of some values:\n\n'4', type('4'), int('4'), type(int('4'))\n\n('4', str, 4, int)\n\n\n\nfloat(3)\n\n3.0\n\n\n\nstr(3.4)\n\n'3.4'\n\n\n\nbool(0)\n\nFalse\n\n\n\nbool(1)\n\nTrue\n\n\n\nint(True)\n\n1\n\n\n\nfloat(False)\n\n0.0\n\n\nOf course, not all conversions are possible:\n\nint('red')\n\nValueError: invalid literal for int() with base 10: 'red'\n\n\nYou might be surprised by some of the conversions:\n\nint(3.9)\n\n3\n\n\n\nbool(3.4)\n\nTrue"
  },
  {
    "objectID": "python/basics.html#quotes",
    "href": "python/basics.html#quotes",
    "title": "Python: the basics",
    "section": "Quotes",
    "text": "Quotes\nPairs of single and double quotes are used to create strings. PEP 8 does not recommend one style over the other. It does suggest however that once you have chosen a style, you stick to it to make scripts consistent.\n\n\"This is a string.\"\n\n'This is a string.'\n\n\n\ntype(\"This is a string.\")\n\nstr\n\n\n\n'This is also a string.'\n\n'This is also a string.'\n\n\n\ntype('This is also a string.')\n\nstr\n\n\nApostrophes and textual quotes interfere with Python quotes. In these cases, use the opposite style to avoid any problem:\n\n# This doesn't work\n'This string isn't easy'\n\nSyntaxError: unterminated string literal (detected at line 2) (368933316.py, line 2)\n\n\n\n# This is good\n\"This string isn't easy\"\n\n\"This string isn't easy\"\n\n\n\n# This doesn't work\n\"He said: \"this is a problem.\"\"\n\nSyntaxError: invalid syntax (466663664.py, line 2)\n\n\n\n# This is good\n'He said: \"this is a problem.\"'\n\n'He said: \"this is a problem.\"'\n\n\nSometimes, neither option works and you have to escape some of the quotes with \\:\n\n# This doesn't work\n\"He said: \"this string isn't easy\"\"\n\nSyntaxError: unterminated string literal (detected at line 2) (392662328.py, line 2)\n\n\n\n# This doesn't work either\n'He said: \"this string isn't easy\"'\n\nSyntaxError: unterminated string literal (detected at line 2) (521375870.py, line 2)\n\n\n\n# You can use double quotes and escape double quotes in the string\n\"He said: \\\"this string isn't easy\\\"\"\n\n'He said: \"this string isn\\'t easy\"'\n\n\n\n# Or you can use single quotes and escape single quotes in the string\n'He said: \"this string isn\\'t easy\"'\n\n'He said: \"this string isn\\'t easy\"'"
  },
  {
    "objectID": "python/basics.html#basic-operations",
    "href": "python/basics.html#basic-operations",
    "title": "Python: the basics",
    "section": "Basic operations",
    "text": "Basic operations\n\n3 + 2\n\n5\n\n\n\n3.0 - 2.0\n\n1.0\n\n\n\n10 / 2\n\n5.0\n\n\n\nNotice how the result can be of a different type\n\nVariables can be used in operations:\n\na = 3\na + 2\n\n5\n\n\na = a + 10 can be replaced by the more elegant:\n\na += 10\na\n\n13"
  },
  {
    "objectID": "ml/ws_pretrained_models.html",
    "href": "ml/ws_pretrained_models.html",
    "title": "Finding pretrained models for transfer learning",
    "section": "",
    "text": "Training models from scratch requires way too much data, time, and computing power (or money) to be a practical option. This is why transfer learning has become such a common practice: by starting with models trained on related problems, you are saving time and achieving good results with little data.\nNow, where do you find such models?\nIn this workshop, we will see how to use pre-trained models included in PyTorch libraries, have a look at some of the most popular pre-trained models repositories, and learn how to search models in the literature and on GitHub."
  },
  {
    "objectID": "ml/ws_pretrained_models.html#what-are-pre-trained-models",
    "href": "ml/ws_pretrained_models.html#what-are-pre-trained-models",
    "title": "Finding pretrained models for transfer learning",
    "section": "What are pre-trained models?",
    "text": "What are pre-trained models?\n\nTransfer learning\nIf you build models from scratch, expect their performance to be mediocre. Totally naive models with random weights and biases usually need to be trained for a long time on very large datasets, using vast amounts of computing resources, before they produce competitive results. You may not even have enough data to train a model from scratch.\nInstead of starting from zero however, you can use a model that has been trained on a similar task. For instance, if your goal is to create a model able to identify bird species from pictures, you could look for a model developed for image recognition tasks trained on a classic dataset such as ImageNet. Classic such models include AlexNet (2012) and ResNet (2015). These models will already have features that are useful to you and you will get better performance with less training time and fewer data. This is called transfer learning.\n\n\nHow transfer learning works\nTypically, you remove the last layer (for instance, with AlexNet, you would remove the classification layer), replace it with a layer suitable to your task, then, optionally, you can fine tune the model.\nFine tuning a model consists of freezing the first layers (fixing their weights and biases) while retraining the model with data specific to the new task. This will only train the last few layers, greatly reducing the size of the model actually being trained and taking advantage of the early features from the source model.\nI will talk about transfer learning in another workshop, but today, we are focusing on finding a suitable pre-trained model.\nNote that the most powerful recent transformers such as GPT-3 and 4 and their competitors perform well in different tasks without the need for re-training.\n\n\nHow to find a pre-trained model\nKey to transfer learning is the search for an appropriate source model. The great news is that the world of machine learning research is incredibly open: many teams make their papers and models available online. But you need a way to navigate this abundance of resource.\nThings you should probably care about when looking for a pre-trained model include:\n\nHow pertinent is the model relative to your task?\nDoes the model have an open license?\nIs the performance good?\nIs the model size suitable for the resources I have?"
  },
  {
    "objectID": "ml/ws_pretrained_models.html#models-in-pytorch-libraries",
    "href": "ml/ws_pretrained_models.html#models-in-pytorch-libraries",
    "title": "Finding pretrained models for transfer learning",
    "section": "Models in PyTorch libraries",
    "text": "Models in PyTorch libraries\nThe PyTorch ecosystem contains domain specific libraries (e.g. torchvision, torchtext, torchaudio). Among many domain specific utilities, these libraries contain many pretrained models in vision, text, and audio.\nThese models benefit from optimum convenience since they are entirely integrated into PyTorch.\n\nLoading ResNet-18 is as simple as:\n\nimport torchvision\nmodel = torchvision.models.resnet18()\n\nInitializing a pretrained ResNet-50 model with the best currently available weights is as simple as:\n\nfrom torchvision.models import resnet50, ResNet50_Weights\nmodel = resnet50(weights=ResNet50_Weights.DEFAULT)"
  },
  {
    "objectID": "ml/ws_pretrained_models.html#pytorch-hub",
    "href": "ml/ws_pretrained_models.html#pytorch-hub",
    "title": "Finding pretrained models for transfer learning",
    "section": "PyTorch Hub",
    "text": "PyTorch Hub\nPyTorch Hub is a repository of pretrained models.\n\nLoading ResNet-18 from the hub is done with:\n\nimport torch\nmodel = torch.hub.load('pytorch/vision', 'resnet18', pretrained=True)\n\n\nYour turn:\n\nLook for a small image classification model in the PyTorch Hub."
  },
  {
    "objectID": "ml/ws_pretrained_models.html#hugging-face",
    "href": "ml/ws_pretrained_models.html#hugging-face",
    "title": "Finding pretrained models for transfer learning",
    "section": "Hugging Face",
    "text": "Hugging Face\nHugging Face, launched in 2016, provides a Model Hub. Let’s explore it together.\n\nNote that Hugging Face also has a Dataset Hub.\n\n\n\nYour turn:\n\nFind a pre-trained model for image classification in PyTorch, trained on ImageNet, with an open license, and less than 100MB in size.\n\n\ntimm\nFor computer vision specifically, the timm (PyTorch Image Models) library contains more than 700 pretrained models, as well as scripts, utilities, optimizers, data-loaders, etc. The repo can be found here.\nYou can load models from the Hugging Face Hub with:\nimport timm\nmodel = timm.create_model('hf_hub:author/model', pretrained=True)"
  },
  {
    "objectID": "ml/ws_pretrained_models.html#github",
    "href": "ml/ws_pretrained_models.html#github",
    "title": "Finding pretrained models for transfer learning",
    "section": "GitHub",
    "text": "GitHub\nA large number of open source models are hosted on GitHub and the platform can be searched directly for specific models.\n\n\nYour turn:\n\nDo a search on GitHub, trying to find pre-trained models in PyTorch for image classification."
  },
  {
    "objectID": "ml/ws_pretrained_models.html#literature",
    "href": "ml/ws_pretrained_models.html#literature",
    "title": "Finding pretrained models for transfer learning",
    "section": "Literature",
    "text": "Literature\nWhile a less direct way to find pre-trained models, the literature is invaluable to (try to) keep up with what people are doing in the field.\nPapers With Code gathers machine learning papers with open source code.\narXiv is an open-source repository of scientific preprints created by Paul Ginsparg from Cornell University in 1991. It contains a huge number of e-prints on machine learning in the computer science and the statistics fields. arxiv-sanity, created by Andrej Karpathy, tracks arXiv machine learning papers and is easier to browse."
  },
  {
    "objectID": "ml/ws_hss_intro.html",
    "href": "ml/ws_hss_intro.html",
    "title": "Introduction to machine learning for the humanities",
    "section": "",
    "text": "We hear about it all the time, but what really is machine learning? And what about deep learning? Neural networks?? How can any of this help me with my work? And how? Which tools do I need to make use of the transformative advances happening in that field??\nThis workshop will answer these questions in a non-technical manner to give you a high level overview of a discipline that has become crucial in all fields of research.\n\nSlides (Click and wait: the presentation might take a few instants to load)"
  },
  {
    "objectID": "ml/wb_upscaling_slides.html#can-be-broken-down-into-2-main-periods",
    "href": "ml/wb_upscaling_slides.html#can-be-broken-down-into-2-main-periods",
    "title": "Super-resolution with PyTorch",
    "section": "Can be broken down into 2 main periods:",
    "text": "Can be broken down into 2 main periods:\n\nA rather slow history with various interpolation algorithms of increasing complexity before deep neural networks\nAn incredibly fast evolution since the advent of deep learning (DL)"
  },
  {
    "objectID": "ml/wb_upscaling_slides.html#sr-history-pre-dl",
    "href": "ml/wb_upscaling_slides.html#sr-history-pre-dl",
    "title": "Super-resolution with PyTorch",
    "section": "SR history Pre-DL",
    "text": "SR history Pre-DL\nPixel-wise interpolation prior to DL\nVarious methods ranging from simple (e.g. nearest-neighbour, bicubic) to complex (e.g. Gaussian process regression, iterative FIR Wiener filter) algorithms"
  },
  {
    "objectID": "ml/wb_upscaling_slides.html#sr-history-pre-dl-1",
    "href": "ml/wb_upscaling_slides.html#sr-history-pre-dl-1",
    "title": "Super-resolution with PyTorch",
    "section": "SR history Pre-DL",
    "text": "SR history Pre-DL\nNearest-neighbour interpolation\nSimplest method of interpolation\nSimply uses the value of the nearest pixel\nBicubic interpolation\nConsists of determining the 16 coefficients \\(a_{ij}\\) in:\n\\[p(x, y) = \\sum_{i=0}^3\\sum_{i=0}^3 a\\_{ij} x^i y^j\\]"
  },
  {
    "objectID": "ml/wb_upscaling_slides.html#sr-history-with-dl",
    "href": "ml/wb_upscaling_slides.html#sr-history-with-dl",
    "title": "Super-resolution with PyTorch",
    "section": "SR history with DL",
    "text": "SR history with DL\nDeep learning has seen a fast evolution marked by the successive emergence of various frameworks and architectures over the past 10 years\nSome key network architectures and frameworks:\n\nCNN\nGAN\nTransformers\n\nThese have all been applied to SR"
  },
  {
    "objectID": "ml/wb_upscaling_slides.html#srcnn",
    "href": "ml/wb_upscaling_slides.html#srcnn",
    "title": "Super-resolution with PyTorch",
    "section": "SRCNN",
    "text": "SRCNN\n\n\nDong, C., Loy, C. C., He, K., & Tang, X. (2015). Image super-resolution using deep convolutional networks. IEEE transactions on pattern analysis and machine intelligence, 38(2), 295-307\n\n\nGiven a low-resolution image Y, the first convolutional layer of the SRCNN extracts a set of feature maps. The second layer maps these feature maps nonlinearly to high-resolution patch representations. The last layer combines the predictions within a spatial neighbourhood to produce the final high-resolution image F(Y)"
  },
  {
    "objectID": "ml/wb_upscaling_slides.html#srcnn-1",
    "href": "ml/wb_upscaling_slides.html#srcnn-1",
    "title": "Super-resolution with PyTorch",
    "section": "SRCNN",
    "text": "SRCNN\nCan use sparse-coding-based methods\n\n\nDong, C., Loy, C. C., He, K., & Tang, X. (2015). Image super-resolution using deep convolutional networks. IEEE transactions on pattern analysis and machine intelligence, 38(2), 295-307"
  },
  {
    "objectID": "ml/wb_upscaling_slides.html#srgan",
    "href": "ml/wb_upscaling_slides.html#srgan",
    "title": "Super-resolution with PyTorch",
    "section": "SRGAN",
    "text": "SRGAN\nDo not provide the best PSNR, but can give more realistic results by providing more texture (less smoothing)"
  },
  {
    "objectID": "ml/wb_upscaling_slides.html#gan",
    "href": "ml/wb_upscaling_slides.html#gan",
    "title": "Super-resolution with PyTorch",
    "section": "GAN",
    "text": "GAN\n\n\nStevens E., Antiga L., & Viehmann T. (2020). Deep Learning with PyTorch"
  },
  {
    "objectID": "ml/wb_upscaling_slides.html#srgan-1",
    "href": "ml/wb_upscaling_slides.html#srgan-1",
    "title": "Super-resolution with PyTorch",
    "section": "SRGAN",
    "text": "SRGAN\n\n\nLedig, C., Theis, L., Huszár, F., Caballero, J., Cunningham, A., Acosta, A., … & Shi, W. (2017). Photo-realistic single image super-resolution using a generative adversarial network. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 4681-4690)"
  },
  {
    "objectID": "ml/wb_upscaling_slides.html#srgan-2",
    "href": "ml/wb_upscaling_slides.html#srgan-2",
    "title": "Super-resolution with PyTorch",
    "section": "SRGAN",
    "text": "SRGAN\nFollowed by the ESRGAN and many other flavours of SRGANs"
  },
  {
    "objectID": "ml/wb_upscaling_slides.html#attention",
    "href": "ml/wb_upscaling_slides.html#attention",
    "title": "Super-resolution with PyTorch",
    "section": "Attention",
    "text": "Attention\n\nMnih, V., Heess, N., & Graves, A. (2014). Recurrent models of visual attention. In Advances in neural information processing systems (pp. 2204-2212)\n\n(cited 2769 times)\n\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., … & Polosukhin, I. (2017). Attention is all you need. In Advances in neural information processing systems (pp. 5998-6008)\n\n(cited 30999 times…)"
  },
  {
    "objectID": "ml/wb_upscaling_slides.html#transformers",
    "href": "ml/wb_upscaling_slides.html#transformers",
    "title": "Super-resolution with PyTorch",
    "section": "Transformers",
    "text": "Transformers\n\n\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., … & Polosukhin, I. (2017). Attention is all you need. In Advances in neural information processing systems (pp. 5998-6008)"
  },
  {
    "objectID": "ml/wb_upscaling_slides.html#transformers-1",
    "href": "ml/wb_upscaling_slides.html#transformers-1",
    "title": "Super-resolution with PyTorch",
    "section": "Transformers",
    "text": "Transformers\nInitially used for NLP to replace RNN as they allow parallelization Now entering the domain of vision and others Very performant with relatively few parameters"
  },
  {
    "objectID": "ml/wb_upscaling_slides.html#swin-transformer",
    "href": "ml/wb_upscaling_slides.html#swin-transformer",
    "title": "Super-resolution with PyTorch",
    "section": "Swin Transformer",
    "text": "Swin Transformer\nThe Swin Transformer improved the use of transformers to the vision domain\nSwin = Shifted WINdows"
  },
  {
    "objectID": "ml/wb_upscaling_slides.html#swin-transformer-1",
    "href": "ml/wb_upscaling_slides.html#swin-transformer-1",
    "title": "Super-resolution with PyTorch",
    "section": "Swin Transformer",
    "text": "Swin Transformer\nSwin transformer (left) vs transformer as initially applied to vision (right):\n\n\nLiu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., … & Guo, B. (2021). Swin transformer: Hierarchical vision transformer using shifted windows. arXiv preprint arXiv:2103.14030"
  },
  {
    "objectID": "ml/wb_upscaling_slides.html#swinir-1",
    "href": "ml/wb_upscaling_slides.html#swinir-1",
    "title": "Super-resolution with PyTorch",
    "section": "SwinIR",
    "text": "SwinIR\n\n\nLiang, J., Cao, J., Sun, G., Zhang, K., Van Gool, L., & Timofte, R. (2021). SwinIR: Image restoration using swin transformer. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 1833-1844)"
  },
  {
    "objectID": "ml/wb_upscaling_slides.html#training-sets-used",
    "href": "ml/wb_upscaling_slides.html#training-sets-used",
    "title": "Super-resolution with PyTorch",
    "section": "Training sets used",
    "text": "Training sets used\nDIV2K, Flickr2K, and other datasets"
  },
  {
    "objectID": "ml/wb_upscaling_slides.html#models-assessment",
    "href": "ml/wb_upscaling_slides.html#models-assessment",
    "title": "Super-resolution with PyTorch",
    "section": "Models assessment",
    "text": "Models assessment\n3 metrics commonly used:\nPeak sign-to-noise ratio (PSNR) measured in dB\n\\(\\frac{\\text{Maximum possible power of signal}}{\\text{Power of noise (calculated as the mean squared error)}}\\)\nCalculated at the pixel level\nStructural similarity index measure (SSIM)\nPrediction of perceived image quality based on a “perfect” reference image\nMean opinion score (MOS)\nMean of subjective quality ratings"
  },
  {
    "objectID": "ml/wb_upscaling_slides.html#models-assessment-1",
    "href": "ml/wb_upscaling_slides.html#models-assessment-1",
    "title": "Super-resolution with PyTorch",
    "section": "Models assessment",
    "text": "Models assessment\nPeak sign-to-noise ratio (PSNR) measured in dB\n\\[PSNR = 10\\,\\cdot\\,log_{10}\\,\\left(\\frac{MAX_I^2}{MSE}\\right)\\]\nStructural similarity index measure (SSIM)\n\\[SSIM(x,y) = \\frac{(2\\mu_x\\mu_y + c_1) + (2 \\sigma _{xy} + c_2)}\n    {(\\mu_x^2 + \\mu_y^2+c_1) (\\sigma_x^2 + \\sigma_y^2+c_2)}\\]\nMean opinion score (MOS)\n\\[MOS = \\frac{\\sum_{n=1}^N R\\_n}{N}\\]"
  },
  {
    "objectID": "ml/wb_upscaling_slides.html#metrics-implementation",
    "href": "ml/wb_upscaling_slides.html#metrics-implementation",
    "title": "Super-resolution with PyTorch",
    "section": "Metrics implementation",
    "text": "Metrics implementation\n\nImplement them yourself (using torch.log10, etc.)\nUse some library that implements them (e.g. kornia)\nUse code of open source project with good implementation (e.g. SwinIR)\nUse some higher level library that provides them (e.g. ignite)"
  },
  {
    "objectID": "ml/wb_upscaling_slides.html#metrics-implementation-1",
    "href": "ml/wb_upscaling_slides.html#metrics-implementation-1",
    "title": "Super-resolution with PyTorch",
    "section": "Metrics implementation",
    "text": "Metrics implementation\n\nImplement them yourself (using torch.log10, etc.)\nUse some library that implements them (e.g. kornia)\nUse code of open source project with good implementation (e.g. SwinIR)\nUse some higher level library that provides them (e.g. ignite)"
  },
  {
    "objectID": "ml/wb_upscaling_slides.html#metrics-implementation-2",
    "href": "ml/wb_upscaling_slides.html#metrics-implementation-2",
    "title": "Super-resolution with PyTorch",
    "section": "Metrics implementation",
    "text": "Metrics implementation\nimport kornia\n\npsnr_value = kornia.metrics.psnr(input, target, max_val)\nssim_value = kornia.metrics.ssim(img1, img2, window_size, max_val=1.0, eps=1e-12)\nSee the Kornia documentation for more info on kornia.metrics.psnr & kornia.metrics.ssim"
  },
  {
    "objectID": "ml/wb_upscaling_slides.html#benchmark-datasets",
    "href": "ml/wb_upscaling_slides.html#benchmark-datasets",
    "title": "Super-resolution with PyTorch",
    "section": "Benchmark datasets",
    "text": "Benchmark datasets\nSet5\n\nSet14\n\nBSD100 (Berkeley Segmentation Dataset)"
  },
  {
    "objectID": "ml/wb_upscaling_slides.html#benchmark-datasets-1",
    "href": "ml/wb_upscaling_slides.html#benchmark-datasets-1",
    "title": "Super-resolution with PyTorch",
    "section": "Benchmark datasets",
    "text": "Benchmark datasets\nSet5\n\nSet14\n\nBSD100 (Berkeley Segmentation Dataset)"
  },
  {
    "objectID": "ml/wb_upscaling_slides.html#the-set5-dataset",
    "href": "ml/wb_upscaling_slides.html#the-set5-dataset",
    "title": "Super-resolution with PyTorch",
    "section": "The Set5 dataset",
    "text": "The Set5 dataset\nA dataset consisting of 5 images which has been used for at least 18 years to assess SR methods"
  },
  {
    "objectID": "ml/wb_upscaling_slides.html#how-to-get-the-dataset",
    "href": "ml/wb_upscaling_slides.html#how-to-get-the-dataset",
    "title": "Super-resolution with PyTorch",
    "section": "How to get the dataset?",
    "text": "How to get the dataset?\nFrom the HuggingFace Datasets Hub with the HuggingFace datasets package:\nfrom datasets import load_dataset\n\nset5 = load_dataset('eugenesiow/Set5', 'bicubic_x4', split='validation')"
  },
  {
    "objectID": "ml/wb_upscaling_slides.html#dataset-exploration",
    "href": "ml/wb_upscaling_slides.html#dataset-exploration",
    "title": "Super-resolution with PyTorch",
    "section": "Dataset exploration",
    "text": "Dataset exploration\nprint(set5)\nlen(set5)\nset5[0]\nset5.shape\nset5.column_names\nset5.features\nset5.set_format('torch', columns=['hr', 'lr'])\nset5.format"
  },
  {
    "objectID": "ml/wb_upscaling_slides.html#benchmarks",
    "href": "ml/wb_upscaling_slides.html#benchmarks",
    "title": "Super-resolution with PyTorch",
    "section": "Benchmarks",
    "text": "Benchmarks\nA 2012 review of interpolation methods for SR gives the metrics for a series of interpolation methods (using other datasets)"
  },
  {
    "objectID": "ml/wb_upscaling_slides.html#interpolation-methods",
    "href": "ml/wb_upscaling_slides.html#interpolation-methods",
    "title": "Super-resolution with PyTorch",
    "section": "Interpolation methods",
    "text": "Interpolation methods"
  },
  {
    "objectID": "ml/wb_upscaling_slides.html#dl-methods",
    "href": "ml/wb_upscaling_slides.html#dl-methods",
    "title": "Super-resolution with PyTorch",
    "section": "DL methods",
    "text": "DL methods\nThe Papers with Code website lists available benchmarks on Set5"
  },
  {
    "objectID": "ml/wb_upscaling_slides.html#lets-use-swinir",
    "href": "ml/wb_upscaling_slides.html#lets-use-swinir",
    "title": "Super-resolution with PyTorch",
    "section": "Let’s use SwinIR",
    "text": "Let’s use SwinIR\n# Get the model\ngit clone git@github.com:JingyunLiang/SwinIR.git\ncd SwinIR\n\n# Copy our test images in the repo\ncp -r &lt;some/path&gt;/my_tests /testsets/my_tests\n\n# Run the model on our images\npython main_test_swinir.py --tile 400 --task real_sr --scale 4 --large_model --model_path model_zoo/swinir/003_realSR_BSRGAN_DFOWMFC_s64w8_SwinIR-L_x4_GAN.pth --folder_lq testsets/my_tests\nRan in 9 min on my machine with one GPU and 32GB of RAM"
  },
  {
    "objectID": "ml/wb_upscaling_slides.html#results",
    "href": "ml/wb_upscaling_slides.html#results",
    "title": "Super-resolution with PyTorch",
    "section": "Results",
    "text": "Results"
  },
  {
    "objectID": "ml/wb_upscaling_slides.html#results-1",
    "href": "ml/wb_upscaling_slides.html#results-1",
    "title": "Super-resolution with PyTorch",
    "section": "Results",
    "text": "Results"
  },
  {
    "objectID": "ml/wb_upscaling_slides.html#results-2",
    "href": "ml/wb_upscaling_slides.html#results-2",
    "title": "Super-resolution with PyTorch",
    "section": "Results",
    "text": "Results"
  },
  {
    "objectID": "ml/wb_upscaling_slides.html#results-3",
    "href": "ml/wb_upscaling_slides.html#results-3",
    "title": "Super-resolution with PyTorch",
    "section": "Results",
    "text": "Results"
  },
  {
    "objectID": "ml/wb_upscaling_slides.html#results-4",
    "href": "ml/wb_upscaling_slides.html#results-4",
    "title": "Super-resolution with PyTorch",
    "section": "Results",
    "text": "Results"
  },
  {
    "objectID": "ml/wb_upscaling_slides.html#results-5",
    "href": "ml/wb_upscaling_slides.html#results-5",
    "title": "Super-resolution with PyTorch",
    "section": "Results",
    "text": "Results"
  },
  {
    "objectID": "ml/wb_upscaling_slides.html#results-6",
    "href": "ml/wb_upscaling_slides.html#results-6",
    "title": "Super-resolution with PyTorch",
    "section": "Results",
    "text": "Results"
  },
  {
    "objectID": "ml/wb_upscaling_slides.html#results-7",
    "href": "ml/wb_upscaling_slides.html#results-7",
    "title": "Super-resolution with PyTorch",
    "section": "Results",
    "text": "Results"
  },
  {
    "objectID": "ml/wb_upscaling_slides.html#results-8",
    "href": "ml/wb_upscaling_slides.html#results-8",
    "title": "Super-resolution with PyTorch",
    "section": "Results",
    "text": "Results"
  },
  {
    "objectID": "ml/wb_upscaling_slides.html#results-9",
    "href": "ml/wb_upscaling_slides.html#results-9",
    "title": "Super-resolution with PyTorch",
    "section": "Results",
    "text": "Results"
  },
  {
    "objectID": "ml/wb_upscaling_slides.html#metrics",
    "href": "ml/wb_upscaling_slides.html#metrics",
    "title": "Super-resolution with PyTorch",
    "section": "Metrics",
    "text": "Metrics\nWe could use the PSNR and SSIM implementations from SwinIR, but let’s try the Kornia functions we mentioned earlier:\n\nkornia.metrics.psnr\nkornia.metrics.ssim"
  },
  {
    "objectID": "ml/wb_upscaling_slides.html#metrics-1",
    "href": "ml/wb_upscaling_slides.html#metrics-1",
    "title": "Super-resolution with PyTorch",
    "section": "Metrics",
    "text": "Metrics\nLet’s load the libraries we need:\nimport kornia\nfrom PIL import Image\nimport torch\nfrom torchvision import transforms"
  },
  {
    "objectID": "ml/wb_upscaling_slides.html#metrics-2",
    "href": "ml/wb_upscaling_slides.html#metrics-2",
    "title": "Super-resolution with PyTorch",
    "section": "Metrics",
    "text": "Metrics\nThen, we load one pair images (LR and HR):\nberlin1_lr = Image.open(\"&lt;some/path&gt;/lr/berlin_1945_1.jpg\")\nberlin1_hr = Image.open(\"&lt;some/path&gt;/hr/berlin_1945_1.png\")\n We can display these images with:\nberlin1_lr.show()\nberlin1_hr.show()"
  },
  {
    "objectID": "ml/wb_upscaling_slides.html#metrics-3",
    "href": "ml/wb_upscaling_slides.html#metrics-3",
    "title": "Super-resolution with PyTorch",
    "section": "Metrics",
    "text": "Metrics\nNow, we need to resize them so that they have identical dimensions and turn them into tensors:\npreprocess = transforms.Compose([\n        transforms.Resize(256),\n        transforms.ToTensor()\n        ])\n\nberlin1_lr_t = preprocess(berlin1_lr)\nberlin1_hr_t = preprocess(berlin1_hr)"
  },
  {
    "objectID": "ml/wb_upscaling_slides.html#metrics-4",
    "href": "ml/wb_upscaling_slides.html#metrics-4",
    "title": "Super-resolution with PyTorch",
    "section": "Metrics",
    "text": "Metrics\nberlin1_lr_t.shape\nberlin1_hr_t.shape\n\n[Out]\n\ntorch.Size([3, 267, 256])\ntorch.Size([3, 267, 256])\nWe now have tensors with 3 dimensions:\n\nthe channels (RGB)\nthe height of the image (in pixels)\nthe width of the image (in pixels)"
  },
  {
    "objectID": "ml/wb_upscaling_slides.html#metrics-5",
    "href": "ml/wb_upscaling_slides.html#metrics-5",
    "title": "Super-resolution with PyTorch",
    "section": "Metrics",
    "text": "Metrics\nAs data processing is done in batch in ML, we need to add a 4th dimension: the batch size\n(It will be equal to 1 since we have a batch size of a single image)\nbatch_berlin1_lr_t = torch.unsqueeze(berlin1_lr_t, 0)\nbatch_berlin1_hr_t = torch.unsqueeze(berlin1_hr_t, 0)"
  },
  {
    "objectID": "ml/wb_upscaling_slides.html#metrics-6",
    "href": "ml/wb_upscaling_slides.html#metrics-6",
    "title": "Super-resolution with PyTorch",
    "section": "Metrics",
    "text": "Metrics\nOur new tensors are now ready:\nbatch_berlin1_lr_t.shape\nbatch_berlin1_hr_t.shape\n\n[Out]\n\ntorch.Size([1, 3, 267, 256])\ntorch.Size([1, 3, 267, 256])"
  },
  {
    "objectID": "ml/wb_upscaling_slides.html#psnr",
    "href": "ml/wb_upscaling_slides.html#psnr",
    "title": "Super-resolution with PyTorch",
    "section": "PSNR",
    "text": "PSNR\npsnr_value = kornia.metrics.psnr(batch_berlin1_lr_t, batch_berlin1_hr_t, max_val=1.0)\npsnr_value.item()\n\n[Out]\n\n33.379642486572266"
  },
  {
    "objectID": "ml/wb_upscaling_slides.html#ssim",
    "href": "ml/wb_upscaling_slides.html#ssim",
    "title": "Super-resolution with PyTorch",
    "section": "SSIM",
    "text": "SSIM\nssim_map = kornia.metrics.ssim(batch_berlin1_lr_t, batch_berlin1_hr_t, window_size=5, max_val=1.0, eps=1e-12)\nssim_map.mean().item()\n\n[Out]\n\n0.9868119359016418"
  },
  {
    "objectID": "ml/wb_torchtensors_slides.html#acknowledgements",
    "href": "ml/wb_torchtensors_slides.html#acknowledgements",
    "title": "Everything you wanted to know (& more) about PyTorch tensors",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nMany drawings in this webinar come from the book:\n\nThe section on storage is also highly inspired by it"
  },
  {
    "objectID": "ml/wb_torchtensors_slides.html#using-tensors-locally",
    "href": "ml/wb_torchtensors_slides.html#using-tensors-locally",
    "title": "Everything you wanted to know (& more) about PyTorch tensors",
    "section": "Using tensors locally",
    "text": "Using tensors locally\nYou need to have Python & PyTorch installed\nAdditionally, you might want to use an IDE such as elpy if you are an Emacs user, JupyterLab, etc.\n\nNote that PyTorch does not yet support Python 3.10 except in some Linux distributions or on systems where a wheel has been built For the time being, you might have to use it with Python 3.9"
  },
  {
    "objectID": "ml/wb_torchtensors_slides.html#using-tensors-on-cc-clusters",
    "href": "ml/wb_torchtensors_slides.html#using-tensors-on-cc-clusters",
    "title": "Everything you wanted to know (& more) about PyTorch tensors",
    "section": "Using tensors on CC clusters",
    "text": "Using tensors on CC clusters\nIn the cluster terminal:\navail_wheels \"torch*\" # List available wheels & compatible Python versions\nmodule avail python   # List available Python versions\nmodule load python/3.9.6             # Load a sensible Python version\nvirtualenv --no-download env         # Create a virtual env\nsource env/bin/activate              # Activate the virtual env\npip install --no-index --upgrade pip # Update pip\npip install --no-index torch         # Install PyTorch\nYou can then launch jobs with sbatch or salloc\nLeave the virtual env with the command: deactivate"
  },
  {
    "objectID": "ml/wb_torchtensors_slides.html#outline",
    "href": "ml/wb_torchtensors_slides.html#outline",
    "title": "Everything you wanted to know (& more) about PyTorch tensors",
    "section": "Outline",
    "text": "Outline\n\nWhat is a PyTorch tensor?\nMemory storage\nData type (dtype)\nBasic operations\nWorking with NumPy\nLinear algebra\nHarvesting the power of GPUs\nDistributed operations"
  },
  {
    "objectID": "ml/wb_torchtensors_slides.html#outline-1",
    "href": "ml/wb_torchtensors_slides.html#outline-1",
    "title": "Everything you wanted to know (& more) about PyTorch tensors",
    "section": "Outline",
    "text": "Outline\n\nWhat is a PyTorch tensor?\nMemory storage\nData type (dtype)\nBasic operations\nWorking with NumPy\nLinear algebra\nHarvesting the power of GPUs\nDistributed operations"
  },
  {
    "objectID": "ml/wb_torchtensors_slides.html#ann-do-not-process-information-directly",
    "href": "ml/wb_torchtensors_slides.html#ann-do-not-process-information-directly",
    "title": "Everything you wanted to know (& more) about PyTorch tensors",
    "section": "ANN do not process information directly",
    "text": "ANN do not process information directly\n\n\nModified from Stevens, E., Antiga, L., & Viehmann, T. (2020). Deep learning with PyTorch. Manning Publications"
  },
  {
    "objectID": "ml/wb_torchtensors_slides.html#it-needs-to-be-converted-to-numbers",
    "href": "ml/wb_torchtensors_slides.html#it-needs-to-be-converted-to-numbers",
    "title": "Everything you wanted to know (& more) about PyTorch tensors",
    "section": "It needs to be converted to numbers",
    "text": "It needs to be converted to numbers\n\n\nModified from Stevens, E., Antiga, L., & Viehmann, T. (2020). Deep learning with PyTorch. Manning Publications"
  },
  {
    "objectID": "ml/wb_torchtensors_slides.html#these-numbers-must-be-stored-in-a-data-structure",
    "href": "ml/wb_torchtensors_slides.html#these-numbers-must-be-stored-in-a-data-structure",
    "title": "Everything you wanted to know (& more) about PyTorch tensors",
    "section": "These numbers must be stored in a data structure",
    "text": "These numbers must be stored in a data structure\n\nPyTorch tensors are Python objects holding multidimensional arrays\n Stevens, E., Antiga, L., & Viehmann, T. (2020). Deep learning with PyTorch. Manning Publications"
  },
  {
    "objectID": "ml/wb_torchtensors_slides.html#why-a-new-object-when-numpy-already-exists",
    "href": "ml/wb_torchtensors_slides.html#why-a-new-object-when-numpy-already-exists",
    "title": "Everything you wanted to know (& more) about PyTorch tensors",
    "section": "Why a new object when NumPy already exists?",
    "text": "Why a new object when NumPy already exists?\n\n\nCan run on accelerators (GPUs, TPUs…)\nKeep track of computation graphs, allowing automatic differentiation\nFuture plan for sharded tensors to run distributed computations"
  },
  {
    "objectID": "ml/wb_torchtensors_slides.html#what-is-a-pytorch-tensor-2",
    "href": "ml/wb_torchtensors_slides.html#what-is-a-pytorch-tensor-2",
    "title": "Everything you wanted to know (& more) about PyTorch tensors",
    "section": "What is a PyTorch tensor?",
    "text": "What is a PyTorch tensor?\nPyTorch is foremost a deep learning library\nIn deep learning, the information contained in objects of interest (e.g. images, texts, sounds) is converted to floating-point numbers (e.g. pixel values, token values, frequencies)\nAs this information is complex, multiple dimensions are required (e.g. two dimensions for the width & height of an image, plus one dimension for the RGB colour channels)\nAdditionally, items are grouped into batches to be processed together, adding yet another dimension\nMultidimensional arrays are thus particularly well suited for deep learning"
  },
  {
    "objectID": "ml/wb_torchtensors_slides.html#what-is-a-pytorch-tensor-3",
    "href": "ml/wb_torchtensors_slides.html#what-is-a-pytorch-tensor-3",
    "title": "Everything you wanted to know (& more) about PyTorch tensors",
    "section": "What is a PyTorch tensor?",
    "text": "What is a PyTorch tensor?\nArtificial neurons perform basic computations on these tensors\nTheir number however is huge & computing efficiency is paramount\nGPUs/TPUs are particularly well suited to perform many simple operations in parallel\nThe very popular NumPy library has, at its core, a mature multidimensional array object well integrated into the scientific Python ecosystem\nBut the PyTorch tensor has additional efficiency characteristics ideal for machine learning & it can be converted to/from NumPy’s ndarray if needed"
  },
  {
    "objectID": "ml/wb_torchtensors_slides.html#outline-2",
    "href": "ml/wb_torchtensors_slides.html#outline-2",
    "title": "Everything you wanted to know (& more) about PyTorch tensors",
    "section": "Outline",
    "text": "Outline\n\nWhat is a PyTorch tensor?\nMemory storage\nData type (dtype)\nBasic operations\nWorking with NumPy\nLinear algebra\nHarvesting the power of GPUs\nDistributed operations"
  },
  {
    "objectID": "ml/wb_torchtensors_slides.html#efficient-memory-storage",
    "href": "ml/wb_torchtensors_slides.html#efficient-memory-storage",
    "title": "Everything you wanted to know (& more) about PyTorch tensors",
    "section": "Efficient memory storage",
    "text": "Efficient memory storage\nIn Python, collections (lists, tuples) are groupings of boxed Python objects\nPyTorch tensors & NumPy ndarrays are made of unboxed C numeric types\n\n\nStevens, E., Antiga, L., & Viehmann, T. (2020). Deep learning with PyTorch. Manning Publications"
  },
  {
    "objectID": "ml/wb_torchtensors_slides.html#efficient-memory-storage-1",
    "href": "ml/wb_torchtensors_slides.html#efficient-memory-storage-1",
    "title": "Everything you wanted to know (& more) about PyTorch tensors",
    "section": "Efficient memory storage",
    "text": "Efficient memory storage\nThey are usually contiguous memory blocks, but the main difference is that they are unboxed: floats will thus take 4 (32-bit) or 8 (64-bit) bytes each\nBoxed values take up more memory (memory for the pointer + memory for the primitive)\n\n\nStevens, E., Antiga, L., & Viehmann, T. (2020). Deep learning with PyTorch. Manning Publications"
  },
  {
    "objectID": "ml/wb_torchtensors_slides.html#implementation",
    "href": "ml/wb_torchtensors_slides.html#implementation",
    "title": "Everything you wanted to know (& more) about PyTorch tensors",
    "section": "Implementation",
    "text": "Implementation\nUnder the hood, the values of a PyTorch tensor are stored as a torch.Storage instance which is a one-dimensional array\n\nimport torch\nt = torch.arange(10.).view(2, 5); print(t) # Functions explained later\n\n[Out]\n\ntensor([[ 0.,  1.,  2., 3.,  4.],\n        [ 5.,  6.,  7.,  8.,  9.]])"
  },
  {
    "objectID": "ml/wb_torchtensors_slides.html#implementation-1",
    "href": "ml/wb_torchtensors_slides.html#implementation-1",
    "title": "Everything you wanted to know (& more) about PyTorch tensors",
    "section": "Implementation",
    "text": "Implementation\nstorage = t.storage(); print(storage)\n\n[Out]\n\n 0.0\n 1.0\n 2.0\n 3.0\n 4.0\n 5.0\n 6.0\n 7.0\n 8.0\n 9.0\n[torch.FloatStorage of size 10]"
  },
  {
    "objectID": "ml/wb_torchtensors_slides.html#implementation-2",
    "href": "ml/wb_torchtensors_slides.html#implementation-2",
    "title": "Everything you wanted to know (& more) about PyTorch tensors",
    "section": "Implementation",
    "text": "Implementation\nThe storage can be indexed\nstorage[3]\n\n[Out]\n\n3.0"
  },
  {
    "objectID": "ml/wb_torchtensors_slides.html#implementation-3",
    "href": "ml/wb_torchtensors_slides.html#implementation-3",
    "title": "Everything you wanted to know (& more) about PyTorch tensors",
    "section": "Implementation",
    "text": "Implementation\nstorage[3] = 10.0; print(storage)\n\n[Out]\n\n 0.0\n 1.0\n 2.0\n 10.0\n 4.0\n 5.0\n 6.0\n 7.0\n 8.0\n 9.0\n[torch.FloatStorage of size 10]"
  },
  {
    "objectID": "ml/wb_torchtensors_slides.html#implementation-4",
    "href": "ml/wb_torchtensors_slides.html#implementation-4",
    "title": "Everything you wanted to know (& more) about PyTorch tensors",
    "section": "Implementation",
    "text": "Implementation\nTo view a multidimensional array from storage, we need metadata:\n\nthe size (shape in NumPy) sets the number of elements in each dimension\nthe offset indicates where the first element of the tensor is in the storage\nthe stride establishes the increment between each element"
  },
  {
    "objectID": "ml/wb_torchtensors_slides.html#storage-metadata",
    "href": "ml/wb_torchtensors_slides.html#storage-metadata",
    "title": "Everything you wanted to know (& more) about PyTorch tensors",
    "section": "Storage metadata",
    "text": "Storage metadata\n\n\nStevens, E., Antiga, L., & Viehmann, T. (2020). Deep learning with PyTorch. Manning Publications"
  },
  {
    "objectID": "ml/wb_torchtensors_slides.html#storage-metadata-1",
    "href": "ml/wb_torchtensors_slides.html#storage-metadata-1",
    "title": "Everything you wanted to know (& more) about PyTorch tensors",
    "section": "Storage metadata",
    "text": "Storage metadata\nt.size()\nt.storage_offset()\nt.stride()\n\n[Out]\n\ntorch.Size([2, 5])\n0\n(5, 1)"
  },
  {
    "objectID": "ml/wb_torchtensors_slides.html#storage-metadata-2",
    "href": "ml/wb_torchtensors_slides.html#storage-metadata-2",
    "title": "Everything you wanted to know (& more) about PyTorch tensors",
    "section": "Storage metadata",
    "text": "Storage metadata"
  },
  {
    "objectID": "ml/wb_torchtensors_slides.html#sharing-storage",
    "href": "ml/wb_torchtensors_slides.html#sharing-storage",
    "title": "Everything you wanted to know (& more) about PyTorch tensors",
    "section": "Sharing storage",
    "text": "Sharing storage\nMultiple tensors can use the same storage, saving a lot of memory since the metadata is a lot lighter than a whole new array\n\n\nStevens, E., Antiga, L., & Viehmann, T. (2020). Deep learning with PyTorch. Manning Publications"
  },
  {
    "objectID": "ml/wb_torchtensors_slides.html#transposing-in-2-dimensions",
    "href": "ml/wb_torchtensors_slides.html#transposing-in-2-dimensions",
    "title": "Everything you wanted to know (& more) about PyTorch tensors",
    "section": "Transposing in 2 dimensions",
    "text": "Transposing in 2 dimensions\nt = torch.tensor([[3, 1, 2], [4, 1, 7]]); print(t)\nt.size()\nt.t()\nt.t().size()\n\n[Out]\n\ntensor([[3, 1, 2],\n        [4, 1, 7]])\ntorch.Size([2, 3])\ntensor([[3, 4],\n        [1, 1],\n        [2, 7]])\ntorch.Size([3, 2])"
  },
  {
    "objectID": "ml/wb_torchtensors_slides.html#transposing-in-2-dimensions-1",
    "href": "ml/wb_torchtensors_slides.html#transposing-in-2-dimensions-1",
    "title": "Everything you wanted to know (& more) about PyTorch tensors",
    "section": "Transposing in 2 dimensions",
    "text": "Transposing in 2 dimensions\n= flipping the stride elements around\n\n\nStevens, E., Antiga, L., & Viehmann, T. (2020). Deep learning with PyTorch. Manning Publications"
  },
  {
    "objectID": "ml/wb_torchtensors_slides.html#transposing-in-higher-dimensions",
    "href": "ml/wb_torchtensors_slides.html#transposing-in-higher-dimensions",
    "title": "Everything you wanted to know (& more) about PyTorch tensors",
    "section": "Transposing in higher dimensions",
    "text": "Transposing in higher dimensions\ntorch.t() is a shorthand for torch.transpose(0, 1):\ntorch.equal(t.t(), t.transpose(0, 1))\n\n[Out]\n\nTrue\nWhile torch.t() only works for 2D tensors, torch.transpose() can be used to transpose 2 dimensions in tensors of any number of dimensions"
  },
  {
    "objectID": "ml/wb_torchtensors_slides.html#transposing-in-higher-dimensions-1",
    "href": "ml/wb_torchtensors_slides.html#transposing-in-higher-dimensions-1",
    "title": "Everything you wanted to know (& more) about PyTorch tensors",
    "section": "Transposing in higher dimensions",
    "text": "Transposing in higher dimensions\nt = torch.zeros(1, 2, 3); print(t)\n\nt.size()\nt.stride()\n\n[Out]\n\ntensor([[[0., 0., 0.],\n         [0., 0., 0.]]])\n\ntorch.Size([1, 2, 3])\n(6, 3, 1)"
  },
  {
    "objectID": "ml/wb_torchtensors_slides.html#transposing-in-higher-dimensions-2",
    "href": "ml/wb_torchtensors_slides.html#transposing-in-higher-dimensions-2",
    "title": "Everything you wanted to know (& more) about PyTorch tensors",
    "section": "Transposing in higher dimensions",
    "text": "Transposing in higher dimensions\nt.transpose(0, 1)\n\nt.transpose(0, 1).size()\nt.transpose(0, 1).stride()\n\n[Out]\n\ntensor([[[0., 0., 0.]],\n        [[0., 0., 0.]]])\n\ntorch.Size([2, 1, 3])\n(3, 6, 1)  # Notice how transposing flipped 2 elements of the stride"
  },
  {
    "objectID": "ml/wb_torchtensors_slides.html#transposing-in-higher-dimensions-3",
    "href": "ml/wb_torchtensors_slides.html#transposing-in-higher-dimensions-3",
    "title": "Everything you wanted to know (& more) about PyTorch tensors",
    "section": "Transposing in higher dimensions",
    "text": "Transposing in higher dimensions\nt.transpose(0, 2)\n\nt.transpose(0, 2).size()\nt.transpose(0, 2).stride()\n\n[Out]\n\ntensor([[[0.],\n         [0.]],\n        [[0.],\n         [0.]],\n        [[0.],\n         [0.]]])\n\ntorch.Size([3, 2, 1])\n(1, 3, 6)"
  },
  {
    "objectID": "ml/wb_torchtensors_slides.html#transposing-in-higher-dimensions-4",
    "href": "ml/wb_torchtensors_slides.html#transposing-in-higher-dimensions-4",
    "title": "Everything you wanted to know (& more) about PyTorch tensors",
    "section": "Transposing in higher dimensions",
    "text": "Transposing in higher dimensions\nt.transpose(1, 2)\n\nt.transpose(1, 2).size()\nt.transpose(1, 2).stride()\n\n[Out]\n\ntensor([[[0., 0.],\n         [0., 0.],\n         [0., 0.]]])\n\ntorch.Size([1, 3, 2])\n(6, 1, 3)"
  },
  {
    "objectID": "ml/wb_torchtensors_slides.html#outline-3",
    "href": "ml/wb_torchtensors_slides.html#outline-3",
    "title": "Everything you wanted to know (& more) about PyTorch tensors",
    "section": "Outline",
    "text": "Outline\n\nWhat is a PyTorch tensor?\nMemory storage\nData type (dtype)\nBasic operations\nWorking with NumPy\nLinear algebra\nHarvesting the power of GPUs\nDistributed operations"
  },
  {
    "objectID": "ml/wb_torchtensors_slides.html#default-dtype",
    "href": "ml/wb_torchtensors_slides.html#default-dtype",
    "title": "Everything you wanted to know (& more) about PyTorch tensors",
    "section": "Default dtype",
    "text": "Default dtype\nSince PyTorch tensors were built with utmost efficiency in mind for neural networks, the default data type is 32-bit floating points\nThis is sufficient for accuracy & much faster than 64-bit floating points\n\nNote that, by contrast, NumPy ndarrays use 64-bit as their default"
  },
  {
    "objectID": "ml/wb_torchtensors_slides.html#list-of-pytorch-tensor-dtypes",
    "href": "ml/wb_torchtensors_slides.html#list-of-pytorch-tensor-dtypes",
    "title": "Everything you wanted to know (& more) about PyTorch tensors",
    "section": "List of PyTorch tensor dtypes",
    "text": "List of PyTorch tensor dtypes\n\n\n\n\ntorch.float16 / torch.half\n\n\n  \n\n\n16-bit / half-precision floating-point\n\n\n\n\ntorch.float32 / torch.float\n\n\n\n\n32-bit / single-precision floating-point\n\n\n\n\ntorch.float64 / torch.double\n\n\n\n\n64-bit / double-precision floating-point\n\n\n\n\n\n\n\n\n\n\ntorch.uint8\n\n\n\n\nunsigned 8-bit integers\n\n\n\n\ntorch.int8\n\n\n\n\nsigned 8-bit integers\n\n\n\n\ntorch.int16 / torch.short\n\n\n\n\nsigned 16-bit integers\n\n\n\n\ntorch.int32 / torch.int\n\n\n\n\nsigned 32-bit integers\n\n\n\n\ntorch.int64 / torch.long\n\n\n\n\nsigned 64-bit integers\n\n\n\n\n\n\n\n\n\n\n\n\ntorch.bool\n\n\n\n\nboolean"
  },
  {
    "objectID": "ml/wb_torchtensors_slides.html#checking-changing-dtype",
    "href": "ml/wb_torchtensors_slides.html#checking-changing-dtype",
    "title": "Everything you wanted to know (& more) about PyTorch tensors",
    "section": "Checking & changing dtype",
    "text": "Checking & changing dtype\nt = torch.rand(2, 3); print(t)\nt.dtype   # Remember that the default dtype for PyTorch tensors is float32\nt2 = t.type(torch.float64); print(t2) # If dtype ≠ default, it is printed\nt2.dtype\n\n[Out]\n\ntensor([[0.8130, 0.3757, 0.7682],\n        [0.3482, 0.0516, 0.3772]])\ntorch.float32\ntensor([[0.8130, 0.3757, 0.7682],\n        [0.3482, 0.0516, 0.3772]], dtype=torch.float64)\ntorch.float64"
  },
  {
    "objectID": "ml/wb_torchtensors_slides.html#outline-4",
    "href": "ml/wb_torchtensors_slides.html#outline-4",
    "title": "Everything you wanted to know (& more) about PyTorch tensors",
    "section": "Outline",
    "text": "Outline\n\nWhat is a PyTorch tensor?\nMemory storage\nData type (dtype)\nBasic operations\nWorking with NumPy\nLinear algebra\nHarvesting the power of GPUs\nDistributed operations"
  },
  {
    "objectID": "ml/wb_torchtensors_slides.html#creating-tensors",
    "href": "ml/wb_torchtensors_slides.html#creating-tensors",
    "title": "Everything you wanted to know (& more) about PyTorch tensors",
    "section": "Creating tensors",
    "text": "Creating tensors\n\ntorch.tensor:   Input individual values\ntorch.arange:   Similar to range but creates a 1D tensor\ntorch.linspace:  1D linear scale tensor\ntorch.logspace:  1D log scale tensor\ntorch.rand:     Random numbers from a uniform distribution on [0, 1)\ntorch.randn:    Numbers from the standard normal distribution\ntorch.randperm:   Random permutation of integers\ntorch.empty:    Uninitialized tensor\ntorch.zeros:    Tensor filled with 0\ntorch.ones:      Tensor filled with 1\ntorch.eye:      Identity matrix"
  },
  {
    "objectID": "ml/wb_torchtensors_slides.html#creating-tensors-1",
    "href": "ml/wb_torchtensors_slides.html#creating-tensors-1",
    "title": "Everything you wanted to know (& more) about PyTorch tensors",
    "section": "Creating tensors",
    "text": "Creating tensors\ntorch.manual_seed(0)  # If you want to reproduce the result\ntorch.rand(1)\n\ntorch.manual_seed(0)  # Run before each operation to get the same result\ntorch.rand(1).item()  # Extract the value from a tensor\n\n[Out]\n\ntensor([0.4963])\n\n0.49625658988952637"
  },
  {
    "objectID": "ml/wb_torchtensors_slides.html#creating-tensors-2",
    "href": "ml/wb_torchtensors_slides.html#creating-tensors-2",
    "title": "Everything you wanted to know (& more) about PyTorch tensors",
    "section": "Creating tensors",
    "text": "Creating tensors\ntorch.rand(1)\ntorch.rand(1, 1)\ntorch.rand(1, 1, 1)\ntorch.rand(1, 1, 1, 1)\n\n[Out]\n\ntensor([0.6984])\ntensor([[0.5675]])\ntensor([[[0.8352]]])\ntensor([[[[0.2056]]]])"
  },
  {
    "objectID": "ml/wb_torchtensors_slides.html#creating-tensors-3",
    "href": "ml/wb_torchtensors_slides.html#creating-tensors-3",
    "title": "Everything you wanted to know (& more) about PyTorch tensors",
    "section": "Creating tensors",
    "text": "Creating tensors\ntorch.rand(2)\ntorch.rand(2, 2, 2, 2)\n\n[Out]\n\ntensor([0.5932, 0.1123])\ntensor([[[[0.1147, 0.3168],\n          [0.6965, 0.9143]],\n         [[0.9351, 0.9412],\n          [0.5995, 0.0652]]],\n        [[[0.5460, 0.1872],\n          [0.0340, 0.9442]],\n         [[0.8802, 0.0012],\n          [0.5936, 0.4158]]]])"
  },
  {
    "objectID": "ml/wb_torchtensors_slides.html#creating-tensors-4",
    "href": "ml/wb_torchtensors_slides.html#creating-tensors-4",
    "title": "Everything you wanted to know (& more) about PyTorch tensors",
    "section": "Creating tensors",
    "text": "Creating tensors\ntorch.rand(2)\ntorch.rand(3)\ntorch.rand(1, 1)\ntorch.rand(1, 1, 1)\ntorch.rand(2, 6)\n\n[Out]\n\ntensor([0.7682, 0.0885])\ntensor([0.1320, 0.3074, 0.6341])\ntensor([[0.4901]])\ntensor([[[0.8964]]])\ntensor([[0.4556, 0.6323, 0.3489, 0.4017, 0.0223, 0.1689],\n        [0.2939, 0.5185, 0.6977, 0.8000, 0.1610, 0.2823]])"
  },
  {
    "objectID": "ml/wb_torchtensors_slides.html#creating-tensors-5",
    "href": "ml/wb_torchtensors_slides.html#creating-tensors-5",
    "title": "Everything you wanted to know (& more) about PyTorch tensors",
    "section": "Creating tensors",
    "text": "Creating tensors\ntorch.rand(2, 4, dtype=torch.float64)  # You can set dtype\ntorch.ones(2, 1, 4, 5)\n\n[Out]\n\ntensor([[0.6650, 0.7849, 0.2104, 0.6767],\n        [0.1097, 0.5238, 0.2260, 0.5582]], dtype=torch.float64)\ntensor([[[[1., 1., 1., 1., 1.],\n          [1., 1., 1., 1., 1.],\n          [1., 1., 1., 1., 1.],\n          [1., 1., 1., 1., 1.]]],\n        [[[1., 1., 1., 1., 1.],\n          [1., 1., 1., 1., 1.],\n          [1., 1., 1., 1., 1.],\n          [1., 1., 1., 1., 1.]]]])"
  },
  {
    "objectID": "ml/wb_torchtensors_slides.html#creating-tensors-6",
    "href": "ml/wb_torchtensors_slides.html#creating-tensors-6",
    "title": "Everything you wanted to know (& more) about PyTorch tensors",
    "section": "Creating tensors",
    "text": "Creating tensors\nt = torch.rand(2, 3); print(t)\ntorch.zeros_like(t)             # Matches the size of t\ntorch.ones_like(t)\ntorch.randn_like(t)\n\n[Out]\n\ntensor([[0.4051, 0.6394, 0.0871],\n        [0.4509, 0.5255, 0.5057]])\ntensor([[0., 0., 0.],\n        [0., 0., 0.]])\ntensor([[1., 1., 1.],\n        [1., 1., 1.]])\ntensor([[-0.3088, -0.0104,  1.0461],\n        [ 0.9233,  0.0236, -2.1217]])"
  },
  {
    "objectID": "ml/wb_torchtensors_slides.html#creating-tensors-7",
    "href": "ml/wb_torchtensors_slides.html#creating-tensors-7",
    "title": "Everything you wanted to know (& more) about PyTorch tensors",
    "section": "Creating tensors",
    "text": "Creating tensors\ntorch.arange(2, 10, 4)    # From 2 to 10 in increments of 4\ntorch.linspace(2, 10, 4)  # 4 elements from 2 to 10 on the linear scale\ntorch.logspace(2, 10, 4)  # Same on the log scale\ntorch.randperm(4)\ntorch.eye(3)\n\n[Out]\n\ntensor([2, 6])\ntensor([2.0000,  4.6667,  7.3333, 10.0000])\ntensor([1.0000e+02, 4.6416e+04, 2.1544e+07, 1.0000e+10])\ntensor([1, 3, 2, 0])\ntensor([[1., 0., 0.],\n        [0., 1., 0.],\n        [0., 0., 1.]])"
  },
  {
    "objectID": "ml/wb_torchtensors_slides.html#tensor-information",
    "href": "ml/wb_torchtensors_slides.html#tensor-information",
    "title": "Everything you wanted to know (& more) about PyTorch tensors",
    "section": "Tensor information",
    "text": "Tensor information\nt = torch.rand(2, 3); print(t)\nt.size()\nt.dim()\nt.numel()\n\n[Out]\n\ntensor([[0.5885, 0.7005, 0.1048],\n        [0.1115, 0.7526, 0.0658]])\ntorch.Size([2, 3])\n2\n6"
  },
  {
    "objectID": "ml/wb_torchtensors_slides.html#tensor-indexing",
    "href": "ml/wb_torchtensors_slides.html#tensor-indexing",
    "title": "Everything you wanted to know (& more) about PyTorch tensors",
    "section": "Tensor indexing",
    "text": "Tensor indexing\nx = torch.rand(3, 4)\nx[:]                 # With a range, the comma is implicit: same as x[:, ]\nx[:, 2]\nx[1, :]\nx[2, 3]\n\n[Out]\n\ntensor([[0.6575, 0.4017, 0.7391, 0.6268],\n        [0.2835, 0.0993, 0.7707, 0.1996],\n        [0.4447, 0.5684, 0.2090, 0.7724]])\ntensor([0.7391, 0.7707, 0.2090])\ntensor([0.2835, 0.0993, 0.7707, 0.1996])\ntensor(0.7724)"
  },
  {
    "objectID": "ml/wb_torchtensors_slides.html#tensor-indexing-1",
    "href": "ml/wb_torchtensors_slides.html#tensor-indexing-1",
    "title": "Everything you wanted to know (& more) about PyTorch tensors",
    "section": "Tensor indexing",
    "text": "Tensor indexing\nx[-1:]        # Last element (implicit comma, so all columns)\nx[-1]         # No range, no implicit comma: we are indexing \n# from a list of tensors, so the result is a one dimensional tensor\n# (Each dimension is a list of tensors of the previous dimension)\nx[-1].size()  # Same number of dimensions than x (2 dimensions)\nx[-1:].size() # We dropped one dimension\n\n[Out]\n\ntensor([[0.8168, 0.0879, 0.2642, 0.3777]])\ntensor([0.8168, 0.0879, 0.2642, 0.3777])\n\ntorch.Size([4])\ntorch.Size([1, 4])"
  },
  {
    "objectID": "ml/wb_torchtensors_slides.html#tensor-indexing-2",
    "href": "ml/wb_torchtensors_slides.html#tensor-indexing-2",
    "title": "Everything you wanted to know (& more) about PyTorch tensors",
    "section": "Tensor indexing",
    "text": "Tensor indexing\nx[0:1]     # Python ranges are inclusive to the left, not the right\nx[:-1]     # From start to one before last (& implicit comma)\nx[0:3:2]   # From 0th (included) to 3rd (excluded) in increment of 2\n\n[Out]\n\ntensor([[0.5873, 0.0225, 0.7234, 0.4538]])\ntensor([[0.5873, 0.0225, 0.7234, 0.4538],\n        [0.9525, 0.0111, 0.6421, 0.4647]])\ntensor([[0.5873, 0.0225, 0.7234, 0.4538],\n        [0.8168, 0.0879, 0.2642, 0.3777]])"
  },
  {
    "objectID": "ml/wb_torchtensors_slides.html#tensor-indexing-3",
    "href": "ml/wb_torchtensors_slides.html#tensor-indexing-3",
    "title": "Everything you wanted to know (& more) about PyTorch tensors",
    "section": "Tensor indexing",
    "text": "Tensor indexing\nx[None]          # Adds a dimension of size one as the 1st dimension\nx.size()\nx[None].size()\n\n[Out]\n\ntensor([[[0.5873, 0.0225, 0.7234, 0.4538],\n         [0.9525, 0.0111, 0.6421, 0.4647],\n         [0.8168, 0.0879, 0.2642, 0.3777]]])\ntorch.Size([3, 4])\ntorch.Size([1, 3, 4])"
  },
  {
    "objectID": "ml/wb_torchtensors_slides.html#a-word-of-caution-about-indexing",
    "href": "ml/wb_torchtensors_slides.html#a-word-of-caution-about-indexing",
    "title": "Everything you wanted to know (& more) about PyTorch tensors",
    "section": "A word of caution about indexing",
    "text": "A word of caution about indexing\nWhile indexing elements of a tensor to extract some of the data as a final step of some computation is fine, you should not use indexing to run operations on tensor elements in a loop as this would be extremely inefficient\nInstead, you want to use vectorized operations"
  },
  {
    "objectID": "ml/wb_torchtensors_slides.html#vectorized-operations",
    "href": "ml/wb_torchtensors_slides.html#vectorized-operations",
    "title": "Everything you wanted to know (& more) about PyTorch tensors",
    "section": "Vectorized operations",
    "text": "Vectorized operations\nSince PyTorch tensors are homogeneous (i.e. made of a single data type), as with NumPy’s ndarrays, operations are vectorized & thus staggeringly fast\nNumPy is mostly written in C & PyTorch in C++. With either library, when you run vectorized operations on arrays/tensors, you don’t use raw Python (slow) but compiled C/C++ code (much faster)\nHere is an excellent post explaining Python vectorization & why it makes such a big difference"
  },
  {
    "objectID": "ml/wb_torchtensors_slides.html#vectorized-operations-comparison",
    "href": "ml/wb_torchtensors_slides.html#vectorized-operations-comparison",
    "title": "Everything you wanted to know (& more) about PyTorch tensors",
    "section": "Vectorized operations: comparison",
    "text": "Vectorized operations: comparison\nRaw Python method\n# Create tensor. We use float64 here to avoid truncation errors\nt = torch.rand(10**6, dtype=torch.float64)\n# Initialize the sum\nsum = 0\n# Run loop\nfor i in range(len(t)): sum += t[i]\n# Print result\nprint(sum)\nVectorized function\nt.sum()"
  },
  {
    "objectID": "ml/wb_torchtensors_slides.html#vectorized-operations-comparison-1",
    "href": "ml/wb_torchtensors_slides.html#vectorized-operations-comparison-1",
    "title": "Everything you wanted to know (& more) about PyTorch tensors",
    "section": "Vectorized operations: comparison",
    "text": "Vectorized operations: comparison\nBoth methods give the same result\n\nThis is why we used float64:\nWhile the accuracy remains excellent with float32 if we use the PyTorch function torch.sum(), the raw Python loop gives a fairly inaccurate result\n\n\n[Out]\n\ntensor(500023.0789, dtype=torch.float64)\ntensor(500023.0789, dtype=torch.float64)"
  },
  {
    "objectID": "ml/wb_torchtensors_slides.html#vectorized-operations-timing",
    "href": "ml/wb_torchtensors_slides.html#vectorized-operations-timing",
    "title": "Everything you wanted to know (& more) about PyTorch tensors",
    "section": "Vectorized operations: timing",
    "text": "Vectorized operations: timing\nLet’s compare the timing with PyTorch built-in benchmark utility\n# Load utility\nimport torch.utils.benchmark as benchmark\n\n# Create a function for our loop\ndef sum_loop(t, sum):\n    for i in range(len(t)): sum += t[i]"
  },
  {
    "objectID": "ml/wb_torchtensors_slides.html#vectorized-operations-timing-1",
    "href": "ml/wb_torchtensors_slides.html#vectorized-operations-timing-1",
    "title": "Everything you wanted to know (& more) about PyTorch tensors",
    "section": "Vectorized operations: timing",
    "text": "Vectorized operations: timing\nNow we can create the timers\nt0 = benchmark.Timer(\n    stmt='sum_loop(t, sum)',\n    setup='from __main__ import sum_loop',\n    globals={'t': t, 'sum': sum})\n\nt1 = benchmark.Timer(\n    stmt='t.sum()',\n    globals={'t': t})"
  },
  {
    "objectID": "ml/wb_torchtensors_slides.html#vectorized-operations-timing-2",
    "href": "ml/wb_torchtensors_slides.html#vectorized-operations-timing-2",
    "title": "Everything you wanted to know (& more) about PyTorch tensors",
    "section": "Vectorized operations: timing",
    "text": "Vectorized operations: timing\nLet’s time 100 runs to have a reliable benchmark\nprint(t0.timeit(100))\nprint(t1.timeit(100))\n\nI ran the code on my laptop with a dedicated GPU & 32GB RAM"
  },
  {
    "objectID": "ml/wb_torchtensors_slides.html#vectorized-operations-timing-3",
    "href": "ml/wb_torchtensors_slides.html#vectorized-operations-timing-3",
    "title": "Everything you wanted to know (& more) about PyTorch tensors",
    "section": "Vectorized operations: timing",
    "text": "Vectorized operations: timing\nTiming of raw Python loop\nsum_loop(t, sum)\nsetup: from __main__ import sum_loop\n  1.37 s\n  1 measurement, 100 runs , 1 thread\nTiming of vectorized function\nt.sum()\n  191.26 us\n  1 measurement, 100 runs , 1 thread"
  },
  {
    "objectID": "ml/wb_torchtensors_slides.html#vectorized-operations-timing-4",
    "href": "ml/wb_torchtensors_slides.html#vectorized-operations-timing-4",
    "title": "Everything you wanted to know (& more) about PyTorch tensors",
    "section": "Vectorized operations: timing",
    "text": "Vectorized operations: timing\nSpeedup:\n1.37/(191.26 * 10**-6) = 7163\n\nThe vectorized function runs more than 7,000 times faster!!!"
  },
  {
    "objectID": "ml/wb_torchtensors_slides.html#even-more-important-on-gpus",
    "href": "ml/wb_torchtensors_slides.html#even-more-important-on-gpus",
    "title": "Everything you wanted to know (& more) about PyTorch tensors",
    "section": "Even more important on GPUs",
    "text": "Even more important on GPUs\nWe will talk about GPUs in detail later\nTiming of raw Python loop on GPU (actually slower on GPU!)\nsum_loop(t, sum)\nsetup: from __main__ import sum_loop\n  4.54 s\n  1 measurement, 100 runs , 1 thread\nTiming of vectorized function on GPU (here we do get a speedup)\nt.sum()\n  50.62 us\n  1 measurement, 100 runs , 1 thread"
  },
  {
    "objectID": "ml/wb_torchtensors_slides.html#even-more-important-on-gpus-1",
    "href": "ml/wb_torchtensors_slides.html#even-more-important-on-gpus-1",
    "title": "Everything you wanted to know (& more) about PyTorch tensors",
    "section": "Even more important on GPUs",
    "text": "Even more important on GPUs\nSpeedup:\n4.54/(50.62 * 10**-6) = 89688\n\nOn GPUs, it is even more important not to index repeatedly from a tensor\n\n\nOn GPUs, the vectorized function runs almost 90,000 times faster!!!"
  },
  {
    "objectID": "ml/wb_torchtensors_slides.html#simple-mathematical-operations",
    "href": "ml/wb_torchtensors_slides.html#simple-mathematical-operations",
    "title": "Everything you wanted to know (& more) about PyTorch tensors",
    "section": "Simple mathematical operations",
    "text": "Simple mathematical operations\nt1 = torch.arange(1, 5).view(2, 2); print(t1)\nt2 = torch.tensor([[1, 1], [0, 0]]); print(t2)\nt1 + t2 # Operation performed between elements at corresponding locations\nt1 + 1  # Operation applied to each element of the tensor\n\n[Out]\n\ntensor([[1, 2],\n        [3, 4]])\ntensor([[1, 1],\n        [0, 0]])\ntensor([[2, 3],\n        [3, 4]])\ntensor([[2, 3],\n        [4, 5]])"
  },
  {
    "objectID": "ml/wb_torchtensors_slides.html#reduction",
    "href": "ml/wb_torchtensors_slides.html#reduction",
    "title": "Everything you wanted to know (& more) about PyTorch tensors",
    "section": "Reduction",
    "text": "Reduction\nt = torch.ones(2, 3, 4); print(t)\nt.sum()   # Reduction over all entries\n\n[Out]\n\ntensor([[[1., 1., 1., 1.],\n         [1., 1., 1., 1.],\n         [1., 1., 1., 1.]],\n        [[1., 1., 1., 1.],\n         [1., 1., 1., 1.],\n         [1., 1., 1., 1.]]])\ntensor(24.)\n\nOther reduction functions (e.g. mean) behave the same way"
  },
  {
    "objectID": "ml/wb_torchtensors_slides.html#reduction-1",
    "href": "ml/wb_torchtensors_slides.html#reduction-1",
    "title": "Everything you wanted to know (& more) about PyTorch tensors",
    "section": "Reduction",
    "text": "Reduction\n# Reduction over a specific dimension\nt.sum(0)  \nt.sum(1)\nt.sum(2)\n\n[Out]\n\ntensor([[2., 2., 2., 2.],\n        [2., 2., 2., 2.],\n        [2., 2., 2., 2.]])\ntensor([[3., 3., 3., 3.],\n        [3., 3., 3., 3.]])\ntensor([[4., 4., 4.],\n        [4., 4., 4.]])"
  },
  {
    "objectID": "ml/wb_torchtensors_slides.html#reduction-2",
    "href": "ml/wb_torchtensors_slides.html#reduction-2",
    "title": "Everything you wanted to know (& more) about PyTorch tensors",
    "section": "Reduction",
    "text": "Reduction\n# Reduction over multiple dimensions\nt.sum((0, 1))\nt.sum((0, 2))\nt.sum((1, 2))\n\n[Out]\n\ntensor([6., 6., 6., 6.])\ntensor([8., 8., 8.])\ntensor([12., 12.])"
  },
  {
    "objectID": "ml/wb_torchtensors_slides.html#in-place-operations",
    "href": "ml/wb_torchtensors_slides.html#in-place-operations",
    "title": "Everything you wanted to know (& more) about PyTorch tensors",
    "section": "In-place operations",
    "text": "In-place operations\nWith operators post-fixed with _:\nt1 = torch.tensor([1, 2]); print(t1)\nt2 = torch.tensor([1, 1]); print(t2)\nt1.add_(t2); print(t1)\nt1.zero_(); print(t1)\n\n[Out]\n\ntensor([1, 2])\ntensor([1, 1])\ntensor([2, 3])\ntensor([0, 0])"
  },
  {
    "objectID": "ml/wb_torchtensors_slides.html#in-place-operations-vs-reassignments",
    "href": "ml/wb_torchtensors_slides.html#in-place-operations-vs-reassignments",
    "title": "Everything you wanted to know (& more) about PyTorch tensors",
    "section": "In-place operations vs reassignments",
    "text": "In-place operations vs reassignments\nt1 = torch.ones(1); t1, hex(id(t1))\nt1.add_(1); t1, hex(id(t1))        # In-place operation: same address\nt1 = t1.add(1); t1, hex(id(t1))    # Reassignment: new address in memory\nt1 = t1 + 1; t1, hex(id(t1))       # Reassignment: new address in memory\n\n[Out]\n\n(tensor([1.]), '0x7fc61accc3b0')\n(tensor([2.]), '0x7fc61accc3b0')\n(tensor([3.]), '0x7fc61accc5e0')\n(tensor([4.]), '0x7fc61accc6d0')"
  },
  {
    "objectID": "ml/wb_torchtensors_slides.html#tensor-views",
    "href": "ml/wb_torchtensors_slides.html#tensor-views",
    "title": "Everything you wanted to know (& more) about PyTorch tensors",
    "section": "Tensor views",
    "text": "Tensor views\nt = torch.tensor([[1, 2, 3], [4, 5, 6]]); print(t)\nt.size()\nt.view(6)\nt.view(3, 2)\nt.view(3, -1) # Same: with -1, the size is inferred from other dimensions\n\n[Out]\n\ntensor([[1, 2, 3],\n        [4, 5, 6]])\ntorch.Size([2, 3])\ntensor([1, 2, 3, 4, 5, 6])\ntensor([[1, 2],\n        [3, 4],\n        [5, 6]])"
  },
  {
    "objectID": "ml/wb_torchtensors_slides.html#note-the-difference",
    "href": "ml/wb_torchtensors_slides.html#note-the-difference",
    "title": "Everything you wanted to know (& more) about PyTorch tensors",
    "section": "Note the difference",
    "text": "Note the difference\nt1 = torch.tensor([[1, 2, 3], [4, 5, 6]]); print(t1)\nt2 = t1.t(); print(t2)\nt3 = t1.view(3, 2); print(t3)\n\n[Out]\n\ntensor([[1, 2, 3],\n        [4, 5, 6]])\ntensor([[1, 4],\n        [2, 5],\n        [3, 6]])\ntensor([[1, 2],\n        [3, 4],\n        [5, 6]])"
  },
  {
    "objectID": "ml/wb_torchtensors_slides.html#logical-operations",
    "href": "ml/wb_torchtensors_slides.html#logical-operations",
    "title": "Everything you wanted to know (& more) about PyTorch tensors",
    "section": "Logical operations",
    "text": "Logical operations\nt1 = torch.randperm(5); print(t1)\nt2 = torch.randperm(5); print(t2)\nt1 &gt; 3                            # Test each element\nt1 &lt; t2                           # Test corresponding pairs of elements\n\n[Out]\n\ntensor([4, 1, 0, 2, 3])\ntensor([0, 4, 2, 1, 3])\ntensor([ True, False, False, False, False])\ntensor([False,  True,  True, False, False])"
  },
  {
    "objectID": "ml/wb_torchtensors_slides.html#outline-5",
    "href": "ml/wb_torchtensors_slides.html#outline-5",
    "title": "Everything you wanted to know (& more) about PyTorch tensors",
    "section": "Outline",
    "text": "Outline\n\nWhat is a PyTorch tensor?\nMemory storage\nData type (dtype)\nBasic operations\nWorking with NumPy\nLinear algebra\nHarvesting the power of GPUs\nDistributed operations"
  },
  {
    "objectID": "ml/wb_torchtensors_slides.html#conversion-without-copy",
    "href": "ml/wb_torchtensors_slides.html#conversion-without-copy",
    "title": "Everything you wanted to know (& more) about PyTorch tensors",
    "section": "Conversion without copy",
    "text": "Conversion without copy\nPyTorch tensors can be converted to NumPy ndarrays & vice-versa in a very efficient manner as both objects share the same memory\nt = torch.rand(2, 3); print(t)\nt_np = t.numpy(); print(t_np)      # From PyTorch tensor to NumPy ndarray\n\n[Out]\n\ntensor([[0.8434, 0.0876, 0.7507],\n        [0.1457, 0.3638, 0.0563]])   # PyTorch Tensor\n\n[[0.84344184 0.08764815 0.7506627 ]\n [0.14567494 0.36384273 0.05629885]] # NumPy ndarray"
  },
  {
    "objectID": "ml/wb_torchtensors_slides.html#mind-the-different-defaults",
    "href": "ml/wb_torchtensors_slides.html#mind-the-different-defaults",
    "title": "Everything you wanted to know (& more) about PyTorch tensors",
    "section": "Mind the different defaults",
    "text": "Mind the different defaults\nt_np.dtype\n\n[Out]\n\ndtype('float32')\n\nRemember that PyTorch tensors use 32-bit floating points by default\n(because this is what you want in neural networks)\n\n\nBut NumPy defaults to 64-bit\nDepending on your workflow, you might have to change dtype"
  },
  {
    "objectID": "ml/wb_torchtensors_slides.html#from-numpy-to-pytorch",
    "href": "ml/wb_torchtensors_slides.html#from-numpy-to-pytorch",
    "title": "Everything you wanted to know (& more) about PyTorch tensors",
    "section": "From NumPy to PyTorch",
    "text": "From NumPy to PyTorch\nimport numpy as np\na = np.random.rand(2, 3); print(a)\na_pt = torch.from_numpy(a); print(a_pt)    # From ndarray to tensor\n\n[Out]\n\n[[0.55892276 0.06026952 0.72496545]\n [0.65659463 0.27697739 0.29141587]]\n\ntensor([[0.5589, 0.0603, 0.7250],\n        [0.6566, 0.2770, 0.2914]], dtype=torch.float64)\n\nHere again, you might have to change dtype"
  },
  {
    "objectID": "ml/wb_torchtensors_slides.html#notes-about-conversion-without-copy",
    "href": "ml/wb_torchtensors_slides.html#notes-about-conversion-without-copy",
    "title": "Everything you wanted to know (& more) about PyTorch tensors",
    "section": "Notes about conversion without copy",
    "text": "Notes about conversion without copy\nt & t_np are objects of different Python types, so, as far as Python is concerned, they have different addresses\nid(t) == id(t_np)\n\n[Out]\n\nFalse"
  },
  {
    "objectID": "ml/wb_torchtensors_slides.html#notes-about-conversion-without-copy-1",
    "href": "ml/wb_torchtensors_slides.html#notes-about-conversion-without-copy-1",
    "title": "Everything you wanted to know (& more) about PyTorch tensors",
    "section": "Notes about conversion without copy",
    "text": "Notes about conversion without copy\nHowever—that’s quite confusing—they share an underlying C array in memory & modifying one in-place also modifies the other\nt.zero_()\nprint(t_np)\n\n[Out]\n\ntensor([[0., 0., 0.],\n        [0., 0., 0.]])\n\n[[0. 0. 0.]\n [0. 0. 0.]]"
  },
  {
    "objectID": "ml/wb_torchtensors_slides.html#notes-about-conversion-without-copy-2",
    "href": "ml/wb_torchtensors_slides.html#notes-about-conversion-without-copy-2",
    "title": "Everything you wanted to know (& more) about PyTorch tensors",
    "section": "Notes about conversion without copy",
    "text": "Notes about conversion without copy\nLastly, as NumPy only works on CPU, to convert a PyTorch tensor allocated to the GPU, the content will have to be copied to the CPU first"
  },
  {
    "objectID": "ml/wb_torchtensors_slides.html#outline-6",
    "href": "ml/wb_torchtensors_slides.html#outline-6",
    "title": "Everything you wanted to know (& more) about PyTorch tensors",
    "section": "Outline",
    "text": "Outline\n\nWhat is a PyTorch tensor?\nMemory storage\nData type (dtype)\nBasic operations\nWorking with NumPy\nLinear algebra\nHarvesting the power of GPUs\nDistributed operations"
  },
  {
    "objectID": "ml/wb_torchtensors_slides.html#torch.linalg-module",
    "href": "ml/wb_torchtensors_slides.html#torch.linalg-module",
    "title": "Everything you wanted to know (& more) about PyTorch tensors",
    "section": "torch.linalg module",
    "text": "torch.linalg module\n\nAll functions from numpy.linalg implemented (with accelerator & automatic differentiation support)\nSome additional functions\n\n\nRequires torch &gt;= 1.9\nLinear algebra support was less developed before the introduction of this module"
  },
  {
    "objectID": "ml/wb_torchtensors_slides.html#system-of-linear-equations-solver",
    "href": "ml/wb_torchtensors_slides.html#system-of-linear-equations-solver",
    "title": "Everything you wanted to know (& more) about PyTorch tensors",
    "section": "System of linear equations solver",
    "text": "System of linear equations solver\nLet’s have a look at an extremely basic example:\n2x + 3y - z = 5\nx - 2y + 8z = 21\n6x + y - 3z = -1\nWe are looking for the values of x, y, & z that would satisfy this system"
  },
  {
    "objectID": "ml/wb_torchtensors_slides.html#system-of-linear-equations-solver-1",
    "href": "ml/wb_torchtensors_slides.html#system-of-linear-equations-solver-1",
    "title": "Everything you wanted to know (& more) about PyTorch tensors",
    "section": "System of linear equations solver",
    "text": "System of linear equations solver\nWe create a 2D tensor A of size (3, 3) with the coefficients of the equations\nand a 1D tensor b of size 3 with the right hand sides values of the equations\nA = torch.tensor([[2., 3., -1.], [1., -2., 8.], [6., 1., -3.]]); print(A)\nb = torch.tensor([5., 21., -1.]); print(b)\n\n[Out]\n\ntensor([[ 2.,  3., -1.],\n        [ 1., -2.,  8.],\n        [ 6.,  1., -3.]])\ntensor([ 5., 21., -1.])"
  },
  {
    "objectID": "ml/wb_torchtensors_slides.html#system-of-linear-equations-solver-2",
    "href": "ml/wb_torchtensors_slides.html#system-of-linear-equations-solver-2",
    "title": "Everything you wanted to know (& more) about PyTorch tensors",
    "section": "System of linear equations solver",
    "text": "System of linear equations solver\nSolving this system is as simple as running the torch.linalg.solve function:\nx = torch.linalg.solve(A, b); print(x)\n\n[Out]\n\ntensor([1., 2., 3.])\nOur solution is:\nx = 1\ny = 2\nz = 3"
  },
  {
    "objectID": "ml/wb_torchtensors_slides.html#verify-our-result",
    "href": "ml/wb_torchtensors_slides.html#verify-our-result",
    "title": "Everything you wanted to know (& more) about PyTorch tensors",
    "section": "Verify our result",
    "text": "Verify our result\ntorch.allclose(A @ x, b)\n\n[Out]\n\nTrue"
  },
  {
    "objectID": "ml/wb_torchtensors_slides.html#system-of-linear-equations-solver-3",
    "href": "ml/wb_torchtensors_slides.html#system-of-linear-equations-solver-3",
    "title": "Everything you wanted to know (& more) about PyTorch tensors",
    "section": "System of linear equations solver",
    "text": "System of linear equations solver\nHere is another simple example:\n# Create a square normal random matrix\nA = torch.randn(4, 4); print(A)\n# Create a tensor of right hand side values\nb = torch.randn(4); print(b)\n\n# Solve the system\nx = torch.linalg.solve(A, b); print(x)\n\n# Verify\ntorch.allclose(A @ x, b)"
  },
  {
    "objectID": "ml/wb_torchtensors_slides.html#system-of-linear-equations-solver-4",
    "href": "ml/wb_torchtensors_slides.html#system-of-linear-equations-solver-4",
    "title": "Everything you wanted to know (& more) about PyTorch tensors",
    "section": "System of linear equations solver",
    "text": "System of linear equations solver\n\n[Out]\n\ntensor([[ 1.5091,  2.0820,  1.7067,  2.3804], # A (coefficients)\n        [-1.1256, -0.3170, -1.0925, -0.0852],\n        [ 0.3276, -0.7607, -1.5991,  0.0185],\n        [-0.7504,  0.1854,  0.6211,  0.6382]])\n\ntensor([-1.0886, -0.2666,  0.1894, -0.2190])  # b (right hand side values)\n\ntensor([ 0.1992, -0.7011,  0.2541, -0.1526])  # x (our solution)\n\nTrue                                          # Verification"
  },
  {
    "objectID": "ml/wb_torchtensors_slides.html#with-2-multidimensional-tensors",
    "href": "ml/wb_torchtensors_slides.html#with-2-multidimensional-tensors",
    "title": "Everything you wanted to know (& more) about PyTorch tensors",
    "section": "With 2 multidimensional tensors",
    "text": "With 2 multidimensional tensors\nA = torch.randn(2, 3, 3)              # Must be batches of square matrices\nB = torch.randn(2, 3, 5)              # Dimensions must be compatible\nX = torch.linalg.solve(A, B); print(X)\ntorch.allclose(A @ X, B)\n\n[Out]\n\ntensor([[[-0.0545, -0.1012,  0.7863, -0.0806, -0.0191],\n         [-0.9846, -0.0137, -1.7521, -0.4579, -0.8178],\n         [-1.9142, -0.6225, -1.9239, -0.6972,  0.7011]],\n        [[ 3.2094,  0.3432, -1.6604, -0.7885,  0.0088],\n         [ 7.9852,  1.4605, -1.7037, -0.7713,  2.7319],\n         [-4.1979,  0.0849,  1.0864,  0.3098, -1.0347]]])\nTrue"
  },
  {
    "objectID": "ml/wb_torchtensors_slides.html#matrix-inversions",
    "href": "ml/wb_torchtensors_slides.html#matrix-inversions",
    "title": "Everything you wanted to know (& more) about PyTorch tensors",
    "section": "Matrix inversions",
    "text": "Matrix inversions\n\n\nIt is faster & more numerically stable to solve a system of linear equations directly than to compute the inverse matrix first\n\n\n\nLimit matrix inversions to situations where it is truly necessary"
  },
  {
    "objectID": "ml/wb_torchtensors_slides.html#matrix-inversions-1",
    "href": "ml/wb_torchtensors_slides.html#matrix-inversions-1",
    "title": "Everything you wanted to know (& more) about PyTorch tensors",
    "section": "Matrix inversions",
    "text": "Matrix inversions\nA = torch.rand(2, 3, 3)      # Batch of square matrices\nA_inv = torch.linalg.inv(A)  # Batch of inverse matrices\nA @ A_inv                    # Batch of identity matrices\n\n[Out]\n\ntensor([[[ 1.0000e+00, -6.0486e-07,  1.3859e-06],\n         [ 5.5627e-08,  1.0000e+00,  1.0795e-06],\n         [-1.4133e-07,  7.9992e-08,  1.0000e+00]],\n        [[ 1.0000e+00,  4.3329e-08, -3.6741e-09],\n         [-7.4627e-08,  1.0000e+00,  1.4579e-07],\n         [-6.3580e-08,  8.2354e-08,  1.0000e+00]]])"
  },
  {
    "objectID": "ml/wb_torchtensors_slides.html#other-linear-algebra-functions",
    "href": "ml/wb_torchtensors_slides.html#other-linear-algebra-functions",
    "title": "Everything you wanted to know (& more) about PyTorch tensors",
    "section": "Other linear algebra functions",
    "text": "Other linear algebra functions\ntorch.linalg contains many more functions:\n\ntorch.tensordot which generalizes matrix products\ntorch.linalg.tensorsolve which computes the solution X to the system torch.tensordot(A, X) = B\ntorch.linalg.eigvals which computes the eigenvalues of a square matrix\nand more"
  },
  {
    "objectID": "ml/wb_torchtensors_slides.html#outline-7",
    "href": "ml/wb_torchtensors_slides.html#outline-7",
    "title": "Everything you wanted to know (& more) about PyTorch tensors",
    "section": "Outline",
    "text": "Outline\n\nWhat is a PyTorch tensor?\nMemory storage\nData type (dtype)\nBasic operations\nWorking with NumPy\nLinear algebra\nHarvesting the power of GPUs\nDistributed operations"
  },
  {
    "objectID": "ml/wb_torchtensors_slides.html#device-attribute",
    "href": "ml/wb_torchtensors_slides.html#device-attribute",
    "title": "Everything you wanted to know (& more) about PyTorch tensors",
    "section": "Device attribute",
    "text": "Device attribute\nTensor data can be placed in the memory of various processor types:\n\nthe RAM of CPU\nthe RAM of a GPU with CUDA support\nthe RAM of a GPU with AMD’s ROCm support\nthe RAM of an XLA device (e.g. Cloud TPU) with the torch_xla package"
  },
  {
    "objectID": "ml/wb_torchtensors_slides.html#device-attribute-1",
    "href": "ml/wb_torchtensors_slides.html#device-attribute-1",
    "title": "Everything you wanted to know (& more) about PyTorch tensors",
    "section": "Device attribute",
    "text": "Device attribute\nThe values for the device attributes are:\n\nCPU:  'cpu'\nGPU (CUDA & AMD’s ROCm):  'cuda'\nXLA:  xm.xla_device()\n\nThis last option requires to load the torch_xla package first:\nimport torch_xla\nimport torch_xla.core.xla_model as xm"
  },
  {
    "objectID": "ml/wb_torchtensors_slides.html#creating-a-tensor-on-a-specific-device",
    "href": "ml/wb_torchtensors_slides.html#creating-a-tensor-on-a-specific-device",
    "title": "Everything you wanted to know (& more) about PyTorch tensors",
    "section": "Creating a tensor on a specific device",
    "text": "Creating a tensor on a specific device\nBy default, tensors are created on the CPU\nt1 = torch.rand(2); print(t1)\n\n[Out]\n\ntensor([0.1606, 0.9771])  # Implicit: device='cpu'\n\nPrinted tensors only display attributes with values ≠ default values"
  },
  {
    "objectID": "ml/wb_torchtensors_slides.html#creating-a-tensor-on-a-specific-device-1",
    "href": "ml/wb_torchtensors_slides.html#creating-a-tensor-on-a-specific-device-1",
    "title": "Everything you wanted to know (& more) about PyTorch tensors",
    "section": "Creating a tensor on a specific device",
    "text": "Creating a tensor on a specific device\nYou can create a tensor on an accelerator by specifying the device attribute\nt2_gpu = torch.rand(2, device='cuda'); print(t2_gpu)\n\n[Out]\n\ntensor([0.0664, 0.7829], device='cuda:0')  # :0 means the 1st GPU"
  },
  {
    "objectID": "ml/wb_torchtensors_slides.html#copying-a-tensor-to-a-specific-device",
    "href": "ml/wb_torchtensors_slides.html#copying-a-tensor-to-a-specific-device",
    "title": "Everything you wanted to know (& more) about PyTorch tensors",
    "section": "Copying a tensor to a specific device",
    "text": "Copying a tensor to a specific device\nYou can also make copies of a tensor on other devices\n# Make a copy of t1 on the GPU\nt1_gpu = t1.to(device='cuda'); print(t1_gpu)\nt1_gpu = t1.cuda()  # Same as above written differently\n\n# Make a copy of t2_gpu on the CPU\nt2 = t2_gpu.to(device='cpu'); print(t2)\nt2 = t2_gpu.cpu()   # For the altenative form\n\n[Out]\n\ntensor([0.1606, 0.9771], device='cuda:0')\ntensor([0.0664, 0.7829]) # Implicit: device='cpu'"
  },
  {
    "objectID": "ml/wb_torchtensors_slides.html#multiple-gpus",
    "href": "ml/wb_torchtensors_slides.html#multiple-gpus",
    "title": "Everything you wanted to know (& more) about PyTorch tensors",
    "section": "Multiple GPUs",
    "text": "Multiple GPUs\nIf you have multiple GPUs, you can optionally specify which one a tensor should be created on or copied to\nt3_gpu = torch.rand(2, device='cuda:0')  # Create a tensor on 1st GPU\nt4_gpu = t1.to(device='cuda:0')          # Make a copy of t1 on 1st GPU\nt5_gpu = t1.to(device='cuda:1')          # Make a copy of t1 on 2nd GPU\n\nOr the equivalent short forms for the last two:\nt4_gpu = t1.cuda(0)\nt5_gpu = t1.cuda(1)"
  },
  {
    "objectID": "ml/wb_torchtensors_slides.html#timing",
    "href": "ml/wb_torchtensors_slides.html#timing",
    "title": "Everything you wanted to know (& more) about PyTorch tensors",
    "section": "Timing",
    "text": "Timing\nLet’s compare the timing of some matrix multiplications on CPU & GPU with PyTorch built-in benchmark utility\n# Load utility\nimport torch.utils.benchmark as benchmark\n# Define tensors on the CPU\nA = torch.randn(500, 500)\nB = torch.randn(500, 500)\n# Define tensors on the GPU\nA_gpu = torch.randn(500, 500, device='cuda')\nB_gpu = torch.randn(500, 500, device='cuda')\n\nI ran the code on my laptop with a dedicated GPU & 32GB RAM"
  },
  {
    "objectID": "ml/wb_torchtensors_slides.html#timing-1",
    "href": "ml/wb_torchtensors_slides.html#timing-1",
    "title": "Everything you wanted to know (& more) about PyTorch tensors",
    "section": "Timing",
    "text": "Timing\nLet’s time 100 runs to have a reliable benchmark\nt0 = benchmark.Timer(\n    stmt='A @ B',\n    globals={'A': A, 'B': B})\n\nt1 = benchmark.Timer(\n    stmt='A_gpu @ B_gpu',\n    globals={'A_gpu': A_gpu, 'B_gpu': B_gpu})\n\nprint(t0.timeit(100))\nprint(t1.timeit(100))"
  },
  {
    "objectID": "ml/wb_torchtensors_slides.html#timing-2",
    "href": "ml/wb_torchtensors_slides.html#timing-2",
    "title": "Everything you wanted to know (& more) about PyTorch tensors",
    "section": "Timing",
    "text": "Timing\n\n[Out]\n\nA @ B\n  2.29 ms\n  1 measurement, 100 runs , 1 thread\n\nA_gpu @ B_gpu\n  108.02 us\n  1 measurement, 100 runs , 1 thread\nSpeedup:\n(2.29 * 10**-3)/(108.02 * 10**-6) = 21\nThis computation was 21 times faster on my GPU than on CPU"
  },
  {
    "objectID": "ml/wb_torchtensors_slides.html#timing-3",
    "href": "ml/wb_torchtensors_slides.html#timing-3",
    "title": "Everything you wanted to know (& more) about PyTorch tensors",
    "section": "Timing",
    "text": "Timing\nBy replacing 500 with 5000, we get:\nA @ B\n  2.21 s\n  1 measurement, 100 runs , 1 thread\n\nA_gpu @ B_gpu\n  57.88 ms\n  1 measurement, 100 runs , 1 thread\nSpeedup:\n2.21/(57.88 * 10**-3) = 38\nThe larger the computation, the greater the benefit: now 38 times faster"
  },
  {
    "objectID": "ml/wb_torchtensors_slides.html#outline-8",
    "href": "ml/wb_torchtensors_slides.html#outline-8",
    "title": "Everything you wanted to know (& more) about PyTorch tensors",
    "section": "Outline",
    "text": "Outline\n\nWhat is a PyTorch tensor?\nMemory storage\nData type (dtype)\nBasic operations\nWorking with NumPy\nLinear algebra\nHarvesting the power of GPUs\nDistributed operations"
  },
  {
    "objectID": "ml/wb_torchtensors_slides.html#parallel-tensor-operations",
    "href": "ml/wb_torchtensors_slides.html#parallel-tensor-operations",
    "title": "Everything you wanted to know (& more) about PyTorch tensors",
    "section": "Parallel tensor operations",
    "text": "Parallel tensor operations\nPyTorch already allows for distributed training of ML models\nThe implementation of distributed tensor operations—for instance for linear algebra—is in the work through the use of a ShardedTensor primitive that can be sharded across nodes\nSee also this issue for more comments about upcoming developments on (among other things) tensor sharding"
  },
  {
    "objectID": "ml/wb_flux.html",
    "href": "ml/wb_flux.html",
    "title": "Machine learning in Julia with Flux",
    "section": "",
    "text": "Find this webinar in the Julia section."
  },
  {
    "objectID": "ml/top_pt.html",
    "href": "ml/top_pt.html",
    "title": "Getting started with PyTorch",
    "section": "",
    "text": "This introductory course to deep learning and neural networks with the PyTorch framework does not assume any prior knowledge in machine learning. Some basic Python is useful, but not strictly necessary.\n\n Start course ➤"
  },
  {
    "objectID": "ml/sk_workflow.html",
    "href": "ml/sk_workflow.html",
    "title": "Sklearn workflow",
    "section": "",
    "text": "Scikit-learn has a very clean and consistent API, making it very easy to use: a similar workflow can be applied to most techniques. Let’s go over two examples.\nThis code was modified from Matthew Greenberg."
  },
  {
    "objectID": "ml/sk_workflow.html#load-packages",
    "href": "ml/sk_workflow.html#load-packages",
    "title": "Sklearn workflow",
    "section": "Load packages",
    "text": "Load packages\n\nfrom sklearn.datasets import fetch_california_housing, load_breast_cancer\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, accuracy_score\n\nimport pandas as pd\n\nimport matplotlib\nfrom matplotlib import pyplot as plt\n\nimport numpy as np\n\nfrom collections import Counter"
  },
  {
    "objectID": "ml/sk_workflow.html#example-1-california-housing-dataset",
    "href": "ml/sk_workflow.html#example-1-california-housing-dataset",
    "title": "Sklearn workflow",
    "section": "Example 1: California housing dataset",
    "text": "Example 1: California housing dataset\n\nLoad and explore the data\n\ncal_housing = fetch_california_housing()\ntype(cal_housing)\n\nsklearn.utils._bunch.Bunch\n\n\nLet’s look at the attributes of cal_housing:\n\ndir(cal_housing)\n\n['DESCR', 'data', 'feature_names', 'frame', 'target', 'target_names']\n\n\n\ncal_housing.feature_names\n\n['MedInc',\n 'HouseAge',\n 'AveRooms',\n 'AveBedrms',\n 'Population',\n 'AveOccup',\n 'Latitude',\n 'Longitude']\n\n\n\nprint(cal_housing.DESCR)\n\n.. _california_housing_dataset:\n\nCalifornia Housing dataset\n--------------------------\n\n**Data Set Characteristics:**\n\n    :Number of Instances: 20640\n\n    :Number of Attributes: 8 numeric, predictive attributes and the target\n\n    :Attribute Information:\n        - MedInc        median income in block group\n        - HouseAge      median house age in block group\n        - AveRooms      average number of rooms per household\n        - AveBedrms     average number of bedrooms per household\n        - Population    block group population\n        - AveOccup      average number of household members\n        - Latitude      block group latitude\n        - Longitude     block group longitude\n\n    :Missing Attribute Values: None\n\nThis dataset was obtained from the StatLib repository.\nhttps://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html\n\nThe target variable is the median house value for California districts,\nexpressed in hundreds of thousands of dollars ($100,000).\n\nThis dataset was derived from the 1990 U.S. census, using one row per census\nblock group. A block group is the smallest geographical unit for which the U.S.\nCensus Bureau publishes sample data (a block group typically has a population\nof 600 to 3,000 people).\n\nA household is a group of people residing within a home. Since the average\nnumber of rooms and bedrooms in this dataset are provided per household, these\ncolumns may take surprisingly large values for block groups with few households\nand many empty houses, such as vacation resorts.\n\nIt can be downloaded/loaded using the\n:func:`sklearn.datasets.fetch_california_housing` function.\n\n.. topic:: References\n\n    - Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\n      Statistics and Probability Letters, 33 (1997) 291-297\n\n\n\n\nX = cal_housing.data\ny = cal_housing.target\n\n\nThis can also be obtained with X, y = fetch_california_housing(return_X_y=True).\n\nLet’s have a look at the shape of X and y:\n\nX.shape\n\n(20640, 8)\n\n\n\ny.shape\n\n(20640,)\n\n\nWhile not at all necessary, we can turn this bunch object into a more familiar data frame to explore the data further:\n\ncal_housing_df = pd.DataFrame(cal_housing.data, columns=cal_housing.feature_names)\n\n\ncal_housing_df.head()\n\n\n\n\n\n\n\n\nMedInc\nHouseAge\nAveRooms\nAveBedrms\nPopulation\nAveOccup\nLatitude\nLongitude\n\n\n\n\n0\n8.3252\n41.0\n6.984127\n1.023810\n322.0\n2.555556\n37.88\n-122.23\n\n\n1\n8.3014\n21.0\n6.238137\n0.971880\n2401.0\n2.109842\n37.86\n-122.22\n\n\n2\n7.2574\n52.0\n8.288136\n1.073446\n496.0\n2.802260\n37.85\n-122.24\n\n\n3\n5.6431\n52.0\n5.817352\n1.073059\n558.0\n2.547945\n37.85\n-122.25\n\n\n4\n3.8462\n52.0\n6.281853\n1.081081\n565.0\n2.181467\n37.85\n-122.25\n\n\n\n\n\n\n\n\ncal_housing_df.tail()\n\n\n\n\n\n\n\n\nMedInc\nHouseAge\nAveRooms\nAveBedrms\nPopulation\nAveOccup\nLatitude\nLongitude\n\n\n\n\n20635\n1.5603\n25.0\n5.045455\n1.133333\n845.0\n2.560606\n39.48\n-121.09\n\n\n20636\n2.5568\n18.0\n6.114035\n1.315789\n356.0\n3.122807\n39.49\n-121.21\n\n\n20637\n1.7000\n17.0\n5.205543\n1.120092\n1007.0\n2.325635\n39.43\n-121.22\n\n\n20638\n1.8672\n18.0\n5.329513\n1.171920\n741.0\n2.123209\n39.43\n-121.32\n\n\n20639\n2.3886\n16.0\n5.254717\n1.162264\n1387.0\n2.616981\n39.37\n-121.24\n\n\n\n\n\n\n\n\ncal_housing_df.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 20640 entries, 0 to 20639\nData columns (total 8 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   MedInc      20640 non-null  float64\n 1   HouseAge    20640 non-null  float64\n 2   AveRooms    20640 non-null  float64\n 3   AveBedrms   20640 non-null  float64\n 4   Population  20640 non-null  float64\n 5   AveOccup    20640 non-null  float64\n 6   Latitude    20640 non-null  float64\n 7   Longitude   20640 non-null  float64\ndtypes: float64(8)\nmemory usage: 1.3 MB\n\n\n\ncal_housing_df.describe() \n\n\n\n\n\n\n\n\nMedInc\nHouseAge\nAveRooms\nAveBedrms\nPopulation\nAveOccup\nLatitude\nLongitude\n\n\n\n\ncount\n20640.000000\n20640.000000\n20640.000000\n20640.000000\n20640.000000\n20640.000000\n20640.000000\n20640.000000\n\n\nmean\n3.870671\n28.639486\n5.429000\n1.096675\n1425.476744\n3.070655\n35.631861\n-119.569704\n\n\nstd\n1.899822\n12.585558\n2.474173\n0.473911\n1132.462122\n10.386050\n2.135952\n2.003532\n\n\nmin\n0.499900\n1.000000\n0.846154\n0.333333\n3.000000\n0.692308\n32.540000\n-124.350000\n\n\n25%\n2.563400\n18.000000\n4.440716\n1.006079\n787.000000\n2.429741\n33.930000\n-121.800000\n\n\n50%\n3.534800\n29.000000\n5.229129\n1.048780\n1166.000000\n2.818116\n34.260000\n-118.490000\n\n\n75%\n4.743250\n37.000000\n6.052381\n1.099526\n1725.000000\n3.282261\n37.710000\n-118.010000\n\n\nmax\n15.000100\n52.000000\n141.909091\n34.066667\n35682.000000\n1243.333333\n41.950000\n-114.310000\n\n\n\n\n\n\n\nWe can even plot it:\n\nplt.hist(y)\n\n(array([ 877., 3612., 4099., 3771., 2799., 1769., 1239.,  752.,  479.,\n        1243.]),\n array([0.14999 , 0.634992, 1.119994, 1.604996, 2.089998, 2.575   ,\n        3.060002, 3.545004, 4.030006, 4.515008, 5.00001 ]),\n &lt;BarContainer object of 10 artists&gt;)\n\n\n\n\n\n\n\nCreate and fit a model\nLet’s start with a very simple model: linear regression.\n\nmodel = LinearRegression().fit(X, y)\n\n\nThis is equivalent to:\nmodel = LinearRegression()\nmodel.fit(X, y)\nFirst, we create an instance of the class LinearRegression, then we call .fit() on it to fit the model.\n\n\nmodel.coef_\n\narray([ 4.36693293e-01,  9.43577803e-03, -1.07322041e-01,  6.45065694e-01,\n       -3.97638942e-06, -3.78654265e-03, -4.21314378e-01, -4.34513755e-01])\n\n\n\nTrailing underscores indicate that an attribute is estimated. .coef_ here is an estimated value.\n\n\nmodel.coef_.shape\n\n(8,)\n\n\n\nmodel.intercept_\n\n-36.94192020718441\n\n\nWe can now get our predictions:\n\ny_hat = model.predict(X)\n\nAnd calculate some measures of error:\n\nSum of squared errors\n\n\nnp.sum((y - y_hat) ** 2)\n\n10821.985154850292\n\n\n\nMean squared error\n\n\nmean_squared_error(y, y_hat)\n\n0.5243209861846072\n\n\n\nMSE could also be calculated with np.mean((y - y_hat)**2).\n\n\nmean_absolute_percentage_error(y, y_hat)\n\n0.31715404597233426\n\n\nIndex of minimum value:\n\nmodel.coef_.argmin()\n\n7\n\n\nIndex of maximum value:\n\nmodel.coef_.argmax()\n\n3\n\n\n\nXX = np.concatenate([np.ones((len(X), 1)), X], axis=1)\n\nbeta = np.linalg.lstsq(XX, y, rcond=None)[0]\nintercept_, *coef_ = beta\n\nintercept_, model.intercept_\n\n(-36.941920207184864, -36.94192020718441)\n\n\n\nnp.allclose(coef_, model.coef_)\n\nTrue\n\n\n\nThis means that the two arrays are equal element-wise, within a certain tolerance.\n\n\nX_test = np.random.normal(size=(10, X.shape[1]))\nX_test.shape\n\n(10, 8)\n\n\n\ny_test = X_test @ coef_ + intercept_\ny_test\n\narray([-35.13867457, -36.88130979, -35.98879752, -37.12259218,\n       -35.79823326, -39.42981536, -36.0351945 , -36.93114893,\n       -35.71657203, -36.39289166])\n\n\n\nmodel.predict(X_test)\n\narray([-35.13867457, -36.88130979, -35.98879752, -37.12259218,\n       -35.79823326, -39.42981536, -36.0351945 , -36.93114893,\n       -35.71657203, -36.39289166])\n\n\nOf course, instead of LinearRegression(), we could have used another model such as a random forest regressor (a meta estimator that fits a number of classifying decision trees on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting) for instance:\n\nmodel = RandomForestRegressor().fit(X, y).predict(X_test)\nmodel\n\narray([1.1761601, 1.50388  , 1.50388  , 1.2784001, 1.47662  , 1.50388  ,\n       1.48053  , 1.46917  , 1.50335  , 1.47427  ])\n\n\n\nWhich is equivalent to:\nmodel = RandomForestRegressor()\nmodel.fit(X, y).predict(X_test)"
  },
  {
    "objectID": "ml/sk_workflow.html#example-2-breast-cancer",
    "href": "ml/sk_workflow.html#example-2-breast-cancer",
    "title": "Sklearn workflow",
    "section": "Example 2: breast cancer",
    "text": "Example 2: breast cancer\n\nLoad and explore the data\n\nb_cancer = load_breast_cancer()\n\nLet’s print the description of this dataset:\n\nprint(b_cancer.DESCR)\n\n.. _breast_cancer_dataset:\n\nBreast cancer wisconsin (diagnostic) dataset\n--------------------------------------------\n\n**Data Set Characteristics:**\n\n    :Number of Instances: 569\n\n    :Number of Attributes: 30 numeric, predictive attributes and the class\n\n    :Attribute Information:\n        - radius (mean of distances from center to points on the perimeter)\n        - texture (standard deviation of gray-scale values)\n        - perimeter\n        - area\n        - smoothness (local variation in radius lengths)\n        - compactness (perimeter^2 / area - 1.0)\n        - concavity (severity of concave portions of the contour)\n        - concave points (number of concave portions of the contour)\n        - symmetry\n        - fractal dimension (\"coastline approximation\" - 1)\n\n        The mean, standard error, and \"worst\" or largest (mean of the three\n        worst/largest values) of these features were computed for each image,\n        resulting in 30 features.  For instance, field 0 is Mean Radius, field\n        10 is Radius SE, field 20 is Worst Radius.\n\n        - class:\n                - WDBC-Malignant\n                - WDBC-Benign\n\n    :Summary Statistics:\n\n    ===================================== ====== ======\n                                           Min    Max\n    ===================================== ====== ======\n    radius (mean):                        6.981  28.11\n    texture (mean):                       9.71   39.28\n    perimeter (mean):                     43.79  188.5\n    area (mean):                          143.5  2501.0\n    smoothness (mean):                    0.053  0.163\n    compactness (mean):                   0.019  0.345\n    concavity (mean):                     0.0    0.427\n    concave points (mean):                0.0    0.201\n    symmetry (mean):                      0.106  0.304\n    fractal dimension (mean):             0.05   0.097\n    radius (standard error):              0.112  2.873\n    texture (standard error):             0.36   4.885\n    perimeter (standard error):           0.757  21.98\n    area (standard error):                6.802  542.2\n    smoothness (standard error):          0.002  0.031\n    compactness (standard error):         0.002  0.135\n    concavity (standard error):           0.0    0.396\n    concave points (standard error):      0.0    0.053\n    symmetry (standard error):            0.008  0.079\n    fractal dimension (standard error):   0.001  0.03\n    radius (worst):                       7.93   36.04\n    texture (worst):                      12.02  49.54\n    perimeter (worst):                    50.41  251.2\n    area (worst):                         185.2  4254.0\n    smoothness (worst):                   0.071  0.223\n    compactness (worst):                  0.027  1.058\n    concavity (worst):                    0.0    1.252\n    concave points (worst):               0.0    0.291\n    symmetry (worst):                     0.156  0.664\n    fractal dimension (worst):            0.055  0.208\n    ===================================== ====== ======\n\n    :Missing Attribute Values: None\n\n    :Class Distribution: 212 - Malignant, 357 - Benign\n\n    :Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\n\n    :Donor: Nick Street\n\n    :Date: November, 1995\n\nThis is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\nhttps://goo.gl/U2Uwz2\n\nFeatures are computed from a digitized image of a fine needle\naspirate (FNA) of a breast mass.  They describe\ncharacteristics of the cell nuclei present in the image.\n\nSeparating plane described above was obtained using\nMultisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\nConstruction Via Linear Programming.\" Proceedings of the 4th\nMidwest Artificial Intelligence and Cognitive Science Society,\npp. 97-101, 1992], a classification method which uses linear\nprogramming to construct a decision tree.  Relevant features\nwere selected using an exhaustive search in the space of 1-4\nfeatures and 1-3 separating planes.\n\nThe actual linear program used to obtain the separating plane\nin the 3-dimensional space is that described in:\n[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\nProgramming Discrimination of Two Linearly Inseparable Sets\",\nOptimization Methods and Software 1, 1992, 23-34].\n\nThis database is also available through the UW CS ftp server:\n\nftp ftp.cs.wisc.edu\ncd math-prog/cpo-dataset/machine-learn/WDBC/\n\n|details-start|\n**References**\n|details-split|\n\n- W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction \n  for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on \n  Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\n  San Jose, CA, 1993.\n- O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and \n  prognosis via linear programming. Operations Research, 43(4), pages 570-577, \n  July-August 1995.\n- W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\n  to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) \n  163-171.\n\n|details-end|\n\n\n\nb_cancer.feature_names\n\narray(['mean radius', 'mean texture', 'mean perimeter', 'mean area',\n       'mean smoothness', 'mean compactness', 'mean concavity',\n       'mean concave points', 'mean symmetry', 'mean fractal dimension',\n       'radius error', 'texture error', 'perimeter error', 'area error',\n       'smoothness error', 'compactness error', 'concavity error',\n       'concave points error', 'symmetry error',\n       'fractal dimension error', 'worst radius', 'worst texture',\n       'worst perimeter', 'worst area', 'worst smoothness',\n       'worst compactness', 'worst concavity', 'worst concave points',\n       'worst symmetry', 'worst fractal dimension'], dtype='&lt;U23')\n\n\n\nb_cancer.target_names\n\narray(['malignant', 'benign'], dtype='&lt;U9')\n\n\n\nX = b_cancer.data\ny = b_cancer.target\n\n\nHere again, we could have used instead X, y = load_breast_cancer(return_X_y=True).\n\n\nX.shape\n\n(569, 30)\n\n\n\ny.shape\n\n(569,)\n\n\n\nset(y)\n\n{0, 1}\n\n\n\nCounter(y)\n\nCounter({1: 357, 0: 212})\n\n\n\n\nCreate and fit a first model\n\nmodel = LogisticRegression(max_iter=10000)\ny_hat = model.fit(X, y).predict(X)\n\nGet some measure of accuracy:\n\naccuracy_score(y, y_hat)\n\n0.9578207381370826\n\n\n\nThis can also be obtained with:\nnp.mean(y_hat == y)\n\n\ndef sigmoid(x):\n  return 1/(1 + np.exp(-x))\n\nx = np.linspace(-10, 10, 100)\nplt.plot(x, sigmoid(x), lw=3)\nplt.title(\"The Sigmoid Function $\\\\sigma(x)$\")\n\nText(0.5, 1.0, 'The Sigmoid Function $\\\\sigma(x)$')\n\n\n\n\n\n\ny_pred = 1*(sigmoid(X @ model.coef_.squeeze() + model.intercept_) &gt; 0.5)\nassert np.all(y_pred == model.predict(X))\n\nnp.allclose(\n    model.predict_proba(X)[:, 1],\n    sigmoid(X @ model.coef_.squeeze() + model.intercept_)\n)\n\nTrue\n\n\n\ndef make_spirals(k=20, s=1.0, n=2000):\n    X = np.zeros((n, 2))\n    y = np.round(np.random.uniform(size=n)).astype(int)\n    r = np.random.uniform(size=n)*k*np.pi\n    rr = r**0.5\n    theta = rr + np.random.normal(loc=0, scale=s, size=n)\n    theta[y == 1] = theta[y == 1] + np.pi\n    X[:,0] = rr*np.cos(theta)\n    X[:,1] = rr*np.sin(theta)\n    return X, y\n\nX, y = make_spirals()\ncmap = matplotlib.colormaps[\"viridis\"]\n\na = cmap(0)\na = [*a[:3], 0.3]\nb = cmap(0.99)\nb = [*b[:3], 0.3]\n\nplt.figure(figsize=(7,7))\nax = plt.gca()\nax.set_aspect(\"equal\")\nax.plot(X[y == 0, 0], X[y == 0, 1], 'o', color=a, ms=8, label=\"$y=0$\")\nax.plot(X[y == 1, 0], X[y == 1, 1], 'o', color=b, ms=8, label=\"$y=1$\")\nplt.title(\"Spirals\")\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f17057636d0&gt;\n\n\n\n\n\n\n\nCreate and fit a second model\nHere, we use a logistic regression:\n\nmodel = LogisticRegression()\ny_hat = model.fit(X, y).predict(X)\naccuracy_score(y, y_hat)\n\n0.5825\n\n\n\nu = np.linspace(-8, 8, 100)\nv = np.linspace(-8, 8, 100)\nU, V = np.meshgrid(u, v)\nUV = np.array([U.ravel(), V.ravel()]).T\nU.shape, V.shape, UV.shape\n\n((100, 100), (100, 100), (10000, 2))\n\n\n\nnp.ravel returns a contiguous flattened array.\n\n\nW = model.predict(UV).reshape(U.shape)\nW.shape\n\n(100, 100)\n\n\n\nplt.pcolormesh(U, V, W)\n\n&lt;matplotlib.collections.QuadMesh at 0x7f1705b1b450&gt;\n\n\n\n\n\n\n\nCreate and fit a third model\nLet’s use a k-nearest neighbours classifier this time:\n\nmodel = KNeighborsClassifier(n_neighbors=5)\ny_hat = model.fit(X, y).predict(X)\naccuracy_score(y, y_hat)\n\n0.889\n\n\n\nu = np.linspace(-8, 8, 100)\nv = np.linspace(-8, 8, 100)\nU, V = np.meshgrid(u, v)\nUV = np.array([U.ravel(), V.ravel()]).T\nU.shape, V.shape, UV.shape\n\n((100, 100), (100, 100), (10000, 2))\n\n\n\nW = model.predict(UV).reshape(U.shape)\nW.shape\n\n(100, 100)\n\n\n\nplt.pcolormesh(U, V, W)\n\n&lt;matplotlib.collections.QuadMesh at 0x7f17034d2950&gt;\n\n\n\n\n\nWe can iterate over various values of k to see how the accuracy and pseudocolor plot evolve:\n\nfig, axes = plt.subplots(2, 4, figsize=(9.8, 5))\nfig.suptitle(\"Decision Regions\")\n\nu = np.linspace(-8, 8, 100)\nv = np.linspace(-8, 8, 100)\nU, V = np.meshgrid(u, v)\nUV = np.array([U.ravel(), V.ravel()]).T\n\nks = np.arange(1, 16, 2)\n\nfor k, ax in zip(ks, axes.ravel()):\n  model = KNeighborsClassifier(n_neighbors=k)\n  model.fit(X, y)\n  acc = accuracy_score(y, model.predict(X))\n  W = model.predict(UV).reshape(U.shape)\n  ax.imshow(W, origin=\"lower\", cmap=cmap)\n  ax.set_axis_off()\n  ax.set_title(f\"$k$={k}, acc={acc:.2f}\")"
  },
  {
    "objectID": "ml/pt_workflow.html",
    "href": "ml/pt_workflow.html",
    "title": "Overall workflow",
    "section": "",
    "text": "This classic PyTorch tutorial goes over the entire workflow to create and train a simple image classifier.\nLet’s go over it step by step."
  },
  {
    "objectID": "ml/pt_workflow.html#the-data",
    "href": "ml/pt_workflow.html#the-data",
    "title": "Overall workflow",
    "section": "The data",
    "text": "The data\nCIFAR-10 from the Canadian Institute for Advanced Research is a classic dataset of 60,000 color images falling into 10 classes (6,000 images in each class):\n\nairplane\nautomobile\nbird\ncat\ndeer\ndog\nfrog\nhorse\nship\ntruck\n\nThe images are of size 32x32 pixels (tiny!), which makes it very lightweight, quick to load and easy to play with.\n\nCreate a DataLoader\nA DataLoader is an iterable feeding data to a model at each iteration. The data loader transforms the data to the proper format, sets the batch size, whether the data is shuffled or not, and how the I/O is parallelized. You can create DataLoaders with the torch.utils.data.DataLoader class.\nLet’s create 2 DataLoaders: one for the train set and one for the test set.\n\nLoad packages\n\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\n\n\n\nCreate a transform object\nThe CIFAR-10 images in the TorchVision library are Image objects (from the PIL.Image module of the pillow package).\nWe need to normalize them and turn them into tensors:\n\ntransform = transforms.Compose(\n    [transforms.ToTensor(),\n     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n\n\n\nChoose a batch size\nRemember that the data move forward through the network (forward pass), outputting some estimates which are used to calculate some loss (or error) value. Then we get gradients through automatic differentiation and the model parameters are adjusted a little through gradient descent.\nYou do not have to have the entire training set go through this process each time: you can use batches.\nThe batch size is the number of items from the data that are processed before the model is updated. There is no hard rule to set good batch sizes and sizes tend to be picked through trial and error.\nHere are some rules to chose a batch size:\n\nmake sure that the batch fits in the CPU or GPU,\nsmall batches give faster results (each training iteration is very fast), but give less accuracy,\nlarge batches lead to slower training, but better accuracy.\n\nLet’s set the batch size to 4:\n\nbatch_size = 4\n\n\n\nPut it together into DataLoaders\n\ntrainset = torchvision.datasets.CIFAR10(root='./data',\n                                        train=True,\n                                        download=True,\n                                        transform=transform)\n\ntrainloader = torch.utils.data.DataLoader(trainset,\n                                          batch_size=batch_size,\n                                          shuffle=True,\n                                          num_workers=2)\n\ntestset = torchvision.datasets.CIFAR10(root='./data',\n                                       train=False,\n                                       download=True,\n                                       transform=transform)\n\ntestloader = torch.utils.data.DataLoader(testset,\n                                         batch_size=batch_size,\n                                         shuffle=False,\n                                         num_workers=2)\n\nWe will also need the classes:\n\nclasses = ('plane', 'car', 'bird', 'cat', 'deer',\n           'dog', 'frog', 'horse', 'ship', 'truck')\n\n\n\n\nVisualize a sample of the data\nThough not necessary, it can be useful to have a look at the data:\n\n# Load the packages for this\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Define a function to display an image\ndef imshow(img):\n    img = img / 2 + 0.5     # unnormalize\n    npimg = img.numpy()\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.show()\n\n# Get a batch of random training images\ndataiter = iter(trainloader)\nimages, labels = next(dataiter)\n\n# Display the images\nimshow(torchvision.utils.make_grid(images))\n\n# Print the labels\nprint(' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))\n\n\n\n\ndog   cat   plane horse"
  },
  {
    "objectID": "ml/pt_workflow.html#the-model",
    "href": "ml/pt_workflow.html#the-model",
    "title": "Overall workflow",
    "section": "The model",
    "text": "The model\n\nArchitecture\nFirst, we need to define the architecture of the network. There are many types of architectures. For images, CNN are well suited.\nIn Python, you can define a subclass of an existing class with:\nclass YourSubclass(BaseClass):\n    &lt;definition of your subclass&gt;        \nThe subclass is derived from the base class and inherits its properties. PyTorch contains the class torch.nn.Module which is used as the base class when defining a neural network.\n\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Net(nn.Module):\n    # Define the architecture of the network\n    def __init__(self):\n        super().__init__()\n        # 3 input image channel (3 colour channels)\n        # 6 output channels,\n        # 5x5 square convolution kernel\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        # Max pooling over a (2, 2) window\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        # 5*5 from image dimension\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n        self.fc2 = nn.Linear(120, 84)\n        # 10 is the size of the output layer\n        # since there are 10 classes\n        self.fc3 = nn.Linear(84, 10)\n\n    # Set the flow of data through the network for the forward pass\n    # x represents the data\n    def forward(self, x):\n        # F.relu is the rectified-linear activation function\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        # flatten all dimensions except the batch dimension\n        x = torch.flatten(x, 1)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\nLet’s create an instance of Net and print its structure:\n\nnet = Net()\nprint(net)\n\nNet(\n  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n  (fc1): Linear(in_features=400, out_features=120, bias=True)\n  (fc2): Linear(in_features=120, out_features=84, bias=True)\n  (fc3): Linear(in_features=84, out_features=10, bias=True)\n)\n\n\n\n\nLoss function and optimizer\nWe need to chose a loss function that will be used to calculate the gradients through backpropagation as well as an optimizer to do the gradient descent.\nSGD with momentum has proved a very efficient optimizing technique and is widely used.\n\nimport torch.optim as optim\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
  },
  {
    "objectID": "ml/pt_workflow.html#training",
    "href": "ml/pt_workflow.html#training",
    "title": "Overall workflow",
    "section": "Training",
    "text": "Training\nWe can now train the model:\n\nfor epoch in range(2):  # loop over the dataset twice\n\n    running_loss = 0.0\n\n    for i, data in enumerate(trainloader, 0):\n        # get the inputs; data is a list of [inputs, labels]\n        inputs, labels = data\n\n        # zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward + backward + optimize\n        outputs = net(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        # print statistics\n        running_loss += loss.item()\n        if i % 2000 == 1999:    # print every 2000 mini-batches\n            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n            running_loss = 0.0\n\nprint('Finished Training')\n\n[1,  2000] loss: 2.148\n[1,  4000] loss: 1.805\n[1,  6000] loss: 1.666\n[1,  8000] loss: 1.574\n[1, 10000] loss: 1.508\n[1, 12000] loss: 1.464\n[2,  2000] loss: 1.392\n[2,  4000] loss: 1.356\n[2,  6000] loss: 1.343\n[2,  8000] loss: 1.333\n[2, 10000] loss: 1.284\n[2, 12000] loss: 1.271\nFinished Training"
  },
  {
    "objectID": "ml/pt_workflow.html#testing",
    "href": "ml/pt_workflow.html#testing",
    "title": "Overall workflow",
    "section": "Testing",
    "text": "Testing\n\nLittle test on one batch for fun\nLet’s now test our model on one batch of testing data.\nFirst, let’s get a batch of random testing data:\n\ndataiter = iter(testloader)\nimages, labels = next(dataiter)\n\nLet’s display them and print their true labels:\n\nimshow(torchvision.utils.make_grid(images))\nprint('Real: ', ' '.join(f'{classes[labels[j]]:5s}' for j in range(4)))\n\n\n\n\nReal:  cat   ship  ship  plane\n\n\nNow, let’s run the same batch of testing images through our model:\n\noutputs = net(images)\n\nLet’s get the best predictions for these:\n\n_, predicted = torch.max(outputs, 1)\n\nprint('Predicted: ', ' '.join(f'{classes[predicted[j]]:5s}'\n                              for j in range(4)))\n\nPredicted:  cat   ship  ship  plane\n\n\n\n\nMore serious testing\nThis was fun, but of course, with a sample of one, we can’t say anything about how good our model is. We need to test it on many more images from the test set.\nLet’s use the entire test set:\n\ncorrect = 0\ntotal = 0\n# since we're not training, we don't need to calculate the gradients for our outputs\nwith torch.no_grad():\n    for data in testloader:\n        images, labels = data\n        # calculate outputs by running images through the network\n        outputs = net(images)\n        # the class with the highest energy is what we choose as prediction\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\nprint(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n\nAccuracy of the network on the 10000 test images: 54 %\n\n\n\n\nPer class testing\nWe could see whether the model seem to perform better for some classes than others:\n\n# prepare to count predictions for each class\ncorrect_pred = {classname: 0 for classname in classes}\ntotal_pred = {classname: 0 for classname in classes}\n\n# again no gradients needed\nwith torch.no_grad():\n    for data in testloader:\n        images, labels = data\n        outputs = net(images)\n        _, predictions = torch.max(outputs, 1)\n        # collect the correct predictions for each class\n        for label, prediction in zip(labels, predictions):\n            if label == prediction:\n                correct_pred[classes[label]] += 1\n            total_pred[classes[label]] += 1\n\n\n# print accuracy for each class\nfor classname, correct_count in correct_pred.items():\n    accuracy = 100 * float(correct_count) / total_pred[classname]\n    print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')\n\nAccuracy for class: plane is 67.2 %\nAccuracy for class: car   is 61.5 %\nAccuracy for class: bird  is 53.2 %\nAccuracy for class: cat   is 50.6 %\nAccuracy for class: deer  is 41.5 %\nAccuracy for class: dog   is 43.1 %\nAccuracy for class: frog  is 68.8 %\nAccuracy for class: horse is 41.5 %\nAccuracy for class: ship  is 61.0 %\nAccuracy for class: truck is 61.5 %"
  },
  {
    "objectID": "ml/pt_tensors.html",
    "href": "ml/pt_tensors.html",
    "title": "PyTorch tensors",
    "section": "",
    "text": "Before information can be processed by algorithms, it needs to be converted to floating point numbers. Indeed, you don’t pass a sentence or an image through a model; instead you input numbers representing a sequence of words or pixel values.\nAll these floating point numbers need to be stored in a data structure. The most suited structure is multidimensional (to hold several layers of information) and homogeneous—all data of the same type—for efficiency.\nPython already has several multidimensional array structures (e.g. NumPy’s ndarray) but the particularities of deep learning call for special characteristics such as the ability to run operations on GPUs and/or in a distributed fashion, the ability to keep track of computation graphs for automatic differentiation, and different defaults (lower precision for improved training performance).\nThe PyTorch tensor is a Python data structure with these characteristics that can easily be converted to/from NumPy’s ndarray and integrates well with other Python libraries such as Pandas.\nIn this section, we will explore the basics of PyTorch tensors."
  },
  {
    "objectID": "ml/pt_tensors.html#importing-pytorch",
    "href": "ml/pt_tensors.html#importing-pytorch",
    "title": "PyTorch tensors",
    "section": "Importing PyTorch",
    "text": "Importing PyTorch\nFirst of all, we need to import the torch library:\n\nimport torch\n\nWe can check its version with:\n\ntorch.__version__\n\n'2.1.0+cu121'"
  },
  {
    "objectID": "ml/pt_tensors.html#creating-tensors",
    "href": "ml/pt_tensors.html#creating-tensors",
    "title": "PyTorch tensors",
    "section": "Creating tensors",
    "text": "Creating tensors\nThere are many ways to create tensors:\n\ntorch.tensor:   Input individual values\ntorch.arange:   1D tensor with a sequence of integers\ntorch.linspace:  1D linear scale tensor\ntorch.logspace:  1D log scale tensor\ntorch.rand:     Random numbers from a uniform distribution on [0, 1)\ntorch.randn:     Numbers from the standard normal distribution\ntorch.randperm:   Random permutation of integers\ntorch.empty:     Uninitialized tensor\ntorch.zeros:     Tensor filled with 0\ntorch.ones:     Tensor filled with 1\ntorch.eye:       Identity matrix\n\n\nFrom input values\n\nt = torch.tensor(3)\n\n\n\nYour turn:\n\nWithout using the shape descriptor, try to get the shape of the following tensors:\ntorch.tensor([0.9704, 0.1339, 0.4841])\n\ntorch.tensor([[0.9524, 0.0354],\n        [0.9833, 0.2562],\n        [0.0607, 0.6420]])\n\ntorch.tensor([[[0.4604, 0.2699],\n         [0.8360, 0.0317],\n         [0.3289, 0.1171]]])\n\ntorch.tensor([[[[0.0730, 0.8737],\n          [0.2305, 0.4719],\n          [0.0796, 0.2745]]],\n\n        [[[0.1534, 0.9442],\n          [0.3287, 0.9040],\n          [0.0948, 0.1480]]]])\n\nLet’s create a random tensor with a single element:\n\nt = torch.rand(1)\nt\n\ntensor([0.2082])\n\n\nWe can extract the value from a tensor with one element:\n\nt.item()\n\n0.20821404457092285\n\n\nAll these tensors have a single element, but an increasing number of dimensions:\n\ntorch.rand(1)\n\ntensor([0.8162])\n\n\n\ntorch.rand(1, 1)\n\ntensor([[0.2124]])\n\n\n\ntorch.rand(1, 1, 1)\n\ntensor([[[0.2137]]])\n\n\n\ntorch.rand(1, 1, 1, 1)\n\ntensor([[[[0.9721]]]])\n\n\n\nYou can tell the number of dimensions of a tensor easily by counting the number of opening square brackets.\n\n\ntorch.rand(1, 1, 1, 1).dim()\n\n4\n\n\nTensors can have multiple elements in one dimension:\n\ntorch.rand(6)\n\ntensor([0.7707, 0.3779, 0.7284, 0.5970, 0.2715, 0.0250])\n\n\n\ntorch.rand(6).dim()\n\n1\n\n\nAnd multiple elements in multiple dimensions:\n\ntorch.rand(2, 3, 4, 5)\n\ntensor([[[[0.4616, 0.8672, 0.6540, 0.3157, 0.5885],\n          [0.0820, 0.5754, 0.0829, 0.4797, 0.0266],\n          [0.0516, 0.8681, 0.3600, 0.8134, 0.9124],\n          [0.7722, 0.0579, 0.8160, 0.6851, 0.8277]],\n\n         [[0.0812, 0.7798, 0.1025, 0.7884, 0.4204],\n          [0.7009, 0.0206, 0.4540, 0.7333, 0.4209],\n          [0.4577, 0.2203, 0.1684, 0.8569, 0.9371],\n          [0.3160, 0.0569, 0.7677, 0.9316, 0.4101]],\n\n         [[0.8069, 0.3984, 0.4338, 0.6180, 0.3214],\n          [0.0225, 0.7405, 0.5877, 0.8513, 0.5265],\n          [0.7455, 0.6737, 0.1170, 0.4612, 0.0244],\n          [0.5451, 0.2924, 0.2339, 0.8307, 0.3020]]],\n\n\n        [[[0.9006, 0.5355, 0.5066, 0.4862, 0.5052],\n          [0.4635, 0.8254, 0.7666, 0.5352, 0.4359],\n          [0.0516, 0.3260, 0.5569, 0.0870, 0.0660],\n          [0.3126, 0.1216, 0.4977, 0.3401, 0.3600]],\n\n         [[0.3058, 0.6562, 0.4803, 0.2914, 0.6433],\n          [0.8609, 0.5496, 0.6218, 0.4690, 0.2260],\n          [0.0050, 0.0890, 0.2256, 0.9505, 0.3095],\n          [0.7891, 0.6340, 0.3575, 0.1387, 0.1553]],\n\n         [[0.7385, 0.6851, 0.0741, 0.0652, 0.8278],\n          [0.7487, 0.3711, 0.7701, 0.9613, 0.1483],\n          [0.7081, 0.3672, 0.6963, 0.2179, 0.1964],\n          [0.2730, 0.9572, 0.9906, 0.6960, 0.0372]]]])\n\n\n\ntorch.rand(2, 3, 4, 5).dim()\n\n4\n\n\n\ntorch.rand(2, 3, 4, 5).numel()\n\n120\n\n\n\ntorch.ones(2, 4)\n\ntensor([[1., 1., 1., 1.],\n        [1., 1., 1., 1.]])\n\n\n\nt = torch.rand(2, 3)\ntorch.zeros_like(t)             # Matches the size of t\n\ntensor([[0., 0., 0.],\n        [0., 0., 0.]])\n\n\n\ntorch.ones_like(t)\n\ntensor([[1., 1., 1.],\n        [1., 1., 1.]])\n\n\n\ntorch.randn_like(t)\n\ntensor([[ 0.6773, -0.3568,  0.1980],\n        [-0.7654, -0.5229,  0.6606]])\n\n\n\ntorch.arange(2, 10, 3)    # From 2 to 10 in increments of 3\n\ntensor([2, 5, 8])\n\n\n\ntorch.linspace(2, 10, 3)  # 3 elements from 2 to 10 on the linear scale\n\ntensor([ 2.,  6., 10.])\n\n\n\ntorch.logspace(2, 10, 3)  # Same on the log scale\n\ntensor([1.0000e+02, 1.0000e+06, 1.0000e+10])\n\n\n\ntorch.randperm(3)\n\ntensor([1, 2, 0])\n\n\n\ntorch.eye(3)\n\ntensor([[1., 0., 0.],\n        [0., 1., 0.],\n        [0., 0., 1.]])"
  },
  {
    "objectID": "ml/pt_tensors.html#conversion-tofrom-numpy",
    "href": "ml/pt_tensors.html#conversion-tofrom-numpy",
    "title": "PyTorch tensors",
    "section": "Conversion to/from NumPy",
    "text": "Conversion to/from NumPy\nPyTorch tensors can be converted to NumPy ndarrays and vice-versa in a very efficient manner as both objects share the same memory.\n\nFrom PyTorch tensor to NumPy ndarray\n\nt = torch.rand(2, 3)\nt\n\ntensor([[0.7585, 0.9593, 0.4109],\n        [0.0300, 0.2950, 0.5672]])\n\n\n\nt_np = t.numpy()\nt_np\n\narray([[0.75848234, 0.95931196, 0.4109395 ],\n       [0.03004605, 0.29501003, 0.56719583]], dtype=float32)\n\n\n\n\nFrom NumPy ndarray to PyTorch tensor\n\nimport numpy as np\na = np.random.rand(2, 3)\na\n\narray([[0.21221174, 0.83075061, 0.52989468],\n       [0.176147  , 0.62240534, 0.0447529 ]])\n\n\n\na_pt = torch.from_numpy(a)\na_pt\n\ntensor([[0.2122, 0.8308, 0.5299],\n        [0.1761, 0.6224, 0.0448]], dtype=torch.float64)\n\n\n\nNote the different default data types."
  },
  {
    "objectID": "ml/pt_tensors.html#indexing-tensors",
    "href": "ml/pt_tensors.html#indexing-tensors",
    "title": "PyTorch tensors",
    "section": "Indexing tensors",
    "text": "Indexing tensors\n\nt = torch.rand(3, 4)\nt\n\ntensor([[0.5795, 0.4333, 0.8475, 0.2193],\n        [0.4869, 0.1399, 0.3527, 0.7225],\n        [0.7682, 0.8170, 0.6133, 0.2894]])\n\n\n\nt[:, 2]\n\ntensor([0.8475, 0.3527, 0.6133])\n\n\n\nt[1, :]\n\ntensor([0.4869, 0.1399, 0.3527, 0.7225])\n\n\n\nt[2, 3]\n\ntensor(0.2894)\n\n\n\nA word of caution about indexing\nWhile indexing elements of a tensor to extract some of the data as a final step of some computation is fine, you should not use indexing to run operations on tensor elements in a loop as this would be extremely inefficient.\nInstead, you want to use vectorized operations."
  },
  {
    "objectID": "ml/pt_tensors.html#vectorized-operations",
    "href": "ml/pt_tensors.html#vectorized-operations",
    "title": "PyTorch tensors",
    "section": "Vectorized operations",
    "text": "Vectorized operations\nSince PyTorch tensors are homogeneous (i.e. made of a single data type), as with NumPy’s ndarrays, operations are vectorized and thus fast.\nNumPy is mostly written in C, PyTorch in C++. With either library, when you run vectorized operations on arrays/tensors, you don’t use raw Python (slow) but compiled C/C++ code (much faster).\nHere is an excellent post explaining Python vectorization & why it makes such a big difference."
  },
  {
    "objectID": "ml/pt_tensors.html#data-types",
    "href": "ml/pt_tensors.html#data-types",
    "title": "PyTorch tensors",
    "section": "Data types",
    "text": "Data types\n\nDefault data type\nSince PyTorch tensors were built with efficiency in mind for neural networks, the default data type is 32-bit floating points.\nThis is sufficient for accuracy and much faster than 64-bit floating points.\n\nBy contrast, NumPy ndarrays use 64-bit as their default.\n\n\nt = torch.rand(2, 4)\nt.dtype\n\ntorch.float32\n\n\n\n\nSetting data type at creation\nThe type can be set with the dtype argument:\n\nt = torch.rand(2, 4, dtype=torch.float64)\nt\n\ntensor([[0.6745, 0.1281, 0.9775, 0.6706],\n        [0.9326, 0.3761, 0.6411, 0.9669]], dtype=torch.float64)\n\n\n\nPrinted tensors display attributes with values ≠ default values.\n\n\nt.dtype\n\ntorch.float64\n\n\n\n\nChanging data type\n\nt = torch.rand(2, 4)\nt.dtype\n\ntorch.float32\n\n\n\nt2 = t.type(torch.float64)\nt2.dtype\n\ntorch.float64\n\n\n\n\nList of data types\n\n\n\n\n\n\n\ndtype\nDescription\n\n\n\n\ntorch.float16 / torch.half\n16-bit / half-precision floating-point\n\n\ntorch.float32 / torch.float\n32-bit / single-precision floating-point\n\n\ntorch.float64 / torch.double\n64-bit / double-precision floating-point\n\n\ntorch.uint8\nunsigned 8-bit integers\n\n\ntorch.int8\nsigned 8-bit integers\n\n\ntorch.int16 / torch.short\nsigned 16-bit integers\n\n\ntorch.int32 / torch.int\nsigned 32-bit integers\n\n\ntorch.int64 / torch.long\nsigned 64-bit integers\n\n\ntorch.bool\nboolean"
  },
  {
    "objectID": "ml/pt_tensors.html#simple-operations",
    "href": "ml/pt_tensors.html#simple-operations",
    "title": "PyTorch tensors",
    "section": "Simple operations",
    "text": "Simple operations\n\nt1 = torch.tensor([[1, 2], [3, 4]])\nt1\n\ntensor([[1, 2],\n        [3, 4]])\n\n\n\nt2 = torch.tensor([[1, 1], [0, 0]])\nt2\n\ntensor([[1, 1],\n        [0, 0]])\n\n\nOperation performed between elements at corresponding locations:\n\nt1 + t2\n\ntensor([[2, 3],\n        [3, 4]])\n\n\nOperation applied to each element of the tensor:\n\nt1 + 1\n\ntensor([[2, 3],\n        [4, 5]])\n\n\n\nReduction\n\nt = torch.ones(2, 3, 4);\nt\n\ntensor([[[1., 1., 1., 1.],\n         [1., 1., 1., 1.],\n         [1., 1., 1., 1.]],\n\n        [[1., 1., 1., 1.],\n         [1., 1., 1., 1.],\n         [1., 1., 1., 1.]]])\n\n\n\nt.sum()   # Reduction over all entries\n\ntensor(24.)\n\n\n\nOther reduction functions (e.g. mean) behave the same way.\n\nReduction over a specific dimension:\n\nt.sum(0)\n\ntensor([[2., 2., 2., 2.],\n        [2., 2., 2., 2.],\n        [2., 2., 2., 2.]])\n\n\n\nt.sum(1)\n\ntensor([[3., 3., 3., 3.],\n        [3., 3., 3., 3.]])\n\n\n\nt.sum(2)\n\ntensor([[4., 4., 4.],\n        [4., 4., 4.]])\n\n\nReduction over multiple dimensions:\n\nt.sum((0, 1))\n\ntensor([6., 6., 6., 6.])\n\n\n\nt.sum((0, 2))\n\ntensor([8., 8., 8.])\n\n\n\nt.sum((1, 2))\n\ntensor([12., 12.])\n\n\n\n\nIn-place operations\nWith operators post-fixed with _:\n\nt1 = torch.tensor([1, 2])\nt1\n\ntensor([1, 2])\n\n\n\nt2 = torch.tensor([1, 1])\nt2\n\ntensor([1, 1])\n\n\n\nt1.add_(t2)\nt1\n\ntensor([2, 3])\n\n\n\nt1.zero_()\nt1\n\ntensor([0, 0])\n\n\n\nWhile reassignments will use new addresses in memory, in-place operations will use the same addresses.\n\n\n\nTensor views\nt = torch.tensor([[1, 2, 3], [4, 5, 6]]); print(t)\nt.size()\nt.view(6)\nt.view(3, 2)\nt.view(3, -1) # Same: with -1, the size is inferred from other dimensions\n\nNote the difference\n\nt1 = torch.tensor([[1, 2, 3], [4, 5, 6]])\nt1\n\ntensor([[1, 2, 3],\n        [4, 5, 6]])\n\n\n\nt2 = t1.t()\nt2\n\ntensor([[1, 4],\n        [2, 5],\n        [3, 6]])\n\n\n\nt3 = t1.view(3, 2)\nt3\n\ntensor([[1, 2],\n        [3, 4],\n        [5, 6]])\n\n\n\n\n\nLogical operations\n\nt1 = torch.randperm(5)\nt1\n\ntensor([2, 4, 1, 3, 0])\n\n\n\nt2 = torch.randperm(5)\nt2\n\ntensor([3, 4, 2, 0, 1])\n\n\nTest each element:\n\nt1 &gt; 3\n\ntensor([False,  True, False, False, False])\n\n\nTest corresponding pairs of elements:\n\nt1 &lt; t2\n\ntensor([ True, False,  True, False,  True])"
  },
  {
    "objectID": "ml/pt_tensors.html#device-attribute",
    "href": "ml/pt_tensors.html#device-attribute",
    "title": "PyTorch tensors",
    "section": "Device attribute",
    "text": "Device attribute\nTensor data can be placed in the memory of various processor types:\n\nthe RAM of CPU,\nthe RAM of a GPU with CUDA support,\nthe RAM of a GPU with AMD’s ROCm support,\nthe RAM of an XLA device (e.g. Cloud TPU) with the torch_xla package.\n\nThe values for the device attributes are:\n\nCPU:  'cpu',\nGPU (CUDA & AMD’s ROCm):  'cuda',\nXLA:  xm.xla_device().\n\nThis last option requires to load the torch_xla package first:\nimport torch_xla\nimport torch_xla.core.xla_model as xm\n\nCreating a tensor on a specific device\nBy default, tensors are created on the CPU.\nYou can create a tensor on an accelerator by specifying the device attribute (our current training cluster does not have GPUs, so don’t run this on it):\nt_gpu = torch.rand(2, device='cuda')\n\n\nCopying a tensor to a specific device\nYou can also make copies of a tensor on other devices:\n# Make a copy of t on the GPU\nt_gpu = t.to(device='cuda')\nt_gpu = t.cuda()             # Alternative syntax\n\n# Make a copy of t_gpu on the CPU\nt = t_gpu.to(device='cpu')\nt = t_gpu.cpu()              # Alternative syntax\n\n\nMultiple GPUs\nIf you have multiple GPUs, you can optionally specify which one a tensor should be created on or copied to:\nt1 = torch.rand(2, device='cuda:0')  # Create a tensor on 1st GPU\nt2 = t1.to(device='cuda:0')          # Make a copy of t1 on 1st GPU\nt3 = t1.to(device='cuda:1')          # Make a copy of t1 on 2nd GPU\nOr the equivalent short forms:\nt2 = t1.cuda(0)\nt3 = t1.cuda(1)"
  },
  {
    "objectID": "ml/pt_resources.html",
    "href": "ml/pt_resources.html",
    "title": "Resources",
    "section": "",
    "text": "This section contains a list of general machine learning resources, resources specific to PyTorch, as well as resources for Python and fastai."
  },
  {
    "objectID": "ml/pt_resources.html#machine-learning",
    "href": "ml/pt_resources.html#machine-learning",
    "title": "Resources",
    "section": "Machine learning",
    "text": "Machine learning\nAlliance wiki ML page\n\nOpen-access preprints\nArxiv Sanity Preserver by Andrej Karpathy\nML papers in the computer science category on arXiv\nML papers in the stats category on arXiv\nDistill ML research online journal\n\n\nAdvice and sources\nAdvice and sources from ML research student\n\n\nGetting help\nStack Overflow [machine-learning] tag\nStack Overflow [deep-learning] tag\nStack Overflow [supervised-learning] tag\nStack Overflow [unsupervised-learning] tag\nStack Overflow [semisupervised-learning] tag\nStack Overflow [reinforcement-learning] tag\nStack Overflow [transfer-learning] tag\nStack Overflow [machine-learning-model] tag\nStack Overflow [learning-rate] tag\nStack Overflow [bayesian-deep-learning] tag\n\n\nFree introductory courses\ndeeplearning.ai\nfast.ai\nGoogle\n\n\nLists of open datasets\nbenchmarks.ai\nAIBench\nkaggle\nWikipedia"
  },
  {
    "objectID": "ml/pt_resources.html#pytorch",
    "href": "ml/pt_resources.html#pytorch",
    "title": "Resources",
    "section": "PyTorch",
    "text": "PyTorch\nAlliance wiki PyTorch page\n\n\nDocumentation\nPyTorch website\nPyTorch documentation\nPyTorch tutorials\nPyTorch online courses\nPyTorch examples\n\n\nGetting help\nPyTorch Discourse forum\nStack Overflow [pytorch] tag\nStack Overflow [pytorch-dataloader] tag\nStack Overflow [pytorch-ignite] tag\n\n\nPre-trained models\nPyTorch Hub"
  },
  {
    "objectID": "ml/pt_resources.html#python",
    "href": "ml/pt_resources.html#python",
    "title": "Resources",
    "section": "Python",
    "text": "Python\nAlliance wiki Python page\n\nIDE\nProject Jupyter\nList of IDEs with description\nComparison of IDEs\nEmacs Python IDE\n\n\nShell\nIPython\nbpython\nptpython\n\n\nGetting help\nStack Overflow [python] tag"
  },
  {
    "objectID": "ml/pt_resources.html#fastai",
    "href": "ml/pt_resources.html#fastai",
    "title": "Resources",
    "section": "fastai",
    "text": "fastai\n\nDocumentation\nManual\nTutorials\nPeer-reviewed paper\n\n\nBook\nPaperback version\nFree MOOC version of part 1 of the book\nJupyter notebooks version of the book\n\n\nGetting help\nDiscourse forum"
  },
  {
    "objectID": "ml/pt_nn_slides.html#fully-connected-neural-networks",
    "href": "ml/pt_nn_slides.html#fully-connected-neural-networks",
    "title": "NN vs biological neurons Types of NN",
    "section": "Fully connected neural networks",
    "text": "Fully connected neural networks\n\n\n\n\n\nFrom Glosser.ca, Wikipedia\n\n\n\nEach neuron receives inputs from every neuron of the previous layer and passes its output to every neuron of the next layer."
  },
  {
    "objectID": "ml/pt_nn_slides.html#convolutional-neural-networks",
    "href": "ml/pt_nn_slides.html#convolutional-neural-networks",
    "title": "NN vs biological neurons Types of NN",
    "section": "Convolutional neural networks",
    "text": "Convolutional neural networks\n\n\nFrom Programming Journeys by Rensu Theart\n\nConvolutional neural networks (CNN) are used for spatially structured data (e.g. images).\nImages have huge input sizes and would require a very large number of neurons in a fully connected neural net. In convolutional layers, neurons receive input from a subarea (called local receptive field) of the previous layer. This greatly reduces the number of parameters. Optionally, pooling (combining the outputs of neurons in a subarea) reduces the data dimensions."
  },
  {
    "objectID": "ml/pt_nn_slides.html#recurrent-neural-networks",
    "href": "ml/pt_nn_slides.html#recurrent-neural-networks",
    "title": "NN vs biological neurons Types of NN",
    "section": "Recurrent neural networks",
    "text": "Recurrent neural networks\n\n\nFrom fdeloche, Wikipedia\n\nRecurrent neural networks (RNN) such as Long Short-Term Memory (LSTM) are used for chain structured data (e.g. text).\nThey are not feedforward networks (i.e. networks for which the information moves only in the forward direction without any loop)."
  },
  {
    "objectID": "ml/pt_nn_slides.html#transformers",
    "href": "ml/pt_nn_slides.html#transformers",
    "title": "NN vs biological neurons Types of NN",
    "section": "Transformers",
    "text": "Transformers\nA combination of two RNNs (the encoder and the decoder) is used in sequence to sequence models for translation or picture captioning.\nIn 2014 the concept of attention (giving added weight to important words) was developed, greatly improving the ability of such models to process a lot of data.\nThe problem with recurrence is that it is not easily to parallelize (and thus to run fast on GPUs).\nIn 2017, a new model—the transformer—was proposed: by using only attention mechanisms and no recurrence, the transformer achieves better results in an easily parallelizable fashion.\nWith the addition of transfer learning, powerful transformers emerged in the field of NLP (e.g. Bidirectional Encoder Representations from Transformers (BERT) from Google and Generative Pre-trained Transformer-3 (GPT-3) from OpenAI)."
  },
  {
    "objectID": "ml/pt_model.html",
    "href": "ml/pt_model.html",
    "title": "Building a model",
    "section": "",
    "text": "Key to creating neural networks in PyTorch is the torch.nn package which contains the nn.Module and a forward method which returns an output from some input.\nLet’s build a neural network to classify the MNIST.\n\nFirst, we need to define the architecture of the network. There are many types of architectures. For images, CNN are well suited.\nIn Python, you can define a subclass of an existing class with:\nclass YourSubclass(BaseClass):\n    &lt;definition of your subclass&gt;        \nThe subclass is derived from the base class and inherits its properties. PyTorch contains the class torch.nn.Module which is used as the base class when defining a neural network.\n\n# Load packages\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Net(nn.Module):\n    # Define the architecture of the network\n    def __init__(self):\n        super(Net, self).__init__()\n        # 1 input image channel, 6 output channels,\n        # 5x5 square convolution kernel\n        self.conv1 = nn.Conv2d(1, 6, 5)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        # an affine operation: y = Wx + b\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 5*5 from image dimension\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    # Set the flow of data through the network for the forward pass\n    # x represents the data\n    def forward(self, x):\n        # Max pooling over a (2, 2) window\n        # F.relu is the rectified-linear activation function\n        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n        # If the size is a square, you can specify with a single number\n        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n        # flatten all dimensions except the batch dimension\n        x = torch.flatten(x, 1)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\nLet’s create an instance of Net and print its structure:\n\nnet = Net()\nprint(net)\n\nNet(\n  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n  (fc1): Linear(in_features=400, out_features=120, bias=True)\n  (fc2): Linear(in_features=120, out_features=84, bias=True)\n  (fc3): Linear(in_features=84, out_features=10, bias=True)\n)\n\n\n\nparams = list(net.parameters())\nprint(len(params))\nprint(params[0].size())  # conv1's .weight\n\n10\ntorch.Size([6, 1, 5, 5])"
  },
  {
    "objectID": "ml/pt_intro_slides.html#ml-allows-to-achieve-previously-impossible-tasks",
    "href": "ml/pt_intro_slides.html#ml-allows-to-achieve-previously-impossible-tasks",
    "title": "Introduction to machine learning",
    "section": "ML allows to achieve previously impossible tasks",
    "text": "ML allows to achieve previously impossible tasks\n\nLet’s take the example of image recognition:\n\nIn typical computing, a programmer writes code that gives a computer detailed instructions of what to do.\nCoding all the possible ways—pixel by pixel—that an image can represent, say, a dog is an impossibly large task: there are many breeds of dogs, the image can be a picture, a blurred picture, a drawing, a cartoon, the dog can be in all sorts of positions, wearing clothes, etc.\nThere just aren’t enough resources to make the traditional programming approach able to create a computer program that can identify a dog in images.\nBy feeding a very large number of dog images to a neural network however, we can train that network to recognize dogs in images that it has never seen (without explicitly programming how it does this!)."
  },
  {
    "objectID": "ml/pt_intro_slides.html#old-concept-new-computing-power",
    "href": "ml/pt_intro_slides.html#old-concept-new-computing-power",
    "title": "Introduction to machine learning",
    "section": "Old concept … new computing power",
    "text": "Old concept … new computing power\nThe concept is everything but new: Arthur Samuel came up with it in 1949 and built a self-learning Checkers-playing program in 1959.\n\n\nMachine learning consists of feeding vast amounts of data to algorithms to strengthen pathways, so the excitement for the approach became somewhat dormant due to the lack of computing power and the lack of training data at the time.\nThe advent of powerful computers, GPUs, and massive amounts of data have brought the old concept to the forefront.\n\n\n\n\n\n\nFrom xkcd.com"
  },
  {
    "objectID": "ml/pt_intro_slides.html#supervised-learning",
    "href": "ml/pt_intro_slides.html#supervised-learning",
    "title": "Introduction to machine learning",
    "section": "Supervised learning",
    "text": "Supervised learning\n\nRegression is a form of supervised learning with continuous outputs\nClassification is supervised learning with discrete outputs\n\nSupervised learning uses training data in the form of example input/output pairs.\nGoal\nFind the relationship between inputs and outputs."
  },
  {
    "objectID": "ml/pt_intro_slides.html#unsupervised-learning",
    "href": "ml/pt_intro_slides.html#unsupervised-learning",
    "title": "Introduction to machine learning",
    "section": "Unsupervised learning",
    "text": "Unsupervised learning\nClustering, social network analysis, market segmentation, PCA … are all forms of unsupervised learning.\nUnsupervised learning uses unlabelled data.\nGoal\nFind structure within the data."
  },
  {
    "objectID": "ml/pt_intro_slides.html#reinforcement-learning",
    "href": "ml/pt_intro_slides.html#reinforcement-learning",
    "title": "Introduction to machine learning",
    "section": "Reinforcement learning",
    "text": "Reinforcement learning\nThe algorithm explores by performing random actions and these actions are rewarded or punished (bonus points or penalties).\nThis is how algorithms learn to play games."
  },
  {
    "objectID": "ml/pt_intro_slides.html#decide-on-an-architecture",
    "href": "ml/pt_intro_slides.html#decide-on-an-architecture",
    "title": "Introduction to machine learning",
    "section": "Decide on an architecture",
    "text": "Decide on an architecture\n\nThe architecture won’t change during training.\nThe type of architecture you choose (e.g. CNN, Transformer) depends on the type of data you have (e.g. vision, textual). The depth and breadth of your network depend on the amount of data and computing resource you have."
  },
  {
    "objectID": "ml/pt_intro_slides.html#set-some-initial-parameters",
    "href": "ml/pt_intro_slides.html#set-some-initial-parameters",
    "title": "Introduction to machine learning",
    "section": "Set some initial parameters",
    "text": "Set some initial parameters\n\nYou can initialize them randomly or get much better ones through transfer learning.\nWhile the parameters are also part of the model, those will change during training."
  },
  {
    "objectID": "ml/pt_intro_slides.html#get-some-labelled-data",
    "href": "ml/pt_intro_slides.html#get-some-labelled-data",
    "title": "Introduction to machine learning",
    "section": "Get some labelled data",
    "text": "Get some labelled data\n\nWhen we say that we need a lot of data for machine learning, we mean “lots of labelled data” as this is what gets used for training models."
  },
  {
    "objectID": "ml/pt_intro_slides.html#make-sure-to-keep-some-data-for-testing",
    "href": "ml/pt_intro_slides.html#make-sure-to-keep-some-data-for-testing",
    "title": "Introduction to machine learning",
    "section": "Make sure to keep some data for testing",
    "text": "Make sure to keep some data for testing\n\nThose data won’t be used for training the model. Often people keep around 20% of their data for testing."
  },
  {
    "objectID": "ml/pt_intro_slides.html#pass-data-and-parameters-through-the-architecture",
    "href": "ml/pt_intro_slides.html#pass-data-and-parameters-through-the-architecture",
    "title": "Introduction to machine learning",
    "section": "Pass data and parameters through the architecture",
    "text": "Pass data and parameters through the architecture\n\nThe train data are the inputs and the process of calculating the outputs is the forward pass."
  },
  {
    "objectID": "ml/pt_intro_slides.html#the-outputs-of-the-model-are-predictions",
    "href": "ml/pt_intro_slides.html#the-outputs-of-the-model-are-predictions",
    "title": "Introduction to machine learning",
    "section": "The outputs of the model are predictions",
    "text": "The outputs of the model are predictions"
  },
  {
    "objectID": "ml/pt_intro_slides.html#compare-those-predictions-to-the-train-labels",
    "href": "ml/pt_intro_slides.html#compare-those-predictions-to-the-train-labels",
    "title": "Introduction to machine learning",
    "section": "Compare those predictions to the train labels",
    "text": "Compare those predictions to the train labels\n\nSince our data was labelled, we know what the true outputs are."
  },
  {
    "objectID": "ml/pt_intro_slides.html#calculate-train-loss",
    "href": "ml/pt_intro_slides.html#calculate-train-loss",
    "title": "Introduction to machine learning",
    "section": "Calculate train loss",
    "text": "Calculate train loss\n\nThe deviation of our predictions from the true outputs gives us a measure of training loss."
  },
  {
    "objectID": "ml/pt_intro_slides.html#adjust-parameters",
    "href": "ml/pt_intro_slides.html#adjust-parameters",
    "title": "Introduction to machine learning",
    "section": "Adjust parameters",
    "text": "Adjust parameters\n\nThe parameters get automatically adjusted to reduce the training loss through the mechanism of backpropagation. This is the actual training part.\nThis process is repeated many times. Training models is pretty much a giant for loop."
  },
  {
    "objectID": "ml/pt_intro_slides.html#from-model-to-program",
    "href": "ml/pt_intro_slides.html#from-model-to-program",
    "title": "Introduction to machine learning",
    "section": "From model to program",
    "text": "From model to program\n\nRemember that the model architecture is fixed, but that the parameters change at each iteration of the training process."
  },
  {
    "objectID": "ml/pt_intro_slides.html#evaluate-the-model",
    "href": "ml/pt_intro_slides.html#evaluate-the-model",
    "title": "Introduction to machine learning",
    "section": "Evaluate the model",
    "text": "Evaluate the model\n\nWe can now use the testing set (which was never used to train the model) to evaluate our model: if we pass the test inputs through our program, we get some predictions that we can compare to the test labels (which are the true outputs).\nThis gives us the test loss: a measure of how well our model performs."
  },
  {
    "objectID": "ml/pt_intro_slides.html#use-the-model",
    "href": "ml/pt_intro_slides.html#use-the-model",
    "title": "Introduction to machine learning",
    "section": "Use the model",
    "text": "Use the model\n\nNow that we have a program, we can use it on unlabelled inputs to get what people ultimately want: unknown outputs.\nThis is when we put our model to actual use to solve some problem.\n\n\n\n Back to the course"
  },
  {
    "objectID": "ml/pt_hpc.html",
    "href": "ml/pt_hpc.html",
    "title": "Machine learning on production clusters",
    "section": "",
    "text": "This section is a summary of relevant information while using Python in an HPC context for deep learning."
  },
  {
    "objectID": "ml/pt_hpc.html#run-code-in-a-job",
    "href": "ml/pt_hpc.html#run-code-in-a-job",
    "title": "Machine learning on production clusters",
    "section": "Run code in a job",
    "text": "Run code in a job\nWhen you ssh into one of the Alliance clusters, you log into the login node.\nEverybody using a cluster uses that node to enter the cluster. Do not run anything computationally intensive on this node or you would make the entire cluster very slow for everyone. To run your code, you need to start an interactive job or submit a batch job to Slurm (the job scheduler used by the Alliance clusters)."
  },
  {
    "objectID": "ml/pt_hpc.html#plots",
    "href": "ml/pt_hpc.html#plots",
    "title": "Machine learning on production clusters",
    "section": "Plots",
    "text": "Plots\nDo not run code that displays plots on screen. Instead, have them written to files."
  },
  {
    "objectID": "ml/pt_hpc.html#data",
    "href": "ml/pt_hpc.html#data",
    "title": "Machine learning on production clusters",
    "section": "Data",
    "text": "Data\n\nCopy files to/from the cluster\n\nFew files\nIf you need to copy files to or from the cluster, you can use scp from your local machine.\n\nCopy file from your computer to the cluster\n[local]$ scp &lt;/local/path/to/file&gt; &lt;user&gt;@&lt;hostname&gt;:&lt;path/in/cluster&gt;\n\nExpressions between the &lt; and &gt; signs need to be replaced by the relevant information (without those signs).\n\n\n\nCopy file from the cluster to your computer\n[local]$ scp &lt;user&gt;@&lt;hostname&gt;:&lt;cluster/path/to/file&gt; &lt;/local/path&gt;\n\n\n\nLarge amount of data\nUse Globus for large data transfers.\n\nThe Alliance is starting to store classic ML datasets on its clusters. So if your research uses a common dataset, it may be worth inquiring whether it might be available before downloading a copy.\n\n\n\n\nLarge collections of files\nThe Alliance clusters are optimized for very large files and are slowed by large collections of small files. Datasets with many small files need to be turned into single-file archives with tar. Failing to do so will affect performance not just for you, but for all users of the cluster.\n$ tar cf &lt;data&gt;.tar &lt;path/to/dataset/directory&gt;/*\n\n\nIf you want to also compress the files, replace tar cf with tar czf\nAs a modern alternative to tar, you can use Dar"
  },
  {
    "objectID": "ml/pt_hpc.html#interactive-jobs",
    "href": "ml/pt_hpc.html#interactive-jobs",
    "title": "Machine learning on production clusters",
    "section": "Interactive jobs",
    "text": "Interactive jobs\nInteractive jobs are useful for code testing and development. They are not however the most efficient way to run code, so you should limit their use to testing and development.\nYou start an interactive job with:\n$ salloc --account=def-&lt;account&gt; --cpus-per-task=&lt;n&gt; --gres=gpu:&lt;n&gt; --mem=&lt;mem&gt; --time=&lt;time&gt;\nOur training cluster does not have GPUs, so for this workshop, do not use the --gres=gpu:&lt;n&gt; option.\nFor the workshop, you also don’t have to worry about the --account=def-&lt;account&gt; option (or, if you want, you can use --account=def-sponsor00).\nOur training cluster has a total of 60 CPUs on 5 compute nodes. Since there are many of you in this workshop, please be very mindful when running interactive jobs: if you request a lot of CPUs for a long time, the other workshop attendees won’t be able to use the cluster anymore until your interactive job requested time ends (even if you aren’t running any code).\nHere are my suggestions so that we don’t run into this problem:\n\nOnly start interactive jobs when you need to understand what Python is doing at every step, or to test, explore, and develop code (so where an interactive Python shell is really beneficial). Once you have a model, submit a batch job to Slurm instead\nWhen running interactive jobs on this training cluster, only request 1 CPU (so --cpus-per-task=1)\nOnly request the time that you will really use (e.g. for the lesson on Python tensors, maybe 30 min to 1 hour seems reasonable)\nIf you don’t need your job allocation anymore before it runs out, you can relinquish it with Ctrl+d\n\n\nBe aware that, on Cedar, you are not allowed to submit jobs from ~/home. Instead, you have to submit jobs from ~/scratch or ~/project."
  },
  {
    "objectID": "ml/pt_hpc.html#batch-jobs",
    "href": "ml/pt_hpc.html#batch-jobs",
    "title": "Machine learning on production clusters",
    "section": "Batch jobs",
    "text": "Batch jobs\nAs soon as you have a working Python script, you want to submit a batch job instead of running an interactive job. To do that, you need to write an sbatch script.\n\nJob script\n\nHere is an example script:\n\n#!/bin/bash\n#SBATCH --job-name=&lt;name&gt;*            # job name\n#SBATCH --account=def-&lt;account&gt;\n#SBATCH --time=&lt;time&gt;                 # max walltime in D-HH:MM or HH:MM:SS\n#SBATCH --cpus-per-task=&lt;number&gt;      # number of cores\n#SBATCH --gres=gpu:&lt;type&gt;:&lt;number&gt;    # type and number of GPU(s) per node\n#SBATCH --mem=&lt;mem&gt;                   # max memory (default unit is MB) per node\n#SBATCH --output=%x_%j.out*           # file name for the output\n#SBATCH --error=%x_%j.err*            # file name for errors\n#SBATCH --mail-user=&lt;email_address&gt;*\n#SBATCH --mail-type=ALL*\n\n# Load modules\n# (Do not use this in our workshop since we aren't using GPUs)\n# (Note: loading the Python module is not necessary\n# when you activate a Python virtual environment)\n# module load cudacore/.10.1.243 cuda/10 cudnn/7.6.5\n\n# Create a variable with the directory for your ML project\nSOURCEDIR=~/&lt;path/project/dir&gt;\n\n# Activate your Python virtual environment\nsource ~/env/bin/activate\n\n# Transfer and extract data to a compute node\nmkdir $SLURM_TMPDIR/data\ntar xf ~/projects/def-&lt;user&gt;/&lt;data&gt;.tar -C $SLURM_TMPDIR/data\n\n# Run your Python script on the data\npython $SOURCEDIR/&lt;script&gt;.py $SLURM_TMPDIR/data\n\n\n%x will get replaced by the script name and %j by the job number\nIf you compressed your data with tar czf, you need to extract it with tar xzf\nSBATCH options marked with a * are optional\nThere are various other options for email notifications\n\n\nYou may wonder why we transferred data to a compute node. This makes any I/O operation involving your data a lot faster, so it will speed up your code. Here is how this works:\nFirst, we create a temporary data directory in $SLURM_TMPDIR:\n$ mkdir $SLURM_TMPDIR/data\n\nThe variable $SLURM_TMPDIR is created by Slurm on the compute node where a job is running. Its path is /localscratch/&lt;user&gt;.&lt;jobid&gt;.0. Anything in it gets deleted when the job is done.\n\nThen we extract the data into it:\n$ tar xf ~/projects/def-&lt;user&gt;/&lt;data&gt;.tar -C $SLURM_TMPDIR/data\nIf your data is not in a tar file, you can simply copy it to the compute node running your job:\n$ cp -r ~/projects/def-&lt;user&gt;/&lt;data&gt; $SLURM_TMPDIR/data\n\n\nJob handling\n\nSubmit a job\n$ cd &lt;/dir/containing/job&gt;\n$ sbatch &lt;jobscript&gt;.sh\n\n\nCheck the status of your job(s)\n$ sq\n\nPD = pending\nR = running\nCG = completing (Slurm is doing the closing processes)\nNo information = your job has finished running\n\n\n\nCancel a job\n$ scancel &lt;jobid&gt;\n\n\nDisplay efficiency measures of a completed job\n$ seff &lt;jobid&gt;"
  },
  {
    "objectID": "ml/pt_hpc.html#gpus",
    "href": "ml/pt_hpc.html#gpus",
    "title": "Machine learning on production clusters",
    "section": "GPU(s)",
    "text": "GPU(s)\n\nGPU types\nSeveral Alliance clusters have GPUs. Their numbers and types differ:\n From the Alliance Wiki\nThe default is 12G P100, but you can request another type with SBATCH --gres=gpu:&lt;type&gt;:&lt;number&gt; (example: --gres=gpu:p100l:1 to request a 16G P100 on Cedar). Please refer to the Alliance Wiki for more details.\n\n\nNumber of GPU(s)\nTry running your model on a single GPU first.\nIt is very likely that you do not need more than one GPU. Asking for more than you need will greatly increase your waiting time until your job is run. The lesson on distributed computing with PyTorch gives a few information as to when you might benefit from using several GPUs and provides some links to more resources. We will also offer workshops on distributed ML in the future. In any event, you should test your model before asking for several GPUs.\n\n\nCPU/GPU ratio\nHere are the Alliance recommendations:\nBéluga:\nNo more than 10 CPU per GPU.\nCedar:\nP100 GPU: no more than 6 CPU per GPU.\nV100 GPU: no more than 8 CPU per GPU.\nGraham:\nNo more than 16 CPU per GPU."
  },
  {
    "objectID": "ml/pt_hpc.html#code-testing",
    "href": "ml/pt_hpc.html#code-testing",
    "title": "Machine learning on production clusters",
    "section": "Code testing",
    "text": "Code testing\nIt might be wise to test your code in an interactive job before submitting a really big batch job to Slurm.\n\nActivate your Python virtual environment\n$ source ~/env/bin/activate\n\n\nStart an interactive job\n\nExample:\n\n$ salloc --account=def-&lt;account&gt; --gres=gpu:1 --cpus-per-task=6 --mem=32000 --time=0:30:0\n\n\nPrepare the data\nCreate a temporary data directory in $SLURM_TMPDIR:\n(env) $ mkdir $SLURM_TMPDIR/data\n\nThe variable $SLURM_TMPDIR is created by Slurm on the compute node where a job is running. Its path is /localscratch/&lt;user&gt;.&lt;jobid&gt;.0. Anything in it gets deleted when the job is done.\n\nExtract the data into it:\n(env) $ tar xf ~/projects/def-&lt;user&gt;/&lt;data&gt;.tar -C $SLURM_TMPDIR/data\n\n\nTry to run your code\nPlay in Python to test your code:\n(env) $ python\n&gt;&gt;&gt; import torch\n&gt;&gt;&gt; ...\nTo exit the virtual environment, run:\n(env) $ deactivate"
  },
  {
    "objectID": "ml/pt_hpc.html#checkpoints",
    "href": "ml/pt_hpc.html#checkpoints",
    "title": "Machine learning on production clusters",
    "section": "Checkpoints",
    "text": "Checkpoints\nLong jobs should have a checkpoint at least every 24 hours. This ensures that an outage won’t lead to days of computation lost and it will help get the job started by the scheduler sooner.\nFor instance, you might want to have checkpoints every n epochs (choose n so that n epochs take less than 24 hours to run).\nIn PyTorch, you can create dictionaries with all the information necessary and save them as .tar files with torch.save(). You can then load them back with torch.load().\nThe information you want to save in each checkpoint includes the model’s state_dict, the optimizer’s state_dict, the epoch at which you stopped, the latest training loss, and anything else needed to restart training where you left off.\n\nFor example, saving a checkpoint during training could look something like this:\n\ntorch.save({\n    'epoch': &lt;last epoch run&gt;,\n    'model_state_dict': net.state_dict(),\n    'optimizer_state_dict': optimizer.state_dict(),\n    'loss': &lt;latest loss&gt;,\n}, &lt;path/to/checkpoint-file.tar&gt;)\n\nTo restart, initialize the model and optimizer, load the dictionary, and resume training:\n\n# Initialize the model and optimizer\nmodel = &lt;your model&gt;\noptimizer = &lt;your optimizer&gt;\n\n# Load the dictionary\ncheckpoint = torch.load(&lt;path/to/checkpoint-file.tar&gt;)\nmodel.load_state_dict(checkpoint['model_state_dict'])\noptimizer.load_state_dict(checkpoint['optimizer_state_dict'])\nepoch = checkpoint['epoch']\nloss = checkpoint['loss']\n\n# Resume training\nmodel.train()"
  },
  {
    "objectID": "ml/pt_hpc.html#tensorboard-on-the-cluster",
    "href": "ml/pt_hpc.html#tensorboard-on-the-cluster",
    "title": "Machine learning on production clusters",
    "section": "TensorBoard on the cluster",
    "text": "TensorBoard on the cluster\nTensorBoard allows to visually track your model metrics (e.g. loss, accuracy, model graph, etc.). It requires a lot of processing power however, so if you want to use it on an Alliance cluster, do not run it from the login node. Instead, run it as part of your job. This section guides you through the whole workflow.\n\nLaunch TensorBoard\nFirst, you need to launch TensorBoard in the background (with a trailing &) before running your Python script. To do so, ad to your sbatch script:\ntensorboard --logdir=/tmp/&lt;your log dir&gt; --host 0.0.0.0 &\n\nExample:\n\n#!/bin/bash\n#SBATCH ...\n...\n\ntensorboard --logdir=/tmp/&lt;your log dir&gt; --host 0.0.0.0 &\npython $SOURCEDIR/&lt;script&gt;.py $SLURM_TMPDIR/data\n\n\nCreate a connection between the compute node and your computer\nOnce the job is running, you need to create a connection between the compute node running TensorBoard and your computer.\nFirst, you need to find the hostname of the compute node running the Tensorboard server. This is the value under NODELIST for your job when you run:\n$ sq\nThen, from your computer, enter this ssh command:\n[local]$ ssh -N -f -L localhost:6006:&lt;node hostname&gt;:6006 &lt;user&gt;@&lt;cluster&gt;.computecanada.ca\n\nReplace &lt;node hostname&gt; by the compute node hostname you just identified, &lt;user&gt; by your user name, and &lt;cluster&gt; by the name of the Alliance cluster hostname—e.g. beluga, cedar, graham.\n\n\n\nAccess TensorBoard\nYou can now open a browser (on your computer) and go to http://localhost:6006 to monitor your model running on a compute node in the cluster!"
  },
  {
    "objectID": "ml/pt_hpc.html#running-several-similar-jobs",
    "href": "ml/pt_hpc.html#running-several-similar-jobs",
    "title": "Machine learning on production clusters",
    "section": "Running several similar jobs",
    "text": "Running several similar jobs\nA number of ML tasks (e.g. hyperparameter optimization) require running several instances of similar jobs. Grouping them into a single job with GLOST or GNU Parallel reduces the stress on the scheduler."
  },
  {
    "objectID": "ml/pt_data.html",
    "href": "ml/pt_data.html",
    "title": "Loading data",
    "section": "",
    "text": "import torch\nimport torchvision\nimport torchvision.transforms as transforms\ntransform = transforms.Compose(\n    [transforms.ToTensor(),\n     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n\nbatch_size = 4\n\ntrainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n                                        download=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n                                          shuffle=True, num_workers=2)\n\ntestset = torchvision.datasets.CIFAR10(root='./data', train=False,\n                                       download=True, transform=transform)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n                                         shuffle=False, num_workers=2)\n\nclasses = ('plane', 'car', 'bird', 'cat',\n           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n\nFiles already downloaded and verified\nFiles already downloaded and verified\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# functions to show an image\n\n\ndef imshow(img):\n    img = img / 2 + 0.5     # unnormalize\n    npimg = img.numpy()\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.show()\n\n\n# get some random training images\ndataiter = iter(trainloader)\nimages, labels = next(dataiter)\n\n# show images\nimshow(torchvision.utils.make_grid(images))\n# print labels\nprint(' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))\n\n\n\n\ntruck frog  cat   truck"
  },
  {
    "objectID": "ml/pt_data.html#creating-a-dataloader",
    "href": "ml/pt_data.html#creating-a-dataloader",
    "title": "Loading data",
    "section": "Creating a DataLoader",
    "text": "Creating a DataLoader\nA DataLoader is an iterable feeding data to a model. When we train a model, we run it for each element of the DataLoader in a for loop:\nfor i in data_loader:\n    &lt;some model&gt;"
  },
  {
    "objectID": "ml/pt_choosing_frameworks.html",
    "href": "ml/pt_choosing_frameworks.html",
    "title": "Which framework to choose?",
    "section": "",
    "text": "With the growing popularity of machine learning, many frameworks have appeared in various languages. One of the questions you might be facing is: which tool should I choose?\nThe main focus here is on the downsides of proprietary tools."
  },
  {
    "objectID": "ml/pt_choosing_frameworks.html#points-worth-considering",
    "href": "ml/pt_choosing_frameworks.html#points-worth-considering",
    "title": "Which framework to choose?",
    "section": "Points worth considering",
    "text": "Points worth considering\nThere are a several points you might want to consider in making that choice. For instance, what tools do people use in your field? (what tools are used in the papers you read?) What tools are your colleagues and collaborators using?\nLooking a bit further into the future, whether you are considering staying in academia or working in industry could also influence your choice. If the former, paying attention to the literature is key, if the latter, it may be good to have a look at the trends in job postings.\nSome frameworks offer collections of already-made toolkits. They are thus easy to start using and do not require a lot of programming experience. On the other hand, they may feel like black boxes and they are not very customizable. Scikit-learn and Keras—usually run on top of TensorFlow—fall in that category. Lower level tools allow you full control and tuning of your models, but can come with a steeper learning curve.\nPyTorch, developed by Facebook’s AI Research lab, has seen a huge increase in popularity in research in recent years due to its highly pythonic syntax, very convenient tensors, just-in-time (JIT) compilation, dynamic computation graphs, and because it is free and open-source.\nSeveral libraries are now adding a higher level on top of PyTorch: fastai, which we will use in this course, PyTorch Lightning, and PyTorch Ignite. fastai, in addition to the convenience of being able to write a model in a few lines of code, allows to dive as low as you choose into the PyTorch code, thus making it unconstrained by the optional ease of use. It also adds countless functionality. The downside of using this added layer is that it can make it less straightforward to install on a machine or to tweak and customize.\nThe most popular machine learning library currently remains TensorFlow, developed by the Google Brain Team. While it has a Python API, its syntax can be more obscure.\nJulia’s syntax is well suited for the implementation of mathematical models, GPU kernels can be written directly in Julia, and Julia’s speed is attractive in computation hungry fields. So Julia has also seen the development of many ML packages such as Flux or Knet. The user base of Julia remains quite small however.\nMy main motivation in writing this section however is to raise awareness about one question that should really be considered: whether the tool you decide to learn and use in your research is open-source or proprietary."
  },
  {
    "objectID": "ml/pt_choosing_frameworks.html#proprietary-tools-a-word-of-caution",
    "href": "ml/pt_choosing_frameworks.html#proprietary-tools-a-word-of-caution",
    "title": "Which framework to choose?",
    "section": "Proprietary tools: a word of caution",
    "text": "Proprietary tools: a word of caution\nAs a student, it is tempting to have the following perspective:\n\nMy university pays for this very expensive license. I have free access to this very expensive tool. It would be foolish not to make use of it while I can!\n\nWhen there are no equivalent or better open-source tools, that might be true. But when superior open-source tools exist, these university licenses are more of a trap than a gift.\nHere are some of the reasons you should be wary of proprietary tools:\n\nResearchers who do not have access to the tool cannot reproduce your methods\nLarge Canadian universities may offer a license for the tool, but grad students in other countries, independent researchers, researchers in small organizations, etc. may not have access to a free license (or tens of thousands of dollars to pay for it).\n\n\nOnce you graduate, you may not have access to the tool anymore\nOnce you leave your Canadian institution, you may become one of those researchers who do not have access to that tool. This means that you will not be able to re-run your thesis analyses, re-use your methods, and apply the skills you learnt. The time you spent learning that expensive tool you could play with for free may feel a lot less like a gift then.\n\n\nYour university may stop paying for a license\nAs commercial tools fall behind successful open-source ones, some universities may decide to stop purchasing a license. It happened during my years at SFU with an expensive and clunky citation manager which, after having been promoted for years by the library through countless workshops, got abandoned in favour of a much better free and open-source one.\n\n\nYou may get locked-in\nProprietary tools often come with proprietary formats and, depending on the tool, it may be painful (or impossible) to convert your work to another format. When that happens, you are locked-in.\n\n\nProprietary tools are often black boxes\nIt is often impossible to see the source code of proprietary software.\n\n\nLong-term access\nIt is often very difficult to have access to old versions of proprietary tools (and this can be necessary to reproduce old studies). When companies disappear, the tools they produced usually disappear with them. open-source tools, particularly those who have their code under version control in repositories such as GitHub, remain fully accessible (including all stages of development), and if they get abandoned, their maintenance can be taken over or restarted by others.\n\n\nThe licenses you have access to may be limiting and a cause of headache\nFor instance, the Alliance does not have an unlimited number of MATLAB licenses. Since these licenses come with complex rules (one license needed for each node, additional licenses for each toolbox, additional licenses for newer tools, etc.), it can quickly become a nightmare to navigate through it all. You may want to have a look at some of the comments in this thread.\n\n\nProprietary tools fall behind popular open-source tools\nEven large teams of software engineers cannot compete against an active community of researchers developing open-source tools. When open-source tools become really popular, the number of users contributing to their development vastly outnumbers what any company can provide. The testing, licensing, and production of proprietary tools are also too slow to keep up with quickly evolving fields of research. (Of course, open-source tools which do not take off and remain absolutely obscure do not see the benefit of a vast community.)\n\n\nProprietary tools often fail to address specialized edge cases needed in research\nIt is not commercially sound to develop cutting edge capabilities so specialized in a narrow subfield that they can only target a minuscule number of customers. But this is often what research needs. With open-source tools, researchers can develop the capabilities that fit their very specific needs. So while commercial tools are good and reliable for large audiences, they are often not the best in research. This explains the success of R over tools such as SASS or Stata in the past decade."
  },
  {
    "objectID": "ml/pt_choosing_frameworks.html#conclusion",
    "href": "ml/pt_choosing_frameworks.html#conclusion",
    "title": "Which framework to choose?",
    "section": "Conclusion",
    "text": "Conclusion\nAll that said, sometimes you don’t have a choice over the tool to use for your research as this may be dictated by the culture in your field or by your supervisor. But if you are free to choose and if superior or equal open-source alternatives exist and are popular, do not fall in the trap of thinking that because your university and the Alliance pay for a license, you should make use of it. It may be free for you—for now—but it can have hidden costs."
  },
  {
    "objectID": "ml/pt_autograd.html",
    "href": "ml/pt_autograd.html",
    "title": "Automatic differentiation",
    "section": "",
    "text": "PyTorch has automatic differentiation capabilities—meaning that it can track all the operations performed on tensors during the forward pass and compute all the gradients automatically for the backpropagation—thanks to its package torch.autograd.\nLet’s have a look at this."
  },
  {
    "objectID": "ml/pt_autograd.html#some-definitions",
    "href": "ml/pt_autograd.html#some-definitions",
    "title": "Automatic differentiation",
    "section": "Some definitions",
    "text": "Some definitions\nDerivative of a function:\nRate of change of a function with a single variable w.r.t. its variable.\nPartial derivative:\nRate of change of a function with multiple variables w.r.t. one variable while other variables are considered as constants.\nGradient:\nVector of partial derivatives of function with several variables.\nDifferentiation:\nCalculation of the derivatives of a function.\nChain rule:\nFormula to calculate the derivatives of composite functions.\nAutomatic differentiation:\nAutomatic computation of partial derivatives by algorithms."
  },
  {
    "objectID": "ml/pt_autograd.html#backpropagation",
    "href": "ml/pt_autograd.html#backpropagation",
    "title": "Automatic differentiation",
    "section": "Backpropagation",
    "text": "Backpropagation\nFirst, we need to talk about backpropagation: the backward pass following each forward pass and which adjusts the model’s parameters to minimize the output of the loss function.\nThe last 2 videos of 3Blue1Brown neural network series explains backpropagation and its manual calculation very well.\n\nWhat is backpropagation? (14 min)\n\n\nThere is one minor terminological error in this video: they call the use of mini-batches stochastic gradient descent. In fact, this is called mini-batch gradient descent. Stochastic gradient descent uses a single example at each iteration.\n\n\n\nHow does backpropagation work? (10 min)"
  },
  {
    "objectID": "ml/pt_autograd.html#automatic-differentiation",
    "href": "ml/pt_autograd.html#automatic-differentiation",
    "title": "Automatic differentiation",
    "section": "Automatic differentiation",
    "text": "Automatic differentiation\nIf we had to do all this manually, it would be absolute hell. Thankfully, many tools—including PyTorch—can do this automatically.\n\nTracking computations\nFor the automation of the calculation of all those derivatives through chain rules, PyTorch needs to track computations during the forward pass.\nPyTorch does not however track all the computations on all the tensors (this would be extremely memory intensive!). To start tracking computations on a vector, set the requires_grad attribute to True:\n\nimport torch\n\nx = torch.ones(2, 4, requires_grad=True)\nx\n\ntensor([[1., 1., 1., 1.],\n        [1., 1., 1., 1.]], requires_grad=True)\n\n\n\nThe grad_fun attribute\nWhenever a tensor is created by an operation involving a tracked tensor, it has a grad_fun attribute:\n\ny = x + 1\ny\n\ntensor([[2., 2., 2., 2.],\n        [2., 2., 2., 2.]], grad_fn=&lt;AddBackward0&gt;)\n\n\n\ny.grad_fn\n\n&lt;AddBackward0 at 0x7f65c5b69b70&gt;\n\n\n\n\nJudicious tracking\nYou don’t want to track more than is necessary. There are multiple ways to avoid tracking what you don’t want.\nYou can stop tracking computations on a tensor with the method detach:\n\nx\n\ntensor([[1., 1., 1., 1.],\n        [1., 1., 1., 1.]], requires_grad=True)\n\n\n\nx.detach_()\n\ntensor([[1., 1., 1., 1.],\n        [1., 1., 1., 1.]])\n\n\nYou can change its requires_grad flag:\n\nx = torch.zeros(2, 3, requires_grad=True)\nx\n\ntensor([[0., 0., 0.],\n        [0., 0., 0.]], requires_grad=True)\n\n\n\nx.requires_grad_(False)\n\ntensor([[0., 0., 0.],\n        [0., 0., 0.]])\n\n\nAlternatively, you can wrap any code you don’t want to track under with torch.no_grad():\n\nx = torch.ones(2, 4, requires_grad=True)\n\nwith torch.no_grad():\n    y = x + 1\n\ny\n\ntensor([[2., 2., 2., 2.],\n        [2., 2., 2., 2.]])\n\n\n\nCompare this with what we just did above.\n\n\n\n\nCalculating gradients\nLet’s calculate gradients manually, then use autograd, in a very simple case: imagine that \\(x\\), \\(y\\), and \\(z\\) are tensors containing the parameters of a model and that the error \\(e\\) could be calculated with the equation:\n\\[e=2x^4-y^3+3z^2\\]\n\nManual derivative calculation\nLet’s see how we would do this manually.\nFirst, we need the model parameters tensors:\n\nx = torch.tensor([1., 2.])\ny = torch.tensor([3., 4.])\nz = torch.tensor([5., 6.])\n\nWe calculate \\(e\\) following the above equation:\n\ne = 2*x**4 - y**3 + 3*z**2\n\nThe gradients of the error \\(e\\) w.r.t. the parameters \\(x\\), \\(y\\), and \\(z\\) are:\n\\[\\frac{de}{dx}=8x^3\\] \\[\\frac{de}{dy}=-3y^2\\] \\[\\frac{de}{dz}=6z\\]\nWe can calculate them with:\n\ngradient_x = 8*x**3\ngradient_x\n\ntensor([ 8., 64.])\n\n\n\ngradient_y = -3*y**2\ngradient_y\n\ntensor([-27., -48.])\n\n\n\ngradient_z = 6*z\ngradient_z\n\ntensor([30., 36.])\n\n\n\n\nAutomatic derivative calculation\nFor this method, we need to define our model parameters with requires_grad set to True:\n\nx = torch.tensor([1., 2.], requires_grad=True)\ny = torch.tensor([3., 4.], requires_grad=True)\nz = torch.tensor([5., 6.], requires_grad=True)\n\n\\(e\\) is calculated in the same fashion (except that here, all the computations on \\(x\\), \\(y\\), and \\(z\\) are tracked):\n\ne = 2*x**4 - y**3 + 3*z**2\n\nThe backward propagation is done automatically with:\n\ne.backward(torch.tensor([1., 1.]))\n\nAnd we have our 3 partial derivatives:\n\nprint(x.grad)\nprint(y.grad)\nprint(z.grad)\n\ntensor([ 8., 64.])\ntensor([-27., -48.])\ntensor([30., 36.])\n\n\n\n\nComparison\nThe result is the same, as can be tested with:\n\n8*x**3 == x.grad\n\ntensor([True, True])\n\n\n\n-3*y**2 == y.grad\n\ntensor([True, True])\n\n\n\n6*z == z.grad\n\ntensor([True, True])\n\n\nOf course, calculating the gradients manually here was extremely easy, but imagine how tedious and lengthy it would be to write the chain rules to calculate the gradients of all the composite functions in a neural network manually…"
  },
  {
    "objectID": "ml/jx_numpy.html",
    "href": "ml/jx_numpy.html",
    "title": "Relation to NumPy",
    "section": "",
    "text": "NumPy is a popular Python scientific API at the core of many libraries. JAX uses a NumPy-inspired API. There are however important differences that we will explore in this section."
  },
  {
    "objectID": "ml/jx_numpy.html#a-numpy-inspired-api",
    "href": "ml/jx_numpy.html#a-numpy-inspired-api",
    "title": "Relation to NumPy",
    "section": "A NumPy-inspired API",
    "text": "A NumPy-inspired API\nNumPy being so popular, JAX comes with a convenient high-level wrapper to NumPy: jax.numpy.\n\nBeing familiar with NumPy is thus an advantage to get started with JAX. The NumPy quickstart is a useful resource.\n\n\nFor a more efficient usage, JAX also comes with a lower-level API: jax.lax."
  },
  {
    "objectID": "ml/jx_numpy.html#differences-with-numpy",
    "href": "ml/jx_numpy.html#differences-with-numpy",
    "title": "Relation to NumPy",
    "section": "Differences with NumPy",
    "text": "Differences with NumPy\n\nFunctionally pure functions\n\nNo side effects\n\n\nOutputs only based on inputs\n\n\n\nPseudorandom number generation (PRNG)\nimport numpy as np  # for comparison\n\nimport jax\nimport jax.numpy as jnp\n\n\nDifferent default data types\nx = np.arange(9).reshape((3, 3))\nprint(x)\ntype(x)\ny = jnp.arange(9).reshape((3, 3))\nprint(y)\ntype(y)\n\n\nImmutable arrays\nx[0, 0] = 9\nprint(x)\ny[0, 0] = 9\nprint(y)\ny = y.at[0, 0].set(10)\nprint(y)"
  },
  {
    "objectID": "ml/jx_numpy.html#integration-with-numpy",
    "href": "ml/jx_numpy.html#integration-with-numpy",
    "title": "Relation to NumPy",
    "section": "Integration with NumPy",
    "text": "Integration with NumPy\ndevice_put()"
  },
  {
    "objectID": "ml/jx_map.html",
    "href": "ml/jx_map.html",
    "title": "How does JAX work?",
    "section": "",
    "text": "Before using JAX, it is critical to understand its functioning: JAX architecture is at the core of its efficiency and flexibility, but also the cause of a number of constraints."
  },
  {
    "objectID": "ml/jx_map.html#map",
    "href": "ml/jx_map.html#map",
    "title": "How does JAX work?",
    "section": "Map",
    "text": "Map\nHere is a schematic of JAX’s functioning:\n\n\n\n\n\n  \n\ntracer\n\n Tracing   \n\njaxpr\n\n Jaxpr (JAX expression) intermediate representation (IR)   \n\ntracer-&gt;jaxpr\n\n    \n\njit\n\n  Just-in-time  (JIT) compilation   \n\nhlo\n\n High-level optimized (HLO) program   \n\njit-&gt;hlo\n\n    \n\nxla\n\n Accelerated  Linear Algebra  (XLA)   \n\nCPU\n\n CPU   \n\nxla-&gt;CPU\n\n    \n\nGPU\n\n GPU   \n\nxla-&gt;GPU\n\n    \n\nTPU\n\n TPU   \n\nxla-&gt;TPU\n\n    \n\ntransform\n\n  Transformations    \n\npy\n\n Pure Python functions   \n\npy-&gt;tracer\n\n   \n\njaxpr-&gt;jit\n\n   \n\njaxpr-&gt;transform\n\n     \n\nhlo-&gt;xla"
  },
  {
    "objectID": "ml/jx_map.html#tracing",
    "href": "ml/jx_map.html#tracing",
    "title": "How does JAX work?",
    "section": "Tracing",
    "text": "Tracing\nTracing happens during the first call of a function. Tracer objects are wrapped around each argument and record all operations performed on them, creating a Jaxpr (JAX expression). It is this intermediate representation—rather than the Python code—that JAX then uses.\nThe tracer objects used to create the Jaxpr contain information about the shape and dtype of the initial Python arguments, but not their values. This means that new inputs with the same shape and dtype will use the cached compiled program directly, skipping the Python code entirely. Inputs with new shape and/or dtype will trigger tracing again (so the Python function gets executed again).\nFunction side-effects are not recorded by the tracers, which means that they are not part of the Jaxprs. They will be executed once (during tracing), but are thereafter absent from the cached compiled program.\nFunctions which use values outside of their arguments (e.g. values from the global environment) will not update the cache if such values change.\nFor these reasons, only functionally pure functions (functions without side effects and which do not rely on values outside their arguments) should be used with JAX."
  },
  {
    "objectID": "ml/jx_map.html#transformations",
    "href": "ml/jx_map.html#transformations",
    "title": "How does JAX work?",
    "section": "Transformations",
    "text": "Transformations\nJAX is essentially a functional programming framework. Transformations are higher-order functions transforming Jaxprs.\nTransformations are composable and include:\n\ngrad: creates a function that evaluates the gradient of the input function,\nvmap: implementation of automatic vectorization,\npmap: implementation of data parallelism across processing units,\n\nand finally, once other necessary transformations have been performed:\n\njit: just-in-time compilation for the XLA."
  },
  {
    "objectID": "ml/jx_map.html#xla",
    "href": "ml/jx_map.html#xla",
    "title": "How does JAX work?",
    "section": "XLA",
    "text": "XLA\nThe XLA (Accelerated Linear Algebra) compiler takes JIT-compiled JAX programs and optimizes them for the available hardware (CPUs, GPUs, or TPUs)."
  },
  {
    "objectID": "ml/jx_ecosystem.html",
    "href": "ml/jx_ecosystem.html",
    "title": "The JAX ecosystem for deep learning",
    "section": "",
    "text": "Several packages have been built on top of JAX to accelerate computations in various domains.\nHere, we will look at a common ecosystem for the development of deep neural networks."
  },
  {
    "objectID": "ml/jx_ecosystem.html#deep-learning-workflow",
    "href": "ml/jx_ecosystem.html#deep-learning-workflow",
    "title": "The JAX ecosystem for deep learning",
    "section": "Deep learning workflow",
    "text": "Deep learning workflow\nTraining a neural network requires a number of steps:\n\n\n\n\n\n  \n\nLoad\n\n Load dataset   \n\nDefine\n\n Define architecture   \n\nLoad-&gt;Define\n\n    \n\nTrain\n\n Train   \n\nDefine-&gt;Train\n\n    \n\nTest\n\n Test   \n\nTrain-&gt;Test\n\n    \n\nSave\n\n Save model   \n\nTest-&gt;Save\n\n   \n\n\n\n\n\n Let’s see how to do this with JAX."
  },
  {
    "objectID": "ml/jx_ecosystem.html#a-common-dl-workflow-with-jax",
    "href": "ml/jx_ecosystem.html#a-common-dl-workflow-with-jax",
    "title": "The JAX ecosystem for deep learning",
    "section": "A common DL workflow with JAX",
    "text": "A common DL workflow with JAX\n\nLoad datasets\n\n\nThere are already good tools to load datasets (e.g. PyTorch, TensorFlow, Hugging Face), so JAX did not worry about creating its own implementation.\n\n\nDefine network architecture\n\n\nNeural networks can be build in JAX from scratch, but a number of packages built on JAX provide the necessary toolkit. Flax is a popular option and the one we will use in this course.\n\n\nTrain\n\n\nTraining a model requires optimization functions. These can be implemented in JAX from scratch but the library Optax provides the core components.\n\n\nTest\n\n\nTesting a model is easy to do directly in JAX.\n\n\nSave model\n\n\nFlax provides methods to save a model.\n\n To sum up, here is a common ecosystem of libraries to use JAX for neural networks:\n\n\n\n\n\n  \n\nLoad\n\nLoad dataset   \n\nDefine\n\nDefine architecture    \n\nPyTorch\n\n PyTorch    \n\nTrain\n\nTrain    \n\nflax1\n\n Flax    \n\nTest\n\nTest    \n\nSave\n\nSave model    \n\nTensorFlow\n\n TensorFlow    \n\nPyTorch-&gt;flax1\n\n    \n\nHugging Face\n\n Hugging Face    \n\nTensorFlow-&gt;flax1\n\n    \n\nHugging Face-&gt;flax1\n\n    \n\njax1\n\n JAX +  Optax   \n\nflax1-&gt;jax1\n\n    \n\nFlax\n\n Flax   \n\nJAX\n\n JAX   \n\njax1-&gt;JAX\n\n    \n\nJAX-&gt;Flax"
  },
  {
    "objectID": "ml/index.html",
    "href": "ml/index.html",
    "title": "Machine learning",
    "section": "",
    "text": "Getting started with  \nAn introductory course to deep learning\n\n\n\n\nFaster deep learning with \nAn introduction to JAX\n\n\n\n\n\n\nA brief overview of  \nTraditional machine learning with scikit-learn\n\n\n\n \n\n\n\n\n \n\n\n \n\n\n\n\n\nWorkshops\nWorkshops on deep learning topics\n\n\n\n\nWebinars\n60 min webinars on deep learning topics"
  },
  {
    "objectID": "julia/wb_makie.html",
    "href": "julia/wb_makie.html",
    "title": "Makie",
    "section": "",
    "text": "There are several popular data visualization libraries for the Julia programming language (e.g. Plots, Gadfly, VegaLite, Makie). They vary in their precompilation time, time to first plot, layout capabilities, ability to handle 3D data, ease of use, and syntax style. In this landscape, Makie focuses on high performance, fancy layouts, and extensibility.\nMakie comes with multiple backends. In this webinar, we will cover:\n\nGLMakie (ideal for interactive 2D and 3D plotting)\nWGLMakie (an equivalent that runs within browsers)\nCairoMakie (best for high-quality vector graphics)\n\nWe will also see how to run Makie in the Alliance clusters.\nSlides (Click and wait: this reveal.js presentation is heavy and takes some time to load.)"
  },
  {
    "objectID": "julia/wb_makie.html#plotting-in-julia",
    "href": "julia/wb_makie.html#plotting-in-julia",
    "title": "Makie",
    "section": "Plotting in Julia",
    "text": "Plotting in Julia\nThere are many options to create plots in Julia. Some of the most popular ones are:\n\nPlots.jl: high-level API for working with different back-ends (GR, Pyplot, Plotly…),\nPyPlot.jl: Julia interface to Matplotlib’s matplotlib.pyplot,\nPlotlyJS.jl: Julia interface to plotly.js,\nPlotlyLight.jl: the fastest plotting option in Julia by far, but limited features,\nGadfly.jl: following the grammar of graphics popularized by Hadley Wickham in R,\nVegaLite.jl: grammar of interactive graphics,\nPGFPlotsX.jl: Julia interface to the PGFPlots LaTeX package,\nUnicodePlots.jl: plots in the terminal 🙂,\nMakie.jl: powerful plotting ecosystem: animation, 3D, GPU optimization.\n\nThis webinar focuses on Makie.jl."
  },
  {
    "objectID": "julia/wb_makie.html#the-makie-ecosystem",
    "href": "julia/wb_makie.html#the-makie-ecosystem",
    "title": "Makie",
    "section": "The Makie ecosystem",
    "text": "The Makie ecosystem\nMakie consists of a core package (Makie), with the plots functionalities.\nIn addition to this, a backend is needed to render plots into images or vector graphics. Three backends are available:\n\nCairoMakie: vector graphics or high-quality 2D plots. Creates, but does not display plots (you need an IDE that does or you can use ElectronDisplay.jl),\nGLMakie: based on OpenGL; 3D rendering and interactivity in GLFW window (no vector graphics),\nWGLMakie: web version of GLMakie (plots rendered in a browser instead of a window)."
  },
  {
    "objectID": "julia/wb_makie.html#resources",
    "href": "julia/wb_makie.html#resources",
    "title": "Makie",
    "section": "Resources",
    "text": "Resources\nHere are some links and resources useful to get started with the Makie ecosystem:\n\nthe official Makie documentation,\nJulia Data Science book, chapter 5,\nthe project Beautiful Makie contains many great plot examples,\ncheatsheets:\n\nfor 2D plotting:\n\n\nFrom: Storopoli, Huijzer and Alonso (2021). Julia Data Science. https://juliadatascience.io. ISBN: 97984898\n\nfor 3D plotting:\n\n\nFrom: Storopoli, Huijzer and Alonso (2021). Julia Data Science. https://juliadatascience.io. ISBN: 97984898"
  },
  {
    "objectID": "julia/wb_makie.html#troubleshooting",
    "href": "julia/wb_makie.html#troubleshooting",
    "title": "Makie",
    "section": "Troubleshooting",
    "text": "Troubleshooting\nCairoMakie and WGLMakie should install without issues. Installing GLMakie however can be challenging. This page may lead you towards a solution."
  },
  {
    "objectID": "julia/wb_makie.html#extensions",
    "href": "julia/wb_makie.html#extensions",
    "title": "Makie",
    "section": "Extensions",
    "text": "Extensions\nA number of extensions have been built on top of Makie:\n\nGeoMakie.jl add geographical plotting utilities to Makie,\nAlgebraOfGraphics.jl turns plotting into a simple algebra of building blocks,\nGraphMakie.jl to create network graphs."
  },
  {
    "objectID": "julia/wb_makie.html#fundamental-functioning",
    "href": "julia/wb_makie.html#fundamental-functioning",
    "title": "Makie",
    "section": "Fundamental functioning",
    "text": "Fundamental functioning\n\nFigure\nLoad the package (here, we are using CairoMakie):\n\nusing CairoMakie                        # no need to import Makie itself\n\nCreate a Figure (container object):\n\nfig = Figure()\n\n\n\n\n\ntypeof(fig)\n\nFigure\n\n\nYou can customize a Figure:\n\nfig2 = Figure(backgroundcolor=:grey22, resolution=(300, 300))\n\n\n\n\nMakie uses the Colors.jl package as a dependency. You can find a list of all named colours here.\nTo use CSS specification (e.g. hex), you need to install Colors explicitly and use its color parsing capabilities:\n\nusing Colors\nfig3 = Figure(backgroundcolor=colorant\"#adc2eb\")\n\n\n\n\n\n\nAxis\nThen, you can create an Axis:\n\nax = Axis(Figure()[1, 1])\n\nAxis with 0 plots:\n\n\n\ntypeof(ax)\n\nAxis\n\n\n\nAxis(fig3[1, 1])  # fig3[1, 1] sets the subplot layout: fig[row, col]\nfig3\n\n\n\n\n\nAxis(fig[2, 3])  # This is what happens if we change the layout\nfig\n\n\n\n\n\nAxis(fig3[2, 3])  # We can add another axis on fig3\nfig3\n\n\n\n\nAxis are customizable:\n\nfig4 = Figure()\nAxis(fig4[1, 1],\n     xlabel=\"x label\",\n     ylabel=\"y label\",\n     title=\"Title of the plot\")\nfig4\n\n\n\n\n\n\nPlot\nFinally, you can add a plot:\n\nfig = Figure()\nax = Axis(fig[1, 1])\nx = LinRange(-10, 10, 20)\ny = x\nscatter!(ax, x, y)  # Functions with ! transform their arguments\nfig\n\n\n\n\nOf course, there are many plotting functions, e.g. scatterlines!:\n\nfig = Figure()\nax = Axis(fig[1, 1])\nx = LinRange(-10, 10, 20)\ny = x\nscatterlines!(ax, x, y)  # Functions with ! transform their arguments\nfig\n\n\n\n\nWe can also use lines!:\n\nfig = Figure()\nax = Axis(fig[1, 1])\nx = LinRange(-10, 10, 20)\ny = sin.(x)  # The . means that the function is broadcast to each element of x\nlines!(ax, x, y)\nfig\n\n\n\n\nLet’s add points to get a smoother line:\n\nfig = Figure()\nax = Axis(fig[1, 1])\nx = LinRange(-10, 10, 1000)\ny = sin.(x)  # The . means that the function is broadcast to each element of x\nlines!(ax, x, y)\nfig\n\n\n\n\nNow, you don’t have to create the Figure, Axis, and plot one at a time. You can create them at the same time with, for instance lines:\n\nx = LinRange(-10, 10, 1000)\ny = sin.(x)\nlines(x, y)  # Note the use of lines instead of lines!\n\n\n\n\nOr even more simply:\n\nx = LinRange(-10, 10, 1000)\nlines(x, sin)\n\n\n\n\nThis is a lot simpler, but it is important to understand the concepts of the Figure and Axis objects as you will need it to customize them:\n\nx = LinRange(-10, 10, 1000)\ny = cos.(x)\nlines(x, y;\n      figure=(; backgroundcolor=:green),\n      axis=(; title=\"Cosinus function\", xlabel=\"x label\", ylabel=\"y label\"))\n\n\n\n\nWhen you create the Figure, Axis, and plot at the same time, you create a FigureAxisPlot object:\n\nx = LinRange(-10, 10, 1000)\ny = cos.(x)\nobj = lines(x, y;\n            figure=(; backgroundcolor=:green),\n            axis=(; title=\"Cosinus function\",\n                  xlabel=\"x label\",\n                  ylabel=\"y label\"));\ntypeof(obj)\n\nMakie.FigureAxisPlot\n\n\n\nNote the ; in the figure and axis value. This is because these are one-element NamedTuples.\n\nThe mutating functions (with !) can be used to add plots to an existing figure, but first, you need to decompose the FigureAxisPlot object:\n\nfig, ax, plot = lines(x, sin)\nlines!(ax, x, cos)  # Remember that we are transforming the Axis object\nfig                 # Now we can plot the transformed Figure\n\n\n\n\nOr we can add several plots on different Axis in the same Figure:\n\nfig, ax1, plot = lines(x, sin)\nax2 = Axis(fig[1, 2])\nlines!(ax2, x, cos)\nfig"
  },
  {
    "objectID": "julia/wb_makie.html#examples",
    "href": "julia/wb_makie.html#examples",
    "title": "Makie",
    "section": "Examples",
    "text": "Examples\n\n2D\n\nusing CairoMakie\nusing StatsBase, LinearAlgebra\nusing Interpolations, OnlineStats\nusing Distributions\nCairoMakie.activate!(type = \"png\")\n\nfunction eq_hist(matrix; nbins = 256 * 256)\n    h_eq = fit(Histogram, vec(matrix), nbins = nbins)\n    h_eq = normalize(h_eq, mode = :density)\n    cdf = cumsum(h_eq.weights)\n    cdf = cdf / cdf[end]\n    edg = h_eq.edges[1]\n    interp_linear = LinearInterpolation(edg, [cdf..., cdf[end]])\n    out = reshape(interp_linear(vec(matrix)), size(matrix))\n    return out\nend\n\nfunction getcounts!(h, fn; n = 100)\n    for _ in 1:n\n        vals = eigvals(fn())\n        x0 = real.(vals)\n        y0 = imag.(vals)\n        fit!(h, zip(x0,y0))\n    end\nend\n\nm(;a=10rand()-5, b=10rand()-5) = [0 0 0 a; -1 -1 1 0; b 0 0 0; -1 -1 -1 -1]\n\nh = HeatMap(range(-3.5,3.5,length=1200), range(-3.5,3.5, length=1200))\ngetcounts!(h, m; n=2_000_000)\n\nwith_theme(theme_black()) do\n    fig = Figure(figure_padding=0,resolution=(600,600))\n    ax = Axis(fig[1,1]; aspect = DataAspect())\n    heatmap!(ax,-3.5..3.5, -3.5..3.5, eq_hist(h.counts); colormap = :bone_1)\n    hidedecorations!(ax)\n    hidespines!(ax)\n    fig\nend\n\n\n\n\n\n\n3D\nusing GLMakie, Random\nGLMakie.activate!()\n\nRandom.seed!(13)\nx = -6:0.5:6\ny = -6:0.5:6\nz = 6exp.( -(x.^2 .+ y' .^ 2)./4)\n\nbox = Rect3(Point3f(-0.5), Vec3f(1))\nn = 100\ng(x) = x^(1/10)\nalphas = [g(x) for x in range(0,1,length=n)]\ncmap_alpha = resample_cmap(:linear_worb_100_25_c53_n256, n, alpha = alphas)\n\nwith_theme(theme_dark()) do\n    fig, ax, = meshscatter(x, y, z;\n                           marker=box,\n                           markersize = 0.5,\n                           color = vec(z),\n                           colormap = cmap_alpha,\n                           colorrange = (0,6),\n                           axis = (;\n                                   type = Axis3,\n                                   aspect = :data,\n                                   azimuth = 7.3,\n                                   elevation = 0.189,\n            perspectiveness = 0.5),\n        figure = (;\n            resolution =(1200,800)))\n    meshscatter!(ax, x .+ 7, y, z./2;\n        markersize = 0.25,\n        color = vec(z./2),\n        colormap = cmap_alpha,\n        colorrange = (0, 6),\n        ambient = Vec3f(0.85, 0.85, 0.85),\n        backlight = 1.5f0)\n    xlims!(-5.5,10)\n    ylims!(-5.5,5.5)\n    hidedecorations!(ax; grid = false)\n    hidespines!(ax)\n    fig\nend\n\n\nFor more examples, have a look at Beautiful Makie."
  },
  {
    "objectID": "julia/wb_makie.html#compiling-sysimages",
    "href": "julia/wb_makie.html#compiling-sysimages",
    "title": "Makie",
    "section": "Compiling sysimages",
    "text": "Compiling sysimages\nWhile Makie is extremely powerful, its compilation time and its time to first plot are extremely long. For this reason, it might save you a lot of time to create a sysimage (a file containing information from a Julia session such as loaded packages, global variables, compiled code, etc.) with PackageCompiler.jl.\n\nThe upcoming Julia 1.9 will do this automatically."
  },
  {
    "objectID": "julia/wb_makie.html#using-the-alliance-clusters",
    "href": "julia/wb_makie.html#using-the-alliance-clusters",
    "title": "Makie",
    "section": "Using the Alliance clusters",
    "text": "Using the Alliance clusters\n\nCairoMakie\nCairoMakie will run without problem on the Alliance clusters. It is not designed for interactivity, so saving to file is what makes the most sense.\n\nExample:\n\nsave(\"graph.png\", fig)\n\nRemember however that CairoMakie is 2D only (for now).\n\n\n\nGLMakie\nGLMakie relies on GLFW to create windows with OpenGL. GLFW doesn’t support creating contexts without an associated window. The dependency GLFW.jl will thus not install in the clusters—even with X11 forwarding—unless you use VDI nodes, VNC, or Virtual GL.\n\n\nWGLMakie\nYou can setup a server with JSServe.jl as per the documentation. However, this method is intended for the creation of interactive widgets, e.g. for a website. While this is really cool, it isn’t optimized for performance. There might also be a way to create an SSH tunnel to your local browser, although there is no documentation on this.\nBest probably is to save to file.\n\nConclusion about the Makie ecosystem on production clusters:\n\n2D plots: use CairoMakie and save to file,\n3D plots: use WGLMakie and save to file."
  },
  {
    "objectID": "julia/wb_firstdab.html",
    "href": "julia/wb_firstdab.html",
    "title": "First dab at Julia",
    "section": "",
    "text": "Julia is fast: just-in-time (JIT) compilation and multiple dispatch bring efficiency to interactivity. People often say that using Julia feels like running R or python with a speed almost comparable to that of C.\nBut Julia also comes with parallel computing and multi-threading capabilities.\nIn this webinar, after a quickly presentation of some of the key features of Julia’s beautifully concise syntax, I will dive into using Julia for HPC."
  },
  {
    "objectID": "julia/intro_tabular.html",
    "href": "julia/intro_tabular.html",
    "title": "Working with tabular data:",
    "section": "",
    "text": "Requirements:\n1 - The current Julia stable release\nInstallation instructions can be found here.\n2 - The packages: CSV, DataFrames, TimeSeries, Plots\nPackages can be installed with ] add &lt;package&gt;.\n3 - Covid-19 data from the Johns Hopkins University CSSE repository\nClone (git clone &lt;repo url&gt;) or download and unzip the repository."
  },
  {
    "objectID": "julia/intro_tabular.html#load-packages",
    "href": "julia/intro_tabular.html#load-packages",
    "title": "Working with tabular data:",
    "section": "Load packages",
    "text": "Load packages\nusing CSV\nusing DataFrames\nusing Dates          # From the standard Julia library\nusing TimeSeries\nusing NamedArrays\nusing Plots\n\nWe will use the GR framework as a backend for Plots."
  },
  {
    "objectID": "julia/intro_tabular.html#data-until-march-22-2020",
    "href": "julia/intro_tabular.html#data-until-march-22-2020",
    "title": "Working with tabular data:",
    "section": "Data until March 22, 2020",
    "text": "Data until March 22, 2020\n\n\nfrom xkcd.com\n\nThe files in the Johns Hopkins University CSSE repository have changed over time.\nIn this workshop, we will use 2 sets of files:\n\na first set from January 22, 2020 until March 22, 2020\na second set from January 22, 2020 to the present\n\nBoth sets contain data on confirmed and dead cases for world countries and in some cases their subregions (provinces, states, etc. which I will globally here call “provinces”).\nThe first set also contains numbers of recovered cases which allow to calculate numbers of currently ill persons (of course, keep in mind that all these data represent various degrees of underestimation and are flawed in many ways, amongst which are varying levels of testing efforts both geographically and over time, under-reporting, etc).\nThe second set does not contain recovered cases (many overwhelmed countries stopped monitoring this at some point).\nWe will play with the first set together and you will then try to play with the second set on your own.\n\nLoad the data\nIf you did not clone or download and unzip the Covid-19 data repository in your working directory, adapt the path consequently.\n#= create a variable with the path we are interested in;\nthis makes the code below a bit shorter =#\ndir = \"COVID-19/csse_covid_19_data/csse_covid_19_time_series\"\n\n# create a list of the full paths of all the files in dir\nlist = joinpath.(relpath(dir), readdir(dir))\n\n#= read in the 3 csv files with confirmed, dead, and recovered numbers\ncorresponding to the first set of data (until March 22, 2020) =#\ndat = DataFrame.(CSV.File.(list[collect(2:4)]))\nWe now have a one-dimensional array of 3 DataFrames called dat.\n\n\nTransform data into long format\n# rename some variables to easier names\nDataFrames.rename!.(dat, Dict.(1 =&gt; Symbol(\"province\"),\n                               2 =&gt; Symbol(\"country\")))\n\n# create a one-dimensional array of strings\nvar = [\"total\", \"dead\", \"recovered\"]\n\n#= transform the data into long format in a vectorized fashion\nusing both our one-dimensional arrays of 3 elements =#\ndatlong = map((x, y) -&gt; stack(x, Not(collect(1:4)),\n                              variable_name = Symbol(\"date\"),\n                              value_name = Symbol(\"$y\")),\n              dat, var)\nWe now have a one-dimensional array of 3 DataFrames in long format called datlong.\n# join all elements of this array into a single DataFrame\nall = join(datlong[1], datlong[2], datlong[3],\n           on = [:date, :country, :province, :Lat, :Long])\n\n# get rid of \"Lat\" and \"Long\" and re-order the columns\nselect!(all, [4, 3, 1, 2, 7, 8])\n\n#= turn the year from 2 digits to 4 digits using regular expression\n(in a vectorised fashion by braodcasting with the dot notation);\nthen turn these values into strings, and finally into dates =#\nall.date = Date.(replace.(string.(all[:, 3]),\n                          r\"(.*)(..)$\" =&gt; s\"\\g&lt;1&gt;20\\2\"), \"m/dd/yy\");\n\n#= replace the missing values by the string \"NA\"\n(these are not real missing values, but rather non applicable ones) =#\nreplace!(all.province, missing =&gt; \"NA\");\nWe now have a single DataFrame called all, in long format, with the variables confirmed, dead, recovered, and ill.\nCalculate the number of currently ill individuals (again, in a vectorized fashion, by broadcasting with the dot notation):\nall.current = all.total .- all.dead .- all.recovered;\n\n\nWorld summary\nTo make a single plot with world totals of confirmed, dead, recovered, and ill cases, we want the sums of these variables for each day. We do this by grouping the data by date:\nworld = by(all, :date,\n           total = :total =&gt; sum,\n           dead = :dead =&gt; sum,\n           recovered = :recovered =&gt; sum,\n           current = :current =&gt; sum)\nNow we can plot our new variable world.\nAs our data is a time series, we need to transform it to a TimeArray thanks to the TimeArray() function from the TimeSeries package.\nplot(TimeArray(world, timestamp = :date),\n     title = \"World\",\n     legend = :outertopright,\n     widen = :false)\n Data until March 22, 2020\n\n\nCountries/provinces summaries\nNow, we want to group the data by country:\ncountries = groupby(all, :country)\nWe also need to know how the authors of the dataset decided to label the various countries and their subregions.\nFor example, if you want to see what the data looks like for France, Canada, and India, you can run:\ncountries[findall(x -&gt; \"France\" in x, keys(countries))]\ncountries[findall(x -&gt; \"Canada\" in x, keys(countries))]\ncountries[findall(x -&gt; \"India\" in x, keys(countries))]\nThen you need to subset the data for the countries or provinces you are interested in.\nHere are some examples:\n# countries for which there are data for several provinces\ncanada = all[all[:, :country] .== \"Canada\", :]\nus = all[all[:, :country] .== \"US\", :]\nchina = all[all[:, :country] .== \"China\", :]\n\n# countries with no province data\nskorea = all[all[:, :country] .== \"Korea, South\", :]\ntaiwan = all[all[:, :country] .== \"Taiwan*\", :]\nsingapore = all[all[:, :country] .== \"Singapore\", :]\nitaly = all[all[:, :country] .== \"Italy\", :]\nspain = all[all[:, :country] .== \"Spain\", :]\n\n#= countries wich have subregions spread widely in the world;\nhere, I took the arbitrary decision to only look at the main subregions =#\nfrance = all[all[:, :province] .== \"France\", :]\nuk = all[all[:, :province] .== \"United Kingdom\", :]\n\n# provinces\nbc = all[all[:, :province] .== \"British Columbia\", :]\nny = all[all[:, :province] .== \"New York\", :]\nCalculate the totals for Canada, US, and China which all have data for subregions:\ncanada, us, china = by.([canada, us, china], :date,\n                        total = :total =&gt; sum,\n                        dead = :dead =&gt; sum,\n                        recovered = :recovered =&gt; sum,\n                        current = :current =&gt; sum)\nloclist1 = [canada, us, china]\nloctitles1 = [\"Canada\", \"US\", \"China\"]\n\npcanada, pus, pchina =\n    map((x, y) -&gt; plot(TimeArray(x, timestamp = :date),\n                       title = \"$y\", legend = :outertopright,\n                       widen = :false, dpi = :300),\n        loclist1, loctitles1)\nloclist2 = [france, bc, ny, taiwan, skorea, singapore, spain, italy, uk]\nloctitles2 = [\"France\", \"BC\", \"NY\", \"Taiwan\", \"South Korea\",\n              \"Singapore\", \"Spain\", \"Italy\", \"UK\"]\n\npfrance, pbc, pny, ptaiwan, pskorea,\npsingapore, pspain, pitaly, puk =\n    map((x, y) -&gt; plot(TimeArray(select(x, Not([:country, :province])),\n                                 timestamp = :date),\n                       title = \"$y\", legend = :outertopright,\n                       widen = :false, dpi = :300),\n        loclist2, loctitles2)\nNow, let’s plot a few countries/provinces:\n\nNorth America\nplot(pcanada, pbc, pus, pny,\n     legend = false, titlefontsize = 7, tickfontsize = 6)\n Data until March 22, 2020\n\n\nAsia\nplot(pchina, ptaiwan, pskorea, psingapore,\n     legend = false, titlefontsize = 7, tickfontsize = 6)\n Data until March 22, 2020\n\n\nEurope\nplot(pfrance, pspain, pitaly, puk,\n     legend = false, titlefontsize = 7, tickfontsize = 6)\n Data until March 22, 2020"
  },
  {
    "objectID": "julia/intro_tabular.html#data-up-to-the-present",
    "href": "julia/intro_tabular.html#data-up-to-the-present",
    "title": "Working with tabular data:",
    "section": "Data up to the present",
    "text": "Data up to the present\n\nSummary graphs\n\n\nYour turn:\n\nWrite the code to create an up-to-date graph for the world using the files: time_series_covid19_confirmed_global.csv and time_series_covid19_deaths_global.csv.\n\nHere is the result:\n Data until March 25, 2020\n\n\nYour turn:\n\nCreate up-to-date graphs for the countries and/or provinces of your choice.\n\nHere are a few possible results:\n Data until March 25, 2020\n\n\nCountries comparison\nOur side by side graphs don’t make comparisons very easy since they vary greatly in their axes scales.\nOf course, we could constrain them to have the same axes, but then, why not plot multiple countries or provinces in the same graph?\ncanada[!, :loc] .= \"Canada\";\nchina[!, :loc] .= \"China\";\n\nall = join(all, canada, china, on = [:date, :total, :dead, :loc],\n           kind = :outer)\n\nconfirmed = unstack(all[:, collect(3:5)], :loc, :total)\n\nconf_sel = select(confirmed,\n                  [:date, :Italy, :Spain, :China, :Iran,\n                   :France, :US, Symbol(\"South Korea\"), :Canada])\n\nplot(TimeArray(conf_sel, timestamp = :date),\n     title = \"Confirmed across a few countries\",\n     legend = :outertopright, widen = :false)\n Data until March 25, 2020\n\n\nYour turn:\n\nWrite the code to make a similar graph with the number of deaths in a few countries of your choice.\n\nHere is a possible result:\n Data until March 25, 2020"
  },
  {
    "objectID": "julia/intro_plotting.html",
    "href": "julia/intro_plotting.html",
    "title": "Plotting",
    "section": "",
    "text": "It can be convenient to plot directly in the REPL (for instance when using SSH).\n\nusing UnicodePlots\nhistogram(randn(1000), nbins=40)\n\n\n                ┌                                        ┐ \n   [-3.8, -3.6) ┤▌ 1                                       \n   [-3.6, -3.4) ┤  0                                       \n   [-3.4, -3.2) ┤  0                                       \n   [-3.2, -3.0) ┤  0                                       \n   [-3.0, -2.8) ┤▌ 1                                       \n   [-2.8, -2.6) ┤█▍ 3                                      \n   [-2.6, -2.4) ┤█▋ 4                                      \n   [-2.4, -2.2) ┤█▍ 3                                      \n   [-2.2, -2.0) ┤██▌ 6                                     \n   [-2.0, -1.8) ┤███████▎ 17                               \n   [-1.8, -1.6) ┤███████▎ 17                               \n   [-1.6, -1.4) ┤█████████████▍ 32                         \n   [-1.4, -1.2) ┤█████████████▊ 33                         \n   [-1.2, -1.0) ┤█████████████████████▍ 51                 \n   [-1.0, -0.8) ┤████████████████████▌ 49                  \n   [-0.8, -0.6) ┤████████████████████████████████▎ 77      \n   [-0.6, -0.4) ┤████████████████████████████████▋ 78      \n   [-0.4, -0.2) ┤████████████████████████████████████  86  \n   [-0.2,  0.0) ┤████████████████████████████▊ 69          \n   [ 0.0,  0.2) ┤████████████████████████████▌ 68          \n   [ 0.2,  0.4) ┤████████████████████████████████▎ 77      \n   [ 0.4,  0.6) ┤█████████████████████████▌ 61             \n   [ 0.6,  0.8) ┤█████████████████████████▉ 62             \n   [ 0.8,  1.0) ┤█████████████████████████▉ 62             \n   [ 1.0,  1.2) ┤███████████████████▋ 47                   \n   [ 1.2,  1.4) ┤█████████████▊ 33                         \n   [ 1.4,  1.6) ┤█████▊ 14                                 \n   [ 1.6,  1.8) ┤██████▍ 15                                \n   [ 1.8,  2.0) ┤███▊ 9                                    \n   [ 2.0,  2.2) ┤█████▌ 13                                 \n   [ 2.2,  2.4) ┤▊ 2                                       \n   [ 2.4,  2.6) ┤██▎ 5                                     \n   [ 2.6,  2.8) ┤  0                                       \n   [ 2.8,  3.0) ┤█▍ 3                                      \n   [ 3.0,  3.2) ┤  0                                       \n   [ 3.2,  3.4) ┤▌ 1                                       \n   [ 3.4,  3.6) ┤  0                                       \n   [ 3.6,  3.8) ┤▌ 1                                       \n                └                                        ┘ \n                                 Frequency                 \n\n\n\nMost of the time however, you will want to make nicer looking graphs. There are many options to plot in Julia.\nPlots is a convenient Julia package which allows to use the same code with several graphing backends such as the GR framework (great for speed), Plotly.js (allows interaction with your graphs in a browser), or PyPlot. The default backend is the GR framework.\nStatsPlots is an enhanced version with added stats functionality.\n\nExample:\n\n\n# First run takes time as the package needs to compile\nusing StatsPlots\nStatsPlots.histogram(randn(1000), bins=40)\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere, we need to explicitly run StatsPlots.histogram rather than histogram to prevent a conflict with the function of the same name from the package UnicodePlots."
  },
  {
    "objectID": "julia/intro_non_interactive.html",
    "href": "julia/intro_non_interactive.html",
    "title": "Non interactive execution",
    "section": "",
    "text": "Julia scripts have a .jl extension.\nThe include function sources a Julia script (in a REPL session or in another script):\ninclude(\"file.jl\")\nThe code contained in file.jl is thus run non interactively."
  },
  {
    "objectID": "julia/intro_non_interactive.html#sourcing-a-file",
    "href": "julia/intro_non_interactive.html#sourcing-a-file",
    "title": "Non interactive execution",
    "section": "",
    "text": "Julia scripts have a .jl extension.\nThe include function sources a Julia script (in a REPL session or in another script):\ninclude(\"file.jl\")\nThe code contained in file.jl is thus run non interactively."
  },
  {
    "objectID": "julia/intro_non_interactive.html#running-code-from-the-command-line",
    "href": "julia/intro_non_interactive.html#running-code-from-the-command-line",
    "title": "Non interactive execution",
    "section": "Running code from the command line",
    "text": "Running code from the command line\nYou can run scripts by passing them to the julia command on the command line:\n$ julia script.jl\n\nThis code is run in a terminal, not in Julia, as is indicated by the $ prompt.\n\nYou can also evaluate single expressions in Julia from the command line by using the flag -e:\n$ julia -e 'println(2 + 3)'\n5\n\nPassing arguments\n\nTo the julia command itself\nIf you want to pass arguments to the julia command itself, you need to add them before the script or the Julia expression.\n\nExample:\n\n$ julia -O script.jl\n\n\nTo the script/Julia expression\nTo pass arguments to the script (or Julia expression if you use -e), you add them after the script or expression:\n$ julia script.jl arg1 arg2 arg3\narg1, arg2, arg3 will be passed in the global constant ARGS and interpreted as arguments to the script.\n\nExample passing arguments to an expression:\n\n$ julia -e 'for x in ARGS; println(x); end' 2 3\n2\n3\n\n\nTo both\nTo pass arguments both to the julia command and to the script/expression, you need to add the -- delimiter before the script/expression:\n$ julia [switches] -- [programfile] [args...]\n\nExample:\n\n$ julia -O -- script.jl arg1 arg2"
  },
  {
    "objectID": "julia/intro_intro.html",
    "href": "julia/intro_intro.html",
    "title": "Introduction to Julia",
    "section": "",
    "text": "Why would I want to learn a new language? I already know R/python.\n\nR and python are interpreted languages: the code is executed directly, without prior-compilation. This is extremely convenient: it is what allows you to run code in an interactive shell. The price to pay is low performance: R and python are simply not good at handling large amounts of data. To overcome this limitation, users often turn to C or C++ for the most computation-intensive parts of their analyses. These are compiled—and extremely efficient—languages, but the need to use multiple languages and the non-interactive nature of compiled languages make this approach tedious.\nJulia uses just-in-time (JIT) compilation: the code is compiled at run time. This combines the interactive advantage of interpreted languages with the efficiency of compiled ones. Basically, it feels like running R or python, while it is almost as fast as C. This makes Julia particularly well suited for big data analyses, machine learning, or heavy modelling.\nIn addition, multiple dispatch (generic functions with multiple methods depending on the types of all the arguments) is at the very core of Julia. This is extremly convenient, cutting on conditionals and repetitions, and allowing for easy extensibility without having to rewrite code.\nFinally, Julia shines by its extremely clean and concise syntax. This last feature makes it easy to learn and really enjoyable to use.\nIn this workshop, which does not require any prior experience in Julia (experience in another language—e.g. R or python—would be best), we will go over the basics of Julia’s syntax and package system; then we will push the performance aspect further by looking at how Julia can make use of clusters for large scale parallel computing."
  },
  {
    "objectID": "julia/intro_intro.html#introducing-julia",
    "href": "julia/intro_intro.html#introducing-julia",
    "title": "Introduction to Julia",
    "section": "Introducing Julia",
    "text": "Introducing Julia\n\nBrief history\nStarted in 2009 by Jeff Bezanson, Stefan Karpinski, Viral B. Shah, and Alan Edelman, the general-purpose programming language Julia was launched in 2012 as free and open source software. Version 1.0 was released in 2018.\nRust developer Graydon Hoare wrote an interesting post which places Julia in a historical context of programming languages.\n\n\nWhy another language?\n\nJIT\nComputer languages mostly fall into two categories: compiled languages and interpreted languages.\n\nCompiled languages\nCompiled languages require two steps:\n\nin a first step the code you write in a human-readable format (the source code, usually in plain text) gets compiled into machine code\nit is then this machine code that is used to process your data\n\nSo you write a script, compile it, then use it.\n\nBecause machine code is a lot easier to process by computers, compiled languages are fast. The two step process however makes prototyping new code less practical, these languages are hard to learn, and debugging compilation errors can be challenging.\n\nExamples of compiled languages include C, C++, Fortran, Go, and Haskell.\n\n\n\nInterpreted languages\nInterpreted languages are executed directly which has many advantages such as dynamic typing and direct feed-back from the code and they are easy to learn, but this comes at the cost of efficiency. The source code can facultatively be bytecompiled into non human-readable, more compact, lower level bytecode which is read by the interpreter more efficiently.\n\n\nExamples of interpreted languages include R, Python, Perl, and JavaScript.\n\n\n\nA common workflow\nSo, with this, what do researchers do?\nA common workflow, with the constraints of either type of languages, consists of:\n\nexploring the data and developing code using a sample of the data or reasonably light computations in an interpreted language,\ntranslating the code into a compiled language,\nfinally throwing the full data and all the heavy duty computation at that optimized code.\n\nThis works and it works well.\nBut, as you can imagine, this roundabout approach is tedious, not to mention the fact that it involves mastering 2 languages.\n\n\nJIT compiled languages\nJulia uses just-in-time compilation or JIT based on LLVM: the source code is compiled at run time. This combines the flexibility of interpretation with the speed of compilation, bringing speed to an interactive language. It also allows for dynamic recompilation, continuous weighing of gains and costs of the compilation of parts of the code, and other on the fly optimizations.\nOf course, there are costs here too. They come in the form of overhead time to compile code the first time it is run and increased memory usage.\n\n\n\nMultiple dispatch\nIn languages with multiple dispatch, functions apply different methods at run time based on the type of the operands. This brings great type stability and improves speed.\nJulia is extremely flexible: type declaration is not required. Out of convenience, you can forego the feature if you want. Specifying types however will greatly optimize your code.\nHere is a good post on type stability, multiple dispatch, and Julia efficiency."
  },
  {
    "objectID": "julia/intro_intro.html#how-to-run-julia",
    "href": "julia/intro_intro.html#how-to-run-julia",
    "title": "Introduction to Julia",
    "section": "How to run Julia?",
    "text": "How to run Julia?\nThere are several ways to run Julia interactively:\n\ndirectly in the REPL (read–eval–print loop: the interactive Julia shell),\nin interactive notebooks (e.g. Jupyter, Pluto),\nin an editor able to run Julia interactively (e.g. Emacs, VS Code, Vim).\n\nLet’s have a look at these interfaces.\n\nThe Julia REPL\nYou can launch the REPL from a terminal directly by typing the julia command.\n\nREPL keybindings\nIn the REPL, you can use standard command line keybindings (Emacs kbd):\nC-c     cancel\nC-d     quit\nC-l     clear console\n\nC-u     kill from the start of line\nC-k     kill until the end of line\n\nC-a     go to start of line\nC-e     go to end of line\n\nC-f     move forward one character\nC-b     move backward one character\n\nM-f     move forward one word\nM-b     move backward one word\n\nC-d     delete forward one character\nC-h     delete backward one character\n\nM-d     delete forward one word\nM-Backspace delete backward one word\n\nC-p     previous command\nC-n     next command\n\nC-r     backward search\nC-s     forward search\n\n\nREPL modes\nThe Julia REPL is unique in that it has four distinct modes:\njulia&gt;    The main mode in which you will be running your code.\nhelp?&gt;    A mode to easily access documentation.\nshell&gt;    A mode in which you can run bash commands from within Julia.\n(env) pkg&gt;   A mode to easily perform actions on packages with Julia package manager.\n(env is the name of your current project environment.\nProject environments are similar to Python’s virtual environments and allow you, for instance, to have different package versions for different projects. By default, it is the current Julia version. So what you will see is (v1.3) pkg&gt;).\nEnter the various modes by typing ?, ;, and ]. Go back to the regular mode with the Backspace key.\n\n\n\nText editors\n\nVS Code\nJulia for Visual Studio Code has become the main Julia IDE.\n\n\nEmacs\n\nthrough the julia-emacs and julia-repl packages\nthrough the ESS package\nthrough the Emacs IPython Notebook package if you want to access Jupyter notebooks in Emacs\n\n\n\nVim\nThrough the julia-vim package.\n\n\n\nInteractive notebooks\n\nJupyter\nProject Jupyter allows to create interactive programming documents through its web-based JupyterLab environment and its Jupyter Notebook.\n\n\nPluto\nThe Julia package Juno is a reactive notebook for Julia.\n\n\n\nQuarto\nQuarto builds interactive documents with code and runs Julia through Jupyter."
  },
  {
    "objectID": "julia/intro_intro.html#startup-options",
    "href": "julia/intro_intro.html#startup-options",
    "title": "Introduction to Julia",
    "section": "Startup options",
    "text": "Startup options\nYou can configure Julia by creating the file ~/.julia/config/startup.jl."
  },
  {
    "objectID": "julia/intro_intro.html#help-and-documentation",
    "href": "julia/intro_intro.html#help-and-documentation",
    "title": "Introduction to Julia",
    "section": "Help and documentation",
    "text": "Help and documentation\nAs we already saw, you can type ? to enter the help mode:\n?sum\nsearch: sum sum! summary cumsum cumsum! isnumeric VersionNumber issubnormal \nget_zero_subnormals set_zero_subnormals\n\n  sum(f, itr; [init])\n\n  Sum the results of calling function f on each element of itr.\n\n  The return type is Int for signed integers of less than system word size, \n  and UInt for unsigned integers of less than system word size. For all other \n  arguments a common return type is found to which all arguments are promoted.\n\n  The value returned for empty itr can be specified by init. It must be the \n  additive identity (i.e. zero) as it is unspecified whether init is used for \n  non-empty collections.\n\nI truncated this output as the documentation also contains many examples.\n\nTo print the list of functions containing a certain word in their description, you can use apropos().\n\nExample:\n\n\napropos(\"truncate\")\n\nBase.IOBuffer\nBase.IOContext\nBase.dump\nBase.truncate\nCore.String\nBase.open\nBase.open_flags\nBase.Broadcast.newindex\nArgTools\nNetworkOptions\nLinearAlgebra.eigen\nTar\nDates.format\nBase.trunc\nDates.Date\nIJulia.set_max_stdio\nIJulia.watch_stream\nAbstractTrees.print_tree\nAbstractTrees.TreeCharSet\nOffsetArrays\nPDMats\nStatsFuns\nDistributions\nDistributions.truncated\nDistributions.TruncatedNormal\nDistributions.Distributions\nDistributions.Truncated\nLazyModules\nSimpleRandom\nMods\nMultisets\nPolynomials\nPolynomials.truncate!\nBase.truncate\nSimplePolynomials\nPermutations\nLinearAlgebraX\nDelaunayTriangulation.grow_polygon_outside_of_box\nDelaunayTriangulation.clip_unbounded_polygon_to_bounding_box\nMakie\nMakie.to_vertices"
  },
  {
    "objectID": "julia/intro_intro.html#version-information",
    "href": "julia/intro_intro.html#version-information",
    "title": "Introduction to Julia",
    "section": "Version information",
    "text": "Version information\nJulia version only:\n\nversioninfo()\n\nJulia Version 1.9.3\nCommit bed2cd540a1 (2023-08-24 14:43 UTC)\nBuild Info:\n  Official https://julialang.org/ release\nPlatform Info:\n  OS: Linux (x86_64-linux-gnu)\n  CPU: 16 × Intel(R) Core(TM) i7-10875H CPU @ 2.30GHz\n  WORD_SIZE: 64\n  LIBM: libopenlibm\n  LLVM: libLLVM-14.0.6 (ORCJIT, skylake)\n  Threads: 2 on 16 virtual cores\n\n\nMore information, including commit, OS, CPU, and compiler:\n\nVERSION\n\nv\"1.9.3\""
  },
  {
    "objectID": "julia/intro_intro.html#lets-try-a-few-commands",
    "href": "julia/intro_intro.html#lets-try-a-few-commands",
    "title": "Introduction to Julia",
    "section": "Let’s try a few commands",
    "text": "Let’s try a few commands\nx = 10\nx\nx = 2;\nx\ny = x;\ny\nans\nans + 3\n\na, b, c = 1, 2, 3\nb\n\n3 + 2\n+(3, 2)\n\na = 3\n2a\na += 7\na\n\n2\\8\n\na = [1 2; 3 4]\nb = a\na[1, 1] = 0\nb\n\n[1, 2, 3, 4]\n[1 2; 3 4]\n[1 2 3 4]\n[1 2 3 4]'\ncollect(1:4)\ncollect(1:1:4)\n1:4\na = 1:4\ncollect(a)\n\n[1, 2, 3] .* [1, 2, 3]\n\n4//8\n8//1\n1//2 + 3//4\n\na = true\nb = false\na + b\n\n\nYour turn:\n\nWhat does ; at the end of a command do?\nWhat is surprising about 2a?\nWhat does += do?\nWhat does .*do?\n\na = [3, 1, 2]\n\nsort(a)\nprintln(a)\n\nsort!(a)\nprintln(a)\n\n\nYour turn:\n\nWhat does ! at the end of a function name do?"
  },
  {
    "objectID": "julia/intro_control_flow.html",
    "href": "julia/intro_control_flow.html",
    "title": "Control flow",
    "section": "",
    "text": "Control flow statements alter the linear execution of code, allowing for one or another section of code to be executed, or for one section of code to be executed multiple times."
  },
  {
    "objectID": "julia/intro_control_flow.html#conditional-statements",
    "href": "julia/intro_control_flow.html#conditional-statements",
    "title": "Control flow",
    "section": "Conditional statements",
    "text": "Conditional statements\nConditional statements allow to run instructions based on predicates: different sets of instructions will be executed depending on whether the predicates return true or false.\n\nPredicates\n\nHere are a few examples of predicates with classic operators:\n\noccursin(\"that\", \"this and that\")\n4 &lt; 3\na == b\na != b\n2 in 1:3\n3 &lt;= 4 && 4 &gt; 5\n3 &lt;= 4 || 4 &gt; 5\nIn addition, Julia possesses more exotic operators that can be used in predicates:\n\nThe inexact equality comparator, useful to compare floating-point numbers despite computer rounding.\n\n\nThe function isapprox or the equivalent binary operator ≈ (typed with \\approx&lt;tab&gt;) can be used:\n\n0.1 + 0.2 == 0.3\n\nfalse\n\n\n\n0.1 + 0.2 ≈ 0.3\n\ntrue\n\n\n\nisapprox(0.1 + 0.2, 0.3)\n\ntrue\n\n\nThe negatives are the function !isapprox and ≉ (typed with \\napprox&lt;tab&gt;).\n\n\nThe equivalent or triple equal operator compares objects in deeper ways (address in memory for mutable objects and content at the bit level for immutable objects).\n\n\n=== or ≡ (typed with \\equiv&lt;tab&gt;) can be used:\n\na = [1, 2]; b = [1, 2];\n\n\na == b\n\ntrue\n\n\n\na ≡ b     # This can also be written `a === b`\n\nfalse\n\n\n\na ≡ a\n\ntrue\n\n\n\n\n\nIf statements\nif &lt;predicate&gt;\n    &lt;some action&gt;\nend\n\nIf &lt;predicate&gt; evaluates to true, the body of the if statement gets evaluated (&lt;some action&gt; is run),\nIf &lt;predicate&gt; evaluates to false, nothing happens.\n\n\nExample:\n\n\nfunction testsign1(x)\n    if x &gt;= 0\n        println(\"x is positive\")\n    end\nend\n\ntestsign1 (generic function with 1 method)\n\n\n\ntestsign1(3)\n\nx is positive\n\n\n\ntestsign1(-2)\n\n\nNothing gets returned since the predicate returned false.\n\n\n\nIf else statements\nif &lt;predicate&gt;\n    &lt;some action&gt;\nelse\n    &lt;some other action&gt;\nend\n\nIf &lt;predicate&gt; evaluates to true, &lt;some action&gt; is done,\nIf &lt;predicate&gt; evaluates to false, &lt;some other action&gt; is done.\n\n\nExample:\n\n\nfunction testsign2(x)\n    if x &gt;= 0\n        println(\"x is positive\")\n    else\n        println(\"x is negative\")\n    end\nend\n\ntestsign2 (generic function with 1 method)\n\n\n\ntestsign2(3)\n\nx is positive\n\n\n\ntestsign2(-2)\n\nx is negative\n\n\nIf else statements can be written in a terse format using the ternary operator:\n&lt;predicate&gt; ? &lt;some action&gt; : &lt;some other action&gt;\n\nHere is our function testsign2 written in terse format:\n\n\nfunction testsign2(x)\n    x &gt;= 0 ? println(\"x is positive\") : println(\"x is negative\")\nend\n\ntestsign2(-2)\n\nx is negative\n\n\n\nHere is another example:\n\na = 2\nb = 2.0\n\nif a == b\n    println(\"It's true\")\nelse\n    println(\"It's false\")\nend\nAnd in terse format:\n\na == b ? println(\"It's true\") : println(\"It's false\")\n\nIt's true\n\n\n\n\nIf elseif else statements\nif &lt;predicate1&gt;\n    &lt;some action&gt;\nelseif &lt;predicate2&gt;\n    &lt;some other action&gt;\nelse\n    &lt;yet some other action&gt;\nend\n\nExample:\n\n\nfunction testsign3(x)\n    if x &gt; 0\n        println(\"x is positive\")\n    elseif x == 0\n        println(\"x is zero\")\n    else\n        println(\"x is negative\")\n    end\nend\n\ntestsign3 (generic function with 1 method)\n\n\n\ntestsign3(3)\n\nx is positive\n\n\n\ntestsign3(0)\n\nx is zero\n\n\n\ntestsign3(-2)\n\nx is negative"
  },
  {
    "objectID": "julia/intro_control_flow.html#loops",
    "href": "julia/intro_control_flow.html#loops",
    "title": "Control flow",
    "section": "Loops",
    "text": "Loops\n\nFor loops\nFor loops run a set of instructions for each element of an iterable:\nfor &lt;iterable&gt;\n    &lt;some action&gt;\nend\n\nExamples:\n\n\nfor name = [\"Paul\", \"Lucie\", \"Sophie\"]\n    println(\"Hello $name\")\nend\n\nHello Paul\nHello Lucie\nHello Sophie\n\n\n\nfor i = 1:3, j = 3:5\n    println(i + j)\nend\n\n4\n5\n6\n5\n6\n7\n6\n7\n8\n\n\n\n\nWhile loops\nWhile loops run as long as a condition remains true:\nwhile &lt;predicate&gt;\n    &lt;some action&gt;\nend\n\nExample:\n\n\ni = 0\n\nwhile i &lt;= 10\n    println(i)\n    i += 1\nend\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10"
  },
  {
    "objectID": "julia/intro_basics.html",
    "href": "julia/intro_basics.html",
    "title": "Basics of the Julia language",
    "section": "",
    "text": "Comments do not get evaluated by Julia and are for humans only.\n\n# Comments in Julia are identified by hastags\n\n\n#=\nComments can also spread over multiple lines\nif you enclose them with this syntax\n=#\n\n\nx = 2          # Comments can be added at the end of lines\n\n2"
  },
  {
    "objectID": "julia/intro_basics.html#comments",
    "href": "julia/intro_basics.html#comments",
    "title": "Basics of the Julia language",
    "section": "",
    "text": "Comments do not get evaluated by Julia and are for humans only.\n\n# Comments in Julia are identified by hastags\n\n\n#=\nComments can also spread over multiple lines\nif you enclose them with this syntax\n=#\n\n\nx = 2          # Comments can be added at the end of lines\n\n2"
  },
  {
    "objectID": "julia/intro_basics.html#basic-operations",
    "href": "julia/intro_basics.html#basic-operations",
    "title": "Basics of the Julia language",
    "section": "Basic operations",
    "text": "Basic operations\n\n# By default, Julia returns the output\n2 + 3\n\n5\n\n\n\n# Trailing semi-colons suppress the output\n3 + 7;\n\n\n# Alternative syntax that can be used with operators\n+(2, 5)\n\n7\n\n\n\n# Updating operators\na = 3\na += 8    # this is the same as a = a + 8\n\n11\n\n\n\n# Operator precedence follows standard rules\n3 + 2 ^ 3 * 10\n\n83\n\n\n\nMore exotic operators\n\n# Usual division\n6 / 2\n\n3.0\n\n\n\n# Inverse division\n2 \\ 6\n\n3.0\n\n\n\n# Integer division (division truncated to an integer)\n7 ÷ 2\n\n3\n\n\n\n# Remainder\n7 % 2        # equivalent to rem(7, 2)\n\n1\n\n\n\n# Fraction\n4//8\n\n1//2\n\n\n\n# Julia supports fraction operations\n1//2 + 3//4\n\n5//4"
  },
  {
    "objectID": "julia/intro_basics.html#variables",
    "href": "julia/intro_basics.html#variables",
    "title": "Basics of the Julia language",
    "section": "Variables",
    "text": "Variables\n\n\nfrom xkcd.com\n\nA variable is a name bound to a value:\n\na = 3;\n\nIt can be called:\n\na\n\n3\n\n\nOr used in expressions:\n\na + 2\n\n5\n\n\n\nAssignment\nYou can re-assign new values to variables:\n\na = 3;\na = -8.2;\na\n\n-8.2\n\n\nEven values of a different type:\n\na = \"a is now a string\"\n\n\"a is now a string\"\n\n\nYou can define multiple variables at once:\n\na, b, c = 1, 2, 3\nb\n\n2\n\n\n\n\nVariable names\nThese names are extremely flexible and can use Unicode character:\n\\omega       # press TAB\n\\sum         # press TAB\n\\sqrt        # press TAB\n\\in          # press TAB\n\\:phone:     # press TAB\n\nδ = 8.5;\n🐌 = 3;\nδ + 🐌\n\n11.5\n\n\nAdmittedly, using emojis doesn’t seem very useful, but using Greek letters to write equations really makes Julia a great mathematical language:\n\nσ = 3\nδ = π\nϕ = 8\n\n(5σ + 3δ) / ϕ\n\n3.0530972450961724\n\n\n\nNote how the multiplication operator can be omitted when this does not lead to confusion. Also note how the mathematical constant π is available in Julia without having to load any module.\n\nIf you want to know how to type a symbol, ask Julia: type ? and paste it in the REPL.\nThe only hard rules for variable names are:\n\nThey must begin with a letter or an underscore,\nThey cannot take the names of built-in keywords such as if, do, try, else,\nThey cannot take the names of built-in constants (e.g. π) and keywords in use in a session.\n\n\nWe thus get an error here:\n\n\nfalse = 3\n\nLoadError: syntax: invalid assignment location \"false\" around In[24]:1\n\n\nIn addition, the Julia Style Guide recommends to follow these conventions:\n\nUse lower case,\nWord separation can be indicated by underscores, but better not to use them if the names can be read easily enough without them.\n\n\n\nThe ans variable\nThe keyword ans is a variable which, in the REPL, takes the value of the last computation:\na = 3 ^ 2;\nans + 1\n10\n\n\nPrinting\nTo print the value of a variable in an interactive session, you only need to call it:\n\na = 3;\na\n\n3\n\n\nIn non interactive sessions, you have to use the println function:\n\nprintln(a)\n\n3"
  },
  {
    "objectID": "julia/intro_basics.html#quotes",
    "href": "julia/intro_basics.html#quotes",
    "title": "Basics of the Julia language",
    "section": "Quotes",
    "text": "Quotes\nNote the difference between single and double quotes:\n\ntypeof(\"a\")\n\nString\n\n\n\ntypeof('a')\n\nChar\n\n\n\n\"This is a string\"\n\n\"This is a string\"\n\n\n\n'This is not a sring'\n\nLoadError: syntax: character literal contains multiple characters\n\n\n\nWe got an error here since ' is used for the character type and can thus only contain a single character.\n\n\n'a'\n\n'a': ASCII/Unicode U+0061 (category Ll: Letter, lowercase)"
  },
  {
    "objectID": "julia/hpc_performance.html",
    "href": "julia/hpc_performance.html",
    "title": "Performance",
    "section": "",
    "text": "The one thing you need to remember: avoid global variables.\nThis means: avoid variables defined in the global environment."
  },
  {
    "objectID": "julia/hpc_performance.html#definitions",
    "href": "julia/hpc_performance.html#definitions",
    "title": "Performance",
    "section": "Definitions",
    "text": "Definitions\nScope of variables:   Environment within which a variables exist\nGlobal scope:     Global environment of a module\nLocal scope:      Environment within a function, a loop, a struct, a macro, etc."
  },
  {
    "objectID": "julia/hpc_performance.html#why-avoid-global-variables",
    "href": "julia/hpc_performance.html#why-avoid-global-variables",
    "title": "Performance",
    "section": "Why avoid global variables?",
    "text": "Why avoid global variables?\nThe Julia compiler is not good at optimizing code using global variables.\nPart of the reason is that their type can change.\n\nExample\nWe will use the @time macro to time a loop:\n\nIn the global environment:\n\n\ntotal = 0\nn = 1e6\n\n@time for i in 1:n\n    global total += i\nend\n\n  0.172757 seconds (4.00 M allocations: 76.361 MiB, 12.76% gc time, 8.71% compilation time)\n\n\n\nNote the garbage collection (gc) time: 14% of total time.\nGarbage collection time is a sign of poor code.\n\n\nIn a local environment (a function):\n\n\nfunction local_loop(total, n)\n    total = total\n    @time for i in 1:n\n        global total += i\n    end\nend\n\nlocal_loop(0, 1e6)\n\n  0.027914 seconds (2.00 M allocations: 30.518 MiB, 24.18% gc time)\n\n\n\nWe get a 7.5 speedup.\nThe memory allocation also decreased by more than half.\n\nFor more accurate performance measurements, you should use the @btime macro from the BenchmarkTools package which excludes compilation time from the timing, averages metrics over multiple runs, and is highly customizable."
  },
  {
    "objectID": "julia/hpc_intro.html",
    "href": "julia/hpc_intro.html",
    "title": "Introduction to high performance research computing in Julia",
    "section": "",
    "text": "When you launch a Jupyter session from a JupyterHub, you are running a Slurm job on a compute node. If you want to play for 8 hours in Jupyter, you are requesting an 8 hour job. Now, most of the time you spend on Jupyter is spent typing, running bits and pieces of code, or doing nothing at all. If you ask for GPUs, many CPUs, and lots of RAM, all of it will remain idle most of the time. This is a suboptimal use of resources.\nIn addition, if you ask for lots of resources for a long time, you will have to wait for a while before they get allocated to you.\nLastly, you will go through your allocations quickly.\nAll of this applies equally for interactive sessions launched from an SSH session with salloc."
  },
  {
    "objectID": "julia/hpc_intro.html#interactive-sessions-for-high-performance-computing",
    "href": "julia/hpc_intro.html#interactive-sessions-for-high-performance-computing",
    "title": "Introduction to high performance research computing in Julia",
    "section": "",
    "text": "When you launch a Jupyter session from a JupyterHub, you are running a Slurm job on a compute node. If you want to play for 8 hours in Jupyter, you are requesting an 8 hour job. Now, most of the time you spend on Jupyter is spent typing, running bits and pieces of code, or doing nothing at all. If you ask for GPUs, many CPUs, and lots of RAM, all of it will remain idle most of the time. This is a suboptimal use of resources.\nIn addition, if you ask for lots of resources for a long time, you will have to wait for a while before they get allocated to you.\nLastly, you will go through your allocations quickly.\nAll of this applies equally for interactive sessions launched from an SSH session with salloc."
  },
  {
    "objectID": "julia/hpc_intro.html#a-better-approach",
    "href": "julia/hpc_intro.html#a-better-approach",
    "title": "Introduction to high performance research computing in Julia",
    "section": "A better approach",
    "text": "A better approach\nA more efficient strategy is to develop and test your code with small samples, few iterations, etc. in an interactive job (from an SSH session in the cluster with salloc), on your own computer, or in Jupyter. Once you are confident that your code works, launch an sbatch job from an SSH session in the cluster to run the code as a script on all your data. This ensures that heavy duty resources that you requested are actually put to use to run your heavy calculations and not seating idle while you are thinking, typing, etc."
  },
  {
    "objectID": "julia/hpc_intro.html#logging-on-to-the-cluster",
    "href": "julia/hpc_intro.html#logging-on-to-the-cluster",
    "title": "Introduction to high performance research computing in Julia",
    "section": "Logging on to the cluster",
    "text": "Logging on to the cluster\nOpen a terminal emulator:\nWindows users:  launch MobaXTerm.\nMacOS users:   launch Terminal.\nLinux users:     launch xterm or the terminal emulator of your choice.\nThen access the cluster through secure shell:\n$ ssh &lt;username&gt;@&lt;hostname&gt;    # enter password"
  },
  {
    "objectID": "julia/hpc_intro.html#accessing-julia",
    "href": "julia/hpc_intro.html#accessing-julia",
    "title": "Introduction to high performance research computing in Julia",
    "section": "Accessing Julia",
    "text": "Accessing Julia\nThis is done with the Lmod tool through the module command. You can find the full documentation here and below are the subcommands you will need:\n# get help on the module command\n$ module help\n$ module --help\n$ module -h\n\n# list modules that are already loaded\n$ module list\n\n# see which modules are available for Julia\n$ module spider julia\n\n# see how to load julia 1.3\n$ module spider julia/1.3.0\n\n# load julia 1.3 with the required gcc module first\n# (the order is important)\n$ module load gcc/7.3.0 julia/1.3.0\n\n# you can see that we now have Julia loaded\n$ module list"
  },
  {
    "objectID": "julia/hpc_intro.html#copying-files-to-the-cluster",
    "href": "julia/hpc_intro.html#copying-files-to-the-cluster",
    "title": "Introduction to high performance research computing in Julia",
    "section": "Copying files to the cluster",
    "text": "Copying files to the cluster\nWe will create a julia_workshop directory in ~/scratch, then copy our julia script in it.\n$ mkdir ~/scratch/julia_job\nOpen a new terminal window and from your local terminal (make sure that you are not on the remote terminal by looking at the bash prompt) run:\n$ scp /local/path/to/sort.jl &lt;username&gt;@&lt;hostname&gt;:scratch/julia_job\n$ scp /local/path/to/psort.jl &lt;username&gt;@&lt;hostname&gt;:scratch/julia_job\n\n# enter password"
  },
  {
    "objectID": "julia/hpc_intro.html#job-scripts",
    "href": "julia/hpc_intro.html#job-scripts",
    "title": "Introduction to high performance research computing in Julia",
    "section": "Job scripts",
    "text": "Job scripts\nWe will not run an interactive session with Julia on the cluster: we already have julia scripts ready to run. All we need to do is to write job scripts to submit to Slurm, the job scheduler used by the Alliance clusters.\nWe will create 2 scripts: one to run Julia on one core and one on as many cores as are available.\n\n\nYour turn:\n\nHow many processors are there on our training cluster?\n\nWe can run Julia with multiple threads by running:\n$ JULIA_NUM_THREADS=2 julia\nor:\n$ julia -t 2\nOnce in Julia, you can double check that Julia does indeed have access to 2 threads by running:\nThreads.nthreads()\nSave your job scripts in the files ~/scratch/julia_job/job_julia1c.sh and job_julia2c.sh for one and two cores respectively.\nHere is what our single core Slurm script looks like:\n#!/bin/bash\n#SBATCH --job-name=julia1c          # job name\n#SBATCH --time=00:01:00             # max walltime 1 min\n#SBATCH --cpus-per-task=1           # number of cores\n#SBATCH --mem=1000                  # max memory (default unit is megabytes)\n#SBATCH --output=julia1c%j.out      # file name for the output\n#SBATCH --error=julia1c%j.err       # file name for errors\n# %j gets replaced with the job number\n\necho Running NON parallel script\njulia sort.jl\necho Running parallel script on $SLURM_CPUS_PER_TASK core\njulia -t $SLURM_CPUS_PER_TASK psort.jl\n\n\nYour turn:\n\nWrite the script for 2 cores.\n\nNow, we can submit our jobs to the cluster:\n$ cd ~/scratch/julia_job\n$ sbatch job_julia1c.sh\n$ sbatch job_julia2c.sh\nAnd we can check their status with:\n$ sq      # This is an Alliance alias for `squeue -u $USER $@`\n\nPD stands for pending\nR stands for running"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Training in Research Computing",
    "section": "",
    "text": "Git\nVersion control & collaboration with Git\n\n\n\n\nR\nResearch computing in R\n\n\n\n\nJulia\nResearch computing in Julia\n\n\n\n\nPython\nResearch computing in Python\n\n\n\n\n\n\nMachine learning\nDeep learning with PyTorch & JAX Machine learning with scikit-learn\n\n\n\n\nBash\nBash scripting & modern command line utilities\n\n\n\n\nResearch tools\nOpen source tools for research computing & publishing\n\n\n\n \n\n\n\n\n\nMain website\nThis site contains content by Marie-Hélène Burle. To view all training material, please visit our main website."
  },
  {
    "objectID": "git/top_intro.html",
    "href": "git/top_intro.html",
    "title": "Getting started with Git",
    "section": "",
    "text": "This introductory course to Git will allow you to put your projects under version control, create commits with important versions of your files, revisit these versions, and add remotes on services such as GitHub or GitLab.\nYou will get a deep understanding of the functioning of Git and learn to use it from the command line.\n\n Start course ➤"
  },
  {
    "objectID": "git/time_travel.html",
    "href": "git/time_travel.html",
    "title": "Revisiting old commits",
    "section": "",
    "text": "It’s great to record history, but if we don’t know how to make use of it, it isn’t exactly useful.\nIn this workshop, you will travel through the history of a project."
  },
  {
    "objectID": "git/time_travel.html#looking-at-the-past-without-travelling",
    "href": "git/time_travel.html#looking-at-the-past-without-travelling",
    "title": "Revisiting old commits",
    "section": "Looking at the past without travelling",
    "text": "Looking at the past without travelling\nHEAD is a little file in the .git directory which points to our current location in the Git history.\nYou already saw multiple ways to have a glimpse at your project history without moving HEAD:\n\ngit log and its many variations shows a list or a tree of your commits\ngit show displays information about a Git object such as a past commit\n\nThose are useful options, but Git allows you to really travel in your project history: HEAD can be moved around with the command git checkout to point to any branch, tag, or commit."
  },
  {
    "objectID": "git/time_travel.html#travelling-through-history",
    "href": "git/time_travel.html#travelling-through-history",
    "title": "Revisiting old commits",
    "section": "Travelling through history",
    "text": "Travelling through history\nAs soon as you move HEAD to a new Git object with git checkout, the working directory and the index get updated to match the snapshot that Git object is pointing to. That means that your project will suddenly be back to the state in which it was when you committed that snapshot.\n\nMoving HEAD\nLet’s give this a try and move HEAD to a past commit.\n\nIdentifying the commit we want to move HEAD to\nYou can use the ~ notation:\n\nHEAD~ or HEAD~1 means the commit which precedes the one HEAD is pointing to.\nHEAD~2 means the commit before that.\nHEAD~3 refers to the 3rd commit before the current commit.\netc.\n\nYou can also run git log to find the hash of your commit of interest.\n\n\n\nDetached HEAD\nLet’s look at a hypothetical scenario to see what happens when you checkout a commit.\nThis is our starting point:\n\nNow, we move HEAD to the commit 31fukv1:\ngit checkout 31fukv1\n\nNotice that HEAD is not pointing at a branch anymore: it is pointing directly at a commit. This is called a detached HEAD state and Git will give you plenty of warnings about it.\nIf you look at your files, you will see that they match their state when you committed 31fukv1: your working directory got updated to match the current position of HEAD.\nYou can look at your project at that point in its history, then go back to your main branch (here main) with:\ngit checkout main\n\nAnd that’s that. You took a little trip into the past just to have a look, then came back to “the present” and all went well.\n\n\nCreating commits from a detached HEAD\nNow, when you are at commit 31fukv1, maybe you wanted to try something.\n\nYou can safely try anything you want: when you checkout main to come back to “the present”, those experimental changes will get lost.\nBut what if you want to keep those changes you made at 31fukv1?\nIn that case, as you always do, you create a commit to archive those changes into the project history:\n\nYou can make more commits:\n\nThe thing is that you are still in this detached HEAD state. HEAD is not pointing to a branch as it normally is. Is this a problem?\n\nBad workflow\nWell, it becomes a problem if you checkout main from there:\n\nIf you decide that you don’t care about those commits after all, then all is good. But if you care about them, this is a bad situation because those commits you created when you were in a detached HEAD state are now left behind: they are not in the history of any branch or tag.\nThis is bad for three reasons:\n\nThose commits will not show when you run git log, so it is easy to forget about them.\nIt is not easy to go back to them because there aren’t any tag or branch that you can checkout.\nThe garbage collection (which runs every 30 days by default) will delete those commits which are not on any branch or tag. So you will ultimately loose them.\n\n\n\nGood workflow\nA good workflow would have been to create a new branch on 31fukv1 (let’s call it alternative) and switch to it. That way, the commits created from 31fukv1 are on a branch and they will not be deleted by the next garbage collection:\n\nIn this good workflow, it is totally safe to switch back to main:\n\n\nIf you want to list the commits 23f481q and rthy7wg when you are back on main, you need to run git log with the --all flag.\n\n\n\nRecovering commits left behind\nWhat if you left commits behind (not on a branch)?\nYou can retrieve their hash by running:\ngit reflog\nThis tracks the position of HEAD over time.\nYou can then checkout the commit you care about (so you are going back to a detached HEAD state):\ngit checkout &lt;hash-abandonned-commit&gt;\nThis puts you back into a situation where you can rescue the commit(s) by creating a branch:\nDo this as soon as you can since those commits will be deleted at the next garbage collection (and finding their hash with git reflog will become increasingly complicated as you wait)."
  },
  {
    "objectID": "git/tags.html",
    "href": "git/tags.html",
    "title": "Tags",
    "section": "",
    "text": "When you reach an important point in the development of a project, it is convenient to be able to identify the next commit easily. Rather than having to look for it through date, commit message, or hash, you can create a tag: a pointer to that commit."
  },
  {
    "objectID": "git/tags.html#leightweight-tag",
    "href": "git/tags.html#leightweight-tag",
    "title": "Tags",
    "section": "Leightweight tag",
    "text": "Leightweight tag\nYou create a tag with:\ngit tag &lt;tag-name&gt;\n\nExample:\n\ngit tag J_Climate_2009\n\nAs you keep developing the project and create new commits, the branch and HEAD pointers will move along, but the tag remains on your important commit.\n\nAt any time, you can get info on the commit thus tagged with:\ngit show J_Climate_2009\nOr you can check it out with:\ngit checkout J_Climate_2009"
  },
  {
    "objectID": "git/tags.html#annotated-tag",
    "href": "git/tags.html#annotated-tag",
    "title": "Tags",
    "section": "Annotated tag",
    "text": "Annotated tag\nA more sophisticated form of tag comes with a message:\ngit tag -a &lt;tag-name&gt; -m \"&lt;message&gt;\"\ngit tag -a J_Climate_2009 -m \"State of project at the publication of paper\""
  },
  {
    "objectID": "git/tags.html#list-tags",
    "href": "git/tags.html#list-tags",
    "title": "Tags",
    "section": "List tags",
    "text": "List tags\ngit tag"
  },
  {
    "objectID": "git/tags.html#deleting-tags",
    "href": "git/tags.html#deleting-tags",
    "title": "Tags",
    "section": "Deleting tags",
    "text": "Deleting tags\ngit tag -d &lt;tag-name&gt;\ngit tag -d J_Climate_2009"
  },
  {
    "objectID": "git/resources.html",
    "href": "git/resources.html",
    "title": "Resources",
    "section": "",
    "text": "Official Git manual\nOpen source Pro Git book"
  },
  {
    "objectID": "git/resources.html#online-documentation",
    "href": "git/resources.html#online-documentation",
    "title": "Resources",
    "section": "",
    "text": "Official Git manual\nOpen source Pro Git book"
  },
  {
    "objectID": "git/resources.html#courses-workshops",
    "href": "git/resources.html#courses-workshops",
    "title": "Resources",
    "section": "Courses & workshops",
    "text": "Courses & workshops\n\nWestern Canada Research Computing Git workshops\nWestGrid Summer School 2020 Git course\nWestGrid Autumn School 2020 Git course\nSoftware Carpentry Git lesson"
  },
  {
    "objectID": "git/resources.html#q-a",
    "href": "git/resources.html#q-a",
    "title": "Resources",
    "section": "Q & A",
    "text": "Q & A\n\nStack Overflow [git] tag"
  },
  {
    "objectID": "git/resources.html#troubleshooting",
    "href": "git/resources.html#troubleshooting",
    "title": "Resources",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n\n“Listen” to Git!\nGit is extremely verbose: by default, it will return lots of information. Read it!\nThese messages may feel overwhelming at first, but:\n\nthey will make more and more sense as you gain expertise,\nthey often give you clues as to what the problem is,\neven if you don’t understand them, you can use them as Google search terms.\n\n\n\n(Re-read) the doc\nAs I have no memory, I need to check the man pages all the time. That’s ok! It is quick and easy.\nFor more detailed information and examples, I really like the Official Git manual.\n\n\nSearch online\n\nGoogle\nStack Overflow [git] tag\n\n\n\nDon’t panic\nBe analytical. It is easy to panic and feel lost if something doesn’t work as expected. Take a breath and start with the basis:\n\nmake sure you are in the repo (pwd) and the files are where you think they are (ls -a),\ninspect the repository (git status, git diff, git log). Make sure not to overlook what Git is “telling” you there."
  },
  {
    "objectID": "git/remotes.html",
    "href": "git/remotes.html",
    "title": "Remotes",
    "section": "",
    "text": "Remotes are copies of a project and its history.\nThey can be located anywhere, including on external drive or on the same machine as the project, although they are often on a different machine to serve as backup, or on a network (e.g. internet) to serve as a syncing hub for collaborations.\nPopular online Git repository managers & hosting services:\n\nGitHub\nGitLab\nBitbucket\n\nLet’s see how to create and manage remotes."
  },
  {
    "objectID": "git/remotes.html#creating-a-remote-on-github",
    "href": "git/remotes.html#creating-a-remote-on-github",
    "title": "Remotes",
    "section": "Creating a remote on GitHub",
    "text": "Creating a remote on GitHub\n\nCreate a free GitHub account\nIf you don’t already have one, sign up for a free GitHub account.\n\nTo avoid having to type your password all the time, you should set up SSH for your account.\n\n\n\nCreate an empty repository on GitHub\n\nGo to the GitHub website, login, and go to your home page.\nLook for the Repositories tab & click the green New button.\nEnter the name you want for your repo, without spaces.\nMake the repository public or private.\n\n\n\nLink empty repository to your repo\nClick on the Code green drop-down button, select SSH if you have set SSH for your GitHub account or HTTPS and copy the address.\nIn the command line, cd inside your project, and add the remote:\ngit remote add &lt;remote-name&gt; &lt;remote-address&gt;\nremote-name is a convenience name to identify that remote. You can choose any name, but since Git automatically call the remote origin when you clone a repo, it is common practice to use origin as the name for the first remote.\n\nExample (using an SSH address):\n\ngit remote add origin git@github.com:&lt;user&gt;/&lt;repo&gt;.git\n\nExample (using an HTTPS address):\n\ngit remote add origin https://github.com/&lt;user&gt;/&lt;repo&gt;.git"
  },
  {
    "objectID": "git/remotes.html#getting-information-on-remotes",
    "href": "git/remotes.html#getting-information-on-remotes",
    "title": "Remotes",
    "section": "Getting information on remotes",
    "text": "Getting information on remotes\nList remotes:\ngit remote\nList remotes with their addresses:\ngit remote -v\nGet more information on a remote:\ngit remote show &lt;remote-name&gt;\n\nExample:\n\ngit remote show origin"
  },
  {
    "objectID": "git/remotes.html#managing-remotes",
    "href": "git/remotes.html#managing-remotes",
    "title": "Remotes",
    "section": "Managing remotes",
    "text": "Managing remotes\nRename a remote:\ngit remote rename &lt;old-remote-name&gt; &lt;new-remote-name&gt;\nDelete a remote:\ngit remote remove &lt;remote-name&gt;\nChange the address of a remote:\ngit remote set-url &lt;remote-name&gt; &lt;new-url&gt; [&lt;old-url&gt;]"
  },
  {
    "objectID": "git/remotes.html#getting-data-from-a-remote",
    "href": "git/remotes.html#getting-data-from-a-remote",
    "title": "Remotes",
    "section": "Getting data from a remote",
    "text": "Getting data from a remote\nIf you collaborate on a project, you have to get the data added by your teammates to keep your local project up to date.\nTo download new data from a remote, you have 2 options:\n\ngit fetch\ngit pull\n\n\nFetching changes\nFetching downloads the data from a remote that you don’t already have in your local version of the project:\ngit fetch &lt;remote-name&gt;\nThe branches on the remote are now accessible locally as &lt;remote-name&gt;/&lt;branch&gt;. You can inspect them or you can merge them into your local branches.\n\nExample:\n\ngit fetch origin\n\n\nPulling changes\nPulling fetches the changes & merges them onto your local branches:\ngit pull &lt;remote-name&gt; &lt;branch&gt;\n\nExample:\n\ngit pull origin main\nIf your branch is already tracking a remote branch, you can omit the arguments:\ngit pull"
  },
  {
    "objectID": "git/remotes.html#pushing-to-a-remote",
    "href": "git/remotes.html#pushing-to-a-remote",
    "title": "Remotes",
    "section": "Pushing to a remote",
    "text": "Pushing to a remote\nUploading data to the remote is called pushing:\ngit push &lt;remote-name&gt; &lt;branch-name&gt;\n\nExample:\n\ngit push origin main\nYou can set an upstream branch to track a local branch with the -u flag:\ngit push -u &lt;remote-name&gt; &lt;branch-name&gt;\n\nExample:\n\ngit push -u origin main\nFrom now on, all you have to run when you are on main is:\ngit push\n \n\nby jscript"
  },
  {
    "objectID": "git/logs.html",
    "href": "git/logs.html",
    "title": "Getting information on commits",
    "section": "",
    "text": "Before we can make use of old commits, we need to be able to get information about them. This is critical to know how to navigate the history of a project."
  },
  {
    "objectID": "git/logs.html#displaying-the-commit-history",
    "href": "git/logs.html#displaying-the-commit-history",
    "title": "Getting information on commits",
    "section": "Displaying the commit history",
    "text": "Displaying the commit history\nDo you remember this diagram from the first section of this course?\n\nThis is very useful to get a map of the history of our project. But how can we get a visual for it? The command here is git log and its many options.\nIn its basic form, it outputs the logs of our past commits:\ngit log\ncommit ca3c0360bd8ab961117671dd1f8fb2d2c3d5d7a1 (HEAD -&gt; main)\nAuthor: Marie-Helene Burle &lt;xxx@xxx&gt;\nDate:   Tue Jul 11 23:08:59 2023 -0700\n\n    Add first draft of script\n\ncommit c4ab5e755179a7b28a09c0ca587551bdac504d35\nAuthor: Marie-Helene Burle &lt;xxx@xxx&gt;\nDate:   Tue Jul 11 23:06:40 2023 -0700\n\n    Add .gitignore file with data and results\n\ncommit 61abf96298b54baf6d48cdea2ab1477db1075b5e\nAuthor: Marie-Helene Burle &lt;xxx@xxx&gt;\nDate:   Mon Jul 10 23:23:25 2023 -0700\n\n    Initial commit\nAs you can see, commits are listed from the bottom up.\nCommits can be displayed as one-liners:\ngit log --oneline\nca3c036 (HEAD -&gt; main) Add first draft of script\nc4ab5e7 Add .gitignore file with data and results\n61abf96 Initial commit\nOr as a graph:\ngit log --graph\n* commit ca3c0360bd8ab961117671dd1f8fb2d2c3d5d7a1 (HEAD -&gt; main)\n| Author: Marie-Helene Burle &lt;xxx@xxx&gt;\n| Date:   Tue Jul 11 23:08:59 2023 -0700\n|\n|     Add first draft of script\n|\n* commit c4ab5e755179a7b28a09c0ca587551bdac504d35\n| Author: Marie-Helene Burle &lt;xxx@xxx&gt;\n| Date:   Tue Jul 11 23:06:40 2023 -0700\n|\n|     Add .gitignore file with data and results\n|\n* commit 61abf96298b54baf6d48cdea2ab1477db1075b5e\n  Author: Marie-Helene Burle &lt;xxx@xxx&gt;\n  Date:   Mon Jul 10 23:23:25 2023 -0700\n\n      Initial commit\nOr in any fancy way you like:\ngit log \\\n    --graph \\\n    --date=short \\\n    --pretty=format:'%C(cyan)%h %C(blue)%ar %C(auto)%d'`\n                   `'%C(yellow)%s%+b %C(magenta)%ae'\n* ca3c036 31 minutes ago  (HEAD -&gt; main)Add first draft of script xxx@xxx\n* c4ab5e7 34 minutes ago Add .gitignore file with data and results xxx@xxx\n* 61abf96 24 hours ago Initial commit xxx@xxx\nRun man git-log for a full list of options."
  },
  {
    "objectID": "git/logs.html#information-about-a-commit",
    "href": "git/logs.html#information-about-a-commit",
    "title": "Getting information on commits",
    "section": "Information about a commit",
    "text": "Information about a commit\ngit log is useful to get an overview of our project history, but the information we get about each commit is limited. To get additional information about a particular commit, you can use git show followed by the hash of the commit you are interested in.\nFor instance, let’s explore our last commit\ngit show\ncommit ca3c0360bd8ab961117671dd1f8fb2d2c3d5d7a1 (HEAD -&gt; main)\nAuthor: Marie-Helene Burle &lt;xxx@xxx&gt;\nDate:   Tue Jul 11 23:08:59 2023 -0700\n\n    Add first draft of script\n\ndiff --git a/src/script.py b/src/script.py\nnew file mode 100644\nindex 0000000..263ef67\n--- /dev/null\n+++ b/src/script.py\n@@ -0,0 +1,7 @@\n+import pandas as pd\n+from matplotlib import pyplot as plt\n+\n+df = pd.read_csv('../data/dataset.csv')\n+\n+df.plot()\n+plt.savefig('../results/plot.png', dpi=300)\nOr our second commit:\ngit show c4ab5e7  # Replace the hash by the hash of your second commit\ncommit c4ab5e755179a7b28a09c0ca587551bdac504d35\nAuthor: Marie-Helene Burle &lt;xxx@xxx&gt;\nDate:   Tue Jul 11 23:06:40 2023 -0700\n\n    Add .gitignore file with data and results\n\ndiff --git a/.gitignore b/.gitignore\nnew file mode 100644\nindex 0000000..e85f44a\n--- /dev/null\n+++ b/.gitignore\n@@ -0,0 +1,2 @@\n+/data/\n+/results/\nIn addition to displaying the commit metadata, git show also displays the diff of that commit with its parent commit."
  },
  {
    "objectID": "git/intro.html",
    "href": "git/intro.html",
    "title": "What is Git?",
    "section": "",
    "text": "Git is a free and open source version control system—a software that tracks changes to your files, allowing you to revisit or revert to older versions.\n\nSlides (Click and wait: the presentation might take a few instants to load)"
  },
  {
    "objectID": "git/index.html",
    "href": "git/index.html",
    "title": "Git",
    "section": "",
    "text": "Getting started with Git\nAn introductory course to version control\n\n\n\n \n\n\n\n\n \n\n\n \n\n\n\n\n\nWorkshops\nWorkshops on various Git topics"
  },
  {
    "objectID": "git/first_steps.html",
    "href": "git/first_steps.html",
    "title": "First steps",
    "section": "",
    "text": "In this section, we will initialize our first Git repository, learn to explore it, and create a few commits."
  },
  {
    "objectID": "git/first_steps.html#get-a-mock-project",
    "href": "git/first_steps.html#get-a-mock-project",
    "title": "First steps",
    "section": "Get a mock project",
    "text": "Get a mock project\nLet’s download a mock project with a couple of files to practice with.\n\n1. Download the zip file\nNavigate to a suitable location (e.g. cd ~), then download the file with:\nwget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1SJV5mRGexf91lNyFwdS_JmuAXX0xS4pE' -O project.zip\n\nAlternatively, you can download the file with this button:  Download the data \nNote that this will download the zip file to your local machine. If you are working remotely through SSH, this is not a convenient method. You will also have to know how to navigate to your Downloads directory to find the file.\n\n\n2. Unzip the file\nUnzip the file with:\nunzip project.zip\nYou should now have a project directory with a number of subdirectories and files. This is the project we will use today.\n\n\n3. Enter in the project root\ncd in the root of the project:\ncd project"
  },
  {
    "objectID": "git/first_steps.html#inspect-the-project",
    "href": "git/first_steps.html#inspect-the-project",
    "title": "First steps",
    "section": "Inspect the project",
    "text": "Inspect the project\nFirst, let’s have a look at our (very small) mock project.\nLet’s list the content of project:\nls -F\ndata/\nms/\nresults/\nsrc/\nThere are 4 subdirectories.\nls -a     # Show hidden files\n.\n..\ndata\nms\nresults\nsrc\nAs you can see, there are no hidden files.\nls -R\n.:\ndata\nms\nresults\nsrc\n\n./data:\ndataset.csv\n\n./ms:\nproposal.md\n\n./results:\n\n./src:\nscript.py\nNow we can see the content of each subdirectory.\nYou probably don’t have the tree command, so don’t worry if this doesn’t work on your machine: this is only to show you the same result in a more readable format:\ntree\n.\n├── data\n│   └── dataset.csv\n├── ms\n│   └── proposal.md\n├── results\n└── src\n    └── script.py\n\n5 directories, 3 files\nLet’s look at the content of the files:\ncat data/dataset.csv\nvar1,var2,var3,\n1,2,1,\n0,1,0,\n3,3,3\nThis is our very exciting data set.\ncat ms/proposal.md\n# Summary\n\nThis is the summary for our proposal.\n\n# Funding\n\nInformation on funding for the project.\n\n# Methods\n\nHere we have some methods using our Python scripts.\n\n# Expected results\n\nWe hope to achieve a lot.\n\n# Conclusion\n\nThis is truly a great proposal.\nAnd a no less exciting manuscript (the proposal for our project).\ncat src/script.py\nimport pandas as pd\nfrom matplotlib import pyplot as plt\n\ndf = pd.read_csv('../data/dataset.csv')\n\ndf.plot()\nplt.savefig('../results/plot.png', dpi=300)\nAnd finally, a Python script."
  },
  {
    "objectID": "git/first_steps.html#initializing-a-git-repository",
    "href": "git/first_steps.html#initializing-a-git-repository",
    "title": "First steps",
    "section": "Initializing a Git repository",
    "text": "Initializing a Git repository\nOur project is just a bunch of files and subdirectories. To turn it into a Git repository, we run:\ngit init\nInitialized empty Git repository in project/.git/\n\nMake sure to be at the root of the project (here, inside the project directory) before initializing the repository.\n\n\nGit is verbose: you will often get useful feed-back after running commands. Read them!\n\nWhen you run this command, Git creates a .git repository. This is where it will store all its files (all those blob objects, pointers, and other files).\nYou can see that this repository was created by running:\nls -a\n.\n..\n.git\ndata\nms\nresults\nsrc\n\nIf you run git init in the wrong location, you can easily fix this by deleting the .git directory that you created (e.g. rm -r .git).\n\n\nGit commands\nAll commands start with git.\nA typical command is of the form:\ngit &lt;command&gt; [flags] [arguments]\n\nExample of a command we used to configure Git:\n\ngit config --global \"Your Name\"\n\nExample of a much simpler command with no flag nor argument:\n\ngit init"
  },
  {
    "objectID": "git/first_steps.html#status-of-the-repository",
    "href": "git/first_steps.html#status-of-the-repository",
    "title": "First steps",
    "section": "Status of the repository",
    "text": "Status of the repository\nOne command you will run often when working with Git is git status. It gives you information on new changes to your project:\ngit status\nOn branch main\n\nNo commits yet\n\nUntracked files:\n  (use \"git add &lt;file&gt;...\" to include in what will be committed)\n        data/\n        ms/\n        src/\n\nnothing added to commit but untracked files present (use \"git add\" to track)\nThere is a lot of information here:\n\nWe are on branch main. That is the default name for the branch that gets created automatically as soon as we initialize a Git repository.\nThere are no commits yet (normal: we just initialized a new repository).\nThere are untracked files in our repo.\n\nIt is time to create a first commit…"
  },
  {
    "objectID": "git/first_steps.html#creating-commits",
    "href": "git/first_steps.html#creating-commits",
    "title": "First steps",
    "section": "Creating commits",
    "text": "Creating commits\nRemember that commits are those “snapshots” of your project at certain moments in time. You should create a new commit whenever you think that your project is at a point to which you might want to go back to later. There is no rule about when or how often to create commits: it is really up to you.\nBefore we create our first commit, we need to decide what file(s) we want to add to this commit.\nWe could add everything.\nWe can also be more selective and add files one by one.\nWe can even add only sections of files.\nHow do we tell Git what to add to the next commit?\n\nThe staging area\nGit has a staging area: a way to select what to add to the next commit. Files (or sections of files) get added to the staging area with git add.\nTo add all the files at once, we run, from the root of the project:\ngit add .\nThe . represents the current directory. Because Git adds files recursively, this will add all new files.\nTo add a particular file, we add its path as an argument to git add.\n\nExample:\n\ngit add ms/proposal.md\nTo add all the files in a directory, we add the path of that directory as an argument to git add.\n\nExample:\n\ngit add ms\n\nSince there is a single file in the ms directory, both commands will in our case lead to the same result.\n\nLet’s run that last command:\ngit add ms\nIt looks like nothing happened. Did it work? How can I know?\nAnswer: by running git status again. That’s the command to go to whenever you need to get some update on the status of the repo:\ngit status\nOn branch main\n\nNo commits yet\n\nChanges to be committed:\n  (use \"git rm --cached &lt;file&gt;...\" to unstage)\n        new file:   ms/proposal.md\n\nUntracked files:\n  (use \"git add &lt;file&gt;...\" to include in what will be committed)\n        data/\n        src/\nIt worked! We can see that the content of the ms subdirectory is ready to be committed. It is in the staging area.\n\n\nYour turn:\n\nWhat if we change our mind as to the composition of our first commit and we want to create our first commit with the Python script instead? To do that, we need to unstage proposal.md first.\nHow can we do that? Give it a try.\nThen, how can we stage the Python script?\n\n\n\nCommitting\nOnce we are happy with the content of the future commit, it is time to create it. The command for this is git commit.\nRemember that each commit contains the following metadata:\n\nauthor,\ndate and time,\nthe hash of parent commit(s),\na message.\n\nThe first three can be set by Git automatically (you have configured Git so it knows who the author is).\nGit has no way to come up with the forth one however. You need to create it yourself.\nIf you run git commit, Git will open your text editor so that you can type the message. If you want to enter the message directly from the command line, you use the -m flag followed by the quoted message:\ngit commit -m \"Initial commit\"\n[main (root-commit) 61abf96] Initial commit\n 1 file changed, 19 insertions(+)\n create mode 100644 ms/proposal.md\nWe now have a first commit. Its hash starts (in my case—yours will be different of course) with 61abf96.\n\nAdvice for great commit messages\n\n\nfrom xkcd.com\n\n\nUse the present tense.\nThe first line is a summary of the commit and is less than 50 characters long.\nLeave a blank line below.\nThen add the body of your commit message with more details.\n\n\nExample of a good commit message:\n\ngit commit -m \"Reduce boundary conditions by a factor of 0.3\n\nUpdate boundaries\nRerun model and update table\nRephrase method section in ms\"\nFuture you will thank you! (And so will your collaborators).\n\nIf we run git status once more, we now get:\nOn branch main\nUntracked files:\n  (use \"git add &lt;file&gt;...\" to include in what will be committed)\n        data/\n        src/\n\nnothing added to commit but untracked files present (use \"git add\" to track)\nThe directory ms has disappeared from the list of untracked files: its content has now been committed to history.\n\n\nUnderstanding the staging area\nNew Git users are often confused about the two-step commit process (first, you stage with git add, then you commit with git commit). This intermediate step seems, at first, totally unnecessary. In fact, it is very useful: without it, commits would always include all new changes made to a project and they would thus be very messy. The staging area allows to prepare (“stage”) the next commit. This way, you only commit what you want when you want.\n\nLet’s go over a simple example:\n\nWe don’t always work linearly. Maybe you are working on a section of your manuscript when you realize by chance that there is a mistake in your script. You fix that mistake. On your next commit, it might make little sense to commit together that fix and your manuscript changes since they are not related. If your commits are random bag of changes, it will be very hard for future you to navigate your project history.\nIt is a lot better to only stage your script fix, commit it, then only stage your manuscript update, and commit this in a different commit.\nThe staging area allows you to pick and chose the changes from one or various files that constitute some coherent change to the project and that make sense to commit together."
  },
  {
    "objectID": "git/contrib.html",
    "href": "git/contrib.html",
    "title": "Collaborating to projects on GitHub",
    "section": "",
    "text": "There are countless free and open source tools on GitHub and if you use one such tool and find a problem or think that you can improve the project, or if you would like to request a novel feature, how do you go about it?\nIn this workshop, we will learn how to contribute to open source projects hosted on GitHub by opening issues and submitting pull requests."
  },
  {
    "objectID": "git/contrib.html#opening-issues",
    "href": "git/contrib.html#opening-issues",
    "title": "Collaborating to projects on GitHub",
    "section": "Opening issues",
    "text": "Opening issues\nThe easiest thing to do, if for instance, you are having problems with the tool, found a bug, or want to submit a feature request, is to open an issue.\nGitHub has also now implemented the ability to open “Discussions”. If enabled by the maintainer of a project, this is the place where you want to ask for help."
  },
  {
    "objectID": "git/contrib.html#forking-a-project",
    "href": "git/contrib.html#forking-a-project",
    "title": "Collaborating to projects on GitHub",
    "section": "Forking a project",
    "text": "Forking a project\nNow, a more advanced approach is to actually make changes to the code of the project.\nIf you want to develop your own version of the project, you can fork the GitHub repository: go to GitHub and fork the project by clicking on the “Fork” button in the top right corner.\nYou have all privileges on the forked project. So you can make any change you want there. You can clone it to your machine and develop the fork. But your fork does not get updated to the improvements made to the initial project. It is an independent project of its own.\n\nKeeping a fork up to date\nIf you want to keep your fork up to date with the initial project, you need to:\n\n1. Clone your fork on your machine\nThis will automatically set your fork on GitHub as the remote called origin:\n# If you have set SSH for your GitHub account\ngit clone git@github.com:&lt;user&gt;/&lt;repo&gt;.git &lt;name&gt;\n\n# If you haven't set SSH\ngit clone https://github.com/&lt;user&gt;/&lt;repo&gt;.git &lt;name&gt;\n\n\n2. Add the initial project as upstream\nAdd a second remote, this one pointing to the initial project. It is usual to call this remote upstream:\n# If you have set SSH for your GitHub account\ngit remote add upstream git@github.com:&lt;user&gt;/&lt;repo&gt;.git\n\n# If you haven't set SSH\ngit remote add upstream https://github.com/&lt;user&gt;/&lt;repo&gt;.git\n\n\n3. Pull from upstream\nYou can now pull from upstream to keep your fork up to date.\nFrom there on, you can pull from and push to origin (your fork) and you can pull from upstream (the initial repo).\nOf course, if your project and the initial one diverge in places, this will lead to conflicts that you will have to resolve as you merge the pulls from upstream.\nMost of the time however, you don’t want to develop your own version of the project. Instead, you want to make the initial project better by contributing to it. But you can’t push changes to upstream directly since you are not part of that project. You don’t have write access to that repository. If anybody could push to any project, that would be utter chaos.\nSo how do you contribute code to someone else’s project?"
  },
  {
    "objectID": "git/contrib.html#creating-pull-requests",
    "href": "git/contrib.html#creating-pull-requests",
    "title": "Collaborating to projects on GitHub",
    "section": "Creating pull requests",
    "text": "Creating pull requests\nHere is the workflow as described in the Git manual:\n\nPull from upstream to make sure that your contributions are made on an up-to-date version of the project\nCreate and checkout a new branch\nMake and commit your changes on that branch\nPush that branch to your fork (i.e. origin—remember that you do not have write access on upstream)\nGo to the original project GitHub’s page and open a pull request from your fork. Note that after you have pushed your branch to origin, GitHub will automatically offer you to do so.\n\nThe maintainer of the initial project may accept or decline the PR. They may also make comments and ask you to make changes. If so, make new changes and push additional commits to that branch until they are happy with the change.\nOnce the PR is merged by the maintainer, you can delete the branch on your fork and pull from upstream to update your local fork with the recently accepted changes."
  },
  {
    "objectID": "git/changes.html",
    "href": "git/changes.html",
    "title": "Inspecting changes",
    "section": "",
    "text": "While git status gives you information on the files that were changed since the last commit, it doesn’t provide any information on what those changes are.\nIn this section, we will see how we can get information on the changes to the files contents."
  },
  {
    "objectID": "git/changes.html#the-three-trees-of-git",
    "href": "git/changes.html#the-three-trees-of-git",
    "title": "Inspecting changes",
    "section": "The three trees of Git",
    "text": "The three trees of Git\nBefore we can jump into this section, we need to understand a bit more how Git works.\nOne useful mental representation is to imagine three file trees:\n\n\nThe working tree\nLet’s imagine that you are starting to work on a project.\nFirst, you create a directory.\nIn it, you create several sub-directories.\nIn those, you create a number of files.\nYou can open these files, read them, edit them, etc. This is something you are very familiar with.\nIn the Git world, this is the working directory or working tree of the project.\nThat is: an uncompressed version of your files that you can access and edit.\nYou can think of it as a sandbox because this is where you can experiment with the project. This is where the project gets developed.\nNow, Git has two other important pieces in its architecture.\n\n\nThe index\nIf you want the project history to be useful to future you, it has to be nice and tidy. You don’t want to record snapshots haphazardly or you will never be able to find anything back.\nBefore you record a snapshot, you carefully select the elements of the project as it is now that would be useful to write to the project history together. The index or staging area is what allows to do that: it contains the suggested future commit.\n\n\nHEAD\nFinally, the last tree in Git architecture is one snapshot in the project history that serves as a reference version of the project: if you want to see what you have been experimenting on in your “sandbox”, you need to compare the state of the working directory with some snapshot.\nRemember that HEAD is a pointer pointing at a branch, that a branch is itself a pointer pointing at a commit, and finally that a commit is a Git object pointing at compressed blobs containing data about your project at a certain commit. When the HEAD pointer moves around, whatever commit it points to populates the HEAD tree with the corresponding data.\nAs we saw earlier, when you create a commit, HEAD automatically points to the new commit. So the HEAD tree is often filled with the last snapshot you created. But—as we will see later—we can move the HEAD pointer around through other ways. So the HEAD tree can be populated by any snapshot in your project history."
  },
  {
    "objectID": "git/changes.html#git-diff",
    "href": "git/changes.html#git-diff",
    "title": "Inspecting changes",
    "section": "git diff",
    "text": "git diff\nThe command git diff prints the differences between any two Git objects. In particular, it allows to compare any two of the three trees with each other.\n\nDiff between working directory & index\n\ngit diff displays the differences between the working directory (the uncompressed files) and the index (the staging area):\ngit diff\nRight now, git diff does not return anything because our working tree and staging area are at the same point.\nLet’s make some changes in our proposal:\nnano ms/proposal.md\nNow, if we run git diff again, we get:\ndiff --git a/ms/proposal.md b/ms/proposal.md\nindex 0fb5cc8..4fdc1b0 100644\n--- a/ms/proposal.md\n+++ b/ms/proposal.md\n@@ -16,4 +16,4 @@ We hope to achieve a lot.\n\n # Conclusion\n\n-This is truly a great proposal.\n+This is truly a great proposal, but it needs a little more work.\n\n\nDiff between index & last commit\n\nTo see what would be committed if you ran git commit (that is the differences between the index and the last commit), you need to run instead:\ngit diff --cached\nWe aren’t getting any output because we haven’t staged anything that isn’t in our last commit. Let’s stage our changes and try again:\ngit add .\ngit diff --cached\ndiff --git a/ms/proposal.md b/ms/proposal.md\nindex 0fb5cc8..4fdc1b0 100644\n--- a/ms/proposal.md\n+++ b/ms/proposal.md\n@@ -16,4 +16,4 @@ We hope to achieve a lot.\n\n # Conclusion\n\n-This is truly a great proposal.\n+This is truly a great proposal, but it needs a little more work.\nThe changes are now between the staging area and HEAD.\n\nIf we run git diff now, we aren’t getting any output because the staging area has caught up with the working tree: they are now the same, so no differences.\n\ngit diff --cached is very convenient to check what will enter in the next commit.\n\n\nDiff between working directory & last commit\n\nFinally, to see the differences between your working directory and HEAD, you run:\ngit diff HEAD\nThis will be the sum of the previous two.\nRight now, git diff --cached and git diff HEAD print the same result, but if we make new changes, they will become different since git diff HEAD will reflect all the changes between the working tree and the last commit, while git diff --cached will only contain the differences between the staging area and the last commit.\n\n\nYour turn:\n\nModify one of the tracked files to visualize this."
  },
  {
    "objectID": "git/aliases.html",
    "href": "git/aliases.html",
    "title": "Aliases",
    "section": "",
    "text": "Sometimes, you want to replace longer Git commands with a shorter version. For example, wouldn’t it be nice if instead of typing git status you could type git st? It turns out this is easy to do via Git aliases – all you need to do is run the following command:\ngit config --global alias.st 'status'\nThis will create a Git alias (new Git command) st that will run status underneath.\nOf course, this becomes very useful with longer commands, e.g.\ngit config --global alias.last 'diff HEAD~1 HEAD'\ngit last\nwill show the file changes in the last commit, i.e. underneath git last will run git diff HEAD~1 HEAD.\nAll these definitions are stored in your ~/.gitconfig file, and you can always check them with (pay attention to the alias section in the output):\ngit config --list\nConsider the following useful aliases:\ngit config --global alias.list 'ls-tree --full-tree -r HEAD'\ngit config --global alias.one \"log --graph --date-order --date=short --pretty=format:'%C(cyan)%h %C(yellow)%ar %C(auto)%s%+b %C(green)%ae'\"\nNow try running:\ngit list   # list all files inside the repository\ngit one    # very nicely formatted version of git log\nNow, let’s build an alias for a more complex command: git grep \"test\" $(git rev-list --all). This example from the “Searching a Git project” section below will search for the string “test” in all previous commits. There are two problems with this command: (1) it takes an argument (the string “test”), and (2) it uses an output from another Unix command (git rev-list --all) as its input.\nIt turns out you can still automate this with a Git alias stored as a bash script! You need to place an executable bash script into a directory that is listed in your $PATH environment variable. For example, let’s assume that $HOME/bin is listed in your $PATH. Doing the following\ncat &lt;&lt; EOF &gt; $HOME/bin/git-search\n#!/bin/bash\ngit grep \\${1} \\$(git rev-list --all)\nEOF\nchmod u+x $HOME/bin/git-search\nwill place the script into the file $HOME/bin/git-search and will make it executable. Now, running\ngit search test\nshould search the entire current Git project history for “test”."
  },
  {
    "objectID": "calendar.html",
    "href": "calendar.html",
    "title": "Upcoming training events",
    "section": "",
    "text": "Our training events also get posted in our main site."
  },
  {
    "objectID": "bash/transfer.html",
    "href": "bash/transfer.html",
    "title": "Transferring files",
    "section": "",
    "text": "If you want to use the Alliance clusters to run some of your heavy computations, you will have to move files back and forth between your machine and the clusters.\nThis section covers various ways to do this."
  },
  {
    "objectID": "bash/transfer.html#remote-copies-with-scp",
    "href": "bash/transfer.html#remote-copies-with-scp",
    "title": "Transferring files",
    "section": "Remote copies with scp",
    "text": "Remote copies with scp\nSecure copy protocol (SCP) allows to copy files over the Secure Shell Protocol (SSH) with the scp utility. scp follows a syntax similar to that of the cp command.\nNote that you need to run it from your local machines (not from the cluster).\n\nCopy from your machine to the cluster\n# Copy a local file to your home directory on the cluster\nscp /local/path/file userxxx@bobthewren.c3.ca:\n# Copy a local file to some path on the cluster\nscp /local/path/file userxxx@bobthewren.c3.ca:/remote/path\n\n\nCopy from the cluster to your machine\n# Copy a file from the cluster to some path on your machine\nscp userxxx@bobthewren.c3.ca:/remote/path/file /local/path\n# Copy a file from the cluster to your current location on your machine\nscp userxxx@bobthewren.c3.ca:/remote/path/file .\nYou can also use wildcards to transfer multiple files:\n# Copy all the Bash scripts from your cluster home dir to some local path\nscp userxxx@bobthewren.c3.ca:*.sh /local/path\n\n\nCopying directories\nTo copy a directory, you need to add the -r (recursive) flag:\nscp -r /local/path/folder userxxx@bobthewren.c3.ca:/remote/path\n\n\nCopying for Windows users\nMobaXterm users (on Windows) can copy files by dragging them between the local and remote machines in the GUI. Alternatively, they can use the download and upload buttons.\n\n\nYour turn:\n\nCopy a file from your local computer to your home directory in the training cluster.\n\nHere is a video of a previous version of this workshop."
  },
  {
    "objectID": "bash/transfer.html#interactive-transfers-with-sftp",
    "href": "bash/transfer.html#interactive-transfers-with-sftp",
    "title": "Transferring files",
    "section": "Interactive transfers with sftp",
    "text": "Interactive transfers with sftp\nThe Secure File Transfer Protocol (SFTP) is more sophisticated and allows additional operations. The sftp command provided by OpenSSH and other packages launches an SFTP client:\nsftp userxxx@bobthewren.c3.ca\n\nLook at your prompt: your usual Bash/Zsh prompt has been replaced with sftp&gt;.\n\nFrom this prompt, you can access a number of SFTP commands. Type help for a list:\nsftp&gt; help\nAvailable commands:\nbye                                Quit sftp\ncd path                            Change remote directory to 'path'\nchgrp [-h] grp path                Change group of file 'path' to 'grp'\nchmod [-h] mode path               Change permissions of file 'path' to 'mode'\nchown [-h] own path                Change owner of file 'path' to 'own'\ncopy oldpath newpath               Copy remote file\ncp oldpath newpath                 Copy remote file\ndf [-hi] [path]                    Display statistics for current directory or\n                                   filesystem containing 'path'\nexit                               Quit sftp\nget [-afpR] remote [local]         Download file\nhelp                               Display this help text\nlcd path                           Change local directory to 'path'\nlls [ls-options [path]]            Display local directory listing\nlmkdir path                        Create local directory\nln [-s] oldpath newpath            Link remote file (-s for symlink)\nlpwd                               Print local working directory\nls [-1afhlnrSt] [path]             Display remote directory listing\nlumask umask                       Set local umask to 'umask'\nmkdir path                         Create remote directory\nprogress                           Toggle display of progress meter\nput [-afpR] local [remote]         Upload file\npwd                                Display remote working directory\nquit                               Quit sftp\nreget [-fpR] remote [local]        Resume download file\nrename oldpath newpath             Rename remote file\nreput [-fpR] local [remote]        Resume upload file\nrm path                            Delete remote file\nrmdir path                         Remove remote directory\nsymlink oldpath newpath            Symlink remote file\nversion                            Show SFTP version\n!command                           Execute 'command' in local shell\n!                                  Escape to local shell\n?                                  Synonym for help\nAs this list shows, you have access to a number of classic Unix command such as cd, pwd, ls, etc. These commands will be executed on the remote machine.\nIn addition, there are a number of commands of the form l&lt;command&gt;. “l” stands for “local”.\nThese commands will be executed on your local machine.\nFor instance, ls will list the files in your current directory in the remote machine while lls (“local ls”) will list the files in your current directory on your computer.\nThis means that you are now able to navigate two file systems at once: your local machine and the remote machine.\n\nHere are a few examples:\n\nsftp&gt; pwd              # print remote working directory\nsftp&gt; lpwd             # print local working directory\nsftp&gt; ls               # list files in remote working directory\nsftp&gt; lls              # list files in local working directory\nsftp&gt; cd               # change the remote directory\nsftp&gt; lcd              # change the local directory\nsftp&gt; put local_file   # upload a file\nsftp&gt; get remote_file  # download a file\n\nCopying directories\nTo upload/download directories, you first need to create them in the destination, then copy the content with the -r (recursive) flag.\n\nIf you have a local directory called dir and you want to copy it to the cluster you need to run:\n\nsftp&gt; mkdir dir    # First create the directory\nsftp&gt; put -r dir   # Then copy the content\nTo terminate the session, press &lt;Ctrl+D&gt;.\n\n\nYour turn:\n\nIn an SFTP session:\n\nList the content of projects (projects is in your home on the training cluster).\nCopy a file from the training cluster to your local computer."
  },
  {
    "objectID": "bash/transfer.html#syncing",
    "href": "bash/transfer.html#syncing",
    "title": "Transferring files",
    "section": "Syncing",
    "text": "Syncing\nIf, instead of an occasional copying of files between your machine and the cluster, you want to keep a directory in sync between both machines, you might want to use rsync instead. You can look at the Alliance wiki page on rsync for complete instructions."
  },
  {
    "objectID": "bash/transfer.html#heavy-transfers",
    "href": "bash/transfer.html#heavy-transfers",
    "title": "Transferring files",
    "section": "Heavy transfers",
    "text": "Heavy transfers\nWhile the methods covered above work very well for limited amounts of data, if you need to make large transfers, you should use globus instead, following the instructions in the Alliance wiki page on this service."
  },
  {
    "objectID": "bash/transfer.html#windows-line-endings",
    "href": "bash/transfer.html#windows-line-endings",
    "title": "Transferring files",
    "section": "Windows line endings",
    "text": "Windows line endings\nOn modern Mac operating systems and on Linux, lines in files are terminated with a newline (\\n). On Windows, they are terminated with a carriage return + newline (\\r\\n).\nWhen you transfer files between Windows and Linux (the cluster uses Linux), this creates a mismatch. Most modern software handle this correctly, but you may occasionally run into problems.\nThe solution is to convert a file from Windows encoding to Unix encoding with:\ndos2unix file\nTo convert a file back to Windows encoding, run:\nunix2dos file"
  },
  {
    "objectID": "bash/tools2.html",
    "href": "bash/tools2.html",
    "title": "A few more of our favourite tools",
    "section": "",
    "text": "In a previous webinar, we presented three of our favourite command line tools. Today, we will introduce other tools we find really useful in our daily workflow:\n\nlazygit: a wonderful terminal UI for Git,\nbat: a great syntax highlighter,\nripgrep: a fast alternative to grep,\nfd: a /really/ fast alternative to find,\npass: a command line password manager.\n\nAlong the way, I will use a few other neat command line tools such as hyperfine—for sophisticated benchmarking—and diff-so-fancy—which makes your diffs a lot more readable.\nFor the Emacs users among you, we will finish the workshop with two Emacs utilities:\n\nTRAMP: a remote file access system,\nHelm: a “framework for incremental completions and narrowing selections”."
  },
  {
    "objectID": "bash/text.html",
    "href": "bash/text.html",
    "title": "Searching & manipulating text",
    "section": "",
    "text": "cd ~/Desktop/data-shell/writing\nmore haiku.txt\nFirst let’s search for text in files:\ngrep not haiku.txt     # let's find all lines that contain the word 'not'\ngrep day haiku.txt     # now search for word 'day'\ngrep -w day haiku.txt     # search for a separate word 'day' (not 'today', etc.)\ngrep -w today haiku.txt   # search for 'today'\ngrep -w Today haiku.txt   # search for 'Today'\ngrep -i -w today haiku.txt       # both upper and lower case 'today'\ngrep -n -i -w today haiku.txt    # -n prints out numbers the matching lines\ngrep -n -i -w -v the haiku.txt   # -v searches for lines that do not contain 'the'\nman grep\nMore than two arguments to grep:\ngrep pattern file1 file2 file3   # all argument after the first one are assumed to be filenames\ngrep pattern *.txt   # the last argument will expand to the list of *.txt files\n\nThe Tao that is seen\nIs not the true Tao, until\nYou bring fresh toner.\nWith searching comes loss\nand the presence of absence:\n\"My Thesis\" not found.\nYesterday it worked.\nToday it is not working.\nSoftware is like that.\nFrom the above text, contained in the file haiku.txt, which command would result in the following output:\nand the presence of absence:\n\ngrep of haiku.txt\ngrep -E of haiku.txt\ngrep -w of haiku.txt \n\nHere is a video on this topic."
  },
  {
    "objectID": "bash/text.html#searching-inside-files-with-grep",
    "href": "bash/text.html#searching-inside-files-with-grep",
    "title": "Searching & manipulating text",
    "section": "",
    "text": "cd ~/Desktop/data-shell/writing\nmore haiku.txt\nFirst let’s search for text in files:\ngrep not haiku.txt     # let's find all lines that contain the word 'not'\ngrep day haiku.txt     # now search for word 'day'\ngrep -w day haiku.txt     # search for a separate word 'day' (not 'today', etc.)\ngrep -w today haiku.txt   # search for 'today'\ngrep -w Today haiku.txt   # search for 'Today'\ngrep -i -w today haiku.txt       # both upper and lower case 'today'\ngrep -n -i -w today haiku.txt    # -n prints out numbers the matching lines\ngrep -n -i -w -v the haiku.txt   # -v searches for lines that do not contain 'the'\nman grep\nMore than two arguments to grep:\ngrep pattern file1 file2 file3   # all argument after the first one are assumed to be filenames\ngrep pattern *.txt   # the last argument will expand to the list of *.txt files\n\nThe Tao that is seen\nIs not the true Tao, until\nYou bring fresh toner.\nWith searching comes loss\nand the presence of absence:\n\"My Thesis\" not found.\nYesterday it worked.\nToday it is not working.\nSoftware is like that.\nFrom the above text, contained in the file haiku.txt, which command would result in the following output:\nand the presence of absence:\n\ngrep of haiku.txt\ngrep -E of haiku.txt\ngrep -w of haiku.txt \n\nHere is a video on this topic."
  },
  {
    "objectID": "bash/text.html#text-manipulation",
    "href": "bash/text.html#text-manipulation",
    "title": "Searching & manipulating text",
    "section": "Text manipulation",
    "text": "Text manipulation\n(This example was kindly provided by John Simpson.)\nIn this section we’ll use two tools for text manipulation: sed and tr. Our goal is to calculate the frequency of all dictionary words in the novel “The Invisible Man” by Herbert Wells (public domain). First, let’s apply our knowledge of grep to this text:\n$ cd ~/Desktop/data-shell\n$ ls   # shows wellsInvisibleMan.txt\n$ wc wellsInvisibleMan.txt                          # number of lines, words, characters\n$ grep invisible wellsInvisibleMan.txt              # see the invisible man\n$ grep invisible wellsInvisibleMan.txt | wc -l      # returns 60; adding -w gives the same count\n$ grep -i invisible wellsInvisibleMan.txt | wc -l   # returns 176 (includes: invisible Invisible INVISIBLE)\nLet’s sidetrack for a second and see how we can use the “stream editor” sed:\n$ sed 's/[iI]nvisible/supervisible/g' wellsInvisibleMan.txt &gt; visibleMan.txt   # make him visible\n$ cat wellsInvisibleMan.txt | sed 's/[iI]nvisible/supervisible/g' &gt; visibleMan.txt   # this also works (standard input)\n$ grep supervisible visibleMan.txt   # see what happened to the now visible man\n$ grep -i invisible visibleMan.txt   # see what was not converted\n$ man sed\nNow let’s remove punctuation from the original file using “tr” (translate) command:\n$ cat wellsInvisibleMan.txt | tr -d \"[:punct:]\" &gt; invisibleNoPunct.txt    # tr only takes standard input\n$ tail wellsInvisibleMan.txt\n$ tail invisibleNoPunct.txt\nNext convert all upper case to lower case:\n$ cat invisibleNoPunct.txt | tr '[:upper:]' '[:lower:]' &gt; invisibleClean.txt\n$ tail invisibleClean.txt\nNext replace spaces with new lines:\n$ cat invisibleClean.txt | sed 's/ /\\'$'\\n/g' &gt; invisibleList.txt   # \\'$'\\n is a shortcut for a new line\n$ more invisibleList.txt\nNext remove empty lines:\n$ sed '/^$/d' invisibleList.txt  &gt; invisibleCompact.txt\nNext sort the list alphabetically, count each word’s occurrence, and remove duplicate words:\n$ cat invisibleCompact.txt | sort | uniq -c &gt; invisibleWords.txt\n$ more invisibleWords.txt\nNext sort the list into most frequent words:\n$ cat invisibleWords.txt | sort -gr &gt; invisibleFrequencyList.txt   # use 'man sort'\n$ more invisibleFrequencyList.txt\n\n\n\n\n\n\nYou can {{&lt;a “https://youtu.be/4IkHY84uUss” “watch a video for this topic”&gt;}} after the workshop.\nQuick reference:\nsed 's/pattern1/pattern2/' filename    # replace pattern1 with pattern2, one per line\nsed 's/pattern1/pattern2/g' filename   # same but multiple per line\nsed 's|pattern1|pattern2|g' filename   # same\n\ncat wellsInvisibleMan.txt | tr -d \"[:punct:]\" &gt; invisibleNoPunct.txt       # remove punctuation; tr only takes standard input\ncat invisibleNoPunct.txt | tr '[:upper:]' '[:lower:]' &gt; invisibleClean.txt # convert all upper case to lower case:\ncat invisibleClean.txt | sed 's/ /\\'$'\\n/g' &gt; invisibleList.txt            # replace spaces with new lines;\n                                                                           # \\'$'\\n is a shortcut for a new line\nsed '/^$/d' invisibleList.txt  &gt; invisibleCompact.txt   # remove empty lines\ncat invisibleCompact.txt | sort | uniq -c &gt; invisibleWords.txt   # sort the list alphabetically, count each word's occurrence\ncat invisibleWords.txt | sort -gr &gt; invisibleFrequencyList.txt   # sort the list into most frequent words\n\nWrite a script that takes an English-language file and print the list of its 100 most common words, along with the word count. Hint: use the workflow from the text manipulation video. Finally, convert this script into a bash function. (no need to type any answer)"
  },
  {
    "objectID": "bash/text.html#column-based-text-processing-with-awk-scripting-language",
    "href": "bash/text.html#column-based-text-processing-with-awk-scripting-language",
    "title": "Searching & manipulating text",
    "section": "Column-based text processing with awk scripting language",
    "text": "Column-based text processing with awk scripting language\ncd .../data-shell/writing\ncat haiku.txt   # 11 lines\nYou can define inline awk scripts with braces surrounded by single quotation:\nawk '{print $1}' haiku.txt       # $1 is the first field (word) in each line =&gt; processing columns\nawk '{print $0}' haiku.txt       # $0 is the whole line\nawk '{print}' haiku.txt          # the whole line is the default action\nawk -Fa '{print $1}' haiku.txt   # can specify another separator with -F (\"a\" in this case)\nYou can use multiple commands inside your awk script:\necho Hello Tom &gt; hello.txt\necho Hello John &gt;&gt; hello.txt\nawk '{$2=\"Adam\"; print $0}' hello.txt   # we replaced the second word in each line with \"Adam\"\nMost common awk usage is to postprocess output of other commands:\n/bin/ps aux    # display all running processes as multi-column output\n/bin/ps aux | awk '{print $2 \" \" $11}'   # print only the process number and the command\nAwk also takes patterns in addition to scripts:\nawk '/Yesterday|Today/' haiku.txt   # print the lines that contain the words Yesterday or Today\nAnd then you act on these patterns: if the pattern evaluates to True, then run the script:\nawk '/Yesterday|Today/{print $3}' haiku.txt\nawk '/Yesterday|Today/' haiku.txt | awk '{print $3}'   # same as previous line\nAwk has a number of built-in variables; the most commonly used is NR:\nawk 'NR&gt;1' haiku.txt    # if NumberRecord &gt;1 then print it (default action), i.e. skip the first line\nawk 'NR&gt;1{print $0}' haiku.txt   # last command expanded\nawk 'NR&gt;1 && NR &lt; 5' haiku.txt   # print lines 2-4\n\nExercise: write a awk script to process cities.csv to print only town/city names and their population and store it in a separate file populations.csv. Try to do everything in a single-line command.\n\nQuick reference:\nls -l | awk 'NR&gt;3 {print $5 \"  \" $9}'   # print 5th and 9th columns starting with line 4\nawk 'NR&gt;1 && NR &lt; 5' haiku.txt          # print lines 2-4\nawk '/Yesterday|Today/' haiku.txt       # print lines that contain Yesterday or Today\n\nWrite a one-line command that finds 5 largest files in the current directory and prints only their names and file sizes in the human-readable format (indicating bytes, kB, MB, GB, …) in the decreasing file-size order. Hint: use find, xargs, and awk. \n\nLet’s study together these commands:\n$ source ~/projects/def-sponsor00/shared/fzf/.fzf.bash\n$ kill -9 `/bin/ps aux | fzf | awk '{print $2}'`\n\nHere is a video on this topic."
  },
  {
    "objectID": "bash/resources.html",
    "href": "bash/resources.html",
    "title": "Resources",
    "section": "",
    "text": "This section lists a few useful Bash resources.\n\nOne very useful (although very dense) resource is the Bash manual. This is the absolute reference.\nThere are countless sites with Bash courses and guides. Here are a few:\n\nBash Guide for Beginners\nLearn X in Y minutes\nLinux Bash Shell Scripting Tutorial\n\nYou can also get information on Bash from within Bash with:\ninfo bash\nand:\nman bash\nThere are also countless resources online and don’t forget to Google anything you don’t know how to do: you will almost certainly find the answer on StackOverflow or some Stack Exchange site."
  },
  {
    "objectID": "bash/molecules/wildcards.html",
    "href": "bash/molecules/wildcards.html",
    "title": "Wildcards",
    "section": "",
    "text": "Wildcards are a convenient way to select items matching patterns.\n\n\n\n\n\n\n\nData for this section\n\n\n\n\n\nFor this section, we will play with files created by The Carpentries.\nYou can download them into a zip file called bash.zip with:\nwget http://bit.ly/bashfile -O bash.zip\nYou can then unzip that file with:\nunzip bash.zip\nYou should now have a data-shell directory with a molecules subdirectory.\ncd into it:\ncd data-shell/molecules\n\n\n\nLet’s list the files in the molecules directory:\nls\ncubane.pdb  ethane.pdb  methane.pdb  octane.pdb  pentane.pdb  propane.pdb\nYou could do the same with:\nls *\nThe star expands to all files/directories matching any pattern. It is a wildcard.\nOf course, you can match more interesting patterns.\nFor instance, to list all files starting with the letter o, we can run:\nls o*\noctane.pdb\nWhile to list all files containing the letter o anywhere in their name, you can use:\nls *o*\noctane.pdb  propane.pdb\nThis saves a lot of typing and is a powerful way to apply a command to a subset of files/directories.\n\n\nYour turn:\n\nWildcards are often used to select all files with a certain extension.\nLet’s create 3 new files:\ntouch file1.txt file2.txt file3.md\nHow would you list all files with the .txt extension and only those?"
  },
  {
    "objectID": "bash/molecules/redirections.html",
    "href": "bash/molecules/redirections.html",
    "title": "Redirections & pipes",
    "section": "",
    "text": "By default, commands that produce an output print to the terminal. This output can however be redirected to be printed elsewhere (e.g. to files) or to be passed as the argument of another command.\nThis section will cover the most basic cases."
  },
  {
    "objectID": "bash/molecules/redirections.html#redirections",
    "href": "bash/molecules/redirections.html#redirections",
    "title": "Redirections & pipes",
    "section": "Redirections",
    "text": "Redirections\nBy default, commands that produce an output print it to standard output—that is, the terminal. That is what we have been doing so far.\nThis output can however be redirected with the &gt; sign. For instance, it can be redirected to a file, which is very handy if you want to save the result.\n\nExample:\n\nLet’s print the number of lines in each .pdb file in the molecules directory:\n\nwc -l *.pdb\n\n  20 cubane.pdb\n  12 ethane.pdb\n   9 methane.pdb\n  30 octane.pdb\n  21 pentane.pdb\n  15 propane.pdb\n 107 total\n\n\n\n\nYour turn:\n\n\nWhat does the wc command do?\nWhat does the -l flag for this command do?\nHow did you find out?\n\n\nTo save this result into a file called lengths.txt, we run:\n\nwc -l *.pdb &gt; lengths.txt\n\n\nNote that &gt; always creates a new file. If a file called lengths.txt already exists, it will be overwritten. Be careful not to lose data this way!\nIf you don’t want to lose the content of the old file, you can append the output to the existing file with &gt;&gt; (&gt;&gt; will create a file lengths.txt if it doesn’t exist yet, but if it exists, it will append the new content below the old one).\n\n\n\nYour turn:\n\nHow can you make sure that you did create a file called lengths.txt?\n\nLet’s print its content to the terminal:\n\ncat lengths.txt\n\n  20 cubane.pdb\n  12 ethane.pdb\n   9 methane.pdb\n  30 octane.pdb\n  21 pentane.pdb\n  15 propane.pdb\n 107 total\n\n\nAs you can see, it contains the output of the command wc -l *.pdb.\nOf course, we can print the content of the file with modification. For instance, we can sort it:\n\nsort -n lengths.txt\n\n   9 methane.pdb\n  12 ethane.pdb\n  15 propane.pdb\n  20 cubane.pdb\n  21 pentane.pdb\n  30 octane.pdb\n 107 total\n\n\nAnd we can redirect this new output to a new file:\n\nsort -n lengths.txt &gt; sorted.txt\n\nInstead of printing an entire file to the terminal, you can print only part of it.\nLet’s print the first line of the new file sorted.txt:\n\nhead -1 sorted.txt\n\n   9 methane.pdb"
  },
  {
    "objectID": "bash/molecules/redirections.html#pipes",
    "href": "bash/molecules/redirections.html#pipes",
    "title": "Redirections & pipes",
    "section": "Pipes",
    "text": "Pipes\nAnother form of redirection is the Bash pipe. Instead of redirecting the output to a different stream for printing, the output is passed as an argument to another command. This is very convenient because it allows to chain multiple commands without having to create files or variables to save intermediate results.\nFor instance, we could run the three commands we ran previously at once, without the creation of the two intermediate files:\n\nwc -l *.pdb | sort -n | head -1\n\n   9 methane.pdb\n\n\nIn each case, the output of the command on the left-hand side (LHS) is passed as the input of the command on the right-hand side (RHS).\n\n\nYour turn:\n\nIn a directory we want to find the 3 files that have the least number of lines. Which command would work for this?\n\nwc -l * &gt; sort -n &gt; head -3\nwc -l * | sort -n | head 1-3\nwc -l * | sort -n | head -3\nwc -l * | head -3 | sort -n\n\n\nHere is a video of a previous version of this workshop."
  },
  {
    "objectID": "bash/molecules/control_flow.html#for-loops",
    "href": "bash/molecules/control_flow.html#for-loops",
    "title": "Control flow",
    "section": "For loops",
    "text": "For loops\nTo apply a set of commands to all the elements of a list, you can use a for loop.\n\nSyntax\nThe general structure of a for loop is as follows:\nfor &lt;iterable&gt; in &lt;list&gt;\ndo\n    &lt;command1&gt;\n    &lt;command2&gt;\n    ...\ndone\n\n\nExample\nThe molecules directory contains the following .pdb files:\nls *.pdb\ncubane.pdb  ethane.pdb  methane.pdb  octane.pdb  pentane.pdb  propane.pdb\nWe want to rename these files by prepending “gas_” to their current names.\nWildcards don’t work here:\n\nmv *.pdb gas_*.pdb\n\nmv: target 'gas_*.pdb': No such file or directory\n\n\nThe solution is to use a for loop:\nfor file in *.pdb\ndo\n    mv $file gas_$file\ndone\nThis can also be written as a one-liner, although it is harder to read:\nfor file in *.pdb; do mv $file gas_$file; done\n\n\nYour turn:\n\nUsing what we learnt in the string manipulation section, how could you remove the gas_ prefix to all these files?\n\n\n\nCollections\nFor loops run a set of commands for each item of a collection. How do you create those collections?\n\nListing items one by one\nThe least efficient method is to list all the items one by one:\n\nExample:\n\nfor i in file1 file2 file3\ndo\n    echo $i\ndone\nfile1\nfile2\nfile3\n\n\nWildcards\nAs we have already seen, wildcards are very useful to build for loops.\n\n\nBrace expansion\nCollections can also be created with brace expansion.\n\nExamples:\n\n\necho {1,2,5}\n\n1 2 5\n\n\n\nMake sure not to add a space after the commas.\n\n\necho {list,of,strings}\n\nlist of strings\n\n\n\necho {file1,file2}.sh\n\nfile1.sh file2.sh\n\n\n\nls -l {ethane,methane,pentane}.pdb\n\n-rw-rw-r-- 1 marie marie  622 Sep 16  2021 ethane.pdb\n-rw-rw-r-- 1 marie marie  422 Sep 16  2021 methane.pdb\n-rw-rw-r-- 1 marie marie 1226 Sep 16  2021 pentane.pdb\n\n\n\necho {1..5}\n\n1 2 3 4 5\n\n\n\necho {01..10}\n\n01 02 03 04 05 06 07 08 09 10\n\n\n\necho {r..v}\n\nr s t u v\n\n\n\necho {v..r}\n\nv u t s r\n\n\n\necho {a..e}{1..3}\n\na1 a2 a3 b1 b2 b3 c1 c2 c3 d1 d2 d3 e1 e2 e3\n\n\n\necho {a..c}{a..c}\n\naa ab ac ba bb bc ca cb cc\n\n\n\necho {1..5}.txt\n\n1.txt 2.txt 3.txt 4.txt 5.txt\n\n\n\necho file{3..6}.sh\n\nfile3.sh file4.sh file5.sh file6.sh\n\n\nBrace expansion can be used to create lists iterated over in loops, but also to apply commands to files or directories.\n\n\nSequences\nCollections can also be sequences:\n\nseq 1 2 10\n\n1\n3\n5\n7\n9\n\n\n\nHere, 1 is the start of the sequence, 10 is the end, and 2 is the step.\n\nSuch a sequence could be used in a loop this way:\n\nfor i in $(seq 1 2 10)\ndo\n    echo file$i.txt\ndone\n\nfile1.txt\nfile3.txt\nfile5.txt\nfile7.txt\nfile9.txt\n\n\n\n\n\nYour turn:\n\nIn a directory the command ls returns:\nfructose.dat  glucose.dat  sucrose.dat  maltose.txt\nWhat would be the output of the following loop?\nfor datafile in *.dat\ndo\n  cat $datafile &gt;&gt; sugar.dat\ndone\n\nAll of the text from fructose.dat, glucose.dat and sucrose.dat would be concatenated and saved to a file called sugar.dat.\nThe text from sucrose.dat will be saved to a file called sugar.dat.\nAll of the text from fructose.dat, glucose.dat, sucrose.dat, and maltose.txt would be concatenated and saved to a file called sugar.dat.\nAll of the text from fructose.dat, glucose.dat and sucrose.dat will be printed to the screen and saved into a file called sugar.dat."
  },
  {
    "objectID": "bash/molecules/control_flow.html#while-loops",
    "href": "bash/molecules/control_flow.html#while-loops",
    "title": "Control flow",
    "section": "While loops",
    "text": "While loops\n\nSyntax\nThe syntax of a while loop in Bash is:\nwhile predicate\ndo\n    command1\n    command2\n    ...\ndone\nThe set of commands in the body of the while loop are executed as long as the predicate returns true.\nBe careful that while loop can lead to infinite loops. Such loops need to be manually interrupted (by pressing &lt;Ctrl+C&gt;).\n\nExample of infinite loop:\n\nwhile true\ndo\n    echo \"Press &lt;Ctrl+C&gt; to stop\"\n    sleep 1\ndone\nHere is a video of a previous version of this workshop."
  },
  {
    "objectID": "bash/molecules/control_flow.html#conditionals",
    "href": "bash/molecules/control_flow.html#conditionals",
    "title": "Control flow",
    "section": "Conditionals",
    "text": "Conditionals\n\nSyntax\nif [ predicate1 ]\nthen\n    command1\n    command2\n    ...\nelif [ predicate2 ]\nthen\n    command3\n    command4\n    ...\nelse\n    command5\n    command6\n    ...\nfi\n\n\nExample\nLet’s create a file called check.sh with the following if statement:\nfor f in $@\ndo\n    if [ -e $f ]      # Make sure to have spaces around each bracket\n    then\n        echo $f exists\n    else\n        echo $f does not exist\n    fi\ndone\nNow, let’s make it executable:\nchmod u+x check.sh\nAnd let’s run this:\n./check.sh file1 file2 check.sh file3\n\n\nPredicates\nHere are a few predicates:\n[ $var == 'text' ] checks whether var is equal to 'text'.\n[ $var == number ] checks whether var is equal to number.\n[ -e file ] checks whether file exists.\n[ -d name ] checks whether name is a directory.\n[ -f name ] checks whether name is a file."
  },
  {
    "objectID": "bash/intro_scripting.html#background",
    "href": "bash/intro_scripting.html#background",
    "title": "Automation & scripting in bash for beginners",
    "section": "Background",
    "text": "Background\n\nWhat are Unix shells?\nA Unix shell is a command line interpreter: the user enters commands as text, either interactively in the command line or in a script, and the shell passes them to the operating system.\n\nBash\nBash (Bourne Again SHell), released in 1989, is part of the GNU Project and is the default Unix shell on many systems (MacOS recently changed its default to zsh).\n\n\nOther shells\nPrior to Bash, the default was the Bourne shell (sh).\nA new and popular shell (backward compatible with Bash) is zsh. It extends Bash’s capabilities.\nAnother shell in the same family is the KornShell (ksh).\nAll these shells are quite similar. The C shell (csh) however was modeled on the C programming language.\nBash is the most common shell and the one which makes the most sense to learn as a first Unix shell.\n\n\n\nWhy use a shell?\nWhile automating GUI operations is really difficult, it is easy to rerun a script (a file with a number of commands). Unix shells thus allow the creation of reproducible workflows and the automation of repetitive tasks.\nThey are powerful to launch tools, modify files, search text, or combine commands.\nThey also allow to work on remote machines and HPC systems."
  },
  {
    "objectID": "bash/intro_scripting.html#how-we-will-use-bash-today",
    "href": "bash/intro_scripting.html#how-we-will-use-bash-today",
    "title": "Automation & scripting in bash for beginners",
    "section": "How we will use Bash today",
    "text": "How we will use Bash today\nBash is a Unix shell. You thus need a Unix or Unix-like operating system.\nWe will connect to a remote HPC system via SSH (secure shell). HPC systems always run Linux.\nThose on Linux or MacOS can alternatively use Bash directly on their machine. On MacOS, the default is now zsh (you can see that by typing echo $SHELL in Terminal), but zsh is fully compatible with Bash commands, so it is totally fine to use it instead. If you really want to use Bash, simply launch it by typing in Terminal: bash.\n\nConnecting to a remote HPC system via SSH\n\nUsernames and password\nWe will give you a link to an etherpad during the workshop. Add your name next to a free username to claim it.\nWe will also give you the password for our training cluster. When prompted, enter it.\n\nNote that you will not see any character as you type the password: this is called blind typing and is a Linux safety feature. Type slowly and make sure not to make typos. It can be unsettling at first not to get any feed-back while typing.\n\n\n\nLinux and MacOS users\nLinux users: open the terminal emulator of your choice.\nMacOS users: open “Terminal”.\nThen type:\nssh userxx@bashworkshop.c3.ca  # Replace userxx by your username (e.g. user09)\n\n\nWindows users\nWe suggest using the free version of MobaXterm.\nMobaXterm comes with a terminal emulator and a GUI interface for SSH sessions.\nOpen MobaXterm, click on “Session”, then “SSH”, and fill in the Remote host name and your username. Here is a live demo."
  },
  {
    "objectID": "bash/intro_scripting.html#bash-the-basics",
    "href": "bash/intro_scripting.html#bash-the-basics",
    "title": "Automation & scripting in bash for beginners",
    "section": "Bash: the basics",
    "text": "Bash: the basics\n\nThe prompt\nIn command-line interfaces, a command prompt is a sequence of characters indicating that the interpreter is ready to accept input. It can also provide some information (e.g. time, error types, username and hostname, etc.)\nThe Bash prompt is customizable. By default, it often gives the username and the hostname, and it typically ends with $.\n\n\nHelp on commands\nMan pages:\nman &lt;command&gt;\n\nMan pages open in a pager (usually less).\nNavigate up/down with the space bar and the b key.\nQuit the pager with the q key.\n\nHelp pages:\n&lt;command&gt; --help\nInspect commands:\ncommand -V &lt;command&gt;\n\n\nExamples of commands\n\nPrint working directory: pwd\nChange directory: cd\nPrint: echo\nPrint content of a file: cat\nList: ls\nCopy: cp\nMove or rename: mv\nCreate a new directory: mkdir\nCreate a new file: touch\n\n\n\nKeybindings\nClear the terminal (command clear) with C-l (this means: press the Ctrl and L keys at the same time).\nNavigate command history with C-p and C-n (or up and down arrows).\nYou can auto-complete commands by pressing the tab key."
  },
  {
    "objectID": "bash/intro_scripting.html#bash-scripting-the-basics",
    "href": "bash/intro_scripting.html#bash-scripting-the-basics",
    "title": "Automation & scripting in bash for beginners",
    "section": "Bash scripting: the basics",
    "text": "Bash scripting: the basics\nInstead of typing commands one at a time directly in a terminal, you can write them down, one per line, in a text file called a script.\nThey will be run in the order in which they are written when you execute the script.\nThis is a great way to automate tasks: to rerun this sequence of commands, you simply have to rerun the script.\n\nFile name\nShell scripts, including Bash scripts, are usually given the extension sh (e.g. my_script.sh).\nYou can store scripts anywhere, but a common practice is to store them in a ~/bin directory.\n\n\nSyntax\n\nShebang\nScripts can be written for any interpreter (e.g. Bash, Python, R, etc.) The way to tell the system which one to use is to use a shebang (#!) followed by the path of the interpreter on the first line of the script.\nTo use Bash, start your scripts with:\n#!/bin/bash\nYou may also encounter this notation:\n#!/usr/bin/env bash\nIf you are curious, you can read the answers to this Stack Overflow question for the differences between the two.\n\n\nComments\nAnything to the left of # is ignored by the interpreter and is for human consumption only.\n# You can write full-line comments\n\npwd       # You can also write comments after commands\n\n\n\nExecuting scripts\nThere are two ways to execute a script:\nbash my_script.sh\n./my_script.sh  # The dot represents the current directory\nIn the latter case, you need to make sure that your script is executable by first running:\nchmod u+x my_script.sh  # This makes the script executable by the user (i.e. you)\n\n\nOur first script\nOpen a text editor (e.g. nano) and type:\n#!/bin/bash\n\necho \"This is our first script.\"\nSave and close the file.\n\n\nYour turn:\n\nNow run the script with one, then the other method.\nWhat does this script do?"
  },
  {
    "objectID": "bash/intro_scripting.html#variables",
    "href": "bash/intro_scripting.html#variables",
    "title": "Automation & scripting in bash for beginners",
    "section": "Variables",
    "text": "Variables\n\nDeclaring variables\nYou can declare a variable (i.e. a name that holds a value) with the = sign.\n!! Make sure not to put spaces around the equal sign.\nvariable=Test\n\n\nQuotes\nLet’s experiment with quotes:\n\nvariable=This string is the value of the variable\necho $variable\n\nbash: line 1: string: command not found\n\n\nOops…\n\nvariable=\"This string is the value of the variable\"\necho $variable\n\nThis string is the value of the variable\n\n\n\nvariable='This string is the value of the variable'\necho $variable\n\nThis string is the value of the variable\n\n\n\nvariable='This string's the value of the variable'\necho $variable\n\nbash: -c: line 1: unexpected EOF while looking for matching `''\nbash: -c: line 3: syntax error: unexpected end of file\n\n\nOops…\nOne solution to this is to use double quotes:\n\nvariable=\"This string's the value of the variable\"\necho $variable\n\nThis string's the value of the variable\n\n\nAlternatively, single quotes can be escaped:\n\nvariable='This string'\"'\"'s the value of the variable'\necho $variable\n\nThis string's the value of the variable\n\n\n\nAdmittedly, this last one is a little crazy. It is the way to escape single quotes in single-quoted strings.\nThe first ' ends the first string, both \" create a double-quoted string with ' (escaped) in it, then the last ' starts the second string.\nEscaping double quotes is a lot easier and simply requires \\\".\n\n\n\nExpanding a variable’s value\nTo expand a variable (to access its value), you need to prepend its name with $:\n\nvariable=Test\necho variable\n\nvariable\n\n\nMmmm… not really want we want!\n\nvariable=Test\necho $variable\n\nTest\n\n\n\nvariable=Test; echo \"$variable\"\n\nTest\n\n\n!! Single quotes don’t expand variables.\n\nvariable=Test; echo '$variable'\n\n$variable\n\n\n\n\nPassing variables to a Bash script\nCreate a script called name.sh with the following content:\n#!/bin/bash\n\necho \"My name is $1.\"  # $1 refers to the first variable passed to the script\nYou can now pass a variable to this script with:\nbash name.sh Marie\nMy name is Marie.\nYou can pass several variables to a script. Copy name.sh to name2.sh and edit name2.sh to look like the following:\n#!/bin/bash\n\necho \"My name is $1 and I am $2 years old.\"\nbash name2.sh Marie 43\nMy name is Marie and I am 43 years old.\nYou can also pass any number of variables to a script:\n#!/bin/bash\n\necho $@\nbash script.sh argument1 argument2 argument3 argument4\nargument1 argument2 argument3 argument4\n\n\nBrace expansion\n\necho {1..5}\n\n1 2 3 4 5\n\n\n\necho {01..10}\n\n01 02 03 04 05 06 07 08 09 10\n\n\n\necho {1..5}.txt\n\n1.txt 2.txt 3.txt 4.txt 5.txt\n\n\n\necho {r..v}\n\nr s t u v\n\n\n\necho {file1,file2}.sh\n\nfile1.sh file2.sh\n\n\n!! Make sure not to add a space after the comma.\ntouch {file1,file2}.sh\ntouch file{3..6}.sh\n\necho {list,of,strings}\n\nlist of strings\n\n\n\n\nWildcards\nWildcards are really powerful to apply a command to all the elements having a common pattern.\nFor instance, we can delete all the files we created earlier (file1.sh, file2.sh, etc.) with a single command:\nrm file*.sh\n!! Be very careful that rm is irreversible. Deleted files do not go to the trash: they are gone."
  },
  {
    "objectID": "bash/intro_scripting.html#loops",
    "href": "bash/intro_scripting.html#loops",
    "title": "Automation & scripting in bash for beginners",
    "section": "Loops",
    "text": "Loops\nTo apply a set of commands to all the elements of a list, you can use for loops. The general structure is as follows:\nfor &lt;iterable&gt; in &lt;list&gt;\ndo\n    &lt;statement1&gt;\n    &lt;statement2&gt;\n    ...\ndone\nLet’s create the script names.sh:\n#!/bin/bash\n\nfor name in $@\ndo\n    echo $name\ndone\nNow let’s run it with a list of arguments:\nbash names.sh Patrick Paul Marie Alex\nPatrick\nPaul\nMarie\nAlex\n\n\nYour turn:\n\nCompare the outputs of the following 2 scripts:\n\nscript1.sh:\n\n#!/bin/bash\n\necho $@\n\nscript2.sh:\n\n#!/bin/bash\n\nfor i in $@\ndo\n    echo $i\ndone\nHow do you explain the difference between running:\nbash script1.sh arg1 arg2 arg3\nand running:\nbash script2.sh arg1 arg2 arg3"
  },
  {
    "objectID": "bash/intro_scripting.html#lets-put-it-all-together-to-automate-some-task",
    "href": "bash/intro_scripting.html#lets-put-it-all-together-to-automate-some-task",
    "title": "Automation & scripting in bash for beginners",
    "section": "Let’s put it all together to automate some task",
    "text": "Let’s put it all together to automate some task\nThis is a rather silly example, but bear with me and let’s imagine that it actually makes sense (of course, you don’t write that many thesis chapters so you would probably never automate these tasks…)\nSo… let’s imagine that each time you write a thesis chapter, you do the same things:\n\nyou create a directory with the name of the chapter,\nyou create a number of subdirectories (for your source code, your manuscript, your data, and your results),\nyou create a Python script in the source code directory,\nyou create a markdown document in your manuscript directory,\nyou put the whole thing under version control with Git,\nyou create a .gitignore file in which you put the data subdirectory.\n\n\n\nYour turn:\n\nWrite a script that would do all this, then test the script.\nGive it a try on your own before looking at the solution below…"
  },
  {
    "objectID": "bash/intro_scripting.html#solution",
    "href": "bash/intro_scripting.html#solution",
    "title": "Automation & scripting in bash for beginners",
    "section": "Solution",
    "text": "Solution\nHere is what the script looks like (let’s call it chapter.sh):\n#!/bin/bash\n\nmkdir $1\ncd $1\nmkdir src data results ms\ntouch src/$1.py ms/$1.md\ngit init\necho data/ &gt; .gitignore\nYou then run the script:\nbash chapter.sh chapter1\nYou can verify that all the files and directories got created with:\ntree chapter1\nchapter1/\n├── data\n├── ms\n│   └── chapter1.md\n├── results\n└── src\n    └── chapter1.py\nand:\nls -aF chapter1\n./  ../  data/  .git/  .gitignore  ms/  results/  src/\nYou can also verify the content of your .gitignore file with:\ncat chapter1/.gitignore\ndata/"
  },
  {
    "objectID": "bash/intro_scripting.html#resources",
    "href": "bash/intro_scripting.html#resources",
    "title": "Automation & scripting in bash for beginners",
    "section": "Resources",
    "text": "Resources\nOne very useful (although very dense) resource is the Bash manual.\nYou can also get information on Bash from within Bash with:\ninfo bash\nand:\nman bash\nThere are also countless resources online and don’t forget to Google anything you don’t know how to do: you will almost certainly find the answer on StackOverflow or some Stack Exchange site."
  },
  {
    "objectID": "bash/functions.html",
    "href": "bash/functions.html",
    "title": "Functions",
    "section": "",
    "text": "As in any programming language, Bash functions are blocks of code that can be accessed by their names."
  },
  {
    "objectID": "bash/functions.html#function-definition",
    "href": "bash/functions.html#function-definition",
    "title": "Functions",
    "section": "Function definition",
    "text": "Function definition\n\nSyntax\nYou define a new function with the following syntax:\nname() {\n    command1\n    command2\n    ...\n}\n\n\nExample\ngreetings() {\n  echo hello\n}"
  },
  {
    "objectID": "bash/functions.html#storing-functions",
    "href": "bash/functions.html#storing-functions",
    "title": "Functions",
    "section": "Storing functions",
    "text": "Storing functions\nYou can define a new function directly in the terminal. Such function would however only be available during your current session. Since functions contain code that is intended to be run repeatedly, it makes sense to store function definitions in a file. Before functions become available, the file needs to be sourced (e.g. source file.sh).\nA convenient file is ~/.bashrc. The file is automatically sourced every time you start a shell so your functions will always be defined and ready for use."
  },
  {
    "objectID": "bash/functions.html#example-1",
    "href": "bash/functions.html#example-1",
    "title": "Functions",
    "section": "Example",
    "text": "Example\nLet’s write a function called combine that takes all the files we pass to it, copies them into a randomly named directory, and prints that directory to the terminal:\ncombine() {\n  if [ $# -eq 0 ]; then\n    echo \"No arguments specified. Usage: combine file1 [file2 ...]\"\n    return 1                # Return a non-zero error code\n  fi\n  dir=$RANDOM$RANDOM\n  mkdir $dir\n  cp $@ $dir\n  echo look in the directory $dir\n}\n\n\nYour turn:\n\nWrite a function to swap two file names.\nAdd a check that both files exist before renaming them.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere is a video of a previous version of this workshop."
  },
  {
    "objectID": "bash/basics.html",
    "href": "bash/basics.html",
    "title": "Bash: the basics",
    "section": "",
    "text": "What does it feel like to work in a shell? Here is a first basic orientation."
  },
  {
    "objectID": "bash/basics.html#the-prompt",
    "href": "bash/basics.html#the-prompt",
    "title": "Bash: the basics",
    "section": "The prompt",
    "text": "The prompt\nIn command-line interfaces, a command prompt is a sequence of characters indicating that the interpreter is ready to accept input. It can also provide some information (e.g. time, error types, username and hostname, etc.)\nThe Bash prompt is customizable. By default, it often gives the username and the hostname, and it typically ends with $."
  },
  {
    "objectID": "bash/basics.html#commands",
    "href": "bash/basics.html#commands",
    "title": "Bash: the basics",
    "section": "Commands",
    "text": "Commands\nBash comes with a number of commands: directives to the shell to perform particular tasks.\n\nExamples of commands:\n\n\nPrint working directory: pwd\nChange directory: cd\nPrint: echo\nPrint content of a file: cat\nList files and directories in working directory: ls\nCopy: cp\nMove or rename: mv\nCreate a new directory: mkdir\nCreate a new file: touch\n\n\nCommand options\nCommands come with a number of flags (options).\n\nExamples of flags for the ls command:\n\n\nList all files and directories (not ignoring hidden files): ls -a\nList files and directories in a long format: ls -l\nList files and directories in a human readable format (using units such as K, M, G): ls -h\n\nFlags can be combined. The order doesn’t matter and the followings are all the same:\n\nls -alh\nls -a -l -h\nls -ahl\nls -l -ha\n…\n\n\n\nHelp on commands\nThe command man provides an interface to the system reference manuals.\nTo access the manual page of a command, you thus can type:\nman &lt;command&gt;\n\nMan pages open in a pager (usually less). Here are some usual key bindings to navigate the pager:\n\nYou can scroll down with the space bar and back up with the b (for “back”) key.\nGo to the top of the document with the g (for “go”) key. G will send you to the end of the document and 7g will send you to line 7 (of course, this works with any number).\nTo search for a term, press / followed by the term, then enter. You can then find subsequent/previous results with the n (for “next”) and N keys.\nQuit the pager with the q (for “quit”) key.\n\n\n\n\nYour turn:\n\n\nOpen the man page for the ls command.\nNavigate down a few pages, then navigate back up.\nSearch for the first 5 occurrences of the word “directory”.\nWhat does ls -r do?\nFinally, leave the pager.\n\n\nHelp pages can be accessed with:\n&lt;command&gt; --help\n\n\nYour turn:\n\nAccess the help of the ls command.\n\nTo know the nature of a command (e.g. shell built-in function, an alias that you created, or the path of an utility) run:\ncommand -V &lt;command&gt;\n\n\nYour turn:\n\nWhat is the nature of the pwd command?"
  },
  {
    "objectID": "bash/basics.html#keybindings",
    "href": "bash/basics.html#keybindings",
    "title": "Bash: the basics",
    "section": "Keybindings",
    "text": "Keybindings\nClear the terminal (command clear) with C-l (this means: press the Ctrl and L keys at the same time).\nNavigate command history with C-p and C-n (or up and down arrows).\nYou can auto-complete commands by pressing the tab key."
  },
  {
    "objectID": "bash/basics.html#comments",
    "href": "bash/basics.html#comments",
    "title": "Bash: the basics",
    "section": "Comments",
    "text": "Comments\nAnything to the left of # is ignored by the interpreter and is for human consumption only.\n# You can write full-line comments\n\npwd       # You can also write comments after commands\nComments are usually used in scripts to document what sections of the code do or add information for the reader of the script (which is often future you, so commenting your scripts will help you a lot later on)."
  },
  {
    "objectID": "bash/aliases.html",
    "href": "bash/aliases.html",
    "title": "Aliases",
    "section": "",
    "text": "Aliases are a convenient way to assign a custom command to a name. You can use new names or re-assign existing command names.\n\nalias myip=\"ip -json route get 8.8.8.8 | jq -r '.[].prefsrc'\"\nalias ls='ls -F'\nYou can retrieve the definition of an alias by running the alias command without argument. To remove an alias, use unalias.\nYou can use the non-aliased version of a command with \\ (e.g. \\ls)."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About this site",
    "section": "",
    "text": "This site contains Marie-Hélène Burle’s latest content.\nHer older training material can be found on the archived sites:"
  },
  {
    "objectID": "about.html#main-training-website",
    "href": "about.html#main-training-website",
    "title": "About this site",
    "section": "Main training website",
    "text": "Main training website\nThis is the mint (“mint is not training”) website.\nTo view all our training material, please visit our main training website."
  },
  {
    "objectID": "about.html#other-websites",
    "href": "about.html#other-websites",
    "title": "About this site",
    "section": "Other websites",
    "text": "Other websites\nIn addition, here are a few of our websites for various training events:\n\nAutumn School 2022\nTraining Modules 2022\nTraining Modules 2021\nSummer School 2020\nCoding Fundamentals for Humanists 2022 for the Digital Humanities Summer Institute\nCoding Fundamentals for Humanists 2021 for the Digital Humanities Summer Institute\nHSS Winter Series 2023\nHSS Winter Series 2022"
  },
  {
    "objectID": "bash/filesystem.html",
    "href": "bash/filesystem.html",
    "title": "The Unix filesystem",
    "section": "",
    "text": "Bash allows to give instructions to a Unix operating system. The first thing we need to know is how storage is organized on such as system."
  },
  {
    "objectID": "bash/filesystem.html#structure",
    "href": "bash/filesystem.html#structure",
    "title": "The Unix filesystem",
    "section": "Structure",
    "text": "Structure\nThe Unix filesystem is a rooted tree of directories. The root is denoted by /.\nSeveral directories exist under the root. Here are a few:\n\n/bin     This is where binaries are stored.\n/boot    There, you can find the files necessary for booting the system.\n/home    This directory contains all the users home directories.\n\nThese directories in turn can contain other directories. /home for instance contains the directories:\n\n/home/user001\n/home/user002\n/home/user003\n…\n\nThe home directory of each user contain many files and directories."
  },
  {
    "objectID": "bash/filesystem.html#navigation",
    "href": "bash/filesystem.html#navigation",
    "title": "The Unix filesystem",
    "section": "Navigation",
    "text": "Navigation\n\nWorking directory\nThe current working directory can be obtained with:\npwd       # print working directory\n\n\nYour turn:\n\nWhat is your current working directory?\n\n\n\nChanging directory\nTo navigate to another directory, you use cd (change directory) followed by the path of the directory.\n\nExample:\n\ncd /home\nBecause /home was the parent directory (one level above in the rooted tree) of our working directory, we could have also navigated there with cd .. — the two dots represent one level up (a single dot represents the working directory).\n\n\nYour turn:\n\n\nWhat do you think will happens if you run cd .. from /home?\nWhat do you think will happens if you run cd . from /home?\n\n\nFrom any location, you can always go back to your home directory (e.g. /home/user009) by running cd without argument. Alternatively, you can use cd ~. This is because ~ gets expanded by the shell into the path of your home. Finally, you can use cd $HOME. $HOME is an environment variable representing the path of your home.\n\n\nYour turn:\n\nTry using cd - (that’s the minus sign) a few times. What does this command do?\n\n\n\nAbsolute and relative paths\nAbsolute paths give the full path from the root (e.g. /bin, /home/user009/file).\nRelative paths give the path relative to the working directory (e.g. ../dir/file, dir)\n\n\nYour turn:\n\nIs ~ an absolute or relative path?\n\n\nIn the filesystem below, names with edges represent directories and names without represent files. The current working directory is /home/user001.\n\nWhat is the output of ls? (have a look at this section if you need a refresher)\nWhat is the output of ls ../..?\nThe output of ls /thesis/src is:\n\n\nls: cannot access ‘/thesis/src’: No such file or directory\n\n  Why?\n\nWhat are 2 ways to navigate to the results directory?\nFrom the results directory, what are 2 ways to print the content of the src directory?\n\n\n\n\n\n\n \n\ncluster\n\n   \n\nuser001–.bashrc\n\n   \n\nuser001–.bash_profile\n\n   \n\nuser001–thesis\n\n   \n\n/–bin\n\n   \n\n/–boot\n\n   \n\n/–home\n\n   \n\nhome–user001\n\n   \n\nhome–user002\n\n   \n\nhome–user003\n\n   \n\nthesis–data\n\n   \n\nthesis–ms\n\n   \n\nthesis–results\n\n   \n\nthesis–src\n\n   \n\nresults–graph1\n\n   \n\nresults–graph2\n\n   \n\nsrc–script1\n\n   \n\n.bashrc\n\n.bashrc   \n\n.bash_profile\n\n.bash_profile   \n\ngraph1\n\ngraph1   \n\ngraph2\n\ngraph2   \n\nscript1\n\nscript1   \n\nuser001\n\n user001   \n\n/\n\n /   \n\nbin\n\n bin   \n\nboot\n\n boot   \n\nhome\n\n home   \n\nuser002\n\n user002   \n\nuser003\n\n user003   \n\nthesis\n\n thesis   \n\ndata\n\n data   \n\nms\n\n ms   \n\nresults\n\n results   \n\nsrc\n\n src"
  },
  {
    "objectID": "bash/filesystem.html#creating-files-and-directories",
    "href": "bash/filesystem.html#creating-files-and-directories",
    "title": "The Unix filesystem",
    "section": "Creating files and directories",
    "text": "Creating files and directories\nFiles can be created with a text editor:\nnano newfile.txt\n\nThe file actually gets created when you save it from within the text editor.\n\nor with the command touch:\ntouch newfile.txt\n\nThis creates an empty file.\n\ntouch can create multiple files at once:\ntouch file1 file2 file3\nNew directories can be created with mkdir. This command can also accept multiple arguments to create multiple directories at once:\nmkdir dir1 dir2"
  },
  {
    "objectID": "bash/filesystem.html#deleting",
    "href": "bash/filesystem.html#deleting",
    "title": "The Unix filesystem",
    "section": "Deleting",
    "text": "Deleting\nFiles can be deleted with rm.\nDirectories can be deleted with rm -r (recursive) or, if they are empty, with rmdir.\nBe careful that these commands are irreversible."
  },
  {
    "objectID": "bash/filesystem.html#copying-moving-and-renaming",
    "href": "bash/filesystem.html#copying-moving-and-renaming",
    "title": "The Unix filesystem",
    "section": "Copying, moving, and renaming",
    "text": "Copying, moving, and renaming\nCopying is done with the cp command.\n\nExample:\n\ncp thesis/src/script1 thesis/ms\nMoving and renaming are both done with the mv command.\n\nExamples:\n\n# rename script1 to script\nmv thesis/src/script1 thesis/src/script\n\n# move graph1 to the ms directory\nmv thesis/results/graph1 thesis/ms\n# this also works:\n# mv thesis/results/graph1 thesis/ms/graph1\n\n\nYour turn:\n\nWhy is there only one command to move and rename?"
  },
  {
    "objectID": "bash/index.html",
    "href": "bash/index.html",
    "title": "Bash",
    "section": "",
    "text": "Getting started with Bash\nAn introductory course to the Unix shell\n\n\n\n \n\n\n\n\n \n\n\n \n\n\n\n\n\nWorkshops\nWorkshops on various Bash topics\n\n\n\n\nWebinars\n60 min webinars on various Bash topics"
  },
  {
    "objectID": "bash/introduction.html",
    "href": "bash/introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "This section explains what Bash is, why it is useful to know how to use it, and how we are going to run it in this course."
  },
  {
    "objectID": "bash/introduction.html#unix-shells",
    "href": "bash/introduction.html#unix-shells",
    "title": "Introduction",
    "section": "Unix shells",
    "text": "Unix shells\nUnix shells are command line interpreters for Unix-like operating systems1: the user enters commands as text—interactively in a terminal or in scripts—and the shell passes them to the operating system.1 Unix-like systems include Linux, macOS, and a few others.\nIt is thus a way to give instructions to the machine through text instead of using a graphical user interface (GUI).\n\nTypes of Unix shells\nBash (Bourne Again SHell)—released in 1989—is part of the GNU Project and is the default Unix shell on many systems (although macOS recently changed its default to Zsh).\nPrior to Bash, the default was the Bourne shell (sh).\nA new and popular shell (backward compatible with Bash) is Zsh (zsh). It extends Bash’s capabilities.\nAnother shell in the same family is the KornShell (ksh).\nBash is the most common shell and the one which makes the most sense to learn as a first Unix shell. It is also the one used by default on the Alliance clusters.\n\n\nWhy use a shell?\nWhile automating GUI operations is really difficult, it is easy to rerun a script (a file with a number of commands). Unix shells thus allow the creation of reproducible workflows and the automation of repetitive tasks.\nThey are powerful to launch tools, modify files, search text, or combine commands.\nThey also allow to work on remote machines and HPC systems."
  },
  {
    "objectID": "bash/introduction.html#running-bash",
    "href": "bash/introduction.html#running-bash",
    "title": "Introduction",
    "section": "Running Bash",
    "text": "Running Bash\nSince Bash is a Unix shell, you need a Unix or Unix-like operating system. This means that people on Linux or MacOS can use Bash directly on their machine.\nFor Windows users, there are various options:\n\nusing Windows Subsystem for Linux (WSL),\nusing a Bash emulator (e.g. Git BASH), but those only have a subset of the usual Bash utilities,\nusing a Unix-like environment for Windows (e.g. Cygwin),\nusing a Unix Virtual machine,\naccessing a remote Unix machine.\n\n\nHow we will use Bash today\nToday, we will connect to a remote HPC system via SSH (secure shell). HPC systems always run Linux.\nThose on Linux or MacOS can alternatively use Bash directly on their machine.\n\nOn MacOS, the default is now Zsh (you can see that by typing echo $SHELL in Terminal), but Zsh is fully compatible with Bash commands, so it is totally fine to use it instead. If you really want to use Bash, simply launch it by typing in Terminal: bash."
  },
  {
    "objectID": "bash/introduction.html#connecting-to-a-remote-hpc-system-via-ssh",
    "href": "bash/introduction.html#connecting-to-a-remote-hpc-system-via-ssh",
    "title": "Introduction",
    "section": "Connecting to a remote HPC system via SSH",
    "text": "Connecting to a remote HPC system via SSH\n\nUsernames and password\nWe will give you a link to an etherpad during the workshop. Add your name next to a free username to claim it.\nWe will also give you the password for our training cluster. When prompted, enter it.\n\nNote that you will not see any character as you type the password: this is called blind typing and is a Linux safety feature. Type slowly and make sure not to make typos. It can be unsettling at first not to get any feed-back while typing.\n\n\n\nLinux and MacOS users\nLinux users: open the terminal emulator of your choice.\nMacOS users: open “Terminal”.\nThen type:\nssh userxxx@hostname\n\n\nReplace userxxx by your username (e.g. user009)\nReplace hostname by the hostname we will give you the day of the workshop.\n\n\n\n\nWindows users\nWe suggest using the free version of MobaXterm. MobaXterm comes with a terminal emulator and a GUI interface for SSH sessions.\nOpen MobaXterm, click on “Session”, then “SSH”, and fill in the Remote host name and your username.\n\nHere is a live demo."
  },
  {
    "objectID": "bash/molecules/find.html#data-for-this-section",
    "href": "bash/molecules/find.html#data-for-this-section",
    "title": "Finding files",
    "section": "Data for this section",
    "text": "Data for this section\nFor this section, we will play with files created by The Carpentries.\nYou can download them into a zip file called bash.zip with:\nwget http://bit.ly/bashfile -O bash.zip\nYou can then unzip that file with:\nunzip bash.zip\nFinally, you can delete the zip file:\nrm bash.zip\nYou should now have a data-shell directory with a molecules subdirectory.\ncd into it:\ncd data-shell/molecules"
  },
  {
    "objectID": "bash/molecules/find.html#command-find",
    "href": "bash/molecules/find.html#command-find",
    "title": "Finding files",
    "section": "Command find",
    "text": "Command find\nSearch for files inside the current working directory:\nfind . -type f\n./methane.pdb\n./pentane.pdb\n./sorted.txt\n./propane.pdb\n./lengths.txt\n./cubane.pdb\n./ethane.pdb\n./octane.pdb\nfind . -type d will instead search for directories inside the current working directory.\nHere are other examples:\nfind . -maxdepth 1 -type f     # depth 1 is the current directory\nfind . -mindepth 2 -type f     # current directory and one level down\nfind . -name haiku.txt      # finds specific file\nls data       # shows one.txt two.txt\nfind . -name *.txt      # still finds one file -- why? answer: expands *.txt to haiku.txt\nfind . -name '*.txt'    # finds all three files -- good!\nLet’s wrap the last command into $()—called command substitution—as if it were a variable:\necho $(find . -name '*.txt')   # will print ./data/one.txt ./data/two.txt ./haiku.txt\nls -l $(find . -name '*.txt')   # will expand to ls -l ./data/one.txt ./data/two.txt ./haiku.txt\nwc -l $(find . -name '*.txt')   # will expand to wc -l ./data/one.txt ./data/two.txt ./haiku.txt\ngrep elegant $(find . -name '*.txt')   # will look for 'elegant' inside all *.txt files\n\n\nYour turn:\n\ngrep’s -v flag inverts pattern matching, so that only lines that do not match the pattern are printed.\nGiven that, which of the following commands will find all files in /data whose names end in ose.dat (e.g. sucrose.dat or maltose.dat), but do not contain the word temp?\n\nfind /data -name '*.dat' | grep ose | grep -v temp\nfind /data -name ose.dat | grep -v temp\ngrep -v temp $(find /data -name '*ose.dat')\nNone of the above\n\n\nHere is a video of a previous version of this workshop."
  },
  {
    "objectID": "bash/molecules/find.html#running-a-command-on-the-results-of-find",
    "href": "bash/molecules/find.html#running-a-command-on-the-results-of-find",
    "title": "Finding files",
    "section": "Running a command on the results of find",
    "text": "Running a command on the results of find\nLet’s say that you want to run a command on each of the files in the output of find. You can always do something using command substitution like this:\nfor f in $(find . -name \"*.txt\")\ndo\n    command on $f\ndone\nAlternatively, you can make it a one-liner:\nfind . -name \"*.txt\" -exec command {} \\;\nAnother—perhaps more elegant—one-line alternative is to use xargs. In its simplest usage, xargs command lets you construct a list of arguments:\nfind . -name \"*.txt\"                   # returns multiple lines\nfind . -name \"*.txt\" | xargs           # use those lines to construct a list\nfind . -name \"*.txt\" | xargs command   # pass this list as arguments to `command`\ncommand $(find . -name \"*.txt\")        # command substitution, achieving the same result (this is riskier!)\ncommand `(find . -name \"*.txt\")`       # alternative syntax for command substitution\nIn these examples, xargs achieves the same result as command substitution, but it is safer in terms of memory usage and the length of lists you can pass.\nWhen would you need to use this? A good example is with the command grep. grep takes a search stream (and not a list of files) as its standard input:\ncat filename | grep pattern\nTo pass a list of files to grep, you can use xargs that takes that list from its standard input and converts it into a list of arguments that is then passed to grep:\nfind . -name \"*.txt\" | xargs grep pattern   # search for `pattern` inside all those files (`grep` does not take a list of files as standard input)\n\n\nHere is a video of a previous version of this workshop."
  },
  {
    "objectID": "bash/molecules/script.html",
    "href": "bash/molecules/script.html",
    "title": "Writing scripts",
    "section": "",
    "text": "There are series of commands that you need to run regularly. Instead of having to type them each time, you can write them in a text file (called a script) with a .sh extension and execute that file whenever you want to run that set of commands. This is a great way to automate work.\nThis section covers scripts syntax and execution."
  },
  {
    "objectID": "bash/molecules/script.html#writing-and-executing-scripts",
    "href": "bash/molecules/script.html#writing-and-executing-scripts",
    "title": "Writing scripts",
    "section": "Writing and executing scripts",
    "text": "Writing and executing scripts\n\nScripts as arguments to bash\nA shell script is simply a text file. You can create it with a text editor such as nano which is installed on most systems.\nLet’s try to create one that we will call test.sh:\nnano test.sh\nIn the file, write the command: echo This is my first script.\nThis is the content of our test.sh file:\n\n\ntest.sh\n\necho This is my first script\n\nNow, how do we run this?\nWe simply pass it as an argument to the bash command:\nbash test.sh\nThis is my first script\nAnd it worked!\n\n\nShebang\nThere is another way to write and execute scripts: we can use a shebang.\nA shebang consists of the characters #! followed by the path of an executable. Here, the executable we want is bash and its path is /bin/bash.\nSo our script becomes:\n\n\ntest.sh\n\n#!/bin/bash\n\necho This is my first script.\n\nNow, the cool thing about this is that we don’t need to pass the script as an argument of the bash command anymore since the information that this should be executed by Bash is already written in the shebang. Instead, we can execute it with ./test.sh.\nBut there is a little twist:\n./test.sh\nbash: ./test.sh: Permission denied\nWe first need to make the file executable by changing its permissions.\n\n\nUnix permissions\nUnix systems such as Linux use POSIX permissions.\nTo add an executable permission to a file, you need to run:\nchmod u+x test.sh\nNow that our script is executable, we can run:\n./test.sh\nThis is my first script\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere and here are two videos of a previous version of this workshop."
  },
  {
    "objectID": "bash/molecules/script.html#using-other-computing-languages-in-bash",
    "href": "bash/molecules/script.html#using-other-computing-languages-in-bash",
    "title": "Writing scripts",
    "section": "Using other computing languages in Bash",
    "text": "Using other computing languages in Bash\nIt is possible to incorporate scripts written in other computing languages into your bash code.\n\nExample:\n\nfunction test() {\n    randomFile=${RANDOM}${RANDOM}.py\n    cat &lt;&lt; EOF &gt; $randomFile\n#!/usr/bin/python3\nprint(\"do something in Python\")\nEOF\n    chmod u+x $randomFile\n    ./$randomFile\n    /bin/rm $randomFile\n}\n\nEOF is a random delimiter string and &lt;&lt; tells Bash to wait for that delimiter to end the input.\nHere is an example of this syntax:\ncat &lt;&lt; the_end\nThis text\nwill be printed\nin the terminal.\nthe_end"
  },
  {
    "objectID": "bash/quotes.html",
    "href": "bash/quotes.html",
    "title": "Quotes",
    "section": "",
    "text": "Let’s experiment with quotes:\n\nvariable=This string is the value of the variable\necho $variable\n\nbash: line 1: string: command not found\n\n\nOops…\n\nvariable=\"This string is the value of the variable\"\necho $variable\n\nThis string is the value of the variable\n\n\n\nvariable='This string is the value of the variable'\necho $variable\n\nThis string is the value of the variable\n\n\n\nvariable='This string's the value of the variable'\necho $variable\n\nbash: -c: line 1: unexpected EOF while looking for matching `''\nbash: -c: line 3: syntax error: unexpected end of file\n\n\nOops…\nOne solution to this is to use double quotes:\n\nvariable=\"This string's the value of the variable\"\necho $variable\n\nThis string's the value of the variable\n\n\nAlternatively, single quotes can be escaped:\n\nvariable='This string'\"'\"'s the value of the variable'\necho $variable\n\nThis string's the value of the variable\n\n\n\nAdmittedly, this last one is a little crazy. It is the way to escape single quotes in single-quoted strings.\nThe first ' ends the first string, both \" create a double-quoted string with ' (escaped) in it, then the last ' starts the second string.\nEscaping double quotes is a lot easier and simply requires \\\"."
  },
  {
    "objectID": "bash/special_parameters.html",
    "href": "bash/special_parameters.html",
    "title": "Special parameters",
    "section": "",
    "text": "A number of special parameters, all starting with $, get expanded by Bash.\n\n\n$1, $2, $3, … are positional special characters\n$@ is an array-like construct referring of all positional parameters\n$# expands to the number of arguments\n$$ pid of the current shell\n$! expands to the PID of the most recent background command\n$0 expands to the name of the current shell or script"
  },
  {
    "objectID": "bash/tools1.html",
    "href": "bash/tools1.html",
    "title": "Fun tools to simplify your life in the shell",
    "section": "",
    "text": "Working in the command-line has many advantages and it is often necessary, but can it be fun?\nIn this webinar, aimed at any command-line user, I intend to demonstrate that yes, it can! by introducing three free and open source utilities which make navigating your system and your outputs a lot easier:\n\nfzf is a simple, yet extremely powerful interactive fuzzy finder allowing for incremental completion and narrowing selection of any command line output. I will show you how to build simple shell functions which harvest its power to instantly refresh your memory on your custom keybindings or aliases, navigate your command history, find and kill processes, and explore and checkout your git commits. After this, you will be able to use fzf for any number of other applications in your work in the command-line.\nautojump lets you jump anywhere you want in your directories in just a few keystrokes (no more of this painful navigation writing down long paths).\nWith the ranger file manager, you can browse (with preview!), open, copy, move, delete, etc. your files and directories in a friendly way from the command line. Added bonus: you can use fzf and autojump within ranger!\n\nWarning: too much fun in the command-line can lead to addiction and geek behaviours. Use in moderation."
  },
  {
    "objectID": "bash/top_intro.html",
    "href": "bash/top_intro.html",
    "title": "Getting started with Bash",
    "section": "",
    "text": "This course is a hands-on introduction to the Linux command line and the interaction with a remote server. It will cover common utilities, loops, redirections, functions, wild cards, Bash scripting, and the basics of working through secure shell.\n\n Start course ➤"
  },
  {
    "objectID": "bash/variables.html#declaring-variables",
    "href": "bash/variables.html#declaring-variables",
    "title": "Variables",
    "section": "Declaring variables",
    "text": "Declaring variables\nYou declare a variable (i.e. a name that holds a value) with the = sign:\nvar=value\n\nMake sure not to put spaces around the equal sign.\n\n\nExample:\n\n\nvar=5\n\nYou can delete a variable with:\n\nunset var"
  },
  {
    "objectID": "bash/variables.html#expanding-variables",
    "href": "bash/variables.html#expanding-variables",
    "title": "Variables",
    "section": "Expanding variables",
    "text": "Expanding variables\nTo expand a variable (to access its value), you need to prepend its name with $.\n\nThis is not what we want:\n\n\nvar=value\necho var\n\nvar\n\n\n\nThis however works:\n\n\nvar=value\necho $var\n\nvalue"
  },
  {
    "objectID": "bash/variables.html#quotes",
    "href": "bash/variables.html#quotes",
    "title": "Variables",
    "section": "Quotes",
    "text": "Quotes\n\nWhen declaring\nQuotes are necessary for values containing special characters such as spaces.\n\nThis doesn’t work:\n\n\nvar=string with spaces\necho $var\n\nbash: line 1: with: command not found\n\n\n\nThis works:\n\n\nvar=\"string with spaces\"\necho $var\n\nstring with spaces\n\n\n\nThis also works:\n\n\nvar='string with spaces'\necho $var\n\nstring with spaces\n\n\nWhen declaring variables, single and double quotes are equivalent. Which one should you use then? Use the one that is most convenient.\n\nNot good:\n\n\nvar='that's a string with spaces'\necho $var\n\nbash: -c: line 1: unexpected EOF while looking for matching `''\nbash: -c: line 3: syntax error: unexpected end of file\n\n\n\nGood:\n\n\nvar=\"that's a string with spaces\"\necho $var\n\nthat's a string with spaces\n\n\n\nAlternatively, single quotes can be escaped, but it is a little crazy: the first ' ends the first string, both \" create a double-quoted string with ' (escaped) in it, then the last ' starts the second string.\n\nvar='that'\"'\"'s a string with spaces'\necho $var\n\nthat's a string with spaces\n\n\n\n\nConversely, this is not good:\n\n\nvar=\"he said: \"string with spaces\"\"\necho $var\n\nbash: line 1: with: command not found\n\n\n\nWhile this works:\n\n\nvar='he said: \"string with spaces\"'\necho $var\n\nhe said: \"string with spaces\"\n\n\n\nDouble quotes as well can be escaped (simply by prepending them with \\):\n\nvar=\"he said: \\\"string with spaces\\\"\"\necho $var\n\nhe said: \"string with spaces\"\n\n\n\n\n\nWhen expanding\nWhile not necessary in many situations, it is safer to expand variables in double quotes, in case the expansion leads to problematic special characters. In the example above, this was not problematic and using $var or \"$var\" are the same.\nIn the following example however, it is problematic:\nvar=\"string with spaces\"\ntouch $var\nThis creates 3 files called string, with, and spaces. Probably not what you want.\nThe following creates a single file called string with spaces:\nvar=\"string with spaces\"\ntouch \"$var\"\n\nTo be safe, it is thus a good habit to quote expanded variables.\n\nIt is important to note however that single quotes don’t expand variables (only double quotes do).\nThe following would thus create a file called $var:\nvar=\"string with spaces\"\ntouch '$var'"
  },
  {
    "objectID": "bash/variables.html#exporting-variables",
    "href": "bash/variables.html#exporting-variables",
    "title": "Variables",
    "section": "Exporting variables",
    "text": "Exporting variables\nUsing export ensures that all inherited processes of this shell also have access to this variable:\nexport var=3"
  },
  {
    "objectID": "bash/variables.html#string-manipulation",
    "href": "bash/variables.html#string-manipulation",
    "title": "Variables",
    "section": "String manipulation",
    "text": "String manipulation\n\nGetting a subset\n\nvar=\"hello\"\necho ${var:2}      # Print from character 2\necho ${var:2:1}    # Print 1 character from character 2\n\nllo\nl\n\n\n\n\nSearch and replace\n\nvar=\"hello\"\necho ${var/l/L}    # Replace the first match of l by L\necho ${var//l/L}   # Replace all matches of l by L\n\nheLlo\nheLLo\n\n\n\n\nString concatenation\nIf you want to concatenate the expanded variable with another string, you need to use curly braces or quotes.\n\nThis does not return anything because there is no variable called varshine:\n\n\nvar=sun\necho $varshine\n\n\nThese two syntaxes do work:\n\n\nvar=sun\necho ${var}shine\necho \"$var\"shine\n\nsunshine\nsunshine"
  },
  {
    "objectID": "bash/variables.html#environment-variables",
    "href": "bash/variables.html#environment-variables",
    "title": "Variables",
    "section": "Environment variables",
    "text": "Environment variables\nEnvironment variables help control the behaviour of processes on a machine. You can think of them as customizations of your system.\nMany are set automatically.\n\nExample:\n\necho $HOME\n/home/user009\nThere are many other environment variables (e.g. PATH, PWD, PS1). To see the list, you can run printenv or env.\nIf you want to add new environment variables, you can add them to your ~/.bashrc file which is sourced each time you start a new shell.\nHere is a video of a previous version of this workshop."
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact",
    "section": "",
    "text": "Please email us at: training at westdri dot ca."
  },
  {
    "objectID": "git/branches.html",
    "href": "git/branches.html",
    "title": "Branches",
    "section": "",
    "text": "One of the reasons Git has become so popular is its branching system: unlike in other version control tools in which creating branches is a lengthy and expensive process involving heavy copies, a branch in Git is just a lightweight pointer to a commit. This makes creating branches extremely quick and cheap."
  },
  {
    "objectID": "git/branches.html#what-is-a-branch",
    "href": "git/branches.html#what-is-a-branch",
    "title": "Branches",
    "section": "What is a branch?",
    "text": "What is a branch?\nA branch is a pointer to a commit (under the hood, it is a small file containing the 40 character hash checksum of the commit it points to).\nRemember that little pointer called main? That’s our main branch: the one Git creates automatically when we create our first commit.\nWhen you run git status and get “On branch main” in the output, or when you run git log and see “(HEAD -&gt; main)” in the log, it means that the HEAD pointer (your position in the Git history) points to the branch main (which itself points to a commit).\n\nI know that is a lot of pointers … but this is really what makes Git so nimble, powerful, and fantastic. Because these pointers are very cheap (tiny files) and so useful."
  },
  {
    "objectID": "git/branches.html#why-use-multiple-branches",
    "href": "git/branches.html#why-use-multiple-branches",
    "title": "Branches",
    "section": "Why use multiple branches?",
    "text": "Why use multiple branches?\nBranches are useful in so many situations:\n\nIf your changes break code, you still have a fully functional branch to go back to if needed.\nIf you develop a tool being used, this allows you to experiment with new features until they are ready without messing up with the working project.\nYou can create a branch for each alternative approach. This allows you to jump back and forth between various alternatives.\nYou can work on different aspects of the project on different branches. This prevents having messy incomplete work all over the place on the same branch.\nIf you want to revisit an old commit, you can create a branch there and switch to it instead of moving HEAD (creating a detached HEAD situation). This way, if you decide to create new commits from that old one, you don’t risk loosing them.\nBranches are great for collaboration: each person can work on their own branch and merge it back to the main branch when they are done with one section of a project.\n\nAnd since branches are so cheap to create, there is no downside to their creation."
  },
  {
    "objectID": "git/branches.html#creating-branches-and-switching-between-branches",
    "href": "git/branches.html#creating-branches-and-switching-between-branches",
    "title": "Branches",
    "section": "Creating branches and switching between branches",
    "text": "Creating branches and switching between branches\nYou can create a new branch with:\ngit branch &lt;new-branch-name&gt;\n\nExample:\n\ngit branch test\n\nand you can then switch to it with:\ngit switch &lt;new-branch-name&gt;\n\nExample:\n\ngit switch test\n\nAlternatively, you can do both at once with the convenient:\ngit switch -c &lt;new-branch-name&gt;\n\n-c flag for “create”. So you create a branch and switch to it directly.\n\nI find this last command most useful as it is all too easy otherwise to create a new branch, forget to switch to it, and create commits on the wrong branch …"
  },
  {
    "objectID": "git/branches.html#listing-branches",
    "href": "git/branches.html#listing-branches",
    "title": "Branches",
    "section": "Listing branches",
    "text": "Listing branches\ngit branch\n  main\n* test\nThe * shows the branch you are currently on (i.e. the branch to which HEAD points to). In our example, the project has two branches and we are on the branch test."
  },
  {
    "objectID": "git/branches.html#comparing-branches",
    "href": "git/branches.html#comparing-branches",
    "title": "Branches",
    "section": "Comparing branches",
    "text": "Comparing branches\nYou can use git diff to compare branches:\ngit diff main test\nThis shows all the lines that have been modified (added or deleted) between the commits both branches point to."
  },
  {
    "objectID": "git/branches.html#merging-branches",
    "href": "git/branches.html#merging-branches",
    "title": "Branches",
    "section": "Merging branches",
    "text": "Merging branches\nWhen you are happy with the changes you made on your test branch, you can merge it into main.\n\nFast-forward merge\nIf you have only created new commits on the branch test, the merge is called a “fast-forward merge” because main and test have not diverged: it is simply a question of having main catch up to test.\n\nFirst, you switch to main:\ngit switch main\n\nThen you do the fast-forward merge:\ngit merge test\n\nThen, usually, you delete the branch test as it has served its purpose:\ngit branch -d test\n\nAlternatively, you can switch back to test and do the next bit of experimental work on it. This allows to keep main free of mishaps and bad developments.\n\n\nThree-way merge\nIf the branches have diverged (you created commits on both branches), the merge requires the creation of an additional commit called a “merge commit”.\nLet’s go back to our situation before we created the branch test:\n\nThis time, you create a branch called test2:\n\nand you switch to it:\n\nThen you create some commits:\n\n\nNow you switch back to main:\n\nAnd you create commits from main too:\n\n\nTo merge your branch test2 into main, a new commit is now required. Git will create this new commit automatically. As long as there is no conflict, it is just as easy as a fast-forward merge:\ngit merge test2\n\nAfter which, you can delete the (now useless) test branch (with git branch -d test2):"
  },
  {
    "objectID": "git/branches.html#resolving-conflicts",
    "href": "git/branches.html#resolving-conflicts",
    "title": "Branches",
    "section": "Resolving conflicts",
    "text": "Resolving conflicts\nGit works line by line. As long as you aren’t working on the same line(s) of the same file(s) on different branches, there will not be any merging difficulty. If however you modified one or more of the same line(s) of the same file(s) on different branches, Git has no way to decide which version should be kept and will thus not be able to complete the merge. It will then ask you to resolve the conflict(s). Conveniently, it will list the file(s) containing the conflict(s).\nThere are fancy tools to resolve conflicts, but you can do it in any text editor: simply open the file(s) listed by Git as having conflicts and look for the following markers:\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\nThis is your version.\n=======\nThis is the alternative version of the same section of the file.\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; alternative version\nIn our case, it could look something like:\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\nGreat sentence.\n=======\nGreat sentence with some variations.\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; test2\nThese markers are added by Git to signal the areas of conflict. It is up to you to choose between the two versions (or create a third one) and remove the conflict markers. After that, you can stage the file(s) which contained the conflicts to finish the merge (and then you can commit)."
  },
  {
    "objectID": "git/collab.html",
    "href": "git/collab.html",
    "title": "Collaborating through Git & GitHub",
    "section": "",
    "text": "Using Internet hosting services such as GitHub, Git is a powerful collaboration tool.\nIn this workshop, we will cover the three classic collaboration situations and see how a collaborative workflow works."
  },
  {
    "objectID": "git/collab.html#three-situations",
    "href": "git/collab.html#three-situations",
    "title": "Collaborating through Git & GitHub",
    "section": "Three situations",
    "text": "Three situations\nWhen you collaborate on a project through Git and a remote such as GitHub, there are three situations:\n\nyou create a project on your machine and want others to contribute to it (1),\nyou want to contribute to a project started by others and\n\nyou have write access to it (2),\nyou do not have write access to it (3).\n\n\n\n(1) You start the project\nIn this first situation, you are the author of a project (you have a project under version control on your own machine) and you want to initiate a collaboration with others on it using GitHub as a remote.\n\nCreate a remote on GitHub\nYou need to create a remote on GitHub.\n\nCreate a free GitHub account\nIf you don’t already have one, sign up for a free GitHub account.\n\nTo avoid having to type your password all the time, you should set up SSH for your account.\n\n\n\nCreate an empty repository on GitHub\n\nGo to the GitHub website, login, and go to your home page.\nLook for the Repositories tab & click the green New button.\nEnter the name you want for your repo, without spaces.\nMake the repository public or private.\n\n\n\nLink empty repository to your repo\nClick on the Code green drop-down button, select SSH if you have set SSH for your GitHub account or HTTPS and copy the address.\nIn the command line, cd inside your project, and add the remote:\ngit remote add &lt;remote-name&gt; &lt;remote-address&gt;\nremote-name is a convenience name to identify that remote. You can choose any name, but since Git automatically call the remote origin when you clone a repo, it is common practice to use origin as the name for the first remote.\n\nExample (using an SSH address):\n\ngit remote add origin git@github.com:&lt;user&gt;/&lt;repo&gt;.git\n\nExample (using an HTTPS address):\n\ngit remote add origin https://github.com/&lt;user&gt;/&lt;repo&gt;.git\nIf you don’t want to grant others write access to the project, and you only accept contributions through pull requests, you are set.\nIf you want to grant your collaborators write access to the project however, you need to add them to it.\n\n\n\nInvite collaborators\n\nGo to your GitHub project page.\nClick on the Settings tab.\nClick on the Manage access section on the left-hand side (you will be prompted for your GitHub password).\nClick on the Invite a collaborator green button.\nInvite your collaborators with one of their GitHub user name, their email address, or their full name.\n\n\n\n\n(2) Write access to project\nIn this second situation, someone else started a project and they are inviting you to collaborate to it, giving you write access to the project.\nIn this case, you need to clone the project: cd to the location where you want your local copy, then:\ngit clone &lt;remote-address&gt; &lt;local-name&gt;\nThis sets the project as a remote to your new local copy and that remote is automatically called origin.\nWithout &lt;local-name&gt;, the repo will have the name of the last part of the remote address.\n\n\n(3) No write access to project\nIn this third situation, someone else started a project and you want to collaborate to it, but you do not have write access to it.\nIn this case, you will have to submit pull requests.\nHere is the workflow for a pull request (PR):\n\nFork the project on GitHub.\nClone your fork on your machine.\nAdd the initial project as a second remote & call it upstream.\nPull from upstream to update your local project.\nCreate & checkout a new branch.\nMake & commit your changes on that branch.\nPush that branch to your fork (i.e. origin — remember that you do not have write access to upstream).\nGo to the original project GitHub’s page & open a pull request."
  },
  {
    "objectID": "git/collab.html#collaborative-workflow",
    "href": "git/collab.html#collaborative-workflow",
    "title": "Collaborating through Git & GitHub",
    "section": "Collaborative workflow",
    "text": "Collaborative workflow\n\nPulling and pushing\nWhen you collaborate with others using GitHub (or other remote), you and others will work simultaneously on some project. How does this work?\nTo upload your changes to the remote on GitHub you push to it with git push.\nIf one of your collaborators has made changes to the remote (pushing from their own local version of the project), you won’t be able to push. Instead, you will get the following message:\nTo xxx.git\n ! [rejected]        main -&gt; main (fetch first)\nerror: failed to push some refs to 'xxx.git'\nhint: Updates were rejected because the remote contains work that you do\nhint: not have locally. This is usually caused by another repository pushing\nhint: to the same ref. You may want to first integrate the remote changes\nhint: (e.g., 'git pull ...') before pushing again.\nhint: See the 'Note about fast-forwards' in 'git push --help' for details.\nThe solution?\nYou first have to download (git pull) their work onto your machine, merge it with yours (which will happen automatically if there are no conflicts), before you can push your work to GitHub.\nNow… what if there are conflicts?\n\n\nResolving conflicts\n\n\nFrom crystallize.com\n\nGit works line by line. As long as your collaborators and you aren’t working on the same line(s) of the same file(s) at the same time, there will not be any problem. If however you modified one or more of the same line(s) of the same file(s), Git will not be able to decide which version should be kept. When you git pull their work on your machine, the automatic merging will get interrupted and Git will ask you to resolve the conflict(s) before the merge can resume. It will conveniently tell you which file(s) contain the conflict(s).\nThere are fancy tools to resolve conflicts, but you can do it in any text editor: simply open the file(s) listed by Git as having conflicts and look for the following markers:\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\nThis is your version.\n=======\nThis is the alternative version of the same section of the file.\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; alternative version\nThese markers are added by Git to signal the areas of conflict. It is up to you to choose between the two versions (or create a third one) and remove the conflict markers. After that, you can stage the file(s) which contained the conflicts to finish the merge (and then you can commit)."
  },
  {
    "objectID": "git/documentation.html",
    "href": "git/documentation.html",
    "title": "Documentation",
    "section": "",
    "text": "Git comes with internal documentation. This section covers how to access it."
  },
  {
    "objectID": "git/documentation.html#man-pages",
    "href": "git/documentation.html#man-pages",
    "title": "Documentation",
    "section": "Man pages",
    "text": "Man pages\ngit &lt;command&gt; --help\ngit help &lt;command&gt;\nman git-&lt;command&gt;\n\nExample:\n\ngit commit --help\ngit help commit\nman git-commit\n\nUseful keybindings when you are in the pager\nSPACE      scroll one screen down\nb          scroll one screen up\nq          quit"
  },
  {
    "objectID": "git/documentation.html#command-options",
    "href": "git/documentation.html#command-options",
    "title": "Documentation",
    "section": "Command options",
    "text": "Command options\ngit &lt;command&gt; -h\n\nExample:\n\ngit commit -h"
  },
  {
    "objectID": "git/ignore.html",
    "href": "git/ignore.html",
    "title": "Excluding from version control",
    "section": "",
    "text": "Not everything should be under version control, yet we don’t want a cluttered working directory. The solution: a list of files or patterns that Git disregards."
  },
  {
    "objectID": "git/ignore.html#what-to-exclude",
    "href": "git/ignore.html#what-to-exclude",
    "title": "Excluding from version control",
    "section": "What to exclude",
    "text": "What to exclude\nThere are files you really want to put under version control, but there are files you shouldn’t.\nPut under version control:\n\nScripts\nManuscripts and notes\nMakefile & similar\n\nDo NOT put under version control:\n\nNon-text files (e.g. images, office documents)\nYour initial data\nOutputs that can be recreated by running code (e.g. graphs, results)\n\nHowever, you don’t want to have such documents constantly showing up when you run git status. In order to have a clean working directory while keeping them out of version control, you can create a file called .gitignore and add to it a list of files or patterns that you want Git to disregard.\n\n\nYour turn:\n\nIn the case of our mock project,\n\nwhat should we put under version control?\nwhat should we ignore?"
  },
  {
    "objectID": "git/ignore.html#how-to-exclude",
    "href": "git/ignore.html#how-to-exclude",
    "title": "Excluding from version control",
    "section": "How to exclude",
    "text": "How to exclude\n\nThe .gitignore file\nTo exclude files from version control, create a file called .gitignore in the root of your project and add those files to it, one per line.\n\nExample:\n\n# Create .gitignore and add 'graph.png' to it\necho graph.png &gt; .gitignore\n\n# `&gt;` would overwrite the content. `&gt;&gt;` appends\necho output.txt &gt;&gt; .gitignore\nYou can also ignore entire directories.\n\nExample:\n\necho /results/ &gt;&gt; .gitignore\nFinally, you can use globbing patterns to ignore all files matching a certain pattern.\n\nExample:\n\n# Exclude all .png files\necho *.png &gt;&gt; .gitignore\n\n.gitignore syntax\nEach line in a .gitignore file specifies a pattern.\nBlank lines are ignored and can serve as separators for readability.\nLines starting with # are comments.\nTo add patterns starting with a special character (e.g. #, !), that character needs to be escaped with \\.\nTrailing spaces are ignored unless they are escaped with \\.\n! negates patterns.\nPatterns ending with / match directories. Otherwise patterns match both files and directories.\n/ at the beginning or within a search pattern indicates that the pattern is relative to the directory level of the .gitignore file (usually the root of the project). Otherwise the pattern matches anywhere below the .gitignore level.\n\nExamples:\n/foo/bar/ matches the directory foo/bar, but not the directory a/foo/bar\nfoo/bar/ matches both the directories foo/bar and a/foo/bar\n\n* matches anything except /.\n? matches any one character except /.\nThe range notation (e.g. [a-zA-Z]) can be used to match one of the characters in a range.\nA leading **/ matches all directories.\n\nExample:\n**/foo matches file or directory foo anywhere. This is the same as foo.\n\nA trailing /** matches everything inside what it precedes.\n\nExample:\nabc/** matches all files (recursively) inside directory abc\n\n/**/ matches zero or more directories.\n\nExample:\na/**/b matches a/b, a/x/b, and a/x/y/b\n\n\n\n\nYour turn:\n\nCreate a .gitignore file suitable for our mock project.\n\nThe .gitignore is a file like any other file, so you’ll want to stage and commit it:\ngit status\nOn branch main\nUntracked files:\n  (use \"git add &lt;file&gt;...\" to include in what will be committed)\n    .gitignore\n    src/\n\nnothing added to commit but untracked files present (use \"git add\" to track)\n\nNotice how data/ is not listed in the untracked files anymore.\n\nWe stage our .gitignore file:\ngit add .gitignore\ngit status\nOn branch main\nChanges to be committed:\n  (use \"git restore --staged &lt;file&gt;...\" to unstage)\n    new file:   .gitignore\n\nUntracked files:\n  (use \"git add &lt;file&gt;...\" to include in what will be committed)\n    src/\nAnd we create a new commit:\ngit commit -m \"Add .gitignore file with data and results\"\ngit status\n[main a1df8e5] Add .gitignore file with data and results\n 1 file changed, 2 insertions(+)\n create mode 100644 .gitignore\ngit status\nOn branch main\nUntracked files:\n  (use \"git add &lt;file&gt;...\" to include in what will be committed)\n    src/\n\nnothing added to commit but untracked files present (use \"git add\" to track)\nLet’s create a third commit with the Python script:\ngit add src/script.py\ngit commit -m \"Add first draft of script\"\n[main ca3c036] Add first draft of script\n 1 file changed, 7 insertions(+)\n create mode 100644 src/script.py\ngit status\nOn branch main\nnothing to commit, working tree clean\nWhat does “working tree clean” mean? In the next section, we will talk about the three file trees of Git."
  },
  {
    "objectID": "git/install.html",
    "href": "git/install.html",
    "title": "Installation and setup",
    "section": "",
    "text": "In this section, we will install and configure Git."
  },
  {
    "objectID": "git/install.html#installing-git",
    "href": "git/install.html#installing-git",
    "title": "Installation and setup",
    "section": "Installing Git",
    "text": "Installing Git\n\nMacOS/Linux users\nInstall Git from the official website.\n\n\nWindows users\nInstall Git for Windows. This will also install Git Bash, a Bash emulator."
  },
  {
    "objectID": "git/install.html#using-git",
    "href": "git/install.html#using-git",
    "title": "Installation and setup",
    "section": "Using Git",
    "text": "Using Git\nWe will use Git from the command line throughout this workshop.\nMacOS users:    open Terminal.\nWindows users:   open Git Bash.\nLinux users:    open the terminal emulator of your choice."
  },
  {
    "objectID": "git/install.html#configuring-git",
    "href": "git/install.html#configuring-git",
    "title": "Installation and setup",
    "section": "Configuring Git",
    "text": "Configuring Git\nBefore you can use Git, you need to set some basic configuration. You will do this in the terminal you just opened.\n\nList settings\ngit config --list\n\n\nUser identity\ngit config --global user.name \"&lt;Your Name&gt;\"\ngit config --global user.email \"&lt;your@email&gt;\"\n\nExample:\n\ngit config --global user.name \"John Doe\"\ngit config --global user.email \"john.doe@gmail.com\"\n\n\nText editor\ngit config --global core.editor \"&lt;text-editor&gt;\"\n\nExample for nano:\n\ngit config --global core.editor \"nano\"\n\n\nLine ending\n\nmacOS, Linux, or WSL\ngit config --global core.autocrlf input\n\n\nWindows\ngit config --global core.autocrlf true\n\n\n\nProject-specific configuration\nYou can also set project-specific configurations (e.g. maybe you want to use a different email address for a certain project).\nIn that case, navigate to your project and run the command without the --global flag.\n\nExample:\n\ncd /path/to/project\ngit config user.email \"your_other@email\""
  },
  {
    "objectID": "git/practice_repo/search.html",
    "href": "git/practice_repo/search.html",
    "title": "Searching a version-controlled project",
    "section": "",
    "text": "What is the point of creating all these commits if you are unable to make use of them because you can’t find the information you need in them?\nIn this workshop, we will learn how to search:\n\nyour files (at any of their versions) and\nyour commit logs.\n\nBy the end of the workshop, you should be able to retrieve anything you need from your versioned project."
  },
  {
    "objectID": "git/practice_repo/search.html#installation",
    "href": "git/practice_repo/search.html#installation",
    "title": "Searching a version-controlled project",
    "section": "Installation",
    "text": "Installation\nMacOS & Linux users:\nInstall Git from the official website.\nWindows users:\nInstall Git for Windows. This will also install “Git Bash”, a Bash emulator."
  },
  {
    "objectID": "git/practice_repo/search.html#using-git",
    "href": "git/practice_repo/search.html#using-git",
    "title": "Searching a version-controlled project",
    "section": "Using Git",
    "text": "Using Git\nWe will use Git from the command line throughout this workshop.\nMacOS users:    open “Terminal”.\nWindows users:   open “Git Bash”.\nLinux users:    open the terminal emulator of your choice."
  },
  {
    "objectID": "git/practice_repo/search.html#practice-repo",
    "href": "git/practice_repo/search.html#practice-repo",
    "title": "Searching a version-controlled project",
    "section": "Practice repo",
    "text": "Practice repo\n\nGet a repo\nYou are welcome to use a repository of yours to follow this workshop. Alternatively, you can clone a practice repo I have on GitHub:\n\nNavigate to an appropriate location:\n\ncd /path/to/appropriate/location\n\nClone the repo:\n\n# If you have set SSH for your GitHub account\ngit clone git@github.com:prosoitos/practice_repo.git\n# If you haven't set SSH\ngit clone https://github.com/prosoitos/practice_repo.git\n\nEnter the repo:\n\ncd practice_repo"
  },
  {
    "objectID": "git/practice_repo/search.html#searching-files",
    "href": "git/practice_repo/search.html#searching-files",
    "title": "Searching a version-controlled project",
    "section": "Searching files",
    "text": "Searching files\nThe first thing that can happen is that you are looking for a certain pattern somewhere in your project (for instance a certain function or a certain word).\n\ngit grep\nThe main command to look through versioned files is git grep.\nYou might be familiar with the command-line utility grep which allows to search for lines matching a certain pattern in files. git grep does a similar job with these differences:\n\nit is much faster since all files under version control are already indexed by Git,\nyou can easily search any commit without having to check it out,\nit has features lacking in grep such as, for instance, pattern arithmetic or tree search using globs.\n\n\n\nLet’s try it\nBy default, git grep searches recursively through the tracked files in the working directory (that is, the current version of the tracked files).\nFirst, let’s look for the word test in the current version of the tracked files in the test repo:\n\ngit grep test\n\nadrian.txt:Adrian's test text file.\nformerlyadrian.txt:Adrian's test text file.\nms/protocol.md:This is my test.\nms/smabraha.txt:This is a test file that I wanted to make, then push it somehow\nredone17.txt:this is a test file from redone17\nsrc/test_manuel.py:def test(model, device, test_loader):\nsrc/test_manuel.py:    test_loss = 0\nsrc/test_manuel.py:        for data, target in test_loader:\nsrc/test_manuel.py:            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\nsrc/test_manuel.py:    test_loss /= len(test_loader.dataset)\nsrc/test_manuel.py:        test_loss, correct, len(test_loader.dataset),\nsrc/test_manuel.py:        100. * correct / len(test_loader.dataset)))\nsrc/test_manuel.py:    test_data = datasets.MNIST(\nsrc/test_manuel.py:    test_loader = torch.utils.data.DataLoader(test_data, batch_size=100)\nsrc/test_manuel.py:        test(model, device, test_loader)\ntestAV1.txt:This is a test\ntext-collab.txt:This is the collaboration testing\n\n\nLet’s add blank lines between the results of each file for better readability:\n\ngit grep --break test\n\nadrian.txt:Adrian's test text file.\n\nformerlyadrian.txt:Adrian's test text file.\n\nms/protocol.md:This is my test.\n\nms/smabraha.txt:This is a test file that I wanted to make, then push it somehow\n\nredone17.txt:this is a test file from redone17\n\nsrc/test_manuel.py:def test(model, device, test_loader):\nsrc/test_manuel.py:    test_loss = 0\nsrc/test_manuel.py:        for data, target in test_loader:\nsrc/test_manuel.py:            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\nsrc/test_manuel.py:    test_loss /= len(test_loader.dataset)\nsrc/test_manuel.py:        test_loss, correct, len(test_loader.dataset),\nsrc/test_manuel.py:        100. * correct / len(test_loader.dataset)))\nsrc/test_manuel.py:    test_data = datasets.MNIST(\nsrc/test_manuel.py:    test_loader = torch.utils.data.DataLoader(test_data, batch_size=100)\nsrc/test_manuel.py:        test(model, device, test_loader)\n\ntestAV1.txt:This is a test\n\ntext-collab.txt:This is the collaboration testing\n\n\nLet’s also put the file names on separate lines:\n\ngit grep --break --heading test\n\nadrian.txt\nAdrian's test text file.\n\nformerlyadrian.txt\nAdrian's test text file.\n\nms/protocol.md\nThis is my test.\n\nms/smabraha.txt\nThis is a test file that I wanted to make, then push it somehow\n\nredone17.txt\nthis is a test file from redone17\n\nsrc/test_manuel.py\ndef test(model, device, test_loader):\n    test_loss = 0\n        for data, target in test_loader:\n            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n    test_loss /= len(test_loader.dataset)\n        test_loss, correct, len(test_loader.dataset),\n        100. * correct / len(test_loader.dataset)))\n    test_data = datasets.MNIST(\n    test_loader = torch.utils.data.DataLoader(test_data, batch_size=100)\n        test(model, device, test_loader)\n\ntestAV1.txt\nThis is a test\n\ntext-collab.txt\nThis is the collaboration testing\n\n\nWe can display the line numbers for the results with the -n flag:\n\ngit grep --break --heading -n test\n\nadrian.txt\n1:Adrian's test text file.\n\nformerlyadrian.txt\n1:Adrian's test text file.\n\nms/protocol.md\n9:This is my test.\n\nms/smabraha.txt\n1:This is a test file that I wanted to make, then push it somehow\n\nredone17.txt\n1:this is a test file from redone17\n\nsrc/test_manuel.py\n50:def test(model, device, test_loader):\n52:    test_loss = 0\n55:        for data, target in test_loader:\n58:            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n62:    test_loss /= len(test_loader.dataset)\n65:        test_loss, correct, len(test_loader.dataset),\n66:        100. * correct / len(test_loader.dataset)))\n84:    test_data = datasets.MNIST(\n90:    test_loader = torch.utils.data.DataLoader(test_data, batch_size=100)\n97:        test(model, device, test_loader)\n\ntestAV1.txt\n1:This is a test\n\ntext-collab.txt\n1:This is the collaboration testing\n\n\nNotice how the results for the file src/test_manuel.py involve functions. It would be very convenient to have the names of the functions in which test appears.\nWe can do this with the -p flag:\n\ngit grep --break --heading -p test src/test_manuel.py\n\nsrc/test_manuel.py\ndef train(model, device, train_loader, optimizer, epoch):\ndef test(model, device, test_loader):\n    test_loss = 0\n        for data, target in test_loader:\n            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n    test_loss /= len(test_loader.dataset)\n        test_loss, correct, len(test_loader.dataset),\n        100. * correct / len(test_loader.dataset)))\ndef main():\n    test_data = datasets.MNIST(\n    test_loader = torch.utils.data.DataLoader(test_data, batch_size=100)\n        test(model, device, test_loader)\n\n\n\nWe added the argument src/test_manuel.py to limit the search to that file.\n\nWe can now see that the word test appears in the functions test and main.\nNow, instead of printing all the matching lines, let’s print the number of matches per file:\n\ngit grep -c test\n\nadrian.txt:1\nformerlyadrian.txt:1\nms/protocol.md:1\nms/smabraha.txt:1\nredone17.txt:1\nsrc/test_manuel.py:10\ntestAV1.txt:1\ntext-collab.txt:1\n\n\n\n\nMore complex patterns\ngit grep in fact searches for regular expressions. test is a regular expression matching test, but we can look for more complex patterns.\nLet’s look for image:\n\ngit grep image\n\n\nNo output means that the search is not returning any result.\n\nLet’s make this search case insensitive:\n\ngit grep -i image\n\nsrc/new_file.py:from PIL import Image\nsrc/new_file.py:berlin1_lr = Image.open(\"/home/marie/parvus/pwg/wtm/slides/static/img/upscaling/lr/berlin_1945_1.jpg\")\nsrc/new_file.py:berlin1_hr = Image.open(\"/home/marie/parvus/pwg/wtm/slides/static/img/upscaling/hr/berlin_1945_1.png\")\n\n\nWe are now getting some results as Image was present in three lines of the file src/new_file.py.\nLet’s now search for data:\n\ngit grep data\n\n.gitignore:data/\nms/protocol.md:Collected and analyzed amazing data\nsrc/new_file.py:from datasets import load_dataset\nsrc/new_file.py:set5 = load_dataset('eugenesiow/Set5', 'bicubic_x4', split='validation')\nsrc/test_manuel.py:from torchvision import datasets, transforms\nsrc/test_manuel.py:    for batch_idx, (data, target) in enumerate(train_loader):\nsrc/test_manuel.py:        data, target = data.to(device), target.to(device)\nsrc/test_manuel.py:        output = model(data)\nsrc/test_manuel.py:                epoch, batch_idx * len(data), len(train_loader.dataset),\nsrc/test_manuel.py:        for data, target in test_loader:\nsrc/test_manuel.py:            data, target = data.to(device), target.to(device)\nsrc/test_manuel.py:            output = model(data)\nsrc/test_manuel.py:    test_loss /= len(test_loader.dataset)\nsrc/test_manuel.py:        test_loss, correct, len(test_loader.dataset),\nsrc/test_manuel.py:        100. * correct / len(test_loader.dataset)))\nsrc/test_manuel.py:    train_data = datasets.MNIST(\nsrc/test_manuel.py:        '~/parvus/pwg/wtm/tml/data',\nsrc/test_manuel.py:        # '~/projects/def-sponsor00/data',\nsrc/test_manuel.py:    test_data = datasets.MNIST(\nsrc/test_manuel.py:        '~/parvus/pwg/wtm/tml/data',\nsrc/test_manuel.py:        # '~/projects/def-sponsor00/data',\nsrc/test_manuel.py:    train_loader = torch.utils.data.DataLoader(train_data, batch_size=50)\nsrc/test_manuel.py:    test_loader = torch.utils.data.DataLoader(test_data, batch_size=100)\n\n\nWe are getting results for the word data, but also for the pattern data in longer expressions such as train_data or dataset. If we only want results for the word data, we can use the -w flag:\n\ngit grep -w data\n\n.gitignore:data/\nms/protocol.md:Collected and analyzed amazing data\nsrc/test_manuel.py:    for batch_idx, (data, target) in enumerate(train_loader):\nsrc/test_manuel.py:        data, target = data.to(device), target.to(device)\nsrc/test_manuel.py:        output = model(data)\nsrc/test_manuel.py:                epoch, batch_idx * len(data), len(train_loader.dataset),\nsrc/test_manuel.py:        for data, target in test_loader:\nsrc/test_manuel.py:            data, target = data.to(device), target.to(device)\nsrc/test_manuel.py:            output = model(data)\nsrc/test_manuel.py:        '~/parvus/pwg/wtm/tml/data',\nsrc/test_manuel.py:        # '~/projects/def-sponsor00/data',\nsrc/test_manuel.py:        '~/parvus/pwg/wtm/tml/data',\nsrc/test_manuel.py:        # '~/projects/def-sponsor00/data',\nsrc/test_manuel.py:    train_loader = torch.utils.data.DataLoader(train_data, batch_size=50)\nsrc/test_manuel.py:    test_loader = torch.utils.data.DataLoader(test_data, batch_size=100)\n\n\nNow, let’s use a more complex regular expression. We want the counts for the pattern \".*_.*\" (i.e. any name with a snail case such as train_loader):\n\ngit grep -c \".*_.*\"\n\n.gitignore:2\nsrc/new_file.py:22\nsrc/test_manuel.py:29\n\n\nLet’s print the first 3 results per file:\n\ngit grep -m 3 \".*_.*\"\n\n.gitignore:hidden_file\n.gitignore:search_cache/\nsrc/new_file.py:from datasets import load_dataset\nsrc/new_file.py:set5 = load_dataset('eugenesiow/Set5', 'bicubic_x4', split='validation')\nsrc/new_file.py:set5.column_names\nsrc/test_manuel.py:from torch.optim.lr_scheduler import StepLR\nsrc/test_manuel.py:    def __init__(self):\nsrc/test_manuel.py:        super(Net, self).__init__()\n\n\nAs you can see, our results also include __init__ which is not what we were looking for. So let’s exclude __:\n\ngit grep -m 3 -e \".*_.*\" --and --not -e \"__\"\n\n.gitignore:hidden_file\n.gitignore:search_cache/\nsrc/new_file.py:from datasets import load_dataset\nsrc/new_file.py:set5 = load_dataset('eugenesiow/Set5', 'bicubic_x4', split='validation')\nsrc/new_file.py:set5.column_names\nsrc/test_manuel.py:from torch.optim.lr_scheduler import StepLR\nsrc/test_manuel.py:        x = F.max_pool2d(x, 2)\nsrc/test_manuel.py:        output = F.log_softmax(x, dim=1)\n\n\n\nFor simple searches, you don’t have to use the -e flag before the pattern you are searching for. Here however, our command has gotten complex enough that we have to use it before each pattern.\n\nLet’s make sure this worked as expected:\n\ngit grep -c \".*_.*\"\necho \"---\"\ngit grep -c \"__\"\necho \"---\"\ngit grep -ce \".*_.*\" --and --not -e \"__\"\n\n.gitignore:2\nsrc/new_file.py:22\nsrc/test_manuel.py:29\n---\nsrc/test_manuel.py:2\n---\n.gitignore:2\nsrc/new_file.py:22\nsrc/test_manuel.py:27\n\n\nThere were 2 lines matching __ in src/test_manuel.py and we have indeed excluded them from our search.\nExtended regular expressions are also covered with the flag -E.\n\n\nSearching other trees\nSo far, we have searched the current version of tracked files, but we can just as easily search files at any commit.\nLet’s search for test in the tracked files 20 commits ago:\n\ngit grep test HEAD~20\n\nHEAD~20:adrian.txt:Adrian's test text file.\nHEAD~20:formerlyadrian.txt:Adrian's test text file.\nHEAD~20:ms/protocol.md:This is my test.\nHEAD~20:ms/smabraha.txt:This is a test file that I wanted to make, then push it somehow\nHEAD~20:redone17.txt:this is a test file from redone17\nHEAD~20:testAV1.txt:This is a test\nHEAD~20:text-collab.txt:This is the collaboration testing\n\n\n\nAs you can see, the file src/test_manuel.py is not in the results. Either it didn’t exist or it didn’t have the word test at that commit.\n\nIf you want to search tracked files AND untracked files, you need to use the --untracked flag.\nLet’s create a new (thus untracked) file with some content including the word test:\n\necho \"This is a test\" &gt; newfile\n\nNow compare the following:\n\ngit grep -c test\n\nadrian.txt:1\nformerlyadrian.txt:1\nms/protocol.md:1\nms/smabraha.txt:1\nredone17.txt:1\nsrc/test_manuel.py:10\ntestAV1.txt:1\ntext-collab.txt:1\n\n\nwith:\n\ngit grep -c --untracked test\n\nadrian.txt:1\nformerlyadrian.txt:1\nms/protocol.md:1\nms/smabraha.txt:1\nnewfile:1\nredone17.txt:1\nsrc/test_manuel.py:10\ntestAV1.txt:1\ntext-collab.txt:1\n\n\n\nThis last result also returned our untracked file newfile.\n\nIf you want to search untracked and ignored files (meaning all your files), use the flags --untracked --no-exclude-standard.\nLet’s see what the .gitignore file contains:\n\ncat .gitignore\n\ndata/\noutput/\nhidden_file\nsearch_cache/\nsearch.qmd\nsearch.rmarkdown\n\n\nThe directory data is in .gitignore. This means that it is not under version control and it thus doesn’t exist in our repo (since we cloned our repo, we only have the version-controlled files). Let’s create it:\nmkdir data\nNow, let’s create a file in it that contains test:\n\necho \"And another test\" &gt; data/file\n\nWe can rerun our previous two searches to verify that files excluded from version control are not searched:\n\ngit grep -c test\n\nadrian.txt:1\nformerlyadrian.txt:1\nms/protocol.md:1\nms/smabraha.txt:1\nredone17.txt:1\nsrc/test_manuel.py:10\ntestAV1.txt:1\ntext-collab.txt:1\n\n\n\ngit grep -c --untracked test\n\nadrian.txt:1\nformerlyadrian.txt:1\nms/protocol.md:1\nms/smabraha.txt:1\nnewfile:1\nredone17.txt:1\nsrc/test_manuel.py:10\ntestAV1.txt:1\ntext-collab.txt:1\n\n\nAnd now, let’s try:\n\ngit grep -c --untracked --no-exclude-standard test\n\nadrian.txt:1\ndata/file:1\nformerlyadrian.txt:1\nms/protocol.md:1\nms/smabraha.txt:1\nnewfile:1\nredone17.txt:1\nsearch.qmd:41\nsearch.rmarkdown:41\nsrc/test_manuel.py:10\ntestAV1.txt:1\ntext-collab.txt:1\n\n\n\ndata/file, despite being excluded from version control, is also searched.\n\n\n\nSearching all commits\nWe saw that git grep &lt;pattern&gt; &lt;commit&gt; can search a pattern in any commit. Now, what if we all to search all commits for a pattern?\nFor this, we pass the expression $(git rev-list --all) in lieu of &lt;commit&gt;.\ngit rev-list --all creates a list of all the commits in a way that can be used as an argument to other functions. The $() allows to run the expression inside it and pass the result as and argument.\nTo search for test in all the commits, we thus run:\ngit grep \"test\" $(git rev-list --all)\nI am not running this command has it has a huge output. Instead, I will limit the search to the last two commits:\n\ngit grep \"test\" $(git rev-list --all -2)\n\ne3cfb2ebbcb77e52851c32fbba1f6b8b0c788a29:adrian.txt:Adrian's test text file.\ne3cfb2ebbcb77e52851c32fbba1f6b8b0c788a29:formerlyadrian.txt:Adrian's test text file.\ne3cfb2ebbcb77e52851c32fbba1f6b8b0c788a29:ms/protocol.md:This is my test.\ne3cfb2ebbcb77e52851c32fbba1f6b8b0c788a29:ms/smabraha.txt:This is a test file that I wanted to make, then push it somehow\ne3cfb2ebbcb77e52851c32fbba1f6b8b0c788a29:redone17.txt:this is a test file from redone17\ne3cfb2ebbcb77e52851c32fbba1f6b8b0c788a29:src/test_manuel.py:def test(model, device, test_loader):\ne3cfb2ebbcb77e52851c32fbba1f6b8b0c788a29:src/test_manuel.py:    test_loss = 0\ne3cfb2ebbcb77e52851c32fbba1f6b8b0c788a29:src/test_manuel.py:        for data, target in test_loader:\ne3cfb2ebbcb77e52851c32fbba1f6b8b0c788a29:src/test_manuel.py:            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\ne3cfb2ebbcb77e52851c32fbba1f6b8b0c788a29:src/test_manuel.py:    test_loss /= len(test_loader.dataset)\ne3cfb2ebbcb77e52851c32fbba1f6b8b0c788a29:src/test_manuel.py:        test_loss, correct, len(test_loader.dataset),\ne3cfb2ebbcb77e52851c32fbba1f6b8b0c788a29:src/test_manuel.py:        100. * correct / len(test_loader.dataset)))\ne3cfb2ebbcb77e52851c32fbba1f6b8b0c788a29:src/test_manuel.py:    test_data = datasets.MNIST(\ne3cfb2ebbcb77e52851c32fbba1f6b8b0c788a29:src/test_manuel.py:    test_loader = torch.utils.data.DataLoader(test_data, batch_size=100)\ne3cfb2ebbcb77e52851c32fbba1f6b8b0c788a29:src/test_manuel.py:        test(model, device, test_loader)\ne3cfb2ebbcb77e52851c32fbba1f6b8b0c788a29:testAV1.txt:This is a test\ne3cfb2ebbcb77e52851c32fbba1f6b8b0c788a29:text-collab.txt:This is the collaboration testing\n15fdec6afb552e4ba2ec5f7a0b371543c9966c27:adrian.txt:Adrian's test text file.\n15fdec6afb552e4ba2ec5f7a0b371543c9966c27:formerlyadrian.txt:Adrian's test text file.\n15fdec6afb552e4ba2ec5f7a0b371543c9966c27:ms/protocol.md:This is my test.\n15fdec6afb552e4ba2ec5f7a0b371543c9966c27:ms/smabraha.txt:This is a test file that I wanted to make, then push it somehow\n15fdec6afb552e4ba2ec5f7a0b371543c9966c27:redone17.txt:this is a test file from redone17\n15fdec6afb552e4ba2ec5f7a0b371543c9966c27:src/test_manuel.py:def test(model, device, test_loader):\n15fdec6afb552e4ba2ec5f7a0b371543c9966c27:src/test_manuel.py:    test_loss = 0\n15fdec6afb552e4ba2ec5f7a0b371543c9966c27:src/test_manuel.py:        for data, target in test_loader:\n15fdec6afb552e4ba2ec5f7a0b371543c9966c27:src/test_manuel.py:            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n15fdec6afb552e4ba2ec5f7a0b371543c9966c27:src/test_manuel.py:    test_loss /= len(test_loader.dataset)\n15fdec6afb552e4ba2ec5f7a0b371543c9966c27:src/test_manuel.py:        test_loss, correct, len(test_loader.dataset),\n15fdec6afb552e4ba2ec5f7a0b371543c9966c27:src/test_manuel.py:        100. * correct / len(test_loader.dataset)))\n15fdec6afb552e4ba2ec5f7a0b371543c9966c27:src/test_manuel.py:    test_data = datasets.MNIST(\n15fdec6afb552e4ba2ec5f7a0b371543c9966c27:src/test_manuel.py:    test_loader = torch.utils.data.DataLoader(test_data, batch_size=100)\n15fdec6afb552e4ba2ec5f7a0b371543c9966c27:src/test_manuel.py:        test(model, device, test_loader)\n15fdec6afb552e4ba2ec5f7a0b371543c9966c27:testAV1.txt:This is a test\n15fdec6afb552e4ba2ec5f7a0b371543c9966c27:text-collab.txt:This is the collaboration testing\n\n\n\nIn combination with the fuzzy finder tool fzf, this can make finding a particular commit extremely easy.\nFor instance, the code below allows you to dynamically search in the result through incremental completion:\ngit grep \"test\" $(git rev-list --all) | fzf --cycle -i -e\nOr even better, you can automatically copy the short form of the hash of the selected commit to clipboard so that you can use it with git show, git checkout, etc.:\ngit grep \"test\" $(git rev-list --all) |\n    fzf --cycle -i -e |\n    cut -c 1-7 |\n    xclip -r -selection clipboard\n\nHere, I am using xclip to copy to the clipboard as I am on Linux. Depending on your OS you might need to use a different tool.\n\nOf course, you can create a function in your .bashrc file with such code so that you wouldn’t have to type it each time:\ngrep_all_commits () {\n    git grep \"$1\" $(git rev-list --all) |\n        fzf --cycle -i -e |\n        cut -c 1-7 |\n        xclip -r -selection clipboard\n}\nAlternatively, you can pass the result directly into whatever git command you want to use that commit for.\nHere is an example with git show:\ngit grep \"test\" $(git rev-list --all) |\n    fzf --cycle -i -e |\n    cut -c 1-7 |\n    git show\nAnd if you wanted to get really fancy, you could go with:\ngit grep \"test\" $(git rev-list --all) |\n    fzf --cycle -i -e --no-multi \\\n        --ansi --preview=\"$_viewGitLogLine\" \\\n        --header \"enter: view, C-c: copy hash\" \\\n        --bind \"enter:execute:$_viewGitLogLine | less -R\" \\\n        --bind \"ctrl-c:execute:$_gitLogLineToHash |\n        xclip -r -selection clipboard\"\nWrapped in a function:\ngrep_all_commits_preview () {\n    git grep \"$1\" $(git rev-list --all) |\n        fzf --cycle -i -e --no-multi \\\n            --ansi --preview=\"$_viewGitLogLine\" \\\n            --header \"enter: view, C-c: copy hash\" \\\n            --bind \"enter:execute:$_viewGitLogLine |\n              less -R\" \\\n            --bind \"ctrl-c:execute:$_gitLogLineToHash |\n        xclip -r -selection clipboard\"\n}\nThis last function allows you to search through all the results in an incremental fashion while displaying a preview of the selected diff (the changes made at that particular commit). If you want to see more of the diff than the preview displays, press &lt;enter&gt; (then q to quit the pager), if you want to copy the hash of a commit, press C-c (Control + c).\nWith this function, you can now instantly get a preview of the changes made to any line containing an expression for any file, at any commit, and copy the hash of the selected commit. This is really powerful.\n\n\n\nAliases\nIf you don’t want to type a series of flags all the time, you can configure aliases for Git. For instance, Alex Razoumov uses the alias git search for git grep --break --heading -n -i.\nLet’s add to it the -p flag. Here is how you would set this alias:\ngit config --global alias.search 'grep --break --heading -n -i -p'\n\nThis setting gets added to your main Git configuration file (on Linux, by default, at ~/.gitconfig).\n\nFrom there on, you can use your alias with:\n\ngit search test\n\nadrian.txt\n1:Adrian's test text file.\n\nformerlyadrian.txt\n1:Adrian's test text file.\n\nms/protocol.md\n6=using our awesome Rscript.\n9:This is my test.\n\nms/smabraha.txt\n1:This is a test file that I wanted to make, then push it somehow\n\nredone17.txt\n1:this is a test file from redone17\n\nsrc/test_manuel.py\n35=def train(model, device, train_loader, optimizer, epoch):\n50:def test(model, device, test_loader):\n52:    test_loss = 0\n55:        for data, target in test_loader:\n58:            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n62:    test_loss /= len(test_loader.dataset)\n64:    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n65:        test_loss, correct, len(test_loader.dataset),\n66:        100. * correct / len(test_loader.dataset)))\n69=def main():\n84:    test_data = datasets.MNIST(\n90:    test_loader = torch.utils.data.DataLoader(test_data, batch_size=100)\n97:        test(model, device, test_loader)\n\ntestAV1.txt\n1:This is a test\n\ntext-collab.txt\n1:This is the collaboration testing"
  },
  {
    "objectID": "git/practice_repo/search.html#searching-logs",
    "href": "git/practice_repo/search.html#searching-logs",
    "title": "Searching a version-controlled project",
    "section": "Searching logs",
    "text": "Searching logs\nThe second thing that can happen is that you are looking for some pattern in your version control logs.\n\ngit log\ngit log allows to get information on commit logs.\nBy default, it outputs all the commits of the current branch.\nLet’s show the logs of the last 3 commits:\n\ngit log -3\n\ncommit e3cfb2ebbcb77e52851c32fbba1f6b8b0c788a29\nAuthor: Marie-Helene Burle &lt;marie.burle@westdri.ca&gt;\nDate:   Sat Jan 7 22:32:23 2023 -0800\n\n    Update gitignore with Quarto files\n\ncommit 15fdec6afb552e4ba2ec5f7a0b371543c9966c27\nAuthor: Marie-Helene Burle &lt;marie.burle@westgrid.ca&gt;\nDate:   Fri Jan 6 10:18:28 2023 -0800\n\n    Update README.org\n\ncommit 15d4ee937db18fb26f84d17ec4be3f0c81614a1c\nAuthor: Marie-Helene Burle &lt;marie.burle@westgrid.ca&gt;\nDate:   Wed Mar 16 10:55:28 2022 -0700\n\n    change values training\n\n\nThe output can be customized thanks to a plethora of options.\nFor instance, here are the logs of the last 15 commits, in a graph, with one line per commit:\n\ngit log --graph --oneline -n 15\n\n* e3cfb2e Update gitignore with Quarto files\n* 15fdec6 Update README.org\n* 15d4ee9 change values training\n* 06efa34 add lots of code\n* 1457143 remove stupid line\n* 711e1dc add real py content to test_manual.py\n* 90016aa adding new python file\n*   2c0f612 Merge branch 'main' of github.com:prosoitos/git_workshop_collab\n|\\  \n| *   6f7d03d Merge branch 'main' of https://github.com/prosoitos/git_workshop_collab into main\n| |\\  \n| * \\   3c53269 Merge branch 'main' of https://github.com/prosoitos/git_workshop_collab into main\n| |\\ \\  \n| * \\ \\   eef5b78 Merge branch 'main' of https://github.com/prosoitos/git_workshop_collab into main\n| |\\ \\ \\  \n| * | | | a55ca0d new comment add just as test\n* | | | |   dedc94f Merge branch 'main' of github.com:prosoitos/git_workshop_collab\n|\\ \\ \\ \\ \\  \n| | |_|_|/  \n| |/| | |   \n| * | | |   b861a65 Merge branch 'main' of https://github.com/prosoitos/git_workshop_collab\n| |\\ \\ \\ \\  \n| | | |_|/  \n| | |/| |   \n| | * | |   35e8d5a Merge branch 'main' of github.com:prosoitos/git_workshop_collab\n| | |\\ \\ \\  \n| | | | |/  \n| | | |/|   \n\n\nBut git log has also flags that allow to search for patterns.\n\n\nSearching commit messages\nOne of the reasons it is so important to write informative commit messages is that they are key to finding commits later on.\nTo look for a pattern in all your commit messages, use git log --grep=&lt;pattern&gt;.\nLet’s look for test in the commit messages and limit the output to 3 commits:\n\ngit log --grep=test -3\n\ncommit 711e1dc53011e5071b17dc7c35b516f6e066f396\nAuthor: Marie-Helene Burle &lt;marie.burle@westgrid.ca&gt;\nDate:   Tue Mar 15 11:52:48 2022 -0700\n\n    add real py content to test_manual.py\n\ncommit a55ca0d60d82578c94bd49fc4ca987727b851216\nAuthor: Manuelhrokr &lt;zl.manuel@protonmail.com&gt;\nDate:   Thu Feb 17 15:19:42 2022 -0700\n\n    new comment add just as test\n\ncommit ea74e46f487fba09c31524a110fdf060796e3cf8\nAuthor: mpkin &lt;mikin@physics.ubc.ca&gt;\nDate:   Thu Sep 23 14:51:24 2021 -0700\n\n    Add test_mk.txt\n\n\nFor a more compact output:\n\ngit log --grep=\"test\" -3 --oneline\n\n711e1dc add real py content to test_manual.py\na55ca0d new comment add just as test\nea74e46 Add test_mk.txt\n\n\n\nHere too you can use this in combination to fzf with for instance:\ngit log --grep=\"test\" | fzf --cycle -i -e\nOr:\ngit log --grep=\"test\" --oneline |\n    fzf --cycle -i -e --no-multi \\\n        --ansi --preview=\"$_viewGitLogLine\" \\\n        --header \"enter: view, C-c: copy hash\" \\\n        --bind \"enter:execute:$_viewGitLogLine | less -R\" \\\n        --bind \"ctrl-c:execute:$_gitLogLineToHash |\n        xclip -r -selection clipboard\"\n\n\n\nChanges made to a pattern\nRemember that test was present in the file src/test_manuel.py. If we want to see when the pattern was first created and then each time it was modified, we use the -L flag in this fashion:\ngit log -L :&lt;pattern&gt;:file\nIn our case:\n\ngit log -L :test:src/test_manuel.py\n\ncommit 711e1dc53011e5071b17dc7c35b516f6e066f396\nAuthor: Marie-Helene Burle &lt;marie.burle@westgrid.ca&gt;\nDate:   Tue Mar 15 11:52:48 2022 -0700\n\n    add real py content to test_manual.py\n\ndiff --git a/src/test_manuel.py b/src/test_manuel.py\n--- a/src/test_manuel.py\n+++ b/src/test_manuel.py\n@@ -1,1 +50,19 @@\n-test\n+def test(model, device, test_loader):\n+    model.eval()\n+    test_loss = 0\n+    correct = 0\n+    with torch.no_grad():\n+        for data, target in test_loader:\n+            data, target = data.to(device), target.to(device)\n+            output = model(data)\n+            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n+            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n+            correct += pred.eq(target.view_as(pred)).sum().item()\n+\n+    test_loss /= len(test_loader.dataset)\n+\n+    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n+        test_loss, correct, len(test_loader.dataset),\n+        100. * correct / len(test_loader.dataset)))\n+\n+\n\ncommit 90016aa3ed3a6cf71e206392bbf10adfe1a14c17\nAuthor: Manuelhrokr &lt;zl.manuel@protonmail.com&gt;\nDate:   Thu Feb 17 15:33:03 2022 -0700\n\n    adding new python file\n\ndiff --git a/src/test_manuel.py b/src/test_manuel.py\n--- /dev/null\n+++ b/src/test_manuel.py\n@@ -0,0 +1,1 @@\n+test\n\n\nThis is very useful if you want to see, for instance, changes made to a function in a script.\n\n\nChanges in number of occurrences of a pattern\nNow, if we want to list all commits that created a change in the number of occurrences of test in our project, we run:\n\ngit log -S test --oneline\n\n711e1dc add real py content to test_manual.py\n90016aa adding new python file\n652faa5 delete my file\nb684eac Deleted file\n6717236 For collab\nca1845d delete alex.txt\n6b56198 editing adrians text file\n01a7358 test dtrad\ne44a454 Create testAV1.txt\n5ee88e6 For collab\ncf3d4ea Collab-test\n13faa1e test, test\n0366115 Adrian's test file\n9ebd3ce This is my test\n6dfefa8 create redone17.txt\ne43163c added alex.txt\n\n\nThis can be useful to identify the commit you need."
  },
  {
    "objectID": "git/practice_repo/search.html#tldr",
    "href": "git/practice_repo/search.html#tldr",
    "title": "Searching a version-controlled project",
    "section": "TL;DR",
    "text": "TL;DR\nHere are the search functions you are the most likely to use:\n\nSearch for a pattern in the current version of your tracked files:\n\ngit grep &lt;pattern&gt;\n\nSearch for a pattern in your files at a certain commit:\n\ngit grep &lt;pattern&gt; &lt;commit&gt;\n\nSearch for a pattern in your files in all the commits:\n\ngit grep &lt;pattern&gt; $(git rev-list --all)\n\nSearch for a pattern in your commit messages:\n\ngit log --grep=&lt;pattern&gt;\nNow you should be able to find pretty much anything in your projects and their histories."
  },
  {
    "objectID": "git/stash.html",
    "href": "git/stash.html",
    "title": "Stashing",
    "section": "",
    "text": "Stashing is a way to put changes aside for some time. Changes can be reapplied later (and/or applied on other branches).\nWhy would you want to do that? And how?"
  },
  {
    "objectID": "git/stash.html#what-are-git-stashes",
    "href": "git/stash.html#what-are-git-stashes",
    "title": "Stashing",
    "section": "What are Git stashes?",
    "text": "What are Git stashes?\nHaving a clean working tree is necessary for many Git operations and recommended for others. If you have changes from an unfinished piece of work getting in the way, you could do an ugly commit to get rid of them. But if you care about having an organized history with meaningful commits (or if you don’t want others to see your messy drafts), stashing is a much better alternative."
  },
  {
    "objectID": "git/stash.html#creating-a-stash",
    "href": "git/stash.html#creating-a-stash",
    "title": "Stashing",
    "section": "Creating a stash",
    "text": "Creating a stash\nYou can stash the changes in your modified files with:\ngit stash\nTo also include new (untracked) files, you have to use the -u flag:\ngit stash -u"
  },
  {
    "objectID": "git/stash.html#listing-stashes",
    "href": "git/stash.html#listing-stashes",
    "title": "Stashing",
    "section": "Listing stashes",
    "text": "Listing stashes\nOf course, you don’t want to lose or forget about your stashes.\nYou can list them with:\ngit stash list\nStashes are also shown when you run git log (with any of its variations) with the --all flag.\n\nExample:\n\ngit log --graph --oneline --all"
  },
  {
    "objectID": "git/stash.html#re-applying-changes-from-a-stash",
    "href": "git/stash.html#re-applying-changes-from-a-stash",
    "title": "Stashing",
    "section": "Re-applying changes from a stash",
    "text": "Re-applying changes from a stash\nTo re-apply the changes (or apply them on another branch), you run:\ngit stash apply\nIf you had staged changes, you can also restore the state of the index by running instead:\ngit stash apply --index\nIf you created multiple stashes, this will apply the last one you created. If this is not what you want, you have to specify which stash you want to use with the reflog syntax:\n\nstash@{0} is the last stash (so you can omit it)\n\nstash@{1} is the one before it\n\nstash@{2} the one before that\n\netc.\n\n\nTo apply the stash before last:\n\ngit stash apply stash@{1}"
  },
  {
    "objectID": "git/stash.html#deleting-a-stash",
    "href": "git/stash.html#deleting-a-stash",
    "title": "Stashing",
    "section": "Deleting a stash",
    "text": "Deleting a stash\nYou delete the last (or the only) stash with:\ngit stash drop\nHere again, if you want to delete another stash, specify it with its reflog index.\n\nTo delete the antepenultimate stash:\n\ngit stash drop stash@{2}\nYou can apply and delete a stash at the same time with:\ngit stash pop\nThis is convenient, but less flexible."
  },
  {
    "objectID": "git/three_trees.html",
    "href": "git/three_trees.html",
    "title": "The three trees of Git",
    "section": "",
    "text": "One useful mental representation of the functioning of Git is to imagine three file trees."
  },
  {
    "objectID": "git/three_trees.html#the-three-trees-of-git",
    "href": "git/three_trees.html#the-three-trees-of-git",
    "title": "The three trees of Git",
    "section": "The three trees of Git",
    "text": "The three trees of Git\n\nWorking directory\nLet’s imagine that you are starting to work on a project.\nFirst, you create a directory.\nIn it, you create several sub-directories.\nIn those, you create a number of files.\nYou can open these files, read them, edit them, etc. This is something you are very familiar with.\nIn the Git world, this is the working directory or working tree of the project.\nThat is: an uncompressed version of your files that you can access and edit.\nYou can think of it as a sandbox because this is where you can experiment with the project. This is where the project gets developed.\nNow, Git has two other important pieces in its architecture.\n\n\nIndex\nIf you want the project history to be useful to future you, it has to be nice and tidy. You don’t want to record snapshots haphazardly or you will never be able to find anything back.\nBefore you record a snapshot, you carefully select the elements of the project as it is now that would be useful to write to the project history together. The index or staging area is what allows to do that: it contains the suggested future snapshot.\n\n\nHEAD\nFinally, the last tree in Git architecture is one snapshot in the project history that serves as a reference version of the project: if you want to see what you have been experimenting on in your “sandbox”, you need to compare the state of the working directory with some snapshot.\nRemember that HEAD is a pointer pointing at a branch, that a branch is itself a pointer pointing at a commit, and finally that a commit is a Git object pointing at a snapshot. When the HEAD pointer moves around, whatever snapshot it points to populates the HEAD tree.\nAs we saw earlier, when you create a commit, HEAD automatically points to the new commit. So the HEAD tree is often filled with the last snapshot you created. But—as we will see later—we can move the HEAD pointer around through other ways. So the HEAD tree can be populated by any snapshot in your project history."
  },
  {
    "objectID": "git/three_trees.html#status-of-the-three-trees",
    "href": "git/three_trees.html#status-of-the-three-trees",
    "title": "The three trees of Git",
    "section": "Status of the three trees",
    "text": "Status of the three trees\nTo display the status of these trees, you run:\ngit status"
  },
  {
    "objectID": "git/three_trees.html#three-trees-in-action",
    "href": "git/three_trees.html#three-trees-in-action",
    "title": "The three trees of Git",
    "section": "Three trees in action",
    "text": "Three trees in action\n\nClean working tree\nWe say that the working tree is “clean” when all changes tracked by Git were staged and committed:\n\nHere is an example for a project with a single file called File at version v1.\n\n\n\n\nMaking changes to the working tree\nWhen you edit files in your project, you make changes in the working directory or working tree.\n\nFor instance, you make changes to File. Let’s say that it is now at version v2:\n\n\nThe other two trees remain at version v1.\nIf you run git status, this is what you get:\nOn branch main\nChanges not staged for commit:\n  (use \"git add &lt;file&gt;...\" to update what will be committed)\n  (use \"git restore &lt;file&gt;...\" to discard changes in working directory)\n        modified:   File\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\n\n\nStaging changes\nYou stage that file (meaning that you will include the changes of that file in the next commit) with:\ngit add File\nAfter which, your Git trees look like this:\n\nNow, the index also has File at version v2 and git status returns:\nOn branch main\nChanges to be committed:\n  (use \"git restore --staged &lt;file&gt;...\" to unstage)\n        modified:   File\n\n\nCommitting changes\nFinally, you create a snapshot and the commit pointing to it—recording the staged changes to history—with:\ngit commit -m \"Added File\"\n-m is a flag that allows to provide the commit message directly in the command line. If you don’t use it, Git will open a text editor so that you can type the message. Without a message, there can be no commit.\nNow your trees look like this:\n\nOur working tree is clean again and git status returns:\nOn branch main\nnothing to commit, working tree clean\nThis means that there are no uncommitted changes in the working tree or the staging area: all the changes have been written to history.\n\nYou don’t have to stage all the changes in the working directory before making a commit; that is actually the whole point of the staging area.\nThis means that the working directory is not necessarily clean after you have created a new commit."
  },
  {
    "objectID": "git/tools.html",
    "href": "git/tools.html",
    "title": "Tools for a friendlier Git",
    "section": "",
    "text": "Two great open-source tools to work with Git in a nice visual manner while remaining in the command line."
  },
  {
    "objectID": "git/tools.html#fzf",
    "href": "git/tools.html#fzf",
    "title": "Tools for a friendlier Git",
    "section": "fzf",
    "text": "fzf\nfzf is a fantastic multi-platform command line fuzzy finder with a huge versatility.\n\nIn this video, I demo quickly how it can be used with Git:"
  },
  {
    "objectID": "git/tools.html#lazygit",
    "href": "git/tools.html#lazygit",
    "title": "Tools for a friendlier Git",
    "section": "lazygit",
    "text": "lazygit\nlazygit is an excellent multi-platform user interface for Git which works in the command line."
  },
  {
    "objectID": "git/undo.html",
    "href": "git/undo.html",
    "title": "Undoing",
    "section": "",
    "text": "This section covers a few of the ways actions can be undone in Git."
  },
  {
    "objectID": "git/undo.html#amending-the-last-commit",
    "href": "git/undo.html#amending-the-last-commit",
    "title": "Undoing",
    "section": "Amending the last commit",
    "text": "Amending the last commit\nHere is a common scenario: you make a commit, then realize that you forgot to include some changes in that commit; or you aren’t happy with the commit message; or both. You can edit your latest commit with the --amend flag:\ngit commit --amend\nThis will hide your last commit (as if it had never happened), add the changes in the staging area (if any) to the changes in that last commit, open a text editor showing the message of the last commit (you can keep or edit that message), and create a new commit which replaces your last commit.\nSo if you only want to change the commit message, run that command with an empty staging area. If you want to add changes to the last commit, stage them, then run the command.\nIn short, what this does is to replace your last commit with a new commit with the added changes and/or edited message. This prevents having a messy history with commits of the type “add missing file to last commit” or “better message for last commit: bla bla bla”. If you made a typo in your last commit message (and if you care about having a nice, clean history), you can fix it easily this way.\n\n\nYour turn:\n\n\nRun git log --oneline (notice the hash of the last commit)\nEdit your last commit message\nRun git log --oneline again to see that your last commit now has a new hash (so it is a different commit) and a new message\nNow, make some change in your project (add a file, or edit a file… any change you want)\nThen add that new change to your last commit without changing the message"
  },
  {
    "objectID": "git/undo.html#unstaging",
    "href": "git/undo.html#unstaging",
    "title": "Undoing",
    "section": "Unstaging",
    "text": "Unstaging\nYou know how to add changes to the staging area. But what if you want to unstage changes? You don’t want to loose those changes. But you staged them and then realized that you don’t want to include them in your next commit after all.\nHere is the command for this:\ngit restore --staged &lt;file&gt;\n\nNote that Git will remind you about the existence of this command when you run git status and have staged files ready to be committed.\n\n\n\nYour turn:\n\n\nMake changes to one of your existing files\nStage that file\nRun git status and notice Git’s reminder about this command\nUnstage the changes on that file"
  },
  {
    "objectID": "git/undo.html#erasing-modifications",
    "href": "git/undo.html#erasing-modifications",
    "title": "Undoing",
    "section": "Erasing modifications",
    "text": "Erasing modifications\nNow, what if you made changes to a file, then decide that they were no good? You can easily get rid of these edits and restore the file to its last committed version:\ngit restore &lt;file&gt;\n\nNote that Git will tell you about this command when you run git status and have unstaged changes in tracked files.\n\n\n\nYour turn:\n\n\nRun git status again and notice Git’s reminder about the existence of this command\nErase that last change of yours\nOpen your file and notice that your edits are gone\n\n\n\nAs you just experienced, this command leads to data loss.\nThose last edits are gone and unrecoverable. Be very careful when using this!"
  },
  {
    "objectID": "git/undo.html#reverting",
    "href": "git/undo.html#reverting",
    "title": "Undoing",
    "section": "Reverting",
    "text": "Reverting\n\nThe working directory must be clean before you can use git revert.\n\ngit revert creates a new commit which reverses the effect of past commit(s).\n\nTo revert the last commit (current location of HEAD):\n\ngit revert HEAD\n\nYou can use the hash of the last commit instead of HEAD.\n\n\nTo revert the last two commits:\n\ngit revert HEAD~\n\nHEAD~ is equivalent to HEAD~1 and means the commit before the one HEAD is on.\nHere too of course, you can use the hash of the commit before last instead of HEAD~.\n\n\nTo revert the last three commits:\n\ngit revert HEAD~2"
  },
  {
    "objectID": "julia/hpc_distributed.html",
    "href": "julia/hpc_distributed.html",
    "title": "Distributed computing",
    "section": "",
    "text": "Julia supports distributed computing thanks to the module Distributed from its standard library.\nThere are two ways to launch several Julia processes (called “workers”):\n\n\nJulia can be started with the -p flag followed by the number of workers by running (in a terminal):\njulia -p n\nThis launches n workers, available for parallel computations, in addition to the process running the interactive prompt (so there are n + 1 Julia processes in total).\nThe module Distributed is needed whenever you want to use several workers, but the -p flag loads it automatically.\n\nExample:\n\njulia -p 4\nWithin Julia, you can see how many workers are running with:\nnworkers()\nThe total number of processes can be seen with:\nnprocs()\n\n\n\nAlternatively, workers can be started from within a Julia session. In this case, you need to load the module Distributed explicitly:\nusing Distributed\nTo launch n workers:\naddprocs(n)\n\nExample:\n\naddprocs(4)"
  },
  {
    "objectID": "julia/hpc_distributed.html#launching-several-julia-processes",
    "href": "julia/hpc_distributed.html#launching-several-julia-processes",
    "title": "Distributed computing",
    "section": "",
    "text": "Julia supports distributed computing thanks to the module Distributed from its standard library.\nThere are two ways to launch several Julia processes (called “workers”):\n\n\nJulia can be started with the -p flag followed by the number of workers by running (in a terminal):\njulia -p n\nThis launches n workers, available for parallel computations, in addition to the process running the interactive prompt (so there are n + 1 Julia processes in total).\nThe module Distributed is needed whenever you want to use several workers, but the -p flag loads it automatically.\n\nExample:\n\njulia -p 4\nWithin Julia, you can see how many workers are running with:\nnworkers()\nThe total number of processes can be seen with:\nnprocs()\n\n\n\nAlternatively, workers can be started from within a Julia session. In this case, you need to load the module Distributed explicitly:\nusing Distributed\nTo launch n workers:\naddprocs(n)\n\nExample:\n\naddprocs(4)"
  },
  {
    "objectID": "julia/hpc_distributed.html#managing-workers",
    "href": "julia/hpc_distributed.html#managing-workers",
    "title": "Distributed computing",
    "section": "Managing workers",
    "text": "Managing workers\nTo list all the worker process identifiers:\nworkers()\n\nThe process running the Julia prompt has id 1.\n\nTo kill a worker:\nrmprocs(&lt;pid&gt;)\nwhere &lt;pid&gt; is the process identifier of the worker you want to kill (you can kill several workers by providing a list of pids)."
  },
  {
    "objectID": "julia/hpc_distributed.html#using-workers",
    "href": "julia/hpc_distributed.html#using-workers",
    "title": "Distributed computing",
    "section": "Using workers",
    "text": "Using workers\nThere are a number of macros that are very convenient here:\n\nTo execute an expression on all processes, there is @everywhere\n\nFor instance, if your parallel code requires a module or an external package to run, you need to load that module or package with @everywhere:\n@everywhere using DataFrames\nIf the parallel code requires a script to run:\n@everywhere include(\"script.jl\")\nIf it requires a function that you are defining, you need to define it on all the workers:\n@everywhere function &lt;name&gt;(&lt;arguments&gt;)\n    &lt;body&gt;\nend\n\nTo assign a task to a particular worker, you use @spawnat\n\nThe first argument indicates the process id, the second argument is the expression that should be evaluated:\n@spawnat &lt;pid&gt; &lt;expression&gt;\n@spawnat returns of Future: the placeholder for a computation of unknown status and time. The function fetch waits for a Future to complete and returns the result of the computation.\n\nExample:\n\nThe function myid gives the id of the current process. As I mentioned earlier, the process running the interactive Julia prompt has the pid 1. So myid() normally returns 1.\nBut we can “spawn” myid on one of the worker, for instance the first worker (so pid 2):\n@spawnat 2 myid()\nAs you can see, we get a Future as a result. But if we pass it through fetch, we get the result of myid ran on the worker with pid 2:\nfetch(@spawnat 2 myid())\nIf you want tasks to be assigned to any worker automatically, you can pass the symbol :any to @spawnat instead of the worker id:\n@spawnat :any myid()\nTo get the result:\nfetch(@spawnat :any myid())\nIf you run this multiple times, you will see that myid is run on any of your available workers. This will however never return 1, except when you only have one running Julia process (in that case, the process running the prompt is considered a worker)."
  },
  {
    "objectID": "julia/hpc_multithreading.html",
    "href": "julia/hpc_multithreading.html",
    "title": "Multi-threading",
    "section": "",
    "text": "Julia, which was built with efficiency in mind, aimed from the start to have parallel programming abilities. These however came gradually: first, there were coroutines, which is not parallel programming, but allows independent executions of elements of code; then there was a macro allowing for loops to run on several cores, but this would not work on nested loops and it did not integrate with the coroutines or I/O. With version 1.3 however multi-threading capabilities were born.\nWhat is great about Julia’s new task parallelism is that it is incredibly easy to use: no need to write low-level code as with MPI to set where tasks are run. Everything is automatic."
  },
  {
    "objectID": "julia/hpc_multithreading.html#launching-julia-on-multiple-threads",
    "href": "julia/hpc_multithreading.html#launching-julia-on-multiple-threads",
    "title": "Multi-threading",
    "section": "Launching Julia on multiple threads",
    "text": "Launching Julia on multiple threads\nTo use Julia with multiple threads, we need to launch julia with the JULIA_NUM_THREADS environment variable or with the flag --threads/-t:\n$ JULIA_NUM_THREADS=n julia\nor\n$ julia -t n\nFirst, we need to know how many threads we actually have on our machine.\nThere are many Linux tools for this, but here are two particularly convenient options:\n# To get the total number of available processes\n$ nproc\n\n# For more information (# of sockets, cores per socket, threads per core)\n$ lscpu | grep -E '(S|s)ocket|Thread|^CPU\\(s\\)'\nSince I have 4 available processes (2 cores with 2 threads each), I can launch Julia on 4 threads:\n$ JULIA_NUM_THREADS=4 julia\nThis can also be done from within the Juno IDE.\nTo see how many threads we are using, as well as the ID of the current thread, you can run:\nThreads.nthreads()\nThreads.threadid()"
  },
  {
    "objectID": "julia/hpc_multithreading.html#for-loops-on-multiple-threads",
    "href": "julia/hpc_multithreading.html#for-loops-on-multiple-threads",
    "title": "Multi-threading",
    "section": "For loops on multiple threads",
    "text": "For loops on multiple threads\n\n\nYour turn:\n\nLaunch Julia on 1 thread and run the function below. Then run Julia on the maximum number of threads you have on your machine and run the same function.\n\nThreads.@threads for i = 1:10\n    println(\"i = $i on thread $(Threads.threadid())\")\nend\nUtilities such as htop allow you to visualize the working threads."
  },
  {
    "objectID": "julia/hpc_multithreading.html#generalization-of-multi-threading",
    "href": "julia/hpc_multithreading.html#generalization-of-multi-threading",
    "title": "Multi-threading",
    "section": "Generalization of multi-threading",
    "text": "Generalization of multi-threading\nLet’s consider the example presented in a Julia blog post in July 2019.\nBoth scripts sort a one dimensional array of 20,000,000 floats between 0 and 1, one with parallelism and one without.\nScript 1, without parallelism: sort.jl.\n# Create one dimensional array of 20,000,000 floats between 0 and 1\na = rand(20000000);\n\n# Use the MergeSort algorithm of the sort function\n# (in the standard Julia Base library)\nb = copy(a); @time sort!(b, alg = MergeSort);\n\n# Let's run the function a second time to remove the effect\n# of the initial compilation\nb = copy(a); @time sort!(b, alg = MergeSort);\nScript 2, with parallelism: psort.jl.\nimport Base.Threads.@spawn\n\n# The psort function is the same as the MergeSort algorithm\n# of the Base sort function with the addition of\n# the @spawn macro on one of the recursive calls\n\n# Sort the elements of `v` in place, from indices `lo` to `hi` inclusive\n\nfunction psort!(v, lo::Int=1, hi::Int = length(v))\n    \n    # 1 or 0 elements: nothing to do\n    if lo &gt;= hi\n        return v\n    end\n    \n    # Below some cutoff: run in serial\n    if hi - lo &lt; 100000\n        sort!(view(v, lo:hi), alg = MergeSort)\n        return v\n    end\n    \n    # Find the midpoint\n    mid = (lo + hi) &gt;&gt;&gt; 1\n    \n    # Task to sort the lower half\n    # will run in parallel with the current call sorting the upper half\n    half = @spawn psort!(v, lo, mid)\n    psort!(v, mid + 1, hi)\n    # Wait for the lower half to finish\n    wait(half)\n\n    # Workspace for merging\n    temp = v[lo:mid]\n    \n    # Merge the two sorted sub-arrays\n    i, k, j = 1, lo, mid + 1\n    @inbounds while k &lt; j &lt;= hi\n        if v[j] &lt; temp[i]\n            v[k] = v[j]\n            j += 1\n        else\n            v[k] = temp[i]\n            i += 1\n        end\n        k += 1\n    end\n    @inbounds while k &lt; j\n        v[k] = temp[i]\n        k += 1\n        i += 1\n    end\n    \n    return v\nend\n\na = rand(20000000);\n\n# Now, let's use our function\nb = copy(a); @time psort!(b);\n\n# And running it a second time to remove\n# the effect of the initial compilation\nb = copy(a); @time psort!(b);\nNow, we can test both scripts with one or multiple threads.\n\nSingle thread, non-parallel script:\n\n$ julia /path/to/sort.jl\n2.234024 seconds (111.88 k allocations: 82.489 MiB, 0.21% gc time)\n2.158333 seconds (11 allocations: 76.294 MiB, 0.51% gc time)\n\nNote the lower time for the 2nd run due to pre-compilation.\n\n\nSingle thread, parallel script:\n\n$ julia /path/to/psort.jl\n2.748138 seconds (336.77 k allocations: 703.200 MiB, 2.24% gc time)\n2.438032 seconds (3.58 k allocations: 686.932 MiB, 0.27% gc time)\n\nEven longer time: normal, there was more to run (import package, read function).\n\n\n2 threads, non-parallel script:\n\n$ JULIA_NUM_THREADS=2 julia /path/to/sort.jl\n2.233720 seconds (111.87 k allocations: 82.145 MiB, 0.21% gc time)\n2.155232 seconds (11 allocations: 76.294 MiB, 0.54% gc time)\n\nRemarkably similar to the single thread: the addition of a thread did not change anything.\n\n\n2 threads, parallel script:\n\n$ JULIA_NUM_THREADS=2 julia /path/to/psort.jl\n1.773643 seconds (336.99 k allocations: 703.171 MiB, 4.08% gc time)\n1.460539 seconds (3.79 k allocations: 686.935 MiB, 0.47% gc time)\n\n33% faster.\nNot twice as fast as one could have hoped since processes have to wait for each other. But that’s a good improvement.\n\n\n4 threads, non-parallel script:\n\n$ JULIA_NUM_THREADS=4 julia /path/to/sort.jl\n2.231717 seconds (111.87 k allocations: 82.145 MiB, 0.21% gc time)\n2.153509 seconds (11 allocations: 76.294 MiB, 0.53% gc time)\n\nAgain: same result as the single thread.\n\n\n4 threads, parallel script:\n\n$ JULIA_NUM_THREADS=4 julia /path/to/psort.jl\n1.291714 seconds (336.98 k allocations: 703.171 MiB, 3.48% gc time)\n1.194282 seconds (3.78 k allocations: 686.935 MiB, 5.19% gc time)\n\nEven though we only split our code in 2 tasks, there is still an improvement over the 2 thread run."
  },
  {
    "objectID": "julia/index.html",
    "href": "julia/index.html",
    "title": "Julia",
    "section": "",
    "text": "Getting started with Julia\nAn introductory course to programming in Julia\n\n\n\n\nHigh-performance Julia\nAn HPC course in Julia\n\n\n\n\n\n \n\n\n \n\n\n\n\n\nWebinars\n60 min webinars on various Julia topics"
  },
  {
    "objectID": "julia/intro_collections.html",
    "href": "julia/intro_collections.html",
    "title": "Collections",
    "section": "",
    "text": "Values can be stored in collections. This workshop introduces tuples, dictionaries, sets, and arrays in Julia."
  },
  {
    "objectID": "julia/intro_collections.html#tuples",
    "href": "julia/intro_collections.html#tuples",
    "title": "Collections",
    "section": "Tuples",
    "text": "Tuples\nTuples are immutable, indexable, and possibly heterogeneous collections of elements. The order of elements matters.\n\n# Possibly heterogeneous (values can be of different types)\ntypeof((2, 'a', 1.0, \"test\"))\n\nTuple{Int64, Char, Float64, String}\n\n\n\n# Indexable (note that indexing in Julia starts with 1)\nx = (2, 'a', 1.0, \"test\");\nx[3]\n\n1.0\n\n\n\n# Immutable (they cannot be modified)\n# So this returns an error\nx[3] = 8\n\nLoadError: MethodError: no method matching setindex!(::Tuple{Int64, Char, Float64, String}, ::Int64, ::Int64)\n\n\n\nNamed tuples\nTuples can have named components:\n\ntypeof((a=2, b='a', c=1.0, d=\"test\"))\n\nNamedTuple{(:a, :b, :c, :d), Tuple{Int64, Char, Float64, String}}\n\n\n\nx = (a=2, b='a', c=1.0, d=\"test\");\nx.c\n\n1.0"
  },
  {
    "objectID": "julia/intro_collections.html#dictionaries",
    "href": "julia/intro_collections.html#dictionaries",
    "title": "Collections",
    "section": "Dictionaries",
    "text": "Dictionaries\nJulia also has dictionaries: associative collections of key/value pairs:\n\nx = Dict(\"Name\"=&gt;\"Roger\", \"Age\"=&gt;52, \"Index\"=&gt;0.3)\n\nDict{String, Any} with 3 entries:\n  \"Index\" =&gt; 0.3\n  \"Age\"   =&gt; 52\n  \"Name\"  =&gt; \"Roger\"\n\n\n\"Name\", \"Age\", and \"Index\" are the keys; \"Roger\", 52, and 0.3 are the values.\nThe =&gt; operator is the same as the Pair function:\n\np = \"foo\" =&gt; 7\n\n\"foo\" =&gt; 7\n\n\n\nq = Pair(\"bar\", 8)\n\n\"bar\" =&gt; 8\n\n\nDictionaries can be heterogeneous (as in this example) and the order doesn’t matter. They are also indexable:\n\nx[\"Name\"]\n\n\"Roger\"\n\n\nAnd mutable (they can be modified):\n\nx[\"Name\"] = \"Alex\";\nx\n\nDict{String, Any} with 3 entries:\n  \"Index\" =&gt; 0.3\n  \"Age\"   =&gt; 52\n  \"Name\"  =&gt; \"Alex\""
  },
  {
    "objectID": "julia/intro_collections.html#sets",
    "href": "julia/intro_collections.html#sets",
    "title": "Collections",
    "section": "Sets",
    "text": "Sets\nSets are collections without duplicates. The order of elements doesn’t matter.\n\nset1 = Set([9, 4, 8, 2, 7, 8])\n\nSet{Int64} with 5 elements:\n  4\n  7\n  2\n  9\n  8\n\n\n\nNotice how this is a set of 5 (and not 6) elements: the duplicated 8 didn’t matter.\n\n\nset2 = Set([10, 2, 3])\n\nSet{Int64} with 3 elements:\n  2\n  10\n  3\n\n\nYou can compare sets:\n\n# The union is the set of elements that are in one OR the other set\nunion(set1, set2)\n\nSet{Int64} with 7 elements:\n  4\n  7\n  2\n  10\n  9\n  8\n  3\n\n\n\n# The intersect is the set of elements that are in one AND the other set\nintersect(set1, set2)\n\nSet{Int64} with 1 element:\n  2\n\n\n\n# The setdiff is the set of elements that are in the first set but not in the second\n# Note that the order matters here\nsetdiff(set1, set2)\n\nSet{Int64} with 4 elements:\n  4\n  7\n  9\n  8\n\n\nSets can be heterogeneous:\n\nSet([\"test\", 9, :a])\n\nSet{Any} with 3 elements:\n  :a\n  \"test\"\n  9"
  },
  {
    "objectID": "julia/intro_collections.html#arrays",
    "href": "julia/intro_collections.html#arrays",
    "title": "Collections",
    "section": "Arrays",
    "text": "Arrays\n\nVectors\nUnidimensional arrays in Julia are called vectors.\n\nVectors of one element\n\n[3]\n\n1-element Vector{Int64}:\n 3\n\n\n\n[3.4]\n\n1-element Vector{Float64}:\n 3.4\n\n\n\n[\"Hello, World!\"]\n\n1-element Vector{String}:\n \"Hello, World!\"\n\n\n\n\nVectors of multiple elements\n\n[3, 4]\n\n2-element Vector{Int64}:\n 3\n 4\n\n\n\n\n\nTwo dimensional arrays\n\n[3 4]\n\n1×2 Matrix{Int64}:\n 3  4\n\n\n\n[[1, 3] [1, 2]]\n\n2×2 Matrix{Int64}:\n 1  1\n 3  2\n\n\n\n\nSyntax subtleties\nThese 3 syntaxes are equivalent:\n\n[2 4 8]\n\n1×3 Matrix{Int64}:\n 2  4  8\n\n\n\nhcat(2, 4, 8)\n\n1×3 Matrix{Int64}:\n 2  4  8\n\n\n\ncat(2, 4, 8, dims=2)\n\n1×3 Matrix{Int64}:\n 2  4  8\n\n\nThese 4 syntaxes are equivalent:\n\n[2\n 4\n 8]\n\n3-element Vector{Int64}:\n 2\n 4\n 8\n\n\n\n[2; 4; 8]\n\n3-element Vector{Int64}:\n 2\n 4\n 8\n\n\n\nvcat(2, 4, 8)\n\n3-element Vector{Int64}:\n 2\n 4\n 8\n\n\n\ncat(2, 4, 8, dims=1)\n\n3-element Vector{Int64}:\n 2\n 4\n 8\n\n\nElements separated by semi-colons or end of lines get expanded vertically.\nThose separated by commas do not get expanded.\nElements separated by spaces or tabs get expanded horizontally.\n\n\nYour turn:\n\nCompare the outputs of the following:\n\n\n[1:2; 3:4]\n\n4-element Vector{Int64}:\n 1\n 2\n 3\n 4\n\n\n\n[1:2\n 3:4]\n\n4-element Vector{Int64}:\n 1\n 2\n 3\n 4\n\n\n\n[1:2, 3:4]\n\n2-element Vector{UnitRange{Int64}}:\n 1:2\n 3:4\n\n\n\n[1:2 3:4]\n\n2×2 Matrix{Int64}:\n 1  3\n 2  4\n\n\n\n\nArrays and types\nIn Julia, arrays can be heterogeneous:\n\n[3, \"hello\"]\n\n2-element Vector{Any}:\n 3\n  \"hello\"\n\n\nThis is possible because all elements of an array, no matter of what types, will always sit below the Any type in the type hierarchy.\n\n\nInitializing arrays\nBelow are examples of some of the functions initializing arrays:\n\nrand(2, 3, 4)\n\n2×3×4 Array{Float64, 3}:\n[:, :, 1] =\n 0.608682  0.870391  0.919705\n 0.142666  0.201765  0.42284\n\n[:, :, 2] =\n 0.718718  0.476194  0.193151\n 0.33589   0.105283  0.120097\n\n[:, :, 3] =\n 0.122514  0.826952  0.478474\n 0.283888  0.763243  0.862669\n\n[:, :, 4] =\n 0.0550924  0.782833  0.212167\n 0.654787   0.849256  0.644649\n\n\n\nrand(Int64, 2, 3, 4)\n\n2×3×4 Array{Int64, 3}:\n[:, :, 1] =\n -8474860825604754498  -6226430925287724295  2415094125240023278\n  7544132194979483706  -4542581165044106824   -24231648886856364\n\n[:, :, 2] =\n 7732338054494778164  -5929228757282742050  -5721179933272284503\n 6985741582445420974   -756768848276574443  -5430029040828342593\n\n[:, :, 3] =\n -4675580880471131459  6295173927111113061   6260458942923632311\n  5893261738072118021  4275472863029230681  -7288476099620063487\n\n[:, :, 4] =\n  5188232565506242835  -2195976271385440249  -6288873933558964786\n -1265067893838436840  -5700842252172693739   -324883157698223630\n\n\n\nzeros(Int64, 2, 5)\n\n2×5 Matrix{Int64}:\n 0  0  0  0  0\n 0  0  0  0  0\n\n\n\nones(2, 5)\n\n2×5 Matrix{Float64}:\n 1.0  1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0  1.0\n\n\n\nreshape([1, 2, 4, 2], (2, 2))\n\n2×2 Matrix{Int64}:\n 1  4\n 2  2\n\n\n\nfill(\"test\", (2, 2))\n\n2×2 Matrix{String}:\n \"test\"  \"test\"\n \"test\"  \"test\"\n\n\n\n\nBroadcasting\nTo apply a function to each element of a collection rather than to the collection as a whole, Julia uses broadcasting.\n\na = [-3, 2, -5]\n\n3-element Vector{Int64}:\n -3\n  2\n -5\n\n\nabs(a)\nLoadError: MethodError: no method matching abs(::Vector{Int64})\nThis doesn’t work because the function abs only applies to single elements.\nBy broadcasting abs, you apply it to each element of a:\n\nbroadcast(abs, a)\n\n3-element Vector{Int64}:\n 3\n 2\n 5\n\n\nThe dot notation is equivalent:\n\nabs.(a)\n\n3-element Vector{Int64}:\n 3\n 2\n 5\n\n\nIt can also be applied to the pipe, to unary and binary operators, etc.\n\na .|&gt; abs\n\n3-element Vector{Int64}:\n 3\n 2\n 5\n\n\n\n\nYour turn:\n\nTry to understand the difference between the following 2 expressions:\n\n\nabs.(a) == a .|&gt; abs\n\ntrue\n\n\n\nabs.(a) .== a .|&gt; abs\n\n3-element BitVector:\n 1\n 1\n 1\n\n\n\nHint: 0/1 are a short-form notations for false/true in arrays of Booleans.\n\n\n\nComprehensions\nJulia has an array comprehension syntax similar to Python’s:\n\n[ 3i + j for i=1:10, j=3 ]\n\n10-element Vector{Int64}:\n  6\n  9\n 12\n 15\n 18\n 21\n 24\n 27\n 30\n 33"
  },
  {
    "objectID": "julia/intro_collections.html#indexing",
    "href": "julia/intro_collections.html#indexing",
    "title": "Collections",
    "section": "Indexing",
    "text": "Indexing\nAs in other mathematically oriented languages such as R, Julia starts indexing at 1.\nIndexing is done with square brackets:\n\na = [1 2; 3 4]\n\n2×2 Matrix{Int64}:\n 1  2\n 3  4\n\n\n\na[1, 1]\n\n1\n\n\n\na[1, :]\n\n2-element Vector{Int64}:\n 1\n 2\n\n\n\na[:, 1]\n\n2-element Vector{Int64}:\n 1\n 3\n\n\n\n# Here, we are indexing a tuple\n(2, 4, 1.0, \"test\")[2]\n\n4\n\n\n\n\nYour turn:\n\nIndex the element on the 3rd row and 2nd column of b:\n\nb = [\"wrong\" \"wrong\" \"wrong\"; \"wrong\" \"wrong\" \"wrong\"; \"wrong\" \"you got it\" \"wrong\"]\n\n3×3 Matrix{String}:\n \"wrong\"  \"wrong\"       \"wrong\"\n \"wrong\"  \"wrong\"       \"wrong\"\n \"wrong\"  \"you got it\"  \"wrong\"\n\n\n\n\n\nYour turn:\n\na = [1 2; 3 4]\na[1, 1]\na[1, :]\nHow can I get the second column?\nHow can I get the tuple (2, 4)? (a tuple is a list of elements)\n\nAs in Python, by default, arrays are passed by sharing:\n\na = [1, 2, 3];\na[1] = 0;\na\n\n3-element Vector{Int64}:\n 0\n 2\n 3\n\n\nThis prevents the unwanted copying of arrays."
  },
  {
    "objectID": "julia/intro_functions.html",
    "href": "julia/intro_functions.html",
    "title": "Functions",
    "section": "",
    "text": "Functions are objects containing a set of instructions.\nWhen you pass a tuple of argument(s) (possibly an empty tuple) to them, you get one or more values as output."
  },
  {
    "objectID": "julia/intro_functions.html#operators",
    "href": "julia/intro_functions.html#operators",
    "title": "Functions",
    "section": "Operators",
    "text": "Operators\nOperators are functions and can be written in a way that shows the tuple of arguments more explicitly.\n\nFor instance, you can use the addition operator (+) in 2 ways:\n\n\n3 + 2\n+(3, 2)\n\n5\n\n\nThe multiplication operator can be omitted when this does not create any ambiguity:\n\na = 3;\n2a\n\n6\n\n\nJulia has “assignment by operation” operators:\n\na = 2;\na += 7    # this is the same as a = a + 7\n\n9\n\n\nThere is a left division operator:\n\n2\\8 == 8/2\n\ntrue\n\n\nJulia supports fraction operations:\n\n4//8\n\n1//2\n\n\n\n1//2 + 3//4\n\n5//4"
  },
  {
    "objectID": "julia/intro_functions.html#function-definition",
    "href": "julia/intro_functions.html#function-definition",
    "title": "Functions",
    "section": "Function definition",
    "text": "Function definition\nThere are 2 ways to define a new function:\n\nLong form\nfunction &lt;name&gt;(&lt;arguments&gt;)\n    &lt;body&gt;\nend\n\nExample:\n\n\nfunction hello1()\n    println(\"Hello\")\nend\n\nhello1 (generic function with 1 method)\n\n\n\n\nAssignment form\n&lt;name&gt;(&lt;arguments&gt;) = &lt;body&gt;\n\nExample:\n\n\nhello1() = println(\"Hello\")\n\nhello1 (generic function with 1 method)\n\n\nThe function hello1 defined with this terse syntax is exactly the same as the one we defined above.\n\n\nStylistic convention\nJulia suggests to use lower case without underscores as function names when the name is readable enough."
  },
  {
    "objectID": "julia/intro_functions.html#calling-functions",
    "href": "julia/intro_functions.html#calling-functions",
    "title": "Functions",
    "section": "Calling functions",
    "text": "Calling functions\nSince you pass a tuple to a function when you run it, you call a function by appending parentheses to its name:\n\nhello1()\n\nHello\n\n\n\nHere, our function does not take any argument, so the tuple is empty."
  },
  {
    "objectID": "julia/intro_functions.html#arguments",
    "href": "julia/intro_functions.html#arguments",
    "title": "Functions",
    "section": "Arguments",
    "text": "Arguments\n\nNo argument\nOur function hello1 does not accept any argument. If we pass an argument, we get an error message:\nhello1(\"Bob\")\nLoadError: MethodError: no method matching hello1(::String)\n\n\nOne argument\nTo define a function which accepts an argument, we need to add a placeholder for it in the function definition.\n\nSo let’s try this:\n\n\nfunction hello2(name)\n    println(\"Hello name\")\nend\n\nhello2 (generic function with 1 method)\n\n\n\nhello2(\"Bob\")\n\nHello name\n\n\nMmm … not quite … this function works but does not give the result we wanted.\nHere, we need to use string interpolation:\n\nfunction hello3(name)\n    println(\"Hello $name\")\nend\n\nhello3 (generic function with 1 method)\n\n\n$name in the body of the function points to name in the tuple of argument.\nWhen we run the function, $name is replaced by the value we used in lieu of name in the function definition:\n\nhello3(\"Bob\")\n\nHello Bob\n\n\nHere is the corresponding assignment form for hello3:\n\nhello3(name) = println(\"Hello $name\")\n\nhello3 (generic function with 1 method)\n\n\n\nNote that this dollar sign is only required with strings. Here is an example with integers:\n\n\nfunction addTwo(a)\n    a + 2\nend\n\naddTwo (generic function with 1 method)\n\n\nAnd the corresponding assignment form:\n\naddTwo(a) = a + 2\n\naddTwo (generic function with 1 method)\n\n\n\naddTwo(4)\n\n6\n\n\n\n\nMultiple arguments\nNow, let’s write a function which accepts 2 arguments. For this, we put 2 placeholders in the tuple passed to the function in the function definition:\n\nfunction hello4(name1, name2)\n    println(\"Hello $name1 and $name2\")\nend\n\nhello4 (generic function with 1 method)\n\n\nThis means that this function expects a tuple of 2 values:\n\nhello4(\"Bob\", \"Pete\")\n\nHello Bob and Pete\n\n\n\n\nYour turn:\n\nSee what happens when you pass no argument, a single argument, or three arguments to this function.\n\n\n\nDefault arguments\nYou can set a default value for some or all arguments. In this case, the function will run with or without a value passed for those arguments. If no value is given, the default is used. If a value is given, it will replace the default.\n\nExample:\n\n\nfunction hello5(name=\"\")\n    println(\"Hello $name\")\nend\n\nhello5 (generic function with 2 methods)\n\n\n\nhello5()\n\nHello \n\n\n\nhello5(\"Bob\")\n\nHello Bob\n\n\n\nAnother example:\n\n\nfunction addSomethingOrTwo(a, b=2)\n    a + b\nend\n\naddSomethingOrTwo (generic function with 2 methods)\n\n\n\naddSomethingOrTwo(3)\n\n5\n\n\n\naddSomethingOrTwo(3, 4)\n\n7"
  },
  {
    "objectID": "julia/intro_functions.html#returning-the-result",
    "href": "julia/intro_functions.html#returning-the-result",
    "title": "Functions",
    "section": "Returning the result",
    "text": "Returning the result\nIn Julia, functions return the value(s) of the last expression automatically. If you want to return something else instead, you need to use the return statement. This causes the function to exit early.\n\nLook at these 5 functions:\n\nfunction test1(x, y)\n    x + y\nend\n\nfunction test2(x, y)\n    return x + y\nend\n\nfunction test3(x, y)\n    x * y\n    x + y\nend\n\nfunction test4(x, y)\n    return x * y\n    x + y\nend\n\nfunction test5(x, y)\n    return x * y\n    return x + y\nend\n\nfunction test6(x, y)\n    x * y, x + y\nend\n\n\nYour turn:\n\nWithout running the code, try to guess the outputs of:\n\ntest1(1, 2)\ntest2(1, 2)\ntest3(1, 2)\ntest4(1, 2)\ntest5(1, 2)\ntest6(1, 2)\n\n\nYour turn:\n\nNow, run the code and draw some conclusions on the behaviour of the return statement."
  },
  {
    "objectID": "julia/intro_functions.html#anonymous-functions",
    "href": "julia/intro_functions.html#anonymous-functions",
    "title": "Functions",
    "section": "Anonymous functions",
    "text": "Anonymous functions\nAnonymous functions are functions which aren’t given a name:\nfunction (&lt;arguments&gt;)\n    &lt;body&gt;\nend\nIn compact form:\n&lt;arguments&gt; -&gt; &lt;body&gt;\n\nExample:\n\n\nfunction (name)\n    println(\"Hello $name\")\nend\n\n#13 (generic function with 1 method)\n\n\nCompact form:\n\nname -&gt; println(\"Hello $name\")\n\n#15 (generic function with 1 method)\n\n\n\nWhen would you want to use anonymous functions?\nThis is very useful for functional programming (when you apply a function—for instance map—to other functions to apply them in a vectorized manner which avoids repetitions).\n\nExample:\n\n\nmap(name -&gt; println(\"Hello $name\"), [\"Bob\", \"Lucie\", \"Sophie\"]);\n\nHello Bob\nHello Lucie\nHello Sophie"
  },
  {
    "objectID": "julia/intro_functions.html#pipes",
    "href": "julia/intro_functions.html#pipes",
    "title": "Functions",
    "section": "Pipes",
    "text": "Pipes\n|&gt; is the pipe in Julia. It redirects the output of the expression on the left as the input of the expression on the right.\n\nThe following 2 expressions are equivalent:\n\nprintln(\"Hello\")\n\"Hello\" |&gt; println\n\nHere is another example:\n\n\nsqrt(2) == 2 |&gt; sqrt\n\ntrue"
  },
  {
    "objectID": "julia/intro_functions.html#function-composition",
    "href": "julia/intro_functions.html#function-composition",
    "title": "Functions",
    "section": "Function composition",
    "text": "Function composition\nYou can pass a function inside another function:\n&lt;function2&gt;(&lt;function1&gt;(&lt;arguments&gt;))\n&lt;arguments&gt; will be passed to &lt;function1&gt; and the result will then be passed to &lt;function2&gt;.\nAn equivalent syntax is to use the composition operator ∘ (in the REPL, type \\circ then press tab):\n(&lt;function2&gt; ∘ &lt;function1&gt;)(&lt;arguments&gt;)\n\nExample:\n\n\n# sum is our first function\nsum(1:3)\n\n6\n\n\n\n# sqrt is the second function\nsqrt(sum(1:3))\n\n2.449489742783178\n\n\n\n# This is equivalent\n(sqrt ∘ sum)(1:3)\n\n2.449489742783178\n\n\n\n\nYour turn:\n\nWrite three other equivalent expressions using the pipe.\n\n\nAnother example:\n\n\nexp(+(-3, 1))\n\n(exp ∘ +)(-3, 1)\n\n0.1353352832366127\n\n\n\n\nYour turn:\n\nTry to write the same expression in another 2 different ways."
  },
  {
    "objectID": "julia/intro_functions.html#mutating-functions",
    "href": "julia/intro_functions.html#mutating-functions",
    "title": "Functions",
    "section": "Mutating functions",
    "text": "Mutating functions\nFunctions usually do not modify their argument(s):\n\na = [-2, 3, -5]\n\n3-element Vector{Int64}:\n -2\n  3\n -5\n\n\n\nsort(a)\n\n3-element Vector{Int64}:\n -5\n -2\n  3\n\n\n\na\n\n3-element Vector{Int64}:\n -2\n  3\n -5\n\n\nJulia has a set of functions which modify their argument(s). By convention, their names end with !\n\nThe function sort has a mutating equivalent sort!:\n\n\nsort!(a);\na\n\n3-element Vector{Int64}:\n -5\n -2\n  3\n\n\n\nIf you write functions which modify their arguments, make sure to follow this convention too."
  },
  {
    "objectID": "julia/intro_functions.html#broadcasting",
    "href": "julia/intro_functions.html#broadcasting",
    "title": "Functions",
    "section": "Broadcasting",
    "text": "Broadcasting\nTo apply a function to each element of a collection rather than to the collection as a whole, Julia uses broadcasting.\n\nLet’s create a collection (here a tuple):\n\n\na = (2, 3)\n\n(2, 3)\n\n\n\nIf we pass a to the string function, that function applies to the whole collection:\n\n\nstring(a)\n\n\"(2, 3)\"\n\n\n\nIn contrast, we can broadcast the function string to all elements of a:\n\n\nbroadcast(string, a)\n\n(\"2\", \"3\")\n\n\n\nAn alternative syntax is to add a period after the function name:\n\n\nstring.(a)\n\n(\"2\", \"3\")\n\n\n\nHere is another example:\n\na = [-3, 2, -5]\nabs(a)\nERROR: MethodError: no method matching abs(::Array{Int64,1})\nThis doesn’t work because the function abs only applies to single elements.\nBy broadcasting abs, you apply it to each element of a:\n\nbroadcast(abs, a)\n\n(2, 3)\n\n\nThe dot notation is equivalent:\n\nabs.(a)\n\n(2, 3)\n\n\nIt can also be applied to the pipe, to unary and binary operators, etc.\n\nExample:\n\n\na .|&gt; abs\n\n(2, 3)\n\n\n\n\nYour turn:\n\nTry to understand the difference between the following 2 expressions:\n\n\nabs.(a) == a .|&gt; abs\nabs.(a) .== a .|&gt; abs\n\n(true, true)"
  },
  {
    "objectID": "julia/intro_functions.html#multiple-dispatch",
    "href": "julia/intro_functions.html#multiple-dispatch",
    "title": "Functions",
    "section": "Multiple dispatch",
    "text": "Multiple dispatch\nIn some programming languages, functions can be polymorphic (multiple versions exist under the same function name). The process of selecting which version to use is called dispatch.\nThere are multiple types of dispatch depending on the language:\n\nDynamic dispatch: the process of selecting one version of a function at run time.\nSingle dispatch: the choice of version is based on a single object.\n\n\nThis is typical of object-oriented languages such as Python, C++, Java, Smalltalk, etc.\n\n\nMultiple dispatch: the choice of version is based on the combination of all operands and their types.\n\n\nThis the case of Lisp and Julia. In Julia, these versions are called methods."
  },
  {
    "objectID": "julia/intro_functions.html#methods",
    "href": "julia/intro_functions.html#methods",
    "title": "Functions",
    "section": "Methods",
    "text": "Methods\nRunning methods(+) let’s you see that the function + has 206 methods!\nMethods can be added to existing functions.\n\n\nYour turn:\n\nRun the following and try to understand the outputs:\nabssum(x::Int64, y::Int64) = abs(x + y)\nabssum(x::Float64, y::Float64) = abs(x + y)\n\nabssum(2, 4)\nabssum(2.0, 4.0)\nabssum(2, 4.0)\nWhat could you do if you wanted the last expression to work?"
  },
  {
    "objectID": "julia/intro_macros.html",
    "href": "julia/intro_macros.html",
    "title": "Macros",
    "section": "",
    "text": "Julia code is itself data and can be manipulated by the language while it is running."
  },
  {
    "objectID": "julia/intro_macros.html#metaprogramming",
    "href": "julia/intro_macros.html#metaprogramming",
    "title": "Macros",
    "section": "Metaprogramming",
    "text": "Metaprogramming\n\nLarge influence from Lisp.\nSince Julia is entirely written in Julia, it is particularly well suited for metaprogramming."
  },
  {
    "objectID": "julia/intro_macros.html#parsing-and-evaluating",
    "href": "julia/intro_macros.html#parsing-and-evaluating",
    "title": "Macros",
    "section": "Parsing and evaluating",
    "text": "Parsing and evaluating\nLet’s start with something simple:\n\n2 + 3\n\n5\n\n\nHow is this run internally?\nThe string \"2 + 3\" gets parsed into an expression:\n\nMeta.parse(\"2 + 3\")\n\n:(2 + 3)\n\n\nThen that expression gets evaluated:\n\neval(Meta.parse(\"2 + 3\"))\n\n5"
  },
  {
    "objectID": "julia/intro_macros.html#macros",
    "href": "julia/intro_macros.html#macros",
    "title": "Macros",
    "section": "Macros",
    "text": "Macros\nThey resemble functions and just like functions, they accept as input a tuple of arguments.\nBUT macros return an expression which is compiled directly rather than requiring a runtime eval call.\nSo they execute before the rest of the code is run.\nMacro’s names are preceded by @ (e.g. @time).\nJulia comes with many macros and you can create your own with:\nmacro &lt;name&gt;()\n    &lt;body&gt;\nend"
  },
  {
    "objectID": "julia/intro_macros.html#stylistic-conventions",
    "href": "julia/intro_macros.html#stylistic-conventions",
    "title": "Macros",
    "section": "Stylistic conventions",
    "text": "Stylistic conventions\nAs with functions, Julia suggests to use lower case, without underscores, as macro names."
  },
  {
    "objectID": "julia/intro_packages.html",
    "href": "julia/intro_packages.html",
    "title": "Packages",
    "section": "",
    "text": "Julia comes with a collection of packages. In Linux, they are in /usr/share/julia/stdlib/vx.x.\nHere is the list:\nBase64\nCRC32c\nDates\nDelimitedFiles\nDistributed\nFileWatching\nFuture\nInteractiveUtils\nLibdl\nLibGit2\nLinearAlgebra\nLogging\nMarkdown\nMmap\nPkg\nPrintf\nProfile\nRandom\nREPL\nSerialization\nSHA\nSharedArrays\nSockets\nSparseArrays\nStatistics\nSuiteSparse\nTest\nUnicode\nUUIDs"
  },
  {
    "objectID": "julia/intro_packages.html#standard-library",
    "href": "julia/intro_packages.html#standard-library",
    "title": "Packages",
    "section": "",
    "text": "Julia comes with a collection of packages. In Linux, they are in /usr/share/julia/stdlib/vx.x.\nHere is the list:\nBase64\nCRC32c\nDates\nDelimitedFiles\nDistributed\nFileWatching\nFuture\nInteractiveUtils\nLibdl\nLibGit2\nLinearAlgebra\nLogging\nMarkdown\nMmap\nPkg\nPrintf\nProfile\nRandom\nREPL\nSerialization\nSHA\nSharedArrays\nSockets\nSparseArrays\nStatistics\nSuiteSparse\nTest\nUnicode\nUUIDs"
  },
  {
    "objectID": "julia/intro_packages.html#installing-additional-packages",
    "href": "julia/intro_packages.html#installing-additional-packages",
    "title": "Packages",
    "section": "Installing additional packages",
    "text": "Installing additional packages\nYou can install additional packages.\nThese go to your personal library in ~/.julia (this is also where your REPL history is saved).\nAll registered packages are on GitHub and can easily be searched here.\nThe GitHub star system allows you to easily judge the popularity of a package and to see whether it is under current development.\nIn addition to these, there are unregistered packages and you can build your own.\n\n\nYour turn:\n\nTry to find a list of popular plotting packages.\n\nYou can manage your personal library easily in package mode with the commands:\n(env) pkg&gt; add &lt;package&gt;   # install &lt;package&gt;\n(env) pkg&gt; rm &lt;package&gt;    # uninstall &lt;package&gt;\n(env) pkg&gt; up &lt;package&gt;    # upgrade &lt;package&gt;\n(env) pkg&gt; st              # st or status: list installed packages\n(env) pkg&gt; up              # up or upgrade: upgrade all packages\n\nReplace &lt;package&gt; by the name of the package (e.g. Plots ).\n\nYou can install, uninstall, or update several packages at once by listing them with a space:\n(env) pkg&gt; add &lt;package1&gt; &lt;package2&gt; &lt;package3&gt;\nAn alternative to this convenience mode is to load the package manager (package Pkg, part of stdlib) and use it as you would any other package:\nusing Pkg\n\nPkg.add(\"&lt;package&gt;\")        # install &lt;package&gt;\nPkg.rm(\"&lt;package&gt;\")         # uninstall &lt;package&gt;\nPkg.status(\"&lt;package&gt;\")     # status of &lt;package&gt;\nPkg.update(\"&lt;package&gt;\")     # update &lt;package&gt;\nPkg.update()                # status of all installed packages\nPkg.status()                # update all packages\n\nThe short forms up and st do not work in this context.\n\nTo install, uninstall, or update several packages at once in this context, you need to create an array:\nPkg.add([\"&lt;package1&gt;\", \"&lt;package2&gt;\", \"&lt;package3&gt;\"])\n\n\nYour turn:\n\nCheck your list of packages; install the packages Plots, GR, Distributions, StatsPlots, and UnicodePlot; then check that list again.\n\n\n\nYour turn:\n\nNow go explore your ~/.julia. If you don’t find it, make sure that your file explorer allows you to see hidden files."
  },
  {
    "objectID": "julia/intro_packages.html#loading-packages",
    "href": "julia/intro_packages.html#loading-packages",
    "title": "Packages",
    "section": "Loading packages",
    "text": "Loading packages\nWhether a package from the standard library or one you installed, before you can use a package you need to load it. This has to be done at each new Julia session so the code to load packages should be part of your scripts.\nThis is done with the using command (e.g. using Plots)."
  },
  {
    "objectID": "julia/intro_resources.html",
    "href": "julia/intro_resources.html",
    "title": "Resources",
    "section": "",
    "text": "Official Julia website\nOfficial Julia manual\nAlliance wiki Julia page\nOnline training material\nThe Julia YouTube channel\nThe Julia Wikibook\nA blog aggregator for Julia"
  },
  {
    "objectID": "julia/intro_resources.html#documentation",
    "href": "julia/intro_resources.html#documentation",
    "title": "Resources",
    "section": "",
    "text": "Official Julia website\nOfficial Julia manual\nAlliance wiki Julia page\nOnline training material\nThe Julia YouTube channel\nThe Julia Wikibook\nA blog aggregator for Julia"
  },
  {
    "objectID": "julia/intro_resources.html#getting-help",
    "href": "julia/intro_resources.html#getting-help",
    "title": "Resources",
    "section": "Getting help",
    "text": "Getting help\n\nDiscourse forum\n[julia] tag on Stack Overflow\nSlack team (you need to agree to the community code of conduct at slackinvite.julialang.org to receive an invitation)\n#julialang hashtag on Twitter\nSubreddit\nGitter channel\n#julia IRC channel on Freenode"
  },
  {
    "objectID": "julia/intro_types.html",
    "href": "julia/intro_types.html",
    "title": "Types",
    "section": "",
    "text": "Type safety (catching errors of inadequate type) performed at compilation time.\n\nExamples: C, C++, Java, Fortran, Haskell.\n\n\n\n\nType safety performed at runtime.\n\nExamples: Python, JavaScript, PHP, Ruby, Lisp.\n\n\n\n\n\nJulia type system is dynamic (types are unknown until runtime), but types can be declared, optionally bringing the advantages of static type systems.\nThis gives users the freedom to choose between an easy and convenient language, or a clearer, faster, and more robust one (or a combination of the two)."
  },
  {
    "objectID": "julia/intro_types.html#types-systems",
    "href": "julia/intro_types.html#types-systems",
    "title": "Types",
    "section": "",
    "text": "Type safety (catching errors of inadequate type) performed at compilation time.\n\nExamples: C, C++, Java, Fortran, Haskell.\n\n\n\n\nType safety performed at runtime.\n\nExamples: Python, JavaScript, PHP, Ruby, Lisp.\n\n\n\n\n\nJulia type system is dynamic (types are unknown until runtime), but types can be declared, optionally bringing the advantages of static type systems.\nThis gives users the freedom to choose between an easy and convenient language, or a clearer, faster, and more robust one (or a combination of the two)."
  },
  {
    "objectID": "julia/intro_types.html#julia-types-a-hierarchical-tree",
    "href": "julia/intro_types.html#julia-types-a-hierarchical-tree",
    "title": "Types",
    "section": "Julia types: a hierarchical tree",
    "text": "Julia types: a hierarchical tree\nAt the bottom:  concrete types.\nAbove:     abstract types (concepts for collections of concrete types).\nAt the top:     the Any type, encompassing all types.\n\n\nFrom O’Reilly\n\nOne common type missing in this diagram is the boolean type.\nIt is a subtype of the integer type, as can be tested with the subtype operator &lt;:\n\nBool &lt;: Integer\n\ntrue\n\n\nIt can also be made obvious by the following:\n\nfalse == 0\n\ntrue\n\n\n\ntrue == 1\n\ntrue\n\n\n\na = true;\nb = false;\n3a + 2b\n\n3"
  },
  {
    "objectID": "julia/intro_types.html#optional-type-declaration",
    "href": "julia/intro_types.html#optional-type-declaration",
    "title": "Types",
    "section": "Optional type declaration",
    "text": "Optional type declaration\nDone with ::\n&lt;value&gt;::&lt;type&gt;\n\nExample:\n\n\n2::Int\n\n2"
  },
  {
    "objectID": "julia/intro_types.html#illustration-of-type-safety",
    "href": "julia/intro_types.html#illustration-of-type-safety",
    "title": "Types",
    "section": "Illustration of type safety",
    "text": "Illustration of type safety\nThis works:\n\n2::Int\n\n2\n\n\nThis doesn’t work:\n\n2.0::Int\n\nLoadError: TypeError: in typeassert, expected Int64, got a value of type Float64\n\n\nType declaration is not yet supported on global variables; this is used in local contexts such as inside a function.\n\nExample:\n\n\nfunction floatsum(a, b)\n    (a + b)::Float64\nend\n\nfloatsum (generic function with 1 method)\n\n\nThis works:\n\nfloatsum(2.3, 1.0)\n\n3.3\n\n\nThis doesn’t work:\n\nfloatsum(2, 4)\n\nLoadError: TypeError: in typeassert, expected Float64, got a value of type Int64"
  },
  {
    "objectID": "julia/intro_types.html#information-and-conversion",
    "href": "julia/intro_types.html#information-and-conversion",
    "title": "Types",
    "section": "Information and conversion",
    "text": "Information and conversion\nThe typeof function gives the type of an object:\n\ntypeof(2)\n\nInt64\n\n\n\ntypeof(2.0)\n\nFloat64\n\n\n\ntypeof(\"Hello, World!\")\n\nString\n\n\n\ntypeof(true)\n\nBool\n\n\n\ntypeof((2, 4, 1.0, \"test\"))\n\nTuple{Int64, Int64, Float64, String}\n\n\nConversion between types is possible in some cases:\n\nInt(2.0)\n\n2\n\n\n\ntypeof(Int(2.0))\n\nInt64\n\n\n\nChar(2.0)\n\n'\\x02': ASCII/Unicode U+0002 (category Cc: Other, control)\n\n\n\ntypeof(Char(2.0))\n\nChar"
  },
  {
    "objectID": "julia/intro_types.html#stylistic-convention",
    "href": "julia/intro_types.html#stylistic-convention",
    "title": "Types",
    "section": "Stylistic convention",
    "text": "Stylistic convention\nThe names of types start with a capital letter and camel case is used in multiple-word names."
  },
  {
    "objectID": "julia/wb_flux.html",
    "href": "julia/wb_flux.html",
    "title": "Machine learning in Julia with Flux",
    "section": "",
    "text": "This webinar, aimed at users with no experience in machine learning, is an introduction to the basic concepts of neural networks, followed by a simple example—the classic classification of the MNIST database of handwritten digits—using the Julia package Flux."
  },
  {
    "objectID": "julia/wb_makie_slides.html#plotting-in-julia",
    "href": "julia/wb_makie_slides.html#plotting-in-julia",
    "title": "Makie",
    "section": "Plotting in Julia",
    "text": "Plotting in Julia\n\nMany options:\n\nPlots.jl: high-level API for working with different back-ends (GR, Pyplot, Plotly…)\nPyPlot.jl: Julia interface to Matplotlib’s matplotlib.pyplot\nPlotlyJS.jl: Julia interface to plotly.js\nPlotlyLight.jl: the fastest plotting option in Julia by far, but limited features\nGadfly.jl: following the grammar of graphics popularized by Hadley Wickham in R\nVegaLite.jl: grammar of interactive graphics\nPGFPlotsX.jl: Julia interface to the PGFPlots LaTeX package\nUnicodePlots.jl: plots in the terminal 🙂\n\n\n\n\nMakie.jl: powerful plotting ecosystem: animation, 3D, GPU optimization"
  },
  {
    "objectID": "julia/wb_makie_slides.html#makie-ecosystem",
    "href": "julia/wb_makie_slides.html#makie-ecosystem",
    "title": "Makie",
    "section": "Makie ecosystem",
    "text": "Makie ecosystem\n\n\nMain package:\n\nMakie: plots functionalities. Backend needed to render plots into images or vector graphics\n\n\n\n\n\nBackends:\n\nCairoMakie: vector graphics or high-quality 2D plots. Creates, but does not display plots (you need an IDE that does or you can use ElectronDisplay.jl)\nGLMakie: based on OpenGL; 3D rendering and interactivity in GLFW window (no vector graphics)\nWGLMakie: web version of GLMakie (plots rendered in a browser instead of a window)"
  },
  {
    "objectID": "julia/wb_makie_slides.html#extensions",
    "href": "julia/wb_makie_slides.html#extensions",
    "title": "Makie",
    "section": "Extensions",
    "text": "Extensions\n\nGeoMakie.jl add geographical plotting utilities to Makie\nAlgebraOfGraphics.jl turns plotting into a simple algebra of building blocks\nGraphMakie.jl to create network graphs"
  },
  {
    "objectID": "julia/wb_makie_slides.html#cheatsheet-2d",
    "href": "julia/wb_makie_slides.html#cheatsheet-2d",
    "title": "Makie",
    "section": "Cheatsheet 2D",
    "text": "Cheatsheet 2D\n\n\nFrom: Storopoli, Huijzer and Alonso (2021). Julia Data Science. https://juliadatascience.io. ISBN: 97984898"
  },
  {
    "objectID": "julia/wb_makie_slides.html#cheatsheet-3d",
    "href": "julia/wb_makie_slides.html#cheatsheet-3d",
    "title": "Makie",
    "section": "Cheatsheet 3D",
    "text": "Cheatsheet 3D\n\n\nFrom: Storopoli, Huijzer and Alonso (2021). Julia Data Science. https://juliadatascience.io. ISBN: 97984898"
  },
  {
    "objectID": "julia/wb_makie_slides.html#resources",
    "href": "julia/wb_makie_slides.html#resources",
    "title": "Makie",
    "section": "Resources",
    "text": "Resources\n\nOfficial documentation\nJulia Data Science book, chapter 5\nMany examples in the project Beautiful Makie"
  },
  {
    "objectID": "julia/wb_makie_slides.html#troubleshooting",
    "href": "julia/wb_makie_slides.html#troubleshooting",
    "title": "Makie",
    "section": "Troubleshooting",
    "text": "Troubleshooting\nInstalling GLMakie can be challenging. This page may lead you towards solutions\nCairoMakie and WGLMakie should install without issues"
  },
  {
    "objectID": "julia/wb_makie_slides.html#figure",
    "href": "julia/wb_makie_slides.html#figure",
    "title": "Makie",
    "section": "Figure",
    "text": "Figure\nLoad the package (here, we are using CairoMakie):\n\nusing CairoMakie                        # no need to import Makie itself\n\n\n\nCreate a Figure (container object):\n\nfig = Figure()\n\n\n\n\n\n\n\n\n\ntypeof(fig)\n\nFigure"
  },
  {
    "objectID": "julia/wb_makie_slides.html#axis",
    "href": "julia/wb_makie_slides.html#axis",
    "title": "Makie",
    "section": "Axis",
    "text": "Axis\n\n\nThen, you can create an Axis:\n\nax = Axis(Figure()[1, 1])\n\nAxis with 0 plots:\n\n\n\n\n\n\n\ntypeof(ax)\n\nAxis"
  },
  {
    "objectID": "julia/wb_makie_slides.html#plot",
    "href": "julia/wb_makie_slides.html#plot",
    "title": "Makie",
    "section": "Plot",
    "text": "Plot\nFinally, we can add a plot:\n\nfig = Figure()\nax = Axis(fig[1, 1])\nx = LinRange(-10, 10, 20)\ny = x\nscatter!(ax, x, y)  # Functions with ! transform their arguments\nfig"
  },
  {
    "objectID": "julia/wb_makie_slides.html#d",
    "href": "julia/wb_makie_slides.html#d",
    "title": "Makie",
    "section": "2D",
    "text": "2D\n\nusing CairoMakie\nusing StatsBase, LinearAlgebra\nusing Interpolations, OnlineStats\nusing Distributions\nCairoMakie.activate!(type = \"png\")\n\nfunction eq_hist(matrix; nbins = 256 * 256)\n    h_eq = fit(Histogram, vec(matrix), nbins = nbins)\n    h_eq = normalize(h_eq, mode = :density)\n    cdf = cumsum(h_eq.weights)\n    cdf = cdf / cdf[end]\n    edg = h_eq.edges[1]\n    interp_linear = LinearInterpolation(edg, [cdf..., cdf[end]])\n    out = reshape(interp_linear(vec(matrix)), size(matrix))\n    return out\nend\n\nfunction getcounts!(h, fn; n = 100)\n    for _ in 1:n\n        vals = eigvals(fn())\n        x0 = real.(vals)\n        y0 = imag.(vals)\n        fit!(h, zip(x0,y0))\n    end\nend\n\nm(;a=10rand()-5, b=10rand()-5) = [0 0 0 a; -1 -1 1 0; b 0 0 0; -1 -1 -1 -1]\n\nh = HeatMap(range(-3.5,3.5,length=1200), range(-3.5,3.5, length=1200))\ngetcounts!(h, m; n=2_000_000)\n\nwith_theme(theme_black()) do\n    fig = Figure(figure_padding=0,resolution=(600,600))\n    ax = Axis(fig[1,1]; aspect = DataAspect())\n    heatmap!(ax,-3.5..3.5, -3.5..3.5, eq_hist(h.counts); colormap = :bone_1)\n    hidedecorations!(ax)\n    hidespines!(ax)\n    fig\nend"
  },
  {
    "objectID": "julia/wb_makie_slides.html#d-output",
    "href": "julia/wb_makie_slides.html#d-output",
    "title": "Makie",
    "section": "2D",
    "text": "2D"
  },
  {
    "objectID": "julia/wb_makie_slides.html#d-1",
    "href": "julia/wb_makie_slides.html#d-1",
    "title": "Makie",
    "section": "3D",
    "text": "3D\nusing GLMakie, Random\nGLMakie.activate!()\n\nRandom.seed!(13)\nx = -6:0.5:6\ny = -6:0.5:6\nz = 6exp.( -(x.^2 .+ y' .^ 2)./4)\n\nbox = Rect3(Point3f(-0.5), Vec3f(1))\nn = 100\ng(x) = x^(1/10)\nalphas = [g(x) for x in range(0,1,length=n)]\ncmap_alpha = resample_cmap(:linear_worb_100_25_c53_n256, n, alpha = alphas)\n\nwith_theme(theme_dark()) do\n    fig, ax, = meshscatter(x, y, z;\n                           marker=box,\n                           markersize = 0.5,\n                           color = vec(z),\n                           colormap = cmap_alpha,\n                           colorrange = (0,6),\n                           axis = (;\n                                   type = Axis3,\n                                   aspect = :data,\n                                   azimuth = 7.3,\n                                   elevation = 0.189,\n            perspectiveness = 0.5),\n        figure = (;\n            resolution =(1200,800)))\n    meshscatter!(ax, x .+ 7, y, z./2;\n        markersize = 0.25,\n        color = vec(z./2),\n        colormap = cmap_alpha,\n        colorrange = (0, 6),\n        ambient = Vec3f(0.85, 0.85, 0.85),\n        backlight = 1.5f0)\n    xlims!(-5.5,10)\n    ylims!(-5.5,5.5)\n    hidedecorations!(ax; grid = false)\n    hidespines!(ax)\n    fig\nend"
  },
  {
    "objectID": "julia/wb_makie_slides.html#d-2",
    "href": "julia/wb_makie_slides.html#d-2",
    "title": "Makie",
    "section": "3D",
    "text": "3D"
  },
  {
    "objectID": "julia/wb_makie_slides.html#compiling-sysimages",
    "href": "julia/wb_makie_slides.html#compiling-sysimages",
    "title": "Makie",
    "section": "Compiling sysimages",
    "text": "Compiling sysimages\nWhile Makie is extremely powerful, its compilation time and its time to first plot are extremely long\nFor this reason, it might save you a lot of time to create a sysimage (a file containing information from a Julia session such as loaded packages, global variables, compiled code, etc.) with PackageCompiler.jl\n\nThe upcoming Julia 1.9 will do this automatically"
  },
  {
    "objectID": "julia/wb_makie_slides.html#cairomakie",
    "href": "julia/wb_makie_slides.html#cairomakie",
    "title": "Makie",
    "section": "CairoMakie",
    "text": "CairoMakie\nCairoMakie will run without problem on the Alliance clusters\nIt is not designed for interactivity, so saving to file is what makes the most sense\n\nExample:\n\nsave(\"graph.png\", fig)\n Remember however that CairoMakie is 2D only (for now)"
  },
  {
    "objectID": "julia/wb_makie_slides.html#glmakie",
    "href": "julia/wb_makie_slides.html#glmakie",
    "title": "Makie",
    "section": "GLMakie",
    "text": "GLMakie\nGLMakie relies on GLFW to create windows with OpenGL\nGLFW doesn’t support creating contexts without an associated window\nThe dependency GLFW.jl will thus not install in the clusters—even with X11 forwarding—unless you use VDI nodes, VNC, or Virtual GL"
  },
  {
    "objectID": "julia/wb_makie_slides.html#wglmakie",
    "href": "julia/wb_makie_slides.html#wglmakie",
    "title": "Makie",
    "section": "WGLMakie",
    "text": "WGLMakie\nYou can setup a server with JSServe.jl as per the documentation\nHowever, this method is intended for the creation of interactive widgets, e.g. for a website\nWhile this is really cool, it isn’t optimized for performance\nThere might also be a way to create an SSH tunnel to your local browser, although there is no documentation on this\nBest probably is to save to file"
  },
  {
    "objectID": "julia/wb_makie_slides.html#conclusion-makie-on-production-clusters",
    "href": "julia/wb_makie_slides.html#conclusion-makie-on-production-clusters",
    "title": "Makie",
    "section": "Conclusion: Makie on production clusters",
    "text": "Conclusion: Makie on production clusters\n\n2D plots: use CairoMakie and save to file\n3D plots: use WGLMakie and save to file"
  },
  {
    "objectID": "ml/jx_jit.html#jit",
    "href": "ml/jx_jit.html#jit",
    "title": "JIT subtleties",
    "section": "JIT",
    "text": "JIT"
  },
  {
    "objectID": "ml/jx_jit.html#jit-constraints",
    "href": "ml/jx_jit.html#jit-constraints",
    "title": "JIT subtleties",
    "section": "JIT constraints",
    "text": "JIT constraints\n\nStatic vs traced variables\nStatic shapes known at compile time\n\n\nStatic vs traced operations"
  },
  {
    "objectID": "ml/jx_nn.html#load-dataset",
    "href": "ml/jx_nn.html#load-dataset",
    "title": "Let’s build a NN with JAX",
    "section": "Load dataset",
    "text": "Load dataset\nAmong the many options to load a dataset, let’s use Hugging Face:"
  },
  {
    "objectID": "ml/jx_nn.html#define-model-architecture",
    "href": "ml/jx_nn.html#define-model-architecture",
    "title": "Let’s build a NN with JAX",
    "section": "Define model architecture",
    "text": "Define model architecture\nHere, we use Flax:"
  },
  {
    "objectID": "ml/jx_nn.html#train",
    "href": "ml/jx_nn.html#train",
    "title": "Let’s build a NN with JAX",
    "section": "Train",
    "text": "Train\n\nCheckpointing"
  },
  {
    "objectID": "ml/jx_nn.html#test",
    "href": "ml/jx_nn.html#test",
    "title": "Let’s build a NN with JAX",
    "section": "Test",
    "text": "Test"
  },
  {
    "objectID": "ml/jx_nn.html#save-model",
    "href": "ml/jx_nn.html#save-model",
    "title": "Let’s build a NN with JAX",
    "section": "Save model",
    "text": "Save model"
  },
  {
    "objectID": "ml/jx_parallel.html",
    "href": "ml/jx_parallel.html",
    "title": "Parallel computing",
    "section": "",
    "text": "With performance in mind, JAX is built for parallel computing at all levels. This section gives an overview of the various parallel implementations."
  },
  {
    "objectID": "ml/jx_parallel.html#asynchronous-dispatch",
    "href": "ml/jx_parallel.html#asynchronous-dispatch",
    "title": "Parallel computing",
    "section": "Asynchronous dispatch",
    "text": "Asynchronous dispatch\nOne of the efficiencies of JAX is its use of asynchronous execution.\n\nAdvantage\nLet’s consider the code:\nimport jax.numpy as jnp\nfrom jax import random\n\nx = random.normal(random.PRNGKey(0), (1000, 1000))\ny = random.normal(random.PRNGKey(0), (1000, 1000))\nz = jnp.dot(x, y)\nInstead of having to wait for the computation to complete before control returns to Python, this computation is dispatched to an accelerator and a future is created. This future is a jax.Array and can be passed to further computations immediately.\nOf course, if you print the result or convert it to a NumPy ndarray, then JAX forces Python to wait for the result of the computation.\n\n\nConsequence on benchmarking\nTiming jnp.dot(x, y) would not give us the time it takes for the computation to take place, but the time it takes to dispatch the computation.\nOn my laptop, running the computation on one GPU, I get:\nimport timeit\n\ntimeit.timeit(\"jnp.dot(x, y)\",\n              number=1000,\n              globals=globals())/1000\n0.0005148850770000308\nTo get a proper timing, we need to make sure that the future is resolved using the block_until_ready() method: jnp.dot(x, y).block_until_ready().\nOn the same machine:\ntimeit.timeit(\"jnp.dot(x, y).block_until_ready()\",\n              number=1000,\n              globals=globals())/1000\n0.0005967016279998916\nThe difference here is not huge because the GPU executes the matrix multiplication rapidly. Nevertheless, this is the true timing. If you benchmark your JAX code, make sure to do it this way.\n\nIf you are running small computations such as this one without accelerator, the dispatch will be on the same thread as the overhead of the asynchronous execution is larger than the speedup. Nevertheless, because it is difficult to predict when the dispatch will be asynchronous, you should always use block_until_ready() in your benchmarks."
  },
  {
    "objectID": "ml/jx_parallel.html#vectorization",
    "href": "ml/jx_parallel.html#vectorization",
    "title": "Parallel computing",
    "section": "Vectorization",
    "text": "Vectorization"
  },
  {
    "objectID": "ml/jx_parallel.html#data-parallelism",
    "href": "ml/jx_parallel.html#data-parallelism",
    "title": "Parallel computing",
    "section": "Data parallelism",
    "text": "Data parallelism"
  },
  {
    "objectID": "ml/jx_parallel.html#sharding",
    "href": "ml/jx_parallel.html#sharding",
    "title": "Parallel computing",
    "section": "Sharding",
    "text": "Sharding"
  },
  {
    "objectID": "ml/jx_why.html",
    "href": "ml/jx_why.html",
    "title": "Why JAX?",
    "section": "",
    "text": "There are many excellent and popular deep learning frameworks already (e.g. PyTorch). So why did Google—already behind the successful TensorFlow project—start developing JAX?\nIn this section, we will look at the advantages brought by JAX, namely speed and flexible automatic differentiation."
  },
  {
    "objectID": "ml/jx_why.html#a-relatively-new-project",
    "href": "ml/jx_why.html#a-relatively-new-project",
    "title": "Why JAX?",
    "section": "A relatively new project",
    "text": "A relatively new project\nIt is clear that JAX is not a widely adopted project yet.\n\nTrends of Google searches\n\n\n\nAs of October 16, 2023.\n\n\n\n\n\nTrends of Stack Overflow tags\n\n\n\nAs of October 16, 2023.\n\n\n\nSo why JAX?"
  },
  {
    "objectID": "ml/jx_why.html#jax-is-fast",
    "href": "ml/jx_why.html#jax-is-fast",
    "title": "Why JAX?",
    "section": "JAX is fast",
    "text": "JAX is fast\nJAX was built with performance in mind. Its speed relies on design decisions at all levels.\n\nDefault data type\n\n\nLike PyTorch—a popular deep learning library—JAX uses float32 as its default data type. This level of precision is perfectly suitable for deep learning and increases efficiency (by contrast, NumPy defaults to float64).\n\n\nJIT compilation\n\nJIT compilation combines computations, avoids the allocation of memory to temporary objects, and more generally optimizes code for the XLA.\n\nAccelerators\n\n\nThe same code can run on CPUs or on accelerators (GPUs and TPUs).\n\n\nXLA optimization\n\n\nXLA (Accelerated Linear Algebra) is a domain-specific compiler for linear algebra that takes JIT-compiled JAX programs and optimizes them for the available hardware (CPUs, GPUs, or TPUs).\n\n\nAsynchronous dispatch\n\n\nComputations are executed on the accelerators asynchronously.\n\n\nVectorization, data parallelism, and sharding\n\n\nAll levels of shared and distributed memory parallelism are supported in JAX."
  },
  {
    "objectID": "ml/jx_why.html#flexible-differentiation",
    "href": "ml/jx_why.html#flexible-differentiation",
    "title": "Why JAX?",
    "section": "Flexible differentiation",
    "text": "Flexible differentiation\nAutomatic differentiation (autodiff or AD) is the evaluation by computer programs of the partial derivatives of functions. It is a key part of deep learning since training a model consists of updating its weights and biases to decrease some loss function thanks to various gradient-based optimizations.\nSeveral implementations have been developed by different teams over time. This post by Chris Rackauckas summarizes the trade-offs of the various strategies.\nRemoving Julia (which is very interesting: Julia has a lot to offer in the field of AD) and PyTorch stale attempt at JIT compilation, Chris Rackauckas’ post can be summarized this way:\n\n\n\n\n\n  \n\n01\n\n  Autodiff method   \n\n1\n\nStatic graph and XLA    \n\n02\n\n  Framework    \n\n2\n\nDynamic graph   \n\n1-&gt;2\n\n    \n\na\n\n TensorFlow    \n\n4\n\nDynamic graph and XLA   \n\n2-&gt;4\n\n    \n\nb\n\n PyTorch    \n\n5\n\nNon-standard interpretation and XLA   \n\n4-&gt;5\n\n    \n\nd\n\n TensorFlow2    \n\ne\n\n JAX     \n\n03\n\n  Advantage     \n\n7\n\nOptimized autodiff     \n\n8\n\nConvenient     \n\n9\n\nConvenient    \n\n10\n\nFlexible and optimized autodiff     \n\n04\n\n  Disadvantage     \n\nA\n\nInconvenient     \n\nB\n\nCan’t optimize autodiff     \n\nD\n\nDynamism doesn’t play well with XLA    \n\nE\n\nRequires an intermediate representation       \n\n\n\n\n\n\nTensorFlow initial approach with computational graphs in a domain-specific language was unintuitive, inconvenient, and hard to debug. PyTorch came with dynamic graphs—an approach so much more convenient that it marked the beginning of the decline of TensorFlow. However xxxx. TensorFlow2 tried xxxx, but xxx.\nThis leaves room for new strategies. Julia offers several promising approaches, but implementations are not straightforward and projects are not always mature. It is an exciting avenue for developers, not necessarily an easy one for end users. JAX is another attempt at bringing both optimization and flexibility to autodiff. With Google behind it, it is a new but fast growing project."
  },
  {
    "objectID": "ml/pt_checkpoints.html",
    "href": "ml/pt_checkpoints.html",
    "title": "Saving/loading models and checkpointing",
    "section": "",
    "text": "After you have trained your model, obviously you will want to save it to use it thereafter. You will then need to load it in any session in which you want to use it.\nIn addition to saving or loading a fully trained model, it is important to know how to create regular checkpoints: training ML models takes a long time and a cluster crash or countless other issues might interrupt the training process. You don’t want to have to restart from scratch if that happens."
  },
  {
    "objectID": "ml/pt_checkpoints.html#savingloading-models",
    "href": "ml/pt_checkpoints.html#savingloading-models",
    "title": "Saving/loading models and checkpointing",
    "section": "Saving/loading models",
    "text": "Saving/loading models\n\nSaving models\nYou can save a model by serializing its internal state dictionary. The state dictionary is a Python dictionary that contains the learnable parameters of your model (weights and biases).\ntorch.save(model.state_dict(), \"model.pt\")\n\n\nLoading models\nTo recreate your model, you first need to recreate its structure:\nmodel = TheModelClass(*args, **kwargs)\nThen you can load the state dictionary containing the parameters values into it:\nmodel.load_state_dict(torch.load(\"model.pt\"))\nAssuming you want to use your model for inference, you also must run:\nmodel.eval()\n\nIf instead you want to do more training on your model, you would of course run model.train() instead."
  },
  {
    "objectID": "ml/pt_checkpoints.html#checkpointing",
    "href": "ml/pt_checkpoints.html#checkpointing",
    "title": "Saving/loading models and checkpointing",
    "section": "Checkpointing",
    "text": "Checkpointing\n\nCreating a checkpoint\ntorch.save({\n    'epoch': epoch,\n    'model_state_dict': model.state_dict(),\n    'optimizer_state_dict': optimizer.state_dict(),\n    'loss': loss,\n    ...\n}, \"model.pt\")\n\n\nResuming training from a checkpoint\nRecreate the state of your model from the checkpoint:\nmodel = TheModelClass(*args, **kwargs)\noptimizer = TheOptimizerClass(*args, **kwargs)\n\ncheckpoint = torch.load(\"model.pt\")\nmodel.load_state_dict(checkpoint['model_state_dict'])\noptimizer.load_state_dict(checkpoint['optimizer_state_dict'])\nepoch = checkpoint['epoch']\nloss = checkpoint['loss']\nResume training:\nmodel.train()"
  },
  {
    "objectID": "ml/pt_concepts.html",
    "href": "ml/pt_concepts.html",
    "title": "Concepts:",
    "section": "",
    "text": "Artificial intelligence is a vast field: any system mimicking animal intelligence falls in its scope.\nMachine learning (ML) is a subfield of artificial intelligence that can be defined as computer programs whose performance at a task improves with experience.\nSince this experience comes in the form of data, ML consists of feeding vast amounts of data to algorithms to strengthen pathways.\n\n\nFrom xkcd.com\n\n\n\nCoding all the possible ways—pixel by pixel—that a picture can represent a certain object is an impossibly large task. By feeding examples of images of that object to a neural network however, we can train it to recognize that object in images that it has never seen (without explicitly programming how it does this!).\n\n\nFrom xkcd.com"
  },
  {
    "objectID": "ml/pt_concepts.html#what-is-machine-learning",
    "href": "ml/pt_concepts.html#what-is-machine-learning",
    "title": "Concepts:",
    "section": "",
    "text": "Artificial intelligence is a vast field: any system mimicking animal intelligence falls in its scope.\nMachine learning (ML) is a subfield of artificial intelligence that can be defined as computer programs whose performance at a task improves with experience.\nSince this experience comes in the form of data, ML consists of feeding vast amounts of data to algorithms to strengthen pathways.\n\n\nFrom xkcd.com\n\n\n\nCoding all the possible ways—pixel by pixel—that a picture can represent a certain object is an impossibly large task. By feeding examples of images of that object to a neural network however, we can train it to recognize that object in images that it has never seen (without explicitly programming how it does this!).\n\n\nFrom xkcd.com"
  },
  {
    "objectID": "ml/pt_concepts.html#types-of-learning",
    "href": "ml/pt_concepts.html#types-of-learning",
    "title": "Concepts:",
    "section": "Types of learning",
    "text": "Types of learning\nThere are now more types of learning than those presented here. But these initial types are interesting because they will already be familiar to you.\n\nSupervised learning\nYou have been doing supervised machine learning for years without looking at it in the framework of machine learning:\n\nRegression is a form of supervised learning with continuous outputs\nClassification is supervised learning with discrete outputs\n\nSupervised learning uses training data in the form of example input/output \\((x_i, y_i)\\) pairs.\nGoal:\nIf \\(X\\) is the space of inputs and \\(Y\\) the space of outputs, the goal is to find a function \\(h\\) so that\nfor each \\(x_i \\in X\\):\n\n\\(h_\\theta(x_i)\\) is a predictor for the corresponding value \\(y_i\\)\n\n(\\(\\theta\\) represents the set of parameters of \\(h_\\theta\\)).\n\n→ i.e. we want to find the relationship between inputs and outputs.\n\n\nUnsupervised learning\nHere too, you are familiar with some forms of unsupervised learning that you weren’t thinking about in such terms:\nClustering, social network analysis, market segmentation, PCA … are all forms of unsupervised learning.\nUnsupervised learning uses unlabelled data (training set of \\(x_i\\)).\nGoal:\nFind structure within the data.\n\n\nFrom xkcd.com"
  },
  {
    "objectID": "ml/pt_concepts.html#artificial-neural-networks",
    "href": "ml/pt_concepts.html#artificial-neural-networks",
    "title": "Concepts:",
    "section": "Artificial neural networks",
    "text": "Artificial neural networks\nArtificial neural networks (ANN) are one of the machine learning models (other models include decision trees or Bayesian networks). Their potential and popularity has truly exploded in recent years and this is what we will focus on in this course.\nArtificial neural networks are a series of layered units mimicking the concept of biological neurons: inputs are received by every unit of a layer, computed, then transmitted to units of the next layer. In the process of learning, experience strengthens some connections between units and weakens others.\nIn biological networks, the information consists of action potentials (neuron membrane rapid depolarizations) propagating through the network. In artificial ones, the information consists of tensors (multidimensional arrays) of weights and biases: each unit passes a weighted sum of an input tensor with an additional—possibly weighted—bias through an activation function before passing on the output tensor to the next layer of units.\n\n\nThe bias allows to shift the output of the activation function to the right or to the left (i.e. it creates an offset).\n\nSchematic of a biological neuron:\n\n\nFrom Dhp1080, Wikipedia\n\nSchematic of an artificial neuron:\n\n\nModified from O.C. Akgun & J. Mei 2019\n\nWhile biological neurons are connected in extremely intricate patterns, artificial ones follow a layered structure. Another difference in complexity is in the number of units: the human brain has 65–90 billion neurons. ANN have much fewer units.\nNeurons in mouse cortex:\n\n\nNeurons are in green, the dark branches are blood vessels. Image by Na Ji, UC Berkeley\n\nNeural network with 2 hidden layers:\n\n\nFrom The Maverick Meerkat\n\nThe information in biological neurons is an all-or-nothing electrochemical pulse or action potential. Greater stimuli don’t produce stronger signals but increase firing frequency. In contrast, artificial neurons pass the computation of their inputs through an activation function and the output can take any of the values possible with that function.\nThreshold potential in biological neurons:\n\n\nModified from Blacktc, Wikimedia\n\nSome of the most common activation functions in artificial neurons:\n\n\nFrom Diganta Misra 2019\n\nWhich activation function to use depends on the type of problem and the available computing budget. Some early functions have fallen out of use while new ones have emerged (e.g. sigmoid got replaced by ReLU which is easier to train).\n\nLearning\nThe process of learning in biological NN happens through neuron death or growth and through the creation or loss of synaptic connections between neurons. In ANN, learning happens through optimization algorithms such as gradient descent which minimize cross entropy loss functions by adjusting the weights and biases connecting each layer of neurons over many iterations (cross entropy is the difference between the predicted and the real distributions).\n\n\nFrom xkcd.com\n\n\n\nGradient descent\nThere are several gradient descent methods:\nBatch gradient descent uses all examples in each iteration and is thus slow for large datasets (the parameters are adjusted only after all the samples have been processed).\nMini-batch gradient descent is an intermediate approach: it uses mini-batch sized examples in each iteration. This allows a vectorized approach (and hence parallelization).\nThe Adam optimization algorithm is a popular variation of mini-batch gradient descent.\nStochastic gradient descent uses one example in each iteration. It is thus much faster than batch gradient descent (the parameters are adjusted after each example). But it does not allow any vectorization.\n\n\nFrom Imad Dabbura\n\n\n\n3Blue1Brown by Grant Sanderson videos\n3Blue1Brown by Grant Sanderson has a series of 4 videos on neural networks which is easy to watch, fun, and does an excellent job at introducing the functioning of a simple neural network.\n\nWhat are NN? (19 min)\n\nWatch this video beyond the acknowledgement as the function ReLU (a really important function in modern neural networks) is introduced at the very end.\n\n\n\nAs you develop your own ML models, if you find that your mathematical background is shaky, 3blue1brown also has an excellent series of videos on linear algebra and an equally great one on calculus.\n\n\n\nHow do NN learn? (21 min)\n\n\n\nWhat is backpropagation? (14 min)\n\n\nThere is one minor terminological error in this video: they call the use of mini-batches stochastic gradient descent. In fact, this is called mini-batch gradient descent. Stochastic gradient descent uses a single example at each iteration.\n\n\n\nHow does backpropagation work? (10 min)\n\n\n\n\nTypes of ANN\n\nFully connected neural networks\n\n\nFrom Glosser.ca, Wikipedia\n\nEach neuron receives inputs from every neuron of the previous layer and passes its output to every neuron of the next layer.\n\n\nConvolutional neural networks\n\n\nFrom Programming Journeys by Rensu Theart\n\nConvolutional neural networks (CNN) are used for spatially structured data (e.g. in image recognition).\nImages have huge input sizes and would require a very large number of neurons in a fully connected neural net. In convolutional layers, neurons receive input from a subarea (called local receptive field) of the previous layer. This greatly reduces the number of parameters.\nOptionally, pooling (combining the outputs of neurons in a subarea) reduces the data dimensions. The stride then dictates how the subarea is moved across the image. Max-pooling is one of the forms of pooling which uses the maximum for each subarea.\n\n\nRecurrent neural networks\n\n\nFrom fdeloche, Wikipedia\n\nRecurrent neural networks (RNN) such as Long Short-Term Memory (LSTM) are used for chain structured data (e.g. in speech recognition).\nThey are not feedforward networks (i.e. networks for which the information moves only in the forward direction without any loop).\n\n\nTransformers\nA combination of two RNNs or sets of RNNs (the encoder and the decoder) is used in sequence to sequence models for translation or picture captioning. Such models were slow to process a lot of data.\nIn 2014 and 2015, the concept of attention (giving added weight to important words) was developed, greatly improving the ability of such models to process a lot of data.\nThis blog post by Jay Alammar—a blogger whose high-quality posts have been referenced in MIT and Stanford courses—explains this in a high-level visual fashion.\nThe problem with recurrence is that it is not easily to parallelize (and thus to run fast on GPUs).\nIn 2017, a new model—the transformer—was proposed: by using only attention mechanisms and no recurrence, the transformer achieves better results in an easily parallelizable fashion.\nJay Alammar has also a blog post on the transformer. The post includes a 30 min video.\nWith the addition of transfer learning, powerful transformers emerged in the field of NLP (Natural Language Processing). Examples include BERT (Bidirectional Encoder Representations from Transformers) from Google and GPT-3 (Generative Pre-trained Transformer-3) from OpenAI.\nJay Alammar has yet another great blog post on these advanced NLP models.\n\n\n\nDeep learning\nThe first layer of a neural net is the input layer. The last one is the output layer. All the layers in-between are called hidden layers. Shallow neural networks have only one hidden layer and deep networks have two or more hidden layers. When an ANN is deep, we talk about Deep Learning (DL).\n\n\nFrom xkcd.com"
  },
  {
    "objectID": "ml/pt_high_level_frameworks.html",
    "href": "ml/pt_high_level_frameworks.html",
    "title": "High-level frameworks for PyTorch",
    "section": "",
    "text": "Several popular higher-level frameworks are built on top of PyTorch and make the code easier to write and run:\nThe following tag trends on Stack Overflow might give an idea of the popularity of these frameworks over time (catalyst doesn’t have any Stack Overflow tag):\nIf this data is to be believed, ignite never really took off (it also has a lower number of stars on GitHub), fast-ai was extremely popular when it came out, but its usage is going down, and PyTorch-lightning is currently the most popular."
  },
  {
    "objectID": "ml/pt_high_level_frameworks.html#should-you-use-one-and-which-one",
    "href": "ml/pt_high_level_frameworks.html#should-you-use-one-and-which-one",
    "title": "High-level frameworks for PyTorch",
    "section": "Should you use one (and which one)?",
    "text": "Should you use one (and which one)?\nLearning raw PyTorch is probably the best option for research. PyTorch is stable and here to stay. Higher-level frameworks may rise and drop in popularity and today’s popular one may see little usage tomorrow.\nRaw PyTorch is also the most flexible, the closest to the actual computations happening in your model, and probably the easiest to debug.\nDepending on your deep learning trajectory, you might find some of these tools useful though:\n\nIf you work in industry, you might want or need to get results quickly\nSome operations (e.g. parallel execution on multiple GPUs) can be tricky in raw PyTorch, while being extremely streamlined when using e.g. PyTorch-lightning\nEven in research, it might make sense to spend more time thinking about the structure of your model and the functioning of a network instead of getting bogged down in writing code\n\n\nBefore moving to any of these tools, it is probably a good idea to get a good knowledge of raw PyTorch: use these tools to simplify your workflow, not cloud your understanding of what your code is doing."
  },
  {
    "objectID": "ml/pt_intro.html",
    "href": "ml/pt_intro.html",
    "title": "Introduction to machine learning",
    "section": "",
    "text": "This presentation gives a high-level overview of machine learning.\nI will define concepts, present the different types of learning, and explain the basic functioning of neural networks.\n\nSlides (Click and wait: the presentation might take a few instants to load)"
  },
  {
    "objectID": "ml/pt_mnist.html",
    "href": "ml/pt_mnist.html",
    "title": "Example: classifying the MNIST dataset",
    "section": "",
    "text": "Here is an example you can try on your own after the workshop: the classification of the MNIST dataset—a classic of machine learning."
  },
  {
    "objectID": "ml/pt_mnist.html#the-mnist-dataset",
    "href": "ml/pt_mnist.html#the-mnist-dataset",
    "title": "Example: classifying the MNIST dataset",
    "section": "The MNIST dataset",
    "text": "The MNIST dataset\nThe MNIST is a classic dataset commonly used for testing machine learning systems. It consists of pairs of images of handwritten digits and their corresponding labels.\nThe images are composed of 28x28 pixels of greyscale RGB codes ranging from 0 to 255 and the labels are the digits from 0 to 9 that each image represents.\nThere are 60,000 training pairs and 10,000 testing pairs.\nThe goal is to build a neural network which can learn from the training set to properly identify the handwritten digits and which will perform well when presented with the testing set that it has never seen. This is a typical case of supervised learning.\n\nNow, let’s explore the MNIST with PyTorch."
  },
  {
    "objectID": "ml/pt_mnist.html#download-unzip-and-transform-the-data",
    "href": "ml/pt_mnist.html#download-unzip-and-transform-the-data",
    "title": "Example: classifying the MNIST dataset",
    "section": "Download, unzip, and transform the data",
    "text": "Download, unzip, and transform the data\n\nWhere to store the data in the cluster\nWe will all use the same data. It would make little sense to all download it in our home directory.\nOn the Alliance clusters, a good place to store data shared amongst members of a project is in the /project file system.\nYou usually belong to /project/def-&lt;group&gt;, where &lt;group&gt; is the name of your PI. You can access it from your home directory through the symbolic link ~/projects/def-&lt;group&gt;.\nIn our training cluster, we are all part of the group def-sponsor00, accessible through /project/def-sponsor00 (or the hyperlink ~/projects/def-sponsor00).\nWe will thus all access the MNIST data in ~/projects/def-sponsor00/data.\n\n\nHow to obtain the data?\nThe dataset can be downloaded directly from the MNIST website, but the PyTorch package TorchVision has tools to download and transform several classic vision datasets, including the MNIST.\nhelp(torchvision.datasets.MNIST)\nHelp on class MNIST in module torchvision.datasets.mnist:\n\nclass MNIST(torchvision.datasets.vision.VisionDataset)\n\n |  MNIST(root: str, train: bool = True, \n |    transform: Optional[Callable] = None,\n |    target_transform: Optional[Callable] = None, \n |    download: bool = False) -&gt; None\n |   \n |  Args:\n |    root (string): Root directory of dataset where \n |      MNIST/raw/train-images-idx3-ubyte and \n |      MNIST/raw/t10k-images-idx3-ubyte exists.\n |    train (bool, optional): If True, creates dataset from \n |      train-images-idx3-ubyte, otherwise from t10k-images-idx3-ubyte.\n |    download (bool, optional): If True, downloads the dataset from the \n |      internet and puts it in root directory. If dataset is already \n |      downloaded, it is not downloaded again.\n |    transform (callable, optional): A function/transform that takes in \n |      an PIL image and returns a transformed version.\n |      E.g, transforms.RandomCrop\n |    target_transform (callable, optional): A function/transform that \n |      takes in the target and transforms it.\nNote that here too, the root argument sets the location of the downloaded data and we will use /project/def-sponsor00/data/.\n\n\nPrepare the data\nFirst, let’s load the needed libraries:\n\nimport torch\nfrom torchvision import datasets, transforms\nfrom matplotlib import pyplot as plt\n\nThe MNIST dataset already consists of a training and a testing sets, so we don’t have to split the data manually. Instead, we can directly create 2 different objects with the same function (train=True selects the train set and train=False selects the test set).\nWe will transform the raw data to tensors and normalize them using the mean and standard deviation of the MNIST training set: 0.1307 and 0.3081 respectively (even though the mean and standard deviation of the test data are slightly different, it is important to normalize the test data with the values of the training data to apply the same treatment to both sets).\nSo we first need to define a transformation:\n\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.1307,), (0.3081,))\n])\n\n\n\nWe can now create our data objects\n\nTraining data\n\nRemember that train=True selects the training set of the MNIST.\n\n\ntrain_data = datasets.MNIST(\n    '~/projects/def-sponsor00/data',\n    train=True, download=True, transform=transform)\n\n\n\nTest data\n\ntrain=False selects the test set.\n\n\ntest_data = datasets.MNIST(\n    '~/projects/def-sponsor00/data',\n    train=False, transform=transform)"
  },
  {
    "objectID": "ml/pt_mnist.html#exploring-the-data",
    "href": "ml/pt_mnist.html#exploring-the-data",
    "title": "Example: classifying the MNIST dataset",
    "section": "Exploring the data",
    "text": "Exploring the data\n\nData inspection\nFirst, let’s check the size of train_data:\n\nprint(len(train_data))\n\n60000\n\n\nThat makes sense since the MNIST’s training set has 60,000 pairs. train_data has 60,000 elements and we should expect each element to be of size 2 since it is a pair. Let’s double-check with the first element:\n\nprint(len(train_data[0]))\n\n2\n\n\nSo far, so good. We can print that first pair:\n\nprint(train_data[0])\n\n(tensor([[[-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.3860, -0.1951,\n          -0.1951, -0.1951,  1.1795,  1.3068,  1.8032, -0.0933,  1.6887,\n           2.8215,  2.7197,  1.1923, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.0424,  0.0340,  0.7722,  1.5359,  1.7396,  2.7960,\n           2.7960,  2.7960,  2.7960,  2.7960,  2.4396,  1.7650,  2.7960,\n           2.6560,  2.0578,  0.3904, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n           0.1995,  2.6051,  2.7960,  2.7960,  2.7960,  2.7960,  2.7960,\n           2.7960,  2.7960,  2.7960,  2.7706,  0.7595,  0.6195,  0.6195,\n           0.2886,  0.0722, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.1951,  2.3633,  2.7960,  2.7960,  2.7960,  2.7960,  2.7960,\n           2.0960,  1.8923,  2.7197,  2.6433, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242,  0.5940,  1.5614,  0.9377,  2.7960,  2.7960,  2.1851,\n          -0.2842, -0.4242,  0.1231,  1.5359, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.2460, -0.4115,  1.5359,  2.7960,  0.7213,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242,  1.3450,  2.7960,  1.9942,\n          -0.3988, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.2842,  1.9942,  2.7960,\n           0.4668, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.0213,  2.6433,\n           2.4396,  1.6123,  0.9504, -0.4115, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.6068,\n           2.6306,  2.7960,  2.7960,  1.0904, -0.1060, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n           0.1486,  1.9432,  2.7960,  2.7960,  1.4850, -0.0806, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.2206,  0.7595,  2.7833,  2.7960,  1.9560, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242,  2.7451,  2.7960,  2.7451,  0.3904,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n           0.1613,  1.2305,  1.9051,  2.7960,  2.7960,  2.2105, -0.3988,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.0722,  1.4596,\n           2.4906,  2.7960,  2.7960,  2.7960,  2.7578,  1.8923, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.1187,  1.0268,  2.3887,  2.7960,\n           2.7960,  2.7960,  2.7960,  2.1342,  0.5686, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.1315,  0.4159,  2.2869,  2.7960,  2.7960,  2.7960,\n           2.7960,  2.0960,  0.6068, -0.3988, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.1951,\n           1.7523,  2.3633,  2.7960,  2.7960,  2.7960,  2.7960,  2.0578,\n           0.5940, -0.3097, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242,  0.2758,  1.7650,  2.4524,\n           2.7960,  2.7960,  2.7960,  2.7960,  2.6815,  1.2686, -0.2842,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242,  1.3068,  2.7960,  2.7960,\n           2.7960,  2.2742,  1.2941,  1.2559, -0.2206, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242]]]), 5)\n\n\nAnd you can see that it is a tuple with:\n\nprint(type(train_data[0]))\n\n&lt;class 'tuple'&gt;\n\n\nWhat is that tuple made of?\n\nprint(type(train_data[0][0]))\nprint(type(train_data[0][1]))\n\n&lt;class 'torch.Tensor'&gt;\n&lt;class 'int'&gt;\n\n\nIt is made of the tensor for the first image (remember that we transformed the images into tensors when we created the objects train_data and test_data) and the integer of the first label (which you can see is 5 when you print train_data[0][1]).\nSo since train_data[0][0] is the tensor representing the image of the first pair, let’s check its size:\n\nprint(train_data[0][0].size())\n\ntorch.Size([1, 28, 28])\n\n\nThat makes sense: a color image would have 3 layers of RGB values (so the size in the first dimension would be 3), but because the MNIST has black and white images, there is a single layer of values—the values of each pixel on a gray scale—so the first dimension has a size of 1. The 2nd and 3rd dimensions correspond to the width and length of the image in pixels, hence 28 and 28.\n\n\nYour turn:\n\nRun the following:\nprint(train_data[0][0][0])\nprint(train_data[0][0][0][0])\nprint(train_data[0][0][0][0][0])\nAnd think about what each of them represents.\nThen explore the test_data object.\n\n\n\nPlotting an image from the data\nFor this, we will use pyplot from matplotlib.\nFirst, we select the image of the first pair and we resize it from 3 to 2 dimensions by removing its dimension of size 1 with torch.squeeze:\nimg = torch.squeeze(train_data[0][0])\nThen, we plot it with pyplot, but since we are in a cluster, instead of showing it to screen with plt.show(), we save it to file:\nplt.imshow(img, cmap='gray')\nThis is what that first image looks like:\n\nAnd indeed, it matches the first label we explored earlier (train_data[0][1]).\n\n\nPlotting an image with its pixel values\nWe can plot it with more details by showing the value of each pixel in the image. One little twist is that we need to pick a threshold value below which we print the pixel values in white otherwise they would not be visible (black on near black background). We also round the pixel values to one decimal digit so as not to clutter the result.\nimgplot = plt.figure(figsize = (12, 12))\nsub = imgplot.add_subplot(111)\nsub.imshow(img, cmap='gray')\nwidth, height = img.shape\nthresh = img.max() / 2.5\nfor x in range(width):\n    for y in range(height):\n        val = round(img[x][y].item(), 1)\n        sub.annotate(str(val), xy=(y, x),\n                     horizontalalignment='center',\n                     verticalalignment='center',\n                     color='white' if img[x][y].item() &lt; thresh else 'black')"
  },
  {
    "objectID": "ml/pt_mnist.html#batch-processing",
    "href": "ml/pt_mnist.html#batch-processing",
    "title": "Example: classifying the MNIST dataset",
    "section": "Batch processing",
    "text": "Batch processing\nPyTorch provides the torch.utils.data.DataLoader class which combines a dataset and an optional sampler and provides an iterable (while training or testing our neural network, we will iterate over that object). It allows, among many other things, to set the batch size and shuffle the data.\nSo our last step in preparing the data is to pass it through DataLoader.\n\nCreate DataLoaders\n\nTraining data\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=20, shuffle=True)\n\n\nTest data\ntest_loader = torch.utils.data.DataLoader(\n    test_data, batch_size=20, shuffle=False)\n\n\n\nPlot a full batch of images with their labels\nNow that we have passed our data through DataLoader, it is easy to select one batch from it. Let’s plot an entire batch of images with their labels.\nFirst, we need to get one batch of training images and their labels:\ndataiter = iter(train_loader)\nbatchimg, batchlabel = dataiter.next()\nThen, we can plot them:\nbatchplot = plt.figure(figsize=(20, 5))\nfor i in torch.arange(20):\n    sub = batchplot.add_subplot(2, 10, i+1, xticks=[], yticks=[])\n    sub.imshow(torch.squeeze(batchimg[i]), cmap='gray')\n    sub.set_title(str(batchlabel[i].item()), fontsize=25)"
  },
  {
    "objectID": "ml/pt_mnist.html#time-to-build-a-nn-to-classify-the-mnist",
    "href": "ml/pt_mnist.html#time-to-build-a-nn-to-classify-the-mnist",
    "title": "Example: classifying the MNIST dataset",
    "section": "Time to build a NN to classify the MNIST",
    "text": "Time to build a NN to classify the MNIST\nLet’s build a multi-layer perceptron (MLP): the simplest neural network. It is a feed-forward (i.e. no loop), fully-connected (i.e. each neuron of one layer is connected to all the neurons of the adjacent layers) neural network with a single hidden layer.\n\n\nLoad packages\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torchvision import datasets, transforms\nfrom torch.optim.lr_scheduler import StepLR\nThe torch.nn.functional module contains all the functions of the torch.nn package.\nThese functions include loss functions, activation functions, pooling functions…\n\n\nCreate a SummaryWriter instance for TensorBoard\nwriter = SummaryWriter()\n\n\nDefine the architecture of the network\n# To build a model, create a subclass of torch.nn.Module:\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.fc1 = nn.Linear(784, 128)\n        self.fc2 = nn.Linear(128, 10)\n\n    # Method for the forward pass:\n    def forward(self, x):\n        x = torch.flatten(x, 1)\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.fc2(x)\n        output = F.log_softmax(x, dim=1)\n        return output\nPython’s class inheritance gives our subclass all the functionality of torch.nn.Module while allowing us to customize it.\n\n\nDefine a training function\ndef train(model, device, train_loader, optimizer, epoch):\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n        optimizer.zero_grad()  # reset the gradients to 0\n        output = model(data)\n        loss = F.nll_loss(output, target)  # negative log likelihood\n        writer.add_scalar(\"Loss/train\", loss, epoch)\n        loss.backward()\n        optimizer.step()\n        if batch_idx % 10 == 0:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(data), len(train_loader.dataset),\n                100. * batch_idx / len(train_loader), loss.item()))\n\n\nDefine a testing function\ndef test(model, device, test_loader):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            # Sum up batch loss:\n            test_loss += F.nll_loss(output, target, reduction='sum').item()\n            # Get the index of the max log-probability:\n            pred = output.argmax(dim=1, keepdim=True)\n            correct += pred.eq(target.view_as(pred)).sum().item()\n\n    test_loss /= len(test_loader.dataset)\n\n    # Print a summary\n    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n        test_loss, correct, len(test_loader.dataset),\n        100. * correct / len(test_loader.dataset)))\n\n\nDefine a function main() which runs our network\ndef main():\n    epochs = 1\n    torch.manual_seed(1)\n    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n\n    transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,))\n    ])\n\n    train_data = datasets.MNIST(\n        '~/projects/def-sponsor00/data',\n        train=True, download=True, transform=transform)\n\n    test_data = datasets.MNIST(\n        '~/projects/def-sponsor00/data',\n        train=False, transform=transform)\n\n    train_loader = torch.utils.data.DataLoader(train_data, batch_size=64)\n    test_loader = torch.utils.data.DataLoader(test_data, batch_size=1000)\n    model = Net().to(device)  # create instance of our network and send it to device\n    optimizer = optim.Adadelta(model.parameters(), lr=1.0)\n    scheduler = StepLR(optimizer, step_size=1, gamma=0.7)\n\n    for epoch in range(1, epochs + 1):\n        train(model, device, train_loader, optimizer, epoch)\n        test(model, device, test_loader)\n        scheduler.step()\n\n\nRun the network\nmain()\n\n\nWrite pending events to disk and close the TensorBoard\nwriter.flush()\nwriter.close()\nThe code is working. Time to actually train our model!\nJupyter is a fantastic tool. It has a major downside however: when you launch a Jupyter server, you are running a job on a compute node. If you want to play for 8 hours in Jupyter, you are requesting an 8 hour job. Now, most of the time you spend on Jupyter is spent typing, running bits and pieces of code, or doing nothing at all. If you ask for GPUs, many CPUs, and lots of RAM, all of it will remain idle almost all of the time. It is a really suboptimal use of the Alliance resources.\nIn addition, if you ask for lots of resources for a long time, you will have to wait a long time in the queue before they get allocated to you.\nLastly, you will go through your allocation quickly.\nA much better strategy is to develop and test your code (with very little data, few epochs, etc.) in an interactive job (with salloc) or in Jupyter, then, launch an sbatch job to actually train your model. This ensures that heavy duty resources such as GPU(s) are only allocated to you when you are actually needing and using them."
  },
  {
    "objectID": "ml/pt_mnist.html#lets-train-and-test-our-model",
    "href": "ml/pt_mnist.html#lets-train-and-test-our-model",
    "title": "Example: classifying the MNIST dataset",
    "section": "Let’s train and test our model",
    "text": "Let’s train and test our model\n\nLog in a cluster\nOpen a terminal and SSH to a cluster.\n\n\nLoad necessary modules\nFirst, we need to load the Python and CUDA modules. This is done with the Lmod tool through the module command. Here are some key Lmod commands:\n# Get help on the module command\n$ module help\n\n# List modules that are already loaded\n$ module list\n\n# See which modules are available for a tool\n$ module avail &lt;tool&gt;\n\n# Load a module\n$ module load &lt;module&gt;[/&lt;version&gt;]\nHere are the modules we need:\n$ module load nixpkgs/16.09 gcc/7.3.0 cuda/10.0.130 cudnn/7.6 python/3.8.2\n\n\nInstall Python packages\nYou also need the Python packages matplotlib, torch, torchvision, and tensorboard.\nOn the Alliance clusters, you need to create a virtual environment in which you install packages with pip, then activate that virtual environment.\n\nDo not use Anaconda.\nWhile Anaconda is a great tool on personal computers, it is not an appropriate tool when working on the Alliance clusters: binaries are unoptimized for those clusters and library paths are inconsistent with their architecture. Anaconda installs packages in $HOME where it creates a very large number of small files. It can also create conflicts by modifying .bashrc.\n\nCreate a virtual environment:\n$ virtualenv --no-download ~/env\nActivate the virtual environment:\n$ source ~/env/bin/activate\nUpdate pip:\n(env) $ pip install --no-index --upgrade pip\nInstall the packages you need in the virtual environment:\n(env) $ pip install --no-cache-dir --no-index matplotlib torch torchvision tensorboard\nIf you want to exit the virtual environment, you can press Ctrl-D or run:\n(env) $ deactivate\n\n\nWrite a Python script\nCreate a directory for this project and cd into it:\nmkdir mnist\ncd mnist\nStart a Python script with the text editor of your choice:\nnano nn.py\nIn it, copy-paste the code we played with in Jupyter, but this time have it run for 10 epochs:\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torchvision import datasets, transforms\nfrom torch.optim.lr_scheduler import StepLR\n\nwriter = SummaryWriter()\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.fc1 = nn.Linear(784, 128)\n        self.fc2 = nn.Linear(128, 10)\n\n    def forward(self, x):\n        x = torch.flatten(x, 1)\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.fc2(x)\n        output = F.log_softmax(x, dim=1)\n        return output\n\ndef train(model, device, train_loader, optimizer, epoch):\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = F.nll_loss(output, target)\n        writer.add_scalar(\"Loss/train\", loss, epoch)\n        loss.backward()\n        optimizer.step()\n        if batch_idx % 10 == 0:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(data), len(train_loader.dataset),\n                100. * batch_idx / len(train_loader), loss.item()))\n\ndef test(model, device, test_loader):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            test_loss += F.nll_loss(output, target, reduction='sum').item()\n            pred = output.argmax(dim=1, keepdim=True)\n            correct += pred.eq(target.view_as(pred)).sum().item()\n\n    test_loss /= len(test_loader.dataset)\n\n    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n        test_loss, correct, len(test_loader.dataset),\n        100. * correct / len(test_loader.dataset)))\n\ndef main():\n    epochs = 10  # don't forget to change the number of epochs\n    torch.manual_seed(1)\n    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n\n    transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,))\n    ])\n\n    train_data = datasets.MNIST(\n        '~/projects/def-sponsor00/data',\n        train=True, download=True, transform=transform)\n\n    test_data = datasets.MNIST(\n        '~/projects/def-sponsor00/data',\n        train=False, transform=transform)\n\n    train_loader = torch.utils.data.DataLoader(train_data, batch_size=64)\n    test_loader = torch.utils.data.DataLoader(test_data, batch_size=1000)\n    model = Net().to(device)\n    optimizer = optim.Adadelta(model.parameters(), lr=1.0)\n    scheduler = StepLR(optimizer, step_size=1, gamma=0.7)\n\n    for epoch in range(1, epochs + 1):\n        train(model, device, train_loader, optimizer, epoch)\n        test(model, device, test_loader)\n        scheduler.step()\n\nmain()\n\nwriter.flush()\nwriter.close()\n\n\nWrite a Slurm script\nWrite a shell script with the text editor of your choice:\nnano nn.sh\nThis is what you want in that script:\n#!/bin/bash\n#SBATCH --time=5:0\n#SBATCH --cpus-per-task=1\n#SBATCH --gres=gpu:1\n#SBATCH --mem=4G\n#SBATCH --output=%x_%j.out\n#SBATCH --error=%x_%j.err\n\npython ~/mnist/nn.py\n\n--time accepts these formats: “min”, “min:s”, “h:min:s”, “d-h”, “d-h:min” & “d-h:min:s”\n%x will get replaced by the script name & %j by the job number\n\n\n\nSubmit a job\nFinally, you need to submit your job to Slurm:\n$ sbatch ~/mnist/nn.sh\nYou can check the status of your job with:\n$ sq\n\nPD = pending\nR = running\nCG = completing (Slurm is doing the closing processes)\nNo information = your job has finished running\n\nYou can cancel it with:\n$ scancel &lt;jobid&gt;\nOnce your job has finished running, you can display efficiency measures with:\n$ seff &lt;jobid&gt;"
  },
  {
    "objectID": "ml/pt_mnist.html#lets-explore-our-models-metrics-with-tensorboard",
    "href": "ml/pt_mnist.html#lets-explore-our-models-metrics-with-tensorboard",
    "title": "Example: classifying the MNIST dataset",
    "section": "Let’s explore our model’s metrics with TensorBoard",
    "text": "Let’s explore our model’s metrics with TensorBoard\nTensorBoard is a web visualization toolkit developed by TensorFlow which can be used with PyTorch.\nBecause we have sent our model’s metrics logs to TensorBoard as part of our code, a directory called runs with those logs was created in our ~/mnist directory.\n\nLaunch TensorBoard\nTensorBoard requires too much processing power to be run on the login node. When you run long jobs, the best strategy is to launch it in the background as part of the job. This allows you to monitor your model as it is running (and cancel it if things don’t look right).\n\nExample:\n\n#!/bin/bash\n#SBATCH ...\n#SBATCH ...\n\ntensorboard --logdir=runs --host 0.0.0.0 &\npython ~/mnist/nn.py\nBecause we only have 1 GPU and are taking turns running our jobs, we need to keep our jobs very short here. So we will launch a separate job for TensorBoard. This time, we will launch an interactive job:\nsalloc --time=1:0:0 --mem=2000M\nTo launch TensorBoard, we need to activate our Python virtual environment (TensorBoard was installed by pip):\nsource ~/projects/def-sponsor00/env/bin/activate\nThen we can launch TensorBoard in the background:\ntensorboard --logdir=~/mnist/runs --host 0.0.0.0 &\nNow, we need to create a connection with SSH tunnelling between your computer and the compute note running your TensorBoard job.\n\n\nConnect to TensorBoard from your computer\nFrom a new terminal on your computer, run:\nssh -NfL localhost:6006:&lt;hostname&gt;:6006 userxxx@uu.c3.ca\n\nReplace &lt;hostname&gt; by the name of the compute node running your salloc job. You can find it by looking at your prompt (your prompt shows &lt;username&gt;@&lt;hostname&gt;).\nReplace &lt;userxxx&gt; by your user name.\n\nNow, you can open a browser on your computer and access TensorBoard at http://localhost:6006."
  },
  {
    "objectID": "ml/pt_nn.html",
    "href": "ml/pt_nn.html",
    "title": "Introduction to neural networks",
    "section": "",
    "text": "3Blue1Brown by Grant Sanderson has a series of 4 videos on neural networks which is easy to watch, fun, and does an excellent job at explaining the functioning of a simple neural network. In this section, we will watch the first 2 videos as an introduction to what neural networks are and how they learn."
  },
  {
    "objectID": "ml/pt_nn.html#what-are-nn-19-min",
    "href": "ml/pt_nn.html#what-are-nn-19-min",
    "title": "Introduction to neural networks",
    "section": "What are NN? (19 min)",
    "text": "What are NN? (19 min)\n\n\nAs you develop your own ML models, if you find that your mathematical background is shaky, 3blue1brown also has an excellent series of videos on linear algebra and an equally great one on calculus."
  },
  {
    "objectID": "ml/pt_nn.html#how-do-nn-learn-21-min",
    "href": "ml/pt_nn.html#how-do-nn-learn-21-min",
    "title": "Introduction to neural networks",
    "section": "How do NN learn? (21 min)",
    "text": "How do NN learn? (21 min)"
  },
  {
    "objectID": "ml/pt_nn.html#nn-vs-biological-neurons-and-types-of-nn",
    "href": "ml/pt_nn.html#nn-vs-biological-neurons-and-types-of-nn",
    "title": "Introduction to neural networks",
    "section": "NN vs biological neurons and types of NN",
    "text": "NN vs biological neurons and types of NN\nSlides (Click and wait: the presentation might take a few instants to load)"
  },
  {
    "objectID": "ml/pt_pytorch.html",
    "href": "ml/pt_pytorch.html",
    "title": "The PyTorch API",
    "section": "",
    "text": "PyTorch is a free and open-source machine learning and scientific computing framework based on Torch. While Torch uses a scripting language based on Lua, PyTorch has a Python and a C++ interface.\nCreated by Meta AI (formerly Facebook, Inc.) in 2017, it is now a project of The Linux Foundation.\nPyTorch is widely used in academia and research. Part of its popularity stems from the fact that the Python interface is truly pythonic in nature, making it easier to learn than other popular frameworks such as TensorFlow.\nThe PyTorch API is vast and complex. This section links to the key components to get you started."
  },
  {
    "objectID": "ml/pt_pytorch.html#domain-specific-libraries",
    "href": "ml/pt_pytorch.html#domain-specific-libraries",
    "title": "The PyTorch API",
    "section": "Domain-specific libraries",
    "text": "Domain-specific libraries\nPyTorch is a large framework with domain-specific libraries:\n\nTorchVision for computer vision,\nTorchText for natural languages,\nTorchAudio for audio and signal processing.\n\nThese libraries contain standard datasets and utilities specific to the data in those fields."
  },
  {
    "objectID": "ml/pt_pytorch.html#loading-data",
    "href": "ml/pt_pytorch.html#loading-data",
    "title": "The PyTorch API",
    "section": "Loading data",
    "text": "Loading data\ntorch.utils.data contains everything you need create data loaders (iterables that present the data to a model)."
  },
  {
    "objectID": "ml/pt_pytorch.html#building-models",
    "href": "ml/pt_pytorch.html#building-models",
    "title": "The PyTorch API",
    "section": "Building models",
    "text": "Building models\ntorch.nn contains the elements you need to build your model architecture and chose a loss function."
  },
  {
    "objectID": "ml/pt_pytorch.html#training",
    "href": "ml/pt_pytorch.html#training",
    "title": "The PyTorch API",
    "section": "Training",
    "text": "Training\nTraining a model consists of optimizing the model parameters.\ntorch.autograd contains the tools for automatic differentiation (to compute the gradients, that is the tensors containing the partial derivatives of the error with respect to the parameters of the functions in the model) and torch.optim contains optimization algorithms that can be used for gradient descent."
  },
  {
    "objectID": "ml/pt_supervised_learning.html",
    "href": "ml/pt_supervised_learning.html",
    "title": "Understanding supervised learning",
    "section": "",
    "text": "In supervised learning, neural networks learn by adjusting their parameters automatically in an iterative manner. This is derived from Arthur Samuel’s concept.\nIt is important to get a good understanding of this process, so let’s go over it step by step."
  },
  {
    "objectID": "ml/pt_supervised_learning.html#decide-on-an-architecture",
    "href": "ml/pt_supervised_learning.html#decide-on-an-architecture",
    "title": "Understanding supervised learning",
    "section": "Decide on an architecture",
    "text": "Decide on an architecture\n\nThe architecture won’t change during training. This is set. The type of architecture you choose (e.g. CNN, Transformer, etc.) depends on the type of data you have (e.g. vision, textual, etc.). The depth and breadth of your network depend on the amount of data and computing resource you have."
  },
  {
    "objectID": "ml/pt_supervised_learning.html#set-some-initial-parameters",
    "href": "ml/pt_supervised_learning.html#set-some-initial-parameters",
    "title": "Understanding supervised learning",
    "section": "Set some initial parameters",
    "text": "Set some initial parameters\n\nYou can initialize them randomly or get much better ones through transfer learning.\nWhile the parameters are also part of the model, those will change during training."
  },
  {
    "objectID": "ml/pt_supervised_learning.html#get-some-labelled-data",
    "href": "ml/pt_supervised_learning.html#get-some-labelled-data",
    "title": "Understanding supervised learning",
    "section": "Get some labelled data",
    "text": "Get some labelled data\n\nWhen we say that we need a lot of data for machine learning, we mean “lots of labelled data” as this is what gets used for training models."
  },
  {
    "objectID": "ml/pt_supervised_learning.html#make-sure-to-keep-some-data-for-testing",
    "href": "ml/pt_supervised_learning.html#make-sure-to-keep-some-data-for-testing",
    "title": "Understanding supervised learning",
    "section": "Make sure to keep some data for testing",
    "text": "Make sure to keep some data for testing\n\nThose data won’t be used for training the model. Often people keep around 20% of their data for testing."
  },
  {
    "objectID": "ml/pt_supervised_learning.html#pass-data-and-parameters-through-the-architecture",
    "href": "ml/pt_supervised_learning.html#pass-data-and-parameters-through-the-architecture",
    "title": "Understanding supervised learning",
    "section": "Pass data and parameters through the architecture",
    "text": "Pass data and parameters through the architecture\n\nThe train data are the inputs and the process of calculating the outputs is the forward pass."
  },
  {
    "objectID": "ml/pt_supervised_learning.html#the-outputs-of-the-model-are-predictions",
    "href": "ml/pt_supervised_learning.html#the-outputs-of-the-model-are-predictions",
    "title": "Understanding supervised learning",
    "section": "The outputs of the model are predictions",
    "text": "The outputs of the model are predictions"
  },
  {
    "objectID": "ml/pt_supervised_learning.html#compare-those-predictions-to-the-train-labels",
    "href": "ml/pt_supervised_learning.html#compare-those-predictions-to-the-train-labels",
    "title": "Understanding supervised learning",
    "section": "Compare those predictions to the train labels",
    "text": "Compare those predictions to the train labels\n\nSince our data was labelled, we know what the true outputs are."
  },
  {
    "objectID": "ml/pt_supervised_learning.html#calculate-train-loss",
    "href": "ml/pt_supervised_learning.html#calculate-train-loss",
    "title": "Understanding supervised learning",
    "section": "Calculate train loss",
    "text": "Calculate train loss\n\nThe deviation of our predictions from the true outputs gives us a measure of training loss."
  },
  {
    "objectID": "ml/pt_supervised_learning.html#adjust-parameters",
    "href": "ml/pt_supervised_learning.html#adjust-parameters",
    "title": "Understanding supervised learning",
    "section": "Adjust parameters",
    "text": "Adjust parameters\n\nThe parameters get automatically adjusted to reduce the training loss through the mechanism of backpropagation.\nThis is the actual training part.\nThis process is repeated many times. Training models is pretty much a giant for loop."
  },
  {
    "objectID": "ml/pt_supervised_learning.html#from-model-to-program",
    "href": "ml/pt_supervised_learning.html#from-model-to-program",
    "title": "Understanding supervised learning",
    "section": "From model to program",
    "text": "From model to program\n\nRemember that the model architecture is fixed, but that the parameters change at each iteration of the training process.\nWhile the labelled data are key to training, what we are really interested in is the combination of architecture + final parameters.\n\nWhen the training is over, the parameters become fixed. Which means that our model now behaves like a classic program."
  },
  {
    "objectID": "ml/pt_supervised_learning.html#evaluate-the-model",
    "href": "ml/pt_supervised_learning.html#evaluate-the-model",
    "title": "Understanding supervised learning",
    "section": "Evaluate the model",
    "text": "Evaluate the model\n\nWe can now use the testing set (which was never used to train the model) to evaluate our model: if we pass the test inputs through our program, we get some predictions that we can compare to the test labels (which are the true outputs).\nThis gives us the test loss: a measure of how well our model performs."
  },
  {
    "objectID": "ml/pt_supervised_learning.html#use-the-model",
    "href": "ml/pt_supervised_learning.html#use-the-model",
    "title": "Understanding supervised learning",
    "section": "Use the model",
    "text": "Use the model\n\nNow that we have a program, we can use it on unlabelled inputs to get what people ultimately want: unknown outputs. This is when we put our model to actual use to solve some problem."
  },
  {
    "objectID": "ml/pt_training.html",
    "href": "ml/pt_training.html",
    "title": "Training",
    "section": "",
    "text": "After you have created the data loaders and defined your model, it is time to improve the weights and biases through training."
  },
  {
    "objectID": "ml/pt_training.html#chose-hyperparameters",
    "href": "ml/pt_training.html#chose-hyperparameters",
    "title": "Training",
    "section": "Chose hyperparameters",
    "text": "Chose hyperparameters\nWhile the learning parameters of a model (weights and biases) are the values that get adjusted through training (and they will become part of the final program, along with the model architecture, once training is over), hyperparameters control the training process.\nThey include:\n\nthe batch size: number of samples passed through the model before the parameters are updated,\nthe number of epochs: number iterations,\nthe learning rate (lr): size of the incremental changes to model parameters at each iteration. Smaller values yield slow learning speed, while large values may miss minima.\n\nLet’s define them here:\nlearning_rate = 1e-3\nbatch_size = 64\nepochs = 5"
  },
  {
    "objectID": "ml/pt_training.html#define-the-loss-function",
    "href": "ml/pt_training.html#define-the-loss-function",
    "title": "Training",
    "section": "Define the loss function",
    "text": "Define the loss function\nTo assess the predicted outputs of our model against the true values from the labels, we also need a loss function (e.g. mean square error for regressions: nn.MSELoss or negative log likelihood for classification: nn.NLLLoss)\nThe machine learning literature is rich in information about various loss functions.\nHere is an example with nn.CrossEntropyLoss which combines nn.LogSoftmax and nn.NLLLoss:\nloss_fn = nn.CrossEntropyLoss()"
  },
  {
    "objectID": "ml/pt_training.html#initialize-the-optimizer",
    "href": "ml/pt_training.html#initialize-the-optimizer",
    "title": "Training",
    "section": "Initialize the optimizer",
    "text": "Initialize the optimizer\nThe optimization algorithm determines how the model parameters get adjusted at each iteration.\nThere are many optimizers and you need to search in the literature which one performs best for your time of model and data.\nBelow is an example with stochastic gradient descent:\noptimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n\nlr is the learning rate.\nmomentum is a method increasing convergence rate and reducing oscillation for SDG."
  },
  {
    "objectID": "ml/pt_training.html#define-the-train-and-test-loops",
    "href": "ml/pt_training.html#define-the-train-and-test-loops",
    "title": "Training",
    "section": "Define the train and test loops",
    "text": "Define the train and test loops\nFinally, we need to define the train and test loops.\nThe train loop:\n\ngets a batch of training data from the DataLoader,\nresets the gradients of model parameters with optimizer.zero_grad(),\ncalculates predictions from the model for an input batch,\ncalculates the loss for that set of predictions vs. the labels on the dataset,\ncalculates the backward gradients over the learning parameters (that’s the backpropagation) with loss.backward(),\nadjusts the parameters by the gradients collected in the backward pass with optimizer.step().\n\nThe test loop evaluates the model’s performance against the test data.\ndef train(dataloader, model, loss_fn, optimizer):\n    size = len(dataloader.dataset)\n    for batch, (X, y) in enumerate(dataloader):\n        # Compute prediction and loss\n        pred = model(X)\n        loss = loss_fn(pred, y)\n\n        # Backpropagation\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if batch % 100 == 0:\n            loss, current = loss.item(), batch * len(X)\n            print(f\"loss: {loss:&gt;7f}  [{current:&gt;5d}/{size:&gt;5d}]\")\n\n\ndef test(dataloader, model, loss_fn):\n    size = len(dataloader.dataset)\n    num_batches = len(dataloader)\n    test_loss, correct = 0, 0\n\n    with torch.no_grad():\n        for X, y in dataloader:\n            pred = model(X)\n            test_loss += loss_fn(pred, y).item()\n            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n\n    test_loss /= num_batches\n    correct /= size\n    print(f\"Test Error: \\n Accuracy: {(100*correct):&gt;0.1f}%, Avg loss: {test_loss:&gt;8f} \\n\")"
  },
  {
    "objectID": "ml/pt_training.html#train",
    "href": "ml/pt_training.html#train",
    "title": "Training",
    "section": "Train",
    "text": "Train\nTo train our model, we run the loop over the epochs:\nfor t in range(epochs):\n    print(f\"Epoch {t+1}\\n-------------------------------\")\n    train(train_dataloader, model, loss_fn, optimizer)\n    test(test_dataloader, model, loss_fn)\nprint(\"Training completed\")"
  },
  {
    "objectID": "ml/sk_intro.html",
    "href": "ml/sk_intro.html",
    "title": "Introduction",
    "section": "",
    "text": "What is scikit-learn and how does it fit in the wide landscape of machine learning frameworks?"
  },
  {
    "objectID": "ml/sk_intro.html#what-is-machine-learning",
    "href": "ml/sk_intro.html#what-is-machine-learning",
    "title": "Introduction",
    "section": "What is machine learning?",
    "text": "What is machine learning?\nMaybe we should start with some definitions:\n\nArtificial intelligence (AI) can be defined as any human-made system mimicking animal intelligence. This is a large and very diverse field.\nMachine learning (ML) is a subfield of AI that can be defined as computer programs whose performance at a task improves with experience. This learning can be split into three categories:\n\nSupervised learning\nRegression is a form of supervised learning with continuous outputs. Classification is supervised learning with discrete outputs.\nSupervised learning uses training data in the form of example input/output pairs.\nThe goal is to find the relationship between inputs and outputs.\nUnsupervised learning\nClustering, social network analysis, market segmentation, dimensionality reduction (e.g. PCA), anomaly detection … are all forms of unsupervised learning.\nUnsupervised learning uses unlabelled data and the goal is to find patterns within the data.\nReinforcement learning\nThe algorithm explores by performing random actions and these actions are rewarded or punished (bonus points or penalties).\nThis is how algorithms learn to play games.\n\nDeep learning (DL) is itself a subfield of machine learning using deep artificial neural networks."
  },
  {
    "objectID": "ml/sk_intro.html#what-is-scikit-learn",
    "href": "ml/sk_intro.html#what-is-scikit-learn",
    "title": "Introduction",
    "section": "What is scikit-learn?",
    "text": "What is scikit-learn?\nScikit-learn (also called sklearn) is a free and open source set of libraries for Python built on top of SciPy (thus using NumPy’s ndarray as the main data structure) used for machine learning.\nSklearn is a rich toolkit characterized by a clean, consistent, and very simple API.\nIt is easy to learn and very well documented. You can think of it as a vast collection of routines that are easy to apply and require little computing power.\n\n\nfrom scikit-learn.org"
  },
  {
    "objectID": "ml/sk_intro.html#scikit-learn-in-the-ml-landscape",
    "href": "ml/sk_intro.html#scikit-learn-in-the-ml-landscape",
    "title": "Introduction",
    "section": "Scikit-learn in the ML landscape",
    "text": "Scikit-learn in the ML landscape\nThere are many machine learning frameworks. While sklearn has many advantages (ease of use, wide range of tools), it has clear limitations: there is no GPU support and deep learning and reinforcement learning are out of its scope (the only neural network implemented is a multilayer perceptron (an historical, very basic, and terribly inefficient neural network).\nIf you want to access the power and flexibility that deep neural networks provide, you should turn towards tools such as PyTorch (wonderful tool both in academia, research, and industry) or the higher-level Keras (easy API, easy to learn, but less flexible)."
  },
  {
    "objectID": "ml/top_jx.html",
    "href": "ml/top_jx.html",
    "title": "Faster deep learning with JAX",
    "section": "",
    "text": "Faster than TensorFlow and PyTorch, JAX is the new open source Google deep learning framework. Based on Python and NumPy, it uses the XLA compiler for linear algebra and just-in-time compilation. It runs on accelerators (GPUs and TPUs) and provides a powerful and flexible automatic differentiation engine.\n\n\n\n\n\n\n Start course ➤"
  },
  {
    "objectID": "ml/wb_fastai.html",
    "href": "ml/wb_fastai.html",
    "title": "fastai",
    "section": "",
    "text": "fastai is a deep learning library which provides practitioners with high-level components that can quickly and easily provide state-of-the-art results in standard deep learning domains. This webinar will take a closer look at the features and functionality of fastai."
  },
  {
    "objectID": "ml/wb_frameworks.html",
    "href": "ml/wb_frameworks.html",
    "title": "A map of current machine learning frameworks",
    "section": "",
    "text": "We are in a period of active development of new deep learning techniques, adding to the already mature area of traditional machine learning. This is leading to a vast and ever evolving field of implementations which can be disorienting.\nIn this webinar, I will guide you through a map of the current frameworks, organizing them based on their domain (machine learning vs deep learning) and the languages required to use them. I will also talk about the various automatic differentiation options available.\nTo narrow such a large topic, I am limiting the map to frameworks that can be used from Python, Julia, and R.\n\nSlides (Click and wait: this reveal.js presentation is heavy and takes some time to load.)"
  },
  {
    "objectID": "ml/wb_torchtensors.html",
    "href": "ml/wb_torchtensors.html",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "",
    "text": "Before information can be fed to artificial neural networks (ANNs), it needs to be converted to a form ANNs can process: floating point numbers. Indeed, you don’t pass a sentence or an image through an ANN; you input numbers representing a sequence of words or pixel values.\nAll these floating point numbers need to be stored in a data structure. The most suited structure is multidimensional (to hold several layers of information) and since all data is of the same type, it is an array.\nPython already has several multidimensional array structures—the most popular of which being NumPy’s ndarray—but the particularities of deep learning call for special characteristics: ability to run operations on GPUs and/or in a distributed fashion, as well as the ability to keep track of computation graphs for automatic differentiation.\nThe PyTorch tensor is a Python data structure with these characteristics that can also easily be converted to/from NumPy’s ndarray and integrates well with other Python libraries such as Pandas.\nIn this webinar, suitable for users of all levels, we will have a deep look at this data structure and go much beyond a basic introduction.\nIn particular, we will:\n\nsee how tensors are stored in memory,\nlook at the metadata which allows this efficient memory storage,\ncover the basics of working with tensors (indexing, vectorized operations…),\nmove tensors to/from GPUs,\nconvert tensors to/from NumPy ndarrays,\nsee how tensors work in distributed frameworks,\nsee how linear algebra can be done with PyTorch tensors.\n\n\nSlides (Click and wait: this reveal.js presentation may take a little time to load.)"
  },
  {
    "objectID": "ml/wb_upscaling.html",
    "href": "ml/wb_upscaling.html",
    "title": "Image upscaling",
    "section": "",
    "text": "Super-resolution—the process of (re)creating high resolution images from low resolution ones—is an old field, but deep neural networks have seen a sudden surge of new and very impressive methods over the past 10 years, from SRCCN to SRGAN to Transformers.\nIn this webinar, I will give a quick overview of these methods and show how the latest state-of-the-art model—SwinIR—performs on a few test images. We will use PyTorch as our framework.\n\nSlides (Click and wait: this reveal.js presentation is heavy and takes some time to load.)"
  },
  {
    "objectID": "ml/ws_audio_dataloader.html",
    "href": "ml/ws_audio_dataloader.html",
    "title": "Creating an audio DataLoader",
    "section": "",
    "text": "import torch\nimport torchaudio\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "ml/ws_audio_dataloader.html#load-packages",
    "href": "ml/ws_audio_dataloader.html#load-packages",
    "title": "Creating an audio DataLoader",
    "section": "",
    "text": "import torch\nimport torchaudio\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "ml/ws_audio_dataloader.html#download-and-unzip-data",
    "href": "ml/ws_audio_dataloader.html#download-and-unzip-data",
    "title": "Creating an audio DataLoader",
    "section": "Download and unzip data",
    "text": "Download and unzip data\nPyTorch comes with many classic datasets.\n\nExamples:\n\nlist of available datasets for vision\nlist of audio datasets\nlist of texts datasets\n\n\nThis is convenient to develop and test your model, or to compare its performance with existing models using these datasets.\nHere, we will use the YESNO dataset which can be accessed through the torchaudio.datasets.YESNO class:\nhelp(torchaudio.datasets.YESNO)\nHelp on class YESNO in module torchaudio.datasets.yesno:\n\nclass YESNO(torch.utils.data.dataset.Dataset)\n\n |  YESNO(root: Union[str, pathlib.Path], url: str =\n |    'http://www.openslr.org/resources/1/waves_yesno.tar.gz', \n |    folder_in_archive: str = 'waves_yesno', \n |    download: bool = False) -&gt; None\n |  \n |  Args:\n |    root (str or Path): Path to the directory where the dataset is found \n |      or downloaded.\n |    url (str, optional): The URL to download the dataset from.\n |      (default: \"http://www.openslr.org/resources/1/waves_yesno.tar.gz\")\n |    folder_in_archive (str, optional):\n |      The top-level directory of the dataset. (default: \"waves_yesno\")\n |    download (bool, optional):\n |      Whether to download the dataset if it is not found at root path. \n |      (default: False).\nThe root argument sets the location of the downloaded data.\n\nWhere to store this data in the cluster\nWe will all use the same data. It would make little sense to all download it in our home directory.\n\nIn the Alliance clusters, a good place to store data shared amongst members of a project is in the /project file system.\nYou usually belong to /project/def-&lt;group&gt;, where &lt;group&gt; is the name of your PI. You can access it from your home directory through the symbolic link ~/projects/def-&lt;group&gt;.\n\nIn our training cluster, we are all part of the group def-sponsor00, accessible through /project/def-sponsor00 (or the hyperlink ~/projects/def-sponsor00).\nWe will thus use ~/projects/def-sponsor00/data as the root argument for torchaudio.datasets.yesno):\nyesno_data = torchaudio.datasets.YESNO(\n    '~/projects/def-sponsor00/data/',\n    download=True)"
  },
  {
    "objectID": "ml/ws_audio_dataloader.html#explore-the-data",
    "href": "ml/ws_audio_dataloader.html#explore-the-data",
    "title": "Creating an audio DataLoader",
    "section": "Explore the data",
    "text": "Explore the data\nA data point in YESNO is a tuple of waveform, sample_rate, and labels (the labels are 1 for “yes” and 0 for “no”).\nLet’s have a look at the first data point:\n\nyesno_data[0]\n\n(tensor([[ 3.0518e-05,  6.1035e-05,  3.0518e-05,  ..., -1.8616e-03,\n          -2.2583e-03, -1.3733e-03]]),\n 8000,\n [0, 0, 0, 0, 1, 1, 1, 1])\n\n\nOr, more nicely:\n\nwaveform, sample_rate, labels = yesno_data[0]\nprint(\"Waveform: {}\\nSample rate: {}\\nLabels: {}\".format(waveform, sample_rate, labels))\n\nWaveform: tensor([[ 3.0518e-05,  6.1035e-05,  3.0518e-05,  ..., -1.8616e-03,\n         -2.2583e-03, -1.3733e-03]])\nSample rate: 8000\nLabels: [0, 0, 0, 0, 1, 1, 1, 1]\n\n\nYou can also plot the data. For this, we will use pyplot from matplotlib.\nLet’s look at the waveform:\n\nplt.figure()\nplt.plot(waveform.t().numpy())"
  },
  {
    "objectID": "ml/ws_audio_dataloader.html#split-the-data-into-a-training-set-and-a-testing-set",
    "href": "ml/ws_audio_dataloader.html#split-the-data-into-a-training-set-and-a-testing-set",
    "title": "Creating an audio DataLoader",
    "section": "Split the data into a training set and a testing set",
    "text": "Split the data into a training set and a testing set\n\ntrain_size = int(0.8 * len(yesno_data))\ntest_size = len(yesno_data) - train_size\ntrain_dataset, test_dataset = torch.utils.data.random_split(yesno_data, [train_size, test_size])"
  },
  {
    "objectID": "ml/ws_audio_dataloader.html#create-training-and-testing-dataloaders",
    "href": "ml/ws_audio_dataloader.html#create-training-and-testing-dataloaders",
    "title": "Creating an audio DataLoader",
    "section": "Create training and testing DataLoaders",
    "text": "Create training and testing DataLoaders\nDataLoaders are Python iterables created by the torch.utils.data.DataLoader class from a dataset and a sampler.\nWe already have a dataset (yesno_data). Now we need a sampler (or sampling strategy) to draw samples from it. The sampling strategy contains the batch size, whether the data get shuffled prior to sampling, the number of workers used if the data is loaded in parallel, etc.\nTo create a training DataLoader with shuffled data and batch size of 1 (the default), we run:\n\ntrain_loader = torch.utils.data.DataLoader(train_dataset, shuffle=True)\n\ndata_loader is an iterable of 0.8*60=48 elements (80% of the 60 samples in the YESNO dataset):\n\nlen(train_loader)\n\n48\n\n\nWe do the same to create the testing DataLoader:\n\ntest_loader = torch.utils.data.DataLoader(test_dataset, shuffle=True)"
  },
  {
    "objectID": "ml/ws_hss_intro_slides.html#ml-allows-to-achieve-previously-impossible-tasks",
    "href": "ml/ws_hss_intro_slides.html#ml-allows-to-achieve-previously-impossible-tasks",
    "title": "Introduction to machine learning for the humanities",
    "section": "ML allows to achieve previously impossible tasks",
    "text": "ML allows to achieve previously impossible tasks\n\nLet’s take the example of image recognition:\n\nIn typical computing, a programmer writes code that gives a computer detailed instructions of what to do.\nCoding all the possible ways—pixel by pixel—that an image can represent, say, a dog is an impossibly large task: there are many breeds of dogs, the image can be a picture, a blurred picture, a drawing, a cartoon, the dog can be in all sorts of positions, wearing clothes, etc.\nThere just aren’t enough resources to make the traditional programming approach able to create a computer program that can identify a dog in images.\nBy feeding a very large number of dog images to a neural network however, we can train that network to recognize dogs in images that it has never seen (without explicitly programming how it does this!)."
  },
  {
    "objectID": "ml/ws_hss_intro_slides.html#old-concept-new-computing-power",
    "href": "ml/ws_hss_intro_slides.html#old-concept-new-computing-power",
    "title": "Introduction to machine learning for the humanities",
    "section": "Old concept … new computing power",
    "text": "Old concept … new computing power\nThe concept is everything but new: Arthur Samuel came up with it in 1949 and built a self-learning Checkers-playing program in 1959.\n\n\nMachine learning consists of feeding vast amounts of data to algorithms to strengthen pathways, so the excitement for the approach became somewhat dormant due to the lack of computing power and the lack of training data at the time.\nThe advent of powerful computers, GPUs, and massive amounts of data have brought the old concept to the forefront.\n\n\n\n\n\n\nFrom xkcd.com"
  },
  {
    "objectID": "ml/ws_hss_intro_slides.html#supervised-learning",
    "href": "ml/ws_hss_intro_slides.html#supervised-learning",
    "title": "Introduction to machine learning for the humanities",
    "section": "Supervised learning",
    "text": "Supervised learning\n\nRegression is a form of supervised learning with continuous outputs\nClassification is supervised learning with discrete outputs\n\nSupervised learning uses training data in the form of example input/output pairs.\nGoal\nFind the relationship between inputs and outputs."
  },
  {
    "objectID": "ml/ws_hss_intro_slides.html#unsupervised-learning",
    "href": "ml/ws_hss_intro_slides.html#unsupervised-learning",
    "title": "Introduction to machine learning for the humanities",
    "section": "Unsupervised learning",
    "text": "Unsupervised learning\nClustering, social network analysis, market segmentation, PCA … are all forms of unsupervised learning.\nUnsupervised learning uses unlabelled data.\nGoal\nFind structure within the data."
  },
  {
    "objectID": "ml/ws_hss_intro_slides.html#reinforcement-learning",
    "href": "ml/ws_hss_intro_slides.html#reinforcement-learning",
    "title": "Introduction to machine learning for the humanities",
    "section": "Reinforcement learning",
    "text": "Reinforcement learning\nThe algorithm explores by performing random actions and these actions are rewarded or punished (bonus points or penalties).\nThis is how algorithms learn to play games."
  },
  {
    "objectID": "ml/ws_hss_intro_slides.html#decide-on-an-architecture",
    "href": "ml/ws_hss_intro_slides.html#decide-on-an-architecture",
    "title": "Introduction to machine learning for the humanities",
    "section": "Decide on an architecture",
    "text": "Decide on an architecture\n\nThe architecture won’t change during training.\nThe type of architecture you choose (e.g. CNN, Transformer) depends on the type of data you have (e.g. vision, textual). The depth and breadth of your network depend on the amount of data and computing resource you have."
  },
  {
    "objectID": "ml/ws_hss_intro_slides.html#set-some-initial-parameters",
    "href": "ml/ws_hss_intro_slides.html#set-some-initial-parameters",
    "title": "Introduction to machine learning for the humanities",
    "section": "Set some initial parameters",
    "text": "Set some initial parameters\n\nYou can initialize them randomly or get much better ones through transfer learning.\nWhile the parameters are also part of the model, those will change during training."
  },
  {
    "objectID": "ml/ws_hss_intro_slides.html#get-some-labelled-data",
    "href": "ml/ws_hss_intro_slides.html#get-some-labelled-data",
    "title": "Introduction to machine learning for the humanities",
    "section": "Get some labelled data",
    "text": "Get some labelled data\n\nWhen we say that we need a lot of data for machine learning, we mean “lots of labelled data” as this is what gets used for training models."
  },
  {
    "objectID": "ml/ws_hss_intro_slides.html#make-sure-to-keep-some-data-for-testing",
    "href": "ml/ws_hss_intro_slides.html#make-sure-to-keep-some-data-for-testing",
    "title": "Introduction to machine learning for the humanities",
    "section": "Make sure to keep some data for testing",
    "text": "Make sure to keep some data for testing\n\nThose data won’t be used for training the model. Often people keep around 20% of their data for testing."
  },
  {
    "objectID": "ml/ws_hss_intro_slides.html#pass-data-and-parameters-through-the-architecture",
    "href": "ml/ws_hss_intro_slides.html#pass-data-and-parameters-through-the-architecture",
    "title": "Introduction to machine learning for the humanities",
    "section": "Pass data and parameters through the architecture",
    "text": "Pass data and parameters through the architecture\n\nThe train data are the inputs and the process of calculating the outputs is the forward pass."
  },
  {
    "objectID": "ml/ws_hss_intro_slides.html#the-outputs-of-the-model-are-predictions",
    "href": "ml/ws_hss_intro_slides.html#the-outputs-of-the-model-are-predictions",
    "title": "Introduction to machine learning for the humanities",
    "section": "The outputs of the model are predictions",
    "text": "The outputs of the model are predictions"
  },
  {
    "objectID": "ml/ws_hss_intro_slides.html#compare-those-predictions-to-the-train-labels",
    "href": "ml/ws_hss_intro_slides.html#compare-those-predictions-to-the-train-labels",
    "title": "Introduction to machine learning for the humanities",
    "section": "Compare those predictions to the train labels",
    "text": "Compare those predictions to the train labels\n\nSince our data was labelled, we know what the true outputs are."
  },
  {
    "objectID": "ml/ws_hss_intro_slides.html#calculate-train-loss",
    "href": "ml/ws_hss_intro_slides.html#calculate-train-loss",
    "title": "Introduction to machine learning for the humanities",
    "section": "Calculate train loss",
    "text": "Calculate train loss\n\nThe deviation of our predictions from the true outputs gives us a measure of training loss."
  },
  {
    "objectID": "ml/ws_hss_intro_slides.html#adjust-parameters",
    "href": "ml/ws_hss_intro_slides.html#adjust-parameters",
    "title": "Introduction to machine learning for the humanities",
    "section": "Adjust parameters",
    "text": "Adjust parameters\n\nThe parameters get automatically adjusted to reduce the training loss through the mechanism of backpropagation. This is the actual training part.\nThis process is repeated many times. Training models is pretty much a giant for loop."
  },
  {
    "objectID": "ml/ws_hss_intro_slides.html#from-model-to-program",
    "href": "ml/ws_hss_intro_slides.html#from-model-to-program",
    "title": "Introduction to machine learning for the humanities",
    "section": "From model to program",
    "text": "From model to program\n\nRemember that the model architecture is fixed, but that the parameters change at each iteration of the training process."
  },
  {
    "objectID": "ml/ws_hss_intro_slides.html#evaluate-the-model",
    "href": "ml/ws_hss_intro_slides.html#evaluate-the-model",
    "title": "Introduction to machine learning for the humanities",
    "section": "Evaluate the model",
    "text": "Evaluate the model\n\nWe can now use the testing set (which was never used to train the model) to evaluate our model: if we pass the test inputs through our program, we get some predictions that we can compare to the test labels (which are the true outputs).\nThis gives us the test loss: a measure of how well our model performs."
  },
  {
    "objectID": "ml/ws_hss_intro_slides.html#use-the-model",
    "href": "ml/ws_hss_intro_slides.html#use-the-model",
    "title": "Introduction to machine learning for the humanities",
    "section": "Use the model",
    "text": "Use the model\n\nNow that we have a program, we can use it on unlabelled inputs to get what people ultimately want: unknown outputs.\nThis is when we put our model to actual use to solve some problem."
  },
  {
    "objectID": "ml/ws_hss_intro_slides.html#learning",
    "href": "ml/ws_hss_intro_slides.html#learning",
    "title": "Introduction to machine learning for the humanities",
    "section": "Learning",
    "text": "Learning\n\n\nThe process of learning in biological NN happens through neuron death or growth and the creation or loss of synaptic connections between neurons.\n\n\n\nIn ANN, learning happens through optimization algorithms such as gradient descent which minimize cross entropy loss functions by adjusting the weights and biases connecting each layer of neurons over many iterations."
  },
  {
    "objectID": "ml/ws_hss_intro_slides.html#fully-connected-neural-networks",
    "href": "ml/ws_hss_intro_slides.html#fully-connected-neural-networks",
    "title": "Introduction to machine learning for the humanities",
    "section": "Fully connected neural networks",
    "text": "Fully connected neural networks\n\n\n\n\n\nFrom Glosser.ca, Wikipedia\n\n\n\nEach neuron receives inputs from every neuron of the previous layer and passes its output to every neuron of the next layer."
  },
  {
    "objectID": "ml/ws_hss_intro_slides.html#convolutional-neural-networks",
    "href": "ml/ws_hss_intro_slides.html#convolutional-neural-networks",
    "title": "Introduction to machine learning for the humanities",
    "section": "Convolutional neural networks",
    "text": "Convolutional neural networks\n\n\nFrom Programming Journeys by Rensu Theart\n\nConvolutional neural networks (CNN) are used for spatially structured data (e.g. images).\nImages have huge input sizes and would require a very large number of neurons in a fully connected neural net. In convolutional layers, neurons receive input from a subarea (called local receptive field) of the previous layer. This greatly reduces the number of parameters. Optionally, pooling (combining the outputs of neurons in a subarea) reduces the data dimensions."
  },
  {
    "objectID": "ml/ws_hss_intro_slides.html#recurrent-neural-networks",
    "href": "ml/ws_hss_intro_slides.html#recurrent-neural-networks",
    "title": "Introduction to machine learning for the humanities",
    "section": "Recurrent neural networks",
    "text": "Recurrent neural networks\n\n\nFrom fdeloche, Wikipedia\n\nRecurrent neural networks (RNN) such as Long Short-Term Memory (LSTM) are used for chain structured data (e.g. text).\nThey are not feedforward networks (i.e. networks for which the information moves only in the forward direction without any loop)."
  },
  {
    "objectID": "ml/ws_hss_intro_slides.html#transformers",
    "href": "ml/ws_hss_intro_slides.html#transformers",
    "title": "Introduction to machine learning for the humanities",
    "section": "Transformers",
    "text": "Transformers\nA combination of two RNNs (the encoder and the decoder) is used in sequence to sequence models for translation or picture captioning.\nIn 2014 the concept of attention (giving added weight to important words) was developed, greatly improving the ability of such models to process a lot of data.\nThe problem with recurrence is that it is not easily to parallelize (and thus to run fast on GPUs).\nIn 2017, a new model—the transformer—was proposed: by using only attention mechanisms and no recurrence, the transformer achieves better results in an easily parallelizable fashion.\nWith the addition of transfer learning, powerful transformers emerged in the field of NLP (e.g. Bidirectional Encoder Representations from Transformers (BERT) from Google and Generative Pre-trained Transformer-3 (GPT-3) from OpenAI)."
  },
  {
    "objectID": "ml/ws_hss_intro_slides.html#data-bias",
    "href": "ml/ws_hss_intro_slides.html#data-bias",
    "title": "Introduction to machine learning for the humanities",
    "section": "Data bias",
    "text": "Data bias\nBias is always present in data.\nDocument the limitations and scope of your data as best as possible.\nProblems to watch for:\n\nOut of domain data: data used for training are not relevant to the model application.\nDomain shift: model becoming inadapted as conditions evolve.\nFeedback loop: initial bias exacerbated over the time."
  },
  {
    "objectID": "ml/ws_hss_intro_slides.html#transformation-of-subjects",
    "href": "ml/ws_hss_intro_slides.html#transformation-of-subjects",
    "title": "Introduction to machine learning for the humanities",
    "section": "Transformation of subjects",
    "text": "Transformation of subjects\nAlgorithms are supposed to help us, not transform us (e.g. YouTube recommendation algorithms)."
  },
  {
    "objectID": "ml/ws_hss_intro_slides.html#bugs",
    "href": "ml/ws_hss_intro_slides.html#bugs",
    "title": "Introduction to machine learning for the humanities",
    "section": "Bugs",
    "text": "Bugs\nExample of bug with real life consequences"
  },
  {
    "objectID": "ml/ws_hss_intro_slides.html#many-options",
    "href": "ml/ws_hss_intro_slides.html#many-options",
    "title": "Introduction to machine learning for the humanities",
    "section": "Many options",
    "text": "Many options\nHere are just a few:\n\nscikit-learn: a Python ML library built on top of SciPy.\nNatural Language Toolkit (NLTK): a suite of Python libraries geared towards teaching and research.\nspaCy: Python library geared towards production.\ntorchtext, part of the PyTorch project (and many options of added layers on top such as PyTorch-NLP): Python library.\nGenSim: Python library.\nStanford CoreNLP: Java library.\nMany libraries in the Julia programming language."
  },
  {
    "objectID": "ml/ws_hss_intro_slides.html#which-one-to-choose",
    "href": "ml/ws_hss_intro_slides.html#which-one-to-choose",
    "title": "Introduction to machine learning for the humanities",
    "section": "Which one to choose?",
    "text": "Which one to choose?\nChoose an open source tool (i.e. stay away from proprietary software such as MATLAB).\n\nResearchers who do not have access to the tool cannot reproduce your methods (open tools = open equitable research).\nOnce you graduate, you may not have access to the tool anymore\nYour university may stop paying for a license\nYou may get locked-in\nProprietary tools are often black boxes\nLong-term access is not guaranty (problem to replicate studies)\nThe licenses you have access to may be limiting and a cause of headache\nProprietary tools fall behind popular open-source tools\nProprietary tools often fail to address specialized edge cases needed in research"
  },
  {
    "objectID": "ml/ws_hss_intro_slides.html#neural-nets",
    "href": "ml/ws_hss_intro_slides.html#neural-nets",
    "title": "Introduction to machine learning for the humanities",
    "section": "Neural nets",
    "text": "Neural nets\n3Blue1Brown by Grant Sanderson has a series of 4 videos on neural networks which is easy to watch, fun, and does an excellent job at introducing the functioning of a simple neural network.\n\nWhat are NN? (19 min)\nHow do NN learn? (21 min)\nWhat is backpropagation? (14 min)\nHow does backpropagation work? (10 min)\n\n\nAs you develop your own ML models, if you find that your mathematical background is shaky, 3blue1brown also has an excellent series of videos on linear algebra and an equally great one on calculus."
  },
  {
    "objectID": "ml/ws_hss_intro_slides.html#open-access-preprints",
    "href": "ml/ws_hss_intro_slides.html#open-access-preprints",
    "title": "Introduction to machine learning for the humanities",
    "section": "Open-access preprints",
    "text": "Open-access preprints\n\nArxiv Sanity Preserver by Andrej Karpathy\nML papers in the computer science category on arXiv\nML papers in the stats category on arXiv\nDistill ML research online journal"
  },
  {
    "objectID": "ml/ws_hss_intro_slides.html#advice-and-sources",
    "href": "ml/ws_hss_intro_slides.html#advice-and-sources",
    "title": "Introduction to machine learning for the humanities",
    "section": "Advice and sources",
    "text": "Advice and sources\n\nAdvice and sources from ML research student"
  },
  {
    "objectID": "ml/ws_hss_intro_slides.html#getting-help",
    "href": "ml/ws_hss_intro_slides.html#getting-help",
    "title": "Introduction to machine learning for the humanities",
    "section": "Getting help",
    "text": "Getting help\nStack Overflow …\n\n[machine-learning] tag\n[deep-learning] tag\n[supervised-learning] tag\n[unsupervised-learning] tag\n[semisupervised-learning] tag\n[reinforcement-learning] tag\n[transfer-learning] tag\n[machine-learning-model] tag\n[learning-rate] tag"
  },
  {
    "objectID": "ml/ws_hss_intro_slides.html#open-datasets",
    "href": "ml/ws_hss_intro_slides.html#open-datasets",
    "title": "Introduction to machine learning for the humanities",
    "section": "Open datasets",
    "text": "Open datasets\n\nbenchmarks.ai\nAIBench\nkaggle\nWikipedia"
  },
  {
    "objectID": "ml/ws_hss_intro_slides.html#pytorch",
    "href": "ml/ws_hss_intro_slides.html#pytorch",
    "title": "Introduction to machine learning for the humanities",
    "section": "PyTorch",
    "text": "PyTorch\n\nDocumentation\nTutorials\nExamples\n\nGetting help\n\nPyTorch Discourse forum\nStack Overflow [pytorch] tag\n\nPre-trained models\n\nPyTorch Hub"
  },
  {
    "objectID": "ml/ws_hss_intro_slides.html#python",
    "href": "ml/ws_hss_intro_slides.html#python",
    "title": "Introduction to machine learning for the humanities",
    "section": "Python",
    "text": "Python\nIDE\n\nProject Jupyter\nList of IDEs with description\nComparison of IDEs\nEmacs Python IDE\n\nGetting help\n\nStack Overflow [python] tag\n\n\n\n\n\n\n\n\n\n\n Back to workshop page"
  },
  {
    "objectID": "newsletter.html",
    "href": "newsletter.html",
    "title": "Training events mailing list",
    "section": "",
    "text": "If you want to get informed about upcoming training events, please subscribe to our mailing list: \n(We will only email you about training events.)"
  },
  {
    "objectID": "python/collections.html",
    "href": "python/collections.html",
    "title": "Collections",
    "section": "",
    "text": "Values can be stored in collections. This section introduces tuples, dictionaries, sets, and arrays in Python."
  },
  {
    "objectID": "python/collections.html#lists",
    "href": "python/collections.html#lists",
    "title": "Collections",
    "section": "Lists",
    "text": "Lists\nLists are declared in square brackets:\n\nl = [2, 1, 3]\nl\n\n[2, 1, 3]\n\n\n\ntype(l)\n\nlist\n\n\nThey are mutable:\n\nl.append(0)\nl\n\n[2, 1, 3, 0]\n\n\nLists are ordered:\n\n['b', 'a'] == ['a', 'b']\n\nFalse\n\n\nThey can have repeat values:\n\n['a', 'a', 'a', 't']\n\n['a', 'a', 'a', 't']\n\n\nLists can be homogeneous:\n\n['b', 'a', 'x', 'e']\n\n['b', 'a', 'x', 'e']\n\n\n\ntype('b') == type('a') == type('x') == type('e')\n\nTrue\n\n\nor heterogeneous:\n\n[3, 'some string', 2.9, 'z']\n\n[3, 'some string', 2.9, 'z']\n\n\n\ntype(3) == type('some string') == type(2.9) == type('z')\n\nFalse\n\n\nThey can even be nested:\n\n[3, ['b', 'e', 3.9, ['some string', 9.9]], 8]\n\n[3, ['b', 'e', 3.9, ['some string', 9.9]], 8]\n\n\nThe length of a list is the number of items it contains and can be obtained with the function len:\n\nlen([3, ['b', 'e', 3.9, ['some string', 9.9]], 8])\n\n3\n\n\nTo extract an item from a list, you index it:\n\n[3, ['b', 'e', 3.9, ['some string', 9.9]], 8][0]\n\n3\n\n\n\nPython starts indexing at 0, so what we tend to think of as the “first” element of a list is for Python the “zeroth” element.\n\n\n[3, ['b', 'e', 3.9, ['some string', 9.9]], 8][1]\n\n['b', 'e', 3.9, ['some string', 9.9]]\n\n\n\n[3, ['b', 'e', 3.9, ['some string', 9.9]], 8][2]\n\n8\n\n\n\n# Of course you can't extract items that don't exist\n[3, ['b', 'e', 3.9, ['some string', 9.9]], 8][3]\n\nIndexError: list index out of range\n\n\nYou can index from the end of the list with negative values (here you start at -1 for the last element):\n\n[3, ['b', 'e', 3.9, ['some string', 9.9]], 8][-1]\n\n8\n\n\n\n\nYour turn:\n\nHow could you extract the string 'some string' from the list [3, ['b', 'e', 3.9, ['some string', 9.9]], 8]?\n\nYou can also slice a list:\n\n[3, ['b', 'e', 3.9, ['some string', 9.9]], 8][0:1]\n\n[3]\n\n\n\nNotice how slicing returns a list.\nNotice also how the left index is included but the right index excluded.\n\nIf you omit the first index, the slice starts at the beginning of the list:\n\n[1, 2, 3, 4, 5, 6, 7, 8, 9][:6]\n\n[1, 2, 3, 4, 5, 6]\n\n\nIf you omit the second index, the slice goes to the end of the list:\n\n[1, 2, 3, 4, 5, 6, 7, 8, 9][6:]\n\n[7, 8, 9]\n\n\nWhen slicing, you can specify the stride:\n\n[1, 2, 3, 4, 5, 6, 7, 8, 9][2:7:2]\n\n[3, 5, 7]\n\n\n\nThe default stride is 1:\n\n[1, 2, 3, 4, 5, 6, 7, 8, 9][2:7] == [1, 2, 3, 4, 5, 6, 7, 8, 9][2:7:1]\n\nTrue\n\n\n\nYou can reverse the order of a list with a -1 stride applied on the whole list:\n\n[1, 2, 3, 4, 5, 6, 7, 8, 9][::-1]\n\n[9, 8, 7, 6, 5, 4, 3, 2, 1]\n\n\nYou can test whether an item is in a list:\n\n3 in [3, ['b', 'e', 3.9, ['some string', 9.9]], 8]\n\nTrue\n\n\n\n9 in [3, ['b', 'e', 3.9, ['some string', 9.9]], 8]\n\nFalse\n\n\nor not in a list:\n\n3 not in [3, ['b', 'e', 3.9, ['some string', 9.9]], 8]\n\nFalse\n\n\nYou can get the index (position) of an item inside a list:\n\n[3, ['b', 'e', 3.9, ['some string', 9.9]], 8].index(3)\n\n0\n\n\n\nNote that this only returns the index of the first occurrence:\n\n[3, 3, ['b', 'e', 3.9, ['some string', 9.9]], 8].index(3)\n\n0\n\n\n\nLists are mutable (they can be modified). For instance, you can replace items in a list by other items:\n\nL = [3, ['b', 'e', 3.9, ['some string', 9.9]], 8]\nL\n\n[3, ['b', 'e', 3.9, ['some string', 9.9]], 8]\n\n\n\nL[1] = 2\nL\n\n[3, 2, 8]\n\n\nYou can delete items from a list using their indices with list.pop:\n\nL.pop(2)\nL\n\n[3, 2]\n\n\n\nHere, because we are using list.pop, 2 represents the index (the 3rd item).\n\nor with del:\n\ndel L[0]\nL\n\n[2]\n\n\n\nNotice how a list can have a single item:\n\nlen(L)\n\n1\n\n\nIt is then called a “singleton list”.\n\nYou can also delete items from a list using their values with list.remove:\n\nL.remove(2)\nL\n\n[]\n\n\n\nHere, because we are using list.remove, 2 is the value 2.\n\n\nNotice how a list can even be empty:\n\nlen(L)\n\n0\n\n\nYou can actually initialise empty lists:\n\nM = []\ntype(M)\n\nlist\n\n\n\nYou can add items to a list. One at a time:\n\nL.append(7)\nL\n\n[7]\n\n\nAnd if you want to add multiple items at once?\n\n# This doesn't work...\nL.append(3, 6, 9)\n\nTypeError: list.append() takes exactly one argument (3 given)\n\n\n\n# This doesn't work either (that's not what we wanted)\nL.append([3, 6, 9])\nL\n\n[7, [3, 6, 9]]\n\n\n\n\nYour turn:\n\nFix this mistake we just made and remove the nested list [3, 6, 9].\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nOne option is:\n\ndel L[1]\n\n\n\n\nTo add multiple values to a list (and not a nested list), you need to use list.extend:\n\nL.extend([3, 6, 9])\nL\n\n[7, 3, 6, 9]\n\n\nIf you don’t want to add an item at the end of a list, you can use list.insert(&lt;index&gt;, &lt;object&gt;):\n\nL.insert(3, 'test')\nL\n\n[7, 3, 6, 'test', 9]\n\n\n\n\nYour turn:\n\nLet’s have the following list:\n\nL = [7, [3, 6, 9], 3, 'test', 6, 9]\n\nInsert the string 'nested' in the zeroth position of the nested list [3, 6, 9] in L.\n\nYou can sort an homogeneous list:\n\nL = [3, 9, 10, 0]\nL.sort()\nL\n\n[0, 3, 9, 10]\n\n\n\nL = ['some string', 'b', 'a']\nL.sort()\nL\n\n['a', 'b', 'some string']\n\n\n\nHeterogeneous lists cannot be sorted:\n\nL = [3, ['b', 'e', 3.9, ['some string', 9.9]], 8]\nL.sort()\n\nTypeError: '&lt;' not supported between instances of 'list' and 'int'\n\n\n\nYou can also get the min and max value of homogeneous lists:\n\nmin([3, 9, 10, 0])\n\n0\n\n\n\nmax(['some string', 'b', 'a'])\n\n'some string'\n\n\n\nFor heterogeneous lists, this also doesn’t work:\n\nmin([3, ['b', 'e', 3.9, ['some string', 9.9]], 8])\n\nTypeError: '&lt;' not supported between instances of 'list' and 'int'\n\n\n\nLists can be concatenated with +:\n\nL + [3, 6, 9]\n\n[3, ['b', 'e', 3.9, ['some string', 9.9]], 8, 3, 6, 9]\n\n\nor repeated with *:\n\nL * 3\n\n[3,\n ['b', 'e', 3.9, ['some string', 9.9]],\n 8,\n 3,\n ['b', 'e', 3.9, ['some string', 9.9]],\n 8,\n 3,\n ['b', 'e', 3.9, ['some string', 9.9]],\n 8]\n\n\n\nTo sum up, lists are declared in square brackets. They are mutable, ordered (thus indexable), and possibly heterogeneous collections of values."
  },
  {
    "objectID": "python/collections.html#strings",
    "href": "python/collections.html#strings",
    "title": "Collections",
    "section": "Strings",
    "text": "Strings\nStrings behave (a little) like lists of characters in that they have a length (the number of characters):\n\nS = 'This is a string.'\nlen(S)\n\n17\n\n\nThey have a min and a max:\n\nmin(S)\n\n' '\n\n\n\nmax(S)\n\n't'\n\n\nYou can index them:\n\nS[3]\n\n's'\n\n\nSlice them:\n\nS[10:16]\n\n'string'\n\n\n\n\nYour turn:\n\nReverse the order of the string S.\n\nThey can also be concatenated with +:\n\nT = 'This is another string.'\nprint(S + ' ' + T)\n\nThis is a string. This is another string.\n\n\nor repeated with *:\n\nprint(S * 3)\n\nThis is a string.This is a string.This is a string.\n\n\n\nprint((S + ' ') * 3)\n\nThis is a string. This is a string. This is a string. \n\n\nThis is where the similarities stop however: methods such as list.sort, list.append, etc. will not work on strings."
  },
  {
    "objectID": "python/collections.html#arrays",
    "href": "python/collections.html#arrays",
    "title": "Collections",
    "section": "Arrays",
    "text": "Arrays\nPython comes with a built-in array module. When you need arrays for storing and retrieving data, this module is perfectly suitable and extremely lightweight. This tutorial covers the syntax in detail.\nWhenever you plan on performing calculations on your data however (which is the vast majority of cases), you should instead use the NumPy package, covered in another section."
  },
  {
    "objectID": "python/collections.html#tuples",
    "href": "python/collections.html#tuples",
    "title": "Collections",
    "section": "Tuples",
    "text": "Tuples\nTuples are defined with parentheses:\n\nt = (3, 1, 4, 2)\nt\n\n(3, 1, 4, 2)\n\n\n\ntype(t)\n\ntuple\n\n\nTuples are ordered:\n\n(2, 3) == (3, 2)\n\nFalse\n\n\nThis means that they are indexable and sliceable:\n\n(2, 4, 6)[2]\n\n6\n\n\n\n(2, 4, 6)[::-1]\n\n(6, 4, 2)\n\n\nThey can be nested:\n\ntype((3, 1, (0, 2)))\n\ntuple\n\n\n\nlen((3, 1, (0, 2)))\n\n3\n\n\n\nmax((3, 1, 2))\n\n3\n\n\nThey can be heterogeneous:\n\ntype(('string', 2, True))\n\ntuple\n\n\nYou can create empty tuples:\n\ntype(())\n\ntuple\n\n\nYou can also create singleton tuples, but the syntax is a bit odd:\n\n# This is not a tuple...\ntype((1))\n\nint\n\n\n\n# This is the weird way to define a singleton tuple\ntype((1,))\n\ntuple\n\n\nHowever, the big difference with lists is that tuples are immutable:\n\nT = (2, 5)\nT[0] = 8\n\nTypeError: 'tuple' object does not support item assignment\n\n\nTuples are quite fascinating:\n\na, b = 1, 2\na, b\n\n(1, 2)\n\n\n\na, b = b, a\na, b\n\n(2, 1)\n\n\n\nTuples are declared in parentheses. They are immutable, ordered (thus indexable), and possibly heterogeneous collections of values."
  },
  {
    "objectID": "python/collections.html#sets",
    "href": "python/collections.html#sets",
    "title": "Collections",
    "section": "Sets",
    "text": "Sets\nSets are declared in curly braces:\n\ns = {3, 2, 5}\ns\n\n{2, 3, 5}\n\n\n\ntype(s)\n\nset\n\n\nSets are unordered:\n\n{2, 4, 1} == {4, 2, 1}\n\nTrue\n\n\nConsequently, it makes no sense to index a set.\nSets can be heterogeneous:\n\nS = {2, 'a', 'string'}\nisinstance(S, set)\n\nTrue\n\n\n\ntype(2) == type('a') == type('string')\n\nFalse\n\n\nThere are no duplicates in a set:\n\n{2, 2, 'a', 2, 'string', 'a'}\n\n{2, 'a', 'string'}\n\n\nYou can define an empty set, but only with the set function (because empty curly braces define a dictionary):\n\nt = set()\nt\n\nset()\n\n\n\nlen(t)\n\n0\n\n\n\ntype(t)\n\nset\n\n\nSince strings an iterables, you can use set to get a set of the unique characters:\n\nset('abba')\n\n{'a', 'b'}\n\n\n\n\nYour turn:\n\nHow could you create a set with the single element 'abba' in it?\n\n\nSets are declared in curly brackets. They are mutable, unordered (thus non indexable), possibly heterogeneous collections of unique values."
  },
  {
    "objectID": "python/collections.html#dictionaries",
    "href": "python/collections.html#dictionaries",
    "title": "Collections",
    "section": "Dictionaries",
    "text": "Dictionaries\nDictionaries are declared in curly braces. They associate values to keys:\n\nd = {'key1': 'value1', 'key2': 'value2'}\nd\n\n{'key1': 'value1', 'key2': 'value2'}\n\n\n\ntype(d)\n\ndict\n\n\nDictionaries are unordered:\n\n{'a': 1, 'b': 2} == {'b': 2, 'a': 1}\n\nTrue\n\n\nConsequently, the pairs themselves cannot be indexed. However, you can access values in a dictionary from their keys:\n\nD = {'c': 1, 'a': 3, 'b': 2}\nD['b']\n\n2\n\n\n\nD.get('b')\n\n2\n\n\n\nD.items()\n\ndict_items([('c', 1), ('a', 3), ('b', 2)])\n\n\n\nD.values()\n\ndict_values([1, 3, 2])\n\n\n\nD.keys()\n\ndict_keys(['c', 'a', 'b'])\n\n\nTo return a sorted list of keys:\n\nsorted(D)\n\n['a', 'b', 'c']\n\n\nYou can create empty dictionaries:\n\nE = {}\ntype(E)\n\ndict\n\n\nDictionaries are mutable, so you can add, remove, or replace items.\nLet’s add an item to our empty dictionary E:\n\nE['author'] = 'Proust'\nE\n\n{'author': 'Proust'}\n\n\nWe can add another one:\n\nE['title'] = 'In search of lost time'\nE\n\n{'author': 'Proust', 'title': 'In search of lost time'}\n\n\nWe can modify one:\n\nE['author'] = 'Marcel Proust'\nE\n\n{'author': 'Marcel Proust', 'title': 'In search of lost time'}\n\n\n\n\nYour turn:\n\nAdd a third item to E with the number of volumes.\n\nWe can also remove items:\n\nE.pop('author')\nE\n\n{'title': 'In search of lost time'}\n\n\nAnother method to remove items:\n\ndel E['title']\nE\n\n{}\n\n\n\nDictionaries are declared in curly braces. They are mutable and unordered collections of key/value pairs. They play the role of an associative array."
  },
  {
    "objectID": "python/collections.html#conversion-between-collections",
    "href": "python/collections.html#conversion-between-collections",
    "title": "Collections",
    "section": "Conversion between collections",
    "text": "Conversion between collections\nFrom tuple to list:\n\nlist((3, 8, 1))\n\n[3, 8, 1]\n\n\nFrom tuple to set:\n\nset((3, 2, 3, 3))\n\n{2, 3}\n\n\nFrom list to tuple:\n\ntuple([3, 1, 4])\n\n(3, 1, 4)\n\n\nFrom list to set:\n\nset(['a', 2, 4])\n\n{2, 4, 'a'}\n\n\nFrom set to tuple:\n\ntuple({2, 3})\n\n(2, 3)\n\n\nFrom set to list:\n\nlist({2, 3})\n\n[2, 3]"
  },
  {
    "objectID": "python/collections.html#collections-module",
    "href": "python/collections.html#collections-module",
    "title": "Collections",
    "section": "Collections module",
    "text": "Collections module\nPython has a built-in collections module providing the additional data structures: deque, defaultdict, namedtuple, OrderedDict, Counter, ChainMap, UserDict, UserList, and UserList."
  },
  {
    "objectID": "python/functions.html",
    "href": "python/functions.html",
    "title": "Writing functions",
    "section": "",
    "text": "Python comes with a number of built-in functions. Packages can provide additional ones. In many cases however, you will want to create your own functions to perform exactly the computations that you need.\nIn this section, we will see how to define new functions."
  },
  {
    "objectID": "python/functions.html#syntax",
    "href": "python/functions.html#syntax",
    "title": "Writing functions",
    "section": "Syntax",
    "text": "Syntax\nThe function definition syntax follows:\ndef &lt;name&gt;(&lt;arguments&gt;):\n    &lt;body&gt;\nOnce defined, new functions can be used as any other function.\nLet’s give this a try by creating some greeting functions."
  },
  {
    "objectID": "python/functions.html#function-without-argument",
    "href": "python/functions.html#function-without-argument",
    "title": "Writing functions",
    "section": "Function without argument",
    "text": "Function without argument\nLet’s start with the simple case in which our function does not accept any argument:\n\ndef hello():\n    print('Hello')\n\nThen we call it:\n\nhello()\n\nHello\n\n\nThis was great, but …\n\nhello('Marie')\n\nTypeError: hello() takes 0 positional arguments but 1 was given\n\n\n… it does not accept arguments."
  },
  {
    "objectID": "python/functions.html#function-with-one-argument",
    "href": "python/functions.html#function-with-one-argument",
    "title": "Writing functions",
    "section": "Function with one argument",
    "text": "Function with one argument\nLet’s step this up with a function which can accept an argument:\n\ndef greetings(name):\n    print('Hello ' + name)\n\nThis time, this works:\n\ngreetings('Marie')\n\nHello Marie\n\n\nHowever, this does not work anymore:\n\ngreetings()\n\nTypeError: greetings() missing 1 required positional argument: 'name'\n\n\n:("
  },
  {
    "objectID": "python/functions.html#function-with-a-facultative-argument",
    "href": "python/functions.html#function-with-a-facultative-argument",
    "title": "Writing functions",
    "section": "Function with a facultative argument",
    "text": "Function with a facultative argument\nLet’s make this even more fancy: a function with a facultative argument. That is, a function which accepts an argument, but also has a default value for when we do not provide any argument:\n\ndef howdy(name='you'):\n    print('Hello ' + name)\n\nWe can call it without argument (making use of the default value):\n\nhowdy()\n\nHello you\n\n\nAnd we can call it with an argument:\n\nhowdy('Marie')\n\nHello Marie\n\n\nThis was better, but …\n\nhowdy('Marie', 'Paul')\n\nTypeError: howdy() takes from 0 to 1 positional arguments but 2 were given\n\n\n… this does not work."
  },
  {
    "objectID": "python/functions.html#function-with-two-arguments",
    "href": "python/functions.html#function-with-two-arguments",
    "title": "Writing functions",
    "section": "Function with two arguments",
    "text": "Function with two arguments\nWe could create a function which takes two arguments:\n\ndef hey(name1, name2):\n    print('Hello ' + name1 + ', ' + name2)\n\nWhich solves our problem:\n\nhey('Marie', 'Paul')\n\nHello Marie, Paul\n\n\nBut it is terribly limiting:\n\n# This doesn't work\nhey()\n\nTypeError: hey() missing 2 required positional arguments: 'name1' and 'name2'\n\n\n\n# And neither does this\nhey('Marie')\n\nTypeError: hey() missing 1 required positional argument: 'name2'\n\n\n\n# Nor to mention this...\nhey('Marie', 'Paul', 'Alex')\n\nTypeError: hey() takes 2 positional arguments but 3 were given"
  },
  {
    "objectID": "python/functions.html#function-with-any-number-of-arguments",
    "href": "python/functions.html#function-with-any-number-of-arguments",
    "title": "Writing functions",
    "section": "Function with any number of arguments",
    "text": "Function with any number of arguments\nLet’s create a truly great function which handles all our cases:\n\ndef hi(name='you', *args):\n    result = ''\n    for i in args:\n        result += (', ' + i)\n    print('Hello ' + name + result)\n\nAnd let’s test it:\n\nhi()\nhi('Marie')\nhi('Marie', 'Paul')\nhi('Marie', 'Paul', 'Alex')\n\nHello you\nHello Marie\nHello Marie, Paul\nHello Marie, Paul, Alex\n\n\nEverything works!"
  },
  {
    "objectID": "python/functions.html#documenting-functions",
    "href": "python/functions.html#documenting-functions",
    "title": "Writing functions",
    "section": "Documenting functions",
    "text": "Documenting functions\nIt is a good habit to document what your functions do. As with comments, those “documentation strings” or “docstrings” will help future you or other users of your code.\nPEP 257—docstring conventions—suggests to use single-line docstrings surrounded by triple quotes.\nRemember the function definition syntax we saw at the start of this chapter? To be more exhaustive, we should have written it this way:\ndef &lt;name&gt;(&lt;arguments&gt;):\n    \"\"\"&lt;docstrings&gt;\"\"\"\n    &lt;body&gt;\n\nExample:\n\n\ndef hi(name='you', *args):\n    \"\"\"Print a greeting\"\"\"\n    result = ''\n    for i in args:\n        result += (', ' + i)\n    print('Hello ' + name + result)\n\nPEP 8—the style guide for Python code—suggests a maximum of 72 characters per line for docstrings.\nIf your docstring is longer, you should create a multi-line one. In that case, PEP 257 suggests to have a summary line at the top (right after the opening set of triple quotes), then leave a blank line, then have your long docstrings (which can occupy multiple lines), and finally have the closing set of triple quotes on a line of its own:\ndef &lt;name&gt;(&lt;arguments&gt;):\n    \"\"\"&lt;summary docstrings line&gt;\"\"\"\n\n    &lt;more detailed description&gt;\n    \"\"\"\n    &lt;body&gt;\n\nExample:\n\n\ndef hi(name='you', *args):\n    \"\"\"Print a greeting\n\n    Accepts any number of arguments\n    \"\"\"\n    result = ''\n    for i in args:\n        result += (', ' + i)\n    print('Hello ' + name + result)\n\n\nYou can (and should) document modules, classes, and methods in the same way.\n\nYou can now access the documentation of your function as you would any Python function:\n\nhelp(hi)\n\nHelp on function hi in module __main__:\n\nhi(name='you', *args)\n    Print a greeting\n    \n    Accepts any number of arguments\n\n\n\nOr:\n\nprint(hi.__doc__)\n\nPrint a greeting\n\n    Accepts any number of arguments"
  },
  {
    "objectID": "python/intro_hpc.html",
    "href": "python/intro_hpc.html",
    "title": "Introduction to high performance research computing in Python",
    "section": "",
    "text": "When you launch a Jupyter session from a JupyterHub, you are running a Slurm job on a compute node. If you want to play for 8 hours in Jupyter, you are requesting an 8 hour job. Now, most of the time you spend on Jupyter is spent typing, running bits and pieces of code, or doing nothing at all. If you ask for GPUs, many CPUs, and lots of RAM, all of it will remain idle most of the time. This is a suboptimal use of resources.\nIn addition, if you ask for lots of resources for a long time, you will have to wait for a while before they get allocated to you.\nLastly, you will go through your allocations quickly.\nAll of this applies equally for interactive sessions launched from an SSH session with salloc."
  },
  {
    "objectID": "python/intro_hpc.html#interactive-sessions-for-high-performance-computing",
    "href": "python/intro_hpc.html#interactive-sessions-for-high-performance-computing",
    "title": "Introduction to high performance research computing in Python",
    "section": "",
    "text": "When you launch a Jupyter session from a JupyterHub, you are running a Slurm job on a compute node. If you want to play for 8 hours in Jupyter, you are requesting an 8 hour job. Now, most of the time you spend on Jupyter is spent typing, running bits and pieces of code, or doing nothing at all. If you ask for GPUs, many CPUs, and lots of RAM, all of it will remain idle most of the time. This is a suboptimal use of resources.\nIn addition, if you ask for lots of resources for a long time, you will have to wait for a while before they get allocated to you.\nLastly, you will go through your allocations quickly.\nAll of this applies equally for interactive sessions launched from an SSH session with salloc."
  },
  {
    "objectID": "python/intro_hpc.html#a-better-approach",
    "href": "python/intro_hpc.html#a-better-approach",
    "title": "Introduction to high performance research computing in Python",
    "section": "A better approach",
    "text": "A better approach\nA more efficient strategy is to develop and test your code with small samples, few iterations, etc. in an interactive job (from an SSH session in the cluster with salloc), on your own computer, or in Jupyter. Once you are confident that your code works, launch an sbatch job from an SSH session in the cluster to run the code as a script on all your data. This ensures that heavy duty resources that you requested are actually put to use to run your heavy calculations and not seating idle while you are thinking, typing, etc."
  },
  {
    "objectID": "python/intro_hpc.html#logging-on-to-the-cluster",
    "href": "python/intro_hpc.html#logging-on-to-the-cluster",
    "title": "Introduction to high performance research computing in Python",
    "section": "Logging on to the cluster",
    "text": "Logging on to the cluster\nOpen a terminal emulator:\nWindows users:  launch MobaXTerm.\nMacOS users:   launch Terminal.\nLinux users:     launch xterm or the terminal emulator of your choice.\nThen access the cluster through secure shell:\n$ ssh &lt;username&gt;@&lt;hostname&gt;    # enter password"
  },
  {
    "objectID": "python/intro_hpc.html#accessing-python",
    "href": "python/intro_hpc.html#accessing-python",
    "title": "Introduction to high performance research computing in Python",
    "section": "Accessing Python",
    "text": "Accessing Python\nThis is done with the Lmod tool through the module command. You can find the full documentation here and below are the subcommands you will need:\n# get help on the module command\n$ module help\n$ module --help\n$ module -h\n\n# list modules that are already loaded\n$ module list\n\n# see which modules are available for Python\n$ module spider python\n\n# see how to load Python 3.10.2\n$ module spider python/3.10.2\n\n# load Python 3.10.2 with the required gcc module first\n# (the order is important)\n$ module load gcc/7.3.0 python/3.10.2\n\n# you can see that we now have Python 3.10.2 loaded\n$ module list"
  },
  {
    "objectID": "python/intro_hpc.html#copying-files-to-the-cluster",
    "href": "python/intro_hpc.html#copying-files-to-the-cluster",
    "title": "Introduction to high performance research computing in Python",
    "section": "Copying files to the cluster",
    "text": "Copying files to the cluster\nWe will create a python_workshop directory in ~/scratch, then copy our Python script in it.\n$ mkdir ~/scratch/python_job\nOpen a new terminal window and from your local terminal (make sure that you are not on the remote terminal by looking at the bash prompt) run:\n$ scp /local/path/to/sort.jl &lt;username&gt;@&lt;hostname&gt;:scratch/python_job\n$ scp /local/path/to/psort.jl &lt;username&gt;@&lt;hostname&gt;:scratch/python_job\n\n# enter password"
  },
  {
    "objectID": "python/intro_hpc.html#job-scripts",
    "href": "python/intro_hpc.html#job-scripts",
    "title": "Introduction to high performance research computing in Python",
    "section": "Job scripts",
    "text": "Job scripts\nWe will not run an interactive session with Python on the cluster: we already have Python scripts ready to run. All we need to do is to write job scripts to submit to Slurm, the job scheduler used by the Alliance clusters.\nWe will create 2 scripts: one to run Python on one core and one on as many cores as are available.\n\n\nYour turn:\n\nHow many processors are there on our training cluster?\n\nSave your job scripts in the files ~/scratch/python_job/job_python1c.sh and job_python2c.sh for one and two cores respectively.\nHere is what our single core Slurm script looks like:\n#!/bin/bash\n#SBATCH --job-name=python1c         # job name\n#SBATCH --time=00:01:00             # max walltime 1 min\n#SBATCH --cpus-per-task=1           # number of cores\n#SBATCH --mem=1000                  # max memory (default unit is megabytes)\n#SBATCH --output=python1c%j.out     # file name for the output\n#SBATCH --error=python1c%j.err      # file name for errors\n# %j gets replaced with the job number\n\npython sort.py\n\n\nYour turn:\n\nWrite the script for 2 cores.\n\nNow, we can submit our jobs to the cluster:\n$ cd ~/scratch/python_job\n$ sbatch job_python1c.sh\n$ sbatch job_python2c.sh\nAnd we can check their status with:\n$ sq      # This is an Alliance alias for `squeue -u $USER $@`\n\nPD stands for pending\nR stands for running\n\nmodule avail python # several versions available module load python/3.8.10 virtualenv –no-download astro # install Python tools in your $HOME/astro source astro/bin/activate pip install –no-index –upgrade pip pip install –no-index numpy jupyter pandas # all these will go into your $HOME/astro avail_wheels –name “tensorflow_gpu” –all_versions # check out the available packages pip install –no-index tensorflow_gpu==2.2.0 # if needed, install a specific version … deactivate Once created, you would use it with:\nsource ~/astro/bin/activate python … deactivate"
  },
  {
    "objectID": "python/intro_hpc.html#run-python-on-our-training-cluster",
    "href": "python/intro_hpc.html#run-python-on-our-training-cluster",
    "title": "Introduction to high performance research computing in Python",
    "section": "Run Python on our training cluster",
    "text": "Run Python on our training cluster\nThis is not the method I recommend for this workshop, but I am adding it as this is something you might want to use if you need to run heavy computations.\nFirst, you need to load the Python module.\nSee which Python modules are available:\nmodule spider python\nSee how to install one module:\n\nExample:\n\nmodule spider python/3.10.2\nLoad the required dependencies (first) and the module:\nmodule load StdEnv/2020 python/3.10.2\nYou can check that the modules were loaded with:\nmodule list\nAnd verify the Python version with:\npython --version"
  },
  {
    "objectID": "python/pandas.html",
    "href": "python/pandas.html",
    "title": "DataFrames with Pandas",
    "section": "",
    "text": "pandas is a Python library built to manipulate data frames and time series.\nFor this section, we will use the Covid-19 data from the Johns Hopkins University CSSE repository.\nYou can visualize this data in a dashboard created by the Johns Hopkins University Center for Systems Science and Engineering."
  },
  {
    "objectID": "python/pandas.html#setup",
    "href": "python/pandas.html#setup",
    "title": "DataFrames with Pandas",
    "section": "Setup",
    "text": "Setup\nFirst, we need to load the pandas library and read in the data from the web:\n\n# Load the pandas library and create a shorter name for it\nimport pandas as pd\n\n# The global confirmed cases are available in CSV format at the url:\nurl = \"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv\"\n\n# pandas allows to read in data from the web directly\ncases = pd.read_csv(url)"
  },
  {
    "objectID": "python/pandas.html#first-look-at-the-data",
    "href": "python/pandas.html#first-look-at-the-data",
    "title": "DataFrames with Pandas",
    "section": "First look at the data",
    "text": "First look at the data\nWhat does our data look like?\n\ncases\n\n\n\n\n\n\n\n\nProvince/State\nCountry/Region\nLat\nLong\n1/22/20\n1/23/20\n1/24/20\n1/25/20\n1/26/20\n1/27/20\n...\n2/28/23\n3/1/23\n3/2/23\n3/3/23\n3/4/23\n3/5/23\n3/6/23\n3/7/23\n3/8/23\n3/9/23\n\n\n\n\n0\nNaN\nAfghanistan\n33.939110\n67.709953\n0\n0\n0\n0\n0\n0\n...\n209322\n209340\n209358\n209362\n209369\n209390\n209406\n209436\n209451\n209451\n\n\n1\nNaN\nAlbania\n41.153300\n20.168300\n0\n0\n0\n0\n0\n0\n...\n334391\n334408\n334408\n334427\n334427\n334427\n334427\n334427\n334443\n334457\n\n\n2\nNaN\nAlgeria\n28.033900\n1.659600\n0\n0\n0\n0\n0\n0\n...\n271441\n271448\n271463\n271469\n271469\n271477\n271477\n271490\n271494\n271496\n\n\n3\nNaN\nAndorra\n42.506300\n1.521800\n0\n0\n0\n0\n0\n0\n...\n47866\n47875\n47875\n47875\n47875\n47875\n47875\n47875\n47890\n47890\n\n\n4\nNaN\nAngola\n-11.202700\n17.873900\n0\n0\n0\n0\n0\n0\n...\n105255\n105277\n105277\n105277\n105277\n105277\n105277\n105277\n105288\n105288\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n284\nNaN\nWest Bank and Gaza\n31.952200\n35.233200\n0\n0\n0\n0\n0\n0\n...\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n\n\n285\nNaN\nWinter Olympics 2022\n39.904200\n116.407400\n0\n0\n0\n0\n0\n0\n...\n535\n535\n535\n535\n535\n535\n535\n535\n535\n535\n\n\n286\nNaN\nYemen\n15.552727\n48.516388\n0\n0\n0\n0\n0\n0\n...\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n\n\n287\nNaN\nZambia\n-13.133897\n27.849332\n0\n0\n0\n0\n0\n0\n...\n343012\n343012\n343079\n343079\n343079\n343135\n343135\n343135\n343135\n343135\n\n\n288\nNaN\nZimbabwe\n-19.015438\n29.154857\n0\n0\n0\n0\n0\n0\n...\n263921\n264127\n264127\n264127\n264127\n264127\n264127\n264127\n264276\n264276\n\n\n\n\n289 rows × 1147 columns\n\n\n\n\n# Quick summary of the data\ncases.describe()\n\n\n\n\n\n\n\n\nLat\nLong\n1/22/20\n1/23/20\n1/24/20\n1/25/20\n1/26/20\n1/27/20\n1/28/20\n1/29/20\n...\n2/28/23\n3/1/23\n3/2/23\n3/3/23\n3/4/23\n3/5/23\n3/6/23\n3/7/23\n3/8/23\n3/9/23\n\n\n\n\ncount\n287.000000\n287.000000\n289.000000\n289.000000\n289.000000\n289.000000\n289.000000\n289.000000\n289.000000\n289.000000\n...\n2.890000e+02\n2.890000e+02\n2.890000e+02\n2.890000e+02\n2.890000e+02\n2.890000e+02\n2.890000e+02\n2.890000e+02\n2.890000e+02\n2.890000e+02\n\n\nmean\n19.718719\n22.182084\n1.927336\n2.273356\n3.266436\n4.972318\n7.335640\n10.134948\n19.307958\n21.346021\n...\n2.336755e+06\n2.337519e+06\n2.338173e+06\n2.338805e+06\n2.338992e+06\n2.339187e+06\n2.339387e+06\n2.339839e+06\n2.340460e+06\n2.341073e+06\n\n\nstd\n25.956609\n77.870931\n26.173664\n26.270191\n32.707271\n45.523871\n63.623197\n85.724481\n210.329649\n211.628535\n...\n8.506608e+06\n8.511285e+06\n8.514488e+06\n8.518031e+06\n8.518408e+06\n8.518645e+06\n8.519346e+06\n8.521641e+06\n8.524968e+06\n8.527765e+06\n\n\nmin\n-71.949900\n-178.116500\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n\n\n25%\n4.072192\n-32.823050\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n1.456700e+04\n1.456700e+04\n1.456700e+04\n1.456700e+04\n1.456700e+04\n1.456700e+04\n1.456700e+04\n1.456700e+04\n1.456700e+04\n1.456700e+04\n\n\n50%\n21.512583\n20.939400\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n1.032480e+05\n1.032480e+05\n1.032480e+05\n1.032480e+05\n1.032480e+05\n1.032480e+05\n1.032480e+05\n1.032480e+05\n1.032480e+05\n1.032480e+05\n\n\n75%\n40.401784\n89.224350\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n1.051998e+06\n1.052122e+06\n1.052247e+06\n1.052382e+06\n1.052519e+06\n1.052664e+06\n1.052664e+06\n1.052926e+06\n1.053068e+06\n1.053213e+06\n\n\nmax\n71.706900\n178.065000\n444.000000\n444.000000\n549.000000\n761.000000\n1058.000000\n1423.000000\n3554.000000\n3554.000000\n...\n1.034435e+08\n1.035339e+08\n1.035898e+08\n1.036487e+08\n1.036508e+08\n1.036470e+08\n1.036555e+08\n1.036909e+08\n1.037558e+08\n1.038027e+08\n\n\n\n\n8 rows × 1145 columns\n\n\n\n\nOf course, this value is meaningless for Lat and Long!\n\n\n# Data types of the various columns\ncases.dtypes\n\nProvince/State     object\nCountry/Region     object\nLat               float64\nLong              float64\n1/22/20             int64\n                   ...   \n3/5/23              int64\n3/6/23              int64\n3/7/23              int64\n3/8/23              int64\n3/9/23              int64\nLength: 1147, dtype: object\n\n\n\ncases.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 289 entries, 0 to 288\nColumns: 1147 entries, Province/State to 3/9/23\ndtypes: float64(2), int64(1143), object(2)\nmemory usage: 2.5+ MB\n\n\n\ncases.shape\n\n(289, 1147)"
  },
  {
    "objectID": "python/pandas.html#cases-per-country-by-date",
    "href": "python/pandas.html#cases-per-country-by-date",
    "title": "DataFrames with Pandas",
    "section": "Cases per country by date",
    "text": "Cases per country by date\nThe dataset is a time series: this means that we have the cumulative numbers up to each date.\n\n# Let's get rid of the latitude and longitude to simplify our data\nsimple = cases.drop(columns=['Lat', 'Long'])\nsimple\n\n\n\n\n\n\n\n\nProvince/State\nCountry/Region\n1/22/20\n1/23/20\n1/24/20\n1/25/20\n1/26/20\n1/27/20\n1/28/20\n1/29/20\n...\n2/28/23\n3/1/23\n3/2/23\n3/3/23\n3/4/23\n3/5/23\n3/6/23\n3/7/23\n3/8/23\n3/9/23\n\n\n\n\n0\nNaN\nAfghanistan\n0\n0\n0\n0\n0\n0\n0\n0\n...\n209322\n209340\n209358\n209362\n209369\n209390\n209406\n209436\n209451\n209451\n\n\n1\nNaN\nAlbania\n0\n0\n0\n0\n0\n0\n0\n0\n...\n334391\n334408\n334408\n334427\n334427\n334427\n334427\n334427\n334443\n334457\n\n\n2\nNaN\nAlgeria\n0\n0\n0\n0\n0\n0\n0\n0\n...\n271441\n271448\n271463\n271469\n271469\n271477\n271477\n271490\n271494\n271496\n\n\n3\nNaN\nAndorra\n0\n0\n0\n0\n0\n0\n0\n0\n...\n47866\n47875\n47875\n47875\n47875\n47875\n47875\n47875\n47890\n47890\n\n\n4\nNaN\nAngola\n0\n0\n0\n0\n0\n0\n0\n0\n...\n105255\n105277\n105277\n105277\n105277\n105277\n105277\n105277\n105288\n105288\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n284\nNaN\nWest Bank and Gaza\n0\n0\n0\n0\n0\n0\n0\n0\n...\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n\n\n285\nNaN\nWinter Olympics 2022\n0\n0\n0\n0\n0\n0\n0\n0\n...\n535\n535\n535\n535\n535\n535\n535\n535\n535\n535\n\n\n286\nNaN\nYemen\n0\n0\n0\n0\n0\n0\n0\n0\n...\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n\n\n287\nNaN\nZambia\n0\n0\n0\n0\n0\n0\n0\n0\n...\n343012\n343012\n343079\n343079\n343079\n343135\n343135\n343135\n343135\n343135\n\n\n288\nNaN\nZimbabwe\n0\n0\n0\n0\n0\n0\n0\n0\n...\n263921\n264127\n264127\n264127\n264127\n264127\n264127\n264127\n264276\n264276\n\n\n\n\n289 rows × 1145 columns\n\n\n\n Some countries (e.g. Australia) are split between several provinces or states so we will have to add the values of all their provinces/states to get their totals.\nHere is how to make the sum for all Australian states:\nLet’s first select all the data for Australia: we want all the rows for which the Country/Region column is equal to Australia.\nFirst, we want to select the Country/Region column. There are several ways to index in pandas.\nWhen indexing columns, one can use square brackets directly after the DataFrame to index:\n\nsimple['Country/Region']\n\n0               Afghanistan\n1                   Albania\n2                   Algeria\n3                   Andorra\n4                    Angola\n               ...         \n284      West Bank and Gaza\n285    Winter Olympics 2022\n286                   Yemen\n287                  Zambia\n288                Zimbabwe\nName: Country/Region, Length: 289, dtype: object\n\n\nHowever, it is more efficient to use the .loc or .iloc methods.\n\nUse .loc when using labels or booleans:\n\n\nsimple.loc[:, 'Country/Region']\n\n0               Afghanistan\n1                   Albania\n2                   Algeria\n3                   Andorra\n4                    Angola\n               ...         \n284      West Bank and Gaza\n285    Winter Olympics 2022\n286                   Yemen\n287                  Zambia\n288                Zimbabwe\nName: Country/Region, Length: 289, dtype: object\n\n\n\nUse .iloc when using indices:\n\n\nsimple.iloc[:, 1]\n\n0               Afghanistan\n1                   Albania\n2                   Algeria\n3                   Andorra\n4                    Angola\n               ...         \n284      West Bank and Gaza\n285    Winter Olympics 2022\n286                   Yemen\n287                  Zambia\n288                Zimbabwe\nName: Country/Region, Length: 289, dtype: object\n\n\n\nCountry/Region is the 2nd column, but indexing starts at 0 in Python.\n\nThen we need a conditional to filter the rows for which the value is equal to Australia:\n\nsimple.loc[:, 'Country/Region'] == 'Australia'\n\n0      False\n1      False\n2      False\n3      False\n4      False\n       ...  \n284    False\n285    False\n286    False\n287    False\n288    False\nName: Country/Region, Length: 289, dtype: bool\n\n\nFinally, we index, out of our entire data frame, the rows for which that condition returns True:\n\nsimple.loc[simple.loc[:, 'Country/Region'] == 'Australia']\n\n\n\n\n\n\n\n\nProvince/State\nCountry/Region\n1/22/20\n1/23/20\n1/24/20\n1/25/20\n1/26/20\n1/27/20\n1/28/20\n1/29/20\n...\n2/28/23\n3/1/23\n3/2/23\n3/3/23\n3/4/23\n3/5/23\n3/6/23\n3/7/23\n3/8/23\n3/9/23\n\n\n\n\n9\nAustralian Capital Territory\nAustralia\n0\n0\n0\n0\n0\n0\n0\n0\n...\n232018\n232018\n232619\n232619\n232619\n232619\n232619\n232619\n232619\n232974\n\n\n10\nNew South Wales\nAustralia\n0\n0\n0\n0\n3\n4\n4\n4\n...\n3900969\n3900969\n3908129\n3908129\n3908129\n3908129\n3908129\n3908129\n3908129\n3915992\n\n\n11\nNorthern Territory\nAustralia\n0\n0\n0\n0\n0\n0\n0\n0\n...\n104931\n104931\n105021\n105021\n105021\n105021\n105021\n105021\n105021\n105111\n\n\n12\nQueensland\nAustralia\n0\n0\n0\n0\n0\n0\n0\n1\n...\n1796633\n1796633\n1800236\n1800236\n1800236\n1800236\n1800236\n1800236\n1800236\n1800236\n\n\n13\nSouth Australia\nAustralia\n0\n0\n0\n0\n0\n0\n0\n0\n...\n880207\n880207\n881911\n881911\n881911\n881911\n881911\n881911\n881911\n883620\n\n\n14\nTasmania\nAustralia\n0\n0\n0\n0\n0\n0\n0\n0\n...\n286264\n286264\n286264\n286897\n286897\n286897\n286897\n286897\n286897\n287507\n\n\n15\nVictoria\nAustralia\n0\n0\n0\n0\n1\n1\n1\n1\n...\n2874262\n2874262\n2877260\n2877260\n2877260\n2877260\n2877260\n2877260\n2877260\n2880559\n\n\n16\nWestern Australia\nAustralia\n0\n0\n0\n0\n0\n0\n0\n0\n...\n1291077\n1291077\n1293461\n1293461\n1293461\n1293461\n1293461\n1293461\n1293461\n1293461\n\n\n\n\n8 rows × 1145 columns\n\n\n\n\nHere we use .loc to index based on a boolean array.\n\nWe can now make the sum for all of Australia for each day:\n\ntotal_australia = simple.loc[simple.loc[:, 'Country/Region'] == 'Australia'].sum(numeric_only=True)\ntotal_australia\n\n1/22/20           0\n1/23/20           0\n1/24/20           0\n1/25/20           0\n1/26/20           4\n             ...   \n3/5/23     11385534\n3/6/23     11385534\n3/7/23     11385534\n3/8/23     11385534\n3/9/23     11399460\nLength: 1143, dtype: int64\n\n\nWe can do this for all countries by grouping them:\n\ntotals = simple.groupby('Country/Region').sum(numeric_only=True)\ntotals\n\n\n\n\n\n\n\n\n1/22/20\n1/23/20\n1/24/20\n1/25/20\n1/26/20\n1/27/20\n1/28/20\n1/29/20\n1/30/20\n1/31/20\n...\n2/28/23\n3/1/23\n3/2/23\n3/3/23\n3/4/23\n3/5/23\n3/6/23\n3/7/23\n3/8/23\n3/9/23\n\n\nCountry/Region\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAfghanistan\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n209322\n209340\n209358\n209362\n209369\n209390\n209406\n209436\n209451\n209451\n\n\nAlbania\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n334391\n334408\n334408\n334427\n334427\n334427\n334427\n334427\n334443\n334457\n\n\nAlgeria\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n271441\n271448\n271463\n271469\n271469\n271477\n271477\n271490\n271494\n271496\n\n\nAndorra\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n47866\n47875\n47875\n47875\n47875\n47875\n47875\n47875\n47890\n47890\n\n\nAngola\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n105255\n105277\n105277\n105277\n105277\n105277\n105277\n105277\n105288\n105288\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\nWest Bank and Gaza\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n\n\nWinter Olympics 2022\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n535\n535\n535\n535\n535\n535\n535\n535\n535\n535\n\n\nYemen\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n\n\nZambia\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n343012\n343012\n343079\n343079\n343079\n343135\n343135\n343135\n343135\n343135\n\n\nZimbabwe\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n263921\n264127\n264127\n264127\n264127\n264127\n264127\n264127\n264276\n264276\n\n\n\n\n201 rows × 1143 columns\n\n\n\n Now, we can look at the totals for any date:\n\ntotals.loc[:, '6/12/21']\n\nCountry/Region\nAfghanistan              88740\nAlbania                 132449\nAlgeria                 133070\nAndorra                  13813\nAngola                   36600\n                         ...  \nWest Bank and Gaza      311018\nWinter Olympics 2022         0\nYemen                     6857\nZambia                  110332\nZimbabwe                 39852\nName: 6/12/21, Length: 201, dtype: int64\n\n\nTo make it easier to read, let’s order those numbers by decreasing order:\n\ntotals.loc[:, '6/12/21'].sort_values(ascending=False)\n\nCountry/Region\nUS                      33573694\nIndia                   29439989\nBrazil                  17385952\nFrance                   5799565\nTurkey                   5325435\n                          ...   \nPalau                          0\nWinter Olympics 2022           0\nKorea, North                   0\nSummer Olympics 2020           0\nTonga                          0\nName: 6/12/21, Length: 201, dtype: int64\n\n\nWe can also index the data for a particular country by indexing a row instead of a column:\n\ntotals.loc['Albania', :]\n\n1/22/20         0\n1/23/20         0\n1/24/20         0\n1/25/20         0\n1/26/20         0\n            ...  \n3/5/23     334427\n3/6/23     334427\n3/7/23     334427\n3/8/23     334443\n3/9/23     334457\nName: Albania, Length: 1143, dtype: int64\n\n\nWhen indexing rows, this syntax can be simplified to:\n\ntotals.loc['Albania']\n\n1/22/20         0\n1/23/20         0\n1/24/20         0\n1/25/20         0\n1/26/20         0\n            ...  \n3/5/23     334427\n3/6/23     334427\n3/7/23     334427\n3/8/23     334443\n3/9/23     334457\nName: Albania, Length: 1143, dtype: int64"
  },
  {
    "objectID": "python/pandas.html#global-totals",
    "href": "python/pandas.html#global-totals",
    "title": "DataFrames with Pandas",
    "section": "Global totals",
    "text": "Global totals\nNow, what if we want to have the world totals for each day? We calculate the columns totals (i.e. the sum across countries):\n\ntotals.sum()\n\n1/22/20          557\n1/23/20          657\n1/24/20          944\n1/25/20         1437\n1/26/20         2120\n             ...    \n3/5/23     676024901\n3/6/23     676082941\n3/7/23     676213378\n3/8/23     676392824\n3/9/23     676570149\nLength: 1143, dtype: int64\n\n\n\n\nYour turn:\n\nHow many confirmed cases were there in Venezuela by March 10, 2021?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFirst, we need to select the data for Venezuela:\n\nvenez = totals.loc['Venezuela']\nvenez\n\n1/22/20         0\n1/23/20         0\n1/24/20         0\n1/25/20         0\n1/26/20         0\n            ...  \n3/5/23     552051\n3/6/23     552125\n3/7/23     552157\n3/8/23     552157\n3/9/23     552162\nName: Venezuela, Length: 1143, dtype: int64\n\n\nThen, we need to select for the proper date:\n\nanswer = venez.loc['3/10/21']\nanswer\n\n143321\n\n\nWe could have done it at once by indexing the row and column:\n\ntotals.loc['Venezuela', '3/10/21']\n\n143321"
  },
  {
    "objectID": "python/pandas.html#pandas-documentation",
    "href": "python/pandas.html#pandas-documentation",
    "title": "DataFrames with Pandas",
    "section": "Pandas documentation",
    "text": "Pandas documentation\n\nA user Guide to pandas\nFull documentation"
  },
  {
    "objectID": "python/run.html",
    "href": "python/run.html",
    "title": "Running Python",
    "section": "",
    "text": "There are many ways to run Python interactively. For this workshop, you have two options:"
  },
  {
    "objectID": "python/run.html#use-your-own-machine",
    "href": "python/run.html#use-your-own-machine",
    "title": "Running Python",
    "section": "Use your own machine",
    "text": "Use your own machine\nIf you have installed Python 3.7 or greater and the packages NumPy, Matplotlib, scikit-image, pandas, Xarray, you can simply run this workshop on your machine, either directly in the Python shell or, if you also have Jupyter, in JupyterLab."
  },
  {
    "objectID": "python/run.html#use-our-temporary-jupyterhub",
    "href": "python/run.html#use-our-temporary-jupyterhub",
    "title": "Running Python",
    "section": "Use our temporary JupyterHub",
    "text": "Use our temporary JupyterHub\nIf you don’t have the required software on your machine, this is the easiest solution.\n\nLog in to the JupyterHub on our training cluster\n\nGo to https://jupyter.bobthewren.c3.ca,\nSign in with the username and password you just got,\nSet the server options according to the image below:\n\n\n\nThese are the only values that you should edit:\nChange the time to 7.0\n\n\nPress start.\n\n\nNote that, unlike other JupyterHubs you might have used (e.g. Syzygy), this JupyterHub is not permanent and can only be used for this course.\n\nIf you don’t need all the time you asked for after all, it is a great thing to log out (the resources you are using on this cluster are shared amongst many people and when resources are allocated to you, they aren’t available to other people. So it is a good thing not to ask for unnecessary resources and have them sit idle when others could be using them).\nTo log out, click on “File” in the top menu and select “Log out” at the very bottom.\nIf you would like to make a change to the information you entered on the server option page after you have pressed “start”, log out in the same way, log back in, edit the server options, and press start again.\n\n\nStart a Python notebook\nTo start a Jupyter notebook with the Python kernel, click on the button “Python 3” in the “Notebook” section (top row of buttons)."
  },
  {
    "objectID": "python/webscraping.html",
    "href": "python/webscraping.html",
    "title": "Web scraping with Python",
    "section": "",
    "text": "The internet is a trove of information. A lot of it is publicly available and thus suitable for use in research. Extracting that information and putting it in an organized format for analysis can however be extremely tedious.\nWeb scraping tools allow to automate parts of that process and Python is a popular language for the task.\nIn this workshop, I will guide you through a simple example using the package Beautiful Soup."
  },
  {
    "objectID": "python/webscraping.html#html-and-css",
    "href": "python/webscraping.html#html-and-css",
    "title": "Web scraping with Python",
    "section": "HTML and CSS",
    "text": "HTML and CSS\nHyperText Markup Language (HTML) is the standard markup language for websites: it encodes the information related to the formatting and structure of webpages. Additionally, some of the customization can be stored in Cascading Style Sheets (CSS) files.\nHTML uses tags of the form:\n&lt;some_tag&gt;Your content&lt;/some_tag&gt;\nSome tags have attributes:\n&lt;some_tag attribute_name=\"attribute value\"&gt;Your content&lt;/some_tag&gt;\n\nExamples:\n\nSite structure:\n\n&lt;h2&gt;This is a heading of level 2&lt;/h2&gt;\n&lt;p&gt;This is a paragraph&lt;/p&gt;\n\nFormatting:\n\n&lt;b&gt;This is bold&lt;/b&gt;\n&lt;a href=\"https://some.url\"&gt;This is the text for a link&lt;/a&gt;"
  },
  {
    "objectID": "python/webscraping.html#web-scrapping",
    "href": "python/webscraping.html#web-scrapping",
    "title": "Web scraping with Python",
    "section": "Web scrapping",
    "text": "Web scrapping\nWeb scraping is a general term for a set of tools which allow for the extraction of data from the web automatically.\nWhile most of the data on the internet is publicly available, it is illegal to scrape some sites and you should always look into the policy of a site before attempting to scrape it. Some sites will also block you if you submit too many requests in a short amount of time, so remember to scrape responsibly."
  },
  {
    "objectID": "python/webscraping.html#example-for-this-workshop",
    "href": "python/webscraping.html#example-for-this-workshop",
    "title": "Web scraping with Python",
    "section": "Example for this workshop",
    "text": "Example for this workshop\nWe will use a website from the University of Tennessee containing a database of PhD theses from that university.\nOur goal is to scrape data from this site to produce a dataframe with the date, major, and advisor for each dissertation.\n\nWe will only do this for the first page which contains the links to the 100 most recent theses. If you really wanted to gather all the data, you would have to do this for all pages."
  },
  {
    "objectID": "python/webscraping.html#lets-look-at-the-sites",
    "href": "python/webscraping.html#lets-look-at-the-sites",
    "title": "Web scraping with Python",
    "section": "Let’s look at the sites",
    "text": "Let’s look at the sites\nFirst of all, let’s have a close look at the websites we want to scrape to think carefully about what we want to do. Before starting to write code, it is always a good idea to think about what you are trying to achieve with your code.\nTo create a dataframe with the data for all the dissertations on that first page, we need to do two things:\n\nStep 1: from the dissertations database first page, we want to scrape the list of URLs for the dissertation pages.\nStep 2: once we have the URLs, we want to scrape those pages too to get the date, major, and advisor for each dissertation."
  },
  {
    "objectID": "python/webscraping.html#load-packages",
    "href": "python/webscraping.html#load-packages",
    "title": "Web scraping with Python",
    "section": "Load packages",
    "text": "Load packages\nLet’s load the packages that will make scraping websites with Python easier:\n\nimport requests                 # To download the html data from a site\nfrom bs4 import BeautifulSoup   # To parse the html data\nimport time                     # To add a delay between each requests\nimport pandas as pd             # To store our data in a DataFrame"
  },
  {
    "objectID": "python/webscraping.html#send-request-to-the-main-site",
    "href": "python/webscraping.html#send-request-to-the-main-site",
    "title": "Web scraping with Python",
    "section": "Send request to the main site",
    "text": "Send request to the main site\nAs mentioned above, our site is the database of PhD dissertations from the University of Tennessee.\nLet’s create a string with the URL:\n\nurl = \"https://trace.tennessee.edu/utk_graddiss/index.html\"\n\nFirst, we send a request to that URL and save the response in a variable called r:\n\nr = requests.get(url)\n\nLet’s see what our response looks like:\n\nr\n\n&lt;Response [200]&gt;\n\n\nIf you look in the list of HTTP status codes, you can see that a response with a code of 200 means that the request was successful."
  },
  {
    "objectID": "python/webscraping.html#explore-the-raw-data",
    "href": "python/webscraping.html#explore-the-raw-data",
    "title": "Web scraping with Python",
    "section": "Explore the raw data",
    "text": "Explore the raw data\nTo get the actual content of the response as unicode (text), we can use the text property of the response. This will give us the raw HTML markup from the webpage.\nLet’s print the first 200 characters:\n\nprint(r.text[:200])\n\n\n&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\"&gt;\n&lt;head&gt;&lt;!-- inj yui3-seed: --&gt;&lt;script type='text/javascript' src='//cdnjs.cloudflare.com/ajax/libs/yui/3.6.0/yui/yui-min.js'&gt;&lt;/script&gt;&lt;script type='text/javascript' sr"
  },
  {
    "objectID": "python/webscraping.html#parse-the-data",
    "href": "python/webscraping.html#parse-the-data",
    "title": "Web scraping with Python",
    "section": "Parse the data",
    "text": "Parse the data\nThe package Beautiful Soup transforms (parses) such HTML data into a parse tree, which will make extracting information easier.\nLet’s create an object called mainpage with the parse tree:\n\nmainpage = BeautifulSoup(r.text, \"html.parser\")\n\n\nhtml.parser is the name of the parser that we are using here. It is better to use a specific parser to get consistent results across environments.\n\nWe can print the beginning of the parsed result:\n\nprint(mainpage.prettify()[:200])\n\n&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\"&gt;\n &lt;head&gt;\n  &lt;!-- inj yui3-seed: --&gt;\n  &lt;script src=\"//cdnjs.cloudflare.com/ajax/libs/yui/3.6.0/yui/yui-min.js\" type=\"text/javascript\"&gt;\n  &lt;/script&gt;\n  &lt;script src=\"//ajax.g\n\n\n\nThe prettify method turns the BeautifulSoup object we created into a string (which is needed for slicing).\n\nIt doesn’t look any more clear to us, but it is now in a format the Beautiful Soup package can work with.\nFor instance, we can get the HTML segment containing the title with three methods:\n\nusing the title tag name:\n\n\nmainpage.title\n\n&lt;title&gt;\nDoctoral Dissertations | Graduate School | University of Tennessee, Knoxville\n&lt;/title&gt;\n\n\n\nusing find to look for HTML markers (tags, attributes, etc.):\n\n\nmainpage.find(\"title\")\n\n&lt;title&gt;\nDoctoral Dissertations | Graduate School | University of Tennessee, Knoxville\n&lt;/title&gt;\n\n\n\nusing select which accepts CSS selectors:\n\n\nmainpage.select(\"title\")\n\n[&lt;title&gt;\n Doctoral Dissertations | Graduate School | University of Tennessee, Knoxville\n &lt;/title&gt;]\n\n\nfind will only return the first element. find_all will return all elements. select will also return all elements. Which one you chose depends on what you need to extract. There often several ways to get you there.\nHere are other examples of data extraction:\n\nmainpage.head\n\n&lt;head&gt;&lt;!-- inj yui3-seed: --&gt;&lt;script src=\"//cdnjs.cloudflare.com/ajax/libs/yui/3.6.0/yui/yui-min.js\" type=\"text/javascript\"&gt;&lt;/script&gt;&lt;script src=\"//ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js\" type=\"text/javascript\"&gt;&lt;/script&gt;&lt;!-- Adobe Analytics --&gt;&lt;script src=\"https://assets.adobedtm.com/4a848ae9611a/d0e96722185b/launch-d525bb0064d8.min.js\" type=\"text/javascript\"&gt;&lt;/script&gt;&lt;!-- Cookies --&gt;&lt;link href=\"//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.0.3/cookieconsent.min.css\" rel=\"stylesheet\" type=\"text/css\"/&gt;&lt;script src=\"//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.0.3/cookieconsent.min.js\" type=\"text/javascript\"&gt;&lt;/script&gt;&lt;script src=\"/assets/nr_browser_production.js\" type=\"text/javascript\"&gt;&lt;/script&gt;\n&lt;!-- def.1 --&gt;\n&lt;meta charset=\"utf-8\"/&gt;\n&lt;meta content=\"width=device-width\" name=\"viewport\"/&gt;\n&lt;title&gt;\nDoctoral Dissertations | Graduate School | University of Tennessee, Knoxville\n&lt;/title&gt;\n&lt;!-- FILE meta-tags.inc --&gt;&lt;!-- FILE: /srv/sequoia/main/data/assets/site/meta-tags.inc --&gt;\n&lt;!-- FILE: meta-tags.inc (cont) --&gt;\n&lt;!-- sh.1 --&gt;\n&lt;link href=\"/ir-style.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/&gt;\n&lt;link href=\"/ir-custom.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/&gt;\n&lt;link href=\"ir-custom.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/&gt;\n&lt;link href=\"/ir-local.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/&gt;\n&lt;link href=\"ir-local.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/&gt;\n&lt;link href=\"/ir-print.css\" media=\"print\" rel=\"stylesheet\" type=\"text/css\"/&gt;\n&lt;link href=\"/assets/floatbox/floatbox.css\" rel=\"stylesheet\" type=\"text/css\"/&gt;\n&lt;link href=\"/recent.rss\" rel=\"alternate\" title=\"Site Feed\" type=\"application/rss+xml\"/&gt;\n&lt;link href=\"/favicon.ico\" rel=\"shortcut icon\" type=\"image/x-icon\"/&gt;\n&lt;!--[if IE]&gt;\n&lt;link rel=\"stylesheet\" href=\"/ir-ie.css\" type=\"text/css\" media=\"screen\"&gt;\n&lt;![endif]--&gt;\n&lt;!-- JS --&gt;\n&lt;script src=\"/assets/jsUtilities.js\" type=\"text/javascript\"&gt;&lt;/script&gt;\n&lt;script src=\"/assets/footnoteLinks.js\" type=\"text/javascript\"&gt;&lt;/script&gt;\n&lt;script src=\"/assets/scripts/yui-init.pack.js\" type=\"text/javascript\"&gt;&lt;/script&gt;\n&lt;script src=\"/assets/scripts/bepress-init.pack.js\" type=\"text/javascript\"&gt;&lt;/script&gt;\n&lt;script src=\"/assets/scripts/JumpListYUI.pack.js\" type=\"text/javascript\"&gt;&lt;/script&gt;\n&lt;!-- end sh.1 --&gt;\n&lt;script type=\"text/javascript\"&gt;var pageData = {\"page\":{\"environment\":\"prod\",\"productName\":\"bpdg\",\"language\":\"en\",\"name\":\"ir_etd\",\"businessUnit\":\"els:rp:st\"},\"visitor\":{}};&lt;/script&gt;\n&lt;/head&gt;\n\n\n\nmainpage.a\n\n&lt;a data-scroll=\"\" href=\"https://trace.tennessee.edu\" title=\"Home\"&gt;Home&lt;/a&gt;\n\n\n\nmainpage.find_all(\"a\")[:5]\n\n[&lt;a data-scroll=\"\" href=\"https://trace.tennessee.edu\" title=\"Home\"&gt;Home&lt;/a&gt;,\n &lt;a data-scroll=\"\" href=\"https://trace.tennessee.edu/do/search/advanced/\" title=\"Search\"&gt;&lt;i class=\"icon-search\"&gt;&lt;/i&gt; Search&lt;/a&gt;,\n &lt;a data-scroll=\"\" href=\"https://trace.tennessee.edu/communities.html\" title=\"Browse\"&gt;Browse Collections&lt;/a&gt;,\n &lt;a data-scroll=\"\" href=\"/cgi/myaccount.cgi?context=utk_graddiss\" title=\"My Account\"&gt;My Account&lt;/a&gt;,\n &lt;a data-scroll=\"\" href=\"https://trace.tennessee.edu/about.html\" title=\"About\"&gt;About&lt;/a&gt;]\n\n\n\nmainpage.select(\"a\")[:5]\n\n[&lt;a data-scroll=\"\" href=\"https://trace.tennessee.edu\" title=\"Home\"&gt;Home&lt;/a&gt;,\n &lt;a data-scroll=\"\" href=\"https://trace.tennessee.edu/do/search/advanced/\" title=\"Search\"&gt;&lt;i class=\"icon-search\"&gt;&lt;/i&gt; Search&lt;/a&gt;,\n &lt;a data-scroll=\"\" href=\"https://trace.tennessee.edu/communities.html\" title=\"Browse\"&gt;Browse Collections&lt;/a&gt;,\n &lt;a data-scroll=\"\" href=\"/cgi/myaccount.cgi?context=utk_graddiss\" title=\"My Account\"&gt;My Account&lt;/a&gt;,\n &lt;a data-scroll=\"\" href=\"https://trace.tennessee.edu/about.html\" title=\"About\"&gt;About&lt;/a&gt;]"
  },
  {
    "objectID": "python/webscraping.html#test-run",
    "href": "python/webscraping.html#test-run",
    "title": "Web scraping with Python",
    "section": "Test run",
    "text": "Test run\n\nIdentify relevant markers\nThe html code for this webpage contains the data we are interested in, but it is mixed in with a lot of HTML formatting and data we don’t care about. We need to extract the data relevant to us and turn it into a workable format.\nThe first step is to find the HTML markers that contain our data. One option is to use a web inspector or—even easier—the SelectorGadget, a JavaScript bookmarklet built by Andrew Cantino.\nTo use this tool, go to the SelectorGadget website and drag the link of the bookmarklet to your bookmarks bar.\nNow, go to the dissertations database first page and click on the bookmarklet in your bookmarks bar. You will see a floating box at the bottom of your screen. As you move your mouse across the screen, an orange rectangle appears around each element over which you pass.\nClick on one of the dissertation links: now, there is an a appearing in the box at the bottom as well as the number of elements selected. The selected elements are highlighted in yellow. Those elements are links (in HTML, a tags define hyperlinks).\nAs you can see, all the links we want are selected. However, there are many other links we don’t want that are also highlighted. In fact, all links in the document are selected. We need to remove the categories of links that we don’t want. To do this, hover above any of the links we don’t want. You will see a red rectangle around it. Click on it: now all similar links are gone. You might have to do this a few times until only the relevant links (i.e. those that lead to the dissertation information pages) remain highlighted.\nAs there are 100 such links per page, the count of selected elements in the bottom floating box should be down to 100.\nIn the main section of the floating box, you can now see: .article-listing a. This means that the data we want are under the HTML elements .article-listing a (the class .article-listing and the tag a).\n\n\nExtract test URL\nIt is a good idea to test things out on a single element before doing a massive batch scraping of a site, so let’s test our method for the first dissertation.\nTo start, we need to extract the first URL. Here, we will use the CSS selectors (we can get there using find too). mainpage.select(\".article-listing a\") would give us all the results (100 links):\n\nlen(mainpage.select(\".article-listing a\"))\n\n100\n\n\nTo get the first one, we index it:\n\nmainpage.select(\".article-listing a\")[0]\n\n&lt;a href=\"https://trace.tennessee.edu/utk_graddiss/8076\"&gt;Understanding host-microbe interactions in maize kernel and sweetpotato leaf metagenomic profiles.&lt;/a&gt;\n\n\nThe actual URL is contained in the href attribute. Attributes can be extracted with the get method:\n\nmainpage.select(\".article-listing a\")[0].get(\"href\")\n\n'https://trace.tennessee.edu/utk_graddiss/8076'\n\n\nWe now have our URL as a string. We can double-check that it is indeed a string:\n\ntype(mainpage.select(\".article-listing a\")[0].get(\"href\"))\n\nstr\n\n\nThis is exactly what we need to send a request to that site, so let’s create an object url_test with it:\n\nurl_test = mainpage.select(\".article-listing a\")[0].get(\"href\")\n\nWe have our first thesis URL:\n\nprint(url_test)\n\nhttps://trace.tennessee.edu/utk_graddiss/8076\n\n\n\n\nSend request to test URL\nNow that we have the URL for the first dissertation information page, we want to extract the date, major, and advisor for that dissertation.\nThe first thing to do—as we did earlier with the database site—is to send a request to that page. Let’s assign it to a new object that we will call r_test:\n\nr_test = requests.get(url_test)\n\nThen we can parse it with Beautiful Soup (as we did before). Let’s create a dissertpage_test object:\n\ndissertpage_test = BeautifulSoup(r_test.content, \"html.parser\")\n\n\n\nGet data for test URL\nIt is time to extract the publication date, major, and advisor for our test URL.\nLet’s start with the date. Thanks to the SelectorGadget, following the method we saw earlier, we can see that we now need elements marked by #publication_date p.\nWe can use select as we did earlier:\n\ndissertpage_test.select(\"#publication_date p\")\n\n[&lt;p&gt;5-2023&lt;/p&gt;]\n\n\nNotice the square brackets around our result: this is import. It shows us that we have a ResultSet (a list of results specific to Beautiful Soup). This is because select returns all the results. Here, we have a single result, but the format is still list-like. Before we can go further, we need to index the value out of it:\n\ndissertpage_test.select(\"#publication_date p\")[0]\n\n&lt;p&gt;5-2023&lt;/p&gt;\n\n\nWe can now get the text out of this paragraph with the text attribute:\n\ndissertpage_test.select(\"#publication_date p\")[0].text\n\n'5-2023'\n\n\nWe could save it in a variable date_test:\n\ndate_test = dissertpage_test.select(\"#publication_date p\")[0].text\n\n\n\nYour turn:\n\nGet the major and advisor for our test URL."
  },
  {
    "objectID": "python/webscraping.html#full-run",
    "href": "python/webscraping.html#full-run",
    "title": "Web scraping with Python",
    "section": "Full run",
    "text": "Full run\nOnce everything is working for a test site, we can do some bulk scraping.\n\nExtract all URLs\nWe already know how to get the 100 dissertations links from the main page: mainpage.select(\".article-listing a\"). Let’s assign it to a variable:\n\ndissertlinks = mainpage.select(\".article-listing a\")\n\nThis ResultSet is an iterable, meaning that it can be used in a loop.\nLet’s write a loop to extract all the URLs from this ResultSet of links:\n\nurls = []  # We create an empty list before filling it during the loop\n\nfor link in dissertlinks:\n    urls.append(link.get(\"href\"))\n\nLet’s see our first 5 URLs:\n\nurls[:5]\n\n['https://trace.tennessee.edu/utk_graddiss/8076',\n 'https://trace.tennessee.edu/utk_graddiss/8080',\n 'https://trace.tennessee.edu/utk_graddiss/8086',\n 'https://trace.tennessee.edu/utk_graddiss/8078',\n 'https://trace.tennessee.edu/utk_graddiss/8073']\n\n\n\n\nExtract data from each page\nFor each element of urls (i.e. for each dissertation URL), we can now get our information.\n\nls = []                                            # Create an empty list\n\nfor url in urls:                                   # For each element of our list of sites\n    r = requests.get(url)                                     # Send a request to the site\n    dissertpage = BeautifulSoup(r.text, \"html.parser\")        # Parse the result\n    date = dissertpage.select(\"#publication_date p\")[0].text  # Get the date\n    major = dissertpage.select(\"#department p\")[0].text       # Get the major\n    advisor = dissertpage.select(\"#advisor1 p\")[0].text            # Get the advisor\n    ls.append((date, major, advisor))                              # Store the results in the list\n    time.sleep(0.1)                                           # Add a delay at each iteration\n\n\nSome sites will block requests if they are too frequent. Adding a little delay between requests is often a good idea."
  },
  {
    "objectID": "python/webscraping.html#store-results-in-dataframe",
    "href": "python/webscraping.html#store-results-in-dataframe",
    "title": "Web scraping with Python",
    "section": "Store results in DataFrame",
    "text": "Store results in DataFrame\nA DataFrame would be a lot more convenient than a list to hold our results.\nFirst, we create a list with the column names for our future DataFrame:\n\ncols = [\"Date\", \"Major\", \"Advisor\"]\n\nThen we create our DataFrame:\n\ndf = pd.DataFrame(ls, columns=cols)\n\n\ndf\n\n\n\n\n\n\n\n\nDate\nMajor\nAdvisor\n\n\n\n\n0\n5-2023\nLife Sciences\nBode A. Olukolu\n\n\n1\n5-2023\nNuclear Engineering\nErik Lukosi\n\n\n2\n5-2023\nEnergy Science and Engineering\nKyle R. Gluesenkamp\n\n\n3\n5-2023\nEnglish\nMargaret Lazarus Dean\n\n\n4\n5-2023\nMechanical Engineering\nDoug Aaron\n\n\n...\n...\n...\n...\n\n\n95\n5-2023\nChild and Family Studies\nMegan Haselschwerdt\n\n\n96\n5-2023\nMaterials Science and Engineering\nDustin A. Gilbert\n\n\n97\n5-2023\nIndustrial Engineering\nMingzhou Jin\n\n\n98\n5-2023\nComparative and Experimental Medicine\nDebra L. Miller\n\n\n99\n5-2023\nManagement Science\nMichel Ballings\n\n\n\n\n100 rows × 3 columns"
  },
  {
    "objectID": "python/webscraping.html#save-results-to-file",
    "href": "python/webscraping.html#save-results-to-file",
    "title": "Web scraping with Python",
    "section": "Save results to file",
    "text": "Save results to file\nAs a final step, we will save our data to a CSV file:\ndf.to_csv('dissertations_data.csv', index=False)\n\nThe default index=True writes the row numbers. We are not writing these indices in our file by changing the value of this argument to False.\n\nIf you are using a Jupyter notebook or the IPython shell, you can type !ls to see that the file is there and !cat dissertations_data.csv to print its content.\n\n! is a magic command that allows to run Unix shell commands in a notebook or IPython shell."
  },
  {
    "objectID": "r/hpc_data.html",
    "href": "r/hpc_data.html",
    "title": "Data on HPC clusters",
    "section": "",
    "text": "So far, we have played with randomly created data. In your work, you will often need to work with real world data.\nHow do you move it to the cluster? Where should you store it?\nIt’s time to talk about data on HPC clusters."
  },
  {
    "objectID": "r/hpc_data.html#transferring-data-tofrom-the-cluster",
    "href": "r/hpc_data.html#transferring-data-tofrom-the-cluster",
    "title": "Data on HPC clusters",
    "section": "Transferring data to/from the cluster",
    "text": "Transferring data to/from the cluster\n\nSecure Copy Protocol\nSecure Copy Protocol (SCP) allows to copy files over the Secure Shell Protocol (SSH) with the scp utility. scp follows a syntax similar to that of the cp command.\nNote that you need to run it from your local machines (not from the cluster).\n\nCopy from your machine to the cluster\n# Copy a local file to your home directory on the cluster\nscp /local/path/file username@hostname:\n# Copy a local file to some path on the cluster\nscp /local/path/file username@hostname:/remote/path\n\n\nCopy from the cluster to your machine\n# Copy a file from the cluster to some path on your machine\nscp username@hostname:/remote/path/file /local/path\n# Copy a file from the cluster to your current location on your machine\nscp username@hostname:/remote/path/file .\nYou can also use wildcards to transfer multiple files:\n# Copy all the Bash scripts from your cluster home dir to some local path\nscp username@hostname:*.sh /local/path\n\n\nCopying directories\nTo copy a directory, you need to add the -r (recursive) flag:\nscp -r /local/path/folder username@hostname:/remote/path\n\n\nCopying for Windows users\nMobaXterm users (on Windows) can copy files by dragging them between the local and remote machines in the GUI. Alternatively, they can use the download and upload buttons.\n\n\n\nSecure File Transfer Protocol\nThe Secure File Transfer Protocol (SFTP) is more sophisticated and allows additional operations in an interactive shell. The sftp command provided by OpenSSH and other packages launches an SFTP client:\nsftp username@hostname\n\nLook at your prompt: your usual Bash/Zsh prompt has been replaced with sftp&gt;.\n\nFrom this prompt, you can access a number of SFTP commands. Type help for a list:\nsftp&gt; help\nAvailable commands:\nbye                                Quit sftp\ncd path                            Change remote directory to 'path'\nchgrp [-h] grp path                Change group of file 'path' to 'grp'\nchmod [-h] mode path               Change permissions of file 'path' to 'mode'\nchown [-h] own path                Change owner of file 'path' to 'own'\ncopy oldpath newpath               Copy remote file\ncp oldpath newpath                 Copy remote file\ndf [-hi] [path]                    Display statistics for current directory or\n                                   filesystem containing 'path'\nexit                               Quit sftp\nget [-afpR] remote [local]         Download file\nhelp                               Display this help text\nlcd path                           Change local directory to 'path'\nlls [ls-options [path]]            Display local directory listing\nlmkdir path                        Create local directory\nln [-s] oldpath newpath            Link remote file (-s for symlink)\nlpwd                               Print local working directory\nls [-1afhlnrSt] [path]             Display remote directory listing\nlumask umask                       Set local umask to 'umask'\nmkdir path                         Create remote directory\nprogress                           Toggle display of progress meter\nput [-afpR] local [remote]         Upload file\npwd                                Display remote working directory\nquit                               Quit sftp\nreget [-fpR] remote [local]        Resume download file\nrename oldpath newpath             Rename remote file\nreput [-fpR] local [remote]        Resume upload file\nrm path                            Delete remote file\nrmdir path                         Remove remote directory\nsymlink oldpath newpath            Symlink remote file\nversion                            Show SFTP version\n!command                           Execute 'command' in local shell\n!                                  Escape to local shell\n?                                  Synonym for help\nAs this list shows, you have access to a number of classic Unix command such as cd, pwd, ls, etc. These commands will be executed on the remote machine.\nIn addition, there are a number of commands of the form l&lt;command&gt;. “l” stands for “local”.\nThese commands will be executed on your local machine.\nFor instance, ls will list the files in your current directory in the remote machine while lls (“local ls”) will list the files in your current directory on your computer.\nThis means that you are now able to navigate two file systems at once: your local machine and the remote machine.\n\nHere are a few examples:\n\nsftp&gt; pwd              # print remote working directory\nsftp&gt; lpwd             # print local working directory\nsftp&gt; ls               # list files in remote working directory\nsftp&gt; lls              # list files in local working directory\nsftp&gt; cd               # change the remote directory\nsftp&gt; lcd              # change the local directory\nsftp&gt; put local_file   # upload a file\nsftp&gt; get remote_file  # download a file\n\nCopying directories\nTo upload/download directories, you first need to create them in the destination, then copy the content with the -r (recursive) flag.\n\nIf you have a local directory called dir and you want to copy it to the cluster you need to run:\n\nsftp&gt; mkdir dir    # First create the directory\nsftp&gt; put -r dir   # Then copy the content\nTo terminate the session, press &lt;Ctrl+D&gt;.\n\n\n\nSyncing\nIf, instead of an occasional copying of files between your machine and the cluster, you want to keep a directory in sync between both machines, you might want to use rsync instead. You can look at the Alliance wiki page on rsync for complete instructions.\n\n\nHeavy transfers\nWhile the methods covered above work very well for limited amounts of data, if you need to make large transfers, you should use globus instead, following the instructions in the Alliance wiki page on this service.\n\n\nWindows line endings\nOn modern Mac operating systems and on Linux, lines in files are terminated with a newline (\\n). On Windows, they are terminated with a carriage return + newline (\\r\\n).\nWhen you transfer files between Windows and Linux (the cluster uses Linux), this creates a mismatch. Most modern software handle this correctly, but you may occasionally run into problems.\nThe solution is to convert a file from Windows encoding to Unix encoding with:\ndos2unix file\nTo convert a file back to Windows encoding, run:\nunix2dos file"
  },
  {
    "objectID": "r/hpc_data.html#files-management",
    "href": "r/hpc_data.html#files-management",
    "title": "Data on HPC clusters",
    "section": "Files management",
    "text": "Files management\nThe Alliance clusters are designed to handle large files very well. They are however slowed by the presence of many small files. It is thus important to know how to handle large collections of files by archiving them with tools such as tar and dar."
  },
  {
    "objectID": "r/hpc_data.html#where-to-store-data",
    "href": "r/hpc_data.html#where-to-store-data",
    "title": "Data on HPC clusters",
    "section": "Where to store data",
    "text": "Where to store data\nSupercomputers have several filesystems and you should familiarize yourself with the quotas and policies of the clusters you use.\nAll filesystems are mounted on all nodes so that you can access the data on any network storage from any node (e.g. something in /home, /project, or /scratch will be accessible from any login node or compute node).\nA temporary folder gets created directly on the compute nodes while a job is running. In situations with heavy I/O or involving many files, it is worth considering copying data to it as part of the job. In that case, make sure to copy the results back to network storage before the end of the job."
  },
  {
    "objectID": "r/hpc_future.html",
    "href": "r/hpc_future.html",
    "title": "The future package",
    "section": "",
    "text": "The future package is a modern package that brings a consistent and simple API for all evaluation strategies of futures in R.\nExcellent backends have been built on top of it."
  },
  {
    "objectID": "r/hpc_future.html#classic-parallel-packages-in-r",
    "href": "r/hpc_future.html#classic-parallel-packages-in-r",
    "title": "The future package",
    "section": "Classic parallel packages in R",
    "text": "Classic parallel packages in R\nWe talked in the previous section about various types of parallelism. Several options exist in R to run code in shared-memory or distributed parallelism.\nExamples of options for shared-memory parallelism:\n\nThe foreach package with backends such as doMC, now also part of the doParallel package.\nmclapply() and mcmapply() from the parallel package (part of the core distribution of R).\n\nExamples of options for distributed parallelism:\n\nThe foreach package with backends such as doSNOW, now also part of the doParallel package.\nThe suite of clusterApply() and par*apply() functions from the parallel package.\n\n\nThe parallel package is a merger of the former multicore package for shared-memory and of the snow package for distributed parallelism.\nSimilarly, the doParallel package is merger of the doMC package for use with foreach in shared-memory and the doSNOW package for use with foreach for distributed parallelism."
  },
  {
    "objectID": "r/hpc_future.html#the-future-package",
    "href": "r/hpc_future.html#the-future-package",
    "title": "The future package",
    "section": "The future package",
    "text": "The future package\nThe future package opened up a new landscape in the world of parallel R by providing a simple and consistent API for the evaluation of futures sequentially, through shared-memory parallelism, or through distributed parallelism.\n\nA future is an object that acts as an abstract representation for a value in the future. A future can be resolved (if the value has been computed) or unresolved. If the value is queried while the future is unresolved, the process is blocked until the future is resolved. Futures thus allow for asynchronous and parallel evaluations.\n\nThe evaluation strategy is set with the plan() function:\n\nplan(sequential):\nFutures are evaluated sequentially in the current R session.\nplan(multisession):\nFutures are evaluated by new R sessions spawned in the background (multi-processing in shared memory).\nplan(multicore):\nFutures are evaluated in processes forked from the existing process (multi-processing in shared memory).\nplan(cluster):\nFutures are evaluated on an ad-hoc cluster (distributed parallelism across multiple nodes).\n\n\nConsistency\nTo ensure a consistent behaviour across plans, all evaluations are done in a local environment:\n\nlibrary(future)\n\na &lt;- 1\n\nb %&lt;-% {      # %&lt;-% creates futures\n  a &lt;- 2\n}\n\na\n\n[1] 1"
  },
  {
    "objectID": "r/hpc_future.html#the-future-ecosystem",
    "href": "r/hpc_future.html#the-future-ecosystem",
    "title": "The future package",
    "section": "the future ecosystem",
    "text": "the future ecosystem\nSeveral great packages have been built on top of the future API.\n\nThe doFuture package allows to parallelize foreach expressions on the future evaluation strategies.\nSimilarly, the future.apply package parallelizes the *apply() functions on these strategies.\nThe furrr package provides a parallel version of purrr for those who prefer this approach to functional programming.\nThe future.callr package implements a future evaluation based on callr that resolves every future in a new R session. This removes any limitation on the number of background R parallel processes that can be active at the same time.\nThe future.batchtools package implements a future evaluation based on the batchtools package—a package that provides functions to interact with HPC systems schedulers such as Slurm.\n\nIn this course, we will cover foreach with doFuture in great details to explain all the important concepts. After that, you will be able to use any of these backends easily."
  },
  {
    "objectID": "r/hpc_optimizations.html",
    "href": "r/hpc_optimizations.html",
    "title": "Optimizations",
    "section": "",
    "text": "A lot of hardware is not the answer to poorly written code. Before considering parallelization, you should think about ways to optimize your code sequentially.\nWhy?\n\nnot all code can be parallelized,\nparallelization is costly (overhead of parallelization and, if you use a supercomputer, waiting time to access an Alliance cluster or money spent on a commercial cloud),\nthe optimization of the sequential code will also benefit the parallel code.\n\nIn many cases, writing better code will save you more computing time than parallelization.\nIn this section, we will cover several principles by playing with the programmatic implementation of the fizz buzz game."
  },
  {
    "objectID": "r/hpc_optimizations.html#toy-example",
    "href": "r/hpc_optimizations.html#toy-example",
    "title": "Optimizations",
    "section": "Toy example",
    "text": "Toy example\nFizz buzz is a children game to practice divisions. Players take turn counting out loud while replacing:\n\nany number divisible by 3 with the word “Fizz”,\nany number divisible by 5 with the word “Buzz”,\nany number divisible by both 3 and 5 with the word “FizzBuzz”.\n\nLet’s write functions that output series from 1 to n following these rules and time them to draw general principles about code efficiency."
  },
  {
    "objectID": "r/hpc_optimizations.html#setup",
    "href": "r/hpc_optimizations.html#setup",
    "title": "Optimizations",
    "section": "Setup",
    "text": "Setup\nFirst of all, we need to load the necessary modules:\nmodule load StdEnv/2020 gcc/11.3.0 r/4.3.1\nThen we need to launch a job.\n\nInteractive job\nIf there are few of us, we will use interactive sessions with one CPU each with:\nsalloc --time=2:00:00 --mem-per-cpu=3500M\nWe can then launch R and load the benchmarking package we will use throughout this section:\n\nlibrary(bench)\n\n\n\nBatch jobs\nIf there are more of us than there are CPUs in the cluster, we will run batch jobs. In this Case:\n\nCreate an R script called optim.R with the code to run (you can reuse the same script for all sections on this page by editing it). Don’t forget to load the package bench in your script.\nCreate a bash script called optim.sh with the following:\n\n\n\n&lt;your_job&gt;.sh\n\n#!/bin/bash\n#SBATCH --account=def-&lt;your_account&gt;\n#SBATCH --time=15\n#SBATCH --mem-per-cpu=3500M\n#SBATCH --cpus-per-task=4\n#SBATCH --job-name=\"&lt;your_job&gt;\"\nmodule load StdEnv/2020 gcc/11.3.0 r/4.3.1\nRscript &lt;your_script&gt;.R\n\n\nRun the jobs with:\n\nsbatch optim.sh"
  },
  {
    "objectID": "r/hpc_optimizations.html#optimizations",
    "href": "r/hpc_optimizations.html#optimizations",
    "title": "Optimizations",
    "section": "Optimizations",
    "text": "Optimizations\n\nPre-allocate memory\nIn order to store the results of a loop, we need to create an object and assign to it the result of the loop at each iteration. In this first function, we create an empty object z of class integer and of length 0 for that purpose:\n\nf1 &lt;- function(n) {\n  z &lt;- integer()\n  for(i in 1:n) {\n    if(i %% 3 == 0 && i %% 5 == 0) {\n      z[i] &lt;- \"FizzBuzz\"\n    } else if(i %% 3 == 0) {\n      z[i] &lt;- \"Fizz\"\n    } else if(i %% 5 == 0) {\n      z[i] &lt;- \"Buzz\"\n    } else {\n      z[i] &lt;- i\n    }\n  }\n  z\n}\n\nThe second function is similar, but this time, we initialize z with its final length. This means that we are pre-allocating memory for the full vector before we run the loop instead of growing the vector at each iteration:\n\nf2 &lt;- function(n) {\n  z &lt;- integer(n)\n  for(i in 1:n) {\n    if(i %% 3 == 0 && i %% 5 == 0) {\n      z[i] &lt;- \"FizzBuzz\"\n    } else if(i %% 3 == 0) {\n      z[i] &lt;- \"Fizz\"\n    } else if(i %% 5 == 0) {\n      z[i] &lt;- \"Buzz\"\n    } else {\n      z[i] &lt;- i\n    }\n  }\n  z\n}\n\nLet’s make sure that our functions work by testing it on a small number:\n\nf1(20)\n\n [1] \"1\"        \"2\"        \"Fizz\"     \"4\"        \"Buzz\"     \"Fizz\"    \n [7] \"7\"        \"8\"        \"Fizz\"     \"Buzz\"     \"11\"       \"Fizz\"    \n[13] \"13\"       \"14\"       \"FizzBuzz\" \"16\"       \"17\"       \"Fizz\"    \n[19] \"19\"       \"Buzz\"    \n\nf2(20)\n\n [1] \"1\"        \"2\"        \"Fizz\"     \"4\"        \"Buzz\"     \"Fizz\"    \n [7] \"7\"        \"8\"        \"Fizz\"     \"Buzz\"     \"11\"       \"Fizz\"    \n[13] \"13\"       \"14\"       \"FizzBuzz\" \"16\"       \"17\"       \"Fizz\"    \n[19] \"19\"       \"Buzz\"    \n\n\nNow, let’s benchmark them for a large number:\n\nn &lt;- 1e5\nmark(f1(n), f2(n))\n\nWarning: Some expressions had a GC in every iteration; so filtering is\ndisabled.\n\n\n# A tibble: 2 × 6\n  expression      min   median `itr/sec` mem_alloc `gc/sec`\n  &lt;bch:expr&gt; &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt;\n1 f1(n)         139ms    140ms      7.01   16.65MB     26.3\n2 f2(n)         123ms    124ms      8.04    1.24MB     30.5\n\n\nf2() is consistently faster, although very slightly. In many cases, the difference you will find will be a lot greater.\nNote also the large difference in memory allocation.\n\n\nNo, loops are not a big ‘no no’\nBy now, you might be thinking: “Wait… aren’t loops a big ‘no no’ in R? I’ve always been told that they are slow and that one should always use functional programming! We are talking about optimization in this course and we are using loops?!?”\nThere are a lot of misconceptions around R loops. They can be very slow if you don’t pre-allocate memory. Otherwise they are almost always faster than functions (the apply() family or the tidyverse equivalent of the purrr::map() family). You can choose to use a functional programming approach for style and readability, but not for speed.\nLet’s test it.\nFirst we create a function:\n\nf3 &lt;- function(n) {\n  if(n %% 3 == 0 && n %% 5 == 0) {\n    \"FizzBuzz\"\n  } else if(n %% 3 == 0) {\n    \"Fizz\"\n  } else if(n %% 5 == 0) {\n    \"Buzz\"\n  } else {\n    n\n  }\n}\n\nThen we pass it through sapply(). We can test that it works on a small number:\n\nsapply(1:20, f3)\n\n [1] \"1\"        \"2\"        \"Fizz\"     \"4\"        \"Buzz\"     \"Fizz\"    \n [7] \"7\"        \"8\"        \"Fizz\"     \"Buzz\"     \"11\"       \"Fizz\"    \n[13] \"13\"       \"14\"       \"FizzBuzz\" \"16\"       \"17\"       \"Fizz\"    \n[19] \"19\"       \"Buzz\"    \n\n\nFinally, we compare the timing with that of f2():\n\nmark(f2(n), sapply(1:n, f3))\n\nWarning: Some expressions had a GC in every iteration; so filtering is\ndisabled.\n\n\n# A tibble: 2 × 6\n  expression           min   median `itr/sec` mem_alloc `gc/sec`\n  &lt;bch:expr&gt;      &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt;\n1 f2(n)              132ms    135ms      7.40    1.15MB     35.2\n2 sapply(1:n, f3)    180ms    189ms      5.18    3.35MB     31.1\n\n\nAs you can see, the loop is faster.\n\n\nAvoid unnecessary operations\n\nExample 1\nCalling z as the last command in our function is the same as calling return(z).\nFrom the R documentation:\n\nIf the end of a function is reached without calling return, the value of the last evaluated expression is returned.\n\nNow, what about using print() instead?\n\nf4 &lt;- function(n) {\n  z &lt;- integer(n)\n  for(i in 1:n) {\n    if(i %% 3 == 0 && i %% 5 == 0) {\n      z[i] &lt;- \"FizzBuzz\"\n    } else if(i %% 3 == 0) {\n      z[i] &lt;- \"Fizz\"\n    } else if(i %% 5 == 0) {\n      z[i] &lt;- \"Buzz\"\n    } else {\n      z[i] &lt;- i\n    }\n  }\n  print(z)\n}\n\nLet’s benchmark it against f2():\nmark(f2(n), f4(n))\n [1] \"1\"        \"2\"        \"Fizz\"     \"4\"        \"Buzz\"     \"Fizz\"    \n [7] \"7\"        \"8\"        \"Fizz\"     \"Buzz\"     \"11\"       \"Fizz\"    \n[13] \"13\"       \"14\"       \"FizzBuzz\" \"16\"       \"17\"       \"Fizz\"    \n[19] \"19\"       \"Buzz\"     \"Fizz\"     \"22\"       \"23\"       \"Fizz\"    \n[25] \"Buzz\"     \"26\"       \"Fizz\"     \"28\"       \"29\"       \"FizzBuzz\"\n[31] \"31\"       \"32\"       \"Fizz\"     \"34\"       \"Buzz\"     \"Fizz\"    \n[37] \"37\"       \"38\"       \"Fizz\"     \"Buzz\"     \"41\"       \"Fizz\"\n...\n\n  expression      min median `itr/sec` mem_alloc `gc/sec` n_itr  n_gc total_time\n  &lt;bch:expr&gt; &lt;bch:tm&gt; &lt;bch:&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;   &lt;bch:tm&gt;\n1 f2(n)         131ms  139ms      6.71        NA    21.8      4    13      596ms\n2 f4(n)         405ms  411ms      2.43        NA     8.52     2     7      822ms\n# ℹ 4 more variables: result &lt;list&gt;, memory &lt;list&gt;, time &lt;list&gt;, gc &lt;list&gt;\nWarning message:\nSome expressions had a GC in every iteration; so filtering is disabled.\nf4() is 3 times slower.\nWhat happened?\nprint() returns its argument, but it additionally prints it to the standard output. This is why the mark() function printed the output of f4() before printing the timings.\nAs you can see, printing takes a long time.\nIf you are evaluating f2() on its own (e.g. f2(20)), the returned result will also be printed to standard output and both functions will be equivalent. However, if you are using the function in another context, printing becomes an unnecessary and timely operation and f4() would be a very bad option. f4() is thus not a good function.\nHere is an example in which f4() would perform a totally unnecessary operation that f2() avoids:\n\na &lt;- f2(20)\n\n\nNo unnecessary printing.\n\n\na &lt;- f4(20)\n\n [1] \"1\"        \"2\"        \"Fizz\"     \"4\"        \"Buzz\"     \"Fizz\"    \n [7] \"7\"        \"8\"        \"Fizz\"     \"Buzz\"     \"11\"       \"Fizz\"    \n[13] \"13\"       \"14\"       \"FizzBuzz\" \"16\"       \"17\"       \"Fizz\"    \n[19] \"19\"       \"Buzz\"    \n\n\n\nUnnecessary printing.\n\nEven worse would be to use:\nf5 &lt;- function(n) {\n  for(i in 1:n) {\n    if(i %% 3 == 0 && i %% 5 == 0) {\n      print(\"FizzBuzz\")\n    } else if(i %% 3 == 0) {\n      print(\"Fizz\")\n    } else if(i %% 5 == 0) {\n      print(\"Buzz\")\n    } else {\n      print(i)\n    }\n  }\n}\n\nmark(f2(n), f4(n), check = F)\n  expression      min   median `itr/sec` mem_alloc `gc/sec` n_itr  n_gc\n  &lt;bch:expr&gt; &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;\n1 f2(n)       132.8ms 141.69ms     6.77         NA     25.4     4    15\n2 f5(n)         1.65s    1.65s     0.606        NA     12.7     1    21\n# ℹ 5 more variables: total_time &lt;bch:tm&gt;, result &lt;list&gt;, memory &lt;list&gt;,\n#   time &lt;list&gt;, gc &lt;list&gt;\nWarning message:\nSome expressions had a GC in every iteration; so filtering is disabled.\n\nWe have to disable the check here because the results of the two functions are not technically the same (but we don’t care because, in both cases, the series gets created and that’s what we want).\n\nHere the difference in timing is a factor of 12!\n\n\nExample 2\nOne modulo operation and equality test can be removed by replacing i %% 3 == 0 && i %% 5 == 0 by i %% 15 == 0. We now have three modulo operations and equality tests per iteration instead of four. This gives us a little speedup:\n\nf6 &lt;- function(n) {\n  z &lt;- integer(n)\n  for(i in 1:n) {\n    if(i %% 15 == 0) {\n      z[i] &lt;- \"FizzBuzz\"\n    } else if(i %% 3 == 0) {\n      z[i] &lt;- \"Fizz\"\n    } else if(i %% 5 == 0) {\n      z[i] &lt;- \"Buzz\"\n    } else {\n      z[i] &lt;- i\n    }\n  }\n  z\n}\n\nmark(f2(n), f6(n))\n\nWarning: Some expressions had a GC in every iteration; so filtering is\ndisabled.\n\n\n# A tibble: 2 × 6\n  expression      min   median `itr/sec` mem_alloc `gc/sec`\n  &lt;bch:expr&gt; &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt;\n1 f2(n)         125ms    129ms      7.66    1.25MB     28.7\n2 f6(n)         115ms    122ms      8.30    1.22MB     29.9\n\n\nBut we can remove an additional modulo operation and equality test at each iteration by assigning i %% 3 == 0 and i %% 5 == 0 to variables:\n\nf7 &lt;- function(n) {\n  z &lt;- integer(n)\n  for(i in 1:n) {\n    div3 &lt;- (i %% 3 == 0)\n    div5 &lt;- (i %% 5 == 0)\n    if(div3 && div5) {\n      z[i] &lt;- \"FizzBuzz\"\n    } else if(div3) {\n      z[i] &lt;- \"Fizz\"\n    } else if(div5) {\n      z[i] &lt;- \"Buzz\"\n    } else {\n      z[i] &lt;- i\n    }\n  }\n  z\n}\n\nNow we only have two modulo operations and equality tests per iteration and we get another little speedup:\n\nmark(f6(n), f7(n))\n\nWarning: Some expressions had a GC in every iteration; so filtering is\ndisabled.\n\n\n# A tibble: 2 × 6\n  expression      min   median `itr/sec` mem_alloc `gc/sec`\n  &lt;bch:expr&gt; &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt;\n1 f6(n)         118ms    121ms      8.26    1.15MB     33.0\n2 f7(n)         122ms    206ms      5.49    1.22MB     18.3\n\n\n\n\nExample 3\nWe can assign 1:n to z instead of initializing it as an empty vector, thus rendering the assignment of i to z[i] in the last else statement unnecessary:\n\nf8 &lt;- function(n) {\n  z &lt;- 1:n\n  for(i in z) {\n    div3 &lt;- (i %% 3 == 0)\n    div5 &lt;- (i %% 5 == 0)\n    if(div3 && div5) {\n      z[i] &lt;- \"FizzBuzz\"\n    } else if(div3) {\n      z[i] &lt;- \"Fizz\"\n    } else if(div5) {\n      z[i] &lt;- \"Buzz\"\n    } \n  }\n  z\n}\n\nThis function works:\n\nf8(20)\n\n [1] \"1\"        \"2\"        \"Fizz\"     \"4\"        \"Buzz\"     \"Fizz\"    \n [7] \"7\"        \"8\"        \"Fizz\"     \"Buzz\"     \"11\"       \"Fizz\"    \n[13] \"13\"       \"14\"       \"FizzBuzz\" \"16\"       \"17\"       \"Fizz\"    \n[19] \"19\"       \"Buzz\"    \n\n\nand we get a really good speedup here:\n\nmark(f7(n), f8(n))\n\nWarning: Some expressions had a GC in every iteration; so filtering is\ndisabled.\n\n\n# A tibble: 2 × 6\n  expression      min   median `itr/sec` mem_alloc `gc/sec`\n  &lt;bch:expr&gt; &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt;\n1 f7(n)         102ms  116.7ms      7.84    1.15MB     25.5\n2 f8(n)        62.4ms   98.6ms      9.78    1.15MB     26.1\n\n\n\n\n\nVectorize whenever possible\nWe can actually get rid of the loop and use a vectorized approach.\n\nf9 &lt;- function(n) {\n  z &lt;- 1:n\n  div3 &lt;- (z %% 3 == 0)\n  div5 &lt;- (z %% 5 == 0)\n  z[div3] &lt;- \"Fizz\"\n  z[div5] &lt;- \"Buzz\"\n  z[(div3 & div5)] &lt;- \"FizzBuzz\"\n  z\n}\n\nThis still give us the same result:\n\nf9(20)\n\n [1] \"1\"        \"2\"        \"Fizz\"     \"4\"        \"Buzz\"     \"Fizz\"    \n [7] \"7\"        \"8\"        \"Fizz\"     \"Buzz\"     \"11\"       \"Fizz\"    \n[13] \"13\"       \"14\"       \"FizzBuzz\" \"16\"       \"17\"       \"Fizz\"    \n[19] \"19\"       \"Buzz\"    \n\n\n\nmark(f8(n), f9(n))\n\nWarning: Some expressions had a GC in every iteration; so filtering is\ndisabled.\n\n\n# A tibble: 2 × 6\n  expression      min   median `itr/sec` mem_alloc `gc/sec`\n  &lt;bch:expr&gt; &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt;\n1 f8(n)        64.5ms  103.5ms      9.87    1.15MB    25.7 \n2 f9(n)        18.1ms   18.8ms     51.3     5.62MB     5.92\n\n\nThe speedup of 3.8 shows how important it is to use vectorization whenever possible.\n\n\nReplace costly operations where possible\nSometimes, it isn’t obvious that one method will be faster than another. Benchmarking alternative expressions can teach you which ones are faster.\nFor instance, it is much faster to index a column from a dataframe by its name (e.g. dataframe$column1) than by using list indexing (e.g. dataframe[[1]]).\nSometimes, packages exist which bring much more efficiency than can be achieved with base R. In the case of data frames for example, there is data.table.\n\n\nConclusion\nStarting from our first function f1(), we have gained a speedup of 7.4, simply by writing better code and without using parallelization and additional hardware:\n\nmark(f1(n), f9(n))\n\nWarning: Some expressions had a GC in every iteration; so filtering is\ndisabled.\n\n\n# A tibble: 2 × 6\n  expression      min   median `itr/sec` mem_alloc `gc/sec`\n  &lt;bch:expr&gt; &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt;\n1 f1(n)       312.2ms  312.8ms      3.20   16.65MB    14.4 \n2 f9(n)        18.2ms   18.9ms     43.5     5.57MB     3.95\n\n\nIf we used a silly function such as f5() as our starting function, the speedup would be 370."
  },
  {
    "objectID": "r/hpc_parallelism.html",
    "href": "r/hpc_parallelism.html",
    "title": "Parallelism: concepts",
    "section": "",
    "text": "Once all sequential optimizations have been exhausted, it is time to consider whether parallelization makes sense.\nThis section covers important concepts that are necessary to understand clearly before moving on to writing parallel code."
  },
  {
    "objectID": "r/hpc_parallelism.html#hidden-parallelism",
    "href": "r/hpc_parallelism.html#hidden-parallelism",
    "title": "Parallelism: concepts",
    "section": "Hidden parallelism",
    "text": "Hidden parallelism\nAn increasing number of packages run code in parallel under the hood. It is very important to be aware of this before attempting any explicit parallelization or you may end up with recursive multicore parallelization and an explosion of running cores. This can be both inefficient with demultiplied overhead and extremely resource intensive.\nOne way to assess this is to test the package on your machine and look at the number of cores running with tools such as htop."
  },
  {
    "objectID": "r/hpc_parallelism.html#embarrassingly-parallel-problems",
    "href": "r/hpc_parallelism.html#embarrassingly-parallel-problems",
    "title": "Parallelism: concepts",
    "section": "Embarrassingly parallel problems",
    "text": "Embarrassingly parallel problems\nIdeal cases for parallelization are embarrassingly parallel problems: problems which can be broken down into independent tasks without any work.\nExamples:\n\nLoops for which all iterations are independent of each others.\nResampling (e.g. bootstrapping or cross-validation).\nEnsemble learning (e.g. random forests).\n\nExamples of problems which are not embarrassingly parallel:\n\nLoops for which the result of one iteration is needed for the next iteration.\nRecursive function calls.\nProblems that are inherently sequential.\n\nFor non-embarrassingly parallel problems, one solution is to use C++ to improve speed, as we will see at the end of this course."
  },
  {
    "objectID": "r/hpc_parallelism.html#types-of-parallelism",
    "href": "r/hpc_parallelism.html#types-of-parallelism",
    "title": "Parallelism: concepts",
    "section": "Types of parallelism",
    "text": "Types of parallelism\nThere are various ways to run code in parallel and it is important to have a clear understanding of what each method entails.\n\nMulti-threading\nWe talk about multi-threading when a single process (with its own memory) runs multiple threads.\nThe execution can happen in parallel—if each thread has access to a CPU core—or by alternating some of the threads on some CPU cores.\nBecause all threads in a process write to the same memory addresses, multi-threading can lead to race conditions.\nMulti-threading does not seem to be a common approach to parallelizing R code.\n\n\nMulti-processing in shared memory\nMulti-processing in shared memory happens when multiple processes execute code on multiple CPU cores of a single node (or a single machine).\nThe different processes need to communicate with each other, but because they are all running on the CPU cores of a single node, messages can pass via shared memory.\n\n\nMulti-processing in distributed memory\nWhen processes involved in the execution of some code run on multiple nodes of a cluster, messages between them need to travel over the cluster interconnect. In that case, we talk about distributed memory."
  },
  {
    "objectID": "r/hpc_performance.html",
    "href": "r/hpc_performance.html",
    "title": "Measuring performance:",
    "section": "",
    "text": "Before we talk about ways to improve performance, let’s see how to measure it."
  },
  {
    "objectID": "r/hpc_performance.html#when-should-you-care",
    "href": "r/hpc_performance.html#when-should-you-care",
    "title": "Measuring performance:",
    "section": "When should you care?",
    "text": "When should you care?\n\n“There is no doubt that the grail of efficiency leads to abuse. Programmers waste enormous amounts of time thinking about, or worrying about, the speed of noncritical parts of their programs, and these attempts at efficiency actually have a strong negative impact when debugging and maintenance are considered. We should forget about small efficiencies, say about 97% of the time: premature optimization is the root of all evil. Yet we should not pass up our opportunities in that critical 3%.”\n— Donald Knuth\n\nOptimizing code takes time, can lead to mistakes, and may make code harder to read. Consequently, not all code is worth optimizing and before jumping into optimizations, you need a strategy.\nYou should consider optimizations when:\n\nyou have debugged your code (optimization comes last, don’t optimize a code that doesn’t run),\nyou will run a section of code (e.g. a function) many times (your optimization efforts will really pay off),\na section of code is particularly slow.\n\nHow do you know which sections of your code are slow? Don’t rely on intuition. You need to profile your code to identify bottlenecks."
  },
  {
    "objectID": "r/hpc_performance.html#profiling",
    "href": "r/hpc_performance.html#profiling",
    "title": "Measuring performance:",
    "section": "Profiling",
    "text": "Profiling\n\n“It is often a mistake to make a priori judgments about what parts of a program are really critical, since the universal experience of programmers who have been using measurement tools has been that their intuitive guesses fail.”\n— Donald Knuth\n\n\nBase R profiler\nR comes with a profiler: Rprof.\nThe data gets collected with:\n## Start profiler\nRprof()\n\n&lt;Your code to profile&gt;\n\n## Stop profiler\nRprof(NULL)\nThis creates a Rprof.out file in your working directory (you can give it another name by passing a name into the initial call to Rprof (e.g. Rprof(\"test.out\")).\nThe raw data is dense and is better read by running summaryRprof() (or summaryRprof(\"test.out\") if you have created the file test.out rather than the default).\nAlternatively, you can run R CMD Rprof (or R CMD Rprof test.out if you named your file) from the command line.\nYou can find an example here.\n\n\nPackages\nA number of packages run Rprof under the hood and create flame graphs or provide other utilities to visualize the profiling data:\n\nprofr,\nproftools,\nprofvis built by posit (formerly RStudio Inc) is the newest tool. See here for an example."
  },
  {
    "objectID": "r/hpc_performance.html#benchmarking",
    "href": "r/hpc_performance.html#benchmarking",
    "title": "Measuring performance:",
    "section": "Benchmarking",
    "text": "Benchmarking\nOnce you have identified expressions that are particularly slow, you can use benchmarking tools to compare variations of the code.\nIn the most basic fashion, you can use system.time(), but this is limited and imprecise.\nThe microbenchmark package is a much better option. It gives the minimum time, lower quartile, mean, median, upper quartile, and maximum time of R expressions.\nThe newer bench package is very similar, but it has less overhead, is more accurate, and—for sequential code—gives information on memory usage and garbage collections. This is the package that we will use for this course.\nThe main function from this package is mark(). You can pass as argument(s) one or multiple expressions that you want to benchmark. By default, it ensures that all expressions output the same result. If you want to remove this test, add the argument check = FALSE.\nWhile mark() gives memory usage and garbage collection information for sequential code, this functionality is not yet implemented for parallel code. When benchmarking parallel expressions, we will have to use the argument memory = FALSE.\nYou will see many examples throughout this course."
  },
  {
    "objectID": "r/hpc_resources.html",
    "href": "r/hpc_resources.html",
    "title": "Resources for HPC in R",
    "section": "",
    "text": "This section contains resources specific to high-performance R. For introductory/general R resources, see this page instead.\n\n\nCRAN Task Views\n\nCRAN Task Views give information on packages relevant to certain topics.\n\n\nThe High-Performance and Parallel Computing with R task view lists a lot of packages useful for HPC in R.\n\n\n\nRunning R on the Alliance clusters\n\nThe Alliance wiki contains a lot of documentation on how to run code on the Alliance clusters. Here are pages particularly relevant for HPC in R:\n\n\nGetting started: how to get started using the Alliance supercomputers.\nRunning jobs: how to launch Slurm jobs.\nRunning R: how to use R on the Alliance supercomputers.\nTechnical support: what to do if you are stuck running code on one of the Alliance clusters.\n\nIf you are still having issues after reading the documentation, you can open a ticket by emailing support@tech.alliancecan.ca.\n\n\nOnline books\n\nAdvanced R\nEfficient R programming\n\n\n\nRcpp\n\nDocumentation and examples"
  },
  {
    "objectID": "r/index.html",
    "href": "r/index.html",
    "title": "R",
    "section": "",
    "text": "Getting started with R\nAn introductory course to programming in R\n\n\n\n\nHigh-performance R\nA course on high-performance and parallel R\n\n\n\n\n\n \n\n\n \n\n\n\n\n\nWorkshops\nWorkshops on various R topics\n\n\n\n\nWebinars\n60 min webinars on various R topics"
  },
  {
    "objectID": "r/intro_control_flow.html",
    "href": "r/intro_control_flow.html",
    "title": "Control flow",
    "section": "",
    "text": "Control flow statements alter the linear execution of code, allowing for one or another section of code to be executed, or for one section of code to be executed multiple times."
  },
  {
    "objectID": "r/intro_control_flow.html#conditionals",
    "href": "r/intro_control_flow.html#conditionals",
    "title": "Control flow",
    "section": "Conditionals",
    "text": "Conditionals\nConditionals determine which section of code is to be ran based on predicates. A predicate is a test that returns either TRUE or FALSE.\nHere is an example:\n\ntest_sign &lt;- function(x) {\n  if (x &gt; 0) {\n    \"x is positif\"\n  } else if (x &lt; 0) {\n    \"x is negatif\"\n  } else {\n    \"x is equal to zero\"\n  }\n}\n\ntest_sign() is a function that accepts one argument. Depending on the value of that argument, one of three snippets of code is executed:\n\ntest_sign(3)\n\n[1] \"x is positif\"\n\ntest_sign(-2)\n\n[1] \"x is negatif\"\n\ntest_sign(0)\n\n[1] \"x is equal to zero\""
  },
  {
    "objectID": "r/intro_control_flow.html#loops",
    "href": "r/intro_control_flow.html#loops",
    "title": "Control flow",
    "section": "Loops",
    "text": "Loops\nLoops allow to run the same instruction on various elements:\n\nfor (i in 1:10) {\n  print(i)\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n[1] 7\n[1] 8\n[1] 9\n[1] 10"
  },
  {
    "objectID": "r/intro_functions.html",
    "href": "r/intro_functions.html",
    "title": "Function definition",
    "section": "",
    "text": "R comes with a number of built-in functions. Packages can provide additional ones. In many cases however, you will want to create your own functions to perform exactly the computations that you need.\nIn this section, we will see how to define new functions."
  },
  {
    "objectID": "r/intro_functions.html#syntax",
    "href": "r/intro_functions.html#syntax",
    "title": "Function definition",
    "section": "Syntax",
    "text": "Syntax\nHere is the syntax to define a new function:\nname &lt;- function(arguments) {\n  body\n}"
  },
  {
    "objectID": "r/intro_functions.html#example",
    "href": "r/intro_functions.html#example",
    "title": "Function definition",
    "section": "Example",
    "text": "Example\nLet’s define a function that we call compare which will compare the value between 2 numbers:\n\ncompare &lt;- function(x, y) {\n  x == y\n}\n\n\ncompare is the name of our function.\nx and y are the placeholders for the arguments that our function will accept (our function will need 2 arguments to run successfully).\nx == y is the body of the function, that is, the computation performed by our function.\n\nWe can now use our function:\n\ncompare(2, 3)\n\n[1] FALSE"
  },
  {
    "objectID": "r/intro_functions.html#what-is-returned-by-a-function",
    "href": "r/intro_functions.html#what-is-returned-by-a-function",
    "title": "Function definition",
    "section": "What is returned by a function?",
    "text": "What is returned by a function?\nIn R, the result of the last statement is printed automatically:\n\ntest &lt;- function(x, y) {\n  x\n  y\n}\ntest(2, 3)\n\n[1] 3\n\n\nIf you want to also print other results, you need to explicitly use the print() function:\n\ntest &lt;- function(x, y) {\n  print(x)\n  y\n}\ntest(2, 3)\n\n[1] 2\n\n\n[1] 3\n\n\nNote that, unlike print(), the function return() exits the function:\n\ntest &lt;- function(x, y) {\n  return(x)\n  y\n}\ntest(2, 3)\n\n[1] 2\n\n\n\ntest &lt;- function(x, y) {\n  return(x)\n  return(y)\n}\ntest(2, 3)\n\n[1] 2"
  },
  {
    "objectID": "r/intro_packages.html",
    "href": "r/intro_packages.html",
    "title": "Packages",
    "section": "",
    "text": "Packages are a set of functions, constants, and/or data developed by the community that add functionality to R.\nIn this section, we look at where to find packages and how to install them."
  },
  {
    "objectID": "r/intro_packages.html#looking-for-packages",
    "href": "r/intro_packages.html#looking-for-packages",
    "title": "Packages",
    "section": "Looking for packages",
    "text": "Looking for packages\n\nPackage finder.\nYour peers and the literature.\nList of CRAN packages.\nList of CRAN task views (list of packages with information for a large number of wide topics)."
  },
  {
    "objectID": "r/intro_packages.html#package-documentation",
    "href": "r/intro_packages.html#package-documentation",
    "title": "Packages",
    "section": "Package documentation",
    "text": "Package documentation\n\nSelect a package from the list of CRAN packages.\nGoogle “cran” and the name of your package (e.g. “cran dplyr”).\nLook up a package in the package documentation.\nGet a list of functions within a package with the help() function (installed, but not loaded in session):\n\n\nExample to get a list of functions in the dplyr package:\n\nhelp(package = \"dplyr\")\n\nGet help on a function within a package:\n\nIf you are using RStudio or the HTML format for your R help and you already ran the command to get the list of functions within a package (e.g. help(package = \"dplyr\")), you can get help on any function by clicking on its name.\nIf you are using the text format for help (for instance, if you are running R remotely on the command line), you can get help for any function by adding its name at as the first argument of the previous command.\n\nExample to get help on the function bind() of the package dplyr:\n\nhelp(bind, package = \"dplyr\")\nOf course, if the dplyr package is already loaded in your session, you can simply run help(bind).\n\nGet a list of all help files with alias or concept or title matching a regular expression in all installed packages:\n\n\nExample to get a list of all help files with alias or concept or title matching bind:\n\n??bind\nYou can then open those help files as seen previously.\n\nGet a list of all vignettes for all installed packages:\n\nIf you are using RStudio or the HTML help format:\nbrowseVignettes()\nIf you are using the text help format:\nvignette()\n\nGet a list of vignettes available for a package (not all packages have vignettes):\n\n\nExample to get a list of vignettes for the package dplyr:\n\nIf you are using RStudio or the HTML help format:\nvignette(package = \"dplyr\")\nIf you are using the text help format:\nbrowseVignettes(package = \"dplyr\")\nYou can then open those help vignettes as seen previously."
  },
  {
    "objectID": "r/intro_packages.html#managing-r-packages",
    "href": "r/intro_packages.html#managing-r-packages",
    "title": "Packages",
    "section": "Managing R packages",
    "text": "Managing R packages\nR packages can be installed, updated, and removed from within R:\ninstall.packages(\"&lt;package_name&gt;\", repos=\"&lt;url-cran-mirror&gt;\")\nremove.packages(\"&lt;package-name&gt;\")\nupdate_packages()\n\nrepos argument: chose a CRAN mirror close to the location of your cluster or use https://cloud.r-project.org/.\n\n\nThe first time you install a package, R will ask you whether you want to create a personal library in your home directory. Answer yes to both questions. Your packages will now install under ~/.\n\n\nSome packages require additional modules to be loaded before they can be installed. Other packages need additional R packages as dependencies. In either case, you will get explicit error messages. Adding the argument dependencies = T helps in the second case, but you will still have to add packages manually from time to time."
  },
  {
    "objectID": "r/intro_packages.html#loading-packages",
    "href": "r/intro_packages.html#loading-packages",
    "title": "Packages",
    "section": "Loading packages",
    "text": "Loading packages\nTo make a package available in an R session, you load it with the library() function.\n\nExample:\n\nlibrary(readxl)\nAlternatively, you can access a function from a package without loading it with the syntax: package::function().\n\nExample:\n\nreadxl::read_excel(\"file.xlsx\")"
  },
  {
    "objectID": "r/intro_publishing.html",
    "href": "r/intro_publishing.html",
    "title": "Publishing",
    "section": "",
    "text": "You might have heard of R Markdown: a way to intertwine code and prose in a single scientific document. The company behind R Markdown has now developed its successor: Quarto.\n\nQuarto allows the creation of webpages, websites, presentations, books, pdf, etc. from code in R, Python, or Julia and markdown text.\nIf you are interested in an introduction to this tool, you can have a look at our workshop or our webinar on Quarto."
  },
  {
    "objectID": "r/intro_run.html",
    "href": "r/intro_run.html",
    "title": "Running R",
    "section": "",
    "text": "This section covers the various ways R can be run, then shows you how to access our temporary RStudio server for this course."
  },
  {
    "objectID": "r/intro_run.html#running-r",
    "href": "r/intro_run.html#running-r",
    "title": "Running R",
    "section": "Running R",
    "text": "Running R\nR being an interpreted language, it can be run non-interactively or interactively.\n\nRunning R non-interactively\nIf you write code in a text file (called a script), you can then execute it with:\nRscript my_script.R\n\nThe command to execute scripts is Rscript rather than R.\nBy convention, R scripts take the extension .R.\n\n\n\nRunning R interactively\nThere are several ways to run R interactively.\n\nDirectly in the console (the name for the R shell):\n\n\n\nIn Jupyter with the R kernel (IRkernel package).\nIn another IDE (e.g. in Emacs with ESS).\nIn the RStudio IDE.\n\nThe RStudio IDE is popular and this is what we will use today. RStudio can can be run locally, but for this course, we will use an RStudio server."
  },
  {
    "objectID": "r/intro_run.html#accessing-our-rstudio-server",
    "href": "r/intro_run.html#accessing-our-rstudio-server",
    "title": "Running R",
    "section": "Accessing our RStudio server",
    "text": "Accessing our RStudio server\nYou do not need to install anything on your machine for this course as we will provide access to a temporary RStudio server.\n\nA username, a password, and the URL of the RStudio server will be given to you during the workshop.\n\nSign in using the username and password you will be given while ignoring the OTP entry. This will take you to the server options page of a JupyterHub.\n\nSelect the following server options:\n\nTime: 4 hours\nNumber of cores: 1\nMemory: 3700 MB\nUser interface: JupyterLab\n\n\nThen press “Start” to launch the JupyterHub. There, click on the “RStudio” button and the RStudio server will open in a new tab.\n\nNote that this temporary cluster will only be available for the duration of this course."
  },
  {
    "objectID": "r/intro_run.html#using-rstudio",
    "href": "r/intro_run.html#using-rstudio",
    "title": "Running R",
    "section": "Using RStudio",
    "text": "Using RStudio\nFor those unfamiliar with the RStudio IDE, you can download the following cheatsheet:\n\n\n\n\n\n\n\nfrom Posit Cheatsheets"
  },
  {
    "objectID": "r/intro_why.html",
    "href": "r/intro_why.html",
    "title": "R: why and for whom?",
    "section": "",
    "text": "There are other high level programming languages such as Python or Julia, so when might it make sense for you to turn to R?"
  },
  {
    "objectID": "r/intro_why.html#why-r",
    "href": "r/intro_why.html#why-r",
    "title": "R: why and for whom?",
    "section": "Why R?",
    "text": "Why R?\nHere are a number of reasons why you might want to consider using R:\n\nFree and open source\nUnequalled number of statistics and modelling packages\nIntegrated package manager\nEasy connection with fast compiled languages such as C and C++\nPowerful IDEs available (RStudio, Emacs ESS)"
  },
  {
    "objectID": "r/intro_why.html#for-whom",
    "href": "r/intro_why.html#for-whom",
    "title": "R: why and for whom?",
    "section": "For whom?",
    "text": "For whom?\nFor whom is R particularly well suited?\n\nFields with heavy statistics, modelling, or Bayesian analysis such as biology, linguistics, economics, or statistics\nData science using a lot of tabular data"
  },
  {
    "objectID": "r/intro_why.html#downsides-of-r",
    "href": "r/intro_why.html#downsides-of-r",
    "title": "R: why and for whom?",
    "section": "Downsides of R",
    "text": "Downsides of R\nOf course, R also has its downsides:\n\nInconsistent syntax full of quirks\nSlow\nLarge memory usage"
  },
  {
    "objectID": "r/top_intro.html",
    "href": "r/top_intro.html",
    "title": "Getting started with R",
    "section": "",
    "text": "R is a free and open-source programming language for statistical computing, modelling, and graphics, with an unbeatable collection of statistical packages. It is extremely popular in some academic fields such as statistics, biology, bioinformatics, data mining, data analysis, and linguistics.\nThis introductory course does not assume any prior knowledge.\n\n Start course ➤"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#types-of-spatial-data",
    "href": "r/wb_gis_mapping_slides.html#types-of-spatial-data",
    "title": "GIS mapping with R",
    "section": "Types of spatial data",
    "text": "Types of spatial data\n\nVector data\n. . .\nDiscrete objects\nContain:  - geometry:  shape & location of the objects\n    - attributes:  additional variables (e.g. name, year, type)\nCommon file format:  GeoJSON, shapefile\n\nExamples: countries, roads, rivers, towns\n\n. . .\nRaster data\n. . .\nContinuous phenomena or spatial fields\nCommon file formats:  TIFF, GeoTIFF, NetCDF, Esri grid\n\nExamples: temperature, air quality, elevation, water depth"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#vector-data-1",
    "href": "r/wb_gis_mapping_slides.html#vector-data-1",
    "title": "GIS mapping with R",
    "section": "Vector data",
    "text": "Vector data\nTypes\n\npoint:       single set of coordinates\nmulti-point:   multiple sets of coordinates\npolyline:      multiple sets for which the order matters\nmulti-polyline:  multiple of the above\npolygon:      same as polyline but first & last sets are the same\nmulti-polygon:  multiple of the above"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#raster-data-1",
    "href": "r/wb_gis_mapping_slides.html#raster-data-1",
    "title": "GIS mapping with R",
    "section": "Raster data",
    "text": "Raster data\nGrid of equally sized rectangular cells containing values for some variables\nSize of cells = resolution\nFor computing efficiency, rasters do not have coordinates of each cell, but the bounding box & the number of rows & columns"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#coordinate-reference-systems-crs",
    "href": "r/wb_gis_mapping_slides.html#coordinate-reference-systems-crs",
    "title": "GIS mapping with R",
    "section": "Coordinate Reference Systems (CRS)",
    "text": "Coordinate Reference Systems (CRS)\nA location on Earth’s surface can be identified by its coordinates & some reference system called CRS\nThe coordinates (x, y) are called longitude & latitude\nThere can be a 3rd coordinate (z) for elevation or other measurement—usually a vertical one\nAnd a 4th (m) for some other data attribute—usually a horizontal measurement\nIn 3D, longitude & latitude are expressed in angular units (e.g. degrees) & the reference system needed is an angular CRS or geographic coordinate system (GCS)\nIn 2D, they are expressed in linear units (e.g. meters) & the reference system needed is a planar CRS or projected coordinate system (PCS)"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#datums",
    "href": "r/wb_gis_mapping_slides.html#datums",
    "title": "GIS mapping with R",
    "section": "Datums",
    "text": "Datums\nSince the Earth is not a perfect sphere, we use spheroidal models to represent its surface. Those are called geodetic datums\nSome datums are global, others local (more accurate in a particular area of the globe, but only useful there)\n\nExamples of commonly used global datums:\n\nWGS84 (World Geodesic System 1984)\nNAD83 (North American Datum of 1983)"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#angular-crs",
    "href": "r/wb_gis_mapping_slides.html#angular-crs",
    "title": "GIS mapping with R",
    "section": "Angular CRS",
    "text": "Angular CRS\nAn angular CRS contains a datum, an angular unit & references such as a prime meridian (e.g. the Royal Observatory, Greenwich, England)\nIn an angular CRS or GCS:\n\nLongitude (\\(\\lambda\\)) represents the angle between the prime meridian & the meridian that passes through that location\nLatitude (\\(\\phi\\)) represents the angle between the line that passes through the center of the Earth & that location & its projection on the equatorial plane\n\nLongitude & latitude are thus angular coordinates"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#projections",
    "href": "r/wb_gis_mapping_slides.html#projections",
    "title": "GIS mapping with R",
    "section": "Projections",
    "text": "Projections\nTo create a two-dimensional map, you need to project this 3D angular CRS into a 2D one\nVarious projections offer different characteristics. For instance:\n\nsome respect areas (equal-area)\nsome respect the shape of geographic features (conformal)\nsome almost respect both for small areas\n\nIt is important to choose one with sensible properties for your goals\n\nExamples of projections:\n\nMercator\nUTM\nRobinson"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#planar-crs",
    "href": "r/wb_gis_mapping_slides.html#planar-crs",
    "title": "GIS mapping with R",
    "section": "Planar CRS",
    "text": "Planar CRS\nA planar CRS is defined by a datum, a projection & a set of parameters such as a linear unit & the origins\nCommon planar CRS have been assigned a unique ID called EPSG code which is much more convenient to use\nIn a planar CRS, coordinates will not be in degrees anymore but in meters (or other length unit)"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#projecting-into-a-new-crs",
    "href": "r/wb_gis_mapping_slides.html#projecting-into-a-new-crs",
    "title": "GIS mapping with R",
    "section": "Projecting into a new CRS",
    "text": "Projecting into a new CRS\nYou can change the projection of your data\nVector data won’t suffer any loss of precision, but raster data will\n→  best to try to avoid reprojecting rasters: if you want to combine various datasets which have different projections, reproject vector data instead"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#resources",
    "href": "r/wb_gis_mapping_slides.html#resources",
    "title": "GIS mapping with R",
    "section": "Resources",
    "text": "Resources\nOpen GIS data\nFree GIS Data: list of free GIS datasets\nBooks\nGeocomputation with R by Robin Lovelace, Jakub Nowosad & Jannes Muenchow\nSpatial Data Science by Edzer Pebesma & Roger Bivand\nSpatial Data Science with R by Robert J. Hijmans\nUsing Spatial Data with R by Claudia A. Engel\nTutorial\nAn Introduction to Spatial Data Analysis and Visualisation in R by the CDRC"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#resources-1",
    "href": "r/wb_gis_mapping_slides.html#resources-1",
    "title": "GIS mapping with R",
    "section": "Resources",
    "text": "Resources\nWebsite\nr-spatial by Edzer Pebesma, Marius Appel & Daniel Nüst\nCRAN package list\nAnalysis of Spatial Data\nMailing list\nR Special Interest Group on using Geographical data and Mapping"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#data-manipulation",
    "href": "r/wb_gis_mapping_slides.html#data-manipulation",
    "title": "GIS mapping with R",
    "section": "Data manipulation",
    "text": "Data manipulation\nOlder packages\n\nsp\nraster\nrgdal\nrgeos\n\nNewer generation\n\nsf: vector data\nterra: raster data (also has vector data capabilities)"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#mapping",
    "href": "r/wb_gis_mapping_slides.html#mapping",
    "title": "GIS mapping with R",
    "section": "Mapping",
    "text": "Mapping\nStatic maps\n\nggplot2 + ggspatial\ntmap\n\nDynamic maps\n\nleaflet\nggplot2 + gganimate\nmapview\nggmap\ntmap"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#sf-simple-features-in-r",
    "href": "r/wb_gis_mapping_slides.html#sf-simple-features-in-r",
    "title": "GIS mapping with R",
    "section": "sf: Simple Features in R",
    "text": "sf: Simple Features in R\nGeospatial vectors: points, lines, polygons\nSimple Features—defined by the Open Geospatial Consortium (OGC) & formalized by ISO—is a set of standards now used by most GIS libraries\nWell-known text (WKT) is a markup language for representing vector geometry objects according to those standards\nA compact computer version also exists—well-known binary (WKB)—used by spatial databases\nThe package sp predates Simple Features\nsf—launched in 2016—implements these standards in R in the form of sf objects: data.frames (or tibbles) containing the attributes, extended by sfc objects or simple feature geometries list-columns"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#sf",
    "href": "r/wb_gis_mapping_slides.html#sf",
    "title": "GIS mapping with R",
    "section": "sf",
    "text": "sf\nUseful links\n\nGitHub repo\nPaper\nResources\nCheatsheet\n6 vignettes: 1, 2, 3, 4, 5, 6"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#sf-objects",
    "href": "r/wb_gis_mapping_slides.html#sf-objects",
    "title": "GIS mapping with R",
    "section": "sf objects",
    "text": "sf objects"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#sf-objects-1",
    "href": "r/wb_gis_mapping_slides.html#sf-objects-1",
    "title": "GIS mapping with R",
    "section": "sf objects",
    "text": "sf objects"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#sf-objects-2",
    "href": "r/wb_gis_mapping_slides.html#sf-objects-2",
    "title": "GIS mapping with R",
    "section": "sf objects",
    "text": "sf objects"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#sf-objects-3",
    "href": "r/wb_gis_mapping_slides.html#sf-objects-3",
    "title": "GIS mapping with R",
    "section": "sf objects",
    "text": "sf objects"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#sf-objects-4",
    "href": "r/wb_gis_mapping_slides.html#sf-objects-4",
    "title": "GIS mapping with R",
    "section": "sf objects",
    "text": "sf objects"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#sf-functions",
    "href": "r/wb_gis_mapping_slides.html#sf-functions",
    "title": "GIS mapping with R",
    "section": "sf functions",
    "text": "sf functions\nMost functions start with st_ (which refers to “spatial type”)"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#terra-geospatial-rasters",
    "href": "r/wb_gis_mapping_slides.html#terra-geospatial-rasters",
    "title": "GIS mapping with R",
    "section": "terra: Geospatial rasters",
    "text": "terra: Geospatial rasters\nFaster and simpler replacement for the raster package by the same team\nMostly implemented in C++\nCan work with datasets too large to be loaded into memory"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#terra",
    "href": "r/wb_gis_mapping_slides.html#terra",
    "title": "GIS mapping with R",
    "section": "terra",
    "text": "terra\nUseful links\n\nGitHub repo\nResources\nFull manual"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#tmap-layered-grammar-of-graphics-gis-maps",
    "href": "r/wb_gis_mapping_slides.html#tmap-layered-grammar-of-graphics-gis-maps",
    "title": "GIS mapping with R",
    "section": "tmap: Layered grammar of graphics GIS maps",
    "text": "tmap: Layered grammar of graphics GIS maps"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#tmap",
    "href": "r/wb_gis_mapping_slides.html#tmap",
    "title": "GIS mapping with R",
    "section": "tmap",
    "text": "tmap\nUseful links\n\nGitHub repo\nResources\n\nHelp pages and vignettes\n?tmap-element\nvignette(\"tmap-getstarted\")\n# All the usual help pages, e.g.:\n?tm_layout"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#tmap-functions",
    "href": "r/wb_gis_mapping_slides.html#tmap-functions",
    "title": "GIS mapping with R",
    "section": "tmap functions",
    "text": "tmap functions\nMain functions start with tmap_\nFunctions creating map elements start with tm_"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#tmap-functioning",
    "href": "r/wb_gis_mapping_slides.html#tmap-functioning",
    "title": "GIS mapping with R",
    "section": "tmap functioning",
    "text": "tmap functioning\nVery similar to ggplot2\nTypically, a map contains:\n\nOne or multiple layer(s) (the order matters as they stack on top of each other)\nSome layout (e.g. customization of title, background, margins): tm_layout\nA compass: tm_compass\nA scale bar: tm_scale_bar\n\nEach layer contains:\n\nSome data: tm_shape\nHow that data will be represented: e.g. tm_polygons, tm_lines, tm_raster"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#tmap-example",
    "href": "r/wb_gis_mapping_slides.html#tmap-example",
    "title": "GIS mapping with R",
    "section": "tmap example",
    "text": "tmap example"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#tmap-example-1",
    "href": "r/wb_gis_mapping_slides.html#tmap-example-1",
    "title": "GIS mapping with R",
    "section": "tmap example",
    "text": "tmap example"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#tmap-example-2",
    "href": "r/wb_gis_mapping_slides.html#tmap-example-2",
    "title": "GIS mapping with R",
    "section": "tmap example",
    "text": "tmap example"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#tmap-example-3",
    "href": "r/wb_gis_mapping_slides.html#tmap-example-3",
    "title": "GIS mapping with R",
    "section": "tmap example",
    "text": "tmap example"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#tmap-example-4",
    "href": "r/wb_gis_mapping_slides.html#tmap-example-4",
    "title": "GIS mapping with R",
    "section": "tmap example",
    "text": "tmap example"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#tmap-example-5",
    "href": "r/wb_gis_mapping_slides.html#tmap-example-5",
    "title": "GIS mapping with R",
    "section": "tmap example",
    "text": "tmap example"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#tmap-example-6",
    "href": "r/wb_gis_mapping_slides.html#tmap-example-6",
    "title": "GIS mapping with R",
    "section": "tmap example",
    "text": "tmap example"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#tmap-example-7",
    "href": "r/wb_gis_mapping_slides.html#tmap-example-7",
    "title": "GIS mapping with R",
    "section": "tmap example",
    "text": "tmap example"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#ggplot2-the-standard-in-r-plots",
    "href": "r/wb_gis_mapping_slides.html#ggplot2-the-standard-in-r-plots",
    "title": "GIS mapping with R",
    "section": "ggplot2 (the standard in R plots)",
    "text": "ggplot2 (the standard in R plots)\nUseful links\n\nGitHub repo\nResources\nCheatsheet"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#ggplot2",
    "href": "r/wb_gis_mapping_slides.html#ggplot2",
    "title": "GIS mapping with R",
    "section": "ggplot2",
    "text": "ggplot2\ngeom_sf allows to plot sf objects (i.e. make maps)"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#data",
    "href": "r/wb_gis_mapping_slides.html#data",
    "title": "GIS mapping with R",
    "section": "Data",
    "text": "Data\nFor this webinar, we will use:\n\nthe Alaska as well as the Western Canada & USA subsets of the Randolph Glacier Inventory version 6.01\nthe USGS time series of the named glaciers of Glacier National Park2\nthe Alaska as well as the Western Canada & USA subsets of the consensus estimate for the ice thickness distribution of all glaciers on Earth dataset3\n\nThe datasets can be downloaded as zip files from these websites\nRGI Consortium (2017). Randolph Glacier Inventory – A Dataset of Global Glacier Outlines: Version 6.0: Technical Report, Global Land Ice Measurements from Space, Colorado, USA. Digital Media. DOI: https://doi.org/10.7265/N5-RGI-60.Fagre, D.B., McKeon, L.A., Dick, K.A. & Fountain, A.G., 2017, Glacier margin time series (1966, 1998, 2005, 2015) of the named glaciers of Glacier National Park, MT, USA: U.S. Geological Survey data release. DOI: https://doi.org/10.5066/F7P26WB1.Farinotti, Daniel, 2019, A consensus estimate for the ice thickness distribution of all glaciers on Earth - dataset, Zurich. ETH Zurich. DOI: https://doi.org/10.3929/ethz-b-000315707."
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#packages-1",
    "href": "r/wb_gis_mapping_slides.html#packages-1",
    "title": "GIS mapping with R",
    "section": "Packages",
    "text": "Packages\nPackages need to be installed before they can be loaded in a session\nPackages on CRAN can be installed with:\ninstall.packages(\"&lt;package-name&gt;\")\n basemaps is not on CRAN & needs to be installed from GitHub thanks to devtools:\ninstall.packages(\"devtools\")\ndevtools::install_github(\"16EAGLE/basemaps\")"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#packages-2",
    "href": "r/wb_gis_mapping_slides.html#packages-2",
    "title": "GIS mapping with R",
    "section": "Packages",
    "text": "Packages\nWe load all the packages that we will need at the top of the script:\nlibrary(sf)                 # spatial vector data manipulation\nlibrary(tmap)               # map production & tiled web map\nlibrary(dplyr)              # non GIS specific (tabular data manipulation)\nlibrary(magrittr)           # non GIS specific (pipes)\nlibrary(purrr)              # non GIS specific (functional programming)\nlibrary(rnaturalearth)      # basemap data access functions\nlibrary(rnaturalearthdata)  # basemap data\nlibrary(mapview)            # tiled web map\nlibrary(grid)               # (part of base R) used to create inset map\nlibrary(ggplot2)            # alternative to tmap for map production\nlibrary(ggspatial)          # spatial framework for ggplot2\nlibrary(terra)              # gridded spatial data manipulation\nlibrary(ggmap)              # download basemap data\nlibrary(basemaps)           # download basemap data\nlibrary(magick)             # wrapper around ImageMagick STL\nlibrary(leaflet)            # integrate Leaflet JS in R"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#randolph-glacier-inventory",
    "href": "r/wb_gis_mapping_slides.html#randolph-glacier-inventory",
    "title": "GIS mapping with R",
    "section": "Randolph Glacier Inventory",
    "text": "Randolph Glacier Inventory\nThis dataset contains the contour of all glaciers on Earth\nWe will focus on glaciers in Western North America\nYou can download & unzip 02_rgi60_WesternCanadaUS & 01_rgi60_Alaska from the Randolph Glacier Inventory version 6.0"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#reading-in-data",
    "href": "r/wb_gis_mapping_slides.html#reading-in-data",
    "title": "GIS mapping with R",
    "section": "Reading in data",
    "text": "Reading in data\nData get imported & turned into sf objects with the function sf::st_read:\nak &lt;- st_read(\"data/01_rgi60_Alaska\")\n\nMake sure to use the absolute paths or the paths relative to your working directory (which can be obtained with getwd)"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#reading-in-data-1",
    "href": "r/wb_gis_mapping_slides.html#reading-in-data-1",
    "title": "GIS mapping with R",
    "section": "Reading in data",
    "text": "Reading in data\nak &lt;- st_read(\"data/01_rgi60_Alaska\")\n\n[Out]\n\nReading layer `01_rgi60_Alaska' from data source `./data/01_rgi60_Alaska'\n               using driver `ESRI Shapefile'\nSimple feature collection with 27108 features and 22 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -176.1425 ymin: 52.05727 xmax: -126.8545 ymax: 69.35167\nGeodetic CRS:  WGS 84"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#reading-in-data-2",
    "href": "r/wb_gis_mapping_slides.html#reading-in-data-2",
    "title": "GIS mapping with R",
    "section": "Reading in data",
    "text": "Reading in data\n\n\nYour turn:\n\nRead in the data for the rest of north western America (from 02_rgi60_WesternCanadaUS) and create an sf object called wes"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#first-look-at-the-data",
    "href": "r/wb_gis_mapping_slides.html#first-look-at-the-data",
    "title": "GIS mapping with R",
    "section": "First look at the data",
    "text": "First look at the data\nak\n\n[Out]\n\nSimple feature collection with 27108 features and 22 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -176.1425 ymin: 52.05727 xmax: -126.8545 ymax: 69.35167\nGeodetic CRS:  WGS 84\nFirst 10 features:\n           RGIId        GLIMSId  BgnDate  EndDate    CenLon   CenLat O1Region\n1  RGI60-01.00001 G213177E63689N 20090703 -9999999 -146.8230 63.68900        1\n2  RGI60-01.00002 G213332E63404N 20090703 -9999999 -146.6680 63.40400        1\n3  RGI60-01.00003 G213920E63376N 20090703 -9999999 -146.0800 63.37600        1\n4  RGI60-01.00004 G213880E63381N 20090703 -9999999 -146.1200 63.38100        1\n5  RGI60-01.00005 G212943E63551N 20090703 -9999999 -147.0570 63.55100        1\n6  RGI60-01.00006 G213756E63571N 20090703 -9999999 -146.2440 63.57100        1\n7  RGI60-01.00007 G213771E63551N 20090703 -9999999 -146.2295 63.55085        1\n8  RGI60-01.00008 G213704E63543N 20090703 -9999999 -146.2960 63.54300        1\n9  RGI60-01.00009 G212400E63659N 20090703 -9999999 -147.6000 63.65900        1\n10 RGI60-01.00010 G212830E63513N 20090703 -9999999 -147.1700 63.51300        1\nO2Region   Area Zmin Zmax Zmed Slope Aspect  Lmax Status Connect Form\n1         2  0.360 1936 2725 2385    42    346   839      0       0    0\n2         2  0.558 1713 2144 2005    16    162  1197      0       0    0\n3         2  1.685 1609 2182 1868    18    175  2106      0       0    0\n4         2  3.681 1273 2317 1944    19    195  4175      0       0    0\n5         2  2.573 1494 2317 1914    16    181  2981      0       0    0\n6         2 10.470 1201 3547 1740    22     33 10518      0       0    0\n7         2  0.649 1918 2811 2194    23    151  1818      0       0    0\n8         2  0.200 2826 3555 3195    45     80   613      0       0    0\n9         2  1.517 1750 2514 1977    18    274  2255      0       0    0\n10        2  3.806 1280 1998 1666    17     35  3332      0       0    0\nTermType Surging Linkages Name                       geometry\n1         0       9        9 &lt;NA&gt; POLYGON ((-146.818 63.69081...\n2         0       9        9 &lt;NA&gt; POLYGON ((-146.6635 63.4076...\n3         0       9        9 &lt;NA&gt; POLYGON ((-146.0723 63.3834...\n4         0       9        9 &lt;NA&gt; POLYGON ((-146.149 63.37919...\n5         0       9        9 &lt;NA&gt; POLYGON ((-147.0431 63.5502...\n6         0       9        9 &lt;NA&gt; POLYGON ((-146.2436 63.5562...\n7         0       9        9 &lt;NA&gt; POLYGON ((-146.2495 63.5531...\n8         0       9        9 &lt;NA&gt; POLYGON ((-146.2992 63.5443...\n9         0       9        9 &lt;NA&gt; POLYGON ((-147.6147 63.6643...\n10        0       9        9 &lt;NA&gt; POLYGON ((-147.1494 63.5098..."
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#structure-of-the-data",
    "href": "r/wb_gis_mapping_slides.html#structure-of-the-data",
    "title": "GIS mapping with R",
    "section": "Structure of the data",
    "text": "Structure of the data\nstr(ak)\n\n[Out]\n\nClasses ‘sf’ and 'data.frame':  27108 obs. of  23 variables:\n$ RGIId   : chr  \"RGI60-01.00001\" \"RGI60-01.00002\" \"RGI60-01.00003\" ...\n$ GLIMSId : chr  \"G213177E63689N\" \"G213332E63404N\" \"G213920E63376N\" ...\n$ BgnDate : chr  \"20090703\" \"20090703\" \"20090703\" \"20090703\" ...\n$ EndDate : chr  \"-9999999\" \"-9999999\" \"-9999999\" \"-9999999\" ...\n$ CenLon  : num  -147 -147 -146 -146 -147 ...\n$ CenLat  : num  63.7 63.4 63.4 63.4 63.6 ...\n$ O1Region: chr  \"1\" \"1\" \"1\" \"1\" ...\n$ O2Region: chr  \"2\" \"2\" \"2\" \"2\" ...\n$ Area    : num  0.36 0.558 1.685 3.681 2.573 ...\n$ Zmin    : int  1936 1713 1609 1273 1494 1201 1918 2826 1750 1280 ...\n$ Zmax    : int  2725 2144 2182 2317 2317 3547 2811 3555 2514 1998 ...\n$ Zmed    : int  2385 2005 1868 1944 1914 1740 2194 3195 1977 1666 ...\n$ Slope   : num  42 16 18 19 16 22 23 45 18 17 ...\n$ Aspect  : int  346 162 175 195 181 33 151 80 274 35 ...\n$ Lmax    : int  839 1197 2106 4175 2981 10518 1818 613 2255 3332 ...\n$ Status  : int  0 0 0 0 0 0 0 0 0 0 ...\n$ Connect : int  0 0 0 0 0 0 0 0 0 0 ...\n$ Form    : int  0 0 0 0 0 0 0 0 0 0 ...\n$ TermType: int  0 0 0 0 0 0 0 0 0 0 ...\n$ Surging : int  9 9 9 9 9 9 9 9 9 9 ...\n$ Linkages: int  9 9 9 9 9 9 9 9 9 9 ...\n$ Name    : chr  NA NA NA NA ...\n$ geometry:sfc_POLYGON of length 27108; first list element: List of 1\n..$ : num [1:65, 1:2] -147 -147 -147 -147 -147 ...\n..- attr(*, \"class\")= chr [1:3] \"XY\" \"POLYGON\" \"sfg\"\n- attr(*, \"sf_column\")= chr \"geometry\"\n- attr(*, \"agr\")= Factor w/ 3 levels \"constant\",\"aggregate\",..: NA NA NA ...\n..- attr(*, \"names\")= chr [1:22] \"RGIId\" \"GLIMSId\" \"BgnDate\" \"EndDate\" ..."
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#inspect-your-data",
    "href": "r/wb_gis_mapping_slides.html#inspect-your-data",
    "title": "GIS mapping with R",
    "section": "Inspect your data",
    "text": "Inspect your data\n\n\nYour turn:\n\nInspect the wes object you created."
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#glacier-national-park-dataset",
    "href": "r/wb_gis_mapping_slides.html#glacier-national-park-dataset",
    "title": "GIS mapping with R",
    "section": "Glacier National Park dataset",
    "text": "Glacier National Park dataset\nThis dataset contains a time series of the retreat of 39 glaciers of Glacier National Park, MT, USA\nfor the years 1966, 1998, 2005 & 2015\nYou can download and unzip the 4 sets of files from the USGS website"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#read-in-and-clean-datasets",
    "href": "r/wb_gis_mapping_slides.html#read-in-and-clean-datasets",
    "title": "GIS mapping with R",
    "section": "Read in and clean datasets",
    "text": "Read in and clean datasets\nCreate a function that reads and cleans the data:\nprep &lt;- function(dir) {\n  g &lt;- st_read(dir)\n  g %&lt;&gt;% rename_with(~ tolower(gsub(\"Area....\", \"area\", .x)))\n  g %&lt;&gt;% dplyr::select(\n    year,\n    objectid,\n    glacname,\n    area,\n    shape_leng,\n    x_coord,\n    y_coord,\n    source_sca,\n    source\n  )\n}"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#combine-datasets-into-one-sf-object",
    "href": "r/wb_gis_mapping_slides.html#combine-datasets-into-one-sf-object",
    "title": "GIS mapping with R",
    "section": "Combine datasets into one sf object",
    "text": "Combine datasets into one sf object\nCheck that the CRS are all the same:\nall(sapply(\n  list(st_crs(gnp[[1]]),\n       st_crs(gnp[[2]]),\n       st_crs(gnp[[3]]),\n       st_crs(gnp[[4]])),\n  function(x) x == st_crs(gnp[[1]])\n))\n\n[Out]\n\n[1] TRUE"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#combine-datasets-into-one-sf-object-1",
    "href": "r/wb_gis_mapping_slides.html#combine-datasets-into-one-sf-object-1",
    "title": "GIS mapping with R",
    "section": "Combine datasets into one sf object",
    "text": "Combine datasets into one sf object\nWe can rbind the elements of our list:\ngnp &lt;- do.call(\"rbind\", gnp)\nYou can inspect your new sf object by calling it or with str"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#estimate-for-ice-thickness",
    "href": "r/wb_gis_mapping_slides.html#estimate-for-ice-thickness",
    "title": "GIS mapping with R",
    "section": "Estimate for ice thickness",
    "text": "Estimate for ice thickness\nThis dataset contains an estimate for the ice thickness of all glaciers on Earth\nThe nomenclature follows the Randolph Glacier Inventory\nIce thickness being a spatial field, this is raster data\nWe will use data in RGI60-02.16664_thickness.tif from the ETH Zürich Research Collection which corresponds to one of the glaciers (Agassiz) of Glacier National Park"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#load-raster-data",
    "href": "r/wb_gis_mapping_slides.html#load-raster-data",
    "title": "GIS mapping with R",
    "section": "Load raster data",
    "text": "Load raster data\nRead in data and create a SpatRaster object:\nras &lt;- rast(\"data/RGI60-02/RGI60-02.16664_thickness.tif\")"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#inspect-our-spatraster-object",
    "href": "r/wb_gis_mapping_slides.html#inspect-our-spatraster-object",
    "title": "GIS mapping with R",
    "section": "Inspect our SpatRaster object",
    "text": "Inspect our SpatRaster object\nras\n\n[Out]\n\nclass       : SpatRaster \ndimensions  : 93, 74, 1  (nrow, ncol, nlyr)\nresolution  : 25, 25  (x, y)\nextent      : 707362.5, 709212.5, 5422962, 5425288  (xmin, xmax, ymin, ymax)\ncoord. ref. : +proj=utm +zone=11 +datum=WGS84 +units=m +no_defs \nsource      : RGI60-02.16664_thickness.tif \nname        : RGI60-02.16664_thickness \nnlyr gives us the number of bands (a single one here). You can also run str(ras)"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#our-data",
    "href": "r/wb_gis_mapping_slides.html#our-data",
    "title": "GIS mapping with R",
    "section": "Our data",
    "text": "Our data\nWe now have 3 sf objects & 1 SpatRaster object:\n\nak:  contour of glaciers in AK\nwes:  contour of glaciers in the rest of Western North America\ngnp:  time series of 39 glaciers in Glacier National Park, MT, USA\nras:  ice thickness of the Agassiz Glacier from Glacier National Park"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#lets-map-our-sf-object-ak",
    "href": "r/wb_gis_mapping_slides.html#lets-map-our-sf-object-ak",
    "title": "GIS mapping with R",
    "section": "Let’s map our sf object ak",
    "text": "Let’s map our sf object ak\nAt a bare minimum, we need tm_shape with the data & some info as to how to represent that data:\ntm_shape(ak) +\n  tm_polygons()"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#we-need-to-label-customize-it",
    "href": "r/wb_gis_mapping_slides.html#we-need-to-label-customize-it",
    "title": "GIS mapping with R",
    "section": "We need to label & customize it",
    "text": "We need to label & customize it\ntm_shape(ak) +\n  tm_polygons() +\n  tm_layout(\n    title = \"Glaciers of Alaska\",\n    title.position = c(\"center\", \"top\"),\n    title.size = 1.1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.06, 0.01, 0.09, 0.01),\n    outer.margins = 0,\n    frame.lwd = 0.2\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    size = 1.2,\n    text.size = 0.6\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 500, 1000),\n    position = c(\"right\", \"BOTTOM\")\n  )"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#make-a-map-of-the-wes-object",
    "href": "r/wb_gis_mapping_slides.html#make-a-map-of-the-wes-object",
    "title": "GIS mapping with R",
    "section": "Make a map of the wes object",
    "text": "Make a map of the wes object\n\n\nYour turn:\n\nMake a map with the wes object you created with the data for Western North America excluding AK"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#now-lets-make-a-map-with-ak-wes",
    "href": "r/wb_gis_mapping_slides.html#now-lets-make-a-map-with-ak-wes",
    "title": "GIS mapping with R",
    "section": "Now, let’s make a map with ak & wes",
    "text": "Now, let’s make a map with ak & wes\n\nThe Coordinate Reference Systems (CRS) must be the same\n\n\nsf has a function to retrieve the CRS of an sf object: st_crs\n\n\nst_crs(ak) == st_crs(wes)\n\n[Out]\n\n[1] TRUE\n\n\nSo we’re good (we will see later what to do if this is not the case)"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#our-combined-map",
    "href": "r/wb_gis_mapping_slides.html#our-combined-map",
    "title": "GIS mapping with R",
    "section": "Our combined map",
    "text": "Our combined map\nLet’s start again with a minimum map without any layout to test things out:\ntm_shape(ak) +\n  tm_polygons() +\n  tm_shape(wes) +\n  tm_polygons()\n\n\nUh … oh …"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#what-went-wrong",
    "href": "r/wb_gis_mapping_slides.html#what-went-wrong",
    "title": "GIS mapping with R",
    "section": "What went wrong?",
    "text": "What went wrong?\nMaps are bound by “bounding boxes”. In tmap, they are called bbox\ntmap sets the bbox the first time tm_shape is called. In our case, the bbox was thus set to the bbox of the ak object\nWe need to create a new bbox for our new map"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#retrieving-bounding-boxes",
    "href": "r/wb_gis_mapping_slides.html#retrieving-bounding-boxes",
    "title": "GIS mapping with R",
    "section": "Retrieving bounding boxes",
    "text": "Retrieving bounding boxes\nsf has a function to retrieve the bbox of an sf object: st_bbox\nThe bbox of ak is:\nst_bbox(ak)\n\n[Out]\n\nxmin         ymin       xmax         ymax\n-176.14247   52.05727   -126.85450   69.35167"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#combining-bounding-boxes",
    "href": "r/wb_gis_mapping_slides.html#combining-bounding-boxes",
    "title": "GIS mapping with R",
    "section": "Combining bounding boxes",
    "text": "Combining bounding boxes\nbbox objects can’t be combined directly\nHere is how we can create a new bbox encompassing both of our bboxes:\n\nFirst, we transform our bboxes to sfc objects with st_as_sfc\nThen we combine those objects into a new sfc object with st_union\nFinally, we retrieve the bbox of that object with st_bbox:\n\nnwa_bbox &lt;- st_bbox(\n  st_union(\n    st_as_sfc(st_bbox(wes)),\n    st_as_sfc(st_bbox(ak))\n  )\n)"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#back-to-our-map",
    "href": "r/wb_gis_mapping_slides.html#back-to-our-map",
    "title": "GIS mapping with R",
    "section": "Back to our map",
    "text": "Back to our map\nWe can now use our new bounding box for the map of Western North America:\ntm_shape(ak, bbox = nwa_bbox) +\n  tm_polygons() +\n  tm_shape(wes) +\n  tm_polygons() +\n  tm_layout(\n    title = \"Glaciers of Western North America\",\n    title.position = c(\"center\", \"top\"),\n    title.size = 1.1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.06, 0.01, 0.09, 0.01),\n    outer.margins = 0,\n    frame.lwd = 0.2\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    size = 1.2,\n    text.size = 0.6\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 1000, 2000),\n    position = c(\"right\", \"BOTTOM\")\n  )"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#lets-add-a-basemap",
    "href": "r/wb_gis_mapping_slides.html#lets-add-a-basemap",
    "title": "GIS mapping with R",
    "section": "Let’s add a basemap",
    "text": "Let’s add a basemap\nWe will use data from Natural Earth, a public domain map dataset\nThere are much more fancy options, but they usually involve creating accounts (e.g. with Google) to access some API\nIn addition, this dataset can be accessed direction from within R thanks to the rOpenSci packages:\n\nrnaturalearth: provides the functions\nrnaturalearthdata: provides the data"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#create-an-sf-object-with-statesprovinces",
    "href": "r/wb_gis_mapping_slides.html#create-an-sf-object-with-statesprovinces",
    "title": "GIS mapping with R",
    "section": "Create an sf object with states/provinces",
    "text": "Create an sf object with states/provinces\nstates_all &lt;- ne_states(\n  country = c(\"canada\", \"united states of america\"),\n  returnclass = \"sf\"\n)\n\nne_ stands for “Natural Earth”"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#select-relevant-statesprovinces",
    "href": "r/wb_gis_mapping_slides.html#select-relevant-statesprovinces",
    "title": "GIS mapping with R",
    "section": "Select relevant states/provinces",
    "text": "Select relevant states/provinces\nstates &lt;- states_all %&gt;%\n  filter(name_en == \"Alaska\" |\n           name_en == \"British Columbia\" |\n           name_en == \"Yukon\" |\n           name_en == \"Northwest Territories\" |\n           name_en ==  \"Alberta\" |\n           name_en == \"California\" |\n           name_en == \"Washington\" |\n           name_en == \"Oregon\" |\n           name_en == \"Idaho\" |\n           name_en == \"Montana\" |\n           name_en == \"Wyoming\" |\n           name_en == \"Colorado\" |\n           name_en == \"Nevada\" |\n           name_en == \"Utah\"\n         )"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#add-the-basemap-to-our-map",
    "href": "r/wb_gis_mapping_slides.html#add-the-basemap-to-our-map",
    "title": "GIS mapping with R",
    "section": "Add the basemap to our map",
    "text": "Add the basemap to our map\n\n\nWhat do we need to make sure of first?\n\n\n\nst_crs(states) == st_crs(ak)\n\n[Out]\n\n[1] TRUE"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#add-the-basemap-to-our-map-1",
    "href": "r/wb_gis_mapping_slides.html#add-the-basemap-to-our-map-1",
    "title": "GIS mapping with R",
    "section": "Add the basemap to our map",
    "text": "Add the basemap to our map\nWe add the basemap as a 3rd layer\nMind the order! If you put the basemap last, it will cover your data\nOf course, we will use our nwa_bbox bounding box again\nWe will also break tm_polygons into tm_borders and tm_fill for ak and wes in order to colourise them with slightly different colours"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#add-the-basemap-to-our-map-2",
    "href": "r/wb_gis_mapping_slides.html#add-the-basemap-to-our-map-2",
    "title": "GIS mapping with R",
    "section": "Add the basemap to our map",
    "text": "Add the basemap to our map\ntm_shape(states, bbox = nwa_bbox) +\n  tm_polygons(col = \"#f2f2f2\", lwd = 0.2) +\n  tm_shape(ak) +\n  tm_borders(col = \"#3399ff\") +\n  tm_fill(col = \"#86baff\") +\n  tm_shape(wes) +\n  tm_borders(col = \"#3399ff\") +\n  tm_fill(col = \"#86baff\") +\n  tm_layout(\n    title = \"Glaciers of Western North America\",\n    title.position = c(\"center\", \"top\"),\n    title.size = 1.1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.06, 0.01, 0.09, 0.01),\n    outer.margins = 0,\n    frame.lwd = 0.2\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    size = 1.2,\n    text.size = 0.6\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 1000, 2000),\n    position = c(\"right\", \"BOTTOM\")\n  )"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#tmap-styles",
    "href": "r/wb_gis_mapping_slides.html#tmap-styles",
    "title": "GIS mapping with R",
    "section": "tmap styles",
    "text": "tmap styles\ntmap has a number of styles that you can try\nFor instance, to set the style to “classic”, run the following before making your map:\ntmap_style(\"classic\")\n\nOther options are:\n“white” (default), “gray”, “natural”, “cobalt”, “col_blind”, “albatross”, “beaver”, “bw”, “watercolor”"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#tmap-styles-1",
    "href": "r/wb_gis_mapping_slides.html#tmap-styles-1",
    "title": "GIS mapping with R",
    "section": "tmap styles",
    "text": "tmap styles\nTo return to the default, you need to run\ntmap_style(\"white\")\nor\ntmap_options_reset()\nwhich will reset every tmap option"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#first-lets-map-it",
    "href": "r/wb_gis_mapping_slides.html#first-lets-map-it",
    "title": "GIS mapping with R",
    "section": "First, let’s map it",
    "text": "First, let’s map it\nLet’s use the same tm_borders and tm_fill we just used:\ntm_shape(gnp) +\n  tm_borders(col = \"#3399ff\") +\n  tm_fill(col = \"#86baff\") +\n  tm_layout(\n    title = \"Glaciers of Glacier National Park\",\n    title.position = c(\"center\", \"top\"),\n    legend.title.color = \"#fcfcfc\",\n    legend.text.size = 1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.07, 0.03, 0.07, 0.03),\n    outer.margins = 0\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    text.size = 0.7\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 10, 20),\n    position = c(\"right\", \"BOTTOM\"),\n    text.size = 1\n  )"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#create-an-inset-map",
    "href": "r/wb_gis_mapping_slides.html#create-an-inset-map",
    "title": "GIS mapping with R",
    "section": "Create an inset map",
    "text": "Create an inset map\nAs always, first we check that the CRS are the same:\nst_crs(gnp) == st_crs(ak)\n\n[Out]\n\n[1] FALSE\n\nAH!"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#crs-transformation",
    "href": "r/wb_gis_mapping_slides.html#crs-transformation",
    "title": "GIS mapping with R",
    "section": "CRS transformation",
    "text": "CRS transformation\nWe need to reproject gnp into the CRS of our other sf objects (e.g. ak):\ngnp &lt;- st_transform(gnp, st_crs(ak))\n\nWe can verify that the CRS are now the same:\nst_crs(gnp) == st_crs(ak)\n\n[Out]\n\n[1] TRUE"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#inset-maps-first-step",
    "href": "r/wb_gis_mapping_slides.html#inset-maps-first-step",
    "title": "GIS mapping with R",
    "section": "Inset maps: first step",
    "text": "Inset maps: first step\nAdd a rectangle showing the location of the GNP map in the main North America map\nWe need to create a new sfc object from the gnp bbox so that we can add it to our previous map as a new layer:\ngnp_zone &lt;- st_bbox(gnp) %&gt;%\n  st_as_sfc()"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#inset-maps-second-step",
    "href": "r/wb_gis_mapping_slides.html#inset-maps-second-step",
    "title": "GIS mapping with R",
    "section": "Inset maps: second step",
    "text": "Inset maps: second step\nCreate a tmap object of the main map. Of course, we need to edit the title. Also, note the presence of our new layer:\nmain_map &lt;- tm_shape(states, bbox = nwa_bbox) +\n  tm_polygons(col = \"#f2f2f2\", lwd = 0.2) +\n  tm_shape(ak) +\n  tm_borders(col = \"#3399ff\") +\n  tm_fill(col = \"#86baff\") +\n  tm_shape(wes) +\n  tm_borders(col = \"#3399ff\") +\n  tm_fill(col = \"#86baff\") +\n  tm_shape(gnp_zone) +\n  tm_borders(lwd = 1.5, col = \"#ff9900\") +\n  tm_layout(\n    title = \"Glaciers of Glacier National Park\",\n    title.position = c(\"center\", \"top\"),\n    title.size = 1.1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.06, 0.01, 0.09, 0.01),\n    outer.margins = 0,\n    frame.lwd = 0.2\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    size = 1.2,\n    text.size = 0.6\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 500, 1000),\n    position = c(\"right\", \"BOTTOM\")\n  )"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#inset-maps-third-step",
    "href": "r/wb_gis_mapping_slides.html#inset-maps-third-step",
    "title": "GIS mapping with R",
    "section": "Inset maps: third step",
    "text": "Inset maps: third step\nCreate a tmap object of the inset map\nWe make sure to matching colours & edit the layouts for better readability:\ninset_map &lt;- tm_shape(gnp) +\n  tm_borders(col = \"#3399ff\") +\n  tm_fill(col = \"#86baff\") +\n  tm_layout(\n    legend.show = F,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.03, 0.03, 0.03, 0.03),\n    outer.margins = 0,\n    frame = \"#ff9900\",\n    frame.lwd = 3\n  )"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#inset-maps-final-step",
    "href": "r/wb_gis_mapping_slides.html#inset-maps-final-step",
    "title": "GIS mapping with R",
    "section": "Inset maps: final step",
    "text": "Inset maps: final step\nCombine the two tmap objects\nWe print the main map & add the inset map with grid::viewport:\nmain_map\nprint(inset_map, vp = viewport(0.41, 0.26, width = 0.5, height = 0.5))"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#map-of-the-agassiz-glacier",
    "href": "r/wb_gis_mapping_slides.html#map-of-the-agassiz-glacier",
    "title": "GIS mapping with R",
    "section": "Map of the Agassiz Glacier",
    "text": "Map of the Agassiz Glacier\nSelect the data points corresponding to the Agassiz Glacier:\nag &lt;- gnp %&gt;% filter(glacname == \"Agassiz Glacier\")"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#map-of-the-agassiz-glacier-1",
    "href": "r/wb_gis_mapping_slides.html#map-of-the-agassiz-glacier-1",
    "title": "GIS mapping with R",
    "section": "Map of the Agassiz Glacier",
    "text": "Map of the Agassiz Glacier\ntm_shape(ag) +\n  tm_polygons() +\n  tm_layout(\n    title = \"Agassiz Glacier\",\n    title.position = c(\"center\", \"top\"),\n    legend.position = c(\"left\", \"bottom\"),\n    legend.title.color = \"#fcfcfc\",\n    legend.text.size = 1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.07, 0.03, 0.07, 0.03),\n    outer.margins = 0\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    text.size = 0.7\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 0.5, 1),\n    position = c(\"right\", \"BOTTOM\"),\n    text.size = 1\n  )\n\n\nNot great …"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#map-based-on-attribute-variables",
    "href": "r/wb_gis_mapping_slides.html#map-based-on-attribute-variables",
    "title": "GIS mapping with R",
    "section": "Map based on attribute variables",
    "text": "Map based on attribute variables\ntm_shape(ag) +\n  tm_polygons(\"year\", palette = \"Blues\") +\n  tm_layout(\n    title = \"Agassiz Glacier\",\n    title.position = c(\"center\", \"top\"),\n    legend.position = c(\"left\", \"bottom\"),\n    legend.title.color = \"#fcfcfc\",\n    legend.text.size = 1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.07, 0.03, 0.07, 0.03),\n    outer.margins = 0\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    text.size = 0.7\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 0.5, 1),\n    position = c(\"right\", \"BOTTOM\"),\n    text.size = 1\n  )"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#using-ggplot2-instead-of-tmap",
    "href": "r/wb_gis_mapping_slides.html#using-ggplot2-instead-of-tmap",
    "title": "GIS mapping with R",
    "section": "Using ggplot2 instead of tmap",
    "text": "Using ggplot2 instead of tmap\nAs an alternative to tmap, ggplot2 can plot maps with the geom_sf function:\nggplot(ag) +\n  geom_sf(aes(fill = year)) +\n  scale_fill_brewer(palette = \"Blues\") +\n  labs(title = \"Agassiz Glacier\") +\n  annotation_scale(location = \"bl\", width_hint = 0.4) +\n  annotation_north_arrow(location = \"tr\", which_north = \"true\",\n                         pad_x = unit(0.75, \"in\"), pad_y = unit(0.5, \"in\"),\n                         style = north_arrow_fancy_orienteering) +\n  theme_bw() +\n  theme(plot.title = element_text(hjust = 0.5))\nThe package ggspatial adds a lot of functionality to ggplot2 for spatial data"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#faceted-map-of-the-retreat-of-agassiz",
    "href": "r/wb_gis_mapping_slides.html#faceted-map-of-the-retreat-of-agassiz",
    "title": "GIS mapping with R",
    "section": "Faceted map of the retreat of Agassiz",
    "text": "Faceted map of the retreat of Agassiz\ntm_shape(ag) +\n  tm_polygons(col = \"#86baff\") +\n  tm_layout(\n    main.title = \"Agassiz Glacier\",\n    main.title.position = c(\"center\", \"top\"),\n    main.title.size = 1.2,\n    legend.position = c(\"left\", \"bottom\"),\n    legend.title.color = \"#fcfcfc\",\n    legend.text.size = 1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0, 0.03, 0, 0.03),\n    outer.margins = 0,\n    panel.label.bg.color = \"#fcfcfc\",\n    frame = F,\n    asp = 0.6\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    size = 1,\n    text.size = 0.6\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 0.5, 1),\n    position = c(\"right\", \"BOTTOM\"),\n    text.size = 0.6\n  ) +\n  tm_facets(\n    by = \"year\",\n    free.coords = F,\n    ncol = 4\n  )"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#animated-map-of-the-retreat-of-agassiz",
    "href": "r/wb_gis_mapping_slides.html#animated-map-of-the-retreat-of-agassiz",
    "title": "GIS mapping with R",
    "section": "Animated map of the Retreat of Agassiz",
    "text": "Animated map of the Retreat of Agassiz\nFirst, we need to create a tmap object with facets:\nagassiz_anim &lt;- tm_shape(ag) +\n  tm_polygons(col = \"#86baff\") +\n  tm_layout(\n    title = \"Agassiz Glacier\",\n    title.position = c(\"center\", \"top\"),\n    legend.position = c(\"left\", \"bottom\"),\n    legend.title.color = \"#fcfcfc\",\n    legend.text.size = 1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.08, 0, 0.08, 0),\n    outer.margins = 0,\n    panel.label.bg.color = \"#fcfcfc\"\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    text.size = 0.7\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 0.5, 1),\n    position = c(\"right\", \"BOTTOM\"),\n    text.size = 1\n  ) +\n  tm_facets(\n    along = \"year\",\n    free.coords = F\n  )"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#animated-map-of-the-retreat-of-agassiz-1",
    "href": "r/wb_gis_mapping_slides.html#animated-map-of-the-retreat-of-agassiz-1",
    "title": "GIS mapping with R",
    "section": "Animated map of the Retreat of Agassiz",
    "text": "Animated map of the Retreat of Agassiz\nThen we can pass that object to tmap_animation:\ntmap_animation(\n  agassiz_anim,\n  filename = \"ag.gif\",\n  dpi = 300,\n  inner.margins = c(0.08, 0, 0.08, 0),\n  delay = 100\n)"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#map-of-ice-thickness-of-agassiz",
    "href": "r/wb_gis_mapping_slides.html#map-of-ice-thickness-of-agassiz",
    "title": "GIS mapping with R",
    "section": "Map of ice thickness of Agassiz",
    "text": "Map of ice thickness of Agassiz\nNow, let’s map the estimated ice thickness on Agassiz Glacier\nThis time, we use tm_raster:\ntm_shape(ras) +\n  tm_raster(title = \"\") +\n  tm_layout(\n    title = \"Ice thickness (m) of Agassiz Glacier\",\n    title.position = c(\"center\", \"top\"),\n    legend.position = c(\"left\", \"bottom\"),\n    legend.bg.color = \"#ffffff\",\n    legend.text.size = 1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.07, 0.03, 0.07, 0.03),\n    outer.margins = 0\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    text.size = 0.7\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 0.5, 1),\n    position = c(\"right\", \"BOTTOM\"),\n    text.size = 1\n  )"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#combining-with-randolph-data",
    "href": "r/wb_gis_mapping_slides.html#combining-with-randolph-data",
    "title": "GIS mapping with R",
    "section": "Combining with Randolph data",
    "text": "Combining with Randolph data\nAs always, we check whether the CRS are the same:\nst_crs(ag) == st_crs(ras)\n\n[Out]\n\n[1] FALSE\nWe need to reproject ag (remember that it is best to avoid reprojecting raster data):\nag %&lt;&gt;% st_transform(st_crs(ras))"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#combining-with-randolph-data-1",
    "href": "r/wb_gis_mapping_slides.html#combining-with-randolph-data-1",
    "title": "GIS mapping with R",
    "section": "Combining with Randolph data",
    "text": "Combining with Randolph data\nThe retreat & ice thickness layers will hide each other (the order matters!)\nOne option is to use tm_borders for one of them, but we can also use transparency (alpha)\nWe also adjust the legend:\ntm_shape(ras) +\n  tm_raster(title = \"Ice (m)\") +\n  tm_shape(ag) +\n  tm_polygons(\"year\", palette = \"Blues\", alpha = 0.2, title = \"Contour\") +\n  tm_layout(\n    title = \"Ice thickness (m) and retreat of Agassiz Glacier\",\n    title.position = c(\"center\", \"top\"),\n    legend.position = c(\"left\", \"bottom\"),\n    legend.bg.color = \"#ffffff\",\n    legend.text.size = 0.7,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.07, 0.03, 0.07, 0.03),\n    outer.margins = 0\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    text.size = 0.7\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 0.5, 1),\n    position = c(\"right\", \"BOTTOM\"),\n    text.size = 1\n  )"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#refining-raster-maps",
    "href": "r/wb_gis_mapping_slides.html#refining-raster-maps",
    "title": "GIS mapping with R",
    "section": "Refining raster maps",
    "text": "Refining raster maps\nLet’s go back to our ice thickness map:"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#basemap-with-ggmap",
    "href": "r/wb_gis_mapping_slides.html#basemap-with-ggmap",
    "title": "GIS mapping with R",
    "section": "Basemap with ggmap",
    "text": "Basemap with ggmap\nbasemap &lt;- get_map(\n  bbox = c(\n    left = st_bbox(ag)[1],\n    bottom = st_bbox(ag)[2],\n    right = st_bbox(ag)[3],\n    top = st_bbox(ag)[4]\n  ),\n  source = \"osm\"\n)\n\nggmap is a powerful package, but Google now requires an API key obtained through registration"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#basemap-with-basemaps",
    "href": "r/wb_gis_mapping_slides.html#basemap-with-basemaps",
    "title": "GIS mapping with R",
    "section": "Basemap with basemaps",
    "text": "Basemap with basemaps\nThe package basemaps allows to download open source basemap data from several sources, but those cannot easily be combined with sf objects\nThis plots a satellite image of the Agassiz Glacier:\nbasemap_plot(ag, map_service = \"esri\", map_type = \"world_imagery\")"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#satellite-image-of-the-agassiz-glacier",
    "href": "r/wb_gis_mapping_slides.html#satellite-image-of-the-agassiz-glacier",
    "title": "GIS mapping with R",
    "section": "Satellite image of the Agassiz Glacier",
    "text": "Satellite image of the Agassiz Glacier"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#mapview",
    "href": "r/wb_gis_mapping_slides.html#mapview",
    "title": "GIS mapping with R",
    "section": "mapview",
    "text": "mapview\nmapview(gnp)"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#tmap-1",
    "href": "r/wb_gis_mapping_slides.html#tmap-1",
    "title": "GIS mapping with R",
    "section": "tmap",
    "text": "tmap\nSo far, we have used the plot mode of tmap. There is also a view mode which allows interactive viewing in a browser through Leaflet\nChange to view mode:\ntmap_mode(\"view\")\n\nYou can also toggle between modes with ttm\n\nRe-plot the last map we plotted with tmap:\ntmap_last()"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#leaflet",
    "href": "r/wb_gis_mapping_slides.html#leaflet",
    "title": "GIS mapping with R",
    "section": "leaflet",
    "text": "leaflet\nleaflet creates a map widget to which you add layers\nmap &lt;- leaflet()\naddTiles(map)"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#resources-2",
    "href": "r/wb_gis_mapping_slides.html#resources-2",
    "title": "GIS mapping with R",
    "section": "Resources",
    "text": "Resources\nHere are some resources on the topic to get started.\n\nR companion to Geographic Information Analysis\nSpatial data analysis"
  },
  {
    "objectID": "r/wb_hpc_slides.html#intel-vs-gcc-compilers",
    "href": "r/wb_hpc_slides.html#intel-vs-gcc-compilers",
    "title": "High-performance research computing in R",
    "section": "Intel vs GCC compilers",
    "text": "Intel vs GCC compilers\nTo compile R packages, you need a C compiler.\nIn theory, you could use the proprietary Intel compiler which is loaded by default on the Alliance clusters, but it is recommended to replace it with the GCC compiler (R packages can even be compiled with Clang and LLVM, but the default GCC compiler is the best way to avoid headaches).\nIt is thus much simpler to always load a gcc module before loading an r module."
  },
  {
    "objectID": "r/wb_hpc_slides.html#r-module",
    "href": "r/wb_hpc_slides.html#r-module",
    "title": "High-performance research computing in R",
    "section": "R module",
    "text": "R module\nTo see what versions of R are available on a cluster, run:\nmodule spider r\nTo see the dependencies of a particular version (e.g. r/4.2.1), run:\nmodule spider r/4.2.1\n\nStdEnv/2020 is a required module for this version.\nOn most Alliance clusters, it is automatically loaded, so you don’t need to include it. You can double-check with module list or you can include it (before r/4.2.1) just to be sure.\n\nFinally, load your modules:\nmodule load StdEnv/2020 gcc/11.3.0 r/4.2.1"
  },
  {
    "objectID": "r/wb_hpc_slides.html#scripts",
    "href": "r/wb_hpc_slides.html#scripts",
    "title": "High-performance research computing in R",
    "section": "Scripts",
    "text": "Scripts\nTo run an R script called &lt;your_script&gt;.R, you first need to write a job script:\n\nExample:\n\n\n&lt;your_job&gt;.sh\n\n#!/bin/bash\n#SBATCH --account=def-&lt;your_account&gt;\n#SBATCH --time=15\n#SBATCH --mem-per-cpu=3000M\n#SBATCH --cpus-per-task=4\n#SBATCH --job-name=\"&lt;your_job&gt;\"\nmodule load StdEnv/2020 gcc/11.3.0 r/4.2.1\nRscript &lt;your_script&gt;.R   # Note that R scripts are run with the command `Rscript`\n\n\nThen launch your job with:\nsbatch &lt;your_job&gt;.sh\nYou can monitor your job with sq (an alias for squeue -u $USER $@)."
  },
  {
    "objectID": "r/wb_hpc_slides.html#interactive-jobs",
    "href": "r/wb_hpc_slides.html#interactive-jobs",
    "title": "High-performance research computing in R",
    "section": "Interactive jobs",
    "text": "Interactive jobs\n\nWhile it is fine to run R on the login node when you install packages, you must start a SLURM job before any heavy computation.\n\nTo run R interactively, you should launch an salloc session.\nHere is what I will use for this webinar:\nsalloc --time=1:10:00 --mem-per-cpu=7000M --ntasks=8\nThis takes me to a compute node where I can launch R to run computations:\nR"
  },
  {
    "objectID": "r/wb_hpc_slides.html#profiling",
    "href": "r/wb_hpc_slides.html#profiling",
    "title": "High-performance research computing in R",
    "section": "Profiling",
    "text": "Profiling\nThe first thing to do if you want to improve your code efficiency is to identify bottlenecks in your code. Common tools are:\n\nthe base R function Rprof()\nthe package profvis\n\nprofvis is a newer tool, built by posit (formerly RStudio). Under the hood, it runs Rprof() to collect data, then produces an interactive html widget with a flame graph that allows for an easy visual identification of slow sections of code. While this tool integrates well within the RStudio IDE or the RPubs ecosystem, it is not very well suited for remote work on a cluster. One option is to profile your code with small data on your own machine. Another option is to use the base profiler with Rprof() directly as in this example."
  },
  {
    "objectID": "r/wb_hpc_slides.html#benchmarking",
    "href": "r/wb_hpc_slides.html#benchmarking",
    "title": "High-performance research computing in R",
    "section": "Benchmarking",
    "text": "Benchmarking\nOnce you have identified expressions that are particularly slow, you can use benchmarking tools to compare variations of the code.\nIn the most basic fashion, you can use system.time(), but this is limited and imprecise.\nThe microbenchmark package is a popular option.\nIt gives the minimum time, lower quartile, mean, median, upper quartile, and maximum time of R expressions.\nThe newer bench package has less overhead, is more accurate, and—for sequential code—gives information on memory usage and garbage collections. This is the package I will use today."
  },
  {
    "objectID": "r/wb_hpc_slides.html#multi-threading",
    "href": "r/wb_hpc_slides.html#multi-threading",
    "title": "High-performance research computing in R",
    "section": "Multi-threading",
    "text": "Multi-threading\nWe talk about multi-threading when a single process (with its own memory) runs multiple threads.\nThe execution can happen in parallel—if each thread has access to a CPU core—or by alternating some of the threads on some CPU cores.\nBecause all threads in a process write to the same memory addresses, multi-threading can lead to race conditions.\nMulti-threading does not seem to be a common approach to parallelizing R code."
  },
  {
    "objectID": "r/wb_hpc_slides.html#multi-processing-in-shared-memory",
    "href": "r/wb_hpc_slides.html#multi-processing-in-shared-memory",
    "title": "High-performance research computing in R",
    "section": "Multi-processing in shared memory",
    "text": "Multi-processing in shared memory\nMulti-processing in shared memory happens when multiple processes execute code on multiple CPU cores of a single node (or a single machine).\nThe different processes need to communicate with each other, but because they are all running on the CPU cores of a single node, messages can pass via shared memory."
  },
  {
    "objectID": "r/wb_hpc_slides.html#multi-processing-in-distributed-memory",
    "href": "r/wb_hpc_slides.html#multi-processing-in-distributed-memory",
    "title": "High-performance research computing in R",
    "section": "Multi-processing in distributed memory",
    "text": "Multi-processing in distributed memory\nWhen processes involved in the execution of some code run on multiple nodes of a cluster, messages between them need to travel over the cluster interconnect. In that case, we talk about distributed memory."
  },
  {
    "objectID": "r/wb_hpc_slides.html#package-parallel-base-r",
    "href": "r/wb_hpc_slides.html#package-parallel-base-r",
    "title": "High-performance research computing in R",
    "section": "Package parallel (base R)",
    "text": "Package parallel (base R)\nThe parallel package has been part of the “base” package group since version 2.14.0.\nThis means that it is comes with R.\nMost parallel approaches in R build on this package.\nWe will make use of it to create and close an ad-hoc cluster.\n\nThe parallelly package adds functionality to the parallel package."
  },
  {
    "objectID": "r/wb_hpc_slides.html#package-foreach",
    "href": "r/wb_hpc_slides.html#package-foreach",
    "title": "High-performance research computing in R",
    "section": "Package foreach",
    "text": "Package foreach\nThe foreach package implements a looping construct without an explicit counter. It doesn’t require the preallocation of an output container, it brings to R an equivalent of the Python or Julia list comprehensions, and mostly, it allows for an easy execution of loops in parallel. Unlike loops, it creates variables (loops are used for their side-effect).\nLet’s look at an example to calculate the sum of 1e4 random vectors of length 3.\nWe will use foreach and iterators (which creates convenient iterators for foreach):\n\nlibrary(foreach)\nlibrary(iterators)"
  },
  {
    "objectID": "r/wb_hpc_slides.html#package-future",
    "href": "r/wb_hpc_slides.html#package-future",
    "title": "High-performance research computing in R",
    "section": "Package future",
    "text": "Package future\nA future is an object that acts as an abstract representation for a value in the future. A future can be resolved (if the value has been computed) or unresolved. If the value is queried while the future is unresolved, the process is blocked until the future is resolved.\nFutures allow for asynchronous and parallel evaluations. The future package provides a simple and unified API to evaluate futures."
  },
  {
    "objectID": "r/wb_hpc_slides.html#plans",
    "href": "r/wb_hpc_slides.html#plans",
    "title": "High-performance research computing in R",
    "section": "Plans",
    "text": "Plans\nThe future package does this thanks to the plan function:\n\nplan(sequential): futures are evaluated sequentially in the current R session\nplan(multisession): futures are evaluated by new R sessions spawned in the background (multi-processing in shared memory)\nplan(multicore): futures are evaluated in processes forked from the existing process (multi-processing in shared memory)\nplan(cluster): futures are evaluated on an ad-hoc cluster (allows for distributed parallelism across multiple nodes)"
  },
  {
    "objectID": "r/wb_hpc_slides.html#consistency",
    "href": "r/wb_hpc_slides.html#consistency",
    "title": "High-performance research computing in R",
    "section": "Consistency",
    "text": "Consistency\nTo ensure a consistent behaviour across plans, all evaluations are done in a local environment:\n\nlibrary(future)\n\na &lt;- 1\n\nb %&lt;-% {\n  a &lt;- 2\n}\n\na\n\n[1] 1"
  },
  {
    "objectID": "r/wb_hpc_slides.html#lets-return-to-our-example",
    "href": "r/wb_hpc_slides.html#lets-return-to-our-example",
    "title": "High-performance research computing in R",
    "section": "Let’s return to our example",
    "text": "Let’s return to our example\nWe had:\nset.seed(2)\nresult2 &lt;- foreach(icount(1e4), .combine = '+') %do% runif(3)\nWe can replace %do% with %dopar%:\nset.seed(2)\nresult3 &lt;- foreach(icount(1e4), .combine = '+') %dopar% runif(3)\nSince we haven’t registered any parallel backend, the expression will still be evaluated sequentially."
  },
  {
    "objectID": "r/wb_hpc_slides.html#load-packages",
    "href": "r/wb_hpc_slides.html#load-packages",
    "title": "High-performance research computing in R",
    "section": "Load packages",
    "text": "Load packages\nFor this toy example, I will use a modified version of one of the examples in the foreach vignette: I will b uild a classification model made of a forest of decision trees thanks to the randomForest package.\nBecause the code includes randomly generated numbers, I will use the doRNG package which replaces foreach::%dopar% wit h doRNG::%dorng%. This follows the recommendations of Pierre L’Ecuyer (1999)1 and ensures reproducibility.\nlibrary(doFuture)       # This will also load the `future` package\nlibrary(doRNG)          # This will also load the `foreach` package\nlibrary(randomForest)\nlibrary(bench)          # To do some benchmarking\nLoading required package: foreach\nLoading required package: future\nLoading required package: rngtools\nL’Ecuyer, P. (1999). Good parameters and implementations for combined multiple recursive random number generators. Operations Research, 47, 159–164."
  },
  {
    "objectID": "r/wb_hpc_slides.html#the-code-to-parallelize",
    "href": "r/wb_hpc_slides.html#the-code-to-parallelize",
    "title": "High-performance research computing in R",
    "section": "The code to parallelize",
    "text": "The code to parallelize\nThe goal is to create a classifier based on some data (here a matrix of random numbers for simplicity) and a response variable (as factor). This model could then be passed in the predict() function with novel data to generate predictions of classification. But here we are only interested in the creation of the model as this is the part that is computationally intensive. We aren’t interested in actually using it.\nset.seed(11)\ntraindata &lt;- matrix(runif(1e5), 100)\nfac &lt;- gl(2, 50)\n\nrf &lt;- foreach(ntree = rep(250, 8), .combine = combine) %do%\n  randomForest(x = traindata, y = fac, ntree = ntree)\n\nrf\nCall:\n randomForest(x = traindata, y = fac, ntree = ntree)\n               Type of random forest: classification\n                     Number of trees: 2000\nNo. of variables tried at each split: 31"
  },
  {
    "objectID": "r/wb_hpc_slides.html#reference-timing",
    "href": "r/wb_hpc_slides.html#reference-timing",
    "title": "High-performance research computing in R",
    "section": "Reference timing",
    "text": "Reference timing\nThis is the non parallelizable code with %do%:\ntref &lt;- mark(\n  rf1 &lt;- foreach(ntree = rep(250, 8), .combine = combine) %do%\n    randomForest(x = traindata, y = fac, ntree = ntree),\n  memory = FALSE\n)\n\ntref$median\n[1] 5.66s"
  },
  {
    "objectID": "r/wb_hpc_slides.html#plan-sequential",
    "href": "r/wb_hpc_slides.html#plan-sequential",
    "title": "High-performance research computing in R",
    "section": "Plan sequential",
    "text": "Plan sequential\nThis is the parallelizable foreach code, but run sequentially:\nregisterDoFuture()   # Set the parallel backend\nplan(sequential)     # Set the evaluation strategy\n\n# Using bench::mark()\ntseq &lt;- mark(\n  rf2 &lt;- foreach(ntree = rep(250, 8), .combine = combine) %dorng%\n    randomForest(x = traindata, y = fac, ntree = ntree),\n  memory = FALSE\n)\n\ntseq$median\n[1] 5.78s\n\nNo surprise: those are similar."
  },
  {
    "objectID": "r/wb_hpc_slides.html#multi-processing-in-shared-memory-1",
    "href": "r/wb_hpc_slides.html#multi-processing-in-shared-memory-1",
    "title": "High-performance research computing in R",
    "section": "Multi-processing in shared memory",
    "text": "Multi-processing in shared memory\nfuture provides availableCores() to detect the number of available cores:\navailableCores()\nsystem\n     4\n\nSimilar to parallel::detectCores().\n\nThis detects the number of CPU cores available to me on the current compute node, that is, what I can use for shared memory multi-processing."
  },
  {
    "objectID": "r/wb_hpc_slides.html#plan-multisession",
    "href": "r/wb_hpc_slides.html#plan-multisession",
    "title": "High-performance research computing in R",
    "section": "Plan multisession",
    "text": "Plan multisession\nShared memory multi-processing can be run with plan(multisession) that will spawn new R sessions in the background to evaluate futures:\nplan(multisession)\n\ntms &lt;- mark(\n  rf2 &lt;- foreach(ntree = rep(250, 8), .combine = combine) %dorng%\n    randomForest(x = traindata, y = fac, ntree = ntree),\n  memory = FALSE\n)\n\ntms$median\n[1] 2s\n\nWe got a speedup of 5.78 / 2 = 2.9."
  },
  {
    "objectID": "r/wb_hpc_slides.html#plan-multicore",
    "href": "r/wb_hpc_slides.html#plan-multicore",
    "title": "High-performance research computing in R",
    "section": "Plan multicore",
    "text": "Plan multicore\nShared memory multi-processing can also be run with plan(multicore) (except on Windows) that will fork the current R process to evaluate futures:\nplan(multicore)\n\ntmc &lt;- mark(\n  rf2 &lt;- foreach(ntree = rep(250, 8), .combine = combine) %dorng%\n    randomForest(x = traindata, y = fac, ntree = ntree),\n  memory = FALSE\n)\n\ntmc$median\n[1] 1.9s\n\nWe got a very similar speedup of 5.78 / 1.9 = 3.0."
  },
  {
    "objectID": "r/wb_hpc_slides.html#multi-processing-in-distributed-memory-1",
    "href": "r/wb_hpc_slides.html#multi-processing-in-distributed-memory-1",
    "title": "High-performance research computing in R",
    "section": "Multi-processing in distributed memory",
    "text": "Multi-processing in distributed memory\nI requested 8 tasks from Slurm on a training cluster made of nodes with 4 CPU cores each. Let’s verify that I got them by accessing the SLURM_NTASKS environment variable from within R:\nas.numeric(Sys.getenv(\"SLURM_NTASKS\"))\n[1] 8\nI can create a character vector with the name of the node each task is running on:\n(hosts &lt;- system(\"srun hostname | cut -f 1 -d '.'\", intern = TRUE))\nchr [1:8] \"node1\" \"node1\" \"node1\" \"node1\" \"node2\" \"node2\" \"node2\" \"node2\"\nThis allows me to create a cluster of workers:\n(cl &lt;- parallel::makeCluster(hosts))      # Defaults to type=\"PSOCK\"\nsocket cluster with 8 nodes on hosts ‘node1’, ‘node2’"
  },
  {
    "objectID": "r/wb_hpc_slides.html#plan-cluster",
    "href": "r/wb_hpc_slides.html#plan-cluster",
    "title": "High-performance research computing in R",
    "section": "Plan cluster",
    "text": "Plan cluster\nI can now try the code with distributed parallelism using all 8 CPU cores across both nodes:\nplan(cluster, workers = cl)\n\ntdis &lt;- mark(\n  rf2 &lt;- foreach(ntree = rep(250, 8), .combine = combine) %dorng%\n    randomForest(x = traindata, y = fac, ntree = ntree),\n  memory = FALSE\n)\n\ntdis$median\n[1] 1.14s\n\nSpeedup: 5.78 / 1.14 = 5.1.\n\nThe cluster of workers can be stopped with:\nparallel::stopCluster(cl)"
  },
  {
    "objectID": "r/wb_hpc_slides.html#alternative-approaches",
    "href": "r/wb_hpc_slides.html#alternative-approaches",
    "title": "High-performance research computing in R",
    "section": "Alternative approaches",
    "text": "Alternative approaches\nThe multidplyr package partitions data frames across worker processes, allows you to run the usual tidyverse functions on each partition, then collects the processed data.\nThe furrr package is a parallel equivalent to the purrr package from the tidyverse.\nIf you work with genomic data, you might want to have a look at the BiocParallel package from Bioconductor.\nYet another option to run distributed R code is to use the sparklyr package (an R interface to Spark).\nRmpi is a wrapper to MPI (Message-Passing Interface). It has proved slow and problematic on Cedar though.\nThe boot package provides functions and datasets specifically for bootstrapping in parallel."
  },
  {
    "objectID": "r/ws_hss_intro.html",
    "href": "r/ws_hss_intro.html",
    "title": "Introduction to R for the humanities",
    "section": "",
    "text": "R is a free and open-source programming language for statistical computing, modelling, and graphics, with an unbeatable collection of statistical packages. It is extremely popular in some academic fields such as statistics, biology, bioinformatics, data mining, data analysis, and linguistics.\nThis introductory course does not assume any prior knowledge."
  },
  {
    "objectID": "r/ws_hss_intro.html#running-r",
    "href": "r/ws_hss_intro.html#running-r",
    "title": "Introduction to R for the humanities",
    "section": "Running R",
    "text": "Running R\nR being an interpreted language, it can be run non-interactively or interactively.\n\nRunning R non-interactively\nIf you write code in a text file (called a script), you can then execute it with:\nRscript my_script.R\n\nThe command to execute scripts is Rscript rather than R.\nBy convention, R scripts take the extension .R.\n\n\n\nRunning R interactively\nThere are several ways to run R interactively.\n\nDirectly in the console (the name for the R shell):\n\n\n\nIn Jupyter with the R kernel (IRkernel package).\nIn another IDE (e.g. in Emacs with ESS).\nIn the RStudio IDE.\n\nThe RStudio IDE is popular and this is what we will use today. RStudio can can be run locally, but for this course, we will use an RStudio server.\n\n\nAccessing our RStudio server\nFor this workshop, we will use a temporary RStudio server.\nTo access it, go to the website given during the workshop and sign in using the username and password you will be given (you can ignore the OTP entry).\nThis will take you to our JupyterHub. There, click on the “RStudio” button and our RStudio server will open in a new tab.\n\n\nUsing RStudio\nFor those unfamiliar with the RStudio IDE, you can download the following cheatsheet:\n\n\n\n\n\n\n\nfrom Posit Cheatsheets"
  },
  {
    "objectID": "r/ws_hss_intro.html#help-and-documentation",
    "href": "r/ws_hss_intro.html#help-and-documentation",
    "title": "Introduction to R for the humanities",
    "section": "Help and documentation",
    "text": "Help and documentation\nFor some general documentation on R, you can run:\nhelp.start()\nTo get help on a function (e.g. sum), you can run:\nhelp(sum)\nDepending on your settings, this will open a documentation for sum in a pager or in your browser."
  },
  {
    "objectID": "r/ws_hss_intro.html#basic-syntax",
    "href": "r/ws_hss_intro.html#basic-syntax",
    "title": "Introduction to R for the humanities",
    "section": "Basic syntax",
    "text": "Basic syntax\n\nAssignment\nR can accept the equal sign (=) for assignments, but it is more idiomatic to use the assignment sign (&lt;-) whenever you bind a name to a value and to use the equal sign everywhere else.\n\na &lt;- 3\n\nOnce you have bound a name to a value, you can recall the value with that name:\n\na  # Note that you do not need to use a print() function in R\n\n[1] 3\n\n\nYou can remove an object from the environment by deleting its name:\n\nrm(a)\na\n\nError in eval(expr, envir, enclos): object 'a' not found\n\n\nThe garbage collector will take care of deleting the object itself from memory.\n\n\nComments\nAnything to the left of # is a comment and is ignored by R:\n\n# This is an inline comment\n\na &lt;- 3  # This is also a comment"
  },
  {
    "objectID": "r/ws_hss_intro.html#data-types-and-structures",
    "href": "r/ws_hss_intro.html#data-types-and-structures",
    "title": "Introduction to R for the humanities",
    "section": "Data types and structures",
    "text": "Data types and structures\n\n\n\nDimension\nHomogeneous\nHeterogeneous\n\n\n\n\n1 d\nAtomic vector\nList\n\n\n2 d\nMatrix\nData frame\n\n\n3 d\nArray\n\n\n\n\n\nAtomic vectors\n\nvec &lt;- c(2, 4, 1)\nvec\n\n[1] 2 4 1\n\ntypeof(vec)\n\n[1] \"double\"\n\nstr(vec)\n\n num [1:3] 2 4 1\n\n\n\nvec &lt;- c(TRUE, TRUE, NA, FALSE)\nvec\n\n[1]  TRUE  TRUE    NA FALSE\n\ntypeof(vec)\n\n[1] \"logical\"\n\nstr(vec)\n\n logi [1:4] TRUE TRUE NA FALSE\n\n\n\nNA (“Not Available”) is a logical constant of length one. It is an indicator for a missing value.\n\nVectors are homogeneous, so all elements need to be of the same type.\nIf you use elements of different types, R will convert some of them to ensure that they become of the same type:\n\nvec &lt;- c(\"This is a string\", 3, \"test\")\nvec\n\n[1] \"This is a string\" \"3\"                \"test\"            \n\ntypeof(vec)\n\n[1] \"character\"\n\nstr(vec)\n\n chr [1:3] \"This is a string\" \"3\" \"test\"\n\n\n\nvec &lt;- c(TRUE, 3, FALSE)\nvec\n\n[1] 1 3 0\n\ntypeof(vec)\n\n[1] \"double\"\n\nstr(vec)\n\n num [1:3] 1 3 0\n\n\n\n\nData frames\nData frames contain tabular data. Under the hood, a data frame is a list of vectors.\n\ndat &lt;- data.frame(\n  country = c(\"Canada\", \"USA\", \"Mexico\"),\n  var = c(2.9, 3.1, 4.5)\n)\ndat\n\n  country var\n1  Canada 2.9\n2     USA 3.1\n3  Mexico 4.5\n\ntypeof(dat)\n\n[1] \"list\"\n\nstr(dat)\n\n'data.frame':   3 obs. of  2 variables:\n $ country: chr  \"Canada\" \"USA\" \"Mexico\"\n $ var    : num  2.9 3.1 4.5\n\nlength(dat)\n\n[1] 2\n\ndim(dat)\n\n[1] 3 2"
  },
  {
    "objectID": "r/ws_hss_intro.html#function-definition",
    "href": "r/ws_hss_intro.html#function-definition",
    "title": "Introduction to R for the humanities",
    "section": "Function definition",
    "text": "Function definition\n\ncompare &lt;- function(x, y) {\n  x == y\n}\n\nWe can now use our function:\n\ncompare(2, 3)\n\n[1] FALSE\n\n\nNote that the result of the last statement is printed automatically:\n\ntest &lt;- function(x, y) {\n  x\n  y\n}\ntest(2, 3)\n\n[1] 3\n\n\nIf you want to return other results, you need to explicitly use the print() function:\n\ntest &lt;- function(x, y) {\n  print(x)\n  y\n}\ntest(2, 3)\n\n[1] 2\n\n\n[1] 3"
  },
  {
    "objectID": "r/ws_hss_intro.html#control-flow",
    "href": "r/ws_hss_intro.html#control-flow",
    "title": "Introduction to R for the humanities",
    "section": "Control flow",
    "text": "Control flow\n\nConditionals\n\ntest_sign &lt;- function(x) {\n  if (x &gt; 0) {\n    \"x is positif\"\n  } else if (x &lt; 0) {\n    \"x is negatif\"\n  } else {\n    \"x is equal to zero\"\n  }\n}\n\n\ntest_sign(3)\n\n[1] \"x is positif\"\n\ntest_sign(-2)\n\n[1] \"x is negatif\"\n\ntest_sign(0)\n\n[1] \"x is equal to zero\"\n\n\n\n\nLoops\n\nfor (i in 1:10) {\n  print(i)\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n[1] 7\n[1] 8\n[1] 9\n[1] 10\n\n\nNotice that here we need to use the print() function."
  },
  {
    "objectID": "r/ws_hss_intro.html#packages",
    "href": "r/ws_hss_intro.html#packages",
    "title": "Introduction to R for the humanities",
    "section": "Packages",
    "text": "Packages\nPackages are a set of functions and/or data that add functionality to R.\n\nLooking for packages\n\nPackage finder\nYour peers and the literature\n\n\n\nPackage documentation\n\nList of CRAN packages\nPackage documentation\n\n\n\nManaging R packages\nR packages can be installed, updated, and removed from within R:\ninstall.packages(\"package-name\")\nremove.packages(\"package-name\")\nupdate_packages()\n\n\nLoading packages\nTo make a package available in an R session, you load it with the library() function.\n\nExample:\n\nlibrary(readxl)\nAlternatively, you can access a function from a package without loading it with the syntax: package::function().\n\nExample:\n\nreadxl::read_excel(\"file.xlsx\")"
  },
  {
    "objectID": "r/ws_hss_intro.html#publishing",
    "href": "r/ws_hss_intro.html#publishing",
    "title": "Introduction to R for the humanities",
    "section": "Publishing",
    "text": "Publishing\nYou might have heard of R Markdown. It allows for the creation of dynamic publication-quality documents mixing code blocks, text, graphs…\nThe team which created R Markdown has now created an even better tool: Quarto. If you are interested in an introduction to this tool, you can have a look at our workshop or our webinar on Quarto."
  },
  {
    "objectID": "r/ws_hss_intro.html#resources",
    "href": "r/ws_hss_intro.html#resources",
    "title": "Introduction to R for the humanities",
    "section": "Resources",
    "text": "Resources\n\nAlliance wiki\n\nR page\n\n\n\nR main site\n\nDownload page\n\n\n\nRStudio\n\nPosit site (Posit is the brand new name of the RStudio company)\nPosit cheatsheets\n\n\n\nSoftware Carpentry online workshop\n\nData analysis using R in the digital humanities\n\n\n\nOnline book\n\nR for Data Science (heavily based on the tidyverse)"
  },
  {
    "objectID": "r/ws_hss_intro.html#recording",
    "href": "r/ws_hss_intro.html#recording",
    "title": "Introduction to R for the humanities",
    "section": "Recording",
    "text": "Recording\n\nVideos of this workshop for the Digital Research Alliance of Canada HSS Winter Series 2023:\n\n\nFirst part\n\n\n\nSecond part"
  },
  {
    "objectID": "talks/2023_driconnect.html",
    "href": "talks/2023_driconnect.html",
    "title": "The instruments for advanced research computing are here, but are researchers ready?",
    "section": "",
    "text": "The current times are exciting: we are witnessing a growth of computing power while the open source community is vigorously building impressive machine learning and scientific programming tools.\nThis boom of hardware and software assets cannot however translate into research if graduate students aren’t able to take advantage of it. Curricula often lack training pertinent to the use of such resources. Worse yet, in many fields faculties and PIs don’t have the necessary background to help their students with high-performance programming. The training team at Simon Fraser University Research Computing Group aims to fill this gap in the West on behalf of the Alliance and all Western Canadian universities.\nThis talk will present an overview of the training we provide, from introductory skill sets for researchers new to ARC and HPC to advanced topics in parallel programming.\n\nSlides"
  },
  {
    "objectID": "talks/index.html",
    "href": "talks/index.html",
    "title": "The instruments for advanced research computing are here, but are researchers ready?",
    "section": "",
    "text": "The current times are exciting: we are witnessing a growth of computing power while the open source community is vigorously building impressive machine learning and scientific programming tools.\nThis boom of hardware and software assets cannot however translate into research if graduate students aren’t able to take advantage of it. Curricula often lack training pertinent to the use of such resources. Worse yet, in many fields faculties and PIs don’t have the necessary background to help their students with high-performance programming. The training team at Simon Fraser University Research Computing Group aims to fill this gap in the West on behalf of the Alliance and all Western Canadian universities.\nThis talk will present an overview of the training we provide, from introductory skill sets for researchers new to ARC and HPC to advanced topics in parallel programming.\n\nSlides"
  },
  {
    "objectID": "tools/emacs_prog_ide_slides.html#helm",
    "href": "tools/emacs_prog_ide_slides.html#helm",
    "title": "Emacs as a programming IDE",
    "section": "Helm",
    "text": "Helm\nSearching in buffer\n\nNavigating open buffers and recent files\n\n\nNavigating file sections\n\n\nSelecting from kill ring\n\n\nMoving in mark ring\n\n\nLooking at active modes"
  },
  {
    "objectID": "tools/emacs_prog_ide_slides.html#completion",
    "href": "tools/emacs_prog_ide_slides.html#completion",
    "title": "Emacs as a programming IDE",
    "section": "Completion",
    "text": "Completion\ncompany-mode\n\nyasnippet\n\n\nDynamic abbrev expansion"
  },
  {
    "objectID": "tools/emacs_prog_ide_slides.html#undoingredoing-with-undo-tree",
    "href": "tools/emacs_prog_ide_slides.html#undoingredoing-with-undo-tree",
    "title": "Emacs as a programming IDE",
    "section": "Undoing/redoing with undo-tree",
    "text": "Undoing/redoing with undo-tree"
  },
  {
    "objectID": "tools/help_slides.html#when-you-are-stuck-1",
    "href": "tools/help_slides.html#when-you-are-stuck-1",
    "title": "So, you are stuck … now what?",
    "section": "When you are stuck",
    "text": "When you are stuck\n\nFirst, look for information that is already out there\n\n\nThen, ask for help"
  },
  {
    "objectID": "tools/help_slides.html#look-for-information",
    "href": "tools/help_slides.html#look-for-information",
    "title": "So, you are stuck … now what?",
    "section": "Look for information",
    "text": "Look for information\n\nRead carefully any error message\nRead the documentation (local or online)\nMake sure you have up-to-date versions\nGoogle (using carefully selected keywords or the error message)\nLook for open issues & bug reports"
  },
  {
    "objectID": "tools/help_slides.html#error-messages",
    "href": "tools/help_slides.html#error-messages",
    "title": "So, you are stuck … now what?",
    "section": "Error messages",
    "text": "Error messages\n\nRead them!\n. . .\nFamiliarise yourself with the error types in the languages you use\n\n\nExample: Python’s syntax errors vs exceptions\n\n. . .\nWarnings ≠ errors\n. . .\nLook for bits you understand (don’t get put off by what you don’t understand)\n. . .\nIdentify the locations of the errors to go investigate that part of the code"
  },
  {
    "objectID": "tools/help_slides.html#documentation",
    "href": "tools/help_slides.html#documentation",
    "title": "So, you are stuck … now what?",
    "section": "Documentation",
    "text": "Documentation\n\nYou need to find it\n\n\nYou need to understand it"
  },
  {
    "objectID": "tools/help_slides.html#finding-documentation",
    "href": "tools/help_slides.html#finding-documentation",
    "title": "So, you are stuck … now what?",
    "section": "Finding documentation",
    "text": "Finding documentation\n\nOnline:\nTake the time to look for the official documentation & other high quality sources for the languages & tools you use.\n\n\n\nExamples:\nPython: Reference manual, Standard library manual, Tutorial\nNumPy: Tutorial\nR: Open source book “R for Data Science”, Open source book “Advanced R”\nJulia: Documentation\nBash: Manual\nGit: Manual, Open source book\n\n\n\nIn the program itself\n\n\nUnderstanding the documentation"
  },
  {
    "objectID": "tools/help_slides.html#up-to-date-versions",
    "href": "tools/help_slides.html#up-to-date-versions",
    "title": "So, you are stuck … now what?",
    "section": "Up-to-date versions",
    "text": "Up-to-date versions\n\nFirst, you need to know what needs to be updated.\n\n\nKeeping a system up to date includes updating:\n\nthe OS\nthe program\n(any potential IDE)\npackages\n\n\n\nThen, you need to update regularly."
  },
  {
    "objectID": "tools/help_slides.html#google",
    "href": "tools/help_slides.html#google",
    "title": "So, you are stuck … now what?",
    "section": "Google",
    "text": "Google\nGoogle’s algorithms are great at guessing what we are looking for.\n\nBut there is a frequency problem:\nSearches relating to programming-specific questions represent too small a fraction of the overall searches for results to be relevant unless you use key vocabulary.\n\n\nBe precise.\n\n\nLearn the vocabulary of your language/tool to know what to search for."
  },
  {
    "objectID": "tools/help_slides.html#open-issues-bug-reports",
    "href": "tools/help_slides.html#open-issues-bug-reports",
    "title": "So, you are stuck … now what?",
    "section": "Open issues & bug reports",
    "text": "Open issues & bug reports\nIf the tool you are using is open source, look for issues matching your problem in the source repository (e.g. on GitHub or GitLab)."
  },
  {
    "objectID": "tools/help_slides.html#what-if-the-answer-isnt-out-there",
    "href": "tools/help_slides.html#what-if-the-answer-isnt-out-there",
    "title": "So, you are stuck … now what?",
    "section": "What if the answer isn’t out there?",
    "text": "What if the answer isn’t out there?\nWhen everything has failed & you have to ask for help, you need to know:\n\n\nWhere to ask\n\n\n\n\nHow to ask"
  },
  {
    "objectID": "tools/help_slides.html#where-to-ask-1",
    "href": "tools/help_slides.html#where-to-ask-1",
    "title": "So, you are stuck … now what?",
    "section": "Where to ask",
    "text": "Where to ask\nQ&A sites\nMostly, Stack Overflow & the Stack Exchange network.\nCo-founded in 2008 & 2009 by Jeff Atwood & Joel Spolsky.\nForums\nMostly, Discourse.\nCo-founded in 2013 by Jeff Atwood, Robin Ward & Sam Saffron.\nA few other older forums."
  },
  {
    "objectID": "tools/help_slides.html#where-to-ask-2",
    "href": "tools/help_slides.html#where-to-ask-2",
    "title": "So, you are stuck … now what?",
    "section": "Where to ask",
    "text": "Where to ask\nWhich one to choose is a matter of personal preference.\nPossible considerations:\n\nSome niche topics have very active communities on Discourse\nStack Overflow & some older forums can be intimidating with higher expectations for the questions quality & a more direct handling of mistakes\nFor conversations, advice, or multiple step questions, go to Discourse\nStack Overflow has over 13 million users\nStack Overflow & co have a very efficient approach"
  },
  {
    "objectID": "tools/help_slides.html#stack-overflow-co",
    "href": "tools/help_slides.html#stack-overflow-co",
    "title": "So, you are stuck … now what?",
    "section": "Stack Overflow & co",
    "text": "Stack Overflow & co\nPick the best site to ask your question.\nA few of the Stack Exchange network sites:\nStack Overflow: programming\nSuper User: computer hardware & software\nUnix & Linux: *nix OS TEX: TeX/LaTeX\nCross Validated: stats; data mining, collecting, analysis & visualization; ML\nData Science: focus on implementation & processes\nOpen Data\nGIS"
  },
  {
    "objectID": "tools/help_slides.html#how-to-ask-1",
    "href": "tools/help_slides.html#how-to-ask-1",
    "title": "So, you are stuck … now what?",
    "section": "How to ask",
    "text": "How to ask\n\nFamiliarize yourself with the site by reading posts\n\n\nRead the “Tour” page (SO/SE) or take the “New user tutorial” (Discourse)\n\n\nMake sure the question has not already been asked\n\n\nFormat the question properly\n\n\nGive a minimum reproducible example\n\n\nDo not share sensitive data\n\n\nShow your attempts\n\n\nAvoid cross-posting. If you really have to, make sure to cross-reference"
  },
  {
    "objectID": "tools/help_slides.html#how-to-ask-so-co",
    "href": "tools/help_slides.html#how-to-ask-so-co",
    "title": "So, you are stuck … now what?",
    "section": "How to ask: SO & co",
    "text": "How to ask: SO & co\n\nDon’t ask opinion-based questions\n\n\nDon’t ask for package, tool, or service recommendations\n\n\nDon’t ask more than one question in a single post\n\n\nCheck your spelling, grammar, punctuation, capitalized sentences, etc.\n\n\nAvoid greetings, signatures, thank-yous; keep it to the point\n\n\nAvoid apologies about being a beginner, this being your first post, the question being stupid, etc: do the best you can & skip the personal, self-judgmental & irrelevant bits"
  },
  {
    "objectID": "tools/help_slides.html#formatting-your-question",
    "href": "tools/help_slides.html#formatting-your-question",
    "title": "So, you are stuck … now what?",
    "section": "Formatting your question",
    "text": "Formatting your question\nNowadays, most sites (including Stack Overflow & Discourse) allow markdown rendering.\nSome older forums implement other markup languages (e.g. BBCode).\nThe information is always easy to find. Spend the time to format your question properly. People will be much less inclined to help you if you don’t show any effort & if your question is a nightmare to read."
  },
  {
    "objectID": "tools/help_slides.html#example-of-a-typical-downvoted-question",
    "href": "tools/help_slides.html#example-of-a-typical-downvoted-question",
    "title": "So, you are stuck … now what?",
    "section": "Example of a typical downvoted question",
    "text": "Example of a typical downvoted question\nhowdy!!\ni am new to R sorry for a very silly question.i looked all oever the itnernwet, but i dint find\nanyanswer. i tried to use ggplot i get the error: Error in loadNamespace(i, c(lib.loc, .libPaths()),\nversionCheck = vI[[i]]) : there is no package called 'stringi'\nthank youu very much!!!!!\nmarie\n\n[Out]"
  },
  {
    "objectID": "tools/help_slides.html#same-question-fixed",
    "href": "tools/help_slides.html#same-question-fixed",
    "title": "So, you are stuck … now what?",
    "section": "Same question, fixed",
    "text": "Same question, fixed\nWhen I try to load the package `ggplot2` with:\n\n```{r}\nlibrary(ggplot2)\n```\nI get the error:\n\n&gt; Error in loadNamespace(i, c(lib.loc, .libPaths()), versionCheck = vI[[i]]) :\nthere is no package called 'stringi'\n\nWhat am I doing wrong?"
  },
  {
    "objectID": "tools/help_slides.html#still-not-good-enough",
    "href": "tools/help_slides.html#still-not-good-enough",
    "title": "So, you are stuck … now what?",
    "section": "Still not good enough",
    "text": "Still not good enough\nThis question is actually a duplicate of a question asked which is itself a duplicate of another question."
  },
  {
    "objectID": "tools/help_slides.html#creating-a-minimal-reproducible-example",
    "href": "tools/help_slides.html#creating-a-minimal-reproducible-example",
    "title": "So, you are stuck … now what?",
    "section": "Creating a minimal reproducible example",
    "text": "Creating a minimal reproducible example\nThere are great posts on how to create a good minimal reproducible example. In particular:\nHow to create a Minimal, Reproducible Example\nFor R (but concepts apply to any language):\nHow to make a great R reproducible example\nWhat’s a reproducible example (reprex) and how do I do one?"
  },
  {
    "objectID": "tools/help_slides.html#creating-a-minimal-reproducible-example-1",
    "href": "tools/help_slides.html#creating-a-minimal-reproducible-example-1",
    "title": "So, you are stuck … now what?",
    "section": "Creating a minimal reproducible example",
    "text": "Creating a minimal reproducible example\n\nLoad all necessary packages\nLoad or create necessary data\nSimplify the data & the code as much as possible while still reproducing the problem\nUse simple variable names"
  },
  {
    "objectID": "tools/help_slides.html#data-for-your-example-your-own-data",
    "href": "tools/help_slides.html#data-for-your-example-your-own-data",
    "title": "So, you are stuck … now what?",
    "section": "Data for your example: your own data",
    "text": "Data for your example: your own data\nDo not upload data somewhere on the web to be downloaded.\nMake sure that the data is anonymised.\nDon’t keep more variables & more data points than are necessary to reproduce the problem.\nSimplify the variable names.\nIn R, you can use functions such as dput() to turn your reduced, anonymised data into text that is easy to copy/paste & can then be used to recreate the data."
  },
  {
    "objectID": "tools/help_slides.html#data-for-your-example-create-a-toy-dataset",
    "href": "tools/help_slides.html#data-for-your-example-create-a-toy-dataset",
    "title": "So, you are stuck … now what?",
    "section": "Data for your example: create a toy dataset",
    "text": "Data for your example: create a toy dataset\nYou can also create a toy dataset.\nFunctions that create random data, series, or repetitions are very useful here."
  },
  {
    "objectID": "tools/help_slides.html#data-for-your-example-pre-packaged-datasets",
    "href": "tools/help_slides.html#data-for-your-example-pre-packaged-datasets",
    "title": "So, you are stuck … now what?",
    "section": "Data for your example: pre-packaged datasets",
    "text": "Data for your example: pre-packaged datasets\nSome languages/packages come with pre-packaged datasets. If your code involves such languages/packages, you can make use of these datasets to create your reproducible example.\nFor example, R comes with many datasets directly available, including iris, mtcars, trees, airquality. In the R console, try:\n?iris\n?mtcars"
  },
  {
    "objectID": "tools/help_slides.html#additional-considerations",
    "href": "tools/help_slides.html#additional-considerations",
    "title": "So, you are stuck … now what?",
    "section": "Additional considerations",
    "text": "Additional considerations\nEven if you always find answers to your questions without having to post yourself, consider signing up to these sites:\n\nIt allows you to upvote (SO/SE) or like (Discourse) the questions & answers that help you—and why not thank in this fashion those that are making your life easier?\nIt makes you a part of these communities.\nOnce you are signed up, maybe you will start being more involved & contribute with questions & answers of your own."
  },
  {
    "objectID": "tools/help_slides.html#a-last-word",
    "href": "tools/help_slides.html#a-last-word",
    "title": "So, you are stuck … now what?",
    "section": "A last word",
    "text": "A last word\nWhile it takes some work to ask a good question, do not let this discourage you from posting on Stack Overflow: if you ask a good question, you will get many great answers.\nYou will learn in the process of developing your question (you may actually find the answer in that process) & you will learn from the answers.\nIt is forth the effort.\nHere is the Stack Overflow documentation on how to ask a good question.\n\n\n\n Back to webinar page"
  },
  {
    "objectID": "tools/quarto.html",
    "href": "tools/quarto.html",
    "title": "Authoring scientific documents with Markdown and Quarto",
    "section": "",
    "text": "This workshop will show you how to easily create beautiful scientific documents (html, pdf, websites, books…)—complete with formatted text, dynamic code, and figures with Quarto, an open-source tool combining the powers of Jupyter or knitr with Pandoc to turn your text and code blocks into fully dynamic and formatted documents.\nRendered document (click on it to open it in a new tab):\nRendered document (click on it to open it in a new tab):\nRendered document (click on it to open it in a new tab):\nRendered document (click on it to open it in a new tab):"
  },
  {
    "objectID": "tools/quarto.html#markup-and-markdown",
    "href": "tools/quarto.html#markup-and-markdown",
    "title": "Authoring scientific documents with Markdown and Quarto",
    "section": "Markup and Markdown",
    "text": "Markup and Markdown\n\nMarkup languages\nMarkup languages control the formatting of text documents. They are powerful but complex and the raw text (before it is rendered into its formatted version) is visually cluttered and hard to read.\nExamples of markup languages include LaTeX and HTML.\n\nTex (often with the macro package LaTeX) is used to create pdf.\n\n\nExample LaTeX:\n\n\\documentclass{article}\n\\title{My title}\n\\author{My name}\n\\usepackage{datetime}\n\\newdate{date}{24}{11}{2022}\n\\date{\\displaydate{date}}\n\\begin{document}\n \\maketitle\n \\section{First section}\n Some text in the first section.\n\\end{document}\n\nHTML (often with css or scss files to customize the format) is used to create webpages.\n\n\nExample HTML:\n\n&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en-US\"&gt;\n  &lt;head&gt;\n    &lt;meta charset=\"utf-8\" /&gt;\n    &lt;meta name=\"viewport\" content=\"width=device-width\" /&gt;\n    &lt;title&gt;My title&lt;/title&gt;\n    &lt;address class=\"author\"&gt;My name&lt;/address&gt;\n    &lt;input type=\"date\" value=\"2022-11-24\" /&gt;\n  &lt;/head&gt;\n  &lt;h1&gt;First section&lt;/h1&gt;\n  &lt;body&gt;\n    Some text in the first section.\n  &lt;/body&gt;\n&lt;/html&gt;\n\n\nMarkdown\nA number of minimalist markup languages intend to remove all the visual clutter and complexity to create raw texts that are readable prior to rendering. Markdown (note the pun with “markup”), created in 2004, is the most popular of them. Due to its simplicity, it has become quasi-ubiquitous. Many implementations exist which add a varying number of features (as you can imagine, a very simple markup language is also fairly limited).\nMarkdown files are simply text files and they use the .md extension.\n\n\nBasic Markdown syntax\nIn its basic form, Markdown is mostly used to create webpages. Conveniently, raw HTML can be included whenever the limited markdown syntax isn’t sufficient.\nHere is an overview of the Markdown syntax supported by many applications.\n\n\nPandoc and its extended Markdown syntax\nWhile the basic syntax is good enough for HTML outputs, it is very limited for other formats.\nPandoc is a free and open-source markup format converter. Pandoc supports an extended Markdown syntax with functionality for figures, tables, callout blocks, LaTeX mathematical equations, citations, and YAML metadata blocks. In short, everything needed for the creation of scientific documents.\nSuch documents remain as readable as basic Markdown documents (thus respecting the Markdown philosophy), but they can now be rendered in sophisticated pdf, books, entire websites, Word documents, etc.\nAnd of course, as such documents remain text files, you can put them under version control with Git.\n\nPrevious example using Pandoc’s Markdown:\n\n---\ntitle: My title\nauthor: My name\ndate: 2022-11-24\n---\n# First section\nSome text in the first section."
  },
  {
    "objectID": "tools/quarto.html#literate-programming",
    "href": "tools/quarto.html#literate-programming",
    "title": "Authoring scientific documents with Markdown and Quarto",
    "section": "Literate programming",
    "text": "Literate programming\nLiterate programming is a methodology that combines snippets of code and written text. While first introduced in 1984, this approach to the creation of documents has truly exploded in popularity in recent years thanks to the development of new tools such as R Markdown and, later, Jupyter notebooks."
  },
  {
    "objectID": "tools/quarto.html#quarto",
    "href": "tools/quarto.html#quarto",
    "title": "Authoring scientific documents with Markdown and Quarto",
    "section": "Quarto",
    "text": "Quarto\n\nHow it works\nQuarto files are transformed into Pandoc’s extended Markdown by Jupyter (when used with Python or Julia) or by knitr (when used with R), then pandoc turns the Markdown document into the output of your choice.\n\nJulia and Python make use of the Jupyter engine:\n\n From Quarto documentation\n\nR uses the knitr engine:\n\n From Quarto documentation\nQuarto files use the extension .qmd.\nWhen using R, you can use Quarto directly from RStudio: if you are used to R Markdown, Quarto is the new and better R Markdown.\nWhen using Python or Julia, you can use Quarto directly from a Jupyter notebook (with .ipynb extension).\n\nUsing Quarto directly from a Jupyter notebook:\n\n From Quarto documentation\nIn this workshop, we will see the most general workflow: simply using a text editor.\n\n\nSupported languages\nQuarto renders highlighting in countless languages and generates dynamic output for code blocks in:\n\nPython\nR\nJulia\nObservable JS\n\nYou can render documents in a wide variety of formats:\n\nHTML\nPDF\nMS Word\nOpenOffice\nePub\nRevealjs\nPowerPoint\nBeamer\nGitHub Markdown\nCommonMark\nHugo\nDocusaurus\nMarkua\nMediaWiki\nDokuWiki\nZimWiki\nJira Wiki\nXWiki\nJATS\nJupyter\nConTeXt\nRTF\nreST\nAsciiDoc\nOrg-Mode\nMuse\nGNU\nGroff\n\nThis training website is actually built with Quarto!\n\n\nInstallation\n\nDownload Quarto here.\nDownload the language(s) (R, Python, or Julia) you will want to use with Quarto as well as their corresponding engine (knitr for R; Jupyter for Python and Julia):\n\nIf you want to use Quarto with R, you will need:\n\nR (download here if you don’t have R already on your system),\nthe rmarkdown package. For this, launch R and run:\n\ninstall.packages(\"rmarkdown\")\nIf you want to use it with Python, you will need:\n\nPython 3 (download here if don’t have it on your system),\nJupyterLab. For this, open a terminal and run:\n\npython3 -m pip install jupyter  # if you are on MacOS or Linux\npython -m pip install jupyter   # if you are on Windows\nFinally, if you want to use Quarto with Julia, you will need:\n\nJulia (download here if you don’t have Julia),\nthe IJulia and Revise packages. For this, launch Julia and run:\n\n] add IJulia Revise\n# &lt;Backspace&gt;\nusing IJulia\nnotebook()      # to install a minimal Python+Jupyter distribution\nRunning notebook() allows you to install Jupyter if you don’t already have it.\n\n\nDocument structure and syntax\n\nFront matter\nWritten in YAML. Sets the options for the document. Let’s see a few examples.\n\nHTML output:\n\n---\ntitle: \"My title\"\nauthor: \"My name\"\nformat: html\n---\n\nHTML output with a few options:\n\n---\ntitle: \"My title\"\nauthor: \"My name\"\nformat:\n  html:\n    toc: true\n    css: &lt;my_file&gt;.css\n---\n\nMS Word output with Python code blocks:\n\n---\ntitle: \"My title\"\nauthor: \"My name\"\nformat: docx\njupyter: python3\n---\n\nrevealjs output with some options and Julia code blocks:\n\n---\ntitle: \"Some title\"\nsubtitle: \"Some subtitle\"\ninstitute: \"Simon Fraser University\"\ndate: \"2022-11-24\"\nexecute:\n  error: true\n  echo: true\nformat:\n  revealjs:\n    theme: [default, custom.scss]\n    highlight-style: monokai\n    code-line-numbers: false\n    embed-resources: true\njupyter: julia-1.8\n---\nSee the Quarto documentation for an exhaustive list of options for all formats.\n\n\nWritten sections\nWritten sections are written in Pandoc’s extended Markdown.\n\n\nCode blocks\nIf all you want is syntax highlighting of the code blocks, use this syntax:\n{{.language}} &lt;some code&gt;\nIf you want syntax highlighting of the blocks and for the code to run, use instead:\n```{language}\n&lt;some code&gt;\n```\nIn addition, options can be added to individual code blocks:\n```{language}\n#| &lt;some option&gt;: &lt;some option value&gt;\n\n&lt;some code&gt;\n```\n\n\n\nRendering\nUsing Quarto is very simple: there are only two commands you need to know.\nIn a terminal, simply run either of:\nquarto render &lt;file&gt;.qmd     # Render the document\nquarto preview &lt;file&gt;.qmd    # Display a live preview"
  },
  {
    "objectID": "tools/quarto.html#lets-create-a-webpage-together",
    "href": "tools/quarto.html#lets-create-a-webpage-together",
    "title": "Authoring scientific documents with Markdown and Quarto",
    "section": "Let’s create a webpage together",
    "text": "Let’s create a webpage together\nFirst, create a file called test.qmd with the text editor of your choice.\n\nExample:\n\nnano test.qmd\nAdd a minimal front matter with the title of your document and the output format (html here since we are creating a webpage):\n---\ntitle: Test webpage\nformat: html\n---\nThen open a new terminal, cd to the location of the file, and run the command:\nquarto preview test.qmd\nThis will open the rendered document in your browser.\nWe will play with this test.qmd file and see how it is rendered by Quarto as we go."
  },
  {
    "objectID": "tools/quarto.html#examples",
    "href": "tools/quarto.html#examples",
    "title": "Authoring scientific documents with Markdown and Quarto",
    "section": "Examples",
    "text": "Examples\nBelow are a few basic example files and their outputs.\n\nRevealjs presentation"
  },
  {
    "objectID": "tools/quarto.html#code",
    "href": "tools/quarto.html#code",
    "title": "Authoring scientific documents with Markdown and Quarto",
    "section": "Code",
    "text": "Code\n---\ntitle: \"My title\"\nauthor: \"My name\"\ninstitute: \"Simon Fraser University\"\nformat:\n  revealjs:\n    highlight-style: monokai\n    code-line-numbers: false\n    embed-resources: true\n---\n\n## First section\n\nWhen exporting to revealjs, second level sections mark the start of new slides,\nwith a slide title.\n\nThis can be changed in options.\n\n---\n\nNew slides can be started without titles this way.\n\n# There are title slides\n\n## Formatting\n\nText can be rendered *in italic* or **in bold** as well as [underlined]{.underline}.\n\nYou can use superscripts^2^, subscripts~test~, ~~strikethrough~~, and `inline code`.\n\n&gt; This is a quote.\n\n## Columns\n\n:::: {.columns}\n\n::: {.column width=\"30%\"}\n\nYou can create columns.\n\n:::\n\n::: {.column width=\"70%\"}\n\nAnd you can set their respective width.\n\n:::\n\n::::\n\n## Lists\n\n::: {.incremental}\n\n- List can happen one line at a time\n- like\n- this\n\n:::\n\n## Lists\n\n- Or all at the same time\n- like\n- that\n\n## Ordered lists\n\n1. Item 1\n2. Item 2\n3. Item 3\n\n## Images\n\n![Example image](qmd_jupyter.png)\n\n## Tables\n\n| Col 1 | Col 2 | Col 3  |\n|-------|-------|--------|\n| a     | 1     | red    |\n| b     | 2     | orange | \n| c     | 3     | yellow |\n\n:::{.callout-note}\n\nTables can be fully customized (or you could use raw html).\n\n:::\n\n## Equations\n\n$$\n\\frac{\\partial \\mathrm C}{ \\partial \\mathrm t } + \\frac{1}{2}\\sigma^{2} \\mathrm S^{2}\n\\frac{\\partial^{2} \\mathrm C}{\\partial \\mathrm C^2}\n  + \\mathrm r \\mathrm S \\frac{\\partial \\mathrm C}{\\partial \\mathrm S}\\ =\n  \\mathrm r \\mathrm C \n$$"
  },
  {
    "objectID": "tools/quarto.html#code-1",
    "href": "tools/quarto.html#code-1",
    "title": "Authoring scientific documents with Markdown and Quarto",
    "section": "Code",
    "text": "Code\n---\ntitle: \"My title\"\nauthor: \"My name\"\nformat:\n  pdf:\n    toc: true\n---\n\n## Heading\n\nSome text.\n\n### Subheading\n\nMore text.\n\n## Formatting\n\nText can be rendered *in italic* or **in bold** as well as [underlined]{.underline}.\n\nYou can use superscripts^2^, subscripts~test~, ~~strikethrough~~, and `inline code`.\n\n&gt; This is a quote.\n\n## Lists\n\n### Unordered\n\n- Item 1\n- Item 2\n- Item 3\n\n### Ordered\n\n1. Item 1\n2. Item 2\n3. Item 3\n\n## Images\n\n![Example image](qmd_jupyter.png)\n\n## Tables\n\n| Col 1 | Col 2 | Col 3  |\n|-------|-------|--------|\n| a     | 1     | red    |\n| b     | 2     | orange | \n| c     | 3     | yellow |\n\n:::{.callout-note}\n\nTables can be fully customized (or you could use raw html).\n\n:::\n\n## Equations\n\n$$\n\\frac{\\partial \\mathrm C}{ \\partial \\mathrm t } + \\frac{1}{2}\\sigma^{2} \\mathrm S^{2}\n\\frac{\\partial^{2} \\mathrm C}{\\partial \\mathrm C^2}\n  + \\mathrm r \\mathrm S \\frac{\\partial \\mathrm C}{\\partial \\mathrm S}\\ =\n  \\mathrm r \\mathrm C \n$$"
  },
  {
    "objectID": "tools/quarto.html#code-2",
    "href": "tools/quarto.html#code-2",
    "title": "Authoring scientific documents with Markdown and Quarto",
    "section": "Code",
    "text": "Code\n---\ntitle: \"My title\"\nauthor: \"My name\"\ninstitute: \"Simon Fraser University\"\nformat: html\n---\n\n## Heading\n\n### Subheading\n\nSome text.\n\n## Formatting  {#sec-formatting}\n\n::: aside\n\nNote that each heading automatically creates an anchor, making it easy to link to specific sections of your documents.\n\n:::\n\nText can be rendered *in italic* or **in bold** as well as [underlined]{.underline}.\n\nYou can use superscripts^2^, subscripts~test~, ~~strikethrough~~, and `inline code`.\n\n&gt; This is a quote.\n\n## Columns\n\n:::: {.columns}\n\n::: {.column width=\"30%\"}\n\nYou can create columns.\n\n:::\n\n::: {.column width=\"70%\"}\n\nAnd you can set their respective width.\n\n:::\n\n::::\n\n## Lists\n\n- Item 1\n- Item 2\n- Item 3\n\n## Ordered lists\n\n1. Item 1\n2. Item 2\n3. Item 3\n\n## Images\n\n![Example image](qmd_jupyter.png)\n\n## Tables\n\n| Col 1 | Col 2 | Col 3  |\n|-------|-------|--------|\n| a     | 1     | red    |\n| b     | 2     | orange | \n| c     | 3     | yellow |\n\n:::{.callout-note}\n\nTables can be fully customized (or you could use raw html).\n\n:::\n\n## Equations\n\n$$\n\\frac{\\partial \\mathrm C}{ \\partial \\mathrm t } + \\frac{1}{2}\\sigma^{2} \\mathrm S^{2}\n\\frac{\\partial^{2} \\mathrm C}{\\partial \\mathrm C^2}\n  + \\mathrm r \\mathrm S \\frac{\\partial \\mathrm C}{\\partial \\mathrm S}\\ =\n  \\mathrm r \\mathrm C \n$$\n\n## Cross-references\n\nSee @sec-formatting.\n\n*Note that you can add bibliographies, flow charts, the equivalent of HTML \"div\",\nand just so much more. Remember that this is a tiny overview.*\n\n## Let's try some code blocks now\n\n```{r}\n# This is a block that runs\n2 + 3\n```\n\n::: aside\n\nDid you notice that the content of your code blocks can be copied with a click?\nOf course, this is customizable.\n\n:::\n\n```{.r}\n# This is a block that doesn't run\n2 + 3\n```\n\n```{r}\n#| echo: false\n# And this is a block showing only the output\ndata.frame(\n  country = c(\"Canada\", \"USA\", \"Mexico\"),\n  var = c(2.9, 3.1, 4.5)\n)\n```\n\n## Plots\n\n```{r}\nplot(cars)\n```\n\n&lt;br&gt;\nYou can play with options to add a title:\n\n```{r}\n#| fig-cap: \"Stopping distance as a function of speed in cars\"\n\nplot(cars)\n```\n\n&lt;br&gt;\nYou can have more complex multi-plot layouts:\n\n```{r}\n#| layout-ncol: 2\n#| fig-cap: \n#|   - \"Stopping distance as a function of speed in cars\"\n#|   - \"Vapor pressure of mercury as a function of temperature\"\n\nplot(cars)\nplot(pressure)\n```\n\nFor those who have `ggplot2`[^1], you can try that too:\n\n```{r}\nlibrary(ggplot2)\n\nggplot(data = mpg, mapping = aes(x = displ, y = hwy)) + \n  geom_point(mapping = aes(color = class)) + \n  geom_smooth()\n```\n\n[^1]: You can install it with:\n    ```{.r}\n    install.packages(\"ggplot2\")\n    ```"
  },
  {
    "objectID": "tools/quarto.html#code-3",
    "href": "tools/quarto.html#code-3",
    "title": "Authoring scientific documents with Markdown and Quarto",
    "section": "Code",
    "text": "Code\n---\ntitle: \"Some title\"\nauthor: \"Some name\"\nformat: beamer\njupyter: python3\n---\n\n## First slide\n\nWith some content\n\n## Formatting\n\nText can be rendered *in italic* or **in bold** as well as [underlined]{.underline}.\n\nYou can use superscripts^2^, subscripts~test~, ~~strikethrough~~, and `inline code`.\n\n## Lists\n\n- Item 1\n- Item 2\n- Item 3\n\n## Ordered lists\n\n1. Item 1\n2. Item 2\n3. Item 3\n\n## Images\n\n![Example image](qmd_jupyter.png)\n\n## Tables\n\n| Col 1 | Col 2 | Col 3  |\n|-------|-------|--------|\n| a     | 1     | red    |\n| b     | 2     | orange | \n| c     | 3     | yellow |\n\n:::{.callout-note}\n\nTables can be fully customized (or you could use raw html).\n\n:::\n\n## Equations\n\n$$\n\\frac{\\partial \\mathrm C}{ \\partial \\mathrm t } + \\frac{1}{2}\\sigma^{2} \\mathrm S^{2}\n\\frac{\\partial^{2} \\mathrm C}{\\partial \\mathrm C^2}\n  + \\mathrm r \\mathrm S \\frac{\\partial \\mathrm C}{\\partial \\mathrm S}\\ =\n  \\mathrm r \\mathrm C \n$$\n\n## Some basic code block\n\n```{python}\n#| echo: true\n\n2 + 3\n```\n\n## Some plot\n\n```{python}\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Data for plotting\nt = np.arange(0.0, 2.0, 0.01)\ns = 1 + np.sin(2 * np.pi * t)\n\nfig, ax = plt.subplots()\nax.plot(t, s)\n\nax.set(xlabel='time (s)', ylabel='voltage (mV)',\n       title='Here goes the title')\nax.grid()\n\nfig.savefig(\"test.png\")\nplt.show()\n```"
  },
  {
    "objectID": "tools/quarto.html#recording",
    "href": "tools/quarto.html#recording",
    "title": "Authoring scientific documents with Markdown and Quarto",
    "section": "Recording",
    "text": "Recording"
  },
  {
    "objectID": "tools/quarto_webinar.html",
    "href": "tools/quarto_webinar.html",
    "title": "The new R Markdown:",
    "section": "",
    "text": "This webinar will show you how to easily create beautiful publications (webpages, pdf, websites, presentations, books…)—complete with formatted text, dynamic code and figures with Quarto.\nQuarto is the successor to R Markdown. By combining the powers of Jupyter or knitr with Pandoc, it works with R, but also with Python and Julia code blocks and it adds new functionalities to the old tool.\n\nSlides (Click and wait: this reveal.js presentation may take a little time to load.) \n\n\n\nVariation of the talk as a staff to staff webinar:"
  }
]