[
  {
    "objectID": "tools/ws_quarto.html",
    "href": "tools/ws_quarto.html",
    "title": "Authoring scientific documents with Markdown and Quarto",
    "section": "",
    "text": "This workshop will show you how to easily create beautiful scientific documents (html, pdf, websites, books…)—complete with formatted text, dynamic code, and figures with Quarto, an open-source tool combining the powers of Jupyter or knitr with Pandoc to turn your text and code blocks into fully dynamic and formatted documents.",
    "crumbs": [
      "Tools",
      "<em><b>Workshops</b></em>"
    ]
  },
  {
    "objectID": "tools/ws_quarto.html#markup-and-markdown",
    "href": "tools/ws_quarto.html#markup-and-markdown",
    "title": "Authoring scientific documents with Markdown and Quarto",
    "section": "Markup and Markdown",
    "text": "Markup and Markdown\n\nMarkup languages\nMarkup languages control the formatting of text documents. They are powerful but complex and the raw text (before it is rendered into its formatted version) is visually cluttered and hard to read.\nExamples of markup languages include LaTeX and HTML.\n\nTex (often with the macro package LaTeX) is used to create pdf.\n\n\nExample LaTeX:\n\n\\documentclass{article}\n\\title{My title}\n\\author{My name}\n\\usepackage{datetime}\n\\newdate{date}{24}{11}{2022}\n\\date{\\displaydate{date}}\n\\begin{document}\n \\maketitle\n \\section{First section}\n Some text in the first section.\n\\end{document}\n\nHTML (often with css or scss files to customize the format) is used to create webpages.\n\n\nExample HTML:\n\n&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en-US\"&gt;\n  &lt;head&gt;\n    &lt;meta charset=\"utf-8\" /&gt;\n    &lt;meta name=\"viewport\" content=\"width=device-width\" /&gt;\n    &lt;title&gt;My title&lt;/title&gt;\n    &lt;address class=\"author\"&gt;My name&lt;/address&gt;\n    &lt;input type=\"date\" value=\"2022-11-24\" /&gt;\n  &lt;/head&gt;\n  &lt;h1&gt;First section&lt;/h1&gt;\n  &lt;body&gt;\n    Some text in the first section.\n  &lt;/body&gt;\n&lt;/html&gt;\n\n\nMarkdown\nA number of minimalist markup languages intend to remove all the visual clutter and complexity to create raw texts that are readable prior to rendering. Markdown (note the pun with “markup”), created in 2004, is the most popular of them. Due to its simplicity, it has become quasi-ubiquitous. Many implementations exist which add a varying number of features (as you can imagine, a very simple markup language is also fairly limited).\nMarkdown files are simply text files and they use the .md extension.\n\n\nBasic Markdown syntax\nIn its basic form, Markdown is mostly used to create webpages. Conveniently, raw HTML can be included whenever the limited markdown syntax isn’t sufficient.\nHere is an overview of the Markdown syntax supported by many applications.\n\n\nPandoc and its extended Markdown syntax\nWhile the basic syntax is good enough for HTML outputs, it is very limited for other formats.\nPandoc is a free and open-source markup format converter. Pandoc supports an extended Markdown syntax with functionality for figures, tables, callout blocks, LaTeX mathematical equations, citations, and YAML metadata blocks. In short, everything needed for the creation of scientific documents.\nSuch documents remain as readable as basic Markdown documents (thus respecting the Markdown philosophy), but they can now be rendered in sophisticated pdf, books, entire websites, Word documents, etc.\nAnd of course, as such documents remain text files, you can put them under version control with Git.\n\nPrevious example using Pandoc’s Markdown:\n\n---\ntitle: My title\nauthor: My name\ndate: 2022-11-24\n---\n# First section\nSome text in the first section.",
    "crumbs": [
      "Tools",
      "<em><b>Workshops</b></em>"
    ]
  },
  {
    "objectID": "tools/ws_quarto.html#literate-programming",
    "href": "tools/ws_quarto.html#literate-programming",
    "title": "Authoring scientific documents with Markdown and Quarto",
    "section": "Literate programming",
    "text": "Literate programming\nLiterate programming is a methodology that combines snippets of code and written text. While first introduced in 1984, this approach to the creation of documents has truly exploded in popularity in recent years thanks to the development of new tools such as R Markdown and, later, Jupyter notebooks.",
    "crumbs": [
      "Tools",
      "<em><b>Workshops</b></em>"
    ]
  },
  {
    "objectID": "tools/ws_quarto.html#quarto",
    "href": "tools/ws_quarto.html#quarto",
    "title": "Authoring scientific documents with Markdown and Quarto",
    "section": "Quarto",
    "text": "Quarto\n\nHow it works\nQuarto files are transformed into Pandoc’s extended Markdown by Jupyter (when used with Python or Julia) or by knitr (when used with R), then pandoc turns the Markdown document into the output of your choice.\n\nJulia and Python make use of the Jupyter engine:\n\n From Quarto documentation\n\nR uses the knitr engine:\n\n From Quarto documentation\nQuarto files use the extension .qmd.\nWhen using R, you can use Quarto directly from RStudio: if you are used to R Markdown, Quarto is the new and better R Markdown.\nWhen using Python or Julia, you can use Quarto directly from a Jupyter notebook (with .ipynb extension).\n\nUsing Quarto directly from a Jupyter notebook:\n\n From Quarto documentation\nIn this workshop, we will see the most general workflow: simply using a text editor.\n\n\n\n\n\n\nSupported languages\n\n\n\n\n\nQuarto renders highlighting in countless languages and generates dynamic output for code blocks in:\n\nPython\nR\nJulia\nObservable JS\n\nYou can render documents in a wide variety of formats:\n\nHTML\nPDF\nMS Word\nOpenOffice\nePub\nRevealjs\nPowerPoint\nBeamer\nGitHub Markdown\nCommonMark\nHugo\nDocusaurus\nMarkua\nMediaWiki\nDokuWiki\nZimWiki\nJira Wiki\nXWiki\nJATS\nJupyter\nConTeXt\nRTF\nreST\nAsciiDoc\nOrg-Mode\nMuse\nGNU\nGroff\n\nThis training website is actually built with Quarto!\n\n\n\n\n\nInstallation\n\nDownload Quarto here.\nDownload the language(s) (R, Python, or Julia) you will want to use with Quarto as well as their corresponding engine (knitr for R; Jupyter for Python and Julia):\n\nIf you want to use Quarto with R, you will need:\n\nR (download here if you don’t have R already on your system),\nthe rmarkdown package. For this, launch R and run:\n\ninstall.packages(\"rmarkdown\")\nIf you want to use it with Python, you will need:\n\nPython 3 (download here if don’t have it on your system),\nJupyterLab. For this, open a terminal and run:\n\npython3 -m pip install jupyter  # if you are on macOS or Linux\npython -m pip install jupyter   # if you are on Windows\nFinally, if you want to use Quarto with Julia, you will need:\n\nJulia (download here if you don’t have Julia),\nthe IJulia and Revise packages. For this, launch Julia and run:\n\n] add IJulia Revise\n# &lt;Backspace&gt;\nusing IJulia\nnotebook()      # to install a minimal Python+Jupyter distribution\nRunning notebook() allows you to install Jupyter if you don’t already have it.\n\n\nDocument structure and syntax\n\nFront matter\nWritten in YAML. Sets the options for the document. Let’s see a few examples.\n\nHTML output:\n\n---\ntitle: \"My title\"\nauthor: \"My name\"\nformat: html\n---\n\nHTML output with a few options:\n\n---\ntitle: \"My title\"\nauthor: \"My name\"\nformat:\n  html:\n    toc: true\n    css: &lt;my_file&gt;.css\n---\n\nMS Word output with Python code blocks:\n\n---\ntitle: \"My title\"\nauthor: \"My name\"\nformat: docx\njupyter: python3\n---\n\nrevealjs output with some options and Julia code blocks:\n\n---\ntitle: \"Some title\"\nsubtitle: \"Some subtitle\"\ninstitute: \"Simon Fraser University\"\ndate: \"2022-11-24\"\nexecute:\n  error: true\n  echo: true\nformat:\n  revealjs:\n    theme: [default, custom.scss]\n    highlight-style: monokai\n    code-line-numbers: false\n    embed-resources: true\njupyter: julia-1.8\n---\nSee the Quarto documentation for an exhaustive list of options for all formats.\n\n\nWritten sections\nWritten sections are written in Pandoc’s extended Markdown.\n\n\nCode blocks\nIf all you want is syntax highlighting of the code blocks, use this syntax:\n{.language} &lt;some code&gt;\nIf you want syntax highlighting of the blocks and for the code to run, use instead:\n```{language}\n&lt;some code&gt;\n```\nIn addition, options can be added to individual code blocks:\n```{language}\n#| &lt;some option&gt;: &lt;some option value&gt;\n\n&lt;some code&gt;\n```\n\n\n\nRendering\nUsing Quarto is very simple: there are only two commands you need to know.\nIn a terminal, simply run either of:\nquarto render &lt;file&gt;.qmd     # Render the document\nquarto preview &lt;file&gt;.qmd    # Display a live preview",
    "crumbs": [
      "Tools",
      "<em><b>Workshops</b></em>"
    ]
  },
  {
    "objectID": "tools/ws_quarto.html#lets-create-a-webpage-together",
    "href": "tools/ws_quarto.html#lets-create-a-webpage-together",
    "title": "Authoring scientific documents with Markdown and Quarto",
    "section": "Let’s create a webpage together",
    "text": "Let’s create a webpage together\nFirst, create a file called test.qmd with the text editor of your choice.\n\nExample:\n\nnano test.qmd\nAdd a minimal front matter with the title of your document and the output format (html here since we are creating a webpage):\n---\ntitle: Test webpage\nformat: html\n---\nThen open a new terminal, cd to the location of the file, and run the command:\nquarto preview test.qmd\nThis will open the rendered document in your browser.\nWe will play with this test.qmd file and see how it is rendered by Quarto as we go.",
    "crumbs": [
      "Tools",
      "<em><b>Workshops</b></em>"
    ]
  },
  {
    "objectID": "tools/ws_quarto.html#examples",
    "href": "tools/ws_quarto.html#examples",
    "title": "Authoring scientific documents with Markdown and Quarto",
    "section": "Examples",
    "text": "Examples\nBelow are a few basic example files and their outputs.\n\nRevealjs presentation\n\n\n\n\n\n\nCode\n\n\n\n\n\n---\ntitle: \"My title\"\nauthor: \"My name\"\ninstitute: \"Simon Fraser University\"\nformat:\n  revealjs:\n    highlight-style: monokai\n    code-line-numbers: false\n    embed-resources: true\n---\n\n## First section\n\nWhen exporting to revealjs, second level sections mark the start of new slides,\nwith a slide title.\n\nThis can be changed in options.\n\n---\n\nNew slides can be started without titles this way.\n\n# There are title slides\n\n## Formatting\n\nText can be rendered *in italic* or **in bold** as well as [underlined]{.underline}.\n\nYou can use superscripts^2^, subscripts~test~, ~~strikethrough~~, and `inline code`.\n\n&gt; This is a quote.\n\n## Columns\n\n:::: {.columns}\n\n::: {.column width=\"30%\"}\n\nYou can create columns.\n\n:::\n\n::: {.column width=\"70%\"}\n\nAnd you can set their respective width.\n\n:::\n\n::::\n\n## Lists\n\n::: {.incremental}\n\n- List can happen one line at a time\n- like\n- this\n\n:::\n\n## Lists\n\n- Or all at the same time\n- like\n- that\n\n## Ordered lists\n\n1. Item 1\n2. Item 2\n3. Item 3\n\n## Images\n\n![Example image](qmd_jupyter.png)\n\n## Tables\n\n| Col 1 | Col 2 | Col 3  |\n|-------|-------|--------|\n| a     | 1     | red    |\n| b     | 2     | orange | \n| c     | 3     | yellow |\n\n:::{.callout-note}\n\nTables can be fully customized (or you could use raw html).\n\n:::\n\n## Equations\n\n$$\n\\frac{\\partial \\mathrm C}{ \\partial \\mathrm t } + \\frac{1}{2}\\sigma^{2} \\mathrm S^{2}\n\\frac{\\partial^{2} \\mathrm C}{\\partial \\mathrm C^2}\n  + \\mathrm r \\mathrm S \\frac{\\partial \\mathrm C}{\\partial \\mathrm S}\\ =\n  \\mathrm r \\mathrm C \n$$\n\n\n\nRendered document (click on it to open it in a new tab):\n\n\n\npdf\n\n\n\n\n\n\nCode\n\n\n\n\n\n---\ntitle: \"My title\"\nauthor: \"My name\"\nformat:\n  pdf:\n    toc: true\n---\n\n## Heading\n\nSome text.\n\n### Subheading\n\nMore text.\n\n## Formatting\n\nText can be rendered *in italic* or **in bold** as well as [underlined]{.underline}.\n\nYou can use superscripts^2^, subscripts~test~, ~~strikethrough~~, and `inline code`.\n\n&gt; This is a quote.\n\n## Lists\n\n### Unordered\n\n- Item 1\n- Item 2\n- Item 3\n\n### Ordered\n\n1. Item 1\n2. Item 2\n3. Item 3\n\n## Images\n\n![Example image](qmd_jupyter.png)\n\n## Tables\n\n| Col 1 | Col 2 | Col 3  |\n|-------|-------|--------|\n| a     | 1     | red    |\n| b     | 2     | orange | \n| c     | 3     | yellow |\n\n:::{.callout-note}\n\nTables can be fully customized (or you could use raw html).\n\n:::\n\n## Equations\n\n$$\n\\frac{\\partial \\mathrm C}{ \\partial \\mathrm t } + \\frac{1}{2}\\sigma^{2} \\mathrm S^{2}\n\\frac{\\partial^{2} \\mathrm C}{\\partial \\mathrm C^2}\n  + \\mathrm r \\mathrm S \\frac{\\partial \\mathrm C}{\\partial \\mathrm S}\\ =\n  \\mathrm r \\mathrm C \n$$\n\n\n\nRendered document (click on it to open it in a new tab):\n\n\nIn order to export to pdf, you need a TeX distribution. You probably already have one installed on your machine, so you should first try to render or preview a document to pdf to see whether it works. If it doesn’t work, you can install the minimalist distribution TinyTex by running in your terminal:\n\nquarto install tool tinytex\n\n\nHTML with R code blocks\n\n\n\n\n\n\nCode\n\n\n\n\n\n---\ntitle: \"My title\"\nauthor: \"My name\"\ninstitute: \"Simon Fraser University\"\nformat: html\n---\n\n## Heading\n\n### Subheading\n\nSome text.\n\n## Formatting  {#sec-formatting}\n\n::: aside\n\nNote that each heading automatically creates an anchor, making it easy to link to specific sections of your documents.\n\n:::\n\nText can be rendered *in italic* or **in bold** as well as [underlined]{.underline}.\n\nYou can use superscripts^2^, subscripts~test~, ~~strikethrough~~, and `inline code`.\n\n&gt; This is a quote.\n\n## Columns\n\n:::: {.columns}\n\n::: {.column width=\"30%\"}\n\nYou can create columns.\n\n:::\n\n::: {.column width=\"70%\"}\n\nAnd you can set their respective width.\n\n:::\n\n::::\n\n## Lists\n\n- Item 1\n- Item 2\n- Item 3\n\n## Ordered lists\n\n1. Item 1\n2. Item 2\n3. Item 3\n\n## Images\n\n![Example image](qmd_jupyter.png)\n\n## Tables\n\n| Col 1 | Col 2 | Col 3  |\n|-------|-------|--------|\n| a     | 1     | red    |\n| b     | 2     | orange | \n| c     | 3     | yellow |\n\n:::{.callout-note}\n\nTables can be fully customized (or you could use raw html).\n\n:::\n\n## Equations\n\n$$\n\\frac{\\partial \\mathrm C}{ \\partial \\mathrm t } + \\frac{1}{2}\\sigma^{2} \\mathrm S^{2}\n\\frac{\\partial^{2} \\mathrm C}{\\partial \\mathrm C^2}\n  + \\mathrm r \\mathrm S \\frac{\\partial \\mathrm C}{\\partial \\mathrm S}\\ =\n  \\mathrm r \\mathrm C \n$$\n\n## Cross-references\n\nSee @sec-formatting.\n\n*Note that you can add bibliographies, flow charts, the equivalent of HTML \"div\",\nand just so much more. Remember that this is a tiny overview.*\n\n## Let's try some code blocks now\n\n```{r}\n# This is a block that runs\n2 + 3\n```\n\n::: aside\n\nDid you notice that the content of your code blocks can be copied with a click?\nOf course, this is customizable.\n\n:::\n\n```{.r}\n# This is a block that doesn't run\n2 + 3\n```\n\n```{r}\n#| echo: false\n# And this is a block showing only the output\ndata.frame(\n  country = c(\"Canada\", \"USA\", \"Mexico\"),\n  var = c(2.9, 3.1, 4.5)\n)\n```\n\n## Plots\n\n```{r}\nplot(cars)\n```\n\n&lt;br&gt;\nYou can play with options to add a title:\n\n```{r}\n#| fig-cap: \"Stopping distance as a function of speed in cars\"\n\nplot(cars)\n```\n\n&lt;br&gt;\nYou can have more complex multi-plot layouts:\n\n```{r}\n#| layout-ncol: 2\n#| fig-cap: \n#|   - \"Stopping distance as a function of speed in cars\"\n#|   - \"Vapor pressure of mercury as a function of temperature\"\n\nplot(cars)\nplot(pressure)\n```\n\nFor those who have `ggplot2`[^1], you can try that too:\n\n```{r}\nlibrary(ggplot2)\n\nggplot(data = mpg, mapping = aes(x = displ, y = hwy)) + \n  geom_point(mapping = aes(color = class)) + \n  geom_smooth()\n```\n\n[^1]: You can install it with:\n    ```{.r}\n    install.packages(\"ggplot2\")\n    ```\n\n\n\nRendered document (click on it to open it in a new tab):\n\n\n\nBeamer with Python code blocks\nBeamer is LaTeX presentation framework: a way to create beautiful pdf slides.\n\n\n\n\n\n\nCode\n\n\n\n\n\n---\ntitle: \"Some title\"\nauthor: \"Some name\"\nformat: beamer\njupyter: python3\n---\n\n## First slide\n\nWith some content\n\n## Formatting\n\nText can be rendered *in italic* or **in bold** as well as [underlined]{.underline}.\n\nYou can use superscripts^2^, subscripts~test~, ~~strikethrough~~, and `inline code`.\n\n## Lists\n\n- Item 1\n- Item 2\n- Item 3\n\n## Ordered lists\n\n1. Item 1\n2. Item 2\n3. Item 3\n\n## Images\n\n![Example image](qmd_jupyter.png)\n\n## Tables\n\n| Col 1 | Col 2 | Col 3  |\n|-------|-------|--------|\n| a     | 1     | red    |\n| b     | 2     | orange | \n| c     | 3     | yellow |\n\n:::{.callout-note}\n\nTables can be fully customized (or you could use raw html).\n\n:::\n\n## Equations\n\n$$\n\\frac{\\partial \\mathrm C}{ \\partial \\mathrm t } + \\frac{1}{2}\\sigma^{2} \\mathrm S^{2}\n\\frac{\\partial^{2} \\mathrm C}{\\partial \\mathrm C^2}\n  + \\mathrm r \\mathrm S \\frac{\\partial \\mathrm C}{\\partial \\mathrm S}\\ =\n  \\mathrm r \\mathrm C \n$$\n\n## Some basic code block\n\n```{python}\n#| echo: true\n\n2 + 3\n```\n\n## Some plot\n\n```{python}\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Data for plotting\nt = np.arange(0.0, 2.0, 0.01)\ns = 1 + np.sin(2 * np.pi * t)\n\nfig, ax = plt.subplots()\nax.plot(t, s)\n\nax.set(xlabel='time (s)', ylabel='voltage (mV)',\n       title='Here goes the title')\nax.grid()\n\nfig.savefig(\"test.png\")\nplt.show()\n```\n\n\n\nRendered document (click on it to open it in a new tab):",
    "crumbs": [
      "Tools",
      "<em><b>Workshops</b></em>"
    ]
  },
  {
    "objectID": "tools/ws_quarto.html#recording",
    "href": "tools/ws_quarto.html#recording",
    "title": "Authoring scientific documents with Markdown and Quarto",
    "section": "Recording",
    "text": "Recording",
    "crumbs": [
      "Tools",
      "<em><b>Workshops</b></em>"
    ]
  },
  {
    "objectID": "tools/wb_tools3.html",
    "href": "tools/wb_tools3.html",
    "title": "Modern shell utilities",
    "section": "",
    "text": "In recent years, a number of open-source utilities for the Unix shell have emerged. Some are meant as replacements for classic tools with improved performance, better defaults, or nicer-looking outputs; others add novel functionality. Several of them were recently installed on the Alliance clusters.\nIn this webinar I will cover a selection of tools that are very popular, well-maintained, and that have served me well in my daily workflows:\n\nls in colours: eza,\nsmart cd: zoxide,\na cat with wings: bat,\nRIP grep: ripgrep,\nfaster find: fd,\nfuzzy finder: fzf,\nfile system TUIs.\n\nI will also talk about three useful Zsh plugins:\n\na syntax highlighter,\nautosuggestions,\nan improved history searcher.\n\nFor each tool/plugin, I will talk about installation on a personal computer and on the Alliance clusters and I will give live demos.\n\nSlides (Click and wait: this reveal.js presentation may take a little time to load.)",
    "crumbs": [
      "Tools",
      "<b><em>Webinars</em></b>",
      "Modern shell utilities"
    ]
  },
  {
    "objectID": "tools/wb_tools1.html",
    "href": "tools/wb_tools1.html",
    "title": "Fun tools to simplify your life in the shell",
    "section": "",
    "text": "Working in the command-line has many advantages and it is often necessary, but can it be fun?\nIn this webinar, aimed at any command-line user, I intend to demonstrate that yes, it can! by introducing three free and open source utilities which make navigating your system and your outputs a lot easier:\n\nfzf is a simple, yet extremely powerful interactive fuzzy finder allowing for incremental completion and narrowing selection of any command line output. I will show you how to build simple shell functions which harvest its power to instantly refresh your memory on your custom keybindings or aliases, navigate your command history, find and kill processes, and explore and checkout your git commits. After this, you will be able to use fzf for any number of other applications in your work in the command-line.\nautojump lets you jump anywhere you want in your directories in just a few keystrokes (no more of this painful navigation writing down long paths).\nWith the ranger file manager, you can browse (with preview!), open, copy, move, delete, etc. your files and directories in a friendly way from the command line. Added bonus: you can use fzf and autojump within ranger!\n\nWarning: too much fun in the command-line can lead to addiction and geek behaviours. Use in moderation.",
    "crumbs": [
      "Tools",
      "<b><em>Webinars</em></b>",
      "Fun tools for the command line"
    ]
  },
  {
    "objectID": "tools/wb_quarto.html",
    "href": "tools/wb_quarto.html",
    "title": "The new R Markdown:",
    "section": "",
    "text": "This webinar will show you how to easily create beautiful publications (webpages, pdf, websites, presentations, books…)—complete with formatted text, dynamic code and figures with Quarto.\nQuarto is the successor to R Markdown. By combining the powers of Jupyter or knitr with Pandoc, it works with R, but also with Python and Julia code blocks and it adds new functionalities to the old tool.\n\nSlides (Click and wait: this reveal.js presentation may take a little time to load.) \n \n\nVariation of the talk as a staff to staff webinar:",
    "crumbs": [
      "Tools",
      "<b><em>Webinars</em></b>",
      "Quarto: the new R Markdown"
    ]
  },
  {
    "objectID": "tools/wb_lazygit.html",
    "href": "tools/wb_lazygit.html",
    "title": "A great Git UI: lazygit",
    "section": "",
    "text": "While it is important to know how to use Git from the command line, this makes for an austere experience: a series of commands are required to gather information on the state of the working tree, see changes to files, or get a schematic of the commit history. Committing sections of files interactively and other operations are just awkward affairs. On the other hand, the many graphic interfaces for Git are often buggy, slow, and limiting.\nOne option is to write exciting functions with tools such as fzf to make things more friendly and visual. A simpler and more polished option is to use an already built user interface for Git that runs directly in the command line. lazygit is one such open source tool. After years of development, it is a mature, beautiful tool that allows to perform any Git operation in the command line in a convenient, fast, and visual fashion.\nIn this webinar, I will demo how I use lazygit in my daily workflow to run routine as well as more complex Git commands.\n\nSlides (Click and wait: this reveal.js presentation may take a little time to load.)",
    "crumbs": [
      "Tools",
      "<b><em>Webinars</em></b>",
      "A great Git TUI: lazygit"
    ]
  },
  {
    "objectID": "tools/wb_help.html",
    "href": "tools/wb_help.html",
    "title": "So, you are stuck … now what?",
    "section": "",
    "text": "Stack Overflow, Stack Exchange, Discourse forums, and other online platforms … the internet is a treasure trove of online communities where you can find solutions to your coding problems. To have a positive experience and get the answers you need however, you have to know where to ask, how to ask, and when not to ask: if countless people are willing to give you their time for free, they usually expect that you do your part.\nIn this webinar, I will present key online sites, their functioning, and their culture; then I will go over the magic trick to get answers to your questions: knowing how to create minimum reproducible examples. I will not focus on any particular language as the principles (how to create a minimal dataset, how to deal with private data, how to create self-sufficient code, how to reproduce the problem, etc.) can apply to any language.\n\nSlides (Click and wait: this reveal.js presentation may take a little time to load.)",
    "crumbs": [
      "Tools",
      "<b><em>Webinars</em></b>",
      "Getting help online"
    ]
  },
  {
    "objectID": "tools/wb_dvc.html",
    "href": "tools/wb_dvc.html",
    "title": "Version control for data science and machine learning with DVC",
    "section": "",
    "text": "Data version control (DVC) is an open source tool that brings all the versioning and collaboration capabilities you use on your code with Git to your data and machine learning workflow.\nIf you use datasets in your work, it makes it easy to track their evolution.\nIf you are in the field of machine learning, it additionally allows you to track your models, manage your pipelines from parameters to metrics, collaborate on your experiments, and integrate with the continuous integration tool for machine learning projects CML.\nThis webinar will show you how to get started with DVC, first in the simple case where you just want to put your data under version control, then in the more complex situation where you want to manage your machine learning workflow in a more organized and reproducible fashion.\n\nSlides (Click and wait: this reveal.js presentation may take a little time to load.)",
    "crumbs": [
      "Tools",
      "<b><em>Webinars</em></b>",
      "Data version control with DVC"
    ]
  },
  {
    "objectID": "tools/index.html",
    "href": "tools/index.html",
    "title": "Tools",
    "section": "",
    "text": "Workshops\nVarious tools\n\n\n\n\n60 min webinars\nVarious tools",
    "crumbs": [
      "Tools",
      "<br>&nbsp;<em><b>Tools</b></em><br><br>"
    ]
  },
  {
    "objectID": "talks/2024_bccai_part2_slides.html#simon-fraser-university",
    "href": "talks/2024_bccai_part2_slides.html#simon-fraser-university",
    "title": "Harnessing big datafor agricultural excellence",
    "section": "Simon Fraser University",
    "text": "Simon Fraser University\n\nSFU hosts the Cedar supercomputer—a cluster of 100,400 CPUs and 1,352 GPUs soon to be replaced by an even larger computer cluster"
  },
  {
    "objectID": "talks/2024_bccai_part2_slides.html#simon-fraser-university-1",
    "href": "talks/2024_bccai_part2_slides.html#simon-fraser-university-1",
    "title": "Harnessing big datafor agricultural excellence",
    "section": "Simon Fraser University",
    "text": "Simon Fraser University\nSFU also works with the Digital Research Alliance of Canada to offer researchers large amounts of computing power to solve challenging data and technology problems, as well as training to optimize their solutions"
  },
  {
    "objectID": "talks/2024_bccai_part2_slides.html#sfus-big-data-hub",
    "href": "talks/2024_bccai_part2_slides.html#sfus-big-data-hub",
    "title": "Harnessing big datafor agricultural excellence",
    "section": "SFU’s Big Data Hub",
    "text": "SFU’s Big Data Hub\n\nSince 2016, Simon Fraser University’s Big Data Hub has been offering workshops, events, and consulting services to researchers and industry partners helping them remain at the top of the fast evolving data landscape"
  },
  {
    "objectID": "talks/2024_bccai_part2_slides.html#bc-centre-for-agritech-innovation",
    "href": "talks/2024_bccai_part2_slides.html#bc-centre-for-agritech-innovation",
    "title": "Harnessing big datafor agricultural excellence",
    "section": "BC Centre for Agritech Innovation",
    "text": "BC Centre for Agritech Innovation\n\nSince 2022, SFU BCCAI has been helping small and medium enterprises in the farming industry to embrace technology driven solutions"
  },
  {
    "objectID": "talks/2024_bccai_part2_slides.html#bc-centre-for-agritech-innovation-1",
    "href": "talks/2024_bccai_part2_slides.html#bc-centre-for-agritech-innovation-1",
    "title": "Harnessing big datafor agricultural excellence",
    "section": "BC Centre for Agritech Innovation",
    "text": "BC Centre for Agritech Innovation\n\n\n\n\n\n\n\n\n\n\nsupport\n\nAgritech projects\n\n\n\ntraining\n\nTraining & upscaling\n\n\n\n\nnetwork\n\nAgritech network"
  },
  {
    "objectID": "talks/2024_bccai_part2_slides.html#session-1",
    "href": "talks/2024_bccai_part2_slides.html#session-1",
    "title": "Harnessing big datafor agricultural excellence",
    "section": "Session 1",
    "text": "Session 1\n\n\n\nYesterday\n\nA (hopefully) friendly lecture to:\n\nDemystify big data\nDemonstrate the critical importance of big data in agriculture and farming\n\n\nIf you missed the session, you can find the slides here"
  },
  {
    "objectID": "talks/2024_bccai_part2_slides.html#session-1-recap",
    "href": "talks/2024_bccai_part2_slides.html#session-1-recap",
    "title": "Harnessing big datafor agricultural excellence",
    "section": "Session 1 recap",
    "text": "Session 1 recap\n\n\nBig data is defined by the 3 “V”:\n\nVolume (lots of data is generated)\nVariety (images, sounds, text…)\nVelocity (generated continuously)"
  },
  {
    "objectID": "talks/2024_bccai_part2_slides.html#session-1-recap-1",
    "href": "talks/2024_bccai_part2_slides.html#session-1-recap-1",
    "title": "Harnessing big datafor agricultural excellence",
    "section": "Session 1 recap",
    "text": "Session 1 recap\nBig data has become crucial because it allows to train artificial intelligence models"
  },
  {
    "objectID": "talks/2024_bccai_part2_slides.html#session-1-recap-2",
    "href": "talks/2024_bccai_part2_slides.html#session-1-recap-2",
    "title": "Harnessing big datafor agricultural excellence",
    "section": "Session 1 recap",
    "text": "Session 1 recap\n\n\nOnce trained, those models are extremely powerful and capable of performing tasks impossible for traditional computer programs\n(e.g. creating art, generating human text, chat bots, excellent forecasting and optimization, computer vision, self-driving cars…)"
  },
  {
    "objectID": "talks/2024_bccai_part2_slides.html#session-1-recap-3",
    "href": "talks/2024_bccai_part2_slides.html#session-1-recap-3",
    "title": "Harnessing big datafor agricultural excellence",
    "section": "Session 1 recap",
    "text": "Session 1 recap\n\n\nBig data and AI are transforming all sectors, including agriculture because they allow:\n\nReal time monitoring\nBetter decision making\nOptimizations\nAutomation of tasks"
  },
  {
    "objectID": "talks/2024_bccai_part2_slides.html#session-1-recap-4",
    "href": "talks/2024_bccai_part2_slides.html#session-1-recap-4",
    "title": "Harnessing big datafor agricultural excellence",
    "section": "Session 1 recap",
    "text": "Session 1 recap\nHowever there are challenges to the implementation of such transformative methods\n\nInfrastructure development\nSkill gaps among agricultural professionals\n\n\nWe are here to help\nThis is the goal of today’s session"
  },
  {
    "objectID": "talks/2024_bccai_part2_slides.html#session-2",
    "href": "talks/2024_bccai_part2_slides.html#session-2",
    "title": "Harnessing big datafor agricultural excellence",
    "section": "Session 2",
    "text": "Session 2\n\n\n\nToday\n\nAn interactive workshop to:\n\nBrainstorm on how big data can benefit your operation\nHelp you make the transition to smart farming"
  },
  {
    "objectID": "talks/2024_bccai_part2_slides.html#data-management",
    "href": "talks/2024_bccai_part2_slides.html#data-management",
    "title": "Harnessing big datafor agricultural excellence",
    "section": "Data management",
    "text": "Data management\n\n\nFirst, let’s focus on your data\nI will ask you to think about:\n\nThe data you use for your operation\nHow you are collecting it and storing it\nHow you could automate this"
  },
  {
    "objectID": "talks/2024_bccai_part2_slides.html#analytics",
    "href": "talks/2024_bccai_part2_slides.html#analytics",
    "title": "Harnessing big datafor agricultural excellence",
    "section": "Analytics",
    "text": "Analytics\n\n\nNow, let’s think about what this data is actually used for:\n\nWhat is purpose of this data?\nHow do you analyse it?\nWhat could be the benefits of using AI to process your data?"
  },
  {
    "objectID": "talks/2024_bccai_part2_slides.html#challenges",
    "href": "talks/2024_bccai_part2_slides.html#challenges",
    "title": "Harnessing big datafor agricultural excellence",
    "section": "Challenges",
    "text": "Challenges\n\n\nWhat are the challenges of such an implementation\n\nat the financial level\nat the practical level\ndue to knowledge gaps\n\n\n\n\n\nFrom xkcd.com"
  },
  {
    "objectID": "talks/2024_bccai_part2_slides.html#who-to-turn-to",
    "href": "talks/2024_bccai_part2_slides.html#who-to-turn-to",
    "title": "Harnessing big datafor agricultural excellence",
    "section": "Who to turn to?",
    "text": "Who to turn to?\n\nConnecting with other operators can be extremely powerful in this transformation\n\n\nYou may also need to talk with researchers\n\n\nYou will need to find a technology provider\n\n\nHere my colleagues from the Big Data Hub and the BCCAI will jump in to orientate you\n\nSFU’s Big Data Hub\nWebsite\nContact us\nConsultation services\nPartnerships\n\nBC Center for Agriculture Innovation\nWebsite\nContact us\nAgritech development program\nTraining"
  },
  {
    "objectID": "talks/2024_bccai_part2_slides.html#communication-with-experts",
    "href": "talks/2024_bccai_part2_slides.html#communication-with-experts",
    "title": "Harnessing big datafor agricultural excellence",
    "section": "Communication with experts",
    "text": "Communication with experts\n\n\nYou need basic concepts and vocabulary to communicate your needs to technology providers and researchers\n\nWhat concept do you feel that you are lacking and that we should cover?\nVocabulary clarification\n\n\n\n\n\n\n\nFrom xkcd.com"
  },
  {
    "objectID": "talks/2024_bccai_part2_slides.html#understanding-neural-networks",
    "href": "talks/2024_bccai_part2_slides.html#understanding-neural-networks",
    "title": "Harnessing big datafor agricultural excellence",
    "section": "Understanding neural networks",
    "text": "Understanding neural networks\n3Blue1Brown by Grant Sanderson has a series of 4 videos on neural networks which is easy to watch, fun, and does an excellent job at introducing the functioning of a simple neural network"
  },
  {
    "objectID": "talks/2024_bccai_part2_slides.html#literature",
    "href": "talks/2024_bccai_part2_slides.html#literature",
    "title": "Harnessing big datafor agricultural excellence",
    "section": "Literature",
    "text": "Literature\nOpen-access preprints:\nArxiv Sanity Preserver by Andrej Karpathy\nML papers in the computer science category on arXiv\nML papers in the stats category on arXiv\nDistill ML research online journal"
  },
  {
    "objectID": "talks/2024_bccai_part2_slides.html#acknowledgements",
    "href": "talks/2024_bccai_part2_slides.html#acknowledgements",
    "title": "Harnessing big datafor agricultural excellence",
    "section": "Acknowledgements",
    "text": "Acknowledgements\n\n\n\n\n\n\n\n Carson Li (BCCAI) suggested an outline for this talk\n Ian Chan (BCCAI) provided copious feedback"
  },
  {
    "objectID": "talks/2024_bccai_part2_slides.html#feedback",
    "href": "talks/2024_bccai_part2_slides.html#feedback",
    "title": "Harnessing big datafor agricultural excellence",
    "section": "Feedback",
    "text": "Feedback\nPlease give us feedback by scanning the QR code:\n\n\n\nThank you!"
  },
  {
    "objectID": "talks/2024_bccai.html",
    "href": "talks/2024_bccai.html",
    "title": "Big data in the agrotech industry",
    "section": "",
    "text": "Presented at the Agricultural Excellence Conference in Abbotsford, BC, in November 2024, in collaboration with Simon Fraser University’s Big Data Hub and BC Centre for Agritech Innovation.\n\n\nThis two-part workshop illustrates how big data is used in the agriculture sector, clarifies concepts, and provides the basis to communicate in the field.\nSession 1 demystifies big data and covers three case studies in the agrotech industry, while session 2 presents an overview of big data challenges and provides the tools necessary to communicate with consultants to find solutions.\n\nSession 1 (Click and wait: the presentation might take a few instants to load)\nSession 2 (Click and wait: the presentation might take a few instants to load)",
    "crumbs": [
      "<br>&nbsp;<em><b>Conference talks</b></em><br><br>",
      "Big data in the agrotech industry"
    ]
  },
  {
    "objectID": "talks/2023_driconnect.html",
    "href": "talks/2023_driconnect.html",
    "title": "The instruments for advanced research computing are here, but are researchers ready?",
    "section": "",
    "text": "Presented at the DRI (Digital Research Infrastructure) Connect in Vancouver, BC, in June 2023.\n\nThe current times are exciting: we are witnessing a growth of computing power while the open source community is vigorously building impressive machine learning and scientific programming tools.\nThis boom of hardware and software assets cannot however translate into research if graduate students aren’t able to take advantage of it. Curricula often lack training pertinent to the use of such resources. Worse yet, in many fields faculties and PIs don’t have the necessary background to help their students with high-performance programming. The training team at Simon Fraser University Research Computing Group aims to fill this gap in the West on behalf of the Alliance and all Western Canadian universities.\nThis talk will present an overview of the training we provide, from introductory skill sets for researchers new to ARC and HPC to advanced topics in parallel programming.\n\nSlides",
    "crumbs": [
      "<br>&nbsp;<em><b>Conference talks</b></em><br><br>",
      "Advanced research instruments"
    ]
  },
  {
    "objectID": "r/ws_hss_intro.html",
    "href": "r/ws_hss_intro.html",
    "title": "Introduction to R for the humanities",
    "section": "",
    "text": "R is a free and open-source programming language for statistical computing, modelling, and graphics, with an unbeatable collection of statistical packages. It is extremely popular in some academic fields such as statistics, biology, bioinformatics, data mining, data analysis, and linguistics.\nThis introductory course does not assume any prior knowledge.",
    "crumbs": [
      "R",
      "<b><em>Workshops</em></b>",
      "Intro R for the humanities"
    ]
  },
  {
    "objectID": "r/ws_hss_intro.html#running-r",
    "href": "r/ws_hss_intro.html#running-r",
    "title": "Introduction to R for the humanities",
    "section": "Running R",
    "text": "Running R\nR being an interpreted language, it can be run non-interactively or interactively.\n\nRunning R non-interactively\nIf you write code in a text file (called a script), you can then execute it with:\nRscript my_script.R\n\nThe command to execute scripts is Rscript rather than R.\nBy convention, R scripts take the extension .R.\n\n\n\nRunning R interactively\nThere are several ways to run R interactively.\n\nDirectly in the console (the name for the R shell):\n\n\n\nIn Jupyter with the R kernel (IRkernel package).\nIn another IDE (e.g. in Emacs with ESS).\nIn the RStudio IDE.\n\nThe RStudio IDE is popular and this is what we will use today. RStudio can can be run locally, but for this course, we will use an RStudio server.\n\n\nAccessing our RStudio server\nFor this workshop, we will use a temporary RStudio server.\nTo access it, go to the website given during the workshop and sign in using the username and password you will be given (you can ignore the OTP entry).\nThis will take you to our JupyterHub. There, click on the “RStudio” button and our RStudio server will open in a new tab.\n\n\nUsing RStudio\nFor those unfamiliar with the RStudio IDE, you can download the following cheatsheet:\n\n\n\nfrom Posit Cheatsheets",
    "crumbs": [
      "R",
      "<b><em>Workshops</em></b>",
      "Intro R for the humanities"
    ]
  },
  {
    "objectID": "r/ws_hss_intro.html#help-and-documentation",
    "href": "r/ws_hss_intro.html#help-and-documentation",
    "title": "Introduction to R for the humanities",
    "section": "Help and documentation",
    "text": "Help and documentation\nFor some general documentation on R, you can run:\nhelp.start()\nTo get help on a function (e.g. sum), you can run:\nhelp(sum)\nDepending on your settings, this will open a documentation for sum in a pager or in your browser.",
    "crumbs": [
      "R",
      "<b><em>Workshops</em></b>",
      "Intro R for the humanities"
    ]
  },
  {
    "objectID": "r/ws_hss_intro.html#basic-syntax",
    "href": "r/ws_hss_intro.html#basic-syntax",
    "title": "Introduction to R for the humanities",
    "section": "Basic syntax",
    "text": "Basic syntax\n\nAssignment\nR can accept the equal sign (=) for assignments, but it is more idiomatic to use the assignment sign (&lt;-) whenever you bind a name to a value and to use the equal sign everywhere else.\n\na &lt;- 3\n\nOnce you have bound a name to a value, you can recall the value with that name:\n\na  # Note that you do not need to use a print() function in R\n\n[1] 3\n\n\nYou can remove an object from the environment by deleting its name:\n\nrm(a)\na\n\nError in eval(expr, envir, enclos): object 'a' not found\n\n\nThe garbage collector will take care of deleting the object itself from memory.\n\n\nComments\nAnything to the left of # is a comment and is ignored by R:\n\n# This is an inline comment\n\na &lt;- 3  # This is also a comment",
    "crumbs": [
      "R",
      "<b><em>Workshops</em></b>",
      "Intro R for the humanities"
    ]
  },
  {
    "objectID": "r/ws_hss_intro.html#data-types-and-structures",
    "href": "r/ws_hss_intro.html#data-types-and-structures",
    "title": "Introduction to R for the humanities",
    "section": "Data types and structures",
    "text": "Data types and structures\n\n\n\nDimension\nHomogeneous\nHeterogeneous\n\n\n\n\n1 d\nAtomic vector\nList\n\n\n2 d\nMatrix\nData frame\n\n\n3 d\nArray\n\n\n\n\n\nAtomic vectors\n\nvec &lt;- c(2, 4, 1)\nvec\n\n[1] 2 4 1\n\ntypeof(vec)\n\n[1] \"double\"\n\nstr(vec)\n\n num [1:3] 2 4 1\n\n\n\nvec &lt;- c(TRUE, TRUE, NA, FALSE)\nvec\n\n[1]  TRUE  TRUE    NA FALSE\n\ntypeof(vec)\n\n[1] \"logical\"\n\nstr(vec)\n\n logi [1:4] TRUE TRUE NA FALSE\n\n\n\nNA (“Not Available”) is a logical constant of length one. It is an indicator for a missing value.\n\nVectors are homogeneous, so all elements need to be of the same type.\nIf you use elements of different types, R will convert some of them to ensure that they become of the same type:\n\nvec &lt;- c(\"This is a string\", 3, \"test\")\nvec\n\n[1] \"This is a string\" \"3\"                \"test\"            \n\ntypeof(vec)\n\n[1] \"character\"\n\nstr(vec)\n\n chr [1:3] \"This is a string\" \"3\" \"test\"\n\n\n\nvec &lt;- c(TRUE, 3, FALSE)\nvec\n\n[1] 1 3 0\n\ntypeof(vec)\n\n[1] \"double\"\n\nstr(vec)\n\n num [1:3] 1 3 0\n\n\n\n\nData frames\nData frames contain tabular data. Under the hood, a data frame is a list of vectors.\n\ndat &lt;- data.frame(\n  country = c(\"Canada\", \"USA\", \"Mexico\"),\n  var = c(2.9, 3.1, 4.5)\n)\ndat\n\n  country var\n1  Canada 2.9\n2     USA 3.1\n3  Mexico 4.5\n\ntypeof(dat)\n\n[1] \"list\"\n\nstr(dat)\n\n'data.frame':   3 obs. of  2 variables:\n $ country: chr  \"Canada\" \"USA\" \"Mexico\"\n $ var    : num  2.9 3.1 4.5\n\nlength(dat)\n\n[1] 2\n\ndim(dat)\n\n[1] 3 2",
    "crumbs": [
      "R",
      "<b><em>Workshops</em></b>",
      "Intro R for the humanities"
    ]
  },
  {
    "objectID": "r/ws_hss_intro.html#function-definition",
    "href": "r/ws_hss_intro.html#function-definition",
    "title": "Introduction to R for the humanities",
    "section": "Function definition",
    "text": "Function definition\n\ncompare &lt;- function(x, y) {\n  x == y\n}\n\nWe can now use our function:\n\ncompare(2, 3)\n\n[1] FALSE\n\n\nNote that the result of the last statement is printed automatically:\n\ntest &lt;- function(x, y) {\n  x\n  y\n}\ntest(2, 3)\n\n[1] 3\n\n\nIf you want to return other results, you need to explicitly use the print() function:\n\ntest &lt;- function(x, y) {\n  print(x)\n  y\n}\ntest(2, 3)\n\n[1] 2\n\n\n[1] 3",
    "crumbs": [
      "R",
      "<b><em>Workshops</em></b>",
      "Intro R for the humanities"
    ]
  },
  {
    "objectID": "r/ws_hss_intro.html#control-flow",
    "href": "r/ws_hss_intro.html#control-flow",
    "title": "Introduction to R for the humanities",
    "section": "Control flow",
    "text": "Control flow\n\nConditionals\n\ntest_sign &lt;- function(x) {\n  if (x &gt; 0) {\n    \"x is positif\"\n  } else if (x &lt; 0) {\n    \"x is negatif\"\n  } else {\n    \"x is equal to zero\"\n  }\n}\n\n\ntest_sign(3)\n\n[1] \"x is positif\"\n\ntest_sign(-2)\n\n[1] \"x is negatif\"\n\ntest_sign(0)\n\n[1] \"x is equal to zero\"\n\n\n\n\nLoops\n\nfor (i in 1:10) {\n  print(i)\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n[1] 7\n[1] 8\n[1] 9\n[1] 10\n\n\nNotice that here we need to use the print() function.",
    "crumbs": [
      "R",
      "<b><em>Workshops</em></b>",
      "Intro R for the humanities"
    ]
  },
  {
    "objectID": "r/ws_hss_intro.html#packages",
    "href": "r/ws_hss_intro.html#packages",
    "title": "Introduction to R for the humanities",
    "section": "Packages",
    "text": "Packages\nPackages are a set of functions and/or data that add functionality to R.\n\nLooking for packages\n\nPackage finder\nYour peers and the literature\n\n\n\nPackage documentation\n\nList of CRAN packages\nPackage documentation\n\n\n\nManaging R packages\nR packages can be installed, updated, and removed from within R:\ninstall.packages(\"package-name\")\nremove.packages(\"package-name\")\nupdate_packages()\n\n\nLoading packages\nTo make a package available in an R session, you load it with the library() function.\n\nExample:\n\nlibrary(readxl)\nAlternatively, you can access a function from a package without loading it with the syntax: package::function().\n\nExample:\n\nreadxl::read_excel(\"file.xlsx\")",
    "crumbs": [
      "R",
      "<b><em>Workshops</em></b>",
      "Intro R for the humanities"
    ]
  },
  {
    "objectID": "r/ws_hss_intro.html#publishing",
    "href": "r/ws_hss_intro.html#publishing",
    "title": "Introduction to R for the humanities",
    "section": "Publishing",
    "text": "Publishing\nYou might have heard of R Markdown. It allows for the creation of dynamic publication-quality documents mixing code blocks, text, graphs…\nThe team which created R Markdown has now created an even better tool: Quarto. If you are interested in an introduction to this tool, you can have a look at our workshop or our webinar on Quarto.",
    "crumbs": [
      "R",
      "<b><em>Workshops</em></b>",
      "Intro R for the humanities"
    ]
  },
  {
    "objectID": "r/ws_hss_intro.html#resources",
    "href": "r/ws_hss_intro.html#resources",
    "title": "Introduction to R for the humanities",
    "section": "Resources",
    "text": "Resources\n\nAlliance wiki\n\nR page\n\n\n\nR main site\n\nDownload page\n\n\n\nRStudio\n\nPosit site (Posit is the brand new name of the RStudio company)\nPosit cheatsheets\n\n\n\nSoftware Carpentry online workshop\n\nData analysis using R in the digital humanities\n\n\n\nOnline book\n\nR for Data Science (heavily based on the tidyverse)",
    "crumbs": [
      "R",
      "<b><em>Workshops</em></b>",
      "Intro R for the humanities"
    ]
  },
  {
    "objectID": "r/ws_hss_intro.html#recording",
    "href": "r/ws_hss_intro.html#recording",
    "title": "Introduction to R for the humanities",
    "section": "Recording",
    "text": "Recording\n\nVideos of this workshop for the Digital Research Alliance of Canada HSS Winter Series 2023:\n\n\nFirst part\n\n\n\nSecond part",
    "crumbs": [
      "R",
      "<b><em>Workshops</em></b>",
      "Intro R for the humanities"
    ]
  },
  {
    "objectID": "r/ws_demo_slides.html#history",
    "href": "r/ws_demo_slides.html#history",
    "title": "A little demo of programming in",
    "section": "History",
    "text": "History\nCreated by academic statisticians Ross Ihaka and Robert Gentleman\nThe name comes from the language S which was a great influence as well as the first initial of the developers\nLaunched in 1993\nA GNU Project since 1997"
  },
  {
    "objectID": "r/ws_demo_slides.html#why-r",
    "href": "r/ws_demo_slides.html#why-r",
    "title": "A little demo of programming in",
    "section": "Why R?",
    "text": "Why R?\nFree and open source\nHigh-level and easy to learn\nLarge community\nVery well documented\nUnequalled number of statistics and modelling packages\nIntegrated package manager\nEasy connection with fast compiled languages such as C and C++\nPowerful IDEs (e.g. RStudio, ESS, Jupyter)"
  },
  {
    "objectID": "r/ws_demo_slides.html#for-whom",
    "href": "r/ws_demo_slides.html#for-whom",
    "title": "A little demo of programming in",
    "section": "For whom?",
    "text": "For whom?\nFields with heavy statistics, modelling, or Bayesian inference such as biology, linguistics, economics, or statistics\nData science"
  },
  {
    "objectID": "r/ws_demo_slides.html#downsides",
    "href": "r/ws_demo_slides.html#downsides",
    "title": "A little demo of programming in",
    "section": "Downsides",
    "text": "Downsides\nInconsistent syntax full of quirks\nSlow\nLarge memory usage"
  },
  {
    "objectID": "r/ws_demo_slides.html#an-interpreted-language",
    "href": "r/ws_demo_slides.html#an-interpreted-language",
    "title": "A little demo of programming in",
    "section": "An interpreted language",
    "text": "An interpreted language\nR being an interpreted language, it can be run non-interactively or interactively"
  },
  {
    "objectID": "r/ws_demo_slides.html#running-r-non-interactively",
    "href": "r/ws_demo_slides.html#running-r-non-interactively",
    "title": "A little demo of programming in",
    "section": "Running R non-interactively",
    "text": "Running R non-interactively\nIf you write code in a text file (called a script), you can then execute it with:\nRscript my_script.R\n\nThe command to execute scripts is Rscript rather than R\nBy convention, R scripts take the extension .R"
  },
  {
    "objectID": "r/ws_demo_slides.html#running-r-interactively",
    "href": "r/ws_demo_slides.html#running-r-interactively",
    "title": "A little demo of programming in",
    "section": "Running R interactively",
    "text": "Running R interactively\nThere are several ways to run R interactively:\n\ndirectly in the console (the name for the R shell)\nin Jupyter with the R kernel (IRkernel package)\nin another IDE (e.g. in Emacs with ESS)\nin the RStudio IDE"
  },
  {
    "objectID": "r/ws_demo_slides.html#documentation",
    "href": "r/ws_demo_slides.html#documentation",
    "title": "A little demo of programming in",
    "section": "Documentation",
    "text": "Documentation\nThe R documentation is excellent. Get info on any function with ? (e.g. ?sum)"
  },
  {
    "objectID": "r/ws_demo_slides.html#basic-operations",
    "href": "r/ws_demo_slides.html#basic-operations",
    "title": "A little demo of programming in",
    "section": "Basic operations",
    "text": "Basic operations\n\na &lt;- 5\n4 + a\n\n[1] 9\n\nc &lt;- c(2, 4, 1)\nc * 5\n\n[1] 10 20  5\n\nsum(c)\n\n[1] 7"
  },
  {
    "objectID": "r/ws_demo_slides.html#statistics-probabilities-and-modelling",
    "href": "r/ws_demo_slides.html#statistics-probabilities-and-modelling",
    "title": "A little demo of programming in",
    "section": "Statistics, probabilities, and modelling",
    "text": "Statistics, probabilities, and modelling\nR really shines when it comes to statistics and modelling\nWe will spend the rest of the hour diving into very complex and heavy Bayesian statistics"
  },
  {
    "objectID": "r/ws_demo_slides.html#just-kidding",
    "href": "r/ws_demo_slides.html#just-kidding",
    "title": "A little demo of programming in",
    "section": "Just kidding 🙂",
    "text": "Just kidding 🙂\nIn this demo, I will stick to fun topics"
  },
  {
    "objectID": "r/ws_demo_slides.html#datasets",
    "href": "r/ws_demo_slides.html#datasets",
    "title": "A little demo of programming in",
    "section": "Datasets",
    "text": "Datasets\nR comes with a number of datasets. You can get a list by running data()"
  },
  {
    "objectID": "r/ws_demo_slides.html#datasets-1",
    "href": "r/ws_demo_slides.html#datasets-1",
    "title": "A little demo of programming in",
    "section": "Datasets",
    "text": "Datasets\nThe ggplot2 package provides additional ones, such as the mpg dataset:\n\nlibrary(ggplot2)\nhead(mpg)\n\n# A tibble: 6 × 11\n  manufacturer model displ  year   cyl trans      drv     cty   hwy fl   \n  &lt;chr&gt;        &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;      &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;\n1 audi         a4      1.8  1999     4 auto(l5)   f        18    29 p    \n2 audi         a4      1.8  1999     4 manual(m5) f        21    29 p    \n3 audi         a4      2    2008     4 manual(m6) f        20    31 p    \n4 audi         a4      2    2008     4 auto(av)   f        21    30 p    \n5 audi         a4      2.8  1999     6 auto(l5)   f        16    26 p    \n6 audi         a4      2.8  1999     6 manual(m5) f        18    26 p    \n  class  \n  &lt;chr&gt;  \n1 compact\n2 compact\n3 compact\n4 compact\n5 compact\n6 compact"
  },
  {
    "objectID": "r/ws_demo_slides.html#the-canvas",
    "href": "r/ws_demo_slides.html#the-canvas",
    "title": "A little demo of programming in",
    "section": "The canvas",
    "text": "The canvas\n The first component is the data:\n\nggplot(data = mpg)"
  },
  {
    "objectID": "r/ws_demo_slides.html#the-canvas-1",
    "href": "r/ws_demo_slides.html#the-canvas-1",
    "title": "A little demo of programming in",
    "section": "The canvas",
    "text": "The canvas\nThe second component sets the way variables are mapped on the axes. This is done with the aes() (aesthetics) function:\n\nggplot(data = mpg, mapping = aes(x = displ, y = hwy))"
  },
  {
    "objectID": "r/ws_demo_slides.html#geometric-representations-of-the-data",
    "href": "r/ws_demo_slides.html#geometric-representations-of-the-data",
    "title": "A little demo of programming in",
    "section": "Geometric representations of the data",
    "text": "Geometric representations of the data\nOnto this canvas, we can add “geoms” (geometrical objects) representing the data.\nTo represent the data as a scatterplot, we use the geom_point() function:\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point()"
  },
  {
    "objectID": "r/ws_demo_slides.html#colour-coding-based-on-variables",
    "href": "r/ws_demo_slides.html#colour-coding-based-on-variables",
    "title": "A little demo of programming in",
    "section": "Colour-coding based on variables",
    "text": "Colour-coding based on variables\nWe can colour-code the points in the scatterplot based on the drv variable, showing the lower fuel efficiency of 4WD vehicles:\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = drv))"
  },
  {
    "objectID": "r/ws_demo_slides.html#colour-coding-based-on-variables-1",
    "href": "r/ws_demo_slides.html#colour-coding-based-on-variables-1",
    "title": "A little demo of programming in",
    "section": "Colour-coding based on variables",
    "text": "Colour-coding based on variables\nOr we can colour-code them based on the class variable:\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = class))"
  },
  {
    "objectID": "r/ws_demo_slides.html#multiple-geoms",
    "href": "r/ws_demo_slides.html#multiple-geoms",
    "title": "A little demo of programming in",
    "section": "Multiple geoms",
    "text": "Multiple geoms\nMultiple “geoms” can be added on top of each other. For instance, we can add a smoothed conditional means function that aids at seeing patterns in the data with geom_smooth():\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = class)) +\n  geom_smooth()"
  },
  {
    "objectID": "r/ws_demo_slides.html#colour-scales",
    "href": "r/ws_demo_slides.html#colour-scales",
    "title": "A little demo of programming in",
    "section": "Colour scales",
    "text": "Colour scales"
  },
  {
    "objectID": "r/ws_demo_slides.html#ggplot-extensions",
    "href": "r/ws_demo_slides.html#ggplot-extensions",
    "title": "A little demo of programming in",
    "section": "ggplot extensions",
    "text": "ggplot extensions\nMany packages build on ggplot2 and add functionality"
  },
  {
    "objectID": "r/ws_demo_slides.html#combining-plots",
    "href": "r/ws_demo_slides.html#combining-plots",
    "title": "A little demo of programming in",
    "section": "Combining plots",
    "text": "Combining plots\nOne ggplot extension is the patchwork package which allows to combine multiple plots on the same frame"
  },
  {
    "objectID": "r/ws_demo_slides.html#html-and-css",
    "href": "r/ws_demo_slides.html#html-and-css",
    "title": "A little demo of programming in",
    "section": "HTML and CSS",
    "text": "HTML and CSS\nHyperText Markup Language (HTML) is the standard markup language for websites: it encodes the information related to the formatting and structure of webpages. Additionally, some of the customization can be stored in Cascading Style Sheets (CSS) files.\nHTML uses tags of the form:\n&lt;some_tag&gt;Your content&lt;/some_tag&gt;\nSome tags have attributes:\n&lt;some_tag attribute_name=\"attribute value\"&gt;Your content&lt;/some_tag&gt;\n\nExamples:\n\n\n&lt;h2&gt;This is a heading of level 2&lt;/h2&gt;\n&lt;b&gt;This is bold&lt;/b&gt;\n&lt;a href=\"https://some.url\"&gt;This is the text for a link&lt;/a&gt;"
  },
  {
    "objectID": "r/ws_demo_slides.html#example-for-this-workshop",
    "href": "r/ws_demo_slides.html#example-for-this-workshop",
    "title": "A little demo of programming in",
    "section": "Example for this workshop",
    "text": "Example for this workshop\nWe will use a website from the University of Tennessee containing a database of PhD theses from that university\nOur goal is to scrape data from this site to produce a dataframe with the date, major, and advisor for each dissertation\n\nWe will only do this for the first page which contains the links to the 100 most recent theses. If you really wanted to gather all the data, you would have to do this for all pages"
  },
  {
    "objectID": "r/ws_demo_slides.html#package",
    "href": "r/ws_demo_slides.html#package",
    "title": "A little demo of programming in",
    "section": "Package",
    "text": "Package\nTo do all this, we will use the package rvest, part of the tidyverse (a modern set of R packages). It is a package influenced by the popular Python package Beautiful Soup and it makes scraping websites with R really easy\nLet’s load it:\n\nlibrary(rvest)"
  },
  {
    "objectID": "r/ws_demo_slides.html#read-in-html-from-main-site",
    "href": "r/ws_demo_slides.html#read-in-html-from-main-site",
    "title": "A little demo of programming in",
    "section": "Read in HTML from main site",
    "text": "Read in HTML from main site\nAs mentioned above, our site is the database of PhD dissertations from the University of Tennessee\nLet’s create a character vector with the URL:\n\nurl &lt;- \"https://trace.tennessee.edu/utk_graddiss/index.html\"\n\nFirst, we read in the html data from that page:\n\nhtml &lt;- read_html(url)\n\nLet’s have a look at the raw data:\n\nhtml\n\n{html_document}\n&lt;html lang=\"en\"&gt;\n[1] &lt;head&gt;\\n&lt;meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8 ...\n[2] &lt;body&gt;\\n&lt;!-- FILE /srv/sequoia/main/data/trace.tennessee.edu/assets/heade ..."
  },
  {
    "objectID": "r/ws_demo_slides.html#extract-all-urls",
    "href": "r/ws_demo_slides.html#extract-all-urls",
    "title": "A little demo of programming in",
    "section": "Extract all URLs",
    "text": "Extract all URLs\n\ndat &lt;- html %&gt;% html_elements(\".article-listing a\")\n\nError in dyn.load(file, DLLpath = DLLpath, ...): unable to load shared object '/home/marie/R/lib/stringi/libs/stringi.so':\n  libicui18n.so.75: cannot open shared object file: No such file or directory\n\ndat\n\nError: object 'dat' not found"
  },
  {
    "objectID": "r/ws_demo_slides.html#extract-all-urls-1",
    "href": "r/ws_demo_slides.html#extract-all-urls-1",
    "title": "A little demo of programming in",
    "section": "Extract all URLs",
    "text": "Extract all URLs\nWe now have a list of lists\nBefore running for loops, it is important to initialize empty loops. It is much more efficient than growing the result at each iteration\nSo let’s initialize an empty list that we call list_urls of the appropriate size:\n\nlist_urls &lt;- vector(\"list\", length(dat))\n\nError: object 'dat' not found"
  },
  {
    "objectID": "r/ws_demo_slides.html#extract-all-urls-2",
    "href": "r/ws_demo_slides.html#extract-all-urls-2",
    "title": "A little demo of programming in",
    "section": "Extract all URLs",
    "text": "Extract all URLs\nNow we can run a loop to fill in our list:\n\nfor (i in seq_along(dat)) {\n  list_urls[[i]] &lt;- dat[[i]] %&gt;% html_attr(\"href\")\n}\n\nError: object 'dat' not found\n\n\nLet’s print again the first element of list_urls to make sure all looks good:\n\nlist_urls[[1]]\n\nError: object 'list_urls' not found\n\n\nWe now have a list of URLs (in the form of character vectors) as we wanted"
  },
  {
    "objectID": "r/ws_demo_slides.html#extract-data-from-each-page",
    "href": "r/ws_demo_slides.html#extract-data-from-each-page",
    "title": "A little demo of programming in",
    "section": "Extract data from each page",
    "text": "Extract data from each page\nWe will now extract the data (date, major, and advisor) for all URLs in our list.\nAgain, before running a for loop, we need to allocate memory first by creating an empty container (here a list):\n\nlist_data &lt;- vector(\"list\", length(list_urls))\n\nError: object 'list_urls' not found\n\nfor (i in seq_along(list_urls)) {\n  html &lt;- read_html(list_urls[[i]])\n  date &lt;- html %&gt;%\n    html_element(\"#publication_date p\") %&gt;%\n    html_text2()\n  major &lt;- html %&gt;%\n    html_element(\"#department p\") %&gt;%\n    html_text2()\n  advisor &lt;- html %&gt;%\n    html_element(\"#advisor1 p\") %&gt;%\n    html_text2()\n  Sys.sleep(0.1)  # Add a little delay\n  list_data[[i]] &lt;- cbind(date, major, advisor)\n}\n\nError: object 'list_urls' not found"
  },
  {
    "objectID": "r/ws_demo_slides.html#store-results-in-dataframe",
    "href": "r/ws_demo_slides.html#store-results-in-dataframe",
    "title": "A little demo of programming in",
    "section": "Store results in DataFrame",
    "text": "Store results in DataFrame\nWe can turn this big list into a dataframe:\n\nresult &lt;- do.call(rbind.data.frame, list_data)\n\nError: object 'list_data' not found\n\n\nWe can capitalize the headers:\n\nnames(result) &lt;- c(\"Date\", \"Major\", \"Advisor\")\n\nError: object 'result' not found"
  },
  {
    "objectID": "r/ws_demo_slides.html#our-final-data",
    "href": "r/ws_demo_slides.html#our-final-data",
    "title": "A little demo of programming in",
    "section": "Our final data",
    "text": "Our final data\nresult is a long dataframe, so we will only print the first few elements:\n\nhead(result, 15)\n\nError: object 'result' not found"
  },
  {
    "objectID": "r/ws_demo_slides.html#save-results-to-file",
    "href": "r/ws_demo_slides.html#save-results-to-file",
    "title": "A little demo of programming in",
    "section": "Save results to file",
    "text": "Save results to file\nIf we wanted, we could save our data to a CSV file:\nwrite.csv(result, \"dissertations_data.csv\", row.names = FALSE)"
  },
  {
    "objectID": "r/ws_demo_slides.html#data-reading-and-manipulation",
    "href": "r/ws_demo_slides.html#data-reading-and-manipulation",
    "title": "A little demo of programming in",
    "section": "Data reading and manipulation",
    "text": "Data reading and manipulation\n\nSpatial vectors: great modern packages are sf or terra\nRaster data: the package terra\n\nI will skip the data preparation due to lack of time, but you can look at the code in this webinar or this workshop"
  },
  {
    "objectID": "r/ws_demo_slides.html#mapping-data",
    "href": "r/ws_demo_slides.html#mapping-data",
    "title": "A little demo of programming in",
    "section": "Mapping data",
    "text": "Mapping data\nGood options to create maps include ggplot2 (the package we already used for plotting) or tmap"
  },
  {
    "objectID": "r/ws_demo_slides.html#map-of-glaciers-in-western-north-america",
    "href": "r/ws_demo_slides.html#map-of-glaciers-in-western-north-america",
    "title": "A little demo of programming in",
    "section": "Map of glaciers in western North America",
    "text": "Map of glaciers in western North America\ntm_shape(states, bbox = nwa_bbox) +\n  tm_polygons(col = \"#f2f2f2\", lwd = 0.2) +\n  tm_shape(ak) +\n  tm_borders(col = \"#3399ff\") +\n  tm_fill(col = \"#86baff\") +\n  tm_shape(wes) +\n  tm_borders(col = \"#3399ff\") +\n  tm_fill(col = \"#86baff\") +\n  tm_layout(\n    title = \"Glaciers of Western North America\",\n    title.position = c(\"center\", \"top\"),\n    title.size = 1.1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.06, 0.01, 0.09, 0.01),\n    outer.margins = 0,\n    frame.lwd = 0.2\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    size = 1.2,\n    text.size = 0.6\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 1000, 2000),\n    position = c(\"right\", \"BOTTOM\")\n  )"
  },
  {
    "objectID": "r/ws_demo_slides.html#multi-layer-map-of-the-retreat-of-a-glacier",
    "href": "r/ws_demo_slides.html#multi-layer-map-of-the-retreat-of-a-glacier",
    "title": "A little demo of programming in",
    "section": "Multi-layer map of the retreat of a glacier",
    "text": "Multi-layer map of the retreat of a glacier\ntm_shape(ag) +\n  tm_polygons(\"year\", palette = \"Blues\") +\n  tm_layout(\n    title = \"Agassiz Glacier\",\n    title.position = c(\"center\", \"top\"),\n    legend.position = c(\"left\", \"bottom\"),\n    legend.title.color = \"#fcfcfc\",\n    legend.text.size = 1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.07, 0.03, 0.07, 0.03),\n    outer.margins = 0\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    text.size = 0.7\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 0.5, 1),\n    position = c(\"right\", \"BOTTOM\"),\n    text.size = 1\n  )"
  },
  {
    "objectID": "r/ws_demo_slides.html#animated-map-of-the-retreat-of-a-glacier",
    "href": "r/ws_demo_slides.html#animated-map-of-the-retreat-of-a-glacier",
    "title": "A little demo of programming in",
    "section": "Animated map of the retreat of a glacier",
    "text": "Animated map of the retreat of a glacier\ntmap_animation(tm_shape(ag) +\n                 tm_polygons(col = \"#86baff\") +\n                 tm_layout(\n                   title = \"Agassiz Glacier\",\n                   title.position = c(\"center\", \"top\"),\n                   legend.position = c(\"left\", \"bottom\"),\n                   legend.title.color = \"#fcfcfc\",\n                   legend.text.size = 1,\n                   bg.color = \"#fcfcfc\",\n                   inner.margins = c(0.08, 0, 0.08, 0),\n                   outer.margins = 0,\n                   panel.label.bg.color = \"#fcfcfc\"\n                 ) +\n                 tm_compass(\n                   type = \"arrow\",\n                   position = c(\"right\", \"top\"),\n                   text.size = 0.7\n                 ) +\n                 tm_scale_bar(\n                   breaks = c(0, 0.5, 1),\n                   position = c(\"right\", \"BOTTOM\"),\n                   text.size = 1\n                 ) +\n                 tm_facets(\n                   along = \"year\",\n                   free.coords = F\n                 )filename = \"ag.gif\",\n               dpi = 300,\n               inner.margins = c(0.08, 0, 0.08, 0),\n               delay = 100\n               )"
  },
  {
    "objectID": "r/ws_demo_slides.html#three-day-introductory-workshop-for-the-hss",
    "href": "r/ws_demo_slides.html#three-day-introductory-workshop-for-the-hss",
    "title": "A little demo of programming in",
    "section": "Three-day introductory workshop for the HSS",
    "text": "Three-day introductory workshop for the HSS\nAs a follow-up to this year HSS Series, we will be offering a free three-day hands-on introduction to R for researchers in the humanities, arts, and social sciences\nYou can register here"
  },
  {
    "objectID": "r/ws_demo_slides.html#beyond-the-hss-series",
    "href": "r/ws_demo_slides.html#beyond-the-hss-series",
    "title": "A little demo of programming in",
    "section": "Beyond the HSS series",
    "text": "Beyond the HSS series\nEach region under the Alliance offers regular courses and workshops in R (and many other topics)\nIn the west, Alex Razoumov and myself offer regular free workshops, courses, and webinars for researchers in Canadian academic institutions\nYou can find our program here or join our mailing list here"
  },
  {
    "objectID": "r/wb_hpc_slides.html#intel-vs-gcc-compilers",
    "href": "r/wb_hpc_slides.html#intel-vs-gcc-compilers",
    "title": "High-performance research computing in ",
    "section": "Intel vs GCC compilers",
    "text": "Intel vs GCC compilers\nTo compile R packages, you need a C compiler.\nIn theory, you could use the proprietary Intel compiler which is loaded by default on the Alliance clusters, but it is recommended to replace it with the GCC compiler (R packages can even be compiled with Clang and LLVM, but the default GCC compiler is the best way to avoid headaches).\nIt is thus much simpler to always load a gcc module before loading an r module."
  },
  {
    "objectID": "r/wb_hpc_slides.html#r-module",
    "href": "r/wb_hpc_slides.html#r-module",
    "title": "High-performance research computing in ",
    "section": "R module",
    "text": "R module\nTo see what versions of R are available on a cluster, run:\nmodule spider r\nTo see the dependencies of a particular version (e.g. r/4.2.1), run:\nmodule spider r/4.2.1\n\nStdEnv/2020 is a required module for this version.\nOn most Alliance clusters, it is automatically loaded, so you don’t need to include it. You can double-check with module list or you can include it (before r/4.2.1) just to be sure.\n\nFinally, load your modules:\nmodule load StdEnv/2020 gcc/11.3.0 r/4.2.1"
  },
  {
    "objectID": "r/wb_hpc_slides.html#scripts",
    "href": "r/wb_hpc_slides.html#scripts",
    "title": "High-performance research computing in ",
    "section": "Scripts",
    "text": "Scripts\nTo run an R script called &lt;your_script&gt;.R, you first need to write a job script:\n\nExample:\n\n\n&lt;your_job&gt;.sh\n\n#!/bin/bash\n#SBATCH --account=def-&lt;your_account&gt;\n#SBATCH --time=15\n#SBATCH --mem-per-cpu=3000M\n#SBATCH --cpus-per-task=4\n#SBATCH --job-name=\"&lt;your_job&gt;\"\nmodule load StdEnv/2020 gcc/11.3.0 r/4.2.1\nRscript &lt;your_script&gt;.R   # Note that R scripts are run with the command `Rscript`\n\n\nThen launch your job with:\nsbatch &lt;your_job&gt;.sh\nYou can monitor your job with sq (an alias for squeue -u $USER $@)."
  },
  {
    "objectID": "r/wb_hpc_slides.html#interactive-jobs",
    "href": "r/wb_hpc_slides.html#interactive-jobs",
    "title": "High-performance research computing in ",
    "section": "Interactive jobs",
    "text": "Interactive jobs\n\nWhile it is fine to run R on the login node when you install packages, you must start a SLURM job before any heavy computation.\n\nTo run R interactively, you should launch an salloc session.\nHere is what I will use for this webinar:\nsalloc --time=1:10:00 --mem-per-cpu=7000M --ntasks=8\nThis takes me to a compute node where I can launch R to run computations:\nR"
  },
  {
    "objectID": "r/wb_hpc_slides.html#profiling",
    "href": "r/wb_hpc_slides.html#profiling",
    "title": "High-performance research computing in ",
    "section": "Profiling",
    "text": "Profiling\nThe first thing to do if you want to improve your code efficiency is to identify bottlenecks in your code. Common tools are:\n\nthe base R function Rprof()\nthe package profvis\n\nprofvis is a newer tool, built by posit (formerly RStudio). Under the hood, it runs Rprof() to collect data, then produces an interactive html widget with a flame graph that allows for an easy visual identification of slow sections of code. While this tool integrates well within the RStudio IDE or the RPubs ecosystem, it is not very well suited for remote work on a cluster. One option is to profile your code with small data on your own machine. Another option is to use the base profiler with Rprof() directly as in this example."
  },
  {
    "objectID": "r/wb_hpc_slides.html#benchmarking",
    "href": "r/wb_hpc_slides.html#benchmarking",
    "title": "High-performance research computing in ",
    "section": "Benchmarking",
    "text": "Benchmarking\nOnce you have identified expressions that are particularly slow, you can use benchmarking tools to compare variations of the code.\nIn the most basic fashion, you can use system.time(), but this is limited and imprecise.\nThe microbenchmark package is a popular option.\nIt gives the minimum time, lower quartile, mean, median, upper quartile, and maximum time of R expressions.\nThe newer bench package has less overhead, is more accurate, and—for sequential code—gives information on memory usage and garbage collections. This is the package I will use today."
  },
  {
    "objectID": "r/wb_hpc_slides.html#multi-threading",
    "href": "r/wb_hpc_slides.html#multi-threading",
    "title": "High-performance research computing in ",
    "section": "Multi-threading",
    "text": "Multi-threading\nWe talk about multi-threading when a single process (with its own memory) runs multiple threads.\nThe execution can happen in parallel—if each thread has access to a CPU core—or by alternating some of the threads on some CPU cores.\nBecause all threads in a process write to the same memory addresses, multi-threading can lead to race conditions.\nMulti-threading does not seem to be a common approach to parallelizing R code."
  },
  {
    "objectID": "r/wb_hpc_slides.html#multi-processing-in-shared-memory",
    "href": "r/wb_hpc_slides.html#multi-processing-in-shared-memory",
    "title": "High-performance research computing in ",
    "section": "Multi-processing in shared memory",
    "text": "Multi-processing in shared memory\nMulti-processing in shared memory happens when multiple processes execute code on multiple CPU cores of a single node (or a single machine).\nThe different processes need to communicate with each other, but because they are all running on the CPU cores of a single node, messages can pass via shared memory."
  },
  {
    "objectID": "r/wb_hpc_slides.html#multi-processing-in-distributed-memory",
    "href": "r/wb_hpc_slides.html#multi-processing-in-distributed-memory",
    "title": "High-performance research computing in ",
    "section": "Multi-processing in distributed memory",
    "text": "Multi-processing in distributed memory\nWhen processes involved in the execution of some code run on multiple nodes of a cluster, messages between them need to travel over the cluster interconnect. In that case, we talk about distributed memory."
  },
  {
    "objectID": "r/wb_hpc_slides.html#package-parallel-base-r",
    "href": "r/wb_hpc_slides.html#package-parallel-base-r",
    "title": "High-performance research computing in ",
    "section": "Package parallel (base R)",
    "text": "Package parallel (base R)\nThe parallel package has been part of the “base” package group since version 2.14.0.\nThis means that it is comes with R.\nMost parallel approaches in R build on this package.\nWe will make use of it to create and close an ad-hoc cluster.\n\nThe parallelly package adds functionality to the parallel package."
  },
  {
    "objectID": "r/wb_hpc_slides.html#package-foreach",
    "href": "r/wb_hpc_slides.html#package-foreach",
    "title": "High-performance research computing in ",
    "section": "Package foreach",
    "text": "Package foreach\nThe foreach package implements a looping construct without an explicit counter. It doesn’t require the preallocation of an output container, it brings to R an equivalent of the Python or Julia list comprehensions, and mostly, it allows for an easy execution of loops in parallel. Unlike loops, it creates variables (loops are used for their side-effect).\nLet’s look at an example to calculate the sum of 1e4 random vectors of length 3.\nWe will use foreach and iterators (which creates convenient iterators for foreach):\n\nlibrary(foreach)\nlibrary(iterators)"
  },
  {
    "objectID": "r/wb_hpc_slides.html#package-future",
    "href": "r/wb_hpc_slides.html#package-future",
    "title": "High-performance research computing in ",
    "section": "Package future",
    "text": "Package future\nA future is an object that acts as an abstract representation for a value in the future. A future can be resolved (if the value has been computed) or unresolved. If the value is queried while the future is unresolved, the process is blocked until the future is resolved.\nFutures allow for asynchronous and parallel evaluations. The future package provides a simple and unified API to evaluate futures."
  },
  {
    "objectID": "r/wb_hpc_slides.html#plans",
    "href": "r/wb_hpc_slides.html#plans",
    "title": "High-performance research computing in ",
    "section": "Plans",
    "text": "Plans\nThe future package does this thanks to the plan function:\n\nplan(sequential): futures are evaluated sequentially in the current R session\nplan(multisession): futures are evaluated by new R sessions spawned in the background (multi-processing in shared memory)\nplan(multicore): futures are evaluated in processes forked from the existing process (multi-processing in shared memory)\nplan(cluster): futures are evaluated on an ad-hoc cluster (allows for distributed parallelism across multiple nodes)"
  },
  {
    "objectID": "r/wb_hpc_slides.html#consistency",
    "href": "r/wb_hpc_slides.html#consistency",
    "title": "High-performance research computing in ",
    "section": "Consistency",
    "text": "Consistency\nTo ensure a consistent behaviour across plans, all evaluations are done in a local environment:\n\nlibrary(future)\n\na &lt;- 1\n\nb %&lt;-% {\n  a &lt;- 2\n}\n\na\n\n[1] 1"
  },
  {
    "objectID": "r/wb_hpc_slides.html#lets-return-to-our-example",
    "href": "r/wb_hpc_slides.html#lets-return-to-our-example",
    "title": "High-performance research computing in ",
    "section": "Let’s return to our example",
    "text": "Let’s return to our example\nWe had:\nset.seed(2)\nresult2 &lt;- foreach(icount(1e4), .combine = '+') %do% runif(3)\nWe can replace %do% with %dopar%:\nset.seed(2)\nresult3 &lt;- foreach(icount(1e4), .combine = '+') %dopar% runif(3)\nSince we haven’t registered any parallel backend, the expression will still be evaluated sequentially."
  },
  {
    "objectID": "r/wb_hpc_slides.html#load-packages",
    "href": "r/wb_hpc_slides.html#load-packages",
    "title": "High-performance research computing in ",
    "section": "Load packages",
    "text": "Load packages\nFor this toy example, I will use a modified version of one of the examples in the foreach vignette: I will b uild a classification model made of a forest of decision trees thanks to the randomForest package.\nBecause the code includes randomly generated numbers, I will use the doRNG package which replaces foreach::%dopar% wit h doRNG::%dorng%. This follows the recommendations of Pierre L’Ecuyer (1999)1 and ensures reproducibility.\nlibrary(doFuture)       # This will also load the `future` package\nlibrary(doRNG)          # This will also load the `foreach` package\nlibrary(randomForest)\nlibrary(bench)          # To do some benchmarking\nLoading required package: foreach\nLoading required package: future\nLoading required package: rngtools\nL’Ecuyer, P. (1999). Good parameters and implementations for combined multiple recursive random number generators. Operations Research, 47, 159–164."
  },
  {
    "objectID": "r/wb_hpc_slides.html#the-code-to-parallelize",
    "href": "r/wb_hpc_slides.html#the-code-to-parallelize",
    "title": "High-performance research computing in ",
    "section": "The code to parallelize",
    "text": "The code to parallelize\nThe goal is to create a classifier based on some data (here a matrix of random numbers for simplicity) and a response variable (as factor). This model could then be passed in the predict() function with novel data to generate predictions of classification. But here we are only interested in the creation of the model as this is the part that is computationally intensive. We aren’t interested in actually using it.\nset.seed(11)\ntraindata &lt;- matrix(runif(1e5), 100)\nfac &lt;- gl(2, 50)\n\nrf &lt;- foreach(ntree = rep(250, 8), .combine = combine) %do%\n  randomForest(x = traindata, y = fac, ntree = ntree)\n\nrf\nCall:\n randomForest(x = traindata, y = fac, ntree = ntree)\n               Type of random forest: classification\n                     Number of trees: 2000\nNo. of variables tried at each split: 31"
  },
  {
    "objectID": "r/wb_hpc_slides.html#reference-timing",
    "href": "r/wb_hpc_slides.html#reference-timing",
    "title": "High-performance research computing in ",
    "section": "Reference timing",
    "text": "Reference timing\nThis is the non parallelizable code with %do%:\ntref &lt;- mark(\n  rf1 &lt;- foreach(ntree = rep(250, 8), .combine = combine) %do%\n    randomForest(x = traindata, y = fac, ntree = ntree),\n  memory = FALSE\n)\n\ntref$median\n[1] 5.66s"
  },
  {
    "objectID": "r/wb_hpc_slides.html#plan-sequential",
    "href": "r/wb_hpc_slides.html#plan-sequential",
    "title": "High-performance research computing in ",
    "section": "Plan sequential",
    "text": "Plan sequential\nThis is the parallelizable foreach code, but run sequentially:\nregisterDoFuture()   # Set the parallel backend\nplan(sequential)     # Set the evaluation strategy\n\n# Using bench::mark()\ntseq &lt;- mark(\n  rf2 &lt;- foreach(ntree = rep(250, 8), .combine = combine) %dorng%\n    randomForest(x = traindata, y = fac, ntree = ntree),\n  memory = FALSE\n)\n\ntseq$median\n[1] 5.78s\n\nNo surprise: those are similar."
  },
  {
    "objectID": "r/wb_hpc_slides.html#multi-processing-in-shared-memory-1",
    "href": "r/wb_hpc_slides.html#multi-processing-in-shared-memory-1",
    "title": "High-performance research computing in ",
    "section": "Multi-processing in shared memory",
    "text": "Multi-processing in shared memory\nfuture provides availableCores() to detect the number of available cores:\navailableCores()\nsystem\n     4\n\nSimilar to parallel::detectCores().\n\nThis detects the number of CPU cores available to me on the current compute node, that is, what I can use for shared memory multi-processing."
  },
  {
    "objectID": "r/wb_hpc_slides.html#plan-multisession",
    "href": "r/wb_hpc_slides.html#plan-multisession",
    "title": "High-performance research computing in ",
    "section": "Plan multisession",
    "text": "Plan multisession\nShared memory multi-processing can be run with plan(multisession) that will spawn new R sessions in the background to evaluate futures:\nplan(multisession)\n\ntms &lt;- mark(\n  rf2 &lt;- foreach(ntree = rep(250, 8), .combine = combine) %dorng%\n    randomForest(x = traindata, y = fac, ntree = ntree),\n  memory = FALSE\n)\n\ntms$median\n[1] 2s\n\nWe got a speedup of 5.78 / 2 = 2.9."
  },
  {
    "objectID": "r/wb_hpc_slides.html#plan-multicore",
    "href": "r/wb_hpc_slides.html#plan-multicore",
    "title": "High-performance research computing in ",
    "section": "Plan multicore",
    "text": "Plan multicore\nShared memory multi-processing can also be run with plan(multicore) (except on Windows) that will fork the current R process to evaluate futures:\nplan(multicore)\n\ntmc &lt;- mark(\n  rf2 &lt;- foreach(ntree = rep(250, 8), .combine = combine) %dorng%\n    randomForest(x = traindata, y = fac, ntree = ntree),\n  memory = FALSE\n)\n\ntmc$median\n[1] 1.9s\n\nWe got a very similar speedup of 5.78 / 1.9 = 3.0."
  },
  {
    "objectID": "r/wb_hpc_slides.html#multi-processing-in-distributed-memory-1",
    "href": "r/wb_hpc_slides.html#multi-processing-in-distributed-memory-1",
    "title": "High-performance research computing in ",
    "section": "Multi-processing in distributed memory",
    "text": "Multi-processing in distributed memory\nI requested 8 tasks from Slurm on a training cluster made of nodes with 4 CPU cores each. Let’s verify that I got them by accessing the SLURM_NTASKS environment variable from within R:\nas.numeric(Sys.getenv(\"SLURM_NTASKS\"))\n[1] 8\nI can create a character vector with the name of the node each task is running on:\n(hosts &lt;- system(\"srun hostname | cut -f 1 -d '.'\", intern = TRUE))\nchr [1:8] \"node1\" \"node1\" \"node1\" \"node1\" \"node2\" \"node2\" \"node2\" \"node2\"\nThis allows me to create a cluster of workers:\n(cl &lt;- parallel::makeCluster(hosts))      # Defaults to type=\"PSOCK\"\nsocket cluster with 8 nodes on hosts ‘node1’, ‘node2’"
  },
  {
    "objectID": "r/wb_hpc_slides.html#plan-cluster",
    "href": "r/wb_hpc_slides.html#plan-cluster",
    "title": "High-performance research computing in ",
    "section": "Plan cluster",
    "text": "Plan cluster\nI can now try the code with distributed parallelism using all 8 CPU cores across both nodes:\nplan(cluster, workers = cl)\n\ntdis &lt;- mark(\n  rf2 &lt;- foreach(ntree = rep(250, 8), .combine = combine) %dorng%\n    randomForest(x = traindata, y = fac, ntree = ntree),\n  memory = FALSE\n)\n\ntdis$median\n[1] 1.14s\n\nSpeedup: 5.78 / 1.14 = 5.1.\n\nThe cluster of workers can be stopped with:\nparallel::stopCluster(cl)"
  },
  {
    "objectID": "r/wb_hpc_slides.html#alternative-approaches",
    "href": "r/wb_hpc_slides.html#alternative-approaches",
    "title": "High-performance research computing in ",
    "section": "Alternative approaches",
    "text": "Alternative approaches\nThe multidplyr package partitions data frames across worker processes, allows you to run the usual tidyverse functions on each partition, then collects the processed data.\nThe furrr package is a parallel equivalent to the purrr package from the tidyverse.\nIf you work with genomic data, you might want to have a look at the BiocParallel package from Bioconductor.\nYet another option to run distributed R code is to use the sparklyr package (an R interface to Spark).\nRmpi is a wrapper to MPI (Message-Passing Interface). It has proved slow and problematic on Cedar though.\nThe boot package provides functions and datasets specifically for bootstrapping in parallel."
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#types-of-spatial-data",
    "href": "r/wb_gis_mapping_slides.html#types-of-spatial-data",
    "title": "GIS mapping with R",
    "section": "Types of spatial data",
    "text": "Types of spatial data\nVector data\nDiscrete objects\nContain:  - geometry:  shape & location of the objects\n    - attributes:  additional variables (e.g. name, year, type)\nCommon file format:  GeoJSON, shapefile\n\nExamples: countries, roads, rivers, towns"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#types-of-spatial-data-1",
    "href": "r/wb_gis_mapping_slides.html#types-of-spatial-data-1",
    "title": "GIS mapping with R",
    "section": "Types of spatial data",
    "text": "Types of spatial data\nRaster data\nContinuous phenomena or spatial fields\nCommon file formats:  TIFF, GeoTIFF, NetCDF, Esri grid\n\nExamples: temperature, air quality, elevation, water depth"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#vector-data-1",
    "href": "r/wb_gis_mapping_slides.html#vector-data-1",
    "title": "GIS mapping with R",
    "section": "Vector data",
    "text": "Vector data\nTypes\n\npoint:       single set of coordinates\nmulti-point:   multiple sets of coordinates\npolyline:      multiple sets for which the order matters\nmulti-polyline:  multiple of the above\npolygon:      same as polyline but first & last sets are the same\nmulti-polygon:  multiple of the above"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#raster-data-1",
    "href": "r/wb_gis_mapping_slides.html#raster-data-1",
    "title": "GIS mapping with R",
    "section": "Raster data",
    "text": "Raster data\nGrid of equally sized rectangular cells containing values for some variables\nSize of cells = resolution\nFor computing efficiency, rasters do not have coordinates of each cell, but the bounding box & the number of rows & columns"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#coordinate-reference-systems-crs",
    "href": "r/wb_gis_mapping_slides.html#coordinate-reference-systems-crs",
    "title": "GIS mapping with R",
    "section": "Coordinate Reference Systems (CRS)",
    "text": "Coordinate Reference Systems (CRS)\nA location on Earth’s surface can be identified by its coordinates & some reference system called CRS\nThe coordinates (x, y) are called longitude & latitude\nThere can be a 3rd coordinate (z) for elevation or other measurement—usually a vertical one\nAnd a 4th (m) for some other data attribute—usually a horizontal measurement\nIn 3D, longitude & latitude are expressed in angular units (e.g. degrees) & the reference system needed is an angular CRS or geographic coordinate system (GCS)\nIn 2D, they are expressed in linear units (e.g. meters) & the reference system needed is a planar CRS or projected coordinate system (PCS)"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#datums",
    "href": "r/wb_gis_mapping_slides.html#datums",
    "title": "GIS mapping with R",
    "section": "Datums",
    "text": "Datums\nSince the Earth is not a perfect sphere, we use spheroidal models to represent its surface. Those are called geodetic datums\nSome datums are global, others local (more accurate in a particular area of the globe, but only useful there)\n\nExamples of commonly used global datums:\n\nWGS84 (World Geodesic System 1984)\nNAD83 (North American Datum of 1983)"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#angular-crs",
    "href": "r/wb_gis_mapping_slides.html#angular-crs",
    "title": "GIS mapping with R",
    "section": "Angular CRS",
    "text": "Angular CRS\nAn angular CRS contains a datum, an angular unit & references such as a prime meridian (e.g. the Royal Observatory, Greenwich, England)\nIn an angular CRS or GCS:\n\nLongitude (\\(\\lambda\\)) represents the angle between the prime meridian & the meridian that passes through that location\nLatitude (\\(\\phi\\)) represents the angle between the line that passes through the center of the Earth & that location & its projection on the equatorial plane\n\nLongitude & latitude are thus angular coordinates"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#projections",
    "href": "r/wb_gis_mapping_slides.html#projections",
    "title": "GIS mapping with R",
    "section": "Projections",
    "text": "Projections\nTo create a two-dimensional map, you need to project this 3D angular CRS into a 2D one\nVarious projections offer different characteristics. For instance:\n\nsome respect areas (equal-area)\nsome respect the shape of geographic features (conformal)\nsome almost respect both for small areas\n\nIt is important to choose one with sensible properties for your goals\n\nExamples of projections:\n\nMercator\nUTM\nRobinson"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#planar-crs",
    "href": "r/wb_gis_mapping_slides.html#planar-crs",
    "title": "GIS mapping with R",
    "section": "Planar CRS",
    "text": "Planar CRS\nA planar CRS is defined by a datum, a projection & a set of parameters such as a linear unit & the origins\nCommon planar CRS have been assigned a unique ID called EPSG code which is much more convenient to use\nIn a planar CRS, coordinates will not be in degrees anymore but in meters (or other length unit)"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#projecting-into-a-new-crs",
    "href": "r/wb_gis_mapping_slides.html#projecting-into-a-new-crs",
    "title": "GIS mapping with R",
    "section": "Projecting into a new CRS",
    "text": "Projecting into a new CRS\nYou can change the projection of your data\nVector data won’t suffer any loss of precision, but raster data will\n→  best to try to avoid reprojecting rasters: if you want to combine various datasets which have different projections, reproject vector data instead"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#resources",
    "href": "r/wb_gis_mapping_slides.html#resources",
    "title": "GIS mapping with R",
    "section": "Resources",
    "text": "Resources\nOpen GIS data\nFree GIS Data: list of free GIS datasets\nBooks\nGeocomputation with R by Robin Lovelace, Jakub Nowosad & Jannes Muenchow\nSpatial Data Science by Edzer Pebesma & Roger Bivand\nSpatial Data Science with R by Robert J. Hijmans\nUsing Spatial Data with R by Claudia A. Engel\nTutorial\nAn Introduction to Spatial Data Analysis and Visualisation in R by the CDRC"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#resources-1",
    "href": "r/wb_gis_mapping_slides.html#resources-1",
    "title": "GIS mapping with R",
    "section": "Resources",
    "text": "Resources\nWebsite\nr-spatial by Edzer Pebesma, Marius Appel & Daniel Nüst\nCRAN package list\nAnalysis of Spatial Data\nMailing list\nR Special Interest Group on using Geographical data and Mapping"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#data-manipulation",
    "href": "r/wb_gis_mapping_slides.html#data-manipulation",
    "title": "GIS mapping with R",
    "section": "Data manipulation",
    "text": "Data manipulation\nOlder packages\n\nsp\nraster\nrgdal\nrgeos\n\nNewer generation\n\nsf: vector data\nterra: raster data (also has vector data capabilities)"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#mapping",
    "href": "r/wb_gis_mapping_slides.html#mapping",
    "title": "GIS mapping with R",
    "section": "Mapping",
    "text": "Mapping\nStatic maps\n\nggplot2 + ggspatial\ntmap\n\nDynamic maps\n\nleaflet\nggplot2 + gganimate\nmapview\nggmap\ntmap"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#sf-simple-features-in-r",
    "href": "r/wb_gis_mapping_slides.html#sf-simple-features-in-r",
    "title": "GIS mapping with R",
    "section": "sf: Simple Features in R",
    "text": "sf: Simple Features in R\nGeospatial vectors: points, lines, polygons\nSimple Features—defined by the Open Geospatial Consortium (OGC) & formalized by ISO—is a set of standards now used by most GIS libraries\nWell-known text (WKT) is a markup language for representing vector geometry objects according to those standards\nA compact computer version also exists—well-known binary (WKB)—used by spatial databases\nThe package sp predates Simple Features\nsf—launched in 2016—implements these standards in R in the form of sf objects: data.frames (or tibbles) containing the attributes, extended by sfc objects or simple feature geometries list-columns"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#sf",
    "href": "r/wb_gis_mapping_slides.html#sf",
    "title": "GIS mapping with R",
    "section": "sf",
    "text": "sf\nUseful links\n\nGitHub repo\nPaper\nResources\nCheatsheet\n6 vignettes: 1, 2, 3, 4, 5, 6"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#sf-objects",
    "href": "r/wb_gis_mapping_slides.html#sf-objects",
    "title": "GIS mapping with R",
    "section": "sf objects",
    "text": "sf objects"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#sf-objects-1",
    "href": "r/wb_gis_mapping_slides.html#sf-objects-1",
    "title": "GIS mapping with R",
    "section": "sf objects",
    "text": "sf objects"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#sf-objects-2",
    "href": "r/wb_gis_mapping_slides.html#sf-objects-2",
    "title": "GIS mapping with R",
    "section": "sf objects",
    "text": "sf objects"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#sf-objects-3",
    "href": "r/wb_gis_mapping_slides.html#sf-objects-3",
    "title": "GIS mapping with R",
    "section": "sf objects",
    "text": "sf objects"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#sf-objects-4",
    "href": "r/wb_gis_mapping_slides.html#sf-objects-4",
    "title": "GIS mapping with R",
    "section": "sf objects",
    "text": "sf objects"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#sf-functions",
    "href": "r/wb_gis_mapping_slides.html#sf-functions",
    "title": "GIS mapping with R",
    "section": "sf functions",
    "text": "sf functions\nMost functions start with st_ (which refers to “spatial type”)"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#terra-geospatial-rasters",
    "href": "r/wb_gis_mapping_slides.html#terra-geospatial-rasters",
    "title": "GIS mapping with R",
    "section": "terra: Geospatial rasters",
    "text": "terra: Geospatial rasters\nFaster and simpler replacement for the raster package by the same team\nMostly implemented in C++\nCan work with datasets too large to be loaded into memory"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#terra",
    "href": "r/wb_gis_mapping_slides.html#terra",
    "title": "GIS mapping with R",
    "section": "terra",
    "text": "terra\nUseful links\n\nGitHub repo\nResources\nFull manual"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#tmap-layered-grammar-of-graphics-gis-maps",
    "href": "r/wb_gis_mapping_slides.html#tmap-layered-grammar-of-graphics-gis-maps",
    "title": "GIS mapping with R",
    "section": "tmap: Layered grammar of graphics GIS maps",
    "text": "tmap: Layered grammar of graphics GIS maps"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#tmap",
    "href": "r/wb_gis_mapping_slides.html#tmap",
    "title": "GIS mapping with R",
    "section": "tmap",
    "text": "tmap\nUseful links\n\nGitHub repo\nResources\n\nHelp pages and vignettes\n?tmap-element\nvignette(\"tmap-getstarted\")\n# All the usual help pages, e.g.:\n?tm_layout"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#tmap-functions",
    "href": "r/wb_gis_mapping_slides.html#tmap-functions",
    "title": "GIS mapping with R",
    "section": "tmap functions",
    "text": "tmap functions\nMain functions start with tmap_\nFunctions creating map elements start with tm_"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#tmap-functioning",
    "href": "r/wb_gis_mapping_slides.html#tmap-functioning",
    "title": "GIS mapping with R",
    "section": "tmap functioning",
    "text": "tmap functioning\nVery similar to ggplot2\nTypically, a map contains:\n\nOne or multiple layer(s) (the order matters as they stack on top of each other)\nSome layout (e.g. customization of title, background, margins): tm_layout\nA compass: tm_compass\nA scale bar: tm_scale_bar\n\nEach layer contains:\n\nSome data: tm_shape\nHow that data will be represented: e.g. tm_polygons, tm_lines, tm_raster"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#tmap-example",
    "href": "r/wb_gis_mapping_slides.html#tmap-example",
    "title": "GIS mapping with R",
    "section": "tmap example",
    "text": "tmap example"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#tmap-example-1",
    "href": "r/wb_gis_mapping_slides.html#tmap-example-1",
    "title": "GIS mapping with R",
    "section": "tmap example",
    "text": "tmap example"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#tmap-example-2",
    "href": "r/wb_gis_mapping_slides.html#tmap-example-2",
    "title": "GIS mapping with R",
    "section": "tmap example",
    "text": "tmap example"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#tmap-example-3",
    "href": "r/wb_gis_mapping_slides.html#tmap-example-3",
    "title": "GIS mapping with R",
    "section": "tmap example",
    "text": "tmap example"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#tmap-example-4",
    "href": "r/wb_gis_mapping_slides.html#tmap-example-4",
    "title": "GIS mapping with R",
    "section": "tmap example",
    "text": "tmap example"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#tmap-example-5",
    "href": "r/wb_gis_mapping_slides.html#tmap-example-5",
    "title": "GIS mapping with R",
    "section": "tmap example",
    "text": "tmap example"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#tmap-example-6",
    "href": "r/wb_gis_mapping_slides.html#tmap-example-6",
    "title": "GIS mapping with R",
    "section": "tmap example",
    "text": "tmap example"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#tmap-example-7",
    "href": "r/wb_gis_mapping_slides.html#tmap-example-7",
    "title": "GIS mapping with R",
    "section": "tmap example",
    "text": "tmap example"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#ggplot2-the-standard-in-r-plots",
    "href": "r/wb_gis_mapping_slides.html#ggplot2-the-standard-in-r-plots",
    "title": "GIS mapping with R",
    "section": "ggplot2 (the standard in R plots)",
    "text": "ggplot2 (the standard in R plots)\nUseful links\n\nGitHub repo\nResources\nCheatsheet"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#ggplot2",
    "href": "r/wb_gis_mapping_slides.html#ggplot2",
    "title": "GIS mapping with R",
    "section": "ggplot2",
    "text": "ggplot2\ngeom_sf allows to plot sf objects (i.e. make maps)"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#data",
    "href": "r/wb_gis_mapping_slides.html#data",
    "title": "GIS mapping with R",
    "section": "Data",
    "text": "Data\nFor this webinar, we will use:\n\nthe Alaska as well as the Western Canada & USA subsets of the Randolph Glacier Inventory version 6.01\nthe USGS time series of the named glaciers of Glacier National Park2\nthe Alaska as well as the Western Canada & USA subsets of the consensus estimate for the ice thickness distribution of all glaciers on Earth dataset3\n\nThe datasets can be downloaded as zip files from these websites\nRGI Consortium (2017). Randolph Glacier Inventory – A Dataset of Global Glacier Outlines: Version 6.0: Technical Report, Global Land Ice Measurements from Space, Colorado, USA. Digital Media. DOI: https://doi.org/10.7265/N5-RGI-60.Fagre, D.B., McKeon, L.A., Dick, K.A. & Fountain, A.G., 2017, Glacier margin time series (1966, 1998, 2005, 2015) of the named glaciers of Glacier National Park, MT, USA: U.S. Geological Survey data release. DOI: https://doi.org/10.5066/F7P26WB1.Farinotti, Daniel, 2019, A consensus estimate for the ice thickness distribution of all glaciers on Earth - dataset, Zurich. ETH Zurich. DOI: https://doi.org/10.3929/ethz-b-000315707."
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#packages-1",
    "href": "r/wb_gis_mapping_slides.html#packages-1",
    "title": "GIS mapping with R",
    "section": "Packages",
    "text": "Packages\nPackages need to be installed before they can be loaded in a session\nPackages on CRAN can be installed with:\ninstall.packages(\"&lt;package-name&gt;\")\n basemaps is not on CRAN & needs to be installed from GitHub thanks to devtools:\ninstall.packages(\"devtools\")\ndevtools::install_github(\"16EAGLE/basemaps\")"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#packages-2",
    "href": "r/wb_gis_mapping_slides.html#packages-2",
    "title": "GIS mapping with R",
    "section": "Packages",
    "text": "Packages\nWe load all the packages that we will need at the top of the script:\nlibrary(sf)                 # spatial vector data manipulation\nlibrary(tmap)               # map production & tiled web map\nlibrary(dplyr)              # non GIS specific (tabular data manipulation)\nlibrary(magrittr)           # non GIS specific (pipes)\nlibrary(purrr)              # non GIS specific (functional programming)\nlibrary(rnaturalearth)      # basemap data access functions\nlibrary(rnaturalearthdata)  # basemap data\nlibrary(mapview)            # tiled web map\nlibrary(grid)               # (part of base R) used to create inset map\nlibrary(ggplot2)            # alternative to tmap for map production\nlibrary(ggspatial)          # spatial framework for ggplot2\nlibrary(terra)              # gridded spatial data manipulation\nlibrary(ggmap)              # download basemap data\nlibrary(basemaps)           # download basemap data\nlibrary(magick)             # wrapper around ImageMagick STL\nlibrary(leaflet)            # integrate Leaflet JS in R"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#randolph-glacier-inventory",
    "href": "r/wb_gis_mapping_slides.html#randolph-glacier-inventory",
    "title": "GIS mapping with R",
    "section": "Randolph Glacier Inventory",
    "text": "Randolph Glacier Inventory\nThis dataset contains the contour of all glaciers on Earth\nWe will focus on glaciers in Western North America\nYou can download & unzip 02_rgi60_WesternCanadaUS & 01_rgi60_Alaska from the Randolph Glacier Inventory version 6.0"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#reading-in-data",
    "href": "r/wb_gis_mapping_slides.html#reading-in-data",
    "title": "GIS mapping with R",
    "section": "Reading in data",
    "text": "Reading in data\nData get imported & turned into sf objects with the function sf::st_read:\nak &lt;- st_read(\"data/01_rgi60_Alaska\")\n\nMake sure to use the absolute paths or the paths relative to your working directory (which can be obtained with getwd)"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#reading-in-data-1",
    "href": "r/wb_gis_mapping_slides.html#reading-in-data-1",
    "title": "GIS mapping with R",
    "section": "Reading in data",
    "text": "Reading in data\nak &lt;- st_read(\"data/01_rgi60_Alaska\")\nReading layer `01_rgi60_Alaska' from data source `./data/01_rgi60_Alaska'\n               using driver `ESRI Shapefile'\nSimple feature collection with 27108 features and 22 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -176.1425 ymin: 52.05727 xmax: -126.8545 ymax: 69.35167\nGeodetic CRS:  WGS 84"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#reading-in-data-2",
    "href": "r/wb_gis_mapping_slides.html#reading-in-data-2",
    "title": "GIS mapping with R",
    "section": "Reading in data",
    "text": "Reading in data\n\n\nYour turn:\n\nRead in the data for the rest of north western America (from 02_rgi60_WesternCanadaUS) and create an sf object called wes"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#first-look-at-the-data",
    "href": "r/wb_gis_mapping_slides.html#first-look-at-the-data",
    "title": "GIS mapping with R",
    "section": "First look at the data",
    "text": "First look at the data\nak\nSimple feature collection with 27108 features and 22 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -176.1425 ymin: 52.05727 xmax: -126.8545 ymax: 69.35167\nGeodetic CRS:  WGS 84\nFirst 10 features:\n           RGIId        GLIMSId  BgnDate  EndDate    CenLon   CenLat O1Region\n1  RGI60-01.00001 G213177E63689N 20090703 -9999999 -146.8230 63.68900        1\n2  RGI60-01.00002 G213332E63404N 20090703 -9999999 -146.6680 63.40400        1\n3  RGI60-01.00003 G213920E63376N 20090703 -9999999 -146.0800 63.37600        1\n4  RGI60-01.00004 G213880E63381N 20090703 -9999999 -146.1200 63.38100        1\n5  RGI60-01.00005 G212943E63551N 20090703 -9999999 -147.0570 63.55100        1\n6  RGI60-01.00006 G213756E63571N 20090703 -9999999 -146.2440 63.57100        1\n7  RGI60-01.00007 G213771E63551N 20090703 -9999999 -146.2295 63.55085        1\n8  RGI60-01.00008 G213704E63543N 20090703 -9999999 -146.2960 63.54300        1\n9  RGI60-01.00009 G212400E63659N 20090703 -9999999 -147.6000 63.65900        1\n10 RGI60-01.00010 G212830E63513N 20090703 -9999999 -147.1700 63.51300        1\nO2Region   Area Zmin Zmax Zmed Slope Aspect  Lmax Status Connect Form\n1         2  0.360 1936 2725 2385    42    346   839      0       0    0\n2         2  0.558 1713 2144 2005    16    162  1197      0       0    0\n3         2  1.685 1609 2182 1868    18    175  2106      0       0    0\n4         2  3.681 1273 2317 1944    19    195  4175      0       0    0\n5         2  2.573 1494 2317 1914    16    181  2981      0       0    0\n6         2 10.470 1201 3547 1740    22     33 10518      0       0    0\n7         2  0.649 1918 2811 2194    23    151  1818      0       0    0\n8         2  0.200 2826 3555 3195    45     80   613      0       0    0\n9         2  1.517 1750 2514 1977    18    274  2255      0       0    0\n10        2  3.806 1280 1998 1666    17     35  3332      0       0    0\nTermType Surging Linkages Name                       geometry\n1         0       9        9 &lt;NA&gt; POLYGON ((-146.818 63.69081...\n2         0       9        9 &lt;NA&gt; POLYGON ((-146.6635 63.4076...\n3         0       9        9 &lt;NA&gt; POLYGON ((-146.0723 63.3834...\n4         0       9        9 &lt;NA&gt; POLYGON ((-146.149 63.37919...\n5         0       9        9 &lt;NA&gt; POLYGON ((-147.0431 63.5502...\n6         0       9        9 &lt;NA&gt; POLYGON ((-146.2436 63.5562...\n7         0       9        9 &lt;NA&gt; POLYGON ((-146.2495 63.5531...\n8         0       9        9 &lt;NA&gt; POLYGON ((-146.2992 63.5443...\n9         0       9        9 &lt;NA&gt; POLYGON ((-147.6147 63.6643...\n10        0       9        9 &lt;NA&gt; POLYGON ((-147.1494 63.5098..."
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#structure-of-the-data",
    "href": "r/wb_gis_mapping_slides.html#structure-of-the-data",
    "title": "GIS mapping with R",
    "section": "Structure of the data",
    "text": "Structure of the data\nstr(ak)\nClasses ‘sf’ and 'data.frame':  27108 obs. of  23 variables:\n$ RGIId   : chr  \"RGI60-01.00001\" \"RGI60-01.00002\" \"RGI60-01.00003\" ...\n$ GLIMSId : chr  \"G213177E63689N\" \"G213332E63404N\" \"G213920E63376N\" ...\n$ BgnDate : chr  \"20090703\" \"20090703\" \"20090703\" \"20090703\" ...\n$ EndDate : chr  \"-9999999\" \"-9999999\" \"-9999999\" \"-9999999\" ...\n$ CenLon  : num  -147 -147 -146 -146 -147 ...\n$ CenLat  : num  63.7 63.4 63.4 63.4 63.6 ...\n$ O1Region: chr  \"1\" \"1\" \"1\" \"1\" ...\n$ O2Region: chr  \"2\" \"2\" \"2\" \"2\" ...\n$ Area    : num  0.36 0.558 1.685 3.681 2.573 ...\n$ Zmin    : int  1936 1713 1609 1273 1494 1201 1918 2826 1750 1280 ...\n$ Zmax    : int  2725 2144 2182 2317 2317 3547 2811 3555 2514 1998 ...\n$ Zmed    : int  2385 2005 1868 1944 1914 1740 2194 3195 1977 1666 ...\n$ Slope   : num  42 16 18 19 16 22 23 45 18 17 ...\n$ Aspect  : int  346 162 175 195 181 33 151 80 274 35 ...\n$ Lmax    : int  839 1197 2106 4175 2981 10518 1818 613 2255 3332 ...\n$ Status  : int  0 0 0 0 0 0 0 0 0 0 ...\n$ Connect : int  0 0 0 0 0 0 0 0 0 0 ...\n$ Form    : int  0 0 0 0 0 0 0 0 0 0 ...\n$ TermType: int  0 0 0 0 0 0 0 0 0 0 ...\n$ Surging : int  9 9 9 9 9 9 9 9 9 9 ...\n$ Linkages: int  9 9 9 9 9 9 9 9 9 9 ...\n$ Name    : chr  NA NA NA NA ...\n$ geometry:sfc_POLYGON of length 27108; first list element: List of 1\n..$ : num [1:65, 1:2] -147 -147 -147 -147 -147 ...\n..- attr(*, \"class\")= chr [1:3] \"XY\" \"POLYGON\" \"sfg\"\n- attr(*, \"sf_column\")= chr \"geometry\"\n- attr(*, \"agr\")= Factor w/ 3 levels \"constant\",\"aggregate\",..: NA NA NA ...\n..- attr(*, \"names\")= chr [1:22] \"RGIId\" \"GLIMSId\" \"BgnDate\" \"EndDate\" ..."
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#inspect-your-data",
    "href": "r/wb_gis_mapping_slides.html#inspect-your-data",
    "title": "GIS mapping with R",
    "section": "Inspect your data",
    "text": "Inspect your data\n\n\nYour turn:\n\nInspect the wes object you created."
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#glacier-national-park-dataset",
    "href": "r/wb_gis_mapping_slides.html#glacier-national-park-dataset",
    "title": "GIS mapping with R",
    "section": "Glacier National Park dataset",
    "text": "Glacier National Park dataset\nThis dataset contains a time series of the retreat of 39 glaciers of Glacier National Park, MT, USA\nfor the years 1966, 1998, 2005 & 2015\nYou can download and unzip the 4 sets of files from the USGS website"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#read-in-and-clean-datasets",
    "href": "r/wb_gis_mapping_slides.html#read-in-and-clean-datasets",
    "title": "GIS mapping with R",
    "section": "Read in and clean datasets",
    "text": "Read in and clean datasets\nCreate a function that reads and cleans the data:\nprep &lt;- function(dir) {\n  g &lt;- st_read(dir)\n  g %&lt;&gt;% rename_with(~ tolower(gsub(\"Area....\", \"area\", .x)))\n  g %&lt;&gt;% dplyr::select(\n    year,\n    objectid,\n    glacname,\n    area,\n    shape_leng,\n    x_coord,\n    y_coord,\n    source_sca,\n    source\n  )\n}"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#combine-datasets-into-one-sf-object",
    "href": "r/wb_gis_mapping_slides.html#combine-datasets-into-one-sf-object",
    "title": "GIS mapping with R",
    "section": "Combine datasets into one sf object",
    "text": "Combine datasets into one sf object\nCheck that the CRS are all the same:\nall(sapply(\n  list(st_crs(gnp[[1]]),\n       st_crs(gnp[[2]]),\n       st_crs(gnp[[3]]),\n       st_crs(gnp[[4]])),\n  function(x) x == st_crs(gnp[[1]])\n))\n[1] TRUE"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#combine-datasets-into-one-sf-object-1",
    "href": "r/wb_gis_mapping_slides.html#combine-datasets-into-one-sf-object-1",
    "title": "GIS mapping with R",
    "section": "Combine datasets into one sf object",
    "text": "Combine datasets into one sf object\nWe can rbind the elements of our list:\ngnp &lt;- do.call(\"rbind\", gnp)\nYou can inspect your new sf object by calling it or with str"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#estimate-for-ice-thickness",
    "href": "r/wb_gis_mapping_slides.html#estimate-for-ice-thickness",
    "title": "GIS mapping with R",
    "section": "Estimate for ice thickness",
    "text": "Estimate for ice thickness\nThis dataset contains an estimate for the ice thickness of all glaciers on Earth\nThe nomenclature follows the Randolph Glacier Inventory\nIce thickness being a spatial field, this is raster data\nWe will use data in RGI60-02.16664_thickness.tif from the ETH Zürich Research Collection which corresponds to one of the glaciers (Agassiz) of Glacier National Park"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#load-raster-data",
    "href": "r/wb_gis_mapping_slides.html#load-raster-data",
    "title": "GIS mapping with R",
    "section": "Load raster data",
    "text": "Load raster data\nRead in data and create a SpatRaster object:\nras &lt;- rast(\"data/RGI60-02/RGI60-02.16664_thickness.tif\")"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#inspect-our-spatraster-object",
    "href": "r/wb_gis_mapping_slides.html#inspect-our-spatraster-object",
    "title": "GIS mapping with R",
    "section": "Inspect our SpatRaster object",
    "text": "Inspect our SpatRaster object\nras\nclass       : SpatRaster \ndimensions  : 93, 74, 1  (nrow, ncol, nlyr)\nresolution  : 25, 25  (x, y)\nextent      : 707362.5, 709212.5, 5422962, 5425288  (xmin, xmax, ymin, ymax)\ncoord. ref. : +proj=utm +zone=11 +datum=WGS84 +units=m +no_defs \nsource      : RGI60-02.16664_thickness.tif \nname        : RGI60-02.16664_thickness \nnlyr gives us the number of bands (a single one here). You can also run str(ras)"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#our-data",
    "href": "r/wb_gis_mapping_slides.html#our-data",
    "title": "GIS mapping with R",
    "section": "Our data",
    "text": "Our data\nWe now have 3 sf objects & 1 SpatRaster object:\n\nak:  contour of glaciers in AK\nwes:  contour of glaciers in the rest of Western North America\ngnp:  time series of 39 glaciers in Glacier National Park, MT, USA\nras:  ice thickness of the Agassiz Glacier from Glacier National Park"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#lets-map-our-sf-object-ak",
    "href": "r/wb_gis_mapping_slides.html#lets-map-our-sf-object-ak",
    "title": "GIS mapping with R",
    "section": "Let’s map our sf object ak",
    "text": "Let’s map our sf object ak\nAt a bare minimum, we need tm_shape with the data & some info as to how to represent that data:\ntm_shape(ak) +\n  tm_polygons()"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#we-need-to-label-customize-it",
    "href": "r/wb_gis_mapping_slides.html#we-need-to-label-customize-it",
    "title": "GIS mapping with R",
    "section": "We need to label & customize it",
    "text": "We need to label & customize it\ntm_shape(ak) +\n  tm_polygons() +\n  tm_layout(\n    title = \"Glaciers of Alaska\",\n    title.position = c(\"center\", \"top\"),\n    title.size = 1.1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.06, 0.01, 0.09, 0.01),\n    outer.margins = 0,\n    frame.lwd = 0.2\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    size = 1.2,\n    text.size = 0.6\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 500, 1000),\n    position = c(\"right\", \"BOTTOM\")\n  )"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#make-a-map-of-the-wes-object",
    "href": "r/wb_gis_mapping_slides.html#make-a-map-of-the-wes-object",
    "title": "GIS mapping with R",
    "section": "Make a map of the wes object",
    "text": "Make a map of the wes object\n\n\nYour turn:\n\nMake a map with the wes object you created with the data for Western North America excluding AK"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#now-lets-make-a-map-with-ak-wes",
    "href": "r/wb_gis_mapping_slides.html#now-lets-make-a-map-with-ak-wes",
    "title": "GIS mapping with R",
    "section": "Now, let’s make a map with ak & wes",
    "text": "Now, let’s make a map with ak & wes\n\nThe Coordinate Reference Systems (CRS) must be the same\n\n\nsf has a function to retrieve the CRS of an sf object: st_crs\n\n\nst_crs(ak) == st_crs(wes)\n[1] TRUE\n\n\nSo we’re good (we will see later what to do if this is not the case)"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#our-combined-map",
    "href": "r/wb_gis_mapping_slides.html#our-combined-map",
    "title": "GIS mapping with R",
    "section": "Our combined map",
    "text": "Our combined map\nLet’s start again with a minimum map without any layout to test things out:\ntm_shape(ak) +\n  tm_polygons() +\n  tm_shape(wes) +\n  tm_polygons()\n\n\nUh … oh …"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#what-went-wrong",
    "href": "r/wb_gis_mapping_slides.html#what-went-wrong",
    "title": "GIS mapping with R",
    "section": "What went wrong?",
    "text": "What went wrong?\nMaps are bound by “bounding boxes”. In tmap, they are called bbox\ntmap sets the bbox the first time tm_shape is called. In our case, the bbox was thus set to the bbox of the ak object\nWe need to create a new bbox for our new map"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#retrieving-bounding-boxes",
    "href": "r/wb_gis_mapping_slides.html#retrieving-bounding-boxes",
    "title": "GIS mapping with R",
    "section": "Retrieving bounding boxes",
    "text": "Retrieving bounding boxes\nsf has a function to retrieve the bbox of an sf object: st_bbox\nThe bbox of ak is:\nst_bbox(ak)\nxmin         ymin       xmax         ymax\n-176.14247   52.05727   -126.85450   69.35167"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#combining-bounding-boxes",
    "href": "r/wb_gis_mapping_slides.html#combining-bounding-boxes",
    "title": "GIS mapping with R",
    "section": "Combining bounding boxes",
    "text": "Combining bounding boxes\nbbox objects can’t be combined directly\nHere is how we can create a new bbox encompassing both of our bboxes:\n\nFirst, we transform our bboxes to sfc objects with st_as_sfc\nThen we combine those objects into a new sfc object with st_union\nFinally, we retrieve the bbox of that object with st_bbox:\n\nnwa_bbox &lt;- st_bbox(\n  st_union(\n    st_as_sfc(st_bbox(wes)),\n    st_as_sfc(st_bbox(ak))\n  )\n)"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#back-to-our-map",
    "href": "r/wb_gis_mapping_slides.html#back-to-our-map",
    "title": "GIS mapping with R",
    "section": "Back to our map",
    "text": "Back to our map\nWe can now use our new bounding box for the map of Western North America:\ntm_shape(ak, bbox = nwa_bbox) +\n  tm_polygons() +\n  tm_shape(wes) +\n  tm_polygons() +\n  tm_layout(\n    title = \"Glaciers of Western North America\",\n    title.position = c(\"center\", \"top\"),\n    title.size = 1.1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.06, 0.01, 0.09, 0.01),\n    outer.margins = 0,\n    frame.lwd = 0.2\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    size = 1.2,\n    text.size = 0.6\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 1000, 2000),\n    position = c(\"right\", \"BOTTOM\")\n  )"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#lets-add-a-basemap",
    "href": "r/wb_gis_mapping_slides.html#lets-add-a-basemap",
    "title": "GIS mapping with R",
    "section": "Let’s add a basemap",
    "text": "Let’s add a basemap\nWe will use data from Natural Earth, a public domain map dataset\nThere are much more fancy options, but they usually involve creating accounts (e.g. with Google) to access some API\nIn addition, this dataset can be accessed direction from within R thanks to the rOpenSci packages:\n\nrnaturalearth: provides the functions\nrnaturalearthdata: provides the data"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#create-an-sf-object-with-statesprovinces",
    "href": "r/wb_gis_mapping_slides.html#create-an-sf-object-with-statesprovinces",
    "title": "GIS mapping with R",
    "section": "Create an sf object with states/provinces",
    "text": "Create an sf object with states/provinces\nstates_all &lt;- ne_states(\n  country = c(\"canada\", \"united states of america\"),\n  returnclass = \"sf\"\n)\n\nne_ stands for “Natural Earth”"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#select-relevant-statesprovinces",
    "href": "r/wb_gis_mapping_slides.html#select-relevant-statesprovinces",
    "title": "GIS mapping with R",
    "section": "Select relevant states/provinces",
    "text": "Select relevant states/provinces\nstates &lt;- states_all %&gt;%\n  filter(name_en == \"Alaska\" |\n           name_en == \"British Columbia\" |\n           name_en == \"Yukon\" |\n           name_en == \"Northwest Territories\" |\n           name_en ==  \"Alberta\" |\n           name_en == \"California\" |\n           name_en == \"Washington\" |\n           name_en == \"Oregon\" |\n           name_en == \"Idaho\" |\n           name_en == \"Montana\" |\n           name_en == \"Wyoming\" |\n           name_en == \"Colorado\" |\n           name_en == \"Nevada\" |\n           name_en == \"Utah\"\n         )"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#add-the-basemap-to-our-map",
    "href": "r/wb_gis_mapping_slides.html#add-the-basemap-to-our-map",
    "title": "GIS mapping with R",
    "section": "Add the basemap to our map",
    "text": "Add the basemap to our map\n\n\nWhat do we need to make sure of first?\n\n\n\nst_crs(states) == st_crs(ak)\n[1] TRUE"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#add-the-basemap-to-our-map-1",
    "href": "r/wb_gis_mapping_slides.html#add-the-basemap-to-our-map-1",
    "title": "GIS mapping with R",
    "section": "Add the basemap to our map",
    "text": "Add the basemap to our map\nWe add the basemap as a 3rd layer\nMind the order! If you put the basemap last, it will cover your data\nOf course, we will use our nwa_bbox bounding box again\nWe will also break tm_polygons into tm_borders and tm_fill for ak and wes in order to colourise them with slightly different colours"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#add-the-basemap-to-our-map-2",
    "href": "r/wb_gis_mapping_slides.html#add-the-basemap-to-our-map-2",
    "title": "GIS mapping with R",
    "section": "Add the basemap to our map",
    "text": "Add the basemap to our map\ntm_shape(states, bbox = nwa_bbox) +\n  tm_polygons(col = \"#f2f2f2\", lwd = 0.2) +\n  tm_shape(ak) +\n  tm_borders(col = \"#3399ff\") +\n  tm_fill(col = \"#86baff\") +\n  tm_shape(wes) +\n  tm_borders(col = \"#3399ff\") +\n  tm_fill(col = \"#86baff\") +\n  tm_layout(\n    title = \"Glaciers of Western North America\",\n    title.position = c(\"center\", \"top\"),\n    title.size = 1.1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.06, 0.01, 0.09, 0.01),\n    outer.margins = 0,\n    frame.lwd = 0.2\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    size = 1.2,\n    text.size = 0.6\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 1000, 2000),\n    position = c(\"right\", \"BOTTOM\")\n  )"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#tmap-styles",
    "href": "r/wb_gis_mapping_slides.html#tmap-styles",
    "title": "GIS mapping with R",
    "section": "tmap styles",
    "text": "tmap styles\ntmap has a number of styles that you can try\nFor instance, to set the style to “classic”, run the following before making your map:\ntmap_style(\"classic\")\n\nOther options are:\n“white” (default), “gray”, “natural”, “cobalt”, “col_blind”, “albatross”, “beaver”, “bw”, “watercolor”"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#tmap-styles-1",
    "href": "r/wb_gis_mapping_slides.html#tmap-styles-1",
    "title": "GIS mapping with R",
    "section": "tmap styles",
    "text": "tmap styles\nTo return to the default, you need to run\ntmap_style(\"white\")\nor\ntmap_options_reset()\nwhich will reset every tmap option"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#first-lets-map-it",
    "href": "r/wb_gis_mapping_slides.html#first-lets-map-it",
    "title": "GIS mapping with R",
    "section": "First, let’s map it",
    "text": "First, let’s map it\nLet’s use the same tm_borders and tm_fill we just used:\ntm_shape(gnp) +\n  tm_borders(col = \"#3399ff\") +\n  tm_fill(col = \"#86baff\") +\n  tm_layout(\n    title = \"Glaciers of Glacier National Park\",\n    title.position = c(\"center\", \"top\"),\n    legend.title.color = \"#fcfcfc\",\n    legend.text.size = 1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.07, 0.03, 0.07, 0.03),\n    outer.margins = 0\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    text.size = 0.7\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 10, 20),\n    position = c(\"right\", \"BOTTOM\"),\n    text.size = 1\n  )"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#create-an-inset-map",
    "href": "r/wb_gis_mapping_slides.html#create-an-inset-map",
    "title": "GIS mapping with R",
    "section": "Create an inset map",
    "text": "Create an inset map\nAs always, first we check that the CRS are the same:\nst_crs(gnp) == st_crs(ak)\n[1] FALSE\n\nAH!"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#crs-transformation",
    "href": "r/wb_gis_mapping_slides.html#crs-transformation",
    "title": "GIS mapping with R",
    "section": "CRS transformation",
    "text": "CRS transformation\nWe need to reproject gnp into the CRS of our other sf objects (e.g. ak):\ngnp &lt;- st_transform(gnp, st_crs(ak))\n\nWe can verify that the CRS are now the same:\nst_crs(gnp) == st_crs(ak)\n[1] TRUE"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#inset-maps-first-step",
    "href": "r/wb_gis_mapping_slides.html#inset-maps-first-step",
    "title": "GIS mapping with R",
    "section": "Inset maps: first step",
    "text": "Inset maps: first step\nAdd a rectangle showing the location of the GNP map in the main North America map\nWe need to create a new sfc object from the gnp bbox so that we can add it to our previous map as a new layer:\ngnp_zone &lt;- st_bbox(gnp) %&gt;%\n  st_as_sfc()"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#inset-maps-second-step",
    "href": "r/wb_gis_mapping_slides.html#inset-maps-second-step",
    "title": "GIS mapping with R",
    "section": "Inset maps: second step",
    "text": "Inset maps: second step\nCreate a tmap object of the main map. Of course, we need to edit the title. Also, note the presence of our new layer:\nmain_map &lt;- tm_shape(states, bbox = nwa_bbox) +\n  tm_polygons(col = \"#f2f2f2\", lwd = 0.2) +\n  tm_shape(ak) +\n  tm_borders(col = \"#3399ff\") +\n  tm_fill(col = \"#86baff\") +\n  tm_shape(wes) +\n  tm_borders(col = \"#3399ff\") +\n  tm_fill(col = \"#86baff\") +\n  tm_shape(gnp_zone) +\n  tm_borders(lwd = 1.5, col = \"#ff9900\") +\n  tm_layout(\n    title = \"Glaciers of Glacier National Park\",\n    title.position = c(\"center\", \"top\"),\n    title.size = 1.1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.06, 0.01, 0.09, 0.01),\n    outer.margins = 0,\n    frame.lwd = 0.2\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    size = 1.2,\n    text.size = 0.6\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 500, 1000),\n    position = c(\"right\", \"BOTTOM\")\n  )"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#inset-maps-third-step",
    "href": "r/wb_gis_mapping_slides.html#inset-maps-third-step",
    "title": "GIS mapping with R",
    "section": "Inset maps: third step",
    "text": "Inset maps: third step\nCreate a tmap object of the inset map\nWe make sure to matching colours & edit the layouts for better readability:\ninset_map &lt;- tm_shape(gnp) +\n  tm_borders(col = \"#3399ff\") +\n  tm_fill(col = \"#86baff\") +\n  tm_layout(\n    legend.show = F,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.03, 0.03, 0.03, 0.03),\n    outer.margins = 0,\n    frame = \"#ff9900\",\n    frame.lwd = 3\n  )"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#inset-maps-final-step",
    "href": "r/wb_gis_mapping_slides.html#inset-maps-final-step",
    "title": "GIS mapping with R",
    "section": "Inset maps: final step",
    "text": "Inset maps: final step\nCombine the two tmap objects\nWe print the main map & add the inset map with grid::viewport:\nmain_map\nprint(inset_map, vp = viewport(0.41, 0.26, width = 0.5, height = 0.5))"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#map-of-the-agassiz-glacier",
    "href": "r/wb_gis_mapping_slides.html#map-of-the-agassiz-glacier",
    "title": "GIS mapping with R",
    "section": "Map of the Agassiz Glacier",
    "text": "Map of the Agassiz Glacier\nSelect the data points corresponding to the Agassiz Glacier:\nag &lt;- gnp %&gt;% filter(glacname == \"Agassiz Glacier\")"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#map-of-the-agassiz-glacier-1",
    "href": "r/wb_gis_mapping_slides.html#map-of-the-agassiz-glacier-1",
    "title": "GIS mapping with R",
    "section": "Map of the Agassiz Glacier",
    "text": "Map of the Agassiz Glacier\ntm_shape(ag) +\n  tm_polygons() +\n  tm_layout(\n    title = \"Agassiz Glacier\",\n    title.position = c(\"center\", \"top\"),\n    legend.position = c(\"left\", \"bottom\"),\n    legend.title.color = \"#fcfcfc\",\n    legend.text.size = 1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.07, 0.03, 0.07, 0.03),\n    outer.margins = 0\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    text.size = 0.7\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 0.5, 1),\n    position = c(\"right\", \"BOTTOM\"),\n    text.size = 1\n  )"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#map-of-the-agassiz-glacier-2",
    "href": "r/wb_gis_mapping_slides.html#map-of-the-agassiz-glacier-2",
    "title": "GIS mapping with R",
    "section": "Map of the Agassiz Glacier",
    "text": "Map of the Agassiz Glacier\n\n\nNot great …"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#map-based-on-attribute-variables",
    "href": "r/wb_gis_mapping_slides.html#map-based-on-attribute-variables",
    "title": "GIS mapping with R",
    "section": "Map based on attribute variables",
    "text": "Map based on attribute variables\ntm_shape(ag) +\n  tm_polygons(\"year\", palette = \"Blues\") +\n  tm_layout(\n    title = \"Agassiz Glacier\",\n    title.position = c(\"center\", \"top\"),\n    legend.position = c(\"left\", \"bottom\"),\n    legend.title.color = \"#fcfcfc\",\n    legend.text.size = 1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.07, 0.03, 0.07, 0.03),\n    outer.margins = 0\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    text.size = 0.7\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 0.5, 1),\n    position = c(\"right\", \"BOTTOM\"),\n    text.size = 1\n  )"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#using-ggplot2-instead-of-tmap",
    "href": "r/wb_gis_mapping_slides.html#using-ggplot2-instead-of-tmap",
    "title": "GIS mapping with R",
    "section": "Using ggplot2 instead of tmap",
    "text": "Using ggplot2 instead of tmap\nAs an alternative to tmap, ggplot2 can plot maps with the geom_sf function:\nggplot(ag) +\n  geom_sf(aes(fill = year)) +\n  scale_fill_brewer(palette = \"Blues\") +\n  labs(title = \"Agassiz Glacier\") +\n  annotation_scale(location = \"bl\", width_hint = 0.4) +\n  annotation_north_arrow(location = \"tr\", which_north = \"true\",\n                         pad_x = unit(0.75, \"in\"), pad_y = unit(0.5, \"in\"),\n                         style = north_arrow_fancy_orienteering) +\n  theme_bw() +\n  theme(plot.title = element_text(hjust = 0.5))\nThe package ggspatial adds a lot of functionality to ggplot2 for spatial data"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#faceted-map-of-the-retreat-of-agassiz",
    "href": "r/wb_gis_mapping_slides.html#faceted-map-of-the-retreat-of-agassiz",
    "title": "GIS mapping with R",
    "section": "Faceted map of the retreat of Agassiz",
    "text": "Faceted map of the retreat of Agassiz\ntm_shape(ag) +\n  tm_polygons(col = \"#86baff\") +\n  tm_layout(\n    main.title = \"Agassiz Glacier\",\n    main.title.position = c(\"center\", \"top\"),\n    main.title.size = 1.2,\n    legend.position = c(\"left\", \"bottom\"),\n    legend.title.color = \"#fcfcfc\",\n    legend.text.size = 1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0, 0.03, 0, 0.03),\n    outer.margins = 0,\n    panel.label.bg.color = \"#fcfcfc\",\n    frame = F,\n    asp = 0.6\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    size = 1,\n    text.size = 0.6\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 0.5, 1),\n    position = c(\"right\", \"BOTTOM\"),\n    text.size = 0.6\n  ) +\n  tm_facets(\n    by = \"year\",\n    free.coords = F,\n    ncol = 4\n  )"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#animated-map-of-the-retreat-of-agassiz",
    "href": "r/wb_gis_mapping_slides.html#animated-map-of-the-retreat-of-agassiz",
    "title": "GIS mapping with R",
    "section": "Animated map of the Retreat of Agassiz",
    "text": "Animated map of the Retreat of Agassiz\nFirst, we need to create a tmap object with facets:\nagassiz_anim &lt;- tm_shape(ag) +\n  tm_polygons(col = \"#86baff\") +\n  tm_layout(\n    title = \"Agassiz Glacier\",\n    title.position = c(\"center\", \"top\"),\n    legend.position = c(\"left\", \"bottom\"),\n    legend.title.color = \"#fcfcfc\",\n    legend.text.size = 1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.08, 0, 0.08, 0),\n    outer.margins = 0,\n    panel.label.bg.color = \"#fcfcfc\"\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    text.size = 0.7\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 0.5, 1),\n    position = c(\"right\", \"BOTTOM\"),\n    text.size = 1\n  ) +\n  tm_facets(\n    along = \"year\",\n    free.coords = F\n  )"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#animated-map-of-the-retreat-of-agassiz-1",
    "href": "r/wb_gis_mapping_slides.html#animated-map-of-the-retreat-of-agassiz-1",
    "title": "GIS mapping with R",
    "section": "Animated map of the Retreat of Agassiz",
    "text": "Animated map of the Retreat of Agassiz\nThen we can pass that object to tmap_animation:\ntmap_animation(\n  agassiz_anim,\n  filename = \"ag.gif\",\n  dpi = 300,\n  inner.margins = c(0.08, 0, 0.08, 0),\n  delay = 100\n)"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#map-of-ice-thickness-of-agassiz",
    "href": "r/wb_gis_mapping_slides.html#map-of-ice-thickness-of-agassiz",
    "title": "GIS mapping with R",
    "section": "Map of ice thickness of Agassiz",
    "text": "Map of ice thickness of Agassiz\nNow, let’s map the estimated ice thickness on Agassiz Glacier. This time, we use tm_raster:\ntm_shape(ras) +\n  tm_raster(title = \"\") +\n  tm_layout(\n    title = \"Ice thickness (m) of Agassiz Glacier\",\n    title.position = c(\"center\", \"top\"),\n    legend.position = c(\"left\", \"bottom\"),\n    legend.bg.color = \"#ffffff\",\n    legend.text.size = 1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.07, 0.03, 0.07, 0.03),\n    outer.margins = 0\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    text.size = 0.7\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 0.5, 1),\n    position = c(\"right\", \"BOTTOM\"),\n    text.size = 1\n  )"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#combining-with-randolph-data",
    "href": "r/wb_gis_mapping_slides.html#combining-with-randolph-data",
    "title": "GIS mapping with R",
    "section": "Combining with Randolph data",
    "text": "Combining with Randolph data\nAs always, we check whether the CRS are the same:\nst_crs(ag) == st_crs(ras)\n[1] FALSE\nWe need to reproject ag (remember that it is best to avoid reprojecting raster data):\nag %&lt;&gt;% st_transform(st_crs(ras))"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#combining-with-randolph-data-1",
    "href": "r/wb_gis_mapping_slides.html#combining-with-randolph-data-1",
    "title": "GIS mapping with R",
    "section": "Combining with Randolph data",
    "text": "Combining with Randolph data\nThe layers hide each other (the order matters!). You can use tm_borders for one of them or transparency (alpha). We also adjust the legend:\ntm_shape(ras) +\n  tm_raster(title = \"Ice (m)\") +\n  tm_shape(ag) +\n  tm_polygons(\"year\", palette = \"Blues\", alpha = 0.2, title = \"Contour\") +\n  tm_layout(\n    title = \"Ice thickness (m) and retreat of Agassiz Glacier\",\n    title.position = c(\"center\", \"top\"),\n    legend.position = c(\"left\", \"bottom\"),\n    legend.bg.color = \"#ffffff\",\n    legend.text.size = 0.7,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.07, 0.03, 0.07, 0.03),\n    outer.margins = 0\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    text.size = 0.7\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 0.5, 1),\n    position = c(\"right\", \"BOTTOM\"),\n    text.size = 1\n  )"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#refining-raster-maps",
    "href": "r/wb_gis_mapping_slides.html#refining-raster-maps",
    "title": "GIS mapping with R",
    "section": "Refining raster maps",
    "text": "Refining raster maps\nLet’s go back to our ice thickness map:"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#basemap-with-ggmap",
    "href": "r/wb_gis_mapping_slides.html#basemap-with-ggmap",
    "title": "GIS mapping with R",
    "section": "Basemap with ggmap",
    "text": "Basemap with ggmap\nbasemap &lt;- get_map(\n  bbox = c(\n    left = st_bbox(ag)[1],\n    bottom = st_bbox(ag)[2],\n    right = st_bbox(ag)[3],\n    top = st_bbox(ag)[4]\n  ),\n  source = \"osm\"\n)\n\nggmap is a powerful package, but Google now requires an API key obtained through registration"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#basemap-with-basemaps",
    "href": "r/wb_gis_mapping_slides.html#basemap-with-basemaps",
    "title": "GIS mapping with R",
    "section": "Basemap with basemaps",
    "text": "Basemap with basemaps\nThe package basemaps allows to download open source basemap data from several sources, but those cannot easily be combined with sf objects\nThis plots a satellite image of the Agassiz Glacier:\nbasemap_plot(ag, map_service = \"esri\", map_type = \"world_imagery\")"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#satellite-image-of-the-agassiz-glacier",
    "href": "r/wb_gis_mapping_slides.html#satellite-image-of-the-agassiz-glacier",
    "title": "GIS mapping with R",
    "section": "Satellite image of the Agassiz Glacier",
    "text": "Satellite image of the Agassiz Glacier"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#mapview",
    "href": "r/wb_gis_mapping_slides.html#mapview",
    "title": "GIS mapping with R",
    "section": "mapview",
    "text": "mapview\nmapview(gnp)"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#tmap-1",
    "href": "r/wb_gis_mapping_slides.html#tmap-1",
    "title": "GIS mapping with R",
    "section": "tmap",
    "text": "tmap\nSo far, we have used the plot mode of tmap. There is also a view mode which allows interactive viewing in a browser through Leaflet\nChange to view mode:\ntmap_mode(\"view\")\n\nYou can also toggle between modes with ttm\n\nRe-plot the last map we plotted with tmap:\ntmap_last()"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#leaflet",
    "href": "r/wb_gis_mapping_slides.html#leaflet",
    "title": "GIS mapping with R",
    "section": "leaflet",
    "text": "leaflet\nleaflet creates a map widget to which you add layers\nmap &lt;- leaflet()\naddTiles(map)"
  },
  {
    "objectID": "r/wb_gis_mapping_slides.html#resources-2",
    "href": "r/wb_gis_mapping_slides.html#resources-2",
    "title": "GIS mapping with R",
    "section": "Resources",
    "text": "Resources\nHere are some resources on the topic to get started.\n\nR companion to Geographic Information Analysis\nSpatial data analysis"
  },
  {
    "objectID": "r/top_ws.html",
    "href": "r/top_ws.html",
    "title": "R workshops",
    "section": "",
    "text": "Web scraping with  \n\n\n\n\nIntro to GIS mapping with R\n\n\n\n\nIntro R for the humanities\n\n\n\n\nA little demo of R programming",
    "crumbs": [
      "R",
      "<b><em>Workshops</em></b>"
    ]
  },
  {
    "objectID": "r/top_intro.html",
    "href": "r/top_intro.html",
    "title": "Getting started with R",
    "section": "",
    "text": "R is a free and open-source programming language for statistical computing, modelling, and graphics, with an unbeatable collection of statistical packages. It is extremely popular in some academic fields such as statistics, biology, bioinformatics, data mining, data analysis, and linguistics.\nThis introductory course does not assume any prior knowledge.\n\n Start course ➤",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>"
    ]
  },
  {
    "objectID": "r/top_hpc.html",
    "href": "r/top_hpc.html",
    "title": "High-performance R",
    "section": "",
    "text": "R is a free and open-source statistical programming language that has become the norm in several data science fields thanks to its rich packages ecosystem. It is however a slow language with high memory usage.\nPeople interested in training deep learning models, running computations on massive datasets, or carrying out large numerical simulations would be better off turning to faster languages such as Python with Numba or JAX, Julia, or Chapel—all of which we also teach.\nFor those who are already R users though, with computations that are not monstrous but take longer than seems reasonable, this course will cover benchmarking, various forms of optimizations, and several parallelization techniques. You will also learn how to run R on the Alliance supercomputers efficiently.\n\n Start course ➤",
    "crumbs": [
      "R",
      "<b><em>High-performance R</em></b>"
    ]
  },
  {
    "objectID": "r/intro_tidyverse.html",
    "href": "r/intro_tidyverse.html",
    "title": "The tidyverse",
    "section": "",
    "text": "The tidyverse is a set of packages which attempts to make R more consistent. R was written by statisticians and it is a bit quirky. The tidyverse makes it look more like other programming languages which were developed by computer scientists. It is a different style of writing R code and it is by no means necessary.",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "The tidyverse"
    ]
  },
  {
    "objectID": "r/intro_tidyverse.html#a-glimpse-at-the-tidyverse",
    "href": "r/intro_tidyverse.html#a-glimpse-at-the-tidyverse",
    "title": "The tidyverse",
    "section": "A glimpse at the tidyverse",
    "text": "A glimpse at the tidyverse\nThe best introduction to the tidyverse is probably the book R for Data Science by Hadley Wickham and Garrett Grolemund.\nPosit (the company formerly known as RStudio Inc. behind the tidyverse) developed a series of useful cheatsheets. Below are links to the ones you are the most likely to use as you get started with R.\n\nData import\nThe first thing you often need to do is to import your data into R. This is done with readr.\n\n\n\nfrom Posit Cheatsheets\n\n\n\nData transformation\nYou then often need to transformation your data into the right format. This is done with the packages dplyr and tidyr.\n\n\n\nfrom Posit Cheatsheets\n\n\n\n\nfrom Posit Cheatsheets\n\n\n\nVisualization\nVisualization in the tidyverse is done with the ggplot2 package which we will explore in the next section.\n\n\n\nfrom Posit Cheatsheets\n\n\n\nWorking with factors\nThe package forcats offers the tidyverse approach to working with factors.\n\n\nfrom Posit Cheatsheets\n\n\n\nWorking with strings\nstringr is for strings.\n\n\n\nfrom Posit Cheatsheets\n\n\n\nWorking with dates\nlubridate will help you deal with dates.\n\n\n\nfrom Posit Cheatsheets\n\n\n\nFunctional programming\nFinally, purrr is the tidyverse equivalent to the apply functions in base R: a way to run functions on functions.\n\n\n\nfrom Posit Cheatsheets",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "The tidyverse"
    ]
  },
  {
    "objectID": "r/intro_tidyverse.html#base-r-or-tidyverse",
    "href": "r/intro_tidyverse.html#base-r-or-tidyverse",
    "title": "The tidyverse",
    "section": "Base R or tidyverse?",
    "text": "Base R or tidyverse?\n“Base R” refers to the use of the standard R library. The expression is often used in contrast to the tidyverse.\nThere are a many things that you can do with either base R or the tidyverse. Because the syntaxes are quite different, it almost feels like using two different languages and people tend to favour one or the other.\nWhich one you should use is really up to you.\n\n\n\n\n\n\n\nBase R\nTidyverse\n\n\n\n\nPreferred by old-schoolers\nIncreasingly becoming the norm with newer R users\n\n\nMore stable\nMore consistent syntax and behaviour\n\n\nDoesn’t require installing and loading packages\nMore and more resources and documentation available\n\n\n\nIn truth, even though the tidyverse has many detractors amongst old R users, it is increasingly becoming the norm.",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "The tidyverse"
    ]
  },
  {
    "objectID": "r/intro_resources.html",
    "href": "r/intro_resources.html",
    "title": "Resources",
    "section": "",
    "text": "The R community is dynamic and offers a lot of online resources, from IDEs to Q&A, to workshops, books, or publications.\nThis section provides a selection of useful sites.\n\n\nMain sites\n\nR website\nComprehensive R Archive Network (CRAN): R versions and packages\n\n\n\nPosit and RStudio IDE\n\nPosit website (Posit was formerly called RStudio Inc.)\nPosit cheatsheets\n\n\n\nForums and Q&A\n\nStack Overflow [r] tag wiki\nStack Overflow [r] tag questions\nPosit Discourse\n\n\n\nDocumentation as pdf\n\nContributed documentation\nIntro books\n\n\n\nSoftware Carpentry online workshops\n\nProgramming with R\nR for Reproducible Scientific Analysis\nData analysis using R in the digital humanities\n\n\n\nOnline books\n\nR for Data Science (heavily based on the tidyverse)\nR Packages (how to create packages)\nR Programming for Data Science\nMastering Software Development in R\n\n\n\nR research\n\nThe R Journal",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "Resources"
    ]
  },
  {
    "objectID": "r/intro_plotting.html",
    "href": "r/intro_plotting.html",
    "title": "Plotting",
    "section": "",
    "text": "This section focuses on plotting in R with the package ggplot2 from the tidyverse.",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "Plotting"
    ]
  },
  {
    "objectID": "r/intro_plotting.html#the-data",
    "href": "r/intro_plotting.html#the-data",
    "title": "Plotting",
    "section": "The data",
    "text": "The data\nR comes with a number of datasets. You can get a list by running data(). The ggplot2 package provides additional ones. We will use the mpg dataset from ggplot2.\nTo access the data, let’s load the package:\n\nlibrary(ggplot2)\n\nHere is what that dataset looks like:\n\nmpg\n\n# A tibble: 234 × 11\n   manufacturer model      displ  year   cyl trans      drv     cty   hwy fl   \n   &lt;chr&gt;        &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;      &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;\n 1 audi         a4           1.8  1999     4 auto(l5)   f        18    29 p    \n 2 audi         a4           1.8  1999     4 manual(m5) f        21    29 p    \n 3 audi         a4           2    2008     4 manual(m6) f        20    31 p    \n 4 audi         a4           2    2008     4 auto(av)   f        21    30 p    \n 5 audi         a4           2.8  1999     6 auto(l5)   f        16    26 p    \n 6 audi         a4           2.8  1999     6 manual(m5) f        18    26 p    \n 7 audi         a4           3.1  2008     6 auto(av)   f        18    27 p    \n 8 audi         a4 quattro   1.8  1999     4 manual(m5) 4        18    26 p    \n 9 audi         a4 quattro   1.8  1999     4 auto(l5)   4        16    25 p    \n10 audi         a4 quattro   2    2008     4 manual(m6) 4        20    28 p    \n   class  \n   &lt;chr&gt;  \n 1 compact\n 2 compact\n 3 compact\n 4 compact\n 5 compact\n 6 compact\n 7 compact\n 8 compact\n 9 compact\n10 compact\n# ℹ 224 more rows\n\n\n?mpg will give you information on the variables. In particular:\n\ndispl contains data on engine displacement (a measure of engine size and thus power) in litres (L).\nhwy contains data on fuel economy while driving on highways in miles per gallon (mpg).\ndrv represents the type of drive train (front-wheel drive, rear wheel drive, 4WD).\nclass represents the type of car.\n\nWe are interested in the relationship between engine size and fuel economy and see how the type of drive train and/or the type of car might affect this relationship.",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "Plotting"
    ]
  },
  {
    "objectID": "r/intro_plotting.html#base-r-plotting",
    "href": "r/intro_plotting.html#base-r-plotting",
    "title": "Plotting",
    "section": "Base R plotting",
    "text": "Base R plotting\nR contains built-in plotting capability thanks to the plot() function.\nA basic version of our plot would be:\n\nplot(\n  mpg$displ,\n  mpg$hwy,\n  main = \"Fuel consumption per engine size on highways\",\n  xlab = \"Engine size (L)\",\n  ylab = \"Fuel economy (mpg) on highways\"\n)",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "Plotting"
    ]
  },
  {
    "objectID": "r/intro_plotting.html#grammar-of-graphics",
    "href": "r/intro_plotting.html#grammar-of-graphics",
    "title": "Plotting",
    "section": "Grammar of graphics",
    "text": "Grammar of graphics\nLeland Wilkinson developed the concept of grammar of graphics in his 2005 book The Grammar of Graphics. By breaking down statistical graphs into components following a set of rules, any plot can be described and constructed in a rigorous fashion.\nThis was further refined by Hadley Wickham in his 2010 article A Layered Grammar of Graphics and implemented in the package ggplot2 (that’s what the 2 “g” stand for in “ggplot”).\nggplot2 has become the dominant graphing package in R. Let’s see how to construct a plot with this package.",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "Plotting"
    ]
  },
  {
    "objectID": "r/intro_plotting.html#plotting-with-ggplot2",
    "href": "r/intro_plotting.html#plotting-with-ggplot2",
    "title": "Plotting",
    "section": "Plotting with ggplot2",
    "text": "Plotting with ggplot2\n\nYou can find the ggplot2 cheatsheet here.\n\n\nThe Canvas\nThe first component is the data:\n\nggplot(data = mpg)\n\n\n\n\n\n\n\n\n\nThis can be simplified into ggplot(mpg).\n\nThe second component sets the way variables are mapped on the axes. This is done with the aes() (aesthetics) function:\n\nggplot(data = mpg, mapping = aes(x = displ, y = hwy))\n\n\n\n\n\n\n\n\n\nThis can be simplified into ggplot(mpg, aes(displ, hwy)).\n\n\n\nGeometric representations of the data\nOnto this canvas, we can add “geoms” (geometrical objects) representing the data. The type of “geom” defines the type of representation (e.g. boxplot, histogram, bar chart).\nTo represent the data as a scatterplot, we use the geom_point() function:\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point()\n\n\n\n\n\n\n\n\nWe can colour-code the points in the scatterplot based on the drv variable, showing the lower fuel efficiency of 4WD vehicles:\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = drv))\n\n\n\n\n\n\n\n\nOr we can colour-code them based on the class variable:\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = class))\n\n\n\n\n\n\n\n\nMultiple “geoms” can be added on top of each other. For instance, we can add a smoothed conditional means function that aids at seeing patterns in the data with geom_smooth():\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = class)) +\n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThanks to the colour-coding of the types of car, we can see that the cluster of points in the top right corner all belong to the same type: 2 seaters. Those are outliers with high power, yet high few efficiency due to their smaller size.\nThe default smoothing function uses the LOESS (locally estimated scatterplot smoothing) method, which is a nonlinear regression. But maybe a linear model would actually show the general trend better. We can change the method by passing it as an argument to geom_smooth():\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = class)) +\n  geom_smooth(method = lm)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nOf course, we could apply the smoothing function to each class instead of the entire data. It creates a busy plot but shows that the downward trend remains true within each type of car:\n\nggplot(mpg, aes(x = displ, y = hwy, color = class)) +\n  geom_point(aes(color = class)) +\n  geom_smooth(method = lm)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nOther arguments to geom_smooth() can set the line width, color, or whether or not the standard error (se) is shown:\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = class)) +\n  geom_smooth(\n    method = lm,\n    se = FALSE,\n    color = \"#999999\",\n    linewidth = 0.5\n  )\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nColour scales\nIf we want to change the colour scale, we add another layer for this:\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = class)) +\n  scale_color_brewer(palette = \"Dark2\") +\n  geom_smooth(\n    method = lm,\n    se = FALSE,\n    color = \"#999999\",\n    linewidth = 0.5\n  )\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nscale_color_brewer(), based on color brewer 2.0, is one of many methods to change the color scale. Here is the list of available scales for this particular method:\n\n\n\nLabels\nWe can keep on adding layers. For instance, the labs() function allows to set title, subtitle, captions, tags, axes labels, etc.\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = class)) +\n  scale_color_brewer(palette = \"Dark2\") +\n  geom_smooth(\n    method = lm,\n    se = FALSE,\n    color = \"#999999\",\n    linewidth = 0.5\n  ) +\n  labs(\n    title = \"Fuel consumption per engine size on highways\",\n    x = \"Engine size (L)\",\n    y = \"Fuel economy (mpg) on highways\",\n    color = \"Type of car\",\n    caption = \"EPA data from https://fueleconomy.gov/\"\n  )\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nThemes\nAnother optional layer sets one of several preset themes.\nEdward Tufte developed, amongst others, the principle of data-ink ratio which emphasizes that ink should be used primarily where it communicates meaningful messages. It is indeed common to see charts where more ink is used in labels or background than in the actual representation of the data.\nThe default ggplot2 theme could be criticized as not following this principle. Let’s change it:\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = class)) +\n  scale_color_brewer(palette = \"Dark2\") +\n  geom_smooth(\n    method = lm,\n    se = FALSE,\n    color = \"#999999\",\n    linewidth = 0.5\n  ) +\n  labs(\n    title = \"Fuel consumption per engine size on highways\",\n    x = \"Engine size (L)\",\n    y = \"Fuel economy (mpg) on highways\",\n    color = \"Type of car\",\n    caption = \"EPA data from https://fueleconomy.gov/\"\n  ) +\n  theme_classic()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThe theme() function allows to tweak the theme in any number of ways. For instance, what if we don’t like the default position of the title and we would rather have it centered?\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = class)) +\n  scale_color_brewer(palette = \"Dark2\") +\n  geom_smooth(\n    method = lm,\n    se = FALSE,\n    color = \"#999999\",\n    linewidth = 0.5\n  ) +\n  labs(\n    title = \"Fuel consumption per engine size on highways\",\n    x = \"Engine size (L)\",\n    y = \"Fuel economy (mpg) on highways\",\n    color = \"Type of car\",\n    caption = \"EPA data from https://fueleconomy.gov/\"\n  ) +\n  theme_classic() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nWe can also move the legend to give more space to the actual graph:\n\nggplot(mpg, aes(x = displ, y = hwy)) +\n  geom_point(aes(color = class)) +\n  scale_color_brewer(palette = \"Dark2\") +\n  geom_smooth(\n    method = lm,\n    se = FALSE,\n    color = \"#999999\",\n    linewidth = 0.5\n  ) +\n  labs(\n    title = \"Fuel consumption per engine size on highways\",\n    x = \"Engine size (L)\",\n    y = \"Fuel economy (mpg) on highways\",\n    color = \"Type of car\",\n    caption = \"EPA data from https://fueleconomy.gov/\"\n  ) +\n  theme_classic() +\n  theme(plot.title = element_text(hjust = 0.5), legend.position = \"bottom\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nAs you could see, ggplot2 works by adding a number of layers on top of each other, all following a standard set of rules, or “grammar”. This way, a vast array of graphs can be created by organizing simple components.",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "Plotting"
    ]
  },
  {
    "objectID": "r/intro_plotting.html#ggplot2-extensions",
    "href": "r/intro_plotting.html#ggplot2-extensions",
    "title": "Plotting",
    "section": "ggplot2 extensions",
    "text": "ggplot2 extensions\nThanks to its vast popularity, ggplot2 has seen a proliferation of packages extending its capabilities.\n\nCombining plots\nFor instance the patchwork package allows to easily combine multiple plots on the same frame.\nLet’s add a second plot next to our plot. To add plots side by side, we simply add them to each other. We also make a few changes to the labels to improve the plots integration:\n\nlibrary(patchwork)\n\nggplot(mpg, aes(x = displ, y = hwy)) +        # First plot\n  geom_point(aes(color = class)) +\n  scale_color_brewer(palette = \"Dark2\") +\n  geom_smooth(\n    method = lm,\n    se = FALSE,\n    color = \"#999999\",\n    linewidth = 0.5\n  ) +\n  labs(\n    x = \"Engine size (L)\",\n    y = \"Fuel economy (mpg) on highways\",\n    color = \"Type of car\"\n  ) +\n  theme_classic() +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    legend.position = c(0.7, 0.75),           # Better legend position\n    legend.background = element_rect(         # Add a frame to the legend\n      linewidth = 0.1,\n      linetype = \"solid\",\n      colour = \"black\"\n    )\n  ) +\n  ggplot(mpg, aes(x = displ, y = hwy)) +      # Second plot\n  geom_point(aes(color = drv)) +\n  scale_color_brewer(palette = \"Dark2\") +\n  labs(\n    x = \"Engine size (L)\",\n    y = element_blank(),                      # Remove redundant label\n    color = \"Type of drive train\",\n    caption = \"EPA data from https://fueleconomy.gov/\"\n  ) +\n  theme_classic() +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    legend.position = c(0.7, 0.87),\n    legend.background = element_rect(\n      linewidth = 0.1,\n      linetype = \"solid\",\n      colour = \"black\"\n    )\n  )\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nExtensions list\nAnother popular extension is the gganimate package which allows to create data animations.\nA full list of extensions for ggplot2 is shown below (here is the website):",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "Plotting"
    ]
  },
  {
    "objectID": "r/intro_manipulate.html",
    "href": "r/intro_manipulate.html",
    "title": "Data extraction",
    "section": "",
    "text": "It is often useful to focus on sections of the data to plot or analyse. In this section, we will see how to extract various elements of the us_contagious_diseases dataset from the dslabs package.",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "Data extraction"
    ]
  },
  {
    "objectID": "r/intro_manipulate.html#load-packages",
    "href": "r/intro_manipulate.html#load-packages",
    "title": "Data extraction",
    "section": "Load packages",
    "text": "Load packages\nOne of the tidyverse packages is very useful for data manipulation: dplyr. Let’s load the dslabs package again as well as dplyr:\n\nlibrary(dslabs)\nlibrary(dplyr)",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "Data extraction"
    ]
  },
  {
    "objectID": "r/intro_manipulate.html#indexing",
    "href": "r/intro_manipulate.html#indexing",
    "title": "Data extraction",
    "section": "Indexing",
    "text": "Indexing\nYou can extract a subset of the data using their position by indexing. Indexing in R starts with 1 (in many languages, the first index is 0) and it is done with square brackets. Since a data frame has two dimensions, there are two possible indices in the square brackets:\n\nthe row index,\nthe column index.\n\nYou can index a single element:\n\nus_contagious_diseases[1, 1]\n\n[1] Hepatitis A\nLevels: Hepatitis A Measles Mumps Pertussis Polio Rubella Smallpox\n\nus_contagious_diseases[1, 2]\n\n[1] Alabama\n51 Levels: Alabama Alaska Arizona Arkansas California Colorado ... Wyoming\n\n\nOr a full row:\n\nus_contagious_diseases[1, ]\n\n      disease   state year weeks_reporting count population\n1 Hepatitis A Alabama 1966              50   321    3345787\n\nus_contagious_diseases[3000, ]\n\n     disease                state year weeks_reporting count population\n3000 Measles District Of Columbia 1981              27     2     631010\n\n\n\n\nYour turn:\n\nHow would you index the year column?",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "Data extraction"
    ]
  },
  {
    "objectID": "r/intro_manipulate.html#filtering-rows",
    "href": "r/intro_manipulate.html#filtering-rows",
    "title": "Data extraction",
    "section": "Filtering rows",
    "text": "Filtering rows\nYou can also filter data points based on their values:\n\nus_contagious_diseases |&gt;\n  filter(state == \"California\") |&gt;\n  count()\n\n    n\n1 315\n\n\n\n\nYour turn:\n\nHow many data points are there for the state of Arizona?\n\n\nus_contagious_diseases |&gt;\n  filter(state == \"California\" & year &gt; 2000)\n\n       disease      state year weeks_reporting count population\n1  Hepatitis A California 2001              40  1599   34199784\n2  Hepatitis A California 2002              49  1364   34529758\n3  Hepatitis A California 2003              46  1045   34861711\n4  Hepatitis A California 2004              48   788   35195792\n5  Hepatitis A California 2005              49   905   35532154\n6  Hepatitis A California 2006              52   688   35870957\n7  Hepatitis A California 2007              51   312   36212364\n8  Hepatitis A California 2008              52   337   36556548\n9  Hepatitis A California 2009              52   239   36903684\n10 Hepatitis A California 2010              49   201   37253956\n11 Hepatitis A California 2011              49   176   37607525\n12     Measles California 2001              40    34   34199784\n13     Measles California 2002              33     0   34529758\n14       Mumps California 2001              49    37   34199784\n15       Mumps California 2002              49    66   34529758\n16   Pertussis California 2001              40   440   34199784\n17   Pertussis California 2002              43   698   34529758\n18   Pertussis California 2003              41   635   34861711\n19   Pertussis California 2004              36   498   35195792\n20   Pertussis California 2005              45  1609   35532154\n21   Pertussis California 2006              42   831   35870957\n22   Pertussis California 2007              29    95   36212364\n23   Pertussis California 2008              39   276   36556548\n24   Pertussis California 2009              40   415   36903684\n25   Pertussis California 2010              48  1265   37253956\n26   Pertussis California 2011              49  1145   37607525\n27     Rubella California 2001               1     0   34199784\n28     Rubella California 2002              29     2   34529758\n\n\n\nus_contagious_diseases |&gt;\n  filter(state == \"California\" & year &gt; 2000) |&gt;\n  arrange(year)\n\n       disease      state year weeks_reporting count population\n1  Hepatitis A California 2001              40  1599   34199784\n2      Measles California 2001              40    34   34199784\n3        Mumps California 2001              49    37   34199784\n4    Pertussis California 2001              40   440   34199784\n5      Rubella California 2001               1     0   34199784\n6  Hepatitis A California 2002              49  1364   34529758\n7      Measles California 2002              33     0   34529758\n8        Mumps California 2002              49    66   34529758\n9    Pertussis California 2002              43   698   34529758\n10     Rubella California 2002              29     2   34529758\n11 Hepatitis A California 2003              46  1045   34861711\n12   Pertussis California 2003              41   635   34861711\n13 Hepatitis A California 2004              48   788   35195792\n14   Pertussis California 2004              36   498   35195792\n15 Hepatitis A California 2005              49   905   35532154\n16   Pertussis California 2005              45  1609   35532154\n17 Hepatitis A California 2006              52   688   35870957\n18   Pertussis California 2006              42   831   35870957\n19 Hepatitis A California 2007              51   312   36212364\n20   Pertussis California 2007              29    95   36212364\n21 Hepatitis A California 2008              52   337   36556548\n22   Pertussis California 2008              39   276   36556548\n23 Hepatitis A California 2009              52   239   36903684\n24   Pertussis California 2009              40   415   36903684\n25 Hepatitis A California 2010              49   201   37253956\n26   Pertussis California 2010              48  1265   37253956\n27 Hepatitis A California 2011              49   176   37607525\n28   Pertussis California 2011              49  1145   37607525\n\n\n\nus_contagious_diseases |&gt;\n  filter(state == \"California\" & year &gt; 2000) |&gt;\n  arrange(count)\n\n       disease      state year weeks_reporting count population\n1      Measles California 2002              33     0   34529758\n2      Rubella California 2001               1     0   34199784\n3      Rubella California 2002              29     2   34529758\n4      Measles California 2001              40    34   34199784\n5        Mumps California 2001              49    37   34199784\n6        Mumps California 2002              49    66   34529758\n7    Pertussis California 2007              29    95   36212364\n8  Hepatitis A California 2011              49   176   37607525\n9  Hepatitis A California 2010              49   201   37253956\n10 Hepatitis A California 2009              52   239   36903684\n11   Pertussis California 2008              39   276   36556548\n12 Hepatitis A California 2007              51   312   36212364\n13 Hepatitis A California 2008              52   337   36556548\n14   Pertussis California 2009              40   415   36903684\n15   Pertussis California 2001              40   440   34199784\n16   Pertussis California 2004              36   498   35195792\n17   Pertussis California 2003              41   635   34861711\n18 Hepatitis A California 2006              52   688   35870957\n19   Pertussis California 2002              43   698   34529758\n20 Hepatitis A California 2004              48   788   35195792\n21   Pertussis California 2006              42   831   35870957\n22 Hepatitis A California 2005              49   905   35532154\n23 Hepatitis A California 2003              46  1045   34861711\n24   Pertussis California 2011              49  1145   37607525\n25   Pertussis California 2010              48  1265   37253956\n26 Hepatitis A California 2002              49  1364   34529758\n27 Hepatitis A California 2001              40  1599   34199784\n28   Pertussis California 2005              45  1609   35532154\n\n\n\nus_contagious_diseases |&gt;\n  filter(state == \"California\" & year &gt; 2000) |&gt;\n  arrange(desc(count))\n\n       disease      state year weeks_reporting count population\n1    Pertussis California 2005              45  1609   35532154\n2  Hepatitis A California 2001              40  1599   34199784\n3  Hepatitis A California 2002              49  1364   34529758\n4    Pertussis California 2010              48  1265   37253956\n5    Pertussis California 2011              49  1145   37607525\n6  Hepatitis A California 2003              46  1045   34861711\n7  Hepatitis A California 2005              49   905   35532154\n8    Pertussis California 2006              42   831   35870957\n9  Hepatitis A California 2004              48   788   35195792\n10   Pertussis California 2002              43   698   34529758\n11 Hepatitis A California 2006              52   688   35870957\n12   Pertussis California 2003              41   635   34861711\n13   Pertussis California 2004              36   498   35195792\n14   Pertussis California 2001              40   440   34199784\n15   Pertussis California 2009              40   415   36903684\n16 Hepatitis A California 2008              52   337   36556548\n17 Hepatitis A California 2007              51   312   36212364\n18   Pertussis California 2008              39   276   36556548\n19 Hepatitis A California 2009              52   239   36903684\n20 Hepatitis A California 2010              49   201   37253956\n21 Hepatitis A California 2011              49   176   37607525\n22   Pertussis California 2007              29    95   36212364\n23       Mumps California 2002              49    66   34529758\n24       Mumps California 2001              49    37   34199784\n25     Measles California 2001              40    34   34199784\n26     Rubella California 2002              29     2   34529758\n27     Measles California 2002              33     0   34529758\n28     Rubella California 2001               1     0   34199784",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "Data extraction"
    ]
  },
  {
    "objectID": "r/intro_manipulate.html#selecting-columns",
    "href": "r/intro_manipulate.html#selecting-columns",
    "title": "Data extraction",
    "section": "Selecting columns",
    "text": "Selecting columns\nWe saw how to index columns from their position. It is also possible to select them based on their names:\n\nhead(us_contagious_diseases$year, 50)\n\n [1] 1966 1967 1968 1969 1970 1971 1972 1973 1974 1975 1976 1977 1978 1979 1980\n[16] 1981 1982 1983 1984 1985 1986 1987 1988 1989 1990 1991 1992 1993 1994 1995\n[31] 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010\n[46] 2011 1966 1967 1968 1969\n\n\nIf you want to select several columns, you can use the select() function from dplyr:\n\nus_contagious_diseases |&gt;\n  filter(state == \"California\" & year &gt; 2000 & disease == \"Hepatitis A\") |&gt;\n  select(year, count, population)\n\n   year count population\n1  2001  1599   34199784\n2  2002  1364   34529758\n3  2003  1045   34861711\n4  2004   788   35195792\n5  2005   905   35532154\n6  2006   688   35870957\n7  2007   312   36212364\n8  2008   337   36556548\n9  2009   239   36903684\n10 2010   201   37253956\n11 2011   176   37607525",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "Data extraction"
    ]
  },
  {
    "objectID": "r/intro_manipulate.html#grouping-data",
    "href": "r/intro_manipulate.html#grouping-data",
    "title": "Data extraction",
    "section": "Grouping data",
    "text": "Grouping data\nIt is often useful to group data by categories to compute some summary statistics.\nFor instance, we can group by year and calculate the total numbers of infections:\n\nus_contagious_diseases |&gt;\n  group_by(year) |&gt;\n  summarise(total = sum(count))\n\n# A tibble: 84 × 2\n    year  total\n   &lt;dbl&gt;  &lt;dbl&gt;\n 1  1928 524563\n 2  1929 380196\n 3  1930 439289\n 4  1931 482886\n 5  1932 404683\n 6  1933 391485\n 7  1934 739509\n 8  1935 739224\n 9  1936 292530\n10  1937 314425\n# ℹ 74 more rows\n\n\nAlternatively, we can group by state and get the totals:\n\nus_contagious_diseases |&gt;\n  group_by(state) |&gt; \n  summarise(total = sum(count))\n\n# A tibble: 51 × 2\n   state                  total\n   &lt;fct&gt;                  &lt;dbl&gt;\n 1 Alabama               257979\n 2 Alaska                 29136\n 3 Arizona               240233\n 4 Arkansas              177556\n 5 California           1906067\n 6 Colorado              322845\n 7 Connecticut           463148\n 8 Delaware               44427\n 9 District Of Columbia   77012\n10 Florida               268383\n# ℹ 41 more rows\n\n\nWe can also group by year and state and get the totals:\n\nus_contagious_diseases |&gt;\n  group_by(year, state) |&gt; \n  summarise(total = sum(count))\n\n`summarise()` has grouped output by 'year'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 4,284 × 3\n# Groups:   year [84]\n    year state                total\n   &lt;dbl&gt; &lt;fct&gt;                &lt;dbl&gt;\n 1  1928 Alabama               9246\n 2  1928 Alaska                   0\n 3  1928 Arizona               1268\n 4  1928 Arkansas              9157\n 5  1928 California            4960\n 6  1928 Colorado              2510\n 7  1928 Connecticut          10247\n 8  1928 Delaware               607\n 9  1928 District Of Columbia  2609\n10  1928 Florida               1892\n# ℹ 4,274 more rows",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "Data extraction"
    ]
  },
  {
    "objectID": "r/intro_import.html",
    "href": "r/intro_import.html",
    "title": "Data import and export",
    "section": "",
    "text": "So far, we have used a well-formatted dataset. In the real world, things are often not this nice and tidy…\nIn this section, we will learn how to handle real data.",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "Data import and export"
    ]
  },
  {
    "objectID": "r/intro_import.html#reading-in-data",
    "href": "r/intro_import.html#reading-in-data",
    "title": "Data import and export",
    "section": "Reading in data",
    "text": "Reading in data\nThe readr package from the tidyverse provides a number of functions to read in text files with tabular data (e.g. comma-separated values (CSV) or tab-separated values (TSV) files).\nLet’s load it:\n\nlibrary(readr)\n\nThe read_csv() function allows to read in CSV files that are either stored locally or from a URL.\nLet’s use it to load a CSV file with mock archaeological data which is at the URL https://mint.westdri.ca/r/hss_data/arc1.csv:\n\narc1 &lt;- read_csv(\"https://mint.westdri.ca/r/hss_data/arc1.csv\")\n\nRows: 6 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): Site, Date, Number of artifacts, Name of PI, Comments\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nIf the file was in your machine, you would provide its path instead of the URL.\n\nHere is our data:\n\narc1\n\n# A tibble: 6 × 5\n  Site  Date      `Number of artifacts` `Name of PI`\n  &lt;chr&gt; &lt;chr&gt;     &lt;chr&gt;                 &lt;chr&gt;       \n1 E1    13/2/2001 4                     John Doe    \n2 E1    14/2/2001 3                     John Doe    \n3 A2    26/3/2003 N/A                   Paul Smith  \n4 B18   4/5/2006  5                     Paul Smith  \n5 B7    4/5/2006  5                     n/a         \n6 B3    4/5/2006  5                     P. Smith    \n  Comments                          \n  &lt;chr&gt;                             \n1 &lt;NA&gt;                              \n2 &lt;NA&gt;                              \n3 Artifacts still need to be counted\n4 &lt;NA&gt;                              \n5 &lt;NA&gt;                              \n6 &lt;NA&gt;",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "Data import and export"
    ]
  },
  {
    "objectID": "r/intro_import.html#improper-na",
    "href": "r/intro_import.html#improper-na",
    "title": "Data import and export",
    "section": "Improper NA",
    "text": "Improper NA\nIn R, missing values are represented by NA (not available). It is a constant that R understands and can deal with, so it is important that all missing values are represented properly.\nWhen you enter data (say in an Excel file or CSV file), leave an empty cell for missing values: R will then transform them automatically into NA.\nBecause this data was not entered properly, we have to fix our missing values. One way to go about this is to replace the characters representing missing values in the file (\"N/A\" and \"n/a\") by NA:\n\nis.na(arc1) &lt;- arc1 == \"N/A\"\nis.na(arc1) &lt;- arc1 == \"n/a\"\narc1\n\n# A tibble: 6 × 5\n  Site  Date      `Number of artifacts` `Name of PI`\n  &lt;chr&gt; &lt;chr&gt;     &lt;chr&gt;                 &lt;chr&gt;       \n1 E1    13/2/2001 4                     John Doe    \n2 E1    14/2/2001 3                     John Doe    \n3 A2    26/3/2003 &lt;NA&gt;                  Paul Smith  \n4 B18   4/5/2006  5                     Paul Smith  \n5 B7    4/5/2006  5                     &lt;NA&gt;        \n6 B3    4/5/2006  5                     P. Smith    \n  Comments                          \n  &lt;chr&gt;                             \n1 &lt;NA&gt;                              \n2 &lt;NA&gt;                              \n3 Artifacts still need to be counted\n4 &lt;NA&gt;                              \n5 &lt;NA&gt;                              \n6 &lt;NA&gt;                              \n\n\nNow, we have another problem to fix: readr is very good at guessing the types of the various variables. Unfortunately, the character \"N/A\" in the Number of artifacts column prevented it to guess the type properly: it should be a double (a numerical value) and not a character. We can fix this too:\n\narc1$`Number of artifacts` &lt;- as.double(arc1$`Number of artifacts`)\narc1\n\n# A tibble: 6 × 5\n  Site  Date      `Number of artifacts` `Name of PI`\n  &lt;chr&gt; &lt;chr&gt;                     &lt;dbl&gt; &lt;chr&gt;       \n1 E1    13/2/2001                     4 John Doe    \n2 E1    14/2/2001                     3 John Doe    \n3 A2    26/3/2003                    NA Paul Smith  \n4 B18   4/5/2006                      5 Paul Smith  \n5 B7    4/5/2006                      5 &lt;NA&gt;        \n6 B3    4/5/2006                      5 P. Smith    \n  Comments                          \n  &lt;chr&gt;                             \n1 &lt;NA&gt;                              \n2 &lt;NA&gt;                              \n3 Artifacts still need to be counted\n4 &lt;NA&gt;                              \n5 &lt;NA&gt;                              \n6 &lt;NA&gt;                              \n\n\nAlternatively, it is simpler to have read_csv() properly recognize the missing values. This can be done thanks to the na argument:\n\narc1 &lt;- read_csv(\n  \"https://mint.westdri.ca/r/hss_data/arc1.csv\",\n  na = c(\"N/A\", \"n/a\")\n)\n\nRows: 6 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): Site, Date, Name of PI, Comments\ndbl (1): Number of artifacts\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\narc1\n\n# A tibble: 6 × 5\n  Site  Date      `Number of artifacts` `Name of PI`\n  &lt;chr&gt; &lt;chr&gt;                     &lt;dbl&gt; &lt;chr&gt;       \n1 E1    13/2/2001                     4 John Doe    \n2 E1    14/2/2001                     3 John Doe    \n3 A2    26/3/2003                    NA Paul Smith  \n4 B18   4/5/2006                      5 Paul Smith  \n5 B7    4/5/2006                      5 &lt;NA&gt;        \n6 B3    4/5/2006                      5 P. Smith    \n  Comments                            \n  &lt;chr&gt;                               \n1 \"\"                                  \n2 \"\"                                  \n3 \"Artifacts still need to be counted\"\n4 \"\"                                  \n5 \"\"                                  \n6 \"\"                                  \n\n\nA benefit of this approach is that read_csv() now automatically detects the proper data type of Number of artifacts (since there is no more confusing character in what is otherwise a column of doubles).",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "Data import and export"
    ]
  },
  {
    "objectID": "r/intro_import.html#dealing-with-dates",
    "href": "r/intro_import.html#dealing-with-dates",
    "title": "Data import and export",
    "section": "Dealing with dates",
    "text": "Dealing with dates\nThere is another problem in our data frame: the Date variable should be of the date type, but read_csv() failed to recognize the values as dates and processed them as characters. This is because it is not entered in our data following the ISO 8601 format which is YYYY-MM-DD. When you enter data, make sure to follow this format as it will make things work automatically. In our case, we have to convert the date.\nThe tidyverse package dealing with date is lubridate. Let’s load it:\n\nlibrary(lubridate)\n\nlubridate comes with many functions that can convert dates and times from many format to the ISO format. Since our date have the day, then the month, then the year, the function we need is dmy():\n\narc1$Date &lt;- dmy(arc1$Date)\n\nAlternatively, read_csv() will understand dates in a non ISO format, provided you give it the right information. This can be done with the col_types argument and the col_date() function to which the parameters corresponding to your date format are passed.\nHere are the parameters to use:\n\n\n\n\nFormat\nExample\nParameter\n\n\n\n\nYear\n4 digits\n2024\n%Y\n\n\n\n2 digits\n24\n%y\n\n\nMonth\nDecimal\n2\n%m\n\n\n\nAbbreviated name\nFeb\n%b\n\n\n\nFull name\nFebruary\n%B\n\n\nDay\nDecimal\n8\n%d\n\n\n\nIn our case, the date looks like \"%d/%m/%Y\":\n\narc1 &lt;- read_csv(\n  \"https://mint.westdri.ca/r/hss_data/arc1.csv\",\n  na = c(\"N/A\", \"n/a\"),\n  col_types = cols(Date = col_date(\"%d/%m/%Y\"))\n)\narc1\n\n# A tibble: 6 × 5\n  Site  Date       `Number of artifacts` `Name of PI`\n  &lt;chr&gt; &lt;date&gt;                     &lt;dbl&gt; &lt;chr&gt;       \n1 E1    2001-02-13                     4 John Doe    \n2 E1    2001-02-14                     3 John Doe    \n3 A2    2003-03-26                    NA Paul Smith  \n4 B18   2006-05-04                     5 Paul Smith  \n5 B7    2006-05-04                     5 &lt;NA&gt;        \n6 B3    2006-05-04                     5 P. Smith    \n  Comments                            \n  &lt;chr&gt;                               \n1 \"\"                                  \n2 \"\"                                  \n3 \"Artifacts still need to be counted\"\n4 \"\"                                  \n5 \"\"                                  \n6 \"\"",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "Data import and export"
    ]
  },
  {
    "objectID": "r/intro_import.html#renaming-variables",
    "href": "r/intro_import.html#renaming-variables",
    "title": "Data import and export",
    "section": "Renaming variables",
    "text": "Renaming variables\nVariable names cannot contain spaces. Since our data did have spaces in some of the names and since those names were not quoted, R added backticks ``` to be able to make use of them. This makes for rather awkward variables. Let’s rename them.\nWe could use the camel or snake case, but we can also just simplify the names:\n\narc1 &lt;- arc1 |&gt;\n  rename(\n    Artifacts = `Number of artifacts`,\n    PI = `Name of PI`\n  )\n\nError in rename(arc1, Artifacts = `Number of artifacts`, PI = `Name of PI`): could not find function \"rename\"",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "Data import and export"
    ]
  },
  {
    "objectID": "r/intro_import.html#fixing-inconsistencies",
    "href": "r/intro_import.html#fixing-inconsistencies",
    "title": "Data import and export",
    "section": "Fixing inconsistencies",
    "text": "Fixing inconsistencies\nThere is still another problem in our data: Paul Smith and P. Smith are—as far as R is concerned—2 different values. The number of PIs in our data should be two, but R currently interprets it as being three:\n\ndplyr::n_distinct(arc1$PI, na.rm = TRUE)\n\nWarning: Unknown or uninitialised column: `PI`.\n\n\n[1] 0\n\n\n\nWe remove the missing values so that they don’t get counted as an additional PI (although, more PIs could have been involved in the data collection: dealing with missing values programmatically is easy once they are properly formatted, but what to do with them methodologically depends on the situation and is part of the research question).\n\nThis can be a problem for future analysis, so let’s fix it. There are many ways to go about this, but the simplest is to use regular expressions:\n\narc1$PI &lt;- gsub(\"P\\\\.\", \"Paul\", arc1$PI)\n\nWarning: Unknown or uninitialised column: `PI`.\n\n\nError in `$&lt;-`:\n! Assigned data `gsub(\"P\\\\\\\\.\", \"Paul\", arc1$PI)` must be compatible\n  with existing data.\n✖ Existing data has 6 rows.\n✖ Assigned data has 0 rows.\nℹ Only vectors of size 1 are recycled.\nCaused by error in `vectbl_recycle_rhs_rows()`:\n! Can't recycle input of size 0 to size 6.\n\n\nOur data is finally well formatted and can be used for plotting, analyses, etc.:\n\narc1\n\n# A tibble: 6 × 5\n  Site  Date       `Number of artifacts` `Name of PI`\n  &lt;chr&gt; &lt;date&gt;                     &lt;dbl&gt; &lt;chr&gt;       \n1 E1    2001-02-13                     4 John Doe    \n2 E1    2001-02-14                     3 John Doe    \n3 A2    2003-03-26                    NA Paul Smith  \n4 B18   2006-05-04                     5 Paul Smith  \n5 B7    2006-05-04                     5 &lt;NA&gt;        \n6 B3    2006-05-04                     5 P. Smith    \n  Comments                            \n  &lt;chr&gt;                               \n1 \"\"                                  \n2 \"\"                                  \n3 \"Artifacts still need to be counted\"\n4 \"\"                                  \n5 \"\"                                  \n6 \"\"",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "Data import and export"
    ]
  },
  {
    "objectID": "r/intro_import.html#writing-data-to-file",
    "href": "r/intro_import.html#writing-data-to-file",
    "title": "Data import and export",
    "section": "Writing data to file",
    "text": "Writing data to file\nNow that we have a properly formatted data frame, we could, if we needed to, export it to a new file. readr also has functions to write to text files.\nLet’s save our data frame as a new CSV file (make sure to give it a different name from the original file):\nwrite_csv(arc1, \"arc1_clean.csv\")",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "Data import and export"
    ]
  },
  {
    "objectID": "r/intro_explore.html",
    "href": "r/intro_explore.html",
    "title": "Data exploration",
    "section": "",
    "text": "An important first step of data analysis is to have a look at the data. In this section, we will explore the us_contagious_diseases dataset from the dslabs package.",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "Data exploration"
    ]
  },
  {
    "objectID": "r/intro_explore.html#load-the-dslabs-package",
    "href": "r/intro_explore.html#load-the-dslabs-package",
    "title": "Data exploration",
    "section": "Load the dslabs package",
    "text": "Load the dslabs package\nThis package contains a number of datasets. To access any of them, we first need to load the package:\n\nlibrary(dslabs)\n\n\nlibrary() is a function:\n\nclass(library)\n\n[1] \"function\"\n\n\nFunctions are the “verbs” of programming languages. They do things.\nlibrary() is a function that loads packages into the current session so that their content becomes available.\ndslabs is the argument that we pass to the function library(): it is this particular packages that we are loading in the session here.\nclass() is also a function: it tells what class an object belongs to. In class(library), library is the argument of the function class().",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "Data exploration"
    ]
  },
  {
    "objectID": "r/intro_explore.html#printing-data-to-screen",
    "href": "r/intro_explore.html#printing-data-to-screen",
    "title": "Data exploration",
    "section": "Printing data to screen",
    "text": "Printing data to screen\nTo print all the data, we would simply run us_contagious_diseases. There are a lot of rows however, so we only want to print a subset to the screen.\nTo print the first six rows, we use the function head(), using our data as the argument:\n\nhead(us_contagious_diseases)\n\n      disease   state year weeks_reporting count population\n1 Hepatitis A Alabama 1966              50   321    3345787\n2 Hepatitis A Alabama 1967              49   291    3364130\n3 Hepatitis A Alabama 1968              52   314    3386068\n4 Hepatitis A Alabama 1969              49   380    3412450\n5 Hepatitis A Alabama 1970              51   413    3444165\n6 Hepatitis A Alabama 1971              51   378    3481798\n\n\nIf you look at the documentation of the head() function (by running ?head), you can see that it accepts another argument that allows us to set the number of rows to print.\nLet’s print the first 15 rows:\n\nhead(us_contagious_diseases, n = 15)\n\n       disease   state year weeks_reporting count population\n1  Hepatitis A Alabama 1966              50   321    3345787\n2  Hepatitis A Alabama 1967              49   291    3364130\n3  Hepatitis A Alabama 1968              52   314    3386068\n4  Hepatitis A Alabama 1969              49   380    3412450\n5  Hepatitis A Alabama 1970              51   413    3444165\n6  Hepatitis A Alabama 1971              51   378    3481798\n7  Hepatitis A Alabama 1972              45   342    3524543\n8  Hepatitis A Alabama 1973              45   467    3571209\n9  Hepatitis A Alabama 1974              45   244    3620548\n10 Hepatitis A Alabama 1975              46   286    3671246\n11 Hepatitis A Alabama 1976              50   220    3721914\n12 Hepatitis A Alabama 1977              43   206    3771085\n13 Hepatitis A Alabama 1978              41   203    3817217\n14 Hepatitis A Alabama 1979              47   257    3858703\n15 Hepatitis A Alabama 1980              37   200    3893888\n\n\n\nBy default, n = 6 which is why head() prints six rows unless we specify otherwise. The L in the documentation of the print() function (n = 6L) means that 6 is an integer. You can ignore this for now.\nArguments can be passed to functions as positional arguments (then they have to respect the position of the function definition) or as named arguments (in that case, you need to use the arguments names).\nThat means that iff we keep the arguments in the right order, we can omit the name of the argument (n here) and only write its value (15). :\n\nhead(us_contagious_diseases, 15)\n\n       disease   state year weeks_reporting count population\n1  Hepatitis A Alabama 1966              50   321    3345787\n2  Hepatitis A Alabama 1967              49   291    3364130\n3  Hepatitis A Alabama 1968              52   314    3386068\n4  Hepatitis A Alabama 1969              49   380    3412450\n5  Hepatitis A Alabama 1970              51   413    3444165\n6  Hepatitis A Alabama 1971              51   378    3481798\n7  Hepatitis A Alabama 1972              45   342    3524543\n8  Hepatitis A Alabama 1973              45   467    3571209\n9  Hepatitis A Alabama 1974              45   244    3620548\n10 Hepatitis A Alabama 1975              46   286    3671246\n11 Hepatitis A Alabama 1976              50   220    3721914\n12 Hepatitis A Alabama 1977              43   206    3771085\n13 Hepatitis A Alabama 1978              41   203    3817217\n14 Hepatitis A Alabama 1979              47   257    3858703\n15 Hepatitis A Alabama 1980              37   200    3893888\n\n\nIf the arguments are given to the function out of order however, we do need to use their names.\nThis won’t work because R needs an integer for n or for the 2nd argument:\n\nhead(15, us_contagious_diseases)\n\nError in head.default(15, us_contagious_diseases): invalid 'n' - must be numeric, possibly NA.\n\n\nThis however works:\n\nhead(n = 15, us_contagious_diseases)\n\n       disease   state year weeks_reporting count population\n1  Hepatitis A Alabama 1966              50   321    3345787\n2  Hepatitis A Alabama 1967              49   291    3364130\n3  Hepatitis A Alabama 1968              52   314    3386068\n4  Hepatitis A Alabama 1969              49   380    3412450\n5  Hepatitis A Alabama 1970              51   413    3444165\n6  Hepatitis A Alabama 1971              51   378    3481798\n7  Hepatitis A Alabama 1972              45   342    3524543\n8  Hepatitis A Alabama 1973              45   467    3571209\n9  Hepatitis A Alabama 1974              45   244    3620548\n10 Hepatitis A Alabama 1975              46   286    3671246\n11 Hepatitis A Alabama 1976              50   220    3721914\n12 Hepatitis A Alabama 1977              43   206    3771085\n13 Hepatitis A Alabama 1978              41   203    3817217\n14 Hepatitis A Alabama 1979              47   257    3858703\n15 Hepatitis A Alabama 1980              37   200    3893888\n\n\n\nWe can also print the last 6 rows of the data:\n\ntail(us_contagious_diseases)\n\n       disease   state year weeks_reporting count population\n16060 Smallpox Wyoming 1947              49     1     276297\n16061 Smallpox Wyoming 1948              24     1     280803\n16062 Smallpox Wyoming 1949               0     0     285544\n16063 Smallpox Wyoming 1950               1     2     290529\n16064 Smallpox Wyoming 1951               1     1     295744\n16065 Smallpox Wyoming 1952               1     1     301083\n\n\n\n\nYour turn:\n\nHow would you print the last 10 rows of the data?",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "Data exploration"
    ]
  },
  {
    "objectID": "r/intro_explore.html#structure-of-the-data-object",
    "href": "r/intro_explore.html#structure-of-the-data-object",
    "title": "Data exploration",
    "section": "Structure of the data object",
    "text": "Structure of the data object\nus_contagious_diseases is an R object containing the dataset, but what kind of object is it?\n\nclass(us_contagious_diseases)\n\n[1] \"data.frame\"\n\n\nOur data is in a class of R object called a data frame.\nWe can get its full structure with:\n\nstr(us_contagious_diseases)\n\n'data.frame':   16065 obs. of  6 variables:\n $ disease        : Factor w/ 7 levels \"Hepatitis A\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ state          : Factor w/ 51 levels \"Alabama\",\"Alaska\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ year           : num  1966 1967 1968 1969 1970 ...\n $ weeks_reporting: num  50 49 52 49 51 51 45 45 45 46 ...\n $ count          : num  321 291 314 380 413 378 342 467 244 286 ...\n $ population     : num  3345787 3364130 3386068 3412450 3444165 ...\n\n\nThe names of the variables can be obtained with:\n\nnames(us_contagious_diseases)\n\n[1] \"disease\"         \"state\"           \"year\"            \"weeks_reporting\"\n[5] \"count\"           \"population\"     \n\n\nYou can display the data frame in a tabular fashion thanks to:\nView(us_contagious_diseases)",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "Data exploration"
    ]
  },
  {
    "objectID": "r/intro_explore.html#dimensions-of-our-data-frame",
    "href": "r/intro_explore.html#dimensions-of-our-data-frame",
    "title": "Data exploration",
    "section": "Dimensions of our data frame",
    "text": "Dimensions of our data frame\n\ndim(us_contagious_diseases)\n\n[1] 16065     6\n\nncol(us_contagious_diseases)\n\n[1] 6\n\nnrow(us_contagious_diseases)\n\n[1] 16065\n\n\n\nlength(us_contagious_diseases)\n\n[1] 6\n\nlength(us_contagious_diseases$disease)\n\n[1] 16065",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "Data exploration"
    ]
  },
  {
    "objectID": "r/intro_explore.html#summary-statistics",
    "href": "r/intro_explore.html#summary-statistics",
    "title": "Data exploration",
    "section": "Summary statistics",
    "text": "Summary statistics\n\nsummary(us_contagious_diseases)\n\n        disease            state            year      weeks_reporting\n Hepatitis A:2346   Alabama   :  315   Min.   :1928   Min.   : 0.00  \n Measles    :3825   Alaska    :  315   1st Qu.:1950   1st Qu.:31.00  \n Mumps      :1785   Arizona   :  315   Median :1975   Median :46.00  \n Pertussis  :2856   Arkansas  :  315   Mean   :1971   Mean   :37.38  \n Polio      :2091   California:  315   3rd Qu.:1990   3rd Qu.:50.00  \n Rubella    :1887   Colorado  :  315   Max.   :2011   Max.   :52.00  \n Smallpox   :1275   (Other)   :14175                                 \n     count          population      \n Min.   :     0   Min.   :   86853  \n 1st Qu.:     7   1st Qu.: 1018755  \n Median :    69   Median : 2749249  \n Mean   :  1492   Mean   : 4107584  \n 3rd Qu.:   525   3rd Qu.: 4996229  \n Max.   :132342   Max.   :37607525  \n                  NA's   :214",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "Data exploration"
    ]
  },
  {
    "objectID": "r/intro_control_flow.html",
    "href": "r/intro_control_flow.html",
    "title": "Control flow",
    "section": "",
    "text": "Control flow statements alter the linear execution of code, allowing for one or another section of code to be executed, or for one section of code to be executed multiple times.",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "Control flow"
    ]
  },
  {
    "objectID": "r/intro_control_flow.html#conditionals",
    "href": "r/intro_control_flow.html#conditionals",
    "title": "Control flow",
    "section": "Conditionals",
    "text": "Conditionals\nConditionals determine which section of code is to be ran based on predicates. A predicate is a test that returns either TRUE or FALSE.\nHere is an example:\n\ntest_sign &lt;- function(x) {\n  if (x &gt; 0) {\n    \"x is positif\"\n  } else if (x &lt; 0) {\n    \"x is negatif\"\n  } else {\n    \"x is equal to zero\"\n  }\n}\n\ntest_sign() is a function that accepts one argument. Depending on the value of that argument, one of three snippets of code is executed:\n\ntest_sign(3)\n\n[1] \"x is positif\"\n\ntest_sign(-2)\n\n[1] \"x is negatif\"\n\ntest_sign(0)\n\n[1] \"x is equal to zero\"",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "Control flow"
    ]
  },
  {
    "objectID": "r/intro_control_flow.html#loops",
    "href": "r/intro_control_flow.html#loops",
    "title": "Control flow",
    "section": "Loops",
    "text": "Loops\nLoops allow to run the same instruction on various elements:\n\nfor (i in 1:10) {\n  print(i)\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n[1] 7\n[1] 8\n[1] 9\n[1] 10",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "Control flow"
    ]
  },
  {
    "objectID": "r/intro_automation.html",
    "href": "r/intro_automation.html",
    "title": "Automation",
    "section": "",
    "text": "One of the strengths of programming is the ability to automate tasks.\nIn this section, we will see how a loop can automate the creation of file names.\n\nLet’s say that we now want to import data from 5 files arc1.csv, …, arc5.csv and create 5 data frames with their data.\nWe need a character vector with the file names.\nWe could create it this way:\n\nfiles &lt;- c(\n  \"https://mint.westdri.ca/r/hss_data/arc1.csv\",\n  \"https://mint.westdri.ca/r/hss_data/arc2.csv\",\n  \"https://mint.westdri.ca/r/hss_data/arc3.csv\",\n  \"https://mint.westdri.ca/r/hss_data/arc4.csv\",\n  \"https://mint.westdri.ca/r/hss_data/arc5.csv\"\n)\n\nIt works of course:\n\nfiles\n\n[1] \"https://mint.westdri.ca/r/hss_data/arc1.csv\"\n[2] \"https://mint.westdri.ca/r/hss_data/arc2.csv\"\n[3] \"https://mint.westdri.ca/r/hss_data/arc3.csv\"\n[4] \"https://mint.westdri.ca/r/hss_data/arc4.csv\"\n[5] \"https://mint.westdri.ca/r/hss_data/arc5.csv\"\n\n\nBut if we had 50 files instead of 5, it would be quite a tedium! And if we had 500 files, it would be unrealistic. A better approach is to write a loop.\nIn order to store the results of a loop, we need to create an empty object and assign to it the result of the loop at each iteration. It is very important to pre-allocate memory: by creating an empty object of the final size, the necessary memory to hold this object is requested once (then the object gets filled in while the loop runs). Without this, more memory would have to be allocated at each iteration of the loop and this is highly inefficient.\nSo let’s create an empty vector of length 5 and of type character:\n\nfiles &lt;- character(5)\n\nNow we can fill in our vector with the proper values with the loop:\n\nfor (i in 1:5) {\n  files[i] &lt;- paste0(\"https://mint.westdri.ca/r/hss_data/arc\", i, \".csv\")\n}\n\nThis gives us the same result, but the big difference is that it is scalable:\n\nfiles\n\n[1] \"https://mint.westdri.ca/r/hss_data/arc1.csv\"\n[2] \"https://mint.westdri.ca/r/hss_data/arc2.csv\"\n[3] \"https://mint.westdri.ca/r/hss_data/arc3.csv\"\n[4] \"https://mint.westdri.ca/r/hss_data/arc4.csv\"\n[5] \"https://mint.westdri.ca/r/hss_data/arc5.csv\"\n\n\nNow, if our files were not named following such a nice sequence, we would have to modify our loop a little. Below are two examples:\n\nfiles &lt;- character(5)\n\nfor (i in seq_along(c(3, 6, 9, 10, 14))) {\n  files[i] &lt;- paste0(\n    \"https://mint.westdri.ca/r/hss_data/arc\",\n    c(3, 6, 9, 10, 14)[i],\n    \".csv\"\n  )\n}\n\nfiles\n\n[1] \"https://mint.westdri.ca/r/hss_data/arc3.csv\" \n[2] \"https://mint.westdri.ca/r/hss_data/arc6.csv\" \n[3] \"https://mint.westdri.ca/r/hss_data/arc9.csv\" \n[4] \"https://mint.westdri.ca/r/hss_data/arc10.csv\"\n[5] \"https://mint.westdri.ca/r/hss_data/arc14.csv\"\n\n\n\nfiles &lt;- character(5)\n\nfor (i in seq_along(c(\"_a\", \"_b\", \"_c\", \"_d\", \"_e\"))) {\n  files[i] &lt;- paste0(\n    \"https://mint.westdri.ca/r/hss_data/arc\",\n    c(\"_a\", \"_b\", \"_c\", \"_d\", \"_e\")[i],\n    \".csv\"\n  )\n}\n\nfiles\n\n[1] \"https://mint.westdri.ca/r/hss_data/arc_a.csv\"\n[2] \"https://mint.westdri.ca/r/hss_data/arc_b.csv\"\n[3] \"https://mint.westdri.ca/r/hss_data/arc_c.csv\"\n[4] \"https://mint.westdri.ca/r/hss_data/arc_d.csv\"\n[5] \"https://mint.westdri.ca/r/hss_data/arc_e.csv\"\n\n\n\nIf you had all the files in one directory, an alternative approach would be to create a list of all the names matching a regular expression.\nIn our case, we would use:\nfiles &lt;- list.files(pattern=\"^arc\\\\d+\\\\.csv$\")",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "Automation"
    ]
  },
  {
    "objectID": "r/hss_why.html",
    "href": "r/hss_why.html",
    "title": "R: why and for whom?",
    "section": "",
    "text": "There are other high level programming languages such as Python or Julia, so when might it make sense for you to turn to R?",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "R: why and for whom?"
    ]
  },
  {
    "objectID": "r/hss_why.html#why-r",
    "href": "r/hss_why.html#why-r",
    "title": "R: why and for whom?",
    "section": "Why R?",
    "text": "Why R?\nHere are a number of reasons why you might want to consider using R:\n\nFree and open source\nHigh-level and easy to learn\nLarge community\nVery well documented\nUnequalled number of statistics and modelling packages\nIntegrated package manager\nEasy connection with fast compiled languages such as C and C++\nPowerful IDEs (e.g. RStudio, ESS, Jupyter)",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "R: why and for whom?"
    ]
  },
  {
    "objectID": "r/hss_why.html#for-whom",
    "href": "r/hss_why.html#for-whom",
    "title": "R: why and for whom?",
    "section": "For whom?",
    "text": "For whom?\nFor whom is R particularly well suited?\n\nFields with heavy statistics, modelling, or Bayesian analysis such as biology, linguistics, economics, or statistics\nData science using a lot of tabular data",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "R: why and for whom?"
    ]
  },
  {
    "objectID": "r/hss_why.html#downsides-of-r",
    "href": "r/hss_why.html#downsides-of-r",
    "title": "R: why and for whom?",
    "section": "Downsides of R",
    "text": "Downsides of R\nOf course, R also has its downsides:\n\nInconsistent syntax full of quirks\nSlow\nLarge memory usage",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "R: why and for whom?"
    ]
  },
  {
    "objectID": "r/hss_tidyverse.html",
    "href": "r/hss_tidyverse.html",
    "title": "The tidyverse",
    "section": "",
    "text": "The tidyverse is a set of packages which attempts to make R more consistent. R was written by statisticians and it is a bit quirky. The tidyverse makes it look more like other programming languages which were developed by computer scientists. It is a different style of writing R code and it is by no means necessary.",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "The tidyverse"
    ]
  },
  {
    "objectID": "r/hss_tidyverse.html#a-glimpse-at-the-tidyverse",
    "href": "r/hss_tidyverse.html#a-glimpse-at-the-tidyverse",
    "title": "The tidyverse",
    "section": "A glimpse at the tidyverse",
    "text": "A glimpse at the tidyverse\nThe best introduction to the tidyverse is probably the book R for Data Science by Hadley Wickham and Garrett Grolemund.\nPosit (the company formerly known as RStudio Inc. behind the tidyverse) developed a series of useful cheatsheets. Below are links to the ones you are the most likely to use as you get started with R.\n\nData import\nThe first thing you often need to do is to import your data into R. This is done with readr.\n\n\n\nfrom Posit Cheatsheets\n\n\n\nData transformation\nYou then often need to transformation your data into the right format. This is done with the packages dplyr and tidyr.\n\n\n\nfrom Posit Cheatsheets\n\n\n\n\nfrom Posit Cheatsheets\n\n\n\nVisualization\nVisualization in the tidyverse is done with the ggplot2 package which we will explore in the next section.\n\n\n\nfrom Posit Cheatsheets\n\n\n\nWorking with factors\nThe package forcats offers the tidyverse approach to working with factors.\n\n\nfrom Posit Cheatsheets\n\n\n\nWorking with strings\nstringr is for strings.\n\n\n\nfrom Posit Cheatsheets\n\n\n\nWorking with dates\nlubridate will help you deal with dates.\n\n\n\nfrom Posit Cheatsheets\n\n\n\nFunctional programming\nFinally, purrr is the tidyverse equivalent to the apply functions in base R: a way to run functions on functions.\n\n\n\nfrom Posit Cheatsheets",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "The tidyverse"
    ]
  },
  {
    "objectID": "r/hss_tidyverse.html#base-r-or-tidyverse",
    "href": "r/hss_tidyverse.html#base-r-or-tidyverse",
    "title": "The tidyverse",
    "section": "Base R or tidyverse?",
    "text": "Base R or tidyverse?\n“Base R” refers to the use of the standard R library. The expression is often used in contrast to the tidyverse.\nThere are a many things that you can do with either base R or the tidyverse. Because the syntaxes are quite different, it almost feels like using two different languages and people tend to favour one or the other.\nWhich one you should use is really up to you.\n\n\n\n\n\n\n\nBase R\nTidyverse\n\n\n\n\nPreferred by old-schoolers\nIncreasingly becoming the norm with newer R users\n\n\nMore stable\nMore consistent syntax and behaviour\n\n\nDoesn’t require installing and loading packages\nMore and more resources and documentation available\n\n\n\nIn truth, even though the tidyverse has many detractors amongst old R users, it is increasingly becoming the norm.",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "The tidyverse"
    ]
  },
  {
    "objectID": "r/hss_resources.html",
    "href": "r/hss_resources.html",
    "title": "Resources",
    "section": "",
    "text": "The R community is dynamic and offers a lot of online resources, from IDEs to Q&A, to workshops, books, or publications.\nThis section provides a selection of useful sites.\n\n\nMain sites\n\nR website\nComprehensive R Archive Network (CRAN): R versions and packages\n\n\n\nPosit and RStudio IDE\n\nPosit website (Posit was formerly called RStudio Inc.)\nPosit cheatsheets\n\n\n\nForums and Q&A\n\nStack Overflow [r] tag wiki\nStack Overflow [r] tag questions\nPosit Discourse\n\n\n\nDocumentation as pdf\n\nContributed documentation\nIntro books\n\n\n\nSoftware Carpentry online workshops\n\nProgramming with R\nR for Reproducible Scientific Analysis\nData analysis using R in the digital humanities\n\n\n\nOnline books\n\nR for Data Science (heavily based on the tidyverse)\nR Packages (how to create packages)\nR Programming for Data Science\nMastering Software Development in R\n\n\n\nR research\n\nThe R Journal",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Resources"
    ]
  },
  {
    "objectID": "r/hss_packages.html",
    "href": "r/hss_packages.html",
    "title": "Packages",
    "section": "",
    "text": "Packages are a set of functions, constants, and/or data developed by the community that add functionality to R.\nIn this section, we look at where to find packages and how to install them.",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Packages"
    ]
  },
  {
    "objectID": "r/hss_packages.html#looking-for-packages",
    "href": "r/hss_packages.html#looking-for-packages",
    "title": "Packages",
    "section": "Looking for packages",
    "text": "Looking for packages\n\nPackage finder.\nYour peers and the literature.\nList of CRAN packages.\nList of CRAN task views (list of packages with information for a large number of wide topics).",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Packages"
    ]
  },
  {
    "objectID": "r/hss_packages.html#managing-r-packages",
    "href": "r/hss_packages.html#managing-r-packages",
    "title": "Packages",
    "section": "Managing R packages",
    "text": "Managing R packages\n\nFor this course, you won’t have to install any package as they have already been installed in our RStudio server.\n\nR packages can be installed, updated, and removed from within R:\ninstall.packages(\"&lt;package_name&gt;\", repos=\"&lt;url-cran-mirror&gt;\")\nremove.packages(\"&lt;package-name&gt;\")\nupdate_packages()\n\nExample:\n\ninstall.packages(\"rvest\", repos=\"https://mirror.rcg.sfu.ca/mirror/CRAN/\")\n\nrepos argument: chose a CRAN mirror close to the location of your cluster or use https://cloud.r-project.org/.\n\n\nThe first time you install a package, R will ask you whether you want to create a personal library in your home directory. Answer yes to both questions. Your packages will now install under ~/.\n\n\nSome packages require additional modules to be loaded before they can be installed. Other packages need additional R packages as dependencies. In either case, you will get explicit error messages. Adding the argument dependencies = T helps in the second case, but you will still have to add packages manually from time to time.",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Packages"
    ]
  },
  {
    "objectID": "r/hss_packages.html#loading-packages",
    "href": "r/hss_packages.html#loading-packages",
    "title": "Packages",
    "section": "Loading packages",
    "text": "Loading packages\nTo make a package available in an R session, you load it with the library() function.\n\nExample:\n\nlibrary(readxl)\nAlternatively, you can access a function from a package without loading it with the syntax: package::function().\n\nExample:\n\nreadxl::read_excel(\"file.xlsx\")",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Packages"
    ]
  },
  {
    "objectID": "r/hss_packages.html#package-documentation",
    "href": "r/hss_packages.html#package-documentation",
    "title": "Packages",
    "section": "Package documentation",
    "text": "Package documentation\n\nSelect a package from the list of CRAN packages.\nGoogle “cran” and the name of your package (e.g. “cran dplyr”).\nLook up a package in the package documentation.\nGet a list of functions within a package with the help() function (installed, but not loaded in session):\n\n\nExample to get a list of functions in the dplyr package:\n\nhelp(package = \"dplyr\")\n\nGet help on a function within a package:\n\nIf you are using RStudio or the HTML format for your R help and you already ran the command to get the list of functions within a package (e.g. help(package = \"dplyr\")), you can get help on any function by clicking on its name.\nIf you are using the text format for help (for instance, if you are running R remotely on the command line), you can get help for any function by adding its name at as the first argument of the previous command.\n\nExample to get help on the function bind() of the package dplyr:\n\nhelp(bind, package = \"dplyr\")\nOf course, if the dplyr package is already loaded in your session, you can simply run help(bind).\n\nGet a list of all help files with alias or concept or title matching a regular expression in all installed packages:\n\n\nExample to get a list of all help files with alias or concept or title matching bind:\n\n??bind\nYou can then open those help files as seen previously.\n\nGet a list of all vignettes for all installed packages:\n\nIf you are using RStudio or the HTML help format:\nbrowseVignettes()\nIf you are using the text help format:\nvignette()\n\nGet a list of vignettes available for a package (not all packages have vignettes):\n\n\nExample to get a list of vignettes for the package dplyr:\n\nIf you are using RStudio or the HTML help format:\nvignette(package = \"dplyr\")\nIf you are using the text help format:\nbrowseVignettes(package = \"dplyr\")\nYou can then open those help vignettes as seen previously.",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Packages"
    ]
  },
  {
    "objectID": "r/hss_import.html",
    "href": "r/hss_import.html",
    "title": "Data import and export",
    "section": "",
    "text": "So far, we have used a well-formatted dataset. In the real world, things are often not this nice and tidy…\nIn this section, we will learn how to handle real data.",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Data import and export"
    ]
  },
  {
    "objectID": "r/hss_import.html#reading-in-data",
    "href": "r/hss_import.html#reading-in-data",
    "title": "Data import and export",
    "section": "Reading in data",
    "text": "Reading in data\nThe readr package from the tidyverse provides a number of functions to read in text files with tabular data (e.g. comma-separated values (CSV) or tab-separated values (TSV) files).\nLet’s load it:\n\nlibrary(readr)\n\nThe read_csv() function allows to read in CSV files that are either stored locally or from a URL.\nLet’s use it to load a CSV file with mock archaeological data which is at the URL https://mint.westdri.ca/r/hss_data/arc1.csv:\n\narc1 &lt;- read_csv(\"https://mint.westdri.ca/r/hss_data/arc1.csv\")\n\nRows: 6 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): Site, Date, Number of artifacts, Name of PI, Comments\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nIf the file was in your machine, you would provide its path instead of the URL.\n\nHere is our data:\n\narc1\n\n# A tibble: 6 × 5\n  Site  Date      `Number of artifacts` `Name of PI`\n  &lt;chr&gt; &lt;chr&gt;     &lt;chr&gt;                 &lt;chr&gt;       \n1 E1    13/2/2001 4                     John Doe    \n2 E1    14/2/2001 3                     John Doe    \n3 A2    26/3/2003 N/A                   Paul Smith  \n4 B18   4/5/2006  5                     Paul Smith  \n5 B7    4/5/2006  5                     n/a         \n6 B3    4/5/2006  5                     P. Smith    \n  Comments                          \n  &lt;chr&gt;                             \n1 &lt;NA&gt;                              \n2 &lt;NA&gt;                              \n3 Artifacts still need to be counted\n4 &lt;NA&gt;                              \n5 &lt;NA&gt;                              \n6 &lt;NA&gt;",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Data import and export"
    ]
  },
  {
    "objectID": "r/hss_import.html#improper-na",
    "href": "r/hss_import.html#improper-na",
    "title": "Data import and export",
    "section": "Improper NA",
    "text": "Improper NA\nIn R, missing values are represented by NA (not available). It is a constant that R understands and can deal with, so it is important that all missing values are represented properly.\nWhen you enter data (say in an Excel file or CSV file), leave an empty cell for missing values: R will then transform them automatically into NA.\nBecause this data was not entered properly, we have to fix our missing values. One way to go about this is to replace the characters representing missing values in the file (\"N/A\" and \"n/a\") by NA:\n\nis.na(arc1) &lt;- arc1 == \"N/A\"\nis.na(arc1) &lt;- arc1 == \"n/a\"\narc1\n\n# A tibble: 6 × 5\n  Site  Date      `Number of artifacts` `Name of PI`\n  &lt;chr&gt; &lt;chr&gt;     &lt;chr&gt;                 &lt;chr&gt;       \n1 E1    13/2/2001 4                     John Doe    \n2 E1    14/2/2001 3                     John Doe    \n3 A2    26/3/2003 &lt;NA&gt;                  Paul Smith  \n4 B18   4/5/2006  5                     Paul Smith  \n5 B7    4/5/2006  5                     &lt;NA&gt;        \n6 B3    4/5/2006  5                     P. Smith    \n  Comments                          \n  &lt;chr&gt;                             \n1 &lt;NA&gt;                              \n2 &lt;NA&gt;                              \n3 Artifacts still need to be counted\n4 &lt;NA&gt;                              \n5 &lt;NA&gt;                              \n6 &lt;NA&gt;                              \n\n\nNow, we have another problem to fix: readr is very good at guessing the types of the various variables. Unfortunately, the character \"N/A\" in the Number of artifacts column prevented it to guess the type properly: it should be a double (a numerical value) and not a character. We can fix this too:\n\narc1$`Number of artifacts` &lt;- as.double(arc1$`Number of artifacts`)\narc1\n\n# A tibble: 6 × 5\n  Site  Date      `Number of artifacts` `Name of PI`\n  &lt;chr&gt; &lt;chr&gt;                     &lt;dbl&gt; &lt;chr&gt;       \n1 E1    13/2/2001                     4 John Doe    \n2 E1    14/2/2001                     3 John Doe    \n3 A2    26/3/2003                    NA Paul Smith  \n4 B18   4/5/2006                      5 Paul Smith  \n5 B7    4/5/2006                      5 &lt;NA&gt;        \n6 B3    4/5/2006                      5 P. Smith    \n  Comments                          \n  &lt;chr&gt;                             \n1 &lt;NA&gt;                              \n2 &lt;NA&gt;                              \n3 Artifacts still need to be counted\n4 &lt;NA&gt;                              \n5 &lt;NA&gt;                              \n6 &lt;NA&gt;                              \n\n\nAlternatively, it is simpler to have read_csv() properly recognize the missing values. This can be done thanks to the na argument:\n\narc1 &lt;- read_csv(\n  \"https://mint.westdri.ca/r/hss_data/arc1.csv\",\n  na = c(\"N/A\", \"n/a\")\n)\n\nRows: 6 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): Site, Date, Name of PI, Comments\ndbl (1): Number of artifacts\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\narc1\n\n# A tibble: 6 × 5\n  Site  Date      `Number of artifacts` `Name of PI`\n  &lt;chr&gt; &lt;chr&gt;                     &lt;dbl&gt; &lt;chr&gt;       \n1 E1    13/2/2001                     4 John Doe    \n2 E1    14/2/2001                     3 John Doe    \n3 A2    26/3/2003                    NA Paul Smith  \n4 B18   4/5/2006                      5 Paul Smith  \n5 B7    4/5/2006                      5 &lt;NA&gt;        \n6 B3    4/5/2006                      5 P. Smith    \n  Comments                            \n  &lt;chr&gt;                               \n1 \"\"                                  \n2 \"\"                                  \n3 \"Artifacts still need to be counted\"\n4 \"\"                                  \n5 \"\"                                  \n6 \"\"                                  \n\n\nA benefit of this approach is that read_csv() now automatically detects the proper data type of Number of artifacts (since there is no more confusing character in what is otherwise a column of doubles).",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Data import and export"
    ]
  },
  {
    "objectID": "r/hss_import.html#dealing-with-dates",
    "href": "r/hss_import.html#dealing-with-dates",
    "title": "Data import and export",
    "section": "Dealing with dates",
    "text": "Dealing with dates\nThere is another problem in our data frame: the Date variable should be of the date type, but read_csv() failed to recognize the values as dates and processed them as characters. This is because it is not entered in our data following the ISO 8601 format which is YYYY-MM-DD. When you enter data, make sure to follow this format as it will make things work automatically. In our case, we have to convert the date.\nThe tidyverse package dealing with date is lubridate. Let’s load it:\n\nlibrary(lubridate)\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\n\nlubridate comes with many functions that can convert dates and times from many format to the ISO format. Since our date have the day, then the month, then the year, the function we need is dmy():\n\narc1$Date &lt;- dmy(arc1$Date)\n\nAlternatively, read_csv() will understand dates in a non ISO format, provided you give it the right information. This can be done with the col_types argument and the col_date() function to which the parameters corresponding to your date format are passed.\nHere are the parameters to use:\n\n\n\n\nFormat\nExample\nParameter\n\n\n\n\nYear\n4 digits\n2024\n%Y\n\n\n\n2 digits\n24\n%y\n\n\nMonth\nDecimal\n2\n%m\n\n\n\nAbbreviated name\nFeb\n%b\n\n\n\nFull name\nFebruary\n%B\n\n\nDay\nDecimal\n8\n%d\n\n\n\nIn our case, the date looks like \"%d/%m/%Y\":\n\narc1 &lt;- read_csv(\n  \"https://mint.westdri.ca/r/hss_data/arc1.csv\",\n  na = c(\"N/A\", \"n/a\"),\n  col_types = cols(Date = col_date(\"%d/%m/%Y\"))\n)\narc1\n\n# A tibble: 6 × 5\n  Site  Date       `Number of artifacts` `Name of PI`\n  &lt;chr&gt; &lt;date&gt;                     &lt;dbl&gt; &lt;chr&gt;       \n1 E1    2001-02-13                     4 John Doe    \n2 E1    2001-02-14                     3 John Doe    \n3 A2    2003-03-26                    NA Paul Smith  \n4 B18   2006-05-04                     5 Paul Smith  \n5 B7    2006-05-04                     5 &lt;NA&gt;        \n6 B3    2006-05-04                     5 P. Smith    \n  Comments                            \n  &lt;chr&gt;                               \n1 \"\"                                  \n2 \"\"                                  \n3 \"Artifacts still need to be counted\"\n4 \"\"                                  \n5 \"\"                                  \n6 \"\"",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Data import and export"
    ]
  },
  {
    "objectID": "r/hss_import.html#renaming-variables",
    "href": "r/hss_import.html#renaming-variables",
    "title": "Data import and export",
    "section": "Renaming variables",
    "text": "Renaming variables\nVariable names cannot contain spaces. Since our data did have spaces in some of the names and since those names were not quoted, R added backticks ``` to be able to make use of them. This makes for rather awkward variables. Let’s rename them.\nWe could use the camel or snake case, but we can also just simplify the names:\n\narc1 &lt;- arc1 |&gt;\n  rename(\n    Artifacts = `Number of artifacts`,\n    PI = `Name of PI`\n  )\n\nError in rename(arc1, Artifacts = `Number of artifacts`, PI = `Name of PI`): could not find function \"rename\"",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Data import and export"
    ]
  },
  {
    "objectID": "r/hss_import.html#fixing-inconsistencies",
    "href": "r/hss_import.html#fixing-inconsistencies",
    "title": "Data import and export",
    "section": "Fixing inconsistencies",
    "text": "Fixing inconsistencies\nThere is still another problem in our data: Paul Smith and P. Smith are—as far as R is concerned—2 different values. The number of PIs in our data should be two, but R currently interprets it as being three:\n\ndplyr::n_distinct(arc1$PI, na.rm = TRUE)\n\nWarning: Unknown or uninitialised column: `PI`.\n\n\n[1] 0\n\n\n\nWe remove the missing values so that they don’t get counted as an additional PI (although, more PIs could have been involved in the data collection: dealing with missing values programmatically is easy once they are properly formatted, but what to do with them methodologically depends on the situation and is part of the research question).\n\nThis can be a problem for future analysis, so let’s fix it. There are many ways to go about this, but the simplest is to use regular expressions:\n\narc1$PI &lt;- gsub(\"P\\\\.\", \"Paul\", arc1$PI)\n\nWarning: Unknown or uninitialised column: `PI`.\n\n\nError in `$&lt;-`:\n! Assigned data `gsub(\"P\\\\\\\\.\", \"Paul\", arc1$PI)` must be compatible\n  with existing data.\n✖ Existing data has 6 rows.\n✖ Assigned data has 0 rows.\nℹ Only vectors of size 1 are recycled.\nCaused by error in `vectbl_recycle_rhs_rows()`:\n! Can't recycle input of size 0 to size 6.\n\n\nOur data is finally well formatted and can be used for plotting, analyses, etc.:\n\narc1\n\n# A tibble: 6 × 5\n  Site  Date       `Number of artifacts` `Name of PI`\n  &lt;chr&gt; &lt;date&gt;                     &lt;dbl&gt; &lt;chr&gt;       \n1 E1    2001-02-13                     4 John Doe    \n2 E1    2001-02-14                     3 John Doe    \n3 A2    2003-03-26                    NA Paul Smith  \n4 B18   2006-05-04                     5 Paul Smith  \n5 B7    2006-05-04                     5 &lt;NA&gt;        \n6 B3    2006-05-04                     5 P. Smith    \n  Comments                            \n  &lt;chr&gt;                               \n1 \"\"                                  \n2 \"\"                                  \n3 \"Artifacts still need to be counted\"\n4 \"\"                                  \n5 \"\"                                  \n6 \"\"",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Data import and export"
    ]
  },
  {
    "objectID": "r/hss_import.html#writing-data-to-file",
    "href": "r/hss_import.html#writing-data-to-file",
    "title": "Data import and export",
    "section": "Writing data to file",
    "text": "Writing data to file\nNow that we have a properly formatted data frame, we could, if we needed to, export it to a new file. readr also has functions to write to text files.\nLet’s save our data frame as a new CSV file (make sure to give it a different name from the original file):\nwrite_csv(arc1, \"arc1_clean.csv\")",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Data import and export"
    ]
  },
  {
    "objectID": "r/hss_functions.html",
    "href": "r/hss_functions.html",
    "title": "Function definition",
    "section": "",
    "text": "R comes with a number of built-in functions. Packages can provide additional ones. In many cases however, you will want to create your own functions to perform exactly the computations that you need.\nIn this section, we will see how to define new functions.",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Function definition"
    ]
  },
  {
    "objectID": "r/hss_functions.html#syntax",
    "href": "r/hss_functions.html#syntax",
    "title": "Function definition",
    "section": "Syntax",
    "text": "Syntax\nHere is the syntax to define a new function:\nname &lt;- function(arguments) {\n  body\n}",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Function definition"
    ]
  },
  {
    "objectID": "r/hss_functions.html#example",
    "href": "r/hss_functions.html#example",
    "title": "Function definition",
    "section": "Example",
    "text": "Example\nLet’s define a function that we call compare which will compare the value between 2 numbers:\n\ncompare &lt;- function(x, y) {\n  x == y\n}\n\n\ncompare is the name of our function.\nx and y are the placeholders for the arguments that our function will accept (our function will need 2 arguments to run successfully).\nx == y is the body of the function, that is, the computation performed by our function.\n\nWe can now use our function:\n\ncompare(2, 3)\n\n[1] FALSE",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Function definition"
    ]
  },
  {
    "objectID": "r/hss_functions.html#what-is-returned-by-a-function",
    "href": "r/hss_functions.html#what-is-returned-by-a-function",
    "title": "Function definition",
    "section": "What is returned by a function?",
    "text": "What is returned by a function?\nIn R, the result of the last statement is printed automatically:\n\ntest &lt;- function(x, y) {\n  x\n  y\n}\ntest(2, 3)\n\n[1] 3\n\n\nIf you want to also print other results, you need to explicitly use the print() function:\n\ntest &lt;- function(x, y) {\n  print(x)\n  y\n}\ntest(2, 3)\n\n[1] 2\n\n\n[1] 3\n\n\nNote that, unlike print(), the function return() exits the function:\n\ntest &lt;- function(x, y) {\n  return(x)\n  y\n}\ntest(2, 3)\n\n[1] 2\n\n\n\ntest &lt;- function(x, y) {\n  return(x)\n  return(y)\n}\ntest(2, 3)\n\n[1] 2",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Function definition"
    ]
  },
  {
    "objectID": "r/hss_data_types.html",
    "href": "r/hss_data_types.html",
    "title": "Data types and structures",
    "section": "",
    "text": "It might be time to talk a bit more formally about the various data types and structures available in R. The goal of this course is not to get bogged down in the nitty-gritty of R syntax, so this section is kept very short.",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Data types and structures"
    ]
  },
  {
    "objectID": "r/hss_data_types.html#data-types",
    "href": "r/hss_data_types.html#data-types",
    "title": "Data types and structures",
    "section": "Data types",
    "text": "Data types\n\ntypeof(\"Some words\")\n\n[1] \"character\"\n\ntypeof(2)\n\n[1] \"double\"\n\ntypeof(2.0)\n\n[1] \"double\"\n\ntypeof(2L)\n\n[1] \"integer\"\n\ntypeof(TRUE)\n\n[1] \"logical\"",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Data types and structures"
    ]
  },
  {
    "objectID": "r/hss_data_types.html#data-structures",
    "href": "r/hss_data_types.html#data-structures",
    "title": "Data types and structures",
    "section": "Data structures",
    "text": "Data structures\n\n\n\nDimension\nHomogeneous\nHeterogeneous\n\n\n\n\n1 d\nAtomic vector\nList\n\n\n2 d\nMatrix\nData frame\n\n\n3 d\nArray\n\n\n\n\n\nAtomic vectors\n\nc(2, 4, 1)\n\n[1] 2 4 1\n\nstr(c(2, 4, 1))\n\n num [1:3] 2 4 1\n\nc(2.2, 4.4, 1.0)\n\n[1] 2.2 4.4 1.0\n\nstr(c(2.2, 4.4, 1.0))\n\n num [1:3] 2.2 4.4 1\n\n1:3\n\n[1] 1 2 3\n\nstr(1:3)\n\n int [1:3] 1 2 3\n\nc(\"some\", \"random\", \"words\")\n\n[1] \"some\"   \"random\" \"words\" \n\nstr(c(\"some\", \"random\", \"words\"))\n\n chr [1:3] \"some\" \"random\" \"words\"\n\n\n\n\nMatrices\n\nm &lt;- matrix(1:12, nrow = 3, ncol = 4)\nm\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    4    7   10\n[2,]    2    5    8   11\n[3,]    3    6    9   12\n\nstr(m)\n\n int [1:3, 1:4] 1 2 3 4 5 6 7 8 9 10 ...\n\n\n\n\nArrays\n\na &lt;- array(as.double(1:24), c(3, 2, 4))\na\n\n, , 1\n\n     [,1] [,2]\n[1,]    1    4\n[2,]    2    5\n[3,]    3    6\n\n, , 2\n\n     [,1] [,2]\n[1,]    7   10\n[2,]    8   11\n[3,]    9   12\n\n, , 3\n\n     [,1] [,2]\n[1,]   13   16\n[2,]   14   17\n[3,]   15   18\n\n, , 4\n\n     [,1] [,2]\n[1,]   19   22\n[2,]   20   23\n[3,]   21   24\n\nstr(a)\n\n num [1:3, 1:2, 1:4] 1 2 3 4 5 6 7 8 9 10 ...\n\n\n\n\nLists\n\nl &lt;- list(2L, 3, c(2, 1), FALSE, \"string\")\nl\n\n[[1]]\n[1] 2\n\n[[2]]\n[1] 3\n\n[[3]]\n[1] 2 1\n\n[[4]]\n[1] FALSE\n\n[[5]]\n[1] \"string\"\n\nstr(l)\n\nList of 5\n $ : int 2\n $ : num 3\n $ : num [1:2] 2 1\n $ : logi FALSE\n $ : chr \"string\"\n\n\n\n\nData frames\n\nd &lt;- data.frame(\n  country = c(\"Canada\", \"USA\", \"Mexico\"),\n  var = c(2.9, 3.1, 4.5)\n)\nd\n\n  country var\n1  Canada 2.9\n2     USA 3.1\n3  Mexico 4.5\n\nstr(d)\n\n'data.frame':   3 obs. of  2 variables:\n $ country: chr  \"Canada\" \"USA\" \"Mexico\"\n $ var    : num  2.9 3.1 4.5",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Data types and structures"
    ]
  },
  {
    "objectID": "r/hss_analyze.html",
    "href": "r/hss_analyze.html",
    "title": "Data visualization",
    "section": "",
    "text": "To understand the data, it is useful to visualize it."
  },
  {
    "objectID": "r/hpc_resources.html",
    "href": "r/hpc_resources.html",
    "title": "Resources for HPC in R",
    "section": "",
    "text": "This section contains resources specific to high-performance R. For introductory/general R resources, see this page instead.\n\n\nCRAN Task Views\n\nCRAN Task Views give information on packages relevant to certain topics.\n\n\nThe High-Performance and Parallel Computing with R task view lists a lot of packages useful for HPC in R.\n\n\n\nRunning R on the Alliance clusters\n\nThe Alliance wiki contains a lot of documentation on how to run code on the Alliance clusters. Here are pages particularly relevant for HPC in R:\n\n\nGetting started: how to get started using the Alliance supercomputers.\nRunning jobs: how to launch Slurm jobs.\nRunning R: how to use R on the Alliance supercomputers.\nTechnical support: what to do if you are stuck running code on one of the Alliance clusters.\n\nIf you are still having issues after reading the documentation, you can open a ticket by emailing support@tech.alliancecan.ca.\n\n\nOnline books\n\nAdvanced R\nEfficient R programming\n\n\n\nRcpp\n\nDocumentation and examples",
    "crumbs": [
      "R",
      "<b><em>High-performance R</em></b>",
      "Resources for HPC in R"
    ]
  },
  {
    "objectID": "r/hpc_performance.html",
    "href": "r/hpc_performance.html",
    "title": "Measuring performance:",
    "section": "",
    "text": "Before we talk about ways to improve performance, let’s see how to measure it.",
    "crumbs": [
      "R",
      "<b><em>High-performance R</em></b>",
      "Measuring performance"
    ]
  },
  {
    "objectID": "r/hpc_performance.html#when-should-you-care",
    "href": "r/hpc_performance.html#when-should-you-care",
    "title": "Measuring performance:",
    "section": "When should you care?",
    "text": "When should you care?\n\n“There is no doubt that the grail of efficiency leads to abuse. Programmers waste enormous amounts of time thinking about, or worrying about, the speed of noncritical parts of their programs, and these attempts at efficiency actually have a strong negative impact when debugging and maintenance are considered. We should forget about small efficiencies, say about 97% of the time: premature optimization is the root of all evil. Yet we should not pass up our opportunities in that critical 3%.”\n— Donald Knuth\n\nOptimizing code takes time, can lead to mistakes, and may make code harder to read. Consequently, not all code is worth optimizing and before jumping into optimizations, you need a strategy.\nYou should consider optimizations when:\n\nyou have debugged your code (optimization comes last, don’t optimize a code that doesn’t run),\nyou will run a section of code (e.g. a function) many times (your optimization efforts will really pay off),\na section of code is particularly slow.\n\nHow do you know which sections of your code are slow? Don’t rely on intuition. You need to profile your code to identify bottlenecks.",
    "crumbs": [
      "R",
      "<b><em>High-performance R</em></b>",
      "Measuring performance"
    ]
  },
  {
    "objectID": "r/hpc_performance.html#profiling",
    "href": "r/hpc_performance.html#profiling",
    "title": "Measuring performance:",
    "section": "Profiling",
    "text": "Profiling\n\n“It is often a mistake to make a priori judgments about what parts of a program are really critical, since the universal experience of programmers who have been using measurement tools has been that their intuitive guesses fail.”\n— Donald Knuth\n\n\nBase R profiler\nR comes with a profiler: Rprof.\nThe data gets collected with:\n## Start profiler\nRprof()\n\n&lt;Your code to profile&gt;\n\n## Stop profiler\nRprof(NULL)\nThis creates a Rprof.out file in your working directory (you can give it another name by passing a name into the initial call to Rprof (e.g. Rprof(\"test.out\")).\nThe raw data is dense and is better read by running summaryRprof() (or summaryRprof(\"test.out\") if you have created the file test.out rather than the default).\nAlternatively, you can run R CMD Rprof (or R CMD Rprof test.out if you named your file) from the command line.\nYou can find an example here.\n\n\nPackages\nA number of packages run Rprof under the hood and create flame graphs or provide other utilities to visualize the profiling data:\n\nprofr,\nproftools,\nprofvis built by posit (formerly RStudio Inc) is the newest tool. See here for an example.",
    "crumbs": [
      "R",
      "<b><em>High-performance R</em></b>",
      "Measuring performance"
    ]
  },
  {
    "objectID": "r/hpc_performance.html#benchmarking",
    "href": "r/hpc_performance.html#benchmarking",
    "title": "Measuring performance:",
    "section": "Benchmarking",
    "text": "Benchmarking\nOnce you have identified expressions that are particularly slow, you can use benchmarking tools to compare variations of the code.\nIn the most basic fashion, you can use system.time(), but this is limited and imprecise.\nThe microbenchmark package is a much better option. It gives the minimum time, lower quartile, mean, median, upper quartile, and maximum time of R expressions.\nThe newer bench package is very similar, but it has less overhead, is more accurate, and—for sequential code—gives information on memory usage and garbage collections. This is the package that we will use for this course.\nThe main function from this package is mark(). You can pass as argument(s) one or multiple expressions that you want to benchmark. By default, it ensures that all expressions output the same result. If you want to remove this test, add the argument check = FALSE.\nWhile mark() gives memory usage and garbage collection information for sequential code, this functionality is not yet implemented for parallel code. When benchmarking parallel expressions, we will have to use the argument memory = FALSE.\nYou will see many examples of this throughout this course.\n\nWhen benchmarking code, it’s generally best to use the median rather than the minimum or mean, especially if your data might contain outliers, as the median is less affected by extreme values and provides a better representation of the “typical” performance.",
    "crumbs": [
      "R",
      "<b><em>High-performance R</em></b>",
      "Measuring performance"
    ]
  },
  {
    "objectID": "r/hpc_parallelism.html",
    "href": "r/hpc_parallelism.html",
    "title": "Parallelism: concepts",
    "section": "",
    "text": "Once all sequential optimizations on the bottlenecks have been exhausted, it is time to consider whether parallelization makes sense.\nThis section covers important concepts that are necessary to understand before moving on to writing parallel code.",
    "crumbs": [
      "R",
      "<b><em>High-performance R</em></b>",
      "Parallelism: concepts"
    ]
  },
  {
    "objectID": "r/hpc_parallelism.html#hidden-parallelism",
    "href": "r/hpc_parallelism.html#hidden-parallelism",
    "title": "Parallelism: concepts",
    "section": "Hidden parallelism",
    "text": "Hidden parallelism\nAn increasing number of packages run code in parallel under the hood. It is very important to be aware of this before attempting any explicit parallelization or you may end up with recursive multicore parallelization and an explosion of running cores. This can be both inefficient with demultiplied overhead and extremely resource intensive.\nOne way to assess this is to test the package on your machine and look at the number of cores running with tools such as htop.",
    "crumbs": [
      "R",
      "<b><em>High-performance R</em></b>",
      "Parallelism: concepts"
    ]
  },
  {
    "objectID": "r/hpc_parallelism.html#embarrassingly-parallel-problems",
    "href": "r/hpc_parallelism.html#embarrassingly-parallel-problems",
    "title": "Parallelism: concepts",
    "section": "Embarrassingly parallel problems",
    "text": "Embarrassingly parallel problems\nIdeal cases for parallelization are embarrassingly parallel problems: problems which can be broken down into independent tasks without any work.\nExamples:\n\nLoops for which all iterations are independent of each others.\nResampling (e.g. bootstrapping or cross-validation).\nEnsemble learning (e.g. random forests).\n\nExamples of problems which are not embarrassingly parallel:\n\nLoops for which the result of one iteration is needed for the next iteration.\nRecursive function calls.\nProblems that are inherently sequential.\n\nFor non-embarrassingly parallel problems, one solution is to use C++ to improve speed, as we will see at the end of this course.",
    "crumbs": [
      "R",
      "<b><em>High-performance R</em></b>",
      "Parallelism: concepts"
    ]
  },
  {
    "objectID": "r/hpc_parallelism.html#types-of-parallelism",
    "href": "r/hpc_parallelism.html#types-of-parallelism",
    "title": "Parallelism: concepts",
    "section": "Types of parallelism",
    "text": "Types of parallelism\nThere are various ways to run code in parallel and it is important to have a clear understanding of what each method entails.\n\nMulti-threading\nWe talk about multi-threading when a single process (with its own memory) runs multiple threads.\nThe execution can happen in parallel—if each thread has access to a CPU core—or by alternating some of the threads on some CPU cores.\nBecause all threads in a process write to the same memory addresses, multi-threading can lead to race conditions.\nR was not built with multi-threading. Many sites will use the term “multi-threading” improperly and actually mean “multi-processing”. Proper multi-threading cannot be achieved in R. A handful of packages (either very specialized or not under development anymore) bring multi-threading to R by using another language under the hood.\n\n\nMulti-processing in shared memory\nMulti-processing in shared memory happens when multiple processes execute code on multiple CPU cores of a single node (or a single machine).\nThe different processes need to communicate with each other, but because they are all running on the CPU cores of a single node, messages can pass via shared memory.\n\n\nMulti-processing in distributed memory\nWhen processes involved in the execution of some code run on multiple nodes of a cluster, messages between them need to travel over the cluster interconnect. In that case, we talk about distributed memory.",
    "crumbs": [
      "R",
      "<b><em>High-performance R</em></b>",
      "Parallelism: concepts"
    ]
  },
  {
    "objectID": "r/hpc_optimizations.html",
    "href": "r/hpc_optimizations.html",
    "title": "Optimizations",
    "section": "",
    "text": "A lot of hardware is not the answer to poorly written code. Before considering parallelization, you should think about ways to optimize your code sequentially.\nWhy?\n\nNot all code can be parallelized.\nParallelization is costly (overhead of parallelization and, if you use a supercomputer, waiting time to access an Alliance cluster or money spent on a commercial cloud).\nThe optimization of the sequential code will also benefit the parallel code.\n\nIn many cases, writing better code will save you more computing time than parallelization.\nIn this section, we will cover several principles by playing with the programmatic implementation of the fizz buzz game.",
    "crumbs": [
      "R",
      "<b><em>High-performance R</em></b>",
      "Optimizations"
    ]
  },
  {
    "objectID": "r/hpc_optimizations.html#toy-example",
    "href": "r/hpc_optimizations.html#toy-example",
    "title": "Optimizations",
    "section": "Toy example",
    "text": "Toy example\nFizz buzz is a children game to practice divisions. Players take turn counting out loud from “1” while replacing:\n\nany number divisible by 3 with the word “Fizz”,\nany number divisible by 5 with the word “Buzz”,\nany number divisible by both 3 and 5 with the word “FizzBuzz”.\n\nThis creates a series that starts with: \"1, 2, Fizz, 4, Buzz, Fizz, 7, 8, Fizz, Buzz, 11, Fizz, 13, 14, FizzBuzz, 16\", etc.\nLet’s write functions that output series from 1 to n following these rules and time them to draw general principles about code efficiency.",
    "crumbs": [
      "R",
      "<b><em>High-performance R</em></b>",
      "Optimizations"
    ]
  },
  {
    "objectID": "r/hpc_optimizations.html#setup",
    "href": "r/hpc_optimizations.html#setup",
    "title": "Optimizations",
    "section": "Setup",
    "text": "Setup\nFirst of all, we need to load the necessary modules:\nmodule load StdEnv/2023 gcc/13.3 r/4.4.0\nThen we need to launch a job. There are 2 options:\n\nInteractive job\nIf there are few of us, we will use an interactive session with one CPU each. To launch it, run the following (in the Bash terminal, not in R):\nsalloc --time=2:00:00 --mem-per-cpu=3500M\nWe can then launch R:\nR\nNow, we load the benchmarking package that we will use throughout this section:\n\nlibrary(bench)\n\n\n\nBatch jobs\nIf there are more of us than there are CPUs in the cluster, we will run batch jobs. In this Case:\n\nCreate an R script called optim.R with the code to run (you can reuse the same script for all sections on this page by editing it). Don’t forget to load the package bench in your script.\nCreate a bash script called optim.sh with the following:\n\n\n\noptim.sh\n\n#!/bin/bash\n#SBATCH --time=15\n#SBATCH --mem-per-cpu=3500M\n\nmodule load StdEnv/2023 gcc/13.3 r/4.4.0\nRscript optim.R\n\n\nRun the jobs with:\n\nsbatch optim.sh",
    "crumbs": [
      "R",
      "<b><em>High-performance R</em></b>",
      "Optimizations"
    ]
  },
  {
    "objectID": "r/hpc_optimizations.html#optimizations",
    "href": "r/hpc_optimizations.html#optimizations",
    "title": "Optimizations",
    "section": "Optimizations",
    "text": "Optimizations\n\nProper memory pre-allocation\nIn order to store the results of a loop, we need to create an object and assign to it the result of the loop at each iteration. In this first function, we create an empty object z of class integer and of length 0 for that purpose:\n\nf1 &lt;- function(n) {\n  z &lt;- integer()\n  for(i in 1:n) {\n    if(i %% 3 == 0 && i %% 5 == 0) {\n      z[i] &lt;- \"FizzBuzz\"\n    } else if(i %% 3 == 0) {\n      z[i] &lt;- \"Fizz\"\n    } else if(i %% 5 == 0) {\n      z[i] &lt;- \"Buzz\"\n    } else {\n      z[i] &lt;- i\n    }\n  }\n  z\n}\n\nThe second function is similar, but this time, we initialize z with its final length. This means that we are pre-allocating memory for the full vector before we run the loop instead of growing the vector at each iteration:\n\nf2 &lt;- function(n) {\n  z &lt;- integer(n)\n  for(i in 1:n) {\n    if(i %% 3 == 0 && i %% 5 == 0) {\n      z[i] &lt;- \"FizzBuzz\"\n    } else if(i %% 3 == 0) {\n      z[i] &lt;- \"Fizz\"\n    } else if(i %% 5 == 0) {\n      z[i] &lt;- \"Buzz\"\n    } else {\n      z[i] &lt;- i\n    }\n  }\n  z\n}\n\nLet’s make sure that our functions work by testing it on a short series:\n\nf1(20)\n\n [1] \"1\"        \"2\"        \"Fizz\"     \"4\"        \"Buzz\"     \"Fizz\"    \n [7] \"7\"        \"8\"        \"Fizz\"     \"Buzz\"     \"11\"       \"Fizz\"    \n[13] \"13\"       \"14\"       \"FizzBuzz\" \"16\"       \"17\"       \"Fizz\"    \n[19] \"19\"       \"Buzz\"    \n\nf2(20)\n\n [1] \"1\"        \"2\"        \"Fizz\"     \"4\"        \"Buzz\"     \"Fizz\"    \n [7] \"7\"        \"8\"        \"Fizz\"     \"Buzz\"     \"11\"       \"Fizz\"    \n[13] \"13\"       \"14\"       \"FizzBuzz\" \"16\"       \"17\"       \"Fizz\"    \n[19] \"19\"       \"Buzz\"    \n\n\nShort series are good to get a feel for what our functions return, but they would be inadequate for benchmarking because the functions would run too fast and the timing differences would be too small. Always make sure that your function runs are long enough when you benchmark.\nLet’s pick a bigger value for n:\n\nn &lt;- 1e5\n\nNow, we can benchmark our functions:\n\nmark(f1(n), f2(n))\n\nWarning: Some expressions had a GC in every iteration; so filtering is\ndisabled.\n\n\n# A tibble: 2 × 6\n  expression      min   median `itr/sec` mem_alloc `gc/sec`\n  &lt;bch:expr&gt; &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt;\n1 f1(n)         306ms    311ms      3.22   16.55MB     11.3\n2 f2(n)         214ms    219ms      4.58    1.15MB     16.8\n\n\nf2() is consistently faster, although not by much (speedup of 1.4). In many cases, the difference you will find will be a lot greater.\nIn the cluster, because memory is allocated outside of R (by Slurm), it is not tracked by mark() (see documentation).\nThe output you can see on this site was obtained on my laptop. It shows that a properly written function with pre-allocated memory uses 14 times less memory.\nNow, notice how our function actually returns a character and not an integer:\n\ntypeof(f2(n))\n\n[1] \"character\"\n\n\nSo let’s create the object z, which will hold the results of our loop, directly of the proper type:\n\nf3 &lt;- function(n) {\n  z &lt;- character(n)\n  for(i in 1:n) {\n    if(i %% 3 == 0 && i %% 5 == 0) {\n      z[i] &lt;- \"FizzBuzz\"\n    } else if(i %% 3 == 0) {\n      z[i] &lt;- \"Fizz\"\n    } else if(i %% 5 == 0) {\n      z[i] &lt;- \"Buzz\"\n    } else {\n      z[i] &lt;- i\n    }\n  }\n  z\n}\n\nAnd now for the benchmark against f2():\n\nmark(f2(n), f3(n))\n\nWarning: Some expressions had a GC in every iteration; so filtering is\ndisabled.\n\n\n# A tibble: 2 × 6\n  expression      min   median `itr/sec` mem_alloc `gc/sec`\n  &lt;bch:expr&gt; &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt;\n1 f2(n)         214ms    219ms      4.56    1.23MB     15.2\n2 f3(n)         217ms    219ms      4.51   863.8KB     16.5\n\n\nYou can see that there is no difference in timing, but that f3() is still slightly better because it uses a little less memory. This shows that type matters, but the most important thing you want to worry about in memory pre-allocation is the final length of your objects.\n\n\nNo, loops are not a big ‘no no’\nBy now, you might be thinking: “Wait… aren’t loops a big ‘no no’ in R? I’ve always been told that they are slow and that one should always use functional programming! We are talking about optimization in this course and we are using loops?!?”\nThere are a lot of misconceptions around R loops. They can be very slow if you don’t pre-allocate memory. Otherwise they are almost always faster than functions (the apply() family or the tidyverse equivalent of the purrr::map() family). You can choose to use a functional programming approach for style and readability, but not for speed.\nLet’s test it.\nFirst we create a function:\n\nf4 &lt;- function(n) {\n  if(n %% 3 == 0 && n %% 5 == 0) {\n    \"FizzBuzz\"\n  } else if(n %% 3 == 0) {\n    \"Fizz\"\n  } else if(n %% 5 == 0) {\n    \"Buzz\"\n  } else {\n    n\n  }\n}\n\nThen we have to pass our function through sapply().\nLet’s make sure that the code works:\n\nsapply(1:20, f4)\n\n [1] \"1\"        \"2\"        \"Fizz\"     \"4\"        \"Buzz\"     \"Fizz\"    \n [7] \"7\"        \"8\"        \"Fizz\"     \"Buzz\"     \"11\"       \"Fizz\"    \n[13] \"13\"       \"14\"       \"FizzBuzz\" \"16\"       \"17\"       \"Fizz\"    \n[19] \"19\"       \"Buzz\"    \n\n\nNow, we compare the timing with that of f3() (our fastest function so far):\n\nmark(f3(n), sapply(1:n, f4))\n\nWarning: Some expressions had a GC in every iteration; so filtering is\ndisabled.\n\n\n# A tibble: 2 × 6\n  expression           min   median `itr/sec` mem_alloc `gc/sec`\n  &lt;bch:expr&gt;      &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt;\n1 f3(n)              262ms    267ms      3.75   781.3KB     16.9\n2 sapply(1:n, f4)    355ms    372ms      2.68    3.29MB     12.1\n\n\nAs you can see, the loop is faster (speed up of 1.4). On my laptop, it also used 4 times less memory.\n\n\nAvoid unnecessary operations\n\nExample 1\nCalling z as the last command in our function is the same as calling return(z).\nFrom the R documentation:\n\nIf the end of a function is reached without calling return, the value of the last evaluated expression is returned.\n\nNow, what about using print() instead?\n\nf5 &lt;- function(n) {\n  z &lt;- character(n)\n  for(i in 1:n) {\n    if(i %% 3 == 0 && i %% 5 == 0) {\n      z[i] &lt;- \"FizzBuzz\"\n    } else if(i %% 3 == 0) {\n      z[i] &lt;- \"Fizz\"\n    } else if(i %% 5 == 0) {\n      z[i] &lt;- \"Buzz\"\n    } else {\n      z[i] &lt;- i\n    }\n  }\n  print(z)\n}\n\nLet’s test that it works:\n\nf5(20)\n\n [1] \"1\"        \"2\"        \"Fizz\"     \"4\"        \"Buzz\"     \"Fizz\"    \n [7] \"7\"        \"8\"        \"Fizz\"     \"Buzz\"     \"11\"       \"Fizz\"    \n[13] \"13\"       \"14\"       \"FizzBuzz\" \"16\"       \"17\"       \"Fizz\"    \n[19] \"19\"       \"Buzz\"    \n\n\nNow, let’s benchmark it against f3() (still our fastest function so far):\nmark(f3(n), f5(n))\n [1] \"1\"        \"2\"        \"Fizz\"     \"4\"        \"Buzz\"     \"Fizz\"    \n [7] \"7\"        \"8\"        \"Fizz\"     \"Buzz\"     \"11\"       \"Fizz\"    \n[13] \"13\"       \"14\"       \"FizzBuzz\" \"16\"       \"17\"       \"Fizz\"    \n[19] \"19\"       \"Buzz\"     \"Fizz\"     \"22\"       \"23\"       \"Fizz\"    \n[25] \"Buzz\"     \"26\"       \"Fizz\"     \"28\"       \"29\"       \"FizzBuzz\"\n[31] \"31\"       \"32\"       \"Fizz\"     \"34\"       \"Buzz\"     \"Fizz\"    \n[37] \"37\"       \"38\"       \"Fizz\"     \"Buzz\"     \"41\"       \"Fizz\"\n...\n\n# A tibble: 2 × 13\n  expression      min median `itr/sec` mem_alloc `gc/sec` n_itr  n_gc total_time\n  &lt;bch:expr&gt; &lt;bch:tm&gt; &lt;bch:&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;   &lt;bch:tm&gt;\n1 f3(n)         116ms  120ms      7.45        NA    26.1      4    14      537ms\n2 f5(n)         925ms  925ms      1.08        NA     3.24     1     3      925ms\n# ℹ 4 more variables: result &lt;list&gt;, memory &lt;list&gt;, time &lt;list&gt;, gc &lt;list&gt;\nWarning message:\nSome expressions had a GC in every iteration; so filtering is disabled.\nf5() is 7.7 times slower.\nWhat happened?\nprint() returns its argument, but it additionally prints it to the standard output. This is why the mark() function printed the output of f5() before printing the timings.\nAs you can see, printing takes a long time.\nIf you are evaluating f3() on its own (e.g. f3(20)), the returned result will also be printed to standard output and both functions will be equivalent. However, if you are using the function in another context, printing becomes an unnecessary and timely operation and f5() would be a very bad option. f5() is thus not a good function.\nHere is an example in which f5() would perform a totally unnecessary operation that f3() avoids:\n\na &lt;- f3(20)\n\n\nNo unnecessary printing.\n\n\na &lt;- f5(20)\n\n [1] \"1\"        \"2\"        \"Fizz\"     \"4\"        \"Buzz\"     \"Fizz\"    \n [7] \"7\"        \"8\"        \"Fizz\"     \"Buzz\"     \"11\"       \"Fizz\"    \n[13] \"13\"       \"14\"       \"FizzBuzz\" \"16\"       \"17\"       \"Fizz\"    \n[19] \"19\"       \"Buzz\"    \n\n\n\nUnnecessary printing.\n\nEven worse would be to use:\n\nf6 &lt;- function(n) {\n  for(i in 1:n) {\n    if(i %% 3 == 0 && i %% 5 == 0) {\n      print(\"FizzBuzz\")\n    } else if(i %% 3 == 0) {\n      print(\"Fizz\")\n    } else if(i %% 5 == 0) {\n      print(\"Buzz\")\n    } else {\n      print(i)\n    }\n  }\n}\n\nLet’s test it:\n\nf6(20)\n\n[1] 1\n[1] 2\n[1] \"Fizz\"\n[1] 4\n[1] \"Buzz\"\n[1] \"Fizz\"\n[1] 7\n[1] 8\n[1] \"Fizz\"\n[1] \"Buzz\"\n[1] 11\n[1] \"Fizz\"\n[1] 13\n[1] 14\n[1] \"FizzBuzz\"\n[1] 16\n[1] 17\n[1] \"Fizz\"\n[1] 19\n[1] \"Buzz\"\n\n\nThe values are correct, although the output is of a different type (NULL instead of a character since our function didn’t return anything and the values got printed as a side effect of the for loop).\nBenchmark against f3():\nmark(f3(n), f6(n), check = F)\n# A tibble: 2 × 13\n  expression      min median `itr/sec` mem_alloc `gc/sec` n_itr  n_gc total_time\n  &lt;bch:expr&gt; &lt;bch:tm&gt; &lt;bch:&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;   &lt;bch:tm&gt;\n1 f3(n)         105ms  108ms     9.10         NA    30.9      5    17      549ms\n2 f6(n)            6s     6s     0.167        NA     3.34     1    20         6s\n# ℹ 4 more variables: result &lt;list&gt;, memory &lt;list&gt;, time &lt;list&gt;, gc &lt;list&gt;\nWarning message:\nSome expressions had a GC in every iteration; so filtering is disabled.\n\nWe need to disable the check here because the results of the two functions are not the same.\n\nHere the difference in timing is a factor of 55.5 due to all those printing calls.\n\n\nExample 2\nOne modulo operation and equality test can be removed by replacing i %% 3 == 0 && i %% 5 == 0 by i %% 15 == 0. We now have three modulo operations and equality tests per iteration instead of four. This gives us a little speedup:\n\nf7 &lt;- function(n) {\n  z &lt;- character(n)\n  for(i in 1:n) {\n    if(i %% 15 == 0) {\n      z[i] &lt;- \"FizzBuzz\"\n    } else if(i %% 3 == 0) {\n      z[i] &lt;- \"Fizz\"\n    } else if(i %% 5 == 0) {\n      z[i] &lt;- \"Buzz\"\n    } else {\n      z[i] &lt;- i\n    }\n  }\n  z\n}\n\nThe benchmark with our fastest function f3() gives:\n\nmark(f3(n), f7(n))\n\nWarning: Some expressions had a GC in every iteration; so filtering is\ndisabled.\n\n\n# A tibble: 2 × 6\n  expression      min   median `itr/sec` mem_alloc `gc/sec`\n  &lt;bch:expr&gt; &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt;\n1 f3(n)         326ms    330ms      3.03     781KB     12.1\n2 f7(n)         311ms    311ms      3.22     852KB     11.3\n\n\nBut we can remove an additional modulo operation and equality test at each iteration by assigning i %% 3 == 0 and i %% 5 == 0 to variables:\n\nf8 &lt;- function(n) {\n  z &lt;- character(n)\n  for(i in 1:n) {\n    div3 &lt;- (i %% 3 == 0)\n    div5 &lt;- (i %% 5 == 0)\n    if(div3 && div5) {\n      z[i] &lt;- \"FizzBuzz\"\n    } else if(div3) {\n      z[i] &lt;- \"Fizz\"\n    } else if(div5) {\n      z[i] &lt;- \"Buzz\"\n    } else {\n      z[i] &lt;- i\n    }\n  }\n  z\n}\n\nNow we only have two modulo operations and equality tests per iteration and we get another little speedup when we benchmark it against f7(), our new best function:\n\nmark(f7(n), f8(n))\n\nWarning: Some expressions had a GC in every iteration; so filtering is\ndisabled.\n\n\n# A tibble: 2 × 6\n  expression      min   median `itr/sec` mem_alloc `gc/sec`\n  &lt;bch:expr&gt; &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt;\n1 f7(n)         183ms    186ms      5.36     781KB     19.7\n2 f8(n)         173ms    175ms      5.72     856KB     17.2\n\n\n\n\nExample 3\nWe can assign 1:n to z instead of initializing it as an empty vector, thus rendering the assignment of i to z[i] in the last else statement unnecessary:\n\nf9 &lt;- function(n) {\n  z &lt;- 1:n\n  for(i in z) {\n    div3 &lt;- (i %% 3 == 0)\n    div5 &lt;- (i %% 5 == 0)\n    if(div3 && div5) {\n      z[i] &lt;- \"FizzBuzz\"\n    } else if(div3) {\n      z[i] &lt;- \"Fizz\"\n    } else if(div5) {\n      z[i] &lt;- \"Buzz\"\n    } \n  }\n  z\n}\n\nThis function works:\n\nf9(20)\n\n [1] \"1\"        \"2\"        \"Fizz\"     \"4\"        \"Buzz\"     \"Fizz\"    \n [7] \"7\"        \"8\"        \"Fizz\"     \"Buzz\"     \"11\"       \"Fizz\"    \n[13] \"13\"       \"14\"       \"FizzBuzz\" \"16\"       \"17\"       \"Fizz\"    \n[19] \"19\"       \"Buzz\"    \n\n\nand we get a little more speedup when compared to f8()—our current best function:\n\nmark(f8(n), f9(n))\n\nWarning: Some expressions had a GC in every iteration; so filtering is\ndisabled.\n\n\n# A tibble: 2 × 6\n  expression      min   median `itr/sec` mem_alloc `gc/sec`\n  &lt;bch:expr&gt; &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt;\n1 f8(n)         180ms    182ms      5.37   781.3KB     17.9\n2 f9(n)         130ms    134ms      7.47    1.15MB     18.7\n\n\n\n\n\nVectorize whenever possible\nWe can actually get rid of the loop and use a vectorized approach instead, utilizing what really constitutes the strength of the R language. The following is pure R style at its best:\n\nf10 &lt;- function(n) {\n  z &lt;- 1:n\n  div3 &lt;- (z %% 3 == 0)\n  div5 &lt;- (z %% 5 == 0)\n  z[div3] &lt;- \"Fizz\"\n  z[div5] &lt;- \"Buzz\"\n  z[(div3 & div5)] &lt;- \"FizzBuzz\"\n  z\n}\n\nThis still give us the same result:\n\nf10(20)\n\n [1] \"1\"        \"2\"        \"Fizz\"     \"4\"        \"Buzz\"     \"Fizz\"    \n [7] \"7\"        \"8\"        \"Fizz\"     \"Buzz\"     \"11\"       \"Fizz\"    \n[13] \"13\"       \"14\"       \"FizzBuzz\" \"16\"       \"17\"       \"Fizz\"    \n[19] \"19\"       \"Buzz\"    \n\n\nNow for the benchmark with f9() (our best function up to this point):\n\nmark(f9(n), f10(n))\n\nWarning: Some expressions had a GC in every iteration; so filtering is\ndisabled.\n\n\n# A tibble: 2 × 6\n  expression      min   median `itr/sec` mem_alloc `gc/sec`\n  &lt;bch:expr&gt; &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt;\n1 f9(n)       114.6ms  117.8ms      8.11    1.21MB    17.8 \n2 f10(n)       30.2ms   33.9ms     29.2     5.62MB     1.94\n\n\nThe speedup of 3.5 shows the importance of using vectorization whenever possible.\n\n\nReplace costly operations where possible\nSometimes, it isn’t obvious that one method will be faster than another. Benchmarking alternative expressions can teach you which ones are faster.\nFor instance, it is much faster to index a column from a dataframe by its name (e.g. dataframe$column1) than by using list indexing (e.g. dataframe[[1]]). Until you test it, there is nothing obvious about this because it has to do with how R processes the data under the hood.\n\n\nUse faster packages\nTo achieve the best performance, you should look for efficient packages and learn them.\nPackages exist which bring much more efficiency than can be achieved with base R or the tidyverse. In the case of data frames for example, there is data.table.\n\n\nConclusion\nStarting from our first function f1(), we have gained a speedup of 6, simply by writing better code and without using parallelization and additional hardware:\n\nmark(f1(n), f10(n))\n\nWarning: Some expressions had a GC in every iteration; so filtering is\ndisabled.\n\n\n# A tibble: 2 × 6\n  expression      min   median `itr/sec` mem_alloc `gc/sec`\n  &lt;bch:expr&gt; &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt;\n1 f1(n)         279ms  289.6ms      3.45   16.63MB    13.8 \n2 f10(n)       34.7ms   36.6ms     26.9     5.57MB     1.92\n\n\nIf we used a silly function such as f6() as our starting function, the speedup would be 333.\n\nBefore thinking about running R in parallel or throwing GPUs at your problem, hoping that these would solve the slowness of your code, identify the bottlenecks and rewrite the slow sections more efficiently.",
    "crumbs": [
      "R",
      "<b><em>High-performance R</em></b>",
      "Optimizations"
    ]
  },
  {
    "objectID": "r/hpc_future.html",
    "href": "r/hpc_future.html",
    "title": "The future package",
    "section": "",
    "text": "The future package is a modern package that brings a consistent and simple API for all evaluation strategies of futures in R.\nExcellent backends have been built on top of it.",
    "crumbs": [
      "R",
      "<b><em>High-performance R</em></b>",
      "The future package"
    ]
  },
  {
    "objectID": "r/hpc_future.html#classic-parallel-packages-in-r",
    "href": "r/hpc_future.html#classic-parallel-packages-in-r",
    "title": "The future package",
    "section": "Classic parallel packages in R",
    "text": "Classic parallel packages in R\nWe talked in the previous section about various types of parallelism. Several options exist in R to run code in shared-memory or distributed parallelism.\nExamples of options for shared-memory parallelism:\n\nThe foreach package with backends such as doMC, now also part of the doParallel package.\nmclapply() and mcmapply() from the parallel package (part of the core distribution of R).\n\nExamples of options for distributed parallelism:\n\nThe foreach package with backends such as doSNOW, now also part of the doParallel package.\nThe suite of clusterApply() and par*apply() functions from the parallel package.\n\n\nThe parallel package is a merger of the former multicore package for shared-memory and of the snow package for distributed parallelism.\nSimilarly, the doParallel package is merger of the doMC package for use with foreach in shared-memory and the doSNOW package for use with foreach for distributed parallelism.",
    "crumbs": [
      "R",
      "<b><em>High-performance R</em></b>",
      "The future package"
    ]
  },
  {
    "objectID": "r/hpc_future.html#the-future-package",
    "href": "r/hpc_future.html#the-future-package",
    "title": "The future package",
    "section": "The future package",
    "text": "The future package\nThe future package opened up a new landscape in the world of parallel R by providing a simple and consistent API for the evaluation of futures sequentially, through shared-memory parallelism, or through distributed parallelism.\n\nA future is an object that acts as an abstract representation for a value in the future. A future can be resolved (if the value has been computed) or unresolved. If the value is queried while the future is unresolved, the process is blocked until the future is resolved. Futures thus allow for asynchronous and parallel evaluations.\n\nThe evaluation strategy is set with the plan() function:\n\nplan(sequential):\nFutures are evaluated sequentially in the current R session.\nplan(multisession):\nFutures are evaluated by new R sessions spawned in the background (multi-processing in shared memory).\nplan(multicore):\nFutures are evaluated in processes forked from the existing process (multi-processing in shared memory).\nplan(cluster):\nFutures are evaluated on an ad-hoc cluster (distributed parallelism across multiple nodes).\n\n\nConsistency\nTo ensure a consistent behaviour across plans, all evaluations are done in a local environment:\n\nlibrary(future)\n\na &lt;- 1\n\nb %&lt;-% {      # %&lt;-% creates futures\n  a &lt;- 2\n}\n\na\n\n[1] 1",
    "crumbs": [
      "R",
      "<b><em>High-performance R</em></b>",
      "The future package"
    ]
  },
  {
    "objectID": "r/hpc_future.html#the-future-ecosystem",
    "href": "r/hpc_future.html#the-future-ecosystem",
    "title": "The future package",
    "section": "the future ecosystem",
    "text": "the future ecosystem\nSeveral great packages have been built on top of the future API.\n\nThe doFuture package allows to parallelize foreach expressions on the future evaluation strategies.\nSimilarly, the future.apply package parallelizes the *apply() functions on these strategies.\nThe furrr package provides a parallel version of purrr for those who prefer this approach to functional programming.\nThe future.callr package implements a future evaluation based on callr that resolves every future in a new R session. This removes any limitation on the number of background R parallel processes that can be active at the same time.\nThe future.batchtools package implements a future evaluation based on the batchtools package—a package that provides functions to interact with HPC systems schedulers such as Slurm.\n\nIn this course, we will cover foreach with doFuture in great details to explain all the important concepts. After that, you will be able to use any of these backends easily.",
    "crumbs": [
      "R",
      "<b><em>High-performance R</em></b>",
      "The future package"
    ]
  },
  {
    "objectID": "r/hpc_data.html",
    "href": "r/hpc_data.html",
    "title": "Data on HPC clusters",
    "section": "",
    "text": "So far, we have played with randomly created data. In your work, you will often need to work with real world data.\nHow do you move it to the cluster? Where should you store it?\nIt’s time to talk about data on HPC clusters."
  },
  {
    "objectID": "r/hpc_data.html#transferring-data-tofrom-the-cluster",
    "href": "r/hpc_data.html#transferring-data-tofrom-the-cluster",
    "title": "Data on HPC clusters",
    "section": "Transferring data to/from the cluster",
    "text": "Transferring data to/from the cluster\n\nSecure Copy Protocol\nSecure Copy Protocol (SCP) allows to copy files over the Secure Shell Protocol (SSH) with the scp utility. scp follows a syntax similar to that of the cp command.\nNote that you need to run it from your local machines (not from the cluster).\n\nCopy from your machine to the cluster\n# Copy a local file to your home directory on the cluster\nscp /local/path/file username@hostname:\n# Copy a local file to some path on the cluster\nscp /local/path/file username@hostname:/remote/path\n\n\nCopy from the cluster to your machine\n# Copy a file from the cluster to some path on your machine\nscp username@hostname:/remote/path/file /local/path\n# Copy a file from the cluster to your current location on your machine\nscp username@hostname:/remote/path/file .\nYou can also use wildcards to transfer multiple files:\n# Copy all the Bash scripts from your cluster home dir to some local path\nscp username@hostname:*.sh /local/path\n\n\nCopying directories\nTo copy a directory, you need to add the -r (recursive) flag:\nscp -r /local/path/folder username@hostname:/remote/path\n\n\nCopying for Windows users\nMobaXterm users (on Windows) can copy files by dragging them between the local and remote machines in the GUI. Alternatively, they can use the download and upload buttons.\n\n\n\nSecure File Transfer Protocol\nThe Secure File Transfer Protocol (SFTP) is more sophisticated and allows additional operations in an interactive shell. The sftp command provided by OpenSSH and other packages launches an SFTP client:\nsftp username@hostname\n\nLook at your prompt: your usual Bash/Zsh prompt has been replaced with sftp&gt;.\n\nFrom this prompt, you can access a number of SFTP commands. Type help for a list:\nsftp&gt; help\nAvailable commands:\nbye                                Quit sftp\ncd path                            Change remote directory to 'path'\nchgrp [-h] grp path                Change group of file 'path' to 'grp'\nchmod [-h] mode path               Change permissions of file 'path' to 'mode'\nchown [-h] own path                Change owner of file 'path' to 'own'\ncopy oldpath newpath               Copy remote file\ncp oldpath newpath                 Copy remote file\ndf [-hi] [path]                    Display statistics for current directory or\n                                   filesystem containing 'path'\nexit                               Quit sftp\nget [-afpR] remote [local]         Download file\nhelp                               Display this help text\nlcd path                           Change local directory to 'path'\nlls [ls-options [path]]            Display local directory listing\nlmkdir path                        Create local directory\nln [-s] oldpath newpath            Link remote file (-s for symlink)\nlpwd                               Print local working directory\nls [-1afhlnrSt] [path]             Display remote directory listing\nlumask umask                       Set local umask to 'umask'\nmkdir path                         Create remote directory\nprogress                           Toggle display of progress meter\nput [-afpR] local [remote]         Upload file\npwd                                Display remote working directory\nquit                               Quit sftp\nreget [-fpR] remote [local]        Resume download file\nrename oldpath newpath             Rename remote file\nreput [-fpR] local [remote]        Resume upload file\nrm path                            Delete remote file\nrmdir path                         Remove remote directory\nsymlink oldpath newpath            Symlink remote file\nversion                            Show SFTP version\n!command                           Execute 'command' in local shell\n!                                  Escape to local shell\n?                                  Synonym for help\nAs this list shows, you have access to a number of classic Unix command such as cd, pwd, ls, etc. These commands will be executed on the remote machine.\nIn addition, there are a number of commands of the form l&lt;command&gt;. “l” stands for “local”.\nThese commands will be executed on your local machine.\nFor instance, ls will list the files in your current directory in the remote machine while lls (“local ls”) will list the files in your current directory on your computer.\nThis means that you are now able to navigate two file systems at once: your local machine and the remote machine.\n\nHere are a few examples:\n\nsftp&gt; pwd              # print remote working directory\nsftp&gt; lpwd             # print local working directory\nsftp&gt; ls               # list files in remote working directory\nsftp&gt; lls              # list files in local working directory\nsftp&gt; cd               # change the remote directory\nsftp&gt; lcd              # change the local directory\nsftp&gt; put local_file   # upload a file\nsftp&gt; get remote_file  # download a file\n\nCopying directories\nTo upload/download directories, you first need to create them in the destination, then copy the content with the -r (recursive) flag.\n\nIf you have a local directory called dir and you want to copy it to the cluster you need to run:\n\nsftp&gt; mkdir dir    # First create the directory\nsftp&gt; put -r dir   # Then copy the content\nTo terminate the session, press &lt;Ctrl+D&gt;.\n\n\n\nSyncing\nIf, instead of an occasional copying of files between your machine and the cluster, you want to keep a directory in sync between both machines, you might want to use rsync instead. You can look at the Alliance wiki page on rsync for complete instructions.\n\n\nHeavy transfers\nWhile the methods covered above work very well for limited amounts of data, if you need to make large transfers, you should use globus instead, following the instructions in the Alliance wiki page on this service.\n\n\nWindows line endings\nOn modern Mac operating systems and on Linux, lines in files are terminated with a newline (\\n). On Windows, they are terminated with a carriage return + newline (\\r\\n).\nWhen you transfer files between Windows and Linux (the cluster uses Linux), this creates a mismatch. Most modern software handle this correctly, but you may occasionally run into problems.\nThe solution is to convert a file from Windows encoding to Unix encoding with:\ndos2unix file\nTo convert a file back to Windows encoding, run:\nunix2dos file"
  },
  {
    "objectID": "r/hpc_data.html#files-management",
    "href": "r/hpc_data.html#files-management",
    "title": "Data on HPC clusters",
    "section": "Files management",
    "text": "Files management\nThe Alliance clusters are designed to handle large files very well. They are however slowed by the presence of many small files. It is thus important to know how to handle large collections of files by archiving them with tools such as tar and dar."
  },
  {
    "objectID": "r/hpc_data.html#where-to-store-data",
    "href": "r/hpc_data.html#where-to-store-data",
    "title": "Data on HPC clusters",
    "section": "Where to store data",
    "text": "Where to store data\nSupercomputers have several filesystems and you should familiarize yourself with the quotas and policies of the clusters you use.\nAll filesystems are mounted on all nodes so that you can access the data on any network storage from any node (e.g. something in /home, /project, or /scratch will be accessible from any login node or compute node).\nA temporary folder gets created directly on the compute nodes while a job is running. In situations with heavy I/O or involving many files, it is worth considering copying data to it as part of the job. In that case, make sure to copy the results back to network storage before the end of the job."
  },
  {
    "objectID": "python/ws_webscraping.html",
    "href": "python/ws_webscraping.html",
    "title": "Web scraping with Beautiful Soup",
    "section": "",
    "text": "The internet is a trove of information. A lot of it is publicly available and thus suitable for use in research. Extracting that information and putting it in an organized format for analysis can however be extremely tedious.\nWeb scraping tools allow to automate parts of that process and Python is a popular language for the task.\nIn this workshop, I will guide you through a simple example using the package Beautiful Soup.",
    "crumbs": [
      "Python",
      "<em><b>Workshops</b></em>",
      "Web scraping with bs4"
    ]
  },
  {
    "objectID": "python/ws_webscraping.html#html-and-css",
    "href": "python/ws_webscraping.html#html-and-css",
    "title": "Web scraping with Beautiful Soup",
    "section": "HTML and CSS",
    "text": "HTML and CSS\nHyperText Markup Language (HTML) is the standard markup language for websites: it encodes the information related to the formatting and structure of webpages. Additionally, some of the customization can be stored in Cascading Style Sheets (CSS) files.\nHTML uses tags of the form:\n&lt;some_tag&gt;Your content&lt;/some_tag&gt;\nSome tags have attributes:\n&lt;some_tag attribute_name=\"attribute value\"&gt;Your content&lt;/some_tag&gt;\n\nExamples:\n\nSite structure:\n\n&lt;h2&gt;This is a heading of level 2&lt;/h2&gt;\n&lt;p&gt;This is a paragraph&lt;/p&gt;\n\nFormatting:\n\n&lt;b&gt;This is bold&lt;/b&gt;\n&lt;a href=\"https://some.url\"&gt;This is the text for a link&lt;/a&gt;",
    "crumbs": [
      "Python",
      "<em><b>Workshops</b></em>",
      "Web scraping with bs4"
    ]
  },
  {
    "objectID": "python/ws_webscraping.html#web-scrapping",
    "href": "python/ws_webscraping.html#web-scrapping",
    "title": "Web scraping with Beautiful Soup",
    "section": "Web scrapping",
    "text": "Web scrapping\nWeb scraping is a general term for a set of tools which allow for the extraction of data from the web automatically.\nWhile most of the data on the internet is publicly available, it is illegal to scrape some sites and you should always look into the policy of a site before attempting to scrape it. Some sites will also block you if you submit too many requests in a short amount of time, so remember to scrape responsibly.",
    "crumbs": [
      "Python",
      "<em><b>Workshops</b></em>",
      "Web scraping with bs4"
    ]
  },
  {
    "objectID": "python/ws_webscraping.html#example-for-this-workshop",
    "href": "python/ws_webscraping.html#example-for-this-workshop",
    "title": "Web scraping with Beautiful Soup",
    "section": "Example for this workshop",
    "text": "Example for this workshop\nWe will use a website from the University of Tennessee containing a database of PhD theses from that university.\nOur goal is to scrape data from this site to produce a dataframe with the date, major, and advisor for each dissertation.\n\nWe will only do this for the first page which contains the links to the 100 most recent theses. If you really wanted to gather all the data, you would have to do this for all pages.",
    "crumbs": [
      "Python",
      "<em><b>Workshops</b></em>",
      "Web scraping with bs4"
    ]
  },
  {
    "objectID": "python/ws_webscraping.html#lets-look-at-the-sites",
    "href": "python/ws_webscraping.html#lets-look-at-the-sites",
    "title": "Web scraping with Beautiful Soup",
    "section": "Let’s look at the sites",
    "text": "Let’s look at the sites\nFirst of all, let’s have a close look at the websites we want to scrape to think carefully about what we want to do. Before starting to write code, it is always a good idea to think about what you are trying to achieve with your code.\nTo create a dataframe with the data for all the dissertations on that first page, we need to do two things:\n\nStep 1: from the dissertations database first page, we want to scrape the list of URLs for the dissertation pages.\nStep 2: once we have the URLs, we want to scrape those pages too to get the date, major, and advisor for each dissertation.",
    "crumbs": [
      "Python",
      "<em><b>Workshops</b></em>",
      "Web scraping with bs4"
    ]
  },
  {
    "objectID": "python/ws_webscraping.html#load-packages",
    "href": "python/ws_webscraping.html#load-packages",
    "title": "Web scraping with Beautiful Soup",
    "section": "Load packages",
    "text": "Load packages\nLet’s load the packages that will make scraping websites with Python easier:\n\nimport requests                 # To download the html data from a site\nfrom bs4 import BeautifulSoup   # To parse the html data\nimport time                     # To add a delay between each requests\nimport pandas as pd             # To store our data in a DataFrame",
    "crumbs": [
      "Python",
      "<em><b>Workshops</b></em>",
      "Web scraping with bs4"
    ]
  },
  {
    "objectID": "python/ws_webscraping.html#send-request-to-the-main-site",
    "href": "python/ws_webscraping.html#send-request-to-the-main-site",
    "title": "Web scraping with Beautiful Soup",
    "section": "Send request to the main site",
    "text": "Send request to the main site\nAs mentioned above, our site is the database of PhD dissertations from the University of Tennessee.\nLet’s create a string with the URL:\n\nurl = \"https://trace.tennessee.edu/utk_graddiss/index.html\"\n\nFirst, we send a request to that URL and save the response in a variable called r:\n\nr = requests.get(url)\n\nLet’s see what our response looks like:\n\nr\n\n&lt;Response [200]&gt;\n\n\nIf you look in the list of HTTP status codes, you can see that a response with a code of 200 means that the request was successful.",
    "crumbs": [
      "Python",
      "<em><b>Workshops</b></em>",
      "Web scraping with bs4"
    ]
  },
  {
    "objectID": "python/ws_webscraping.html#explore-the-raw-data",
    "href": "python/ws_webscraping.html#explore-the-raw-data",
    "title": "Web scraping with Beautiful Soup",
    "section": "Explore the raw data",
    "text": "Explore the raw data\nTo get the actual content of the response as unicode (text), we can use the text property of the response. This will give us the raw HTML markup from the webpage.\nLet’s print the first 200 characters:\n\nprint(r.text[:200])\n\n\n&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\"&gt;\n&lt;head&gt;&lt;!-- inj yui3-seed: --&gt;&lt;script type='text/javascript' src='//cdnjs.cloudflare.com/ajax/libs/yui/3.6.0/yui/yui-min.js'&gt;&lt;/script&gt;&lt;script type='text/javascript' sr",
    "crumbs": [
      "Python",
      "<em><b>Workshops</b></em>",
      "Web scraping with bs4"
    ]
  },
  {
    "objectID": "python/ws_webscraping.html#parse-the-data",
    "href": "python/ws_webscraping.html#parse-the-data",
    "title": "Web scraping with Beautiful Soup",
    "section": "Parse the data",
    "text": "Parse the data\nThe package Beautiful Soup transforms (parses) such HTML data into a parse tree, which will make extracting information easier.\nLet’s create an object called mainpage with the parse tree:\n\nmainpage = BeautifulSoup(r.text, \"html.parser\")\n\n\nhtml.parser is the name of the parser that we are using here. It is better to use a specific parser to get consistent results across environments.\n\nWe can print the beginning of the parsed result:\n\nprint(mainpage.prettify()[:200])\n\n&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\"&gt;\n &lt;head&gt;\n  &lt;!-- inj yui3-seed: --&gt;\n  &lt;script src=\"//cdnjs.cloudflare.com/ajax/libs/yui/3.6.0/yui/yui-min.js\" type=\"text/javascript\"&gt;\n  &lt;/script&gt;\n  &lt;script src=\"//ajax.g\n\n\n\nThe prettify method turns the BeautifulSoup object we created into a string (which is needed for slicing).\n\nIt doesn’t look any more clear to us, but it is now in a format the Beautiful Soup package can work with.\nFor instance, we can get the HTML segment containing the title with three methods:\n\nusing the title tag name:\n\n\nmainpage.title\n\n&lt;title&gt;\nDoctoral Dissertations | Graduate School | University of Tennessee, Knoxville\n&lt;/title&gt;\n\n\n\nusing find to look for HTML markers (tags, attributes, etc.):\n\n\nmainpage.find(\"title\")\n\n&lt;title&gt;\nDoctoral Dissertations | Graduate School | University of Tennessee, Knoxville\n&lt;/title&gt;\n\n\n\nusing select which accepts CSS selectors:\n\n\nmainpage.select(\"title\")\n\n[&lt;title&gt;\n Doctoral Dissertations | Graduate School | University of Tennessee, Knoxville\n &lt;/title&gt;]\n\n\nfind will only return the first element. find_all will return all elements. select will also return all elements. Which one you chose depends on what you need to extract. There often several ways to get you there.\nHere are other examples of data extraction:\n\nmainpage.head\n\n&lt;head&gt;&lt;!-- inj yui3-seed: --&gt;&lt;script src=\"//cdnjs.cloudflare.com/ajax/libs/yui/3.6.0/yui/yui-min.js\" type=\"text/javascript\"&gt;&lt;/script&gt;&lt;script src=\"//ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js\" type=\"text/javascript\"&gt;&lt;/script&gt;&lt;!-- Adobe Analytics --&gt;&lt;script src=\"https://assets.adobedtm.com/4a848ae9611a/d0e96722185b/launch-d525bb0064d8.min.js\" type=\"text/javascript\"&gt;&lt;/script&gt;&lt;script src=\"/assets/nr_browser_production.js\" type=\"text/javascript\"&gt;&lt;/script&gt;\n&lt;!-- def.1 --&gt;\n&lt;meta charset=\"utf-8\"/&gt;\n&lt;meta content=\"width=device-width\" name=\"viewport\"/&gt;\n&lt;title&gt;\nDoctoral Dissertations | Graduate School | University of Tennessee, Knoxville\n&lt;/title&gt;\n&lt;!-- FILE meta-tags.inc --&gt;&lt;!-- FILE: /srv/sequoia/main/data/assets/site/meta-tags.inc --&gt;\n&lt;!-- FILE: meta-tags.inc (cont) --&gt;\n&lt;!-- sh.1 --&gt;\n&lt;link href=\"/ir-style.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/&gt;\n&lt;link href=\"/ir-custom.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/&gt;\n&lt;link href=\"ir-custom.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/&gt;\n&lt;link href=\"/ir-local.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/&gt;\n&lt;link href=\"ir-local.css\" media=\"screen\" rel=\"stylesheet\" type=\"text/css\"/&gt;\n&lt;link href=\"/ir-print.css\" media=\"print\" rel=\"stylesheet\" type=\"text/css\"/&gt;\n&lt;link href=\"/assets/floatbox/floatbox.css\" rel=\"stylesheet\" type=\"text/css\"/&gt;\n&lt;link href=\"/recent.rss\" rel=\"alternate\" title=\"Site Feed\" type=\"application/rss+xml\"/&gt;\n&lt;link href=\"/favicon.ico\" rel=\"shortcut icon\" type=\"image/x-icon\"/&gt;\n&lt;!--[if IE]&gt;\n&lt;link rel=\"stylesheet\" href=\"/ir-ie.css\" type=\"text/css\" media=\"screen\"&gt;\n&lt;![endif]--&gt;\n&lt;!-- JS --&gt;\n&lt;script src=\"/assets/jsUtilities.js\" type=\"text/javascript\"&gt;&lt;/script&gt;\n&lt;script src=\"/assets/footnoteLinks.js\" type=\"text/javascript\"&gt;&lt;/script&gt;\n&lt;script src=\"/assets/scripts/yui-init.pack.js\" type=\"text/javascript\"&gt;&lt;/script&gt;\n&lt;script src=\"/assets/scripts/bepress-init.pack.js\" type=\"text/javascript\"&gt;&lt;/script&gt;\n&lt;script src=\"/assets/scripts/JumpListYUI.pack.js\" type=\"text/javascript\"&gt;&lt;/script&gt;\n&lt;!-- end sh.1 --&gt;\n&lt;script type=\"text/javascript\"&gt;var pageData = {\"page\":{\"environment\":\"prod\",\"productName\":\"bpdg\",\"language\":\"en\",\"name\":\"ir_etd\",\"businessUnit\":\"els:rp:st\"},\"visitor\":{}};&lt;/script&gt;\n&lt;/head&gt;\n\n\n\nmainpage.a\n\n&lt;a data-scroll=\"\" href=\"https://trace.tennessee.edu\" title=\"Home\"&gt;Home&lt;/a&gt;\n\n\n\nmainpage.find_all(\"a\")[:5]\n\n[&lt;a data-scroll=\"\" href=\"https://trace.tennessee.edu\" title=\"Home\"&gt;Home&lt;/a&gt;,\n &lt;a data-scroll=\"\" href=\"https://trace.tennessee.edu/do/search/advanced/\" title=\"Search\"&gt;&lt;i class=\"icon-search\"&gt;&lt;/i&gt; Search&lt;/a&gt;,\n &lt;a data-scroll=\"\" href=\"https://trace.tennessee.edu/communities.html\" title=\"Browse\"&gt;Browse Collections&lt;/a&gt;,\n &lt;a data-scroll=\"\" href=\"/cgi/myaccount.cgi?context=utk_graddiss\" title=\"My Account\"&gt;My Account&lt;/a&gt;,\n &lt;a data-scroll=\"\" href=\"https://trace.tennessee.edu/about.html\" title=\"About\"&gt;About&lt;/a&gt;]\n\n\n\nmainpage.select(\"a\")[:5]\n\n[&lt;a data-scroll=\"\" href=\"https://trace.tennessee.edu\" title=\"Home\"&gt;Home&lt;/a&gt;,\n &lt;a data-scroll=\"\" href=\"https://trace.tennessee.edu/do/search/advanced/\" title=\"Search\"&gt;&lt;i class=\"icon-search\"&gt;&lt;/i&gt; Search&lt;/a&gt;,\n &lt;a data-scroll=\"\" href=\"https://trace.tennessee.edu/communities.html\" title=\"Browse\"&gt;Browse Collections&lt;/a&gt;,\n &lt;a data-scroll=\"\" href=\"/cgi/myaccount.cgi?context=utk_graddiss\" title=\"My Account\"&gt;My Account&lt;/a&gt;,\n &lt;a data-scroll=\"\" href=\"https://trace.tennessee.edu/about.html\" title=\"About\"&gt;About&lt;/a&gt;]",
    "crumbs": [
      "Python",
      "<em><b>Workshops</b></em>",
      "Web scraping with bs4"
    ]
  },
  {
    "objectID": "python/ws_webscraping.html#test-run",
    "href": "python/ws_webscraping.html#test-run",
    "title": "Web scraping with Beautiful Soup",
    "section": "Test run",
    "text": "Test run\n\nIdentify relevant markers\nThe html code for this webpage contains the data we are interested in, but it is mixed in with a lot of HTML formatting and data we don’t care about. We need to extract the data relevant to us and turn it into a workable format.\nThe first step is to find the HTML markers that contain our data. One option is to use a web inspector or—even easier—the SelectorGadget, a JavaScript bookmarklet built by Andrew Cantino.\nTo use this tool, go to the SelectorGadget website and drag the link of the bookmarklet to your bookmarks bar.\nNow, go to the dissertations database first page and click on the bookmarklet in your bookmarks bar. You will see a floating box at the bottom of your screen. As you move your mouse across the screen, an orange rectangle appears around each element over which you pass.\nClick on one of the dissertation links: now, there is an a appearing in the box at the bottom as well as the number of elements selected. The selected elements are highlighted in yellow. Those elements are links (in HTML, a tags define hyperlinks).\nAs you can see, all the links we want are selected. However, there are many other links we don’t want that are also highlighted. In fact, all links in the document are selected. We need to remove the categories of links that we don’t want. To do this, hover above any of the links we don’t want. You will see a red rectangle around it. Click on it: now all similar links are gone. You might have to do this a few times until only the relevant links (i.e. those that lead to the dissertation information pages) remain highlighted.\nAs there are 100 such links per page, the count of selected elements in the bottom floating box should be down to 100.\nIn the main section of the floating box, you can now see: .article-listing a. This means that the data we want are under the HTML elements .article-listing a (the class .article-listing and the tag a).\n\n\nExtract test URL\nIt is a good idea to test things out on a single element before doing a massive batch scraping of a site, so let’s test our method for the first dissertation.\nTo start, we need to extract the first URL. Here, we will use the CSS selectors (we can get there using find too). mainpage.select(\".article-listing a\") would give us all the results (100 links):\n\nlen(mainpage.select(\".article-listing a\"))\n\n100\n\n\nTo get the first one, we index it:\n\nmainpage.select(\".article-listing a\")[0]\n\n&lt;a href=\"https://trace.tennessee.edu/utk_graddiss/10400\"&gt;The Sons of Melisende: Baldwin III, Amalric, and Kingship in the Kingdom of Jerusalem, 1143-1174 CE&lt;/a&gt;\n\n\nThe actual URL is contained in the href attribute. Attributes can be extracted with the get method:\n\nmainpage.select(\".article-listing a\")[0].get(\"href\")\n\n'https://trace.tennessee.edu/utk_graddiss/10400'\n\n\nWe now have our URL as a string. We can double-check that it is indeed a string:\n\ntype(mainpage.select(\".article-listing a\")[0].get(\"href\"))\n\nstr\n\n\nThis is exactly what we need to send a request to that site, so let’s create an object url_test with it:\n\nurl_test = mainpage.select(\".article-listing a\")[0].get(\"href\")\n\nWe have our first thesis URL:\n\nprint(url_test)\n\nhttps://trace.tennessee.edu/utk_graddiss/10400\n\n\n\n\nSend request to test URL\nNow that we have the URL for the first dissertation information page, we want to extract the date, major, and advisor for that dissertation.\nThe first thing to do—as we did earlier with the database site—is to send a request to that page. Let’s assign it to a new object that we will call r_test:\n\nr_test = requests.get(url_test)\n\nThen we can parse it with Beautiful Soup (as we did before). Let’s create a dissertpage_test object:\n\ndissertpage_test = BeautifulSoup(r_test.text, \"html.parser\")\n\n\n\nGet data for test URL\nIt is time to extract the publication date, major, and advisor for our test URL.\nLet’s start with the date. Thanks to the SelectorGadget, following the method we saw earlier, we can see that we now need elements marked by #publication_date p.\nWe can use select as we did earlier:\n\ndissertpage_test.select(\"#publication_date p\")\n\n[&lt;p&gt;8-2024&lt;/p&gt;]\n\n\nNotice the square brackets around our result: this is import. It shows us that we have a ResultSet (a list of results specific to Beautiful Soup). This is because select returns all the results. Here, we have a single result, but the format is still list-like. Before we can go further, we need to index the value out of it:\n\ndissertpage_test.select(\"#publication_date p\")[0]\n\n&lt;p&gt;8-2024&lt;/p&gt;\n\n\nWe can now get the text out of this paragraph with the text attribute:\n\ndissertpage_test.select(\"#publication_date p\")[0].text\n\n'8-2024'\n\n\nWe could save it in a variable date_test:\n\ndate_test = dissertpage_test.select(\"#publication_date p\")[0].text\n\n\n\nYour turn:\n\nGet the major and advisor for our test URL.",
    "crumbs": [
      "Python",
      "<em><b>Workshops</b></em>",
      "Web scraping with bs4"
    ]
  },
  {
    "objectID": "python/ws_webscraping.html#full-run",
    "href": "python/ws_webscraping.html#full-run",
    "title": "Web scraping with Beautiful Soup",
    "section": "Full run",
    "text": "Full run\nOnce everything is working for a test site, we can do some bulk scraping.\n\nExtract all URLs\nWe already know how to get the 100 dissertations links from the main page: mainpage.select(\".article-listing a\"). Let’s assign it to a variable:\n\ndissertlinks = mainpage.select(\".article-listing a\")\n\nThis ResultSet is an iterable, meaning that it can be used in a loop.\nLet’s write a loop to extract all the URLs from this ResultSet of links:\n\n# Create an empty list before filling it during the loop\nurls = []\n\nfor link in dissertlinks:\n    urls.append(link.get(\"href\"))\n\nLet’s see our first 5 URLs:\n\nurls[:5]\n\n['https://trace.tennessee.edu/utk_graddiss/10400',\n 'https://trace.tennessee.edu/utk_graddiss/11307',\n 'https://trace.tennessee.edu/utk_graddiss/10081',\n 'https://trace.tennessee.edu/utk_graddiss/10401',\n 'https://trace.tennessee.edu/utk_graddiss/11330']\n\n\n\n\nExtract data from each page\nFor each element of urls (i.e. for each dissertation URL), we can now get our information.\n\n# Create an empty list\nls = []\n\n# For each element of our list of sites\nfor url in urls:\n    # Send a request to the site\n    r = requests.get(url)\n    # Parse the result\n    dissertpage = BeautifulSoup(r.text, \"html.parser\")\n    # Get the date\n    date = dissertpage.select(\"#publication_date p\")[0].text\n    # Get the major\n    major = dissertpage.select(\"#department p\")[0].text\n    # Get the advisor\n    advisor = dissertpage.select(\"#advisor1 p\")[0].text\n    # Store the results in the list\n    ls.append((date, major, advisor))\n    # Add a delay at each iteration\n    time.sleep(0.1)\n\n\nSome sites will block requests if they are too frequent. Adding a little delay between requests is often a good idea.",
    "crumbs": [
      "Python",
      "<em><b>Workshops</b></em>",
      "Web scraping with bs4"
    ]
  },
  {
    "objectID": "python/ws_webscraping.html#store-results-in-dataframe",
    "href": "python/ws_webscraping.html#store-results-in-dataframe",
    "title": "Web scraping with Beautiful Soup",
    "section": "Store results in DataFrame",
    "text": "Store results in DataFrame\nA DataFrame would be a lot more convenient than a list to hold our results.\nFirst, we create a list with the column names for our future DataFrame:\n\ncols = [\"Date\", \"Major\", \"Advisor\"]\n\nThen we create our DataFrame:\n\ndf = pd.DataFrame(ls, columns=cols)\n\n\ndf\n\n\n\n\n\n\n\n\nDate\nMajor\nAdvisor\n\n\n\n\n0\n8-2024\nHistory\nJay Rubenstein\n\n\n1\n12-2024\nBiochemistry and Cellular and Molecular Biology\nDr. Rebecca A. Prosser\n\n\n2\n5-2024\nBusiness Administration\nLarry A. Fauver\n\n\n3\n8-2024\nElectrical Engineering\nDan Wilson\n\n\n4\n12-2024\nMechanical Engineering\nPrashant Singh\n\n\n...\n...\n...\n...\n\n\n95\n5-2024\nMechanical Engineering\nFeng-Yuan Zhang\n\n\n96\n8-2024\nComputer Science\nScott Ruoti\n\n\n97\n8-2024\nMechanical Engineering\nTony L. Schmitz\n\n\n98\n8-2024\nMicrobiology\nShigetoshi Eda\n\n\n99\n8-2024\nEnergy Science and Engineering\nDavid C. Donovan\n\n\n\n\n100 rows × 3 columns",
    "crumbs": [
      "Python",
      "<em><b>Workshops</b></em>",
      "Web scraping with bs4"
    ]
  },
  {
    "objectID": "python/ws_webscraping.html#save-results-to-file",
    "href": "python/ws_webscraping.html#save-results-to-file",
    "title": "Web scraping with Beautiful Soup",
    "section": "Save results to file",
    "text": "Save results to file\nAs a final step, we will save our data to a CSV file:\ndf.to_csv('dissertations_data.csv', index=False)\n\nThe default index=True writes the row numbers. We are not writing these indices in our file by changing the value of this argument to False.\n\nIf you are using a Jupyter notebook or the IPython shell, you can type !ls to see that the file is there and !cat dissertations_data.csv to print its content.\n\n! is a magic command that allows to run Unix shell commands in a notebook or IPython shell.",
    "crumbs": [
      "Python",
      "<em><b>Workshops</b></em>",
      "Web scraping with bs4"
    ]
  },
  {
    "objectID": "python/wb_polars_slides.html#tabular-data",
    "href": "python/wb_polars_slides.html#tabular-data",
    "title": "DataFrames on steroids with Polars",
    "section": "Tabular data",
    "text": "Tabular data\nMany fields of machine learning and data science rely on tabular data where\n\ncolumns hold variables and are homogeneous (same data type)\nrows contain observations and can be heterogeneous\n\nEarly computer options to manipulate such data were limited to spreadsheets\nDataframes (data frames or DataFrames) are two dimensional objects that brought tabular data to programming"
  },
  {
    "objectID": "python/wb_polars_slides.html#early-history-of-dataframes",
    "href": "python/wb_polars_slides.html#early-history-of-dataframes",
    "title": "DataFrames on steroids with Polars",
    "section": "Early history of dataframes",
    "text": "Early history of dataframes\n\n\n\n\n\n\n\n\n\ny1\n1990\n\n\n\ny2\n2000\n\n\n\ny1--y2\n\n\n\n\ny3\n2008\n\n\n\ny2--y3\n\n\n\n\nl1\n\nS programming language\n\n\n\n\n\nl2\n\nR\n\n\n\n\n\n\nl3\n\nPandas (Python)\n\n\n\n\n\n\n\n\n\n\n\nThe world was simple … but slow. Another problem: high memory usage"
  },
  {
    "objectID": "python/wb_polars_slides.html#issues-with-pandas",
    "href": "python/wb_polars_slides.html#issues-with-pandas",
    "title": "DataFrames on steroids with Polars",
    "section": "Issues with Pandas",
    "text": "Issues with Pandas\nWes McKinney (pandas creator) himself has complaints about it:\n\n• Internals too far from “the metal”\n• No support for memory-mapped datasets\n• Poor performance in database and file ingest / export\n• Warty missing data support\n• Lack of transparency into memory use, RAM management\n• Weak support for categorical data\n• Complex groupby operations awkward and slow\n• Appending data to a DataFrame tedious and very costly\n• Limited, non-extensible type metadata\n• Eager evaluation model, no query planning\n• “Slow”, limited multicore algorithms for large datasets"
  },
  {
    "objectID": "python/wb_polars_slides.html#parallel-computing",
    "href": "python/wb_polars_slides.html#parallel-computing",
    "title": "DataFrames on steroids with Polars",
    "section": "Parallel computing",
    "text": "Parallel computing\nPython global interpreter lock (GIL) gets in the way of multi-threading\nLibraries such as Ray, Dask, and Apache Spark allow use of multiple cores and bring dataframes on clusters\nDask and Spark have APIs for Pandas and Modin makes this even more trivial by providing a drop-in replacement for Pandas on Dask, Spark, and Ray\nfugue provides a unified interface for distributed computing that works on Spark, Dask, and Ray"
  },
  {
    "objectID": "python/wb_polars_slides.html#accelerators",
    "href": "python/wb_polars_slides.html#accelerators",
    "title": "DataFrames on steroids with Polars",
    "section": "Accelerators",
    "text": "Accelerators\nRAPIDS brings dataframes on the GPUs with the cuDF library\nIntegration with pandas is easy"
  },
  {
    "objectID": "python/wb_polars_slides.html#lazy-out-of-core",
    "href": "python/wb_polars_slides.html#lazy-out-of-core",
    "title": "DataFrames on steroids with Polars",
    "section": "Lazy out-of-core",
    "text": "Lazy out-of-core\nVaex exists as an alternative to pandas (no integration)"
  },
  {
    "objectID": "python/wb_polars_slides.html#sql",
    "href": "python/wb_polars_slides.html#sql",
    "title": "DataFrames on steroids with Polars",
    "section": "SQL",
    "text": "SQL\nStructured query language (SQL) handles relational databases, but the distinction between SQL and dataframe software is getting increasingly blurry with most libraries now able to handle both\nDuckDB is a very fast and popular option with good integration with pandas\nMany additional options such as dbt and the snowflake snowpark Python API exist, although integration with pandas is not always as good"
  },
  {
    "objectID": "python/wb_polars_slides.html#comparison-with-pandas",
    "href": "python/wb_polars_slides.html#comparison-with-pandas",
    "title": "DataFrames on steroids with Polars",
    "section": "Comparison with Pandas",
    "text": "Comparison with Pandas\n\n\n\n\nPandas\nPolars\n\n\n\n\nAvailable for\nPython\nRust, Python, R, NodeJS\n\n\nWritten in\nCython\nRust\n\n\nMultithreading\nSome operations\nYes (GIL released)\n\n\nIndex\nRows are indexed\nInteger positions are used\n\n\nEvaluation\nEager only\nLazy and eager\n\n\nQuery optimizer\nNo\nYes\n\n\nOut-of-core\nNo\nYes\n\n\nSIMD vectorization\nYes\nYes\n\n\nData in memory\nWith NumPy arrays\nWith Apache Arrow arrays\n\n\nMemory efficiency\nPoor\nExcellent\n\n\nHandling of missing data\nInconsistent\nConsistent, promotes type stability"
  },
  {
    "objectID": "python/wb_polars_slides.html#polars-integration-with-other-tools",
    "href": "python/wb_polars_slides.html#polars-integration-with-other-tools",
    "title": "DataFrames on steroids with Polars",
    "section": "Polars integration with other tools",
    "text": "Polars integration with other tools\nAs good as Pandas’ (except for cuDF, still in development)\n\nWith NumPy: see the documentation, the from_numpy and to_numpy functions, the development progress of this integration, and performance advice\nParallel computing: with Ray thanks to this setting; with Spark, Dask, and Ray thanks to fugue\nGPUs: with the cuDF library from RAPIDS (in development)\nSQL: with DuckDB\n\nThe list is growing fast"
  },
  {
    "objectID": "python/wb_polars_slides.html#benchmarks",
    "href": "python/wb_polars_slides.html#benchmarks",
    "title": "DataFrames on steroids with Polars",
    "section": "Benchmarks",
    "text": "Benchmarks\nComparisons between Polars and distributed (Dask, Ray, Spark) or GPU (RAPIDS) libraries aren’t the most pertinent since they can be used in combination with Polars and the benefits can be combined\nIt makes most sense to compare Polars with another library occupying the same “niche” such as Pandas or Vaex"
  },
  {
    "objectID": "python/wb_polars_slides.html#benchmarks-1",
    "href": "python/wb_polars_slides.html#benchmarks-1",
    "title": "DataFrames on steroids with Polars",
    "section": "Benchmarks",
    "text": "Benchmarks\nThe net is full of benchmarks with consistent results: Polars is 3 to 150 times faster than Pandas\nPandas is trying to fight back: v 2.0 came with optional Arrow support instead of NumPy, then it became the default engine, but performance remains way below that of Polars (e.g. in DataCamp benchmarks, official benchmarks, many blog posts for whole scripts or individual tasks)\nAs for Vaex, it seems twice slower and development has stalled over the past 10 months\nThe only framework performing better than Polars in some benchmarks is datatable (derived from the R package data.table), but it hasn’t been developed for 6 months—a sharp contrast with the fast development of Polars"
  },
  {
    "objectID": "python/wb_polars_slides.html#installation",
    "href": "python/wb_polars_slides.html#installation",
    "title": "DataFrames on steroids with Polars",
    "section": "Installation",
    "text": "Installation\nPersonal computer:\npython -m venv ~/env                  # Create virtual env\nsource ~/env/bin/activate             # Activate virtual env\npip install --upgrade pip             # Update pip\npip install polars                    # Install Polars\n Alliance clusters (polars wheels are available, always prefer wheels when possible):\npython -m venv ~/env                  # Create virtual env\nsource ~/env/bin/activate             # Activate virtual env\npip install --upgrade pip --no-index  # Update pip from wheel\npip install polars --no-index         # Install Polars from wheel"
  },
  {
    "objectID": "python/wb_polars_slides.html#syntax",
    "href": "python/wb_polars_slides.html#syntax",
    "title": "DataFrames on steroids with Polars",
    "section": "Syntax",
    "text": "Syntax\nThe package is well documented\nKevin Heavey wrote Modern Polars following the model of the Modern Pandas book. This is a great resource, although getting a little outdated for the scaling chapter since Polars is evolving so fast\nOverall, the syntax feels somewhat similar to R’s dplyr from the tidyverse"
  },
  {
    "objectID": "python/wb_polars_slides.html#table-visualization",
    "href": "python/wb_polars_slides.html#table-visualization",
    "title": "DataFrames on steroids with Polars",
    "section": "Table visualization",
    "text": "Table visualization\nWhile Pandas comes with internal capabilities to make publication ready tables, Polars integrates very well with great-tables"
  },
  {
    "objectID": "python/wb_polars_slides.html#a-rich-new-field",
    "href": "python/wb_polars_slides.html#a-rich-new-field",
    "title": "DataFrames on steroids with Polars",
    "section": "A rich new field",
    "text": "A rich new field\nAfter years with the one Python option (Pandas), there is currently this exuberant explosion of faster alternatives for dataframes\nIt might seem confusing and overwhelming, but in fact, the picture seems quite simple\nFor now, the new memory standard seems to be Apache Arrow and the most efficient library making use of it is Polars"
  },
  {
    "objectID": "python/wb_polars_slides.html#best-performance-strategy-for-software",
    "href": "python/wb_polars_slides.html#best-performance-strategy-for-software",
    "title": "DataFrames on steroids with Polars",
    "section": "Best performance strategy for software",
    "text": "Best performance strategy for software\nThe best strategy thus seems to be at the moment:\n\nSingle machine: Polars\nCluster: Polars + fugue (example benchmark, documentation of integration)\nGPUs available: Polars + RAPIDS library cuDF (integration coming soon)\nSQL: Polars + DuckDB (documentation of integration)\nOr combination of the above (if cluster with GPUs, etc.)\n\nAs so many libraries are developing an integration with Polars, it is becoming hard to still find reasons to use Pandas"
  },
  {
    "objectID": "python/wb_polars_slides.html#performance-tips",
    "href": "python/wb_polars_slides.html#performance-tips",
    "title": "DataFrames on steroids with Polars",
    "section": "Performance tips",
    "text": "Performance tips\n\nRead the migration guide: it will help you write Polars code rather than “literally translated” Pandas code that runs, but doesn’t make use of Polars’ strengths. The differences in style mostly come from the fact that Polars runs in parallel\nExecution: lazy where possible\nFile format: Apache Parquet"
  },
  {
    "objectID": "python/wb_polars_slides.html#course-on-polars-coming-this-fall",
    "href": "python/wb_polars_slides.html#course-on-polars-coming-this-fall",
    "title": "DataFrames on steroids with Polars",
    "section": "Course on Polars coming this fall",
    "text": "Course on Polars coming this fall\nIn fall 2024, I plan to offer an introductory course on Polars covering:\n\nbasic syntax\nhow to use Polars in a Ray cluster on Alliance supercomputers thanks to fugue\nhow to run Polars on GPU thanks to cuDF if the project is available by then"
  },
  {
    "objectID": "python/wb_jax_slides.html#what-is-jax",
    "href": "python/wb_jax_slides.html#what-is-jax",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "What is JAX?",
    "text": "What is JAX?\n\nLibrary for Python developed by Google\n\n\nKey data structure: Array\n\n\nComposition, transformation, and differentiation of numerical programs\n\n\nCompilation for CPUs, GPUs, and TPUs\n\n\nNumPy-like and lower-level APIs\n\n\nRequires strict functional programming"
  },
  {
    "objectID": "python/wb_jax_slides.html#why-jax",
    "href": "python/wb_jax_slides.html#why-jax",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "Why JAX?",
    "text": "Why JAX?\n\n\n\n\n\n\n\n\n\n01\n\n\nAutodiff method\n\n\n\n1\nStatic graph\nand XLA\n\n\n\n\n02\n\n\nFramework\n\n\n\n\n2\nDynamic graph\n\n\n\n1-&gt;2\n\n\n\n\n\na\n\nTensorFlow\n\n\n\n\n4\nDynamic graph\nand XLA\n\n\n\n2-&gt;4\n\n\n\n\n\nb\n\nPyTorch\n\n\n\n\n5\nPseudo-dynamic\nand XLA\n\n\n\n4-&gt;5\n\n\n\n\n\nd\n\nTensorFlow2\n\n\n\n\ne\n\nJAX\n\n\n\n\n\n03\n\n\nAdvantage\n\n\n\n\n\n7\nMostly\noptimized AD\n\n\n\n\n\n8\nConvenient\n\n\n\n\n\n9\nConvenient\n\n\n\n\n10\nConvenient and\nmostly optimized AD\n\n\n\n\n\n04\n\n\nDisadvantage\n\n\n\n\n\nA\nManual writing of IR\n\n\n\n\n\nB\nLimited AD optimization\n\n\n\n\n\nD\nDisappointing speed\n\n\n\n\nE\nPure functions\n\n\n\n\n\n\n\n\n\n\n\n\n\n  Summarized from a blog post by Chris Rackauckas"
  },
  {
    "objectID": "python/wb_jax_slides.html#installation",
    "href": "python/wb_jax_slides.html#installation",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "Installation",
    "text": "Installation\n Install from pip wheels:\n\nPersonal computer: use wheels installation commands from official site\nAlliance clusters: python -m pip install jax --no-index \n\n\nWindows: GPU support only via WSL"
  },
  {
    "objectID": "python/wb_jax_slides.html#the-numpy-api",
    "href": "python/wb_jax_slides.html#the-numpy-api",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "The NumPy API",
    "text": "The NumPy API\n\nNumPyJAX NumPy\n\n\n\nimport numpy as np\n\nprint(np.array([(1, 2, 3), (4, 5, 6)]))\n\n[[1 2 3]\n [4 5 6]]\n\n\n\nprint(np.arange(5))\n\n[0 1 2 3 4]\n\n\n\nprint(np.zeros(2))\n\n[0. 0.]\n\n\n\nprint(np.linspace(0, 2, 9))\n\n[0.   0.25 0.5  0.75 1.   1.25 1.5  1.75 2.  ]\n\n\n\n\nimport jax.numpy as jnp\n\nprint(jnp.array([(1, 2, 3), (4, 5, 6)]))\n[[1 2 3]\n [4 5 6]]\nprint(jnp.arange(5))\n[0 1 2 3 4]\nprint(jnp.zeros(2))\n[0. 0.]\nprint(jnp.linspace(0, 2, 9))\n[0.   0.25 0.5  0.75 1.   1.25 1.5  1.75 2.  ]"
  },
  {
    "objectID": "python/wb_jax_slides.html#different-types",
    "href": "python/wb_jax_slides.html#different-types",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "Different types",
    "text": "Different types\n\nNumpyJAX NumPy\n\n\n\ntype(np.zeros((2, 3)))\n\nnumpy.ndarray\n\n\n\n\ntype(jnp.zeros((2, 3)))\njaxlib.xla_extension.ArrayImpl"
  },
  {
    "objectID": "python/wb_jax_slides.html#different-default-data-types",
    "href": "python/wb_jax_slides.html#different-default-data-types",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "Different default data types",
    "text": "Different default data types\n\nNumpyJAX NumPy\n\n\n\nnp.zeros((2, 3)).dtype\n\ndtype('float64')\n\n\n\n\njnp.zeros((2, 3)).dtype\ndtype('float32')\n\nStandard for DL and libraries built for accelerators\nFloat64 are very slow on GPUs and not supported on TPUs"
  },
  {
    "objectID": "python/wb_jax_slides.html#immutable-arrays",
    "href": "python/wb_jax_slides.html#immutable-arrays",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "Immutable arrays",
    "text": "Immutable arrays\n\nNumpyJAX NumPy\n\n\n\na = np.arange(5)\na[0] = 9\nprint(a)\n\n[9 1 2 3 4]\n\n\n\n\na = jnp.arange(5)\na[0] = 9\nTypeError: '&lt;class 'jaxlib.xla_extension.ArrayImpl'&gt;' object does not support item assignment. JAX arrays are immutable.\nb = a.at[0].set(9)\nprint(b)\n[9 1 2 3 4]"
  },
  {
    "objectID": "python/wb_jax_slides.html#strict-input-control",
    "href": "python/wb_jax_slides.html#strict-input-control",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "Strict input control",
    "text": "Strict input control\n\nNumpyJAX NumPy\n\n\nNumPy is easy-going:\n\nnp.sum([1.0, 2.0])  # argument is a list\n\nnp.float64(3.0)\n\n\n\nnp.sum((1.0, 2.0))  # argument is a tuple\n\nnp.float64(3.0)\n\n\n\n\nTo avoid inefficiencies, JAX will only accept arrays:\njnp.sum([1.0, 2.0])\nTypeError: sum requires ndarray or scalar arguments, got &lt;class 'list'&gt;\njnp.sum((1.0, 2.0))\nTypeError: sum requires ndarray or scalar arguments, got &lt;class 'tuple'&gt;"
  },
  {
    "objectID": "python/wb_jax_slides.html#out-of-bounds-indexing",
    "href": "python/wb_jax_slides.html#out-of-bounds-indexing",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "Out of bounds indexing",
    "text": "Out of bounds indexing\n\nNumpyJAX NumPy\n\n\nNumPy will error if you index out of bounds:\n\nprint(np.arange(5)[10])\n\n\n---------------------------------------------------------------------------\nIndexError                                Traceback (most recent call last)\nCell In[10], line 1\n----&gt; 1 print(np.arange(5)[10])\n\nIndexError: index 10 is out of bounds for axis 0 with size 5\n\n\n\n\n\nJAX will silently return the closest boundary:\nprint(jnp.arange(5)[10])\n4"
  },
  {
    "objectID": "python/wb_jax_slides.html#prng",
    "href": "python/wb_jax_slides.html#prng",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "PRNG",
    "text": "PRNG\nTraditional pseudorandom number generators are based on nondeterministic state of OS\nSlow and problematic for parallel executions\nJAX relies on explicitly-set random state called a key:\nfrom jax import random\n\ninitial_key = random.PRNGKey(18)\nprint(initial_key)\n[ 0 18]"
  },
  {
    "objectID": "python/wb_jax_slides.html#prng-1",
    "href": "python/wb_jax_slides.html#prng-1",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "PRNG",
    "text": "PRNG\nEach key can only be used for one random function, but it can be split into new keys:\nnew_key1, new_key2 = random.split(initial_key)\n\ninitial_key can’t be used anymore now\n\nprint(new_key1)\n[4197003906 1654466292]\nprint(new_key2)\n[1685972163 1654824463]\nWe need to keep one key to split whenever we need and we can use the other one"
  },
  {
    "objectID": "python/wb_jax_slides.html#prng-2",
    "href": "python/wb_jax_slides.html#prng-2",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "PRNG",
    "text": "PRNG\nTo make sure we don’t reuse a key by accident, it is best to overwrite the initial key with one of the new ones\nHere are easier names:\nkey = random.PRNGKey(18)\nkey, subkey = random.split(key)\nWe can now use subkey to generate a random array:\nx = random.normal(subkey, (3, 2))"
  },
  {
    "objectID": "python/wb_jax_slides.html#benchmarking",
    "href": "python/wb_jax_slides.html#benchmarking",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "Benchmarking",
    "text": "Benchmarking\nJAX uses asynchronous dispatch\nInstead of waiting for a computation to complete before control returns to Python, the computation is dispatched to an accelerator and a future is created\nTo get proper timings, we need to make sure the future is resolved by using the block_until_ready() method"
  },
  {
    "objectID": "python/wb_jax_slides.html#jit-syntax",
    "href": "python/wb_jax_slides.html#jit-syntax",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "JIT syntax",
    "text": "JIT syntax\nfrom jax import jit\n\nkey = random.PRNGKey(8)\nkey, subkey1, subkey2 = random.split(key, 3)\n\na = random.normal(subkey1, (500, 500))\nb = random.normal(subkey2, (500, 500))\n\ndef sum_squared_error(a, b):\n    return jnp.sum((a-b)**2)\nOur function could simply be used as:\nsse = sum_squared_error(a, b)"
  },
  {
    "objectID": "python/wb_jax_slides.html#jit-syntax-1",
    "href": "python/wb_jax_slides.html#jit-syntax-1",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "JIT syntax",
    "text": "JIT syntax\nOur code will run faster if we create a JIT compiled version and use that instead:\nsum_squared_error_jit = jit(sum_squared_error)\n\nsse = sum_squared_error_jit(a, b)\nAlternatively, this can be written as:\nsse = jit(sum_squared_error)(a, b)\nOr with the @jit decorator:\n@jit\ndef sum_squared_error(a, b):\n    return jnp.sum((a - b) ** 2)\n\nsse = sum_squared_error(a, b)"
  },
  {
    "objectID": "python/wb_jax_slides.html#static-vs-traced-variables",
    "href": "python/wb_jax_slides.html#static-vs-traced-variables",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "Static vs traced variables",
    "text": "Static vs traced variables\n@jit\ndef cond_func(x):\n    if x &lt; 0.0:\n        return x ** 2.0\n    else:\n        return x ** 3.0\n\nprint(cond_func(1.0))\njax.errors.TracerBoolConversionError: Attempted boolean conversion of traced array with shape bool[]\nJIT compilation uses tracing of the code based on shape and dtype so that the same compiled code can be reused for new values with the same characteristics\nTracer objects are not real values but abstract representation that are more general\nHere, an abstract general value does not work as it wouldn’t know which branch to take"
  },
  {
    "objectID": "python/wb_jax_slides.html#static-vs-traced-variables-1",
    "href": "python/wb_jax_slides.html#static-vs-traced-variables-1",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "Static vs traced variables",
    "text": "Static vs traced variables\nOne solution is to tell jit() to exclude the problematic arguments from tracing\nwith arguments positions:\ndef cond_func(x):\n    if x &lt; 0.0:\n        return x ** 2.0\n    else:\n        return x ** 3.0\n\ncond_func_jit = jit(cond_func, static_argnums=(0,))\n\nprint(cond_func_jit(2.0))\nprint(cond_func_jit(-2.0))\n8.0\n4.0"
  },
  {
    "objectID": "python/wb_jax_slides.html#static-vs-traced-variables-2",
    "href": "python/wb_jax_slides.html#static-vs-traced-variables-2",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "Static vs traced variables",
    "text": "Static vs traced variables\nOne solution is to tell jit() to exclude the problematic arguments from tracing\nwith arguments names:\ndef cond_func(x):\n    if x &lt; 0.0:\n        return x ** 2.0\n    else:\n        return x ** 3.0\n\ncond_func_jit_alt = jit(cond_func, static_argnames=\"x\")\n\nprint(cond_func_jit_alt(2.0))\nprint(cond_func_jit_alt(-2.0))\n8.0\n4.0"
  },
  {
    "objectID": "python/wb_jax_slides.html#control-flow-primitives",
    "href": "python/wb_jax_slides.html#control-flow-primitives",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "Control flow primitives",
    "text": "Control flow primitives\nAnother solution, is to use one of the structured control flow primitives:\nfrom jax import lax\n\nlax.cond(False, lambda x: x ** 2.0, lambda x: x ** 3.0, jnp.array([2.]))\nArray([8.], dtype=float32)\nlax.cond(True, lambda x: x ** 2.0, lambda x: x ** 3.0, jnp.array([-2.]))\nArray([4.], dtype=float32)"
  },
  {
    "objectID": "python/wb_jax_slides.html#control-flow-primitives-1",
    "href": "python/wb_jax_slides.html#control-flow-primitives-1",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "Control flow primitives",
    "text": "Control flow primitives\nOther control flow primitives:\n\nlax.while_loop\nlax.fori_loop\nlax.scan\n\nOther pseudo dynamic control flow functions:\n\nlax.select (NumPy API jnp.where and jnp.select)\nlax.switch (NumPy API jnp.piecewise)"
  },
  {
    "objectID": "python/wb_jax_slides.html#static-vs-traced-operations",
    "href": "python/wb_jax_slides.html#static-vs-traced-operations",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "Static vs traced operations",
    "text": "Static vs traced operations\nSimilarly, you can mark problematic operations as static so that they don’t get traced during JIT compilation:\n@jit\ndef f(x):\n    return x.reshape(jnp.array(x.shape).prod())\n\nx = jnp.ones((2, 3))\nprint(f(x))\nTypeError: Shapes must be 1D sequences of concrete values of integer type, got [Traced&lt;ShapedArray(int32[])&gt;with&lt;DynamicJaxprTrace(level=1/0)&gt;]"
  },
  {
    "objectID": "python/wb_jax_slides.html#static-vs-traced-operations-1",
    "href": "python/wb_jax_slides.html#static-vs-traced-operations-1",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "Static vs traced operations",
    "text": "Static vs traced operations\nThe problem here is that the shape of the argument to prod() depends on the value of x which is unknown at compilation time\nOne solution is to use the NumPy version of prod():\nimport numpy as np\n\n@jit\ndef f(x):\n    return x.reshape((np.prod(x.shape)))\n\nprint(f(x))\n[1. 1. 1. 1. 1. 1.]"
  },
  {
    "objectID": "python/wb_jax_slides.html#jaxprs",
    "href": "python/wb_jax_slides.html#jaxprs",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "Jaxprs",
    "text": "Jaxprs\nimport jax\n\nx = jnp.array([1., 4., 3.])\ny = jnp.array([8., 1., 2.])\n\ndef f(x, y):\n    return 2 * x**2 + y\n\njax.make_jaxpr(f)(x, y) \n{ lambda ; a:f32[3] b:f32[3]. let\n    c:f32[3] = integer_pow[y=2] a\n    d:f32[3] = mul 2.0 c\n    e:f32[3] = add d b\n  in (e,) }"
  },
  {
    "objectID": "python/wb_jax_slides.html#outputs-only-based-on-inputs",
    "href": "python/wb_jax_slides.html#outputs-only-based-on-inputs",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "Outputs only based on inputs",
    "text": "Outputs only based on inputs\ndef f(x):\n    return a + x\nf uses the variable a from the global environment\nThe output does not solely depend on the inputs: not a pure function"
  },
  {
    "objectID": "python/wb_jax_slides.html#outputs-only-based-on-inputs-1",
    "href": "python/wb_jax_slides.html#outputs-only-based-on-inputs-1",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "Outputs only based on inputs",
    "text": "Outputs only based on inputs\na = jnp.ones(3)\nprint(a)\n[1. 1. 1.]\ndef f(x):\n    return a + x\n\nprint(jit(f)(jnp.ones(3)))\n[2. 2. 2.]\n\nThings seem ok here because this is the first run (tracing)"
  },
  {
    "objectID": "python/wb_jax_slides.html#outputs-only-based-on-inputs-2",
    "href": "python/wb_jax_slides.html#outputs-only-based-on-inputs-2",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "Outputs only based on inputs",
    "text": "Outputs only based on inputs\nNow, let’s change the value of a to an array of zeros:\na = jnp.zeros(3)\nprint(a)\n[0. 0. 0.]\nAnd rerun the same code:\nprint(jit(f)(jnp.ones(3)))\n[2. 2. 2.]\n\nOur cached compiled program is run and we get a wrong result"
  },
  {
    "objectID": "python/wb_jax_slides.html#outputs-only-based-on-inputs-3",
    "href": "python/wb_jax_slides.html#outputs-only-based-on-inputs-3",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "Outputs only based on inputs",
    "text": "Outputs only based on inputs\nThe new value for a will only take effect if we re-trigger tracing by changing the shape and/or dtype of x:\na = jnp.zeros(4)\nprint(a)\n[0. 0. 0. 0.]\nprint(jit(f)(jnp.ones(4)))\n[1. 1. 1. 1.]\nPassing to f() an argument of a different shape forced retracing"
  },
  {
    "objectID": "python/wb_jax_slides.html#no-side-effects",
    "href": "python/wb_jax_slides.html#no-side-effects",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "No side effects",
    "text": "No side effects\nSide effects: anything beside returned output\nExamples:\n\nPrinting to standard output\nReading from file/writing to file\nModifying a global variable"
  },
  {
    "objectID": "python/wb_jax_slides.html#no-side-effects-1",
    "href": "python/wb_jax_slides.html#no-side-effects-1",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "No side effects",
    "text": "No side effects\nThe side effects will happen during tracing, but not on subsequent runs. You cannot rely on side effects in your code\ndef f(a, b):\n    print(\"Calculating sum\")\n    return a + b\n\nprint(jit(f)(jnp.arange(3), jnp.arange(3)))\nCalculating sum\n[0 2 4]\n\nPrinting happened here because this is the first run"
  },
  {
    "objectID": "python/wb_jax_slides.html#no-side-effects-2",
    "href": "python/wb_jax_slides.html#no-side-effects-2",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "No side effects",
    "text": "No side effects\nLet’s rerun the function:\nprint(jit(f)(jnp.arange(3), jnp.arange(3)))\n[0 2 4]\nThis time, no printing"
  },
  {
    "objectID": "python/wb_jax_slides.html#automatic-differentiation",
    "href": "python/wb_jax_slides.html#automatic-differentiation",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "Automatic differentiation",
    "text": "Automatic differentiation\nConsidering the function f:\nf = lambda x: x**3 + 2*x**2 - 3*x + 8\nWe can create a new function dfdx that computes the gradient of f w.r.t. x:\nfrom jax import grad\n\ndfdx = grad(f)\ndfdx returns the derivatives\nprint(dfdx(1.))\n4.0"
  },
  {
    "objectID": "python/wb_jax_slides.html#composing-transformations",
    "href": "python/wb_jax_slides.html#composing-transformations",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "Composing transformations",
    "text": "Composing transformations\nTransformations can be composed:\nprint(jit(grad(f))(1.))\n4.0\nprint(grad(jit(f))(1.))\n4.0"
  },
  {
    "objectID": "python/wb_jax_slides.html#forward-and-reverse-modes",
    "href": "python/wb_jax_slides.html#forward-and-reverse-modes",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "Forward and reverse modes",
    "text": "Forward and reverse modes\nOther autodiff methods:\n\nReverse-mode vector-Jacobian products: jax.vjp\nForward-mode Jacobian-vector products: jax.jvp"
  },
  {
    "objectID": "python/wb_jax_slides.html#higher-order-differentiation",
    "href": "python/wb_jax_slides.html#higher-order-differentiation",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "Higher-order differentiation",
    "text": "Higher-order differentiation\n With a single variable, the grad function calls can be nested:\nd2fdx = grad(dfdx)   # function to compute 2nd order derivatives\nd3fdx = grad(d2fdx)  # function to compute 3rd order derivatives\n...\n With several variables:\n\njax.jacfwd for forward-mode\njax.jacrev for reverse-mode"
  },
  {
    "objectID": "python/wb_jax_slides.html#pytrees",
    "href": "python/wb_jax_slides.html#pytrees",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "Pytrees",
    "text": "Pytrees\nJAX has a nested container structure: pytree extremely useful for DNN"
  },
  {
    "objectID": "python/wb_jax_slides.html#vectorization-and-parallelization",
    "href": "python/wb_jax_slides.html#vectorization-and-parallelization",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "Vectorization and parallelization",
    "text": "Vectorization and parallelization\nOther transformations for parallel run of computations across batches of arrays:\n\nAutomatic vectorization with jax.vmap\nParallelization across devices with jax.pmap"
  },
  {
    "objectID": "python/wb_jax_slides.html#lax-api",
    "href": "python/wb_jax_slides.html#lax-api",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "Lax API",
    "text": "Lax API\njax.numpy is a high-level NumPy-like API wrapped around jax.lax\njax.lax is a more efficient lower-level API itself wrapped around XLA"
  },
  {
    "objectID": "python/wb_jax_slides.html#pallas-extension-to-write-gpu-and-tpu-kernels",
    "href": "python/wb_jax_slides.html#pallas-extension-to-write-gpu-and-tpu-kernels",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "Pallas: extension to write GPU and TPU kernels",
    "text": "Pallas: extension to write GPU and TPU kernels\n\n\n\n\n\n\n\n\n\ntracer\n\nTracing\n\n\n\njaxpr\n\nJaxprs\n(JAX expressions)\nintermediate\nrepresentation\n(IR)\n\n\n\ntracer-&gt;jaxpr\n\n\n\n\n\njit\n\n Just-in-time \n(JIT)\ncompilation\n\n\n\nhlo\n\nHigh-level\noptimized (HLO)\nprogram\n\n\n\njit-&gt;hlo\n\n\n\n\n\ntriton\n\nTriton\n\n\n\nGPU\n\nGPU\n\n\n\ntriton-&gt;GPU\n\n\n\n\n\nmosaic\n\nMosaic\n\n\n\nTPU\n\nTPU\n\n\n\nmosaic-&gt;TPU\n\n\n\n\n\ntransform\n\nVectorization\nParallelization\n   Differentiation  \n\n\n\npy\n\nPure Python\nfunctions\n\n\n\npy-&gt;tracer\n\n\n\n\njaxpr-&gt;jit\n\n\n\n\njaxpr-&gt;transform\n\n\n\n\n\n\nhlo-&gt;triton\n\n\n\n\nhlo-&gt;mosaic"
  },
  {
    "objectID": "python/wb_hss_prog_slides.html#computer-programming",
    "href": "python/wb_hss_prog_slides.html#computer-programming",
    "title": "Introduction to programming for the humanities",
    "section": "Computer programming",
    "text": "Computer programming\nProgramming (or coding) consists of writing a set of instructions (a program) for computers so that they perform a task\nThere are many programming languages—each with its own syntax—but the core concepts apply to all languages. For this course, we will use Python as an example\nPrograms accept inputs (data) and produce outputs (transformed data)"
  },
  {
    "objectID": "python/wb_hss_prog_slides.html#things-to-consider",
    "href": "python/wb_hss_prog_slides.html#things-to-consider",
    "title": "Introduction to programming for the humanities",
    "section": "Things to consider",
    "text": "Things to consider\n\nFree and open source software (FOSS) vs proprietary\nCompiled vs interpreted language (speed vs convenience)\nLanguage adapted to particular usage\nLanguage used in your field (colleagues, collaborators, literature)"
  },
  {
    "objectID": "python/wb_hss_prog_slides.html#the-problem-with-proprietary-software",
    "href": "python/wb_hss_prog_slides.html#the-problem-with-proprietary-software",
    "title": "Introduction to programming for the humanities",
    "section": "The problem with proprietary software",
    "text": "The problem with proprietary software\n\nResearchers who do not have access to the tool cannot reproduce your methods\nOnce you leave academia, you may not have access to the tool anymore\nYour university may stop paying for a license\nYou may get locked-in\nProprietary tools are black boxes\nLong-term access is uncertain\nProprietary tools fall behind popular open-source tools\nProprietary tools often fail to address specialized edge cases needed in research"
  },
  {
    "objectID": "python/wb_hss_prog_slides.html#the-argument-for-foss",
    "href": "python/wb_hss_prog_slides.html#the-argument-for-foss",
    "title": "Introduction to programming for the humanities",
    "section": "The argument for FOSS",
    "text": "The argument for FOSS\n\nEqual access to everyone, including poorer countries or organizations (it’s free!)\nOpen science\nTransparency\nThe whole community can contribute to and have a say about development\nYou an build specific capabilities for your edge cases\nGuarantied long term access\nNo risk of getting locked-in"
  },
  {
    "objectID": "python/wb_hss_prog_slides.html#compiled-languages",
    "href": "python/wb_hss_prog_slides.html#compiled-languages",
    "title": "Introduction to programming for the humanities",
    "section": "Compiled languages",
    "text": "Compiled languages\nYou write code, compile it into machine code, then use this to process your data\n\nCompiled languages are fast. The two step process however makes prototyping less practical and these languages are hard to learn and debug\n\nExamples of compiled languages include C, C++, Fortran, Go, Haskell"
  },
  {
    "objectID": "python/wb_hss_prog_slides.html#interpreted-languages",
    "href": "python/wb_hss_prog_slides.html#interpreted-languages",
    "title": "Introduction to programming for the humanities",
    "section": "Interpreted languages",
    "text": "Interpreted languages\nInterpreted languages are executed directly\n\nYou get direct feed-back, making it easier to prototype. Interpreted languages are easy to learn and debug, but they are much slower\n\nExamples of interpreted languages include R, Python, Perl, and JavaScript"
  },
  {
    "objectID": "python/wb_hss_prog_slides.html#python",
    "href": "python/wb_hss_prog_slides.html#python",
    "title": "Introduction to programming for the humanities",
    "section": "Python",
    "text": "Python\nPython is free and open-source, interpreted, and general-purpose\nIt was created by Dutch programmer Guido van Rossum in the 80s, with a launch in 1989\nThe PYPL PopularitY of Programming Language index is based on the number of tutorial searches in Google. Python has been going up steadily, reaching the first position in 2018. It is also ahead in other indices and is the language used by most of the deep learning community\nThis doesn’t mean that Python is better than other languages, but it means that there are a lot of resources and a large collection of external packages"
  },
  {
    "objectID": "python/wb_hss_prog_slides.html#text-editor-to-write-scripts",
    "href": "python/wb_hss_prog_slides.html#text-editor-to-write-scripts",
    "title": "Introduction to programming for the humanities",
    "section": "Text editor to write scripts",
    "text": "Text editor to write scripts\nA text editor is not the same as a word processor such as Microsoft Office Word. Word documents are not plain text documents: they contain a lot of hidden formatting and are actually a collection of files. This is not what you want to write scripts\nExamples of good text editors (free and open source):\n\nEmacs\nVisual Studio Code\nVim"
  },
  {
    "objectID": "python/wb_hss_prog_slides.html#optional-an-ide",
    "href": "python/wb_hss_prog_slides.html#optional-an-ide",
    "title": "Introduction to programming for the humanities",
    "section": "Optional: an IDE",
    "text": "Optional: an IDE\nIDE (integrated development environments) are software that make running a language more friendly by adding functionality and convenience tools, usually within a graphical user interface (GUI)"
  },
  {
    "objectID": "python/wb_hss_prog_slides.html#debugging-and-profiling-tools",
    "href": "python/wb_hss_prog_slides.html#debugging-and-profiling-tools",
    "title": "Introduction to programming for the humanities",
    "section": "Debugging and profiling tools",
    "text": "Debugging and profiling tools\nSome languages come with debugging tools that make it easier to find problems in the code\nProfilers allow you to spot bottlenecks in the execution of your code\nBenchmarking tools allow you to compare several versions of code to find which is faster"
  },
  {
    "objectID": "python/wb_hss_prog_slides.html#hardware",
    "href": "python/wb_hss_prog_slides.html#hardware",
    "title": "Introduction to programming for the humanities",
    "section": "Hardware",
    "text": "Hardware\nPython is great in many respects, but it is not a fast language\nMany libraries for Python are written in faster compiled languages (e.g. C, C++, Fortran)\nTo speed things up more, some code or sections of code can be run in parallel (instead of serially). To do this though, you need more hardware\nYou can run code using multiple CPUs (central processing unit). Some code can be accelerated using GPUs (graphical processing unit)\nFor very large scale projects such as very large simulations, deep learning, or big data projects, you can use supercomputers"
  },
  {
    "objectID": "python/wb_hss_prog_slides.html#python-shell",
    "href": "python/wb_hss_prog_slides.html#python-shell",
    "title": "Introduction to programming for the humanities",
    "section": "Python shell",
    "text": "Python shell\nThe simplest way to use Python is to type commands directly in the Python shell. This sends commands directly to the interpreter\nThe Python shell has a prompt that looks like this:\n&gt;&gt;&gt;"
  },
  {
    "objectID": "python/wb_hss_prog_slides.html#ipython",
    "href": "python/wb_hss_prog_slides.html#ipython",
    "title": "Introduction to programming for the humanities",
    "section": "IPython",
    "text": "IPython\nIPython is an improved shell with better performance and more functionality (e.g. colour-coding, magic commands)\nThe prompt looks like:\nIn [x]:\n\nx is the command number (e.g. for your first command, it will show In [1]:"
  },
  {
    "objectID": "python/wb_hss_prog_slides.html#jupyter",
    "href": "python/wb_hss_prog_slides.html#jupyter",
    "title": "Introduction to programming for the humanities",
    "section": "Jupyter",
    "text": "Jupyter\nThe IPython shell was integrated into a fancy interface, the Jupyter notebook. This later lead to a fully fledged IDE (integrated development environment) called JupyterLab which contains notebooks, a command line, a file explorer, and other functionality\n\nEven though JupyterLab runs in your browser, it does not use the internet: it is all run locally on your machine (browsers are software that are great at displaying HTML files, so we use them to access the web, but they can also display files from your computer)"
  },
  {
    "objectID": "python/wb_hss_prog_slides.html#other-ides",
    "href": "python/wb_hss_prog_slides.html#other-ides",
    "title": "Introduction to programming for the humanities",
    "section": "Other IDEs",
    "text": "Other IDEs\nJupyter has probably become the most popular IDE, but it is possible to run Python in other IDE such as Emacs"
  },
  {
    "objectID": "python/wb_hss_prog_slides.html#python-script",
    "href": "python/wb_hss_prog_slides.html#python-script",
    "title": "Introduction to programming for the humanities",
    "section": "Python script",
    "text": "Python script\nYou can write your Python code in a text file with a .py extension and run the script in your terminal with:\npython script.py\nThis will execute the code non-interactively"
  },
  {
    "objectID": "python/wb_hss_prog_slides.html#packages",
    "href": "python/wb_hss_prog_slides.html#packages",
    "title": "Introduction to programming for the humanities",
    "section": "Packages",
    "text": "Packages\nMany languages can have their functionality expanded by the installation of packages developed by the open source community. The potential is unlimited\nMany languages come with their own package manager\nIn Python, the package manager is called pip"
  },
  {
    "objectID": "python/wb_hss_prog_slides.html#syntax",
    "href": "python/wb_hss_prog_slides.html#syntax",
    "title": "Introduction to programming for the humanities",
    "section": "Syntax",
    "text": "Syntax\nEach language uses its own syntax\n\nExamples:\n\nin Python, the tab (equal to four spaces by default) has meaning, while in R, it doesn’t (it only makes it easier for people to read code)"
  },
  {
    "objectID": "python/wb_hss_prog_slides.html#data-types",
    "href": "python/wb_hss_prog_slides.html#data-types",
    "title": "Introduction to programming for the humanities",
    "section": "Data types",
    "text": "Data types\nEach language contains various data types such as integers, floating-point numbers (decimals), strings (series of characters), Booleans (true/false), etc.\n\nPython examples:\n\n\ntype(5)\n\nint\n\n\n\ntype(5.0)\n\nfloat\n\n\n\ntype(\"This is a string\")\n\nstr\n\n\n\ntype(True)\n\nbool"
  },
  {
    "objectID": "python/wb_hss_prog_slides.html#variables",
    "href": "python/wb_hss_prog_slides.html#variables",
    "title": "Introduction to programming for the humanities",
    "section": "Variables",
    "text": "Variables\nValues can be assigned to names to create variables\n\nPython example\n\n\na = 3\n\na is now a variable containing the value 3:\n\nprint(a)\n\n3\n\n\n\na * 2\n\n6"
  },
  {
    "objectID": "python/wb_hss_prog_slides.html#data-structures",
    "href": "python/wb_hss_prog_slides.html#data-structures",
    "title": "Introduction to programming for the humanities",
    "section": "Data structures",
    "text": "Data structures\nA data structure is a collection of values\n\nPython examples:\n\n\ntype([0, 5, \"something\"])\n\nlist\n\n\n\ntype((3, 5, \"something\"))\n\ntuple\n\n\n\ntype({0, 2, 6})\n\nset\n\n\nEach type of structure has its own characteristics (necessarily homogeneous or not, mutable or not, ordered or not, etc.). This gives several data storage options, each best in different situations"
  },
  {
    "objectID": "python/wb_hss_prog_slides.html#functions",
    "href": "python/wb_hss_prog_slides.html#functions",
    "title": "Introduction to programming for the humanities",
    "section": "Functions",
    "text": "Functions\nFunctions are snippets of code that accomplish a specific task\nBuilt-in functions come with the language and are readily available. Other functions become available once a particular module or package is loaded. Finally, the user can definite their own functions\nSome functions take arguments\n\nPython examples:\n\n\nmax([3, 5, 2])\n\n5\n\n\n\ndef hello():\n    print(\"Hello everyone!\")\n\nhello()\n\nHello everyone!"
  },
  {
    "objectID": "python/wb_hss_prog_slides.html#control-flow",
    "href": "python/wb_hss_prog_slides.html#control-flow",
    "title": "Introduction to programming for the humanities",
    "section": "Control flow",
    "text": "Control flow\nCommands are normally run sequentially, from top to bottom, but it is possible to alter the flow of execution by creating repeats (loops) or conditional executions\n\nPython examples:\n\n\nfor i in range(3):\n    print(i)\n\n0\n1\n2\n\n\n\nx = -3\n\nif x &gt; 0:\n    print(x + 2)\nelse:\n    print(x * 3)\n\n-9"
  },
  {
    "objectID": "python/wb_hss_prog_slides.html#internal-documentation",
    "href": "python/wb_hss_prog_slides.html#internal-documentation",
    "title": "Introduction to programming for the humanities",
    "section": "Internal documentation",
    "text": "Internal documentation\nMost languages come with their internal documentation\n\nExample with Python:\n\nhelp(sum)\nHelp on built-in function sum in module builtins:\n\nsum(iterable, /, start=0)\n    Return the sum of a 'start' value (default: 0) plus an iterable of numbers\n\n    When the iterable is empty, return the start value.\n    This function is intended specifically for use with numeric values and may\n    reject non-numeric types."
  },
  {
    "objectID": "python/wb_hss_prog_slides.html#the-internet",
    "href": "python/wb_hss_prog_slides.html#the-internet",
    "title": "Introduction to programming for the humanities",
    "section": "The internet",
    "text": "The internet\nGoogle is often your best bet, but you need to know the vocabulary in order to ask specific questions\nStack Overflow"
  },
  {
    "objectID": "python/top_ws.html",
    "href": "python/top_ws.html",
    "title": "Python workshops",
    "section": "",
    "text": "Web scraping with  \n\n\n\n\nDataFrames with",
    "crumbs": [
      "Python",
      "<em><b>Workshops</b></em>"
    ]
  },
  {
    "objectID": "python/top_nlp.html",
    "href": "python/top_nlp.html",
    "title": "Text analysis",
    "section": "",
    "text": "This course is a gentle introduction to rule-based text analysis for the humanities and social sciences using the Python library TextBlob.\nIt only requires a very basic knowledge of Python.\n\n Start course ➤",
    "crumbs": [
      "Python",
      "<b><em>Text analysis</em></b>"
    ]
  },
  {
    "objectID": "python/top_hpc.html",
    "href": "python/top_hpc.html",
    "title": "Faster Python",
    "section": "",
    "text": "Accelerated arrays with \n\n\n\n\nFaster data frames with",
    "crumbs": [
      "Python",
      "<b><em>Faster Python</em></b>"
    ]
  },
  {
    "objectID": "python/polars_subset.html",
    "href": "python/polars_subset.html",
    "title": "Subsetting data",
    "section": "",
    "text": "The syntax to subset data is very different in Polars compared to the indexing of pandas and other languages. Action verbs are used in a style very similar to that of R’s dplyr from the tidyverse.\nLet’s start with the same data frame we used in the previous section:\nimport polars as pl\n\ndf = pl.read_csv(\"https://raw.githubusercontent.com/razoumov/publish/master/jeopardy.csv\")\n\nprint(df)\n\nshape: (216_930, 7)\n┌─────────────┬──────────┬───────────┬─────────────────┬─────────┬────────────────┬────────────────┐\n│ Show Number ┆ Air Date ┆ Round     ┆ Category        ┆ Value   ┆ Question       ┆ Answer         │\n│ ---         ┆ ---      ┆ ---       ┆ ---             ┆ ---     ┆ ---            ┆ ---            │\n│ i64         ┆ str      ┆ str       ┆ str             ┆ str     ┆ str            ┆ str            │\n╞═════════════╪══════════╪═══════════╪═════════════════╪═════════╪════════════════╪════════════════╡\n│ 4680        ┆ 12/31/04 ┆ Jeopardy! ┆ HISTORY         ┆ $200    ┆ For the last 8 ┆ Copernicus     │\n│             ┆          ┆           ┆                 ┆         ┆ years of his   ┆                │\n│             ┆          ┆           ┆                 ┆         ┆ li…            ┆                │\n│ 4680        ┆ 12/31/04 ┆ Jeopardy! ┆ ESPN's TOP 10   ┆ $200    ┆ No. 2: 1912    ┆ Jim Thorpe     │\n│             ┆          ┆           ┆ ALL-TIME        ┆         ┆ Olympian;      ┆                │\n│             ┆          ┆           ┆ ATHLETE…        ┆         ┆ football…      ┆                │\n│ 4680        ┆ 12/31/04 ┆ Jeopardy! ┆ EVERYBODY TALKS ┆ $200    ┆ The city of    ┆ Arizona        │\n│             ┆          ┆           ┆ ABOUT IT...     ┆         ┆ Yuma in this   ┆                │\n│             ┆          ┆           ┆                 ┆         ┆ state…         ┆                │\n│ 4680        ┆ 12/31/04 ┆ Jeopardy! ┆ THE COMPANY     ┆ $200    ┆ In 1963, live  ┆ McDonald's     │\n│             ┆          ┆           ┆ LINE            ┆         ┆ on \"The Art    ┆                │\n│             ┆          ┆           ┆                 ┆         ┆ Link…          ┆                │\n│ 4680        ┆ 12/31/04 ┆ Jeopardy! ┆ EPITAPHS &      ┆ $200    ┆ Signer of the  ┆ John Adams     │\n│             ┆          ┆           ┆ TRIBUTES        ┆         ┆ Dec. of        ┆                │\n│             ┆          ┆           ┆                 ┆         ┆ Indep., …      ┆                │\n│ …           ┆ …        ┆ …         ┆ …               ┆ …       ┆ …              ┆ …              │\n│ 4999        ┆ 5/11/06  ┆ Double    ┆ RIDDLE ME THIS  ┆ $2,000  ┆ This Puccini   ┆ Turandot       │\n│             ┆          ┆ Jeopardy! ┆                 ┆         ┆ opera turns on ┆                │\n│             ┆          ┆           ┆                 ┆         ┆ th…            ┆                │\n│ 4999        ┆ 5/11/06  ┆ Double    ┆ \"T\" BIRDS       ┆ $2,000  ┆ In North       ┆ a titmouse     │\n│             ┆          ┆ Jeopardy! ┆                 ┆         ┆ America this   ┆                │\n│             ┆          ┆           ┆                 ┆         ┆ term is …      ┆                │\n│ 4999        ┆ 5/11/06  ┆ Double    ┆ AUTHORS IN      ┆ $2,000  ┆ In Penny Lane, ┆ Clive Barker   │\n│             ┆          ┆ Jeopardy! ┆ THEIR YOUTH     ┆         ┆ where this     ┆                │\n│             ┆          ┆           ┆                 ┆         ┆ \"Hel…          ┆                │\n│ 4999        ┆ 5/11/06  ┆ Double    ┆ QUOTATIONS      ┆ $2,000  ┆ From Ft. Sill, ┆ Geronimo       │\n│             ┆          ┆ Jeopardy! ┆                 ┆         ┆ Okla. he made  ┆                │\n│             ┆          ┆           ┆                 ┆         ┆ t…             ┆                │\n│ 4999        ┆ 5/11/06  ┆ Final     ┆ HISTORIC NAMES  ┆ None    ┆ A silent movie ┆ Grigori        │\n│             ┆          ┆ Jeopardy! ┆                 ┆         ┆ title includes ┆ Alexandrovich  │\n│             ┆          ┆           ┆                 ┆         ┆ …              ┆ Potemkin       │\n└─────────────┴──────────┴───────────┴─────────────────┴─────────┴────────────────┴────────────────┘",
    "crumbs": [
      "Python",
      "<b><em>Faster Python</em></b>",
      "Polars: faster data frames",
      "Subsetting data"
    ]
  },
  {
    "objectID": "python/polars_subset.html#selecting-rows",
    "href": "python/polars_subset.html#selecting-rows",
    "title": "Subsetting data",
    "section": "Selecting rows",
    "text": "Selecting rows\nYou can select rows based on any expression that evaluates to a Boolean with filter:\n\ndf_sub = df.filter(\n    pl.col(\"Air Date\") == \"5/8/09\"\n    )\n\nprint(df_sub)\n\nshape: (61, 7)\n┌─────────────┬──────────┬───────────┬─────────────────┬─────────┬────────────────┬────────────────┐\n│ Show Number ┆ Air Date ┆ Round     ┆ Category        ┆ Value   ┆ Question       ┆ Answer         │\n│ ---         ┆ ---      ┆ ---       ┆ ---             ┆ ---     ┆ ---            ┆ ---            │\n│ i64         ┆ str      ┆ str       ┆ str             ┆ str     ┆ str            ┆ str            │\n╞═════════════╪══════════╪═══════════╪═════════════════╪═════════╪════════════════╪════════════════╡\n│ 5690        ┆ 5/8/09   ┆ Jeopardy! ┆ OLD FOLKS IN    ┆ $200    ┆ goop.com is a  ┆ Gwyneth        │\n│             ┆          ┆           ┆ THEIR 30s       ┆         ┆ lifestyles     ┆ Paltrow        │\n│             ┆          ┆           ┆                 ┆         ┆ websi…         ┆                │\n│ 5690        ┆ 5/8/09   ┆ Jeopardy! ┆ MOVIES & TV     ┆ $200    ┆ On March 19,   ┆ Jay Leno       │\n│             ┆          ┆           ┆                 ┆         ┆ 2009 he said,  ┆                │\n│             ┆          ┆           ┆                 ┆         ┆ \"I'…           ┆                │\n│ 5690        ┆ 5/8/09   ┆ Jeopardy! ┆ A STATE OF      ┆ $200    ┆ Baylor,        ┆ Texas          │\n│             ┆          ┆           ┆ COLLEGE-NESS    ┆         ┆ Stephen F.     ┆                │\n│             ┆          ┆           ┆                 ┆         ┆ Austin, Ric…   ┆                │\n│ 5690        ┆ 5/8/09   ┆ Jeopardy! ┆ ANIMAL          ┆ $200    ┆ Synonym for    ┆ a pride        │\n│             ┆          ┆           ┆ COLLECTIVE      ┆         ┆ dignity that's ┆                │\n│             ┆          ┆           ┆                 ┆         ┆ the…           ┆                │\n│ 5690        ┆ 5/8/09   ┆ Jeopardy! ┆ I'D RATHER BE   ┆ $200    ┆ If you're a    ┆ a bunny hill   │\n│             ┆          ┆           ┆ SKIING          ┆         ┆ beginner, you  ┆                │\n│             ┆          ┆           ┆                 ┆         ┆ migh…          ┆                │\n│ …           ┆ …        ┆ …         ┆ …               ┆ …       ┆ …              ┆ …              │\n│ 5690        ┆ 5/8/09   ┆ Double    ┆ ANATOMY         ┆ $2,000  ┆ The pons       ┆ the cerebellum │\n│             ┆          ┆ Jeopardy! ┆                 ┆         ┆ connects the 2 ┆                │\n│             ┆          ┆           ┆                 ┆         ┆ hemisp…        ┆                │\n│ 5690        ┆ 5/8/09   ┆ Double    ┆ MATHEM-ATTACK!  ┆ $2,000  ┆ (&lt;a href=\"http ┆ volume         │\n│             ┆          ┆ Jeopardy! ┆                 ┆         ┆ ://www.j-archi ┆                │\n│             ┆          ┆           ┆                 ┆         ┆ ve…            ┆                │\n│ 5690        ┆ 5/8/09   ┆ Double    ┆ NAME THE DECADE ┆ $2,000  ┆ Man first      ┆ the 1910s      │\n│             ┆          ┆ Jeopardy! ┆                 ┆         ┆ reaches the    ┆                │\n│             ┆          ┆           ┆                 ┆         ┆ South Po…      ┆                │\n│ 5690        ┆ 5/8/09   ┆ Double    ┆ WORD ORIGINS    ┆ $2,000  ┆ A type of ear  ┆ cochlear       │\n│             ┆          ┆ Jeopardy! ┆                 ┆         ┆ implant to     ┆                │\n│             ┆          ┆           ┆                 ┆         ┆ help …         ┆                │\n│ 5690        ┆ 5/8/09   ┆ Final     ┆ EUROPEAN        ┆ None    ┆ He filed for   ┆ Henry VIII     │\n│             ┆          ┆ Jeopardy! ┆ HISTORY         ┆         ┆ divorce citing ┆                │\n│             ┆          ┆           ┆                 ┆         ┆ Le…            ┆                │\n└─────────────┴──────────┴───────────┴─────────────────┴─────────┴────────────────┴────────────────┘\n\n\nYou can combine conditions:\n\ndf_sub = df.filter(\n    pl.col(\"Air Date\") == \"5/8/09\",\n    pl.col(\"Round\") != \"Double Jeopardy!\"\n    )\n\nprint(df_sub)\n\nshape: (31, 7)\n┌─────────────┬──────────┬───────────┬─────────────────┬─────────┬────────────────┬────────────────┐\n│ Show Number ┆ Air Date ┆ Round     ┆ Category        ┆ Value   ┆ Question       ┆ Answer         │\n│ ---         ┆ ---      ┆ ---       ┆ ---             ┆ ---     ┆ ---            ┆ ---            │\n│ i64         ┆ str      ┆ str       ┆ str             ┆ str     ┆ str            ┆ str            │\n╞═════════════╪══════════╪═══════════╪═════════════════╪═════════╪════════════════╪════════════════╡\n│ 5690        ┆ 5/8/09   ┆ Jeopardy! ┆ OLD FOLKS IN    ┆ $200    ┆ goop.com is a  ┆ Gwyneth        │\n│             ┆          ┆           ┆ THEIR 30s       ┆         ┆ lifestyles     ┆ Paltrow        │\n│             ┆          ┆           ┆                 ┆         ┆ websi…         ┆                │\n│ 5690        ┆ 5/8/09   ┆ Jeopardy! ┆ MOVIES & TV     ┆ $200    ┆ On March 19,   ┆ Jay Leno       │\n│             ┆          ┆           ┆                 ┆         ┆ 2009 he said,  ┆                │\n│             ┆          ┆           ┆                 ┆         ┆ \"I'…           ┆                │\n│ 5690        ┆ 5/8/09   ┆ Jeopardy! ┆ A STATE OF      ┆ $200    ┆ Baylor,        ┆ Texas          │\n│             ┆          ┆           ┆ COLLEGE-NESS    ┆         ┆ Stephen F.     ┆                │\n│             ┆          ┆           ┆                 ┆         ┆ Austin, Ric…   ┆                │\n│ 5690        ┆ 5/8/09   ┆ Jeopardy! ┆ ANIMAL          ┆ $200    ┆ Synonym for    ┆ a pride        │\n│             ┆          ┆           ┆ COLLECTIVE      ┆         ┆ dignity that's ┆                │\n│             ┆          ┆           ┆                 ┆         ┆ the…           ┆                │\n│ 5690        ┆ 5/8/09   ┆ Jeopardy! ┆ I'D RATHER BE   ┆ $200    ┆ If you're a    ┆ a bunny hill   │\n│             ┆          ┆           ┆ SKIING          ┆         ┆ beginner, you  ┆                │\n│             ┆          ┆           ┆                 ┆         ┆ migh…          ┆                │\n│ …           ┆ …        ┆ …         ┆ …               ┆ …       ┆ …              ┆ …              │\n│ 5690        ┆ 5/8/09   ┆ Jeopardy! ┆ A STATE OF      ┆ $1,000  ┆ Grambling,     ┆ Louisiana      │\n│             ┆          ┆           ┆ COLLEGE-NESS    ┆         ┆ McNeese State, ┆                │\n│             ┆          ┆           ┆                 ┆         ┆ Sout…          ┆                │\n│ 5690        ┆ 5/8/09   ┆ Jeopardy! ┆ ANIMAL          ┆ $1,000  ┆ A flock of     ┆ crows          │\n│             ┆          ┆           ┆ COLLECTIVE      ┆         ┆ these black    ┆                │\n│             ┆          ┆           ┆                 ┆         ┆ birds i…       ┆                │\n│ 5690        ┆ 5/8/09   ┆ Jeopardy! ┆ I'D RATHER BE   ┆ $1,000  ┆ Bumps or       ┆ moguls         │\n│             ┆          ┆           ┆ SKIING          ┆         ┆ mounds of snow ┆                │\n│             ┆          ┆           ┆                 ┆         ┆ that a…        ┆                │\n│ 5690        ┆ 5/8/09   ┆ Jeopardy! ┆ PARLEZ VOUS?    ┆ $1,000  ┆ \"Huitieme\" is  ┆ eighth         │\n│             ┆          ┆           ┆                 ┆         ┆ French for     ┆                │\n│             ┆          ┆           ┆                 ┆         ┆ this …         ┆                │\n│ 5690        ┆ 5/8/09   ┆ Final     ┆ EUROPEAN        ┆ None    ┆ He filed for   ┆ Henry VIII     │\n│             ┆          ┆ Jeopardy! ┆ HISTORY         ┆         ┆ divorce citing ┆                │\n│             ┆          ┆           ┆                 ┆         ┆ Le…            ┆                │\n└─────────────┴──────────┴───────────┴─────────────────┴─────────┴────────────────┴────────────────┘",
    "crumbs": [
      "Python",
      "<b><em>Faster Python</em></b>",
      "Polars: faster data frames",
      "Subsetting data"
    ]
  },
  {
    "objectID": "python/polars_subset.html#selecting-columns",
    "href": "python/polars_subset.html#selecting-columns",
    "title": "Subsetting data",
    "section": "Selecting columns",
    "text": "Selecting columns\nTo select columns (variables), you use select:\n\ndf_sub = df.select(\n    pl.col(\"Show Number\"),\n    pl.col(\"Category\")\n    )\n\nprint(df_sub)\n\nshape: (216_930, 2)\n┌─────────────┬─────────────────────────────────┐\n│ Show Number ┆ Category                        │\n│ ---         ┆ ---                             │\n│ i64         ┆ str                             │\n╞═════════════╪═════════════════════════════════╡\n│ 4680        ┆ HISTORY                         │\n│ 4680        ┆ ESPN's TOP 10 ALL-TIME ATHLETE… │\n│ 4680        ┆ EVERYBODY TALKS ABOUT IT...     │\n│ 4680        ┆ THE COMPANY LINE                │\n│ 4680        ┆ EPITAPHS & TRIBUTES             │\n│ …           ┆ …                               │\n│ 4999        ┆ RIDDLE ME THIS                  │\n│ 4999        ┆ \"T\" BIRDS                       │\n│ 4999        ┆ AUTHORS IN THEIR YOUTH          │\n│ 4999        ┆ QUOTATIONS                      │\n│ 4999        ┆ HISTORIC NAMES                  │\n└─────────────┴─────────────────────────────────┘",
    "crumbs": [
      "Python",
      "<b><em>Faster Python</em></b>",
      "Polars: faster data frames",
      "Subsetting data"
    ]
  },
  {
    "objectID": "python/polars_subset.html#creating-new-columns-with-output-of-expressions",
    "href": "python/polars_subset.html#creating-new-columns-with-output-of-expressions",
    "title": "Subsetting data",
    "section": "Creating new columns with output of expressions",
    "text": "Creating new columns with output of expressions\nThe jeopardy dataset is made mostly of String variables. Let’s use another one here: the now archived global confirmed Covid-19 cases from John Hopkins University:\n\nurl = \"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv\"\n\ndf = pl.read_csv(url)\n\nprint(df)\n\nshape: (289, 1_147)\n┌────────────────┬────────────────┬────────────┬───────────┬───┬────────┬────────┬────────┬────────┐\n│ Province/State ┆ Country/Region ┆ Lat        ┆ Long      ┆ … ┆ 3/6/23 ┆ 3/7/23 ┆ 3/8/23 ┆ 3/9/23 │\n│ ---            ┆ ---            ┆ ---        ┆ ---       ┆   ┆ ---    ┆ ---    ┆ ---    ┆ ---    │\n│ str            ┆ str            ┆ f64        ┆ f64       ┆   ┆ i64    ┆ i64    ┆ i64    ┆ i64    │\n╞════════════════╪════════════════╪════════════╪═══════════╪═══╪════════╪════════╪════════╪════════╡\n│ null           ┆ Afghanistan    ┆ 33.93911   ┆ 67.709953 ┆ … ┆ 209406 ┆ 209436 ┆ 209451 ┆ 209451 │\n│ null           ┆ Albania        ┆ 41.1533    ┆ 20.1683   ┆ … ┆ 334427 ┆ 334427 ┆ 334443 ┆ 334457 │\n│ null           ┆ Algeria        ┆ 28.0339    ┆ 1.6596    ┆ … ┆ 271477 ┆ 271490 ┆ 271494 ┆ 271496 │\n│ null           ┆ Andorra        ┆ 42.5063    ┆ 1.5218    ┆ … ┆ 47875  ┆ 47875  ┆ 47890  ┆ 47890  │\n│ null           ┆ Angola         ┆ -11.2027   ┆ 17.8739   ┆ … ┆ 105277 ┆ 105277 ┆ 105288 ┆ 105288 │\n│ …              ┆ …              ┆ …          ┆ …         ┆ … ┆ …      ┆ …      ┆ …      ┆ …      │\n│ null           ┆ West Bank and  ┆ 31.9522    ┆ 35.2332   ┆ … ┆ 703228 ┆ 703228 ┆ 703228 ┆ 703228 │\n│                ┆ Gaza           ┆            ┆           ┆   ┆        ┆        ┆        ┆        │\n│ null           ┆ Winter         ┆ 39.9042    ┆ 116.4074  ┆ … ┆ 535    ┆ 535    ┆ 535    ┆ 535    │\n│                ┆ Olympics 2022  ┆            ┆           ┆   ┆        ┆        ┆        ┆        │\n│ null           ┆ Yemen          ┆ 15.552727  ┆ 48.516388 ┆ … ┆ 11945  ┆ 11945  ┆ 11945  ┆ 11945  │\n│ null           ┆ Zambia         ┆ -13.133897 ┆ 27.849332 ┆ … ┆ 343135 ┆ 343135 ┆ 343135 ┆ 343135 │\n│ null           ┆ Zimbabwe       ┆ -19.015438 ┆ 29.154857 ┆ … ┆ 264127 ┆ 264127 ┆ 264276 ┆ 264276 │\n└────────────────┴────────────────┴────────────┴───────────┴───┴────────┴────────┴────────┴────────┘\n\n\nTo create a new variable called daily_avg with the daily average of new cases, we use select again, but this time we add an expression:\n\ndf_new = df.select(\n    daily_avg=pl.col(\"3/9/23\") / 1143\n    )\n\nprint(df_new)\n\nshape: (289, 1)\n┌────────────┐\n│ daily_avg  │\n│ ---        │\n│ f64        │\n╞════════════╡\n│ 183.246719 │\n│ 292.613298 │\n│ 237.529309 │\n│ 41.898513  │\n│ 92.115486  │\n│ …          │\n│ 615.247594 │\n│ 0.468066   │\n│ 10.450569  │\n│ 300.205599 │\n│ 231.212598 │\n└────────────┘\n\n\n\nSince the data is cumulative across dates, we took the last columns (totals cases for each row) and divided by the number of days of this dataset (total number of columns menus the four first columns).\n\nIf you want to keep all columns in the output, you use with_columns:\n\ndf_new = df.with_columns(\n    daily_avg=pl.col(\"3/9/23\") / 1143\n    )\n\nprint(df_new)\n\nshape: (289, 1_148)\n┌──────────────┬──────────────┬────────────┬───────────┬───┬────────┬────────┬────────┬────────────┐\n│ Province/Sta ┆ Country/Regi ┆ Lat        ┆ Long      ┆ … ┆ 3/7/23 ┆ 3/8/23 ┆ 3/9/23 ┆ daily_avg  │\n│ te           ┆ on           ┆ ---        ┆ ---       ┆   ┆ ---    ┆ ---    ┆ ---    ┆ ---        │\n│ ---          ┆ ---          ┆ f64        ┆ f64       ┆   ┆ i64    ┆ i64    ┆ i64    ┆ f64        │\n│ str          ┆ str          ┆            ┆           ┆   ┆        ┆        ┆        ┆            │\n╞══════════════╪══════════════╪════════════╪═══════════╪═══╪════════╪════════╪════════╪════════════╡\n│ null         ┆ Afghanistan  ┆ 33.93911   ┆ 67.709953 ┆ … ┆ 209436 ┆ 209451 ┆ 209451 ┆ 183.246719 │\n│ null         ┆ Albania      ┆ 41.1533    ┆ 20.1683   ┆ … ┆ 334427 ┆ 334443 ┆ 334457 ┆ 292.613298 │\n│ null         ┆ Algeria      ┆ 28.0339    ┆ 1.6596    ┆ … ┆ 271490 ┆ 271494 ┆ 271496 ┆ 237.529309 │\n│ null         ┆ Andorra      ┆ 42.5063    ┆ 1.5218    ┆ … ┆ 47875  ┆ 47890  ┆ 47890  ┆ 41.898513  │\n│ null         ┆ Angola       ┆ -11.2027   ┆ 17.8739   ┆ … ┆ 105277 ┆ 105288 ┆ 105288 ┆ 92.115486  │\n│ …            ┆ …            ┆ …          ┆ …         ┆ … ┆ …      ┆ …      ┆ …      ┆ …          │\n│ null         ┆ West Bank    ┆ 31.9522    ┆ 35.2332   ┆ … ┆ 703228 ┆ 703228 ┆ 703228 ┆ 615.247594 │\n│              ┆ and Gaza     ┆            ┆           ┆   ┆        ┆        ┆        ┆            │\n│ null         ┆ Winter       ┆ 39.9042    ┆ 116.4074  ┆ … ┆ 535    ┆ 535    ┆ 535    ┆ 0.468066   │\n│              ┆ Olympics     ┆            ┆           ┆   ┆        ┆        ┆        ┆            │\n│              ┆ 2022         ┆            ┆           ┆   ┆        ┆        ┆        ┆            │\n│ null         ┆ Yemen        ┆ 15.552727  ┆ 48.516388 ┆ … ┆ 11945  ┆ 11945  ┆ 11945  ┆ 10.450569  │\n│ null         ┆ Zambia       ┆ -13.133897 ┆ 27.849332 ┆ … ┆ 343135 ┆ 343135 ┆ 343135 ┆ 300.205599 │\n│ null         ┆ Zimbabwe     ┆ -19.015438 ┆ 29.154857 ┆ … ┆ 264127 ┆ 264276 ┆ 264276 ┆ 231.212598 │\n└──────────────┴──────────────┴────────────┴───────────┴───┴────────┴────────┴────────┴────────────┘\n\n\n\nNotice that our new variable got added as the last column of the data frame.\n\nIf we want to write in place, we can reassign the output to the initial data frame:\n\ndf = df.with_columns(\n    daily_avg=pl.col(\"3/9/23\") / 1143\n    )",
    "crumbs": [
      "Python",
      "<b><em>Faster Python</em></b>",
      "Polars: faster data frames",
      "Subsetting data"
    ]
  },
  {
    "objectID": "python/polars_subset.html#group-by-operations",
    "href": "python/polars_subset.html#group-by-operations",
    "title": "Subsetting data",
    "section": "Group by operations",
    "text": "Group by operations\nIn this Covid-19 dataset some countries (e.g. Australia) are split between several provinces or states. If we want the total numbers for such countries we have to group the rows by the variable Country/Region, then get the sum for each group.\nGetting the sums of the latitude and longitude wouldn’t make any sense, so first we get rid of those two columns:\n\ndf_clean = df.select(\n    pl.col(\"*\").exclude(\"Lat\", \"Long\")\n    )\n\nprint(df_clean)\n\nshape: (289, 1_146)\n┌────────────────┬─────────────────┬─────────┬─────────┬───┬────────┬────────┬────────┬────────────┐\n│ Province/State ┆ Country/Region  ┆ 1/22/20 ┆ 1/23/20 ┆ … ┆ 3/7/23 ┆ 3/8/23 ┆ 3/9/23 ┆ daily_avg  │\n│ ---            ┆ ---             ┆ ---     ┆ ---     ┆   ┆ ---    ┆ ---    ┆ ---    ┆ ---        │\n│ str            ┆ str             ┆ i64     ┆ i64     ┆   ┆ i64    ┆ i64    ┆ i64    ┆ f64        │\n╞════════════════╪═════════════════╪═════════╪═════════╪═══╪════════╪════════╪════════╪════════════╡\n│ null           ┆ Afghanistan     ┆ 0       ┆ 0       ┆ … ┆ 209436 ┆ 209451 ┆ 209451 ┆ 183.246719 │\n│ null           ┆ Albania         ┆ 0       ┆ 0       ┆ … ┆ 334427 ┆ 334443 ┆ 334457 ┆ 292.613298 │\n│ null           ┆ Algeria         ┆ 0       ┆ 0       ┆ … ┆ 271490 ┆ 271494 ┆ 271496 ┆ 237.529309 │\n│ null           ┆ Andorra         ┆ 0       ┆ 0       ┆ … ┆ 47875  ┆ 47890  ┆ 47890  ┆ 41.898513  │\n│ null           ┆ Angola          ┆ 0       ┆ 0       ┆ … ┆ 105277 ┆ 105288 ┆ 105288 ┆ 92.115486  │\n│ …              ┆ …               ┆ …       ┆ …       ┆ … ┆ …      ┆ …      ┆ …      ┆ …          │\n│ null           ┆ West Bank and   ┆ 0       ┆ 0       ┆ … ┆ 703228 ┆ 703228 ┆ 703228 ┆ 615.247594 │\n│                ┆ Gaza            ┆         ┆         ┆   ┆        ┆        ┆        ┆            │\n│ null           ┆ Winter Olympics ┆ 0       ┆ 0       ┆ … ┆ 535    ┆ 535    ┆ 535    ┆ 0.468066   │\n│                ┆ 2022            ┆         ┆         ┆   ┆        ┆        ┆        ┆            │\n│ null           ┆ Yemen           ┆ 0       ┆ 0       ┆ … ┆ 11945  ┆ 11945  ┆ 11945  ┆ 10.450569  │\n│ null           ┆ Zambia          ┆ 0       ┆ 0       ┆ … ┆ 343135 ┆ 343135 ┆ 343135 ┆ 300.205599 │\n│ null           ┆ Zimbabwe        ┆ 0       ┆ 0       ┆ … ┆ 264127 ┆ 264276 ┆ 264276 ┆ 231.212598 │\n└────────────────┴─────────────────┴─────────┴─────────┴───┴────────┴────────┴────────┴────────────┘\n\n\n\nThere are many ways to select columns from a data frame.\n\nNow we can group by and get our sums:\n\ndf_countries = df_clean.group_by(\n    (pl.col(\"Country/Region\")).alias(\"Country totals\")\n    ).sum()\n\nprint(df_countries)\n\nshape: (201, 1_147)\n┌─────────────┬─────────────┬─────────────┬─────────┬───┬─────────┬─────────┬─────────┬────────────┐\n│ Country     ┆ Province/St ┆ Country/Reg ┆ 1/22/20 ┆ … ┆ 3/7/23  ┆ 3/8/23  ┆ 3/9/23  ┆ daily_avg  │\n│ totals      ┆ ate         ┆ ion         ┆ ---     ┆   ┆ ---     ┆ ---     ┆ ---     ┆ ---        │\n│ ---         ┆ ---         ┆ ---         ┆ i64     ┆   ┆ i64     ┆ i64     ┆ i64     ┆ f64        │\n│ str         ┆ str         ┆ str         ┆         ┆   ┆         ┆         ┆         ┆            │\n╞═════════════╪═════════════╪═════════════╪═════════╪═══╪═════════╪═════════╪═════════╪════════════╡\n│ Honduras    ┆ null        ┆ null        ┆ 0       ┆ … ┆ 472250  ┆ 472250  ┆ 472250  ┆ 413.167104 │\n│ Burkina     ┆ null        ┆ null        ┆ 0       ┆ … ┆ 22056   ┆ 22056   ┆ 22056   ┆ 19.296588  │\n│ Faso        ┆             ┆             ┆         ┆   ┆         ┆         ┆         ┆            │\n│ Venezuela   ┆ null        ┆ null        ┆ 0       ┆ … ┆ 552157  ┆ 552157  ┆ 552162  ┆ 483.081365 │\n│ El Salvador ┆ null        ┆ null        ┆ 0       ┆ … ┆ 201785  ┆ 201785  ┆ 201785  ┆ 176.539808 │\n│ Egypt       ┆ null        ┆ null        ┆ 0       ┆ … ┆ 515698  ┆ 515759  ┆ 515759  ┆ 451.232721 │\n│ …           ┆ …           ┆ …           ┆ …       ┆ … ┆ …       ┆ …       ┆ …       ┆ …          │\n│ Belarus     ┆ null        ┆ null        ┆ 0       ┆ … ┆ 994037  ┆ 994037  ┆ 994037  ┆ 869.673666 │\n│ Saint Lucia ┆ null        ┆ null        ┆ 0       ┆ … ┆ 30004   ┆ 30004   ┆ 30004   ┆ 26.250219  │\n│ Ireland     ┆ null        ┆ null        ┆ 0       ┆ … ┆ 1703850 ┆ 1704502 ┆ 1704502 ┆ 1491.25284 │\n│             ┆             ┆             ┆         ┆   ┆         ┆         ┆         ┆ 3          │\n│ Czechia     ┆ null        ┆ null        ┆ 0       ┆ … ┆ 4615945 ┆ 4617114 ┆ 4618256 ┆ 4040.46894 │\n│             ┆             ┆             ┆         ┆   ┆         ┆         ┆         ┆ 1          │\n│ Switzerland ┆ null        ┆ null        ┆ 0       ┆ … ┆ 4413911 ┆ 4413911 ┆ 4413911 ┆ 3861.68941 │\n│             ┆             ┆             ┆         ┆   ┆         ┆         ┆         ┆ 4          │\n└─────────────┴─────────────┴─────────────┴─────────┴───┴─────────┴─────────┴─────────┴────────────┘\n\n\n\nThe alias method allows us to give a name to the groups.\n\nNotice that the rows became out of order. Not to worry about order makes the code more efficient and does not affect future subsetting of our data frame. If you want to maintain the order however, you can use the maintain_order parameter:\n\ndf_countries = df_clean.group_by(\n    (pl.col(\"Country/Region\")).alias(\"Country\"),\n    maintain_order=True\n    ).sum()\n\nprint(df_countries)\n\nshape: (201, 1_147)\n┌──────────────┬──────────────┬──────────────┬─────────┬───┬────────┬────────┬────────┬────────────┐\n│ Country      ┆ Province/Sta ┆ Country/Regi ┆ 1/22/20 ┆ … ┆ 3/7/23 ┆ 3/8/23 ┆ 3/9/23 ┆ daily_avg  │\n│ ---          ┆ te           ┆ on           ┆ ---     ┆   ┆ ---    ┆ ---    ┆ ---    ┆ ---        │\n│ str          ┆ ---          ┆ ---          ┆ i64     ┆   ┆ i64    ┆ i64    ┆ i64    ┆ f64        │\n│              ┆ str          ┆ str          ┆         ┆   ┆        ┆        ┆        ┆            │\n╞══════════════╪══════════════╪══════════════╪═════════╪═══╪════════╪════════╪════════╪════════════╡\n│ Afghanistan  ┆ null         ┆ null         ┆ 0       ┆ … ┆ 209436 ┆ 209451 ┆ 209451 ┆ 183.246719 │\n│ Albania      ┆ null         ┆ null         ┆ 0       ┆ … ┆ 334427 ┆ 334443 ┆ 334457 ┆ 292.613298 │\n│ Algeria      ┆ null         ┆ null         ┆ 0       ┆ … ┆ 271490 ┆ 271494 ┆ 271496 ┆ 237.529309 │\n│ Andorra      ┆ null         ┆ null         ┆ 0       ┆ … ┆ 47875  ┆ 47890  ┆ 47890  ┆ 41.898513  │\n│ Angola       ┆ null         ┆ null         ┆ 0       ┆ … ┆ 105277 ┆ 105288 ┆ 105288 ┆ 92.115486  │\n│ …            ┆ …            ┆ …            ┆ …       ┆ … ┆ …      ┆ …      ┆ …      ┆ …          │\n│ West Bank    ┆ null         ┆ null         ┆ 0       ┆ … ┆ 703228 ┆ 703228 ┆ 703228 ┆ 615.247594 │\n│ and Gaza     ┆              ┆              ┆         ┆   ┆        ┆        ┆        ┆            │\n│ Winter       ┆ null         ┆ null         ┆ 0       ┆ … ┆ 535    ┆ 535    ┆ 535    ┆ 0.468066   │\n│ Olympics     ┆              ┆              ┆         ┆   ┆        ┆        ┆        ┆            │\n│ 2022         ┆              ┆              ┆         ┆   ┆        ┆        ┆        ┆            │\n│ Yemen        ┆ null         ┆ null         ┆ 0       ┆ … ┆ 11945  ┆ 11945  ┆ 11945  ┆ 10.450569  │\n│ Zambia       ┆ null         ┆ null         ┆ 0       ┆ … ┆ 343135 ┆ 343135 ┆ 343135 ┆ 300.205599 │\n│ Zimbabwe     ┆ null         ┆ null         ┆ 0       ┆ … ┆ 264127 ┆ 264276 ┆ 264276 ┆ 231.212598 │\n└──────────────┴──────────────┴──────────────┴─────────┴───┴────────┴────────┴────────┴────────────┘\n\n\n\n\nYour turn:\n\n\nThe old Country/Region column is now irrelevant. Remove it from df_countries.\nHow could you get the total number of cases for each day for the whole world?",
    "crumbs": [
      "Python",
      "<b><em>Faster Python</em></b>",
      "Polars: faster data frames",
      "Subsetting data"
    ]
  },
  {
    "objectID": "python/polars_resources.html",
    "href": "python/polars_resources.html",
    "title": "Resources",
    "section": "",
    "text": "Here is a list of resources for Polars.\n\n\nPolars website\n\nOfficial documentation\nPolars API\nGitHub repo\n\n\n\nBook\nKevin Heavey wrote Modern Polars following the model of the Modern Pandas book. This is a great resource, although getting a little outdated for the scaling chapter since Polars is evolving so fast.\n\n\nIntegration with other tools\n\nNumPy:\nSee the documentation, the from_numpy and to_numpy functions, the development progress of this integration, and performance advice.\nParallel computing:\nWith Ray thanks to this setting; with Spark, Dask, and Ray thanks to fugue.\nGPUs:\nWith the cuDF library from RAPIDS (in development).\nSQL:\nWith DuckDB.\nPlotting:\nA great section of the documentation shows how Polars integrates with many Python plotting frameworks.\nPublishing:\nThe documentation shows how to publish beautiful tables from Polars data frames with Great Tables.",
    "crumbs": [
      "Python",
      "<b><em>Faster Python</em></b>",
      "Polars: faster data frames",
      "Resources"
    ]
  },
  {
    "objectID": "python/polars_lazy.html",
    "href": "python/polars_lazy.html",
    "title": "Lazy evaluation",
    "section": "",
    "text": "When it comes to high-performance computing, one of the strengths of Polars is that it supports lazy evaluation. Lazy evaluation instantly returns a future that can be used down the code without waiting for the result of the computation to get calculated. It also allows the query optimizer to combine operations, very much the way compiled languages work.\nIf you want to speedup your code, use lazy execution whenever possible.\n\nTry to use the lazy API from the start, when reading a file.\nIn previous examples, we used read_csv to read our data. This returns a Polars DataFrame. Instead, you can use scan_csv to create a LazyFrame:\n\nimport polars as pl\n\nurl = \"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv\"\n\ndf = pl.read_csv(url)\ndf_lazy = pl.scan_csv(url)\n\nprint(type(df))\nprint(type(df_lazy))\n\n&lt;class 'polars.dataframe.frame.DataFrame'&gt;\n&lt;class 'polars.lazyframe.frame.LazyFrame'&gt;\n\n\n\nThere are scan functions for all the numerous IO methods Polars offers.\n\nIf you already have a DataFrame, you can create a LazyFrame from it with the lazy method:\n\ndf_lazy = df.lazy()\n\nWhen you run queries on a LazyFrame, instead of evaluating them, Polars creates a graph and runs many optimizations on it.\nTo evaluate the code and get the result, you use the collect method.\nWe will see this in action in the next section.",
    "crumbs": [
      "Python",
      "<b><em>Faster Python</em></b>",
      "Polars: faster data frames",
      "Lazy evaluation"
    ]
  },
  {
    "objectID": "python/polars_inspection.html",
    "href": "python/polars_inspection.html",
    "title": "Data frame inspection",
    "section": "",
    "text": "Once we have a data frame, it is important to quickly get some basic information about it. In this section, we will see how to do so.\nLet’s start by reading an online CSV file from a URL:\nimport polars as pl\n\ndf = pl.read_csv(\"https://raw.githubusercontent.com/razoumov/publish/master/jeopardy.csv\")\nprint(df)\n\nshape: (216_930, 7)\n┌─────────────┬──────────┬───────────┬─────────────────┬─────────┬────────────────┬────────────────┐\n│ Show Number ┆ Air Date ┆ Round     ┆ Category        ┆ Value   ┆ Question       ┆ Answer         │\n│ ---         ┆ ---      ┆ ---       ┆ ---             ┆ ---     ┆ ---            ┆ ---            │\n│ i64         ┆ str      ┆ str       ┆ str             ┆ str     ┆ str            ┆ str            │\n╞═════════════╪══════════╪═══════════╪═════════════════╪═════════╪════════════════╪════════════════╡\n│ 4680        ┆ 12/31/04 ┆ Jeopardy! ┆ HISTORY         ┆ $200    ┆ For the last 8 ┆ Copernicus     │\n│             ┆          ┆           ┆                 ┆         ┆ years of his   ┆                │\n│             ┆          ┆           ┆                 ┆         ┆ li…            ┆                │\n│ 4680        ┆ 12/31/04 ┆ Jeopardy! ┆ ESPN's TOP 10   ┆ $200    ┆ No. 2: 1912    ┆ Jim Thorpe     │\n│             ┆          ┆           ┆ ALL-TIME        ┆         ┆ Olympian;      ┆                │\n│             ┆          ┆           ┆ ATHLETE…        ┆         ┆ football…      ┆                │\n│ 4680        ┆ 12/31/04 ┆ Jeopardy! ┆ EVERYBODY TALKS ┆ $200    ┆ The city of    ┆ Arizona        │\n│             ┆          ┆           ┆ ABOUT IT...     ┆         ┆ Yuma in this   ┆                │\n│             ┆          ┆           ┆                 ┆         ┆ state…         ┆                │\n│ 4680        ┆ 12/31/04 ┆ Jeopardy! ┆ THE COMPANY     ┆ $200    ┆ In 1963, live  ┆ McDonald's     │\n│             ┆          ┆           ┆ LINE            ┆         ┆ on \"The Art    ┆                │\n│             ┆          ┆           ┆                 ┆         ┆ Link…          ┆                │\n│ 4680        ┆ 12/31/04 ┆ Jeopardy! ┆ EPITAPHS &      ┆ $200    ┆ Signer of the  ┆ John Adams     │\n│             ┆          ┆           ┆ TRIBUTES        ┆         ┆ Dec. of        ┆                │\n│             ┆          ┆           ┆                 ┆         ┆ Indep., …      ┆                │\n│ …           ┆ …        ┆ …         ┆ …               ┆ …       ┆ …              ┆ …              │\n│ 4999        ┆ 5/11/06  ┆ Double    ┆ RIDDLE ME THIS  ┆ $2,000  ┆ This Puccini   ┆ Turandot       │\n│             ┆          ┆ Jeopardy! ┆                 ┆         ┆ opera turns on ┆                │\n│             ┆          ┆           ┆                 ┆         ┆ th…            ┆                │\n│ 4999        ┆ 5/11/06  ┆ Double    ┆ \"T\" BIRDS       ┆ $2,000  ┆ In North       ┆ a titmouse     │\n│             ┆          ┆ Jeopardy! ┆                 ┆         ┆ America this   ┆                │\n│             ┆          ┆           ┆                 ┆         ┆ term is …      ┆                │\n│ 4999        ┆ 5/11/06  ┆ Double    ┆ AUTHORS IN      ┆ $2,000  ┆ In Penny Lane, ┆ Clive Barker   │\n│             ┆          ┆ Jeopardy! ┆ THEIR YOUTH     ┆         ┆ where this     ┆                │\n│             ┆          ┆           ┆                 ┆         ┆ \"Hel…          ┆                │\n│ 4999        ┆ 5/11/06  ┆ Double    ┆ QUOTATIONS      ┆ $2,000  ┆ From Ft. Sill, ┆ Geronimo       │\n│             ┆          ┆ Jeopardy! ┆                 ┆         ┆ Okla. he made  ┆                │\n│             ┆          ┆           ┆                 ┆         ┆ t…             ┆                │\n│ 4999        ┆ 5/11/06  ┆ Final     ┆ HISTORIC NAMES  ┆ None    ┆ A silent movie ┆ Grigori        │\n│             ┆          ┆ Jeopardy! ┆                 ┆         ┆ title includes ┆ Alexandrovich  │\n│             ┆          ┆           ┆                 ┆         ┆ …              ┆ Potemkin       │\n└─────────────┴──────────┴───────────┴─────────────────┴─────────┴────────────────┴────────────────┘",
    "crumbs": [
      "Python",
      "<b><em>Faster Python</em></b>",
      "Polars: faster data frames",
      "Data frame inspection"
    ]
  },
  {
    "objectID": "python/polars_inspection.html#printing-a-few-rows",
    "href": "python/polars_inspection.html#printing-a-few-rows",
    "title": "Data frame inspection",
    "section": "Printing a few rows",
    "text": "Printing a few rows\nPrint first rows (5 by default):\n\nprint(df.head())\n\nshape: (5, 7)\n┌─────────────┬──────────┬───────────┬────────────────────┬───────┬───────────────────┬────────────┐\n│ Show Number ┆ Air Date ┆ Round     ┆ Category           ┆ Value ┆ Question          ┆ Answer     │\n│ ---         ┆ ---      ┆ ---       ┆ ---                ┆ ---   ┆ ---               ┆ ---        │\n│ i64         ┆ str      ┆ str       ┆ str                ┆ str   ┆ str               ┆ str        │\n╞═════════════╪══════════╪═══════════╪════════════════════╪═══════╪═══════════════════╪════════════╡\n│ 4680        ┆ 12/31/04 ┆ Jeopardy! ┆ HISTORY            ┆ $200  ┆ For the last 8    ┆ Copernicus │\n│             ┆          ┆           ┆                    ┆       ┆ years of his li…  ┆            │\n│ 4680        ┆ 12/31/04 ┆ Jeopardy! ┆ ESPN's TOP 10      ┆ $200  ┆ No. 2: 1912       ┆ Jim Thorpe │\n│             ┆          ┆           ┆ ALL-TIME ATHLETE…  ┆       ┆ Olympian;         ┆            │\n│             ┆          ┆           ┆                    ┆       ┆ football…         ┆            │\n│ 4680        ┆ 12/31/04 ┆ Jeopardy! ┆ EVERYBODY TALKS    ┆ $200  ┆ The city of Yuma  ┆ Arizona    │\n│             ┆          ┆           ┆ ABOUT IT...        ┆       ┆ in this state…    ┆            │\n│ 4680        ┆ 12/31/04 ┆ Jeopardy! ┆ THE COMPANY LINE   ┆ $200  ┆ In 1963, live on  ┆ McDonald's │\n│             ┆          ┆           ┆                    ┆       ┆ \"The Art Link…    ┆            │\n│ 4680        ┆ 12/31/04 ┆ Jeopardy! ┆ EPITAPHS &         ┆ $200  ┆ Signer of the     ┆ John Adams │\n│             ┆          ┆           ┆ TRIBUTES           ┆       ┆ Dec. of Indep., … ┆            │\n└─────────────┴──────────┴───────────┴────────────────────┴───────┴───────────────────┴────────────┘\n\n\n\nprint(df.head(2))\n\nshape: (2, 7)\n┌─────────────┬──────────┬───────────┬────────────────────┬───────┬───────────────────┬────────────┐\n│ Show Number ┆ Air Date ┆ Round     ┆ Category           ┆ Value ┆ Question          ┆ Answer     │\n│ ---         ┆ ---      ┆ ---       ┆ ---                ┆ ---   ┆ ---               ┆ ---        │\n│ i64         ┆ str      ┆ str       ┆ str                ┆ str   ┆ str               ┆ str        │\n╞═════════════╪══════════╪═══════════╪════════════════════╪═══════╪═══════════════════╪════════════╡\n│ 4680        ┆ 12/31/04 ┆ Jeopardy! ┆ HISTORY            ┆ $200  ┆ For the last 8    ┆ Copernicus │\n│             ┆          ┆           ┆                    ┆       ┆ years of his li…  ┆            │\n│ 4680        ┆ 12/31/04 ┆ Jeopardy! ┆ ESPN's TOP 10      ┆ $200  ┆ No. 2: 1912       ┆ Jim Thorpe │\n│             ┆          ┆           ┆ ALL-TIME ATHLETE…  ┆       ┆ Olympian;         ┆            │\n│             ┆          ┆           ┆                    ┆       ┆ football…         ┆            │\n└─────────────┴──────────┴───────────┴────────────────────┴───────┴───────────────────┴────────────┘\n\n\nPrint last rows (5 by default):\n\nprint(df.tail(2))\n\nshape: (2, 7)\n┌─────────────┬──────────┬───────────┬─────────────────┬─────────┬────────────────┬────────────────┐\n│ Show Number ┆ Air Date ┆ Round     ┆ Category        ┆ Value   ┆ Question       ┆ Answer         │\n│ ---         ┆ ---      ┆ ---       ┆ ---             ┆ ---     ┆ ---            ┆ ---            │\n│ i64         ┆ str      ┆ str       ┆ str             ┆ str     ┆ str            ┆ str            │\n╞═════════════╪══════════╪═══════════╪═════════════════╪═════════╪════════════════╪════════════════╡\n│ 4999        ┆ 5/11/06  ┆ Double    ┆ QUOTATIONS      ┆ $2,000  ┆ From Ft. Sill, ┆ Geronimo       │\n│             ┆          ┆ Jeopardy! ┆                 ┆         ┆ Okla. he made  ┆                │\n│             ┆          ┆           ┆                 ┆         ┆ t…             ┆                │\n│ 4999        ┆ 5/11/06  ┆ Final     ┆ HISTORIC NAMES  ┆ None    ┆ A silent movie ┆ Grigori        │\n│             ┆          ┆ Jeopardy! ┆                 ┆         ┆ title includes ┆ Alexandrovich  │\n│             ┆          ┆           ┆                 ┆         ┆ …              ┆ Potemkin       │\n└─────────────┴──────────┴───────────┴─────────────────┴─────────┴────────────────┴────────────────┘\n\n\nPrint random rows (this is very useful as the head and tail of your data frame may not be representative of your data):\n\nimport random\n\nprint(df.sample(4))\n\nshape: (4, 7)\n┌─────────────┬──────────┬───────────┬─────────────────┬─────────┬────────────────┬────────────────┐\n│ Show Number ┆ Air Date ┆ Round     ┆ Category        ┆ Value   ┆ Question       ┆ Answer         │\n│ ---         ┆ ---      ┆ ---       ┆ ---             ┆ ---     ┆ ---            ┆ ---            │\n│ i64         ┆ str      ┆ str       ┆ str             ┆ str     ┆ str            ┆ str            │\n╞═════════════╪══════════╪═══════════╪═════════════════╪═════════╪════════════════╪════════════════╡\n│ 5861        ┆ 2/22/10  ┆ Double    ┆ \"G\" IN THE GOOD ┆ $1,200  ┆ \"Balm\"-y       ┆ Gilead         │\n│             ┆          ┆ Jeopardy! ┆ BOOK            ┆         ┆ region in      ┆                │\n│             ┆          ┆           ┆                 ┆         ┆ present-day…   ┆                │\n│ 6061        ┆ 1/10/11  ┆ Jeopardy! ┆ THE \"CAR\" POOL  ┆ $1,000  ┆ 14-letter      ┆ cardiovascular │\n│             ┆          ┆           ┆                 ┆         ┆ adjective      ┆                │\n│             ┆          ┆           ┆                 ┆         ┆ meaning pe…    ┆                │\n│ 3865        ┆ 5/25/01  ┆ Jeopardy! ┆ SCIENCE &       ┆ $300    ┆ Of the 6       ┆ a screw        │\n│             ┆          ┆           ┆ NATURE          ┆         ┆ simple         ┆                │\n│             ┆          ┆           ┆                 ┆         ┆ machines in    ┆                │\n│             ┆          ┆           ┆                 ┆         ┆ ph…            ┆                │\n│ 5404        ┆ 2/21/08  ┆ Jeopardy! ┆ THEM LITERATURE ┆ $200    ┆ In 1928 A.A.   ┆ Pooh           │\n│             ┆          ┆           ┆ TYPE FACTS      ┆         ┆ Milne          ┆                │\n│             ┆          ┆           ┆                 ┆         ┆ published \"…   ┆                │\n└─────────────┴──────────┴───────────┴─────────────────┴─────────┴────────────────┴────────────────┘",
    "crumbs": [
      "Python",
      "<b><em>Faster Python</em></b>",
      "Polars: faster data frames",
      "Data frame inspection"
    ]
  },
  {
    "objectID": "python/polars_inspection.html#structure",
    "href": "python/polars_inspection.html#structure",
    "title": "Data frame inspection",
    "section": "Structure",
    "text": "Structure\nOverview of the data frame and its structure:\n\nprint(df.glimpse())\n\nRows: 216930\nColumns: 7\n$ Show Number &lt;i64&gt; 4680, 4680, 4680, 4680, 4680, 4680, 4680, 4680, 4680, 4680\n$ Air Date    &lt;str&gt; '12/31/04', '12/31/04', '12/31/04', '12/31/04', '12/31/04', '12/31/04', '12/31/04', '12/31/04', '12/31/04', '12/31/04'\n$ Round       &lt;str&gt; 'Jeopardy!', 'Jeopardy!', 'Jeopardy!', 'Jeopardy!', 'Jeopardy!', 'Jeopardy!', 'Jeopardy!', 'Jeopardy!', 'Jeopardy!', 'Jeopardy!'\n$ Category    &lt;str&gt; 'HISTORY', \"ESPN's TOP 10 ALL-TIME ATHLETES\", 'EVERYBODY TALKS ABOUT IT...', 'THE COMPANY LINE', 'EPITAPHS & TRIBUTES', '3-LETTER WORDS', 'HISTORY', \"ESPN's TOP 10 ALL-TIME ATHLETES\", 'EVERYBODY TALKS ABOUT IT...', 'THE COMPANY LINE'\n$ Value       &lt;str&gt; '$200 ', '$200 ', '$200 ', '$200 ', '$200 ', '$200 ', '$400 ', '$400 ', '$400 ', '$400 '\n$ Question    &lt;str&gt; \"For the last 8 years of his life, Galileo was under house arrest for espousing this man's theory\", 'No. 2: 1912 Olympian; football star at Carlisle Indian School; 6 MLB seasons with the Reds, Giants & Braves', 'The city of Yuma in this state has a record average of 4,055 hours of sunshine each year', 'In 1963, live on \"The Art Linkletter Show\", this company served its billionth burger', 'Signer of the Dec. of Indep., framer of the Constitution of Mass., second President of the United States', 'In the title of an Aesop fable, this insect shared billing with a grasshopper', \"Built in 312 B.C. to link Rome & the South of Italy, it's still in use today\", 'No. 8: 30 steals for the Birmingham Barons; 2,306 steals for the Bulls', 'In the winter of 1971-72, a record 1,122 inches of snow fell at Rainier Paradise Ranger Station in this state', 'This housewares store was named for the packaging its merchandise came in & was first displayed on'\n$ Answer      &lt;str&gt; 'Copernicus', 'Jim Thorpe', 'Arizona', \"McDonald's\", 'John Adams', 'the ant', 'the Appian Way', 'Michael Jordan', 'Washington', 'Crate & Barrel'\n\nNone\n\n\n\nThis is similar to the str() function in R.\n\nTo print a list of the data types of each variable, you can use:\n\nprint(df.dtypes)\n\n[Int64, String, String, String, String, String, String]\n\n\nBut the printing of a Polars data frame already gives you this information (along with the shape).\nThe schema of a Polars data frame sets the names of the variables (columns) and their data types:\n\nprint(df.schema)\n\nSchema({'Show Number': Int64, 'Air Date': String, 'Round': String, 'Category': String, 'Value': String, 'Question': String, 'Answer': String})",
    "crumbs": [
      "Python",
      "<b><em>Faster Python</em></b>",
      "Polars: faster data frames",
      "Data frame inspection"
    ]
  },
  {
    "objectID": "python/polars_inspection.html#summary-statistics",
    "href": "python/polars_inspection.html#summary-statistics",
    "title": "Data frame inspection",
    "section": "Summary statistics",
    "text": "Summary statistics\nThis is not always meaningful depending on your data:\n\nprint(df.describe())\n\nshape: (9, 8)\n┌────────────┬────────────┬──────────┬────────────┬────────────┬─────────┬────────────┬────────────┐\n│ statistic  ┆ Show       ┆ Air Date ┆ Round      ┆ Category   ┆ Value   ┆ Question   ┆ Answer     │\n│ ---        ┆ Number     ┆ ---      ┆ ---        ┆ ---        ┆ ---     ┆ ---        ┆ ---        │\n│ str        ┆ ---        ┆ str      ┆ str        ┆ str        ┆ str     ┆ str        ┆ str        │\n│            ┆ f64        ┆          ┆            ┆            ┆         ┆            ┆            │\n╞════════════╪════════════╪══════════╪════════════╪════════════╪═════════╪════════════╪════════════╡\n│ count      ┆ 216930.0   ┆ 216930   ┆ 216930     ┆ 216778     ┆ 216930  ┆ 216928     ┆ 216898     │\n│ null_count ┆ 0.0        ┆ 0        ┆ 0          ┆ 152        ┆ 0       ┆ 2          ┆ 32         │\n│ mean       ┆ 4264.23851 ┆ null     ┆ null       ┆ null       ┆ null    ┆ null       ┆ null       │\n│            ┆ 9          ┆          ┆            ┆            ┆         ┆            ┆            │\n│ std        ┆ 1386.29633 ┆ null     ┆ null       ┆ null       ┆ null    ┆ null       ┆ null       │\n│            ┆ 5          ┆          ┆            ┆            ┆         ┆            ┆            │\n│ min        ┆ 1.0        ┆ 1/1/01   ┆ Double     ┆ A JIM      ┆ $1,000  ┆ \" 'Cause   ┆  Hamlet    │\n│            ┆            ┆          ┆ Jeopardy!  ┆ CARREY     ┆         ┆ I'm never  ┆            │\n│            ┆            ┆          ┆            ┆ FILM       ┆         ┆ gonna stop ┆            │\n│            ┆            ┆          ┆            ┆ FESTIVAL   ┆         ┆ …          ┆            │\n│ 25%        ┆ 3349.0     ┆ null     ┆ null       ┆ null       ┆ null    ┆ null       ┆ null       │\n│ 50%        ┆ 4490.0     ┆ null     ┆ null       ┆ null       ┆ null    ┆ null       ┆ null       │\n│ 75%        ┆ 5393.0     ┆ null     ┆ null       ┆ null       ┆ null    ┆ null       ┆ null       │\n│ max        ┆ 6300.0     ┆ 9/9/99   ┆ Tiebreaker ┆ â€™70s     ┆ None    ┆ â€œYou     ┆ â€œone     │\n│            ┆            ┆          ┆            ┆ CINEMA     ┆         ┆ Can't Lose ┆ giant leap │\n│            ┆            ┆          ┆            ┆            ┆         ┆ Meâ€       ┆ for        │\n│            ┆            ┆          ┆            ┆            ┆         ┆            ┆ mankindâ…  │\n└────────────┴────────────┴──────────┴────────────┴────────────┴─────────┴────────────┴────────────┘",
    "crumbs": [
      "Python",
      "<b><em>Faster Python</em></b>",
      "Polars: faster data frames",
      "Data frame inspection"
    ]
  },
  {
    "objectID": "python/nlp_processing.html",
    "href": "python/nlp_processing.html",
    "title": "Text processing",
    "section": "",
    "text": "In this section, we will use the TextBlob package for part of speech tagging and basic tokenization.",
    "crumbs": [
      "Python",
      "<b><em>Text analysis</em></b>",
      "Text processing"
    ]
  },
  {
    "objectID": "python/nlp_processing.html#textblob",
    "href": "python/nlp_processing.html#textblob",
    "title": "Text processing",
    "section": "TextBlob",
    "text": "TextBlob\nTextBlob is the NLP package that we will use in this course for tagging, tokenization, normalization, and sentiment analysis.\nWe first need to load it in our session:\n\nfrom textblob import TextBlob\n\n\n---------------------------------------------------------------------------\nModuleNotFoundError                       Traceback (most recent call last)\nCell In[2], line 1\n----&gt; 1 from textblob import TextBlob\n\nModuleNotFoundError: No module named 'textblob'\n\n\n\nBefore we can use TextBlob on our text, we need to convert the page1 string into a TextBlob object:\n\ntext = TextBlob(page1)\ntype(text)\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[3], line 1\n----&gt; 1 text = TextBlob(page1)\n      2 type(text)\n\nNameError: name 'TextBlob' is not defined",
    "crumbs": [
      "Python",
      "<b><em>Text analysis</em></b>",
      "Text processing"
    ]
  },
  {
    "objectID": "python/nlp_processing.html#part-of-speech-tagging",
    "href": "python/nlp_processing.html#part-of-speech-tagging",
    "title": "Text processing",
    "section": "Part of speech tagging",
    "text": "Part of speech tagging\nPart of speech tagging attributes parts of speech (POS) tags to each word of a text.\nYou can do this simply by using the tags property on a TextBlob object: text.tags. Because there are a lot of words in the first pdf page, this would create a very long output.\nThe result is a list:\n\ntype(text.tags)\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[4], line 1\n----&gt; 1 type(text.tags)\n\nNameError: name 'text' is not defined\n\n\n\nAnd each element of the list is a tuple:\n\ntype(text.tags[0])\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[5], line 1\n----&gt; 1 type(text.tags[0])\n\nNameError: name 'text' is not defined\n\n\n\nWe don’t have to print the full list. Let’s only print the first 20 tuples:\n\ntext.tags[:20]\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[6], line 1\n----&gt; 1 text.tags[:20]\n\nNameError: name 'text' is not defined\n\n\n\n\n\n\n\n\n\nTagset from the University of Pennsylvania as reference\n\n\n\n\n\n\n\n\nTag\nDescription\n\n\n\n\nCC\nCoordinating conjunction\n\n\nCD\nCardinal number\n\n\nDT\nDeterminer\n\n\nEX\nExistential there\n\n\nFW\nForeign word\n\n\nIN\nPreposition or subordinating conjunction\n\n\nJJ\nAdjective\n\n\nJJR\nAdjective, comparative\n\n\nJJS\nAdjective, superlative\n\n\nLS\nList item marker\n\n\nMD\nModal\n\n\nNN\nNoun, singular or mass\n\n\nNNS\nNoun, plural\n\n\nNNP\nProper noun, singular\n\n\nNNPS\nProper noun, plural\n\n\nPDT\nPredeterminer\n\n\nPOS\nPossessive ending\n\n\nPRP\nPersonal pronoun\n\n\nPRP$\nPossessive pronoun\n\n\nRB\nAdverb\n\n\nRBR\nAdverb, comparative\n\n\nRBS\nAdverb, superlative\n\n\nRP\nParticle\n\n\nSYM\nSymbol\n\n\nTO\nto\n\n\nUH\nInterjection\n\n\nVB\nVerb, base form\n\n\nVBD\nVerb, past tense\n\n\nVBG\nVerb, gerund or present participle\n\n\nVBN\nVerb, past participle\n\n\nVBP\nVerb, non-3rd person singular present\n\n\nVBZ\nVerb, 3rd person singular present\n\n\nWDT\nWh-determiner\n\n\nWP\nWh-pronoun\n\n\nWP$\nPossessive wh-pronoun\n\n\nWRB\nWh-adverb",
    "crumbs": [
      "Python",
      "<b><em>Text analysis</em></b>",
      "Text processing"
    ]
  },
  {
    "objectID": "python/nlp_processing.html#noun-phrases-extraction",
    "href": "python/nlp_processing.html#noun-phrases-extraction",
    "title": "Text processing",
    "section": "Noun phrases extraction",
    "text": "Noun phrases extraction\nNoun phrases can be extracted with the noun_phrases property:\n\nprint(text.noun_phrases)\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[7], line 1\n----&gt; 1 print(text.noun_phrases)\n\nNameError: name 'text' is not defined\n\n\n\nThe output is a WordList object:\n\ntype(text.noun_phrases)\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[8], line 1\n----&gt; 1 type(text.noun_phrases)\n\nNameError: name 'text' is not defined",
    "crumbs": [
      "Python",
      "<b><em>Text analysis</em></b>",
      "Text processing"
    ]
  },
  {
    "objectID": "python/nlp_processing.html#tokenization",
    "href": "python/nlp_processing.html#tokenization",
    "title": "Text processing",
    "section": "Tokenization",
    "text": "Tokenization\n\nWords\nTextBlob allows to extract words easily with the words attribute:\n\nprint(text.words)\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[9], line 1\n----&gt; 1 print(text.words)\n\nNameError: name 'text' is not defined\n\n\n\n\n\nYour turn:\n\nHow many words are there in the first pdf page of Wyrd Sisters?\n\n\n\nSentences\nExtracting sentences is just as easy with the sentences attribute.\nLet’s extract the first 10 sentences:\n\ntext.sentences[:10]\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[10], line 1\n----&gt; 1 text.sentences[:10]\n\nNameError: name 'text' is not defined\n\n\n\nThe output is however quite ugly. We could make this a lot more readable by printing each sentence separated by a blank line:\n\nfor s in text.sentences[:10]:\n    print(s)\n    print(\"\\n\")\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[11], line 1\n----&gt; 1 for s in text.sentences[:10]:\n      2     print(s)\n      3     print(\"\\n\")\n\nNameError: name 'text' is not defined\n\n\n\n\nIn Python strings (as in many other languages), \"\\n\" represents a new line.\n\nOr you could add lines of hyphens between the sentences:\n\nfor s in text.sentences[:10]:\n    print(s)\n    print(\"-\" * 100)\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[12], line 1\n----&gt; 1 for s in text.sentences[:10]:\n      2     print(s)\n      3     print(\"-\" * 100)\n\nNameError: name 'text' is not defined\n\n\n\n\n\nYour turn:\n\n\nWhat is the type of text.sentences?\n\nCould you print just the 5th sentence?\n\nJust the last sentence?",
    "crumbs": [
      "Python",
      "<b><em>Text analysis</em></b>",
      "Text processing"
    ]
  },
  {
    "objectID": "python/nlp_processing.html#word-counts",
    "href": "python/nlp_processing.html#word-counts",
    "title": "Text processing",
    "section": "Word counts",
    "text": "Word counts\nWe already saw that we can extract words with the words attribute. Now, we can add the count method to get the frequency of specific words.\n\ntext.words.count(\"gods\")\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[13], line 1\n----&gt; 1 text.words.count(\"gods\")\n\nNameError: name 'text' is not defined",
    "crumbs": [
      "Python",
      "<b><em>Text analysis</em></b>",
      "Text processing"
    ]
  },
  {
    "objectID": "python/nlp_install.html",
    "href": "python/nlp_install.html",
    "title": "Package installation",
    "section": "",
    "text": "In this first section, we will install the packages needed for this course.",
    "crumbs": [
      "Python",
      "<b><em>Text analysis</em></b>",
      "Package installation"
    ]
  },
  {
    "objectID": "python/nlp_install.html#python-packages",
    "href": "python/nlp_install.html#python-packages",
    "title": "Package installation",
    "section": "Python packages",
    "text": "Python packages\nPython can do a lot out of the box, but for specialized tasks, you need to install additional packages. Packages contain additional functions and variables. Some packages also contain data.\nYou can find Python packages in the Python Package Index (PyPI)—a repository of open source packages.\nTo know which packages to use, you can talk with colleagues or look at the literature to see what people in your community use.",
    "crumbs": [
      "Python",
      "<b><em>Text analysis</em></b>",
      "Package installation"
    ]
  },
  {
    "objectID": "python/nlp_install.html#package-managers",
    "href": "python/nlp_install.html#package-managers",
    "title": "Package installation",
    "section": "Package managers",
    "text": "Package managers\nThere are many ways to install Python packages.\nFor instance, people who need to use the Digital Research Alliance of Canada supercomputers have to use pip: the package installer for Python. This is the most lean and efficient fashion to manage Python packages.\nAn alternative to pip is conda: a package and environment manager for Python and other languages.\nFinally, the most convenient (but also the most bloated) option is to use Anaconda. This is what you have been using so far, so we will stick to the same workflow.\nAnaconda is a big project that comes with Python and a suite of packages needed for science. It also provides conda, as well as a graphical interface to manage your environments and packages. It makes things very easy for you, but it is also a very big and will take time to install and use up a lot of space.\nThe getting started with Navigator page of the Anaconda documentation goes over the steps necessary to create a new Python environment and install packages.\nFirst, launch the Navigator.",
    "crumbs": [
      "Python",
      "<b><em>Text analysis</em></b>",
      "Package installation"
    ]
  },
  {
    "objectID": "python/nlp_install.html#python-environments",
    "href": "python/nlp_install.html#python-environments",
    "title": "Package installation",
    "section": "Python environments",
    "text": "Python environments\nWhy do we need to create a Python environment?\nThere are a very large number of Python packages and some might conflict with each other. They may also have conflicting dependency requirements.\n\nFor instance, package A might need package B at version 3.4 (package B is called a dependency of package A—it is not a package you install explicitly, but it needs to be installed before package A can work), while package C requires package B at version 2.8. Dependencies get automatically installed when you install packages, but if you want to use package A and C together, you run into conflicts. Having different environments for different projects allows to only have the few packages that you need for each project in each environment and makes the whole situation a lot simpler.\n\nFollowing the Anaconda documentation, let’s create a new environment called text and activate it.",
    "crumbs": [
      "Python",
      "<b><em>Text analysis</em></b>",
      "Package installation"
    ]
  },
  {
    "objectID": "python/nlp_install.html#installing-packages",
    "href": "python/nlp_install.html#installing-packages",
    "title": "Package installation",
    "section": "Installing packages",
    "text": "Installing packages\nNow, we can install packages in our environment, still using the Anaconda Navigator.\nThe packages you need for this course are:\n\ntextblob\nrequests\npymupdf\n\nFollowing the Anaconda instructions, look for, then install each of these packages.\nYou are now ready for this course.",
    "crumbs": [
      "Python",
      "<b><em>Text analysis</em></b>",
      "Package installation"
    ]
  },
  {
    "objectID": "python/nlp_analysis.html",
    "href": "python/nlp_analysis.html",
    "title": "Sentiment analysis",
    "section": "",
    "text": "One of the common tasks of natural language processing is sentiment analysis. Let’s see how it works with TextBlob.",
    "crumbs": [
      "Python",
      "<b><em>Text analysis</em></b>",
      "Sentiment analysis"
    ]
  },
  {
    "objectID": "python/nlp_analysis.html#the-data",
    "href": "python/nlp_analysis.html#the-data",
    "title": "Sentiment analysis",
    "section": "The data",
    "text": "The data\nIt wouldn’t make much sense to do sentiment analysis on the first pdf page of Wyrd Sisters. Instead, let’s use some Goodreads reviews of Wyrd Sisters.\nThe method is as easy as everything we have seen so far: you simply use the sentiment attribute of a TextBlob object and you get a named tuple with the polarity on a continuous scale from -1 to +1 and a subjectivity ranging from 0 to 1.\nWe could get very fancy, scrape the site and analyse all the reviews, but this is not the purpose of this course. Instead, we will simply copy and paste a few reviews in the TextBlob function to turn them into TextBlob objects and look at their sentiment attributes.\nIf TextBlob does a good job at analysing those reviews, we should get a polarity close to 1 for four and five-star reviews and a polarity close to -1 for one-star reviews.",
    "crumbs": [
      "Python",
      "<b><em>Text analysis</em></b>",
      "Sentiment analysis"
    ]
  },
  {
    "objectID": "python/nlp_analysis.html#results",
    "href": "python/nlp_analysis.html#results",
    "title": "Sentiment analysis",
    "section": "Results",
    "text": "Results\n\nBe careful that you have to remove all end of lines when you paste the reviews into your code to make sure that you create a Python string.\n\nLet’s create a string with one of the four-star reviews:\n\nreview1 = \"How have I never read Terry Pratchett before? He's like ... Shakespeare and Wodehouse and Monty Python all wrapped into one! A student gave me this book while we were studying Macbeth in class. Wyrd Sisters is a sort of parallel story, which manages to poke fun at the play, revere the play, make inside jokes about the play, and ... well, generally turn the play on its head. All the while, you, the reader, get to feel very smart and superior for getting all the jokes and allusions. And yet it manages to avoid being gimmicky. It really is a good story with good characters, too. This is no Life of Brian where the story itself matters less than the hilarity of the parody. Wyrd Sisters may draw a good deal of life from Macbeth, but its real liveliness comes from Pratchett's skilled characterizations of a regicidal Duke, his murderess Dutchess, their depressed Fool, and three very colorful witches. Oh, it's just genius. My only problem is figuring out what Pratchett novel to read next ... he's dauntingly prolific!\"\n\nWe need to turn the string into a TextBlob object:\n\nr1 = TextBlob(review1)\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[3], line 1\n----&gt; 1 r1 = TextBlob(review1)\n\nNameError: name 'TextBlob' is not defined\n\n\n\nAnd now we can see the result of the sentiment analysis:\n\nprint(r1.sentiment)\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[4], line 1\n----&gt; 1 print(r1.sentiment)\n\nNameError: name 'r1' is not defined\n\n\n\nThe result is a bit far from 1 for a four-star review. Maybe we can do better. TextBlob can use a more fancy approach for sentiment analysis based on a naive Bayes analyzer from NLTK trained on a movie reviews corpus. Let’s see if we get better results.\nFirst, we need to load the module:\n\nfrom textblob.sentiments import NaiveBayesAnalyzer\n\n\n---------------------------------------------------------------------------\nModuleNotFoundError                       Traceback (most recent call last)\nCell In[5], line 1\n----&gt; 1 from textblob.sentiments import NaiveBayesAnalyzer\n\nModuleNotFoundError: No module named 'textblob'\n\n\n\nThen we run the analysis:\n\nr1b = TextBlob(review1, analyzer=NaiveBayesAnalyzer())\nprint(r1b.sentiment)\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[6], line 1\n----&gt; 1 r1b = TextBlob(review1, analyzer=NaiveBayesAnalyzer())\n      2 print(r1b.sentiment)\n\nNameError: name 'TextBlob' is not defined\n\n\n\nIt looks a lot better!\nLet’s try on a five-star review this time:\n\nreview2 = \"A veritable smorgasbord of Shakespeare references sees the 6th Discworld novel come to life, dragging the two most prominent witches (Granny and Nanny) and their occult-leaning protege, Magrat, though thickets and thick-cities alike as they attempt to make sure fate happens. Maybe with a few encouraging prods along the way… A mixture of Macbeth and Hamlet with a little dollop of King Lear thrown in, along with many other Shakespeare nonsense that doesn’t stand out right way, Wyrd Sisters is one of PTerry’s very early masterpieces. His hands and head seem to meld together as one, where his imagination is not shackled by the human-only speed of his writing (or typing) or the darned debilitation of the English language.The plot simmers nicely, following and then not following the true path of how this kind of story goes. It’s so nice to see the Discworld countryside getting a deep look-in, and how well it contrasts with city-life, but also manages to co-exist very peacefully (as long as the city stays far away from the countryside, thank you). Whilst previously we visited vast open vistas on the Disc, these close-quarter scenes are even more brimful of life and curiosity and share yet another facet to not only PTerry’s imagination, but the world he has created. One of the best things about PTerry (and of any comedy at all) is that, whilst he does take the mickey out of Shakespeare and fantasy and all those things, he does it with such love and reverence that it shines through and makes the humour that much more poignant and, well, funny. (My previous review of this had me not enjoying it and I’m actually pretty baffled by that. I loved every single minute of this re-read and it’s curious how your tastes and feelings can change even after only a few years. I’ll leave the old review below just for the sake of it: Wyrd Sisters is the second of the Witch mini-series, in the ever popular Discworld series. Equal Rites was the first and we were introduced to one of the greatest characters of all-time: Granny Weatherwax. Wyrd Sisters brings two more witches-and mentions of many others-in to fray: Nanny Ogg, Granny's best friend, and Magrat Garlick, a new-wave witch who thinks jangling jewellery and occult symbols makes you a better witch. Adding two new witches alongside Granny just emphasises how cantankerous, stubborn and bloody brilliant she is. Even they can't deny that she's the best. She is tolerated most of the time, but there's always an underlying current of total respect, in the same way you respect your grandparents because they lived through the war, even if they do still say \\\"does anyone want to get a Chinky?\\\" The plot is Shakespearean-Macbeth in particular-and takes many plot points from that, as well as a lot of the quotes. It's a wonderful juxtaposition of Discworld nonsense and Shakespearean tragedy that is twisted with unique Pratchett humour. It is written much the same way all the early Discworld books were. Very well, hardly any technical faults and smatterings of Pratchett humour. Despite the wonderful Granny, the amusing Nanny and the Straightforward but naive Magrat, and my love for all the Discworld witches, I couldn't enjoy this as much as I wanted. It was funny in a tittering kind of way, and the plot was interesting, but it never quite held my attention. I never felt like I wanted to read it all the time, or try and finish reading it. It took me quite a while to get through it (for other reasons I won't go in to) but it never really held me enough to want it. Still a better love story than Twilight.)\"\n\nr2 = TextBlob(review2)\nr2b = TextBlob(review2, analyzer=NaiveBayesAnalyzer())\n\nprint(r2.sentiment)\nprint(r2b.sentiment)\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[7], line 3\n      1 review2 = \"A veritable smorgasbord of Shakespeare references sees the 6th Discworld novel come to life, dragging the two most prominent witches (Granny and Nanny) and their occult-leaning protege, Magrat, though thickets and thick-cities alike as they attempt to make sure fate happens. Maybe with a few encouraging prods along the way… A mixture of Macbeth and Hamlet with a little dollop of King Lear thrown in, along with many other Shakespeare nonsense that doesn’t stand out right way, Wyrd Sisters is one of PTerry’s very early masterpieces. His hands and head seem to meld together as one, where his imagination is not shackled by the human-only speed of his writing (or typing) or the darned debilitation of the English language.The plot simmers nicely, following and then not following the true path of how this kind of story goes. It’s so nice to see the Discworld countryside getting a deep look-in, and how well it contrasts with city-life, but also manages to co-exist very peacefully (as long as the city stays far away from the countryside, thank you). Whilst previously we visited vast open vistas on the Disc, these close-quarter scenes are even more brimful of life and curiosity and share yet another facet to not only PTerry’s imagination, but the world he has created. One of the best things about PTerry (and of any comedy at all) is that, whilst he does take the mickey out of Shakespeare and fantasy and all those things, he does it with such love and reverence that it shines through and makes the humour that much more poignant and, well, funny. (My previous review of this had me not enjoying it and I’m actually pretty baffled by that. I loved every single minute of this re-read and it’s curious how your tastes and feelings can change even after only a few years. I’ll leave the old review below just for the sake of it: Wyrd Sisters is the second of the Witch mini-series, in the ever popular Discworld series. Equal Rites was the first and we were introduced to one of the greatest characters of all-time: Granny Weatherwax. Wyrd Sisters brings two more witches-and mentions of many others-in to fray: Nanny Ogg, Granny's best friend, and Magrat Garlick, a new-wave witch who thinks jangling jewellery and occult symbols makes you a better witch. Adding two new witches alongside Granny just emphasises how cantankerous, stubborn and bloody brilliant she is. Even they can't deny that she's the best. She is tolerated most of the time, but there's always an underlying current of total respect, in the same way you respect your grandparents because they lived through the war, even if they do still say \\\"does anyone want to get a Chinky?\\\" The plot is Shakespearean-Macbeth in particular-and takes many plot points from that, as well as a lot of the quotes. It's a wonderful juxtaposition of Discworld nonsense and Shakespearean tragedy that is twisted with unique Pratchett humour. It is written much the same way all the early Discworld books were. Very well, hardly any technical faults and smatterings of Pratchett humour. Despite the wonderful Granny, the amusing Nanny and the Straightforward but naive Magrat, and my love for all the Discworld witches, I couldn't enjoy this as much as I wanted. It was funny in a tittering kind of way, and the plot was interesting, but it never quite held my attention. I never felt like I wanted to read it all the time, or try and finish reading it. It took me quite a while to get through it (for other reasons I won't go in to) but it never really held me enough to want it. Still a better love story than Twilight.)\"\n----&gt; 3 r2 = TextBlob(review2)\n      4 r2b = TextBlob(review2, analyzer=NaiveBayesAnalyzer())\n      6 print(r2.sentiment)\n\nNameError: name 'TextBlob' is not defined\n\n\n\nHere too, the naive Bayesian classifier performs a lot better.\nNow, let’s see what we get for a one-star review:\n\nreview3 = \"I struggled finishing the book as I lost interest.\"\n\nr3 = TextBlob(review3)\nr3b = TextBlob(review3, analyzer=NaiveBayesAnalyzer())\n\nprint(r3.sentiment)\nprint(r3b.sentiment)\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[8], line 3\n      1 review3 = \"I struggled finishing the book as I lost interest.\"\n----&gt; 3 r3 = TextBlob(review3)\n      4 r3b = TextBlob(review3, analyzer=NaiveBayesAnalyzer())\n      6 print(r3.sentiment)\n\nNameError: name 'TextBlob' is not defined\n\n\n\nWell… 🙁 Frankly, both models performed poorly here.\nLet’s try another one-star review:\n\nreview4 = \"I did not enjoy this book at all. Slow and tedious. It had some funny bits in the beginning, but I struggled to finish it.\"\n\nr4 = TextBlob(review4)\nr4b = TextBlob(review4, analyzer=NaiveBayesAnalyzer())\n\nprint(r4.sentiment)\nprint(r4b.sentiment)\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[9], line 3\n      1 review4 = \"I did not enjoy this book at all. Slow and tedious. It had some funny bits in the beginning, but I struggled to finish it.\"\n----&gt; 3 r4 = TextBlob(review4)\n      4 r4b = TextBlob(review4, analyzer=NaiveBayesAnalyzer())\n      6 print(r4.sentiment)\n\nNameError: name 'TextBlob' is not defined\n\n\n\nHere again, both models perform poorly, but the Bayesian model does even worse than the default pattern analyzer.\nLet’s do a few more one-star reviews:\n\nreview5 = \"It doesn't get much more boring than that. Zero improvement from Equal Rights, a complete snore-fest, poorly paced and with a clear lack of stakes and humour. I quit Discworld.\"\n\nr5 = TextBlob(review5)\nr5b = TextBlob(review5, analyzer=NaiveBayesAnalyzer())\n\nprint(r5.sentiment)\nprint(r5b.sentiment)\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[10], line 3\n      1 review5 = \"It doesn't get much more boring than that. Zero improvement from Equal Rights, a complete snore-fest, poorly paced and with a clear lack of stakes and humour. I quit Discworld.\"\n----&gt; 3 r5 = TextBlob(review5)\n      4 r5b = TextBlob(review5, analyzer=NaiveBayesAnalyzer())\n      6 print(r5.sentiment)\n\nNameError: name 'TextBlob' is not defined\n\n\n\n\nreview6 = \"Hated this book. To be fair, not my style. Only read parts of it because it was assigned for a book club.\"\n\nr6 = TextBlob(review6)\nr6b = TextBlob(review6, analyzer=NaiveBayesAnalyzer())\n\nprint(r6.sentiment)\nprint(r6b.sentiment)\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[11], line 3\n      1 review6 = \"Hated this book. To be fair, not my style. Only read parts of it because it was assigned for a book club.\"\n----&gt; 3 r6 = TextBlob(review6)\n      4 r6b = TextBlob(review6, analyzer=NaiveBayesAnalyzer())\n      6 print(r6.sentiment)\n\nNameError: name 'TextBlob' is not defined\n\n\n\n\nreview7 = \"Dnf - boring\"\n\nr7 = TextBlob(review7)\nr7b = TextBlob(review7, analyzer=NaiveBayesAnalyzer())\n\nprint(r7.sentiment)\nprint(r7b.sentiment)\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[12], line 3\n      1 review7 = \"Dnf - boring\"\n----&gt; 3 r7 = TextBlob(review7)\n      4 r7b = TextBlob(review7, analyzer=NaiveBayesAnalyzer())\n      6 print(r7.sentiment)\n\nNameError: name 'TextBlob' is not defined\n\n\n\nThese fared better, although, the results are not exactly impressive.",
    "crumbs": [
      "Python",
      "<b><em>Text analysis</em></b>",
      "Sentiment analysis"
    ]
  },
  {
    "objectID": "python/nlp_analysis.html#rule-based-vs-data-based-nlp",
    "href": "python/nlp_analysis.html#rule-based-vs-data-based-nlp",
    "title": "Sentiment analysis",
    "section": "Rule-based vs data-based NLP",
    "text": "Rule-based vs data-based NLP\nTextBlob (as well as NLTK on which it is built) uses a rule-based or lexicon-based approach to NLP. This is an old technique that requires linguistic knowledge, but is computationally basic. As we saw, it has many limitations.\nThe recent successes of large language models (LLMs) have shown unequivocally that AI algorithms fed vast amounts of data do a much better job. They do not need to be programmed explicitly since they learn by experience. They do however require a lot of computing power and data for training.\nSuch models are built from multiple complex artificial neural networks trained using machine learning frameworks such as PyTorch or JAX.\nAs Frederick Jelinek is famously often quoted for saying (although what he said was probably slightly different):\n\nEvery time I fire a linguist, the performance of the speech recognizer goes up.\n\n\nI will quickly demo how much better any of the LLMs out there does at this task.",
    "crumbs": [
      "Python",
      "<b><em>Text analysis</em></b>",
      "Sentiment analysis"
    ]
  },
  {
    "objectID": "python/intro_run.html",
    "href": "python/intro_run.html",
    "title": "Running Python",
    "section": "",
    "text": "This section covers some of the many ways to run Python either on your machine or on the Alliance clusters.",
    "crumbs": [
      "Python",
      "<b><em>Getting started</em></b>",
      "Running Python"
    ]
  },
  {
    "objectID": "python/intro_run.html#on-your-machine",
    "href": "python/intro_run.html#on-your-machine",
    "title": "Running Python",
    "section": "On your machine",
    "text": "On your machine\n\nPython script\nYou can write your Python code in a text file with a .py extension and run the script in your terminal with:\npython &lt;script-name&gt;.py\nThis will execute the code non-interactively.\n\n\nInteractive Python\nOne key reason why Python is so popular is that it is an interpreted language: you can use it in an interactive session that makes prototyping code very friendly.\n\nPython shell\nThe simplest (and driest) way to run Python interactively is to use the Python shell. If you launch Python by double-clicking on the Python executable or running the command python in a terminal, you end up in the Python shell with its typical &gt;&gt;&gt; prompt.\n\n\nIPython\nIPython is an improved shell with better performance and more functionality (e.g. colour-coding, magic commands).\nYou can install IPython and launch it instead of launching Python by running ipython in your terminal.\n\nNow that everybody is using Jupyter (see below), using the IPython shell directly has fallen out of fashion, but it is actually my favourite method to run Python. It is also the most efficient way to run Python interactively.\n\n\n\nJupyter\nThe IPython shell was integrated into a fancy interface, the Jupyter notebook. This later lead to a fully fledged IDE (integrated development environment) called JupyterLab which contains notebooks, a command line, a file explorer, and other functionality.\nYou can install JupyterLab, launch it by running jupyter lab in a terminal and the IDE will open in your browser.\n\nEven though JupyterLab runs in your browser, it does not use internet: it is all run locally.\n\n\n\nOther IDEs\nJupyter has probably become the most popular IDE, but it is possible to run Python in other IDE such as Emacs.\n\n\nQuarto\nThe very popular RMarkdown developed by Posit (formerly RStudio Inc) lead to a new and more powerful tool called Quarto. Quarto runs code blocks of R, Julia, and Python in markdown documents which can be rendered into websites, pdfs, presentations, and more. This website is built with Quarto.\nUnder the hood, Quarto runs Jupyter notebooks, so it is in fact IPython running in Jupyter that executes the Python code in Quarto.",
    "crumbs": [
      "Python",
      "<b><em>Getting started</em></b>",
      "Running Python"
    ]
  },
  {
    "objectID": "python/intro_run.html#on-alliance-clusters",
    "href": "python/intro_run.html#on-alliance-clusters",
    "title": "Running Python",
    "section": "On Alliance clusters",
    "text": "On Alliance clusters\n\nPython script\nYou can SSH into an Alliance cluster, load the Python module with the Python version of your choice (use module spider python to find the module, then module load to load it), write a Python script, an sbatch script and run your code with a batch job as you saw in our HPC course.\n\n\nInteractive Python\n\nPython shell\nSimilarly, if you SSH to a cluster, load the Python module of your choice, then launch an interactive job with salloc, you can run the Python shell.\n\n\nIPython\nTo use IPython, load the IPython module of your choice (module spider ipython to find it, module load to load it), launch an interactive job with salloc, and finally launch the IPython shell by running ipython in the terminal.\n\n\nJupyter\nTo use JupyterLab on a cluster, you use what is called a JupyterHub: a set of tools that spawn and manage multiple instances of JupyterLab servers. Under the hood, they manage an interactive job used by your JupyterLab server.\nLet’s try it on our training cluster:\n\ngo to https://jupyter.school.c3.ca,\nsign in with the username and password for our summer school,\nleave OTP blank,\nset the server options according to the image below:\n\n\n\nThese are the only values that you should edit:\nChange the time to 3.0\n\n\npress start.\n\n\nNote that, unlike other JupyterHubs you might have used (e.g. Syzygy), this JupyterHub is not permanent and will be destroyed at the end of the summer school.\n\nIf you don’t need all the time you asked for after all, it is a great thing to log out (the resources you are using on this cluster are shared amongst many people and when resources are allocated to you, they aren’t available to other people. So it is a good thing not to ask for unnecessary resources and have them sit idle when others could be using them).\nTo log out, click on “File” in the top menu and select “Log out” at the very bottom.\nIf you would like to make a change to the information you entered on the server option page after you have pressed “start”, log out in the same way, log back in, edit the server options, and press start again.\n\n\n\nStart a Python notebook\nTo start a Jupyter notebook with the Python kernel, click on the button “Python 3” in the “Notebook” section (top row of buttons).",
    "crumbs": [
      "Python",
      "<b><em>Getting started</em></b>",
      "Running Python"
    ]
  },
  {
    "objectID": "python/intro_packages.html",
    "href": "python/intro_packages.html",
    "title": "Modules, packages, and libraries",
    "section": "",
    "text": "“Modules” are Python files containing reusable code (e.g. functions, constants, utilities).\n“Packages” are collections of modules.\n“Libraries”, technically, are collections of packages, although “packages” and “libraries” are often used loosely and interchangeably in Python.",
    "crumbs": [
      "Python",
      "<b><em>Getting started</em></b>",
      "Modules and libraries"
    ]
  },
  {
    "objectID": "python/intro_packages.html#definitions",
    "href": "python/intro_packages.html#definitions",
    "title": "Modules, packages, and libraries",
    "section": "",
    "text": "“Modules” are Python files containing reusable code (e.g. functions, constants, utilities).\n“Packages” are collections of modules.\n“Libraries”, technically, are collections of packages, although “packages” and “libraries” are often used loosely and interchangeably in Python.",
    "crumbs": [
      "Python",
      "<b><em>Getting started</em></b>",
      "Modules and libraries"
    ]
  },
  {
    "objectID": "python/intro_packages.html#installing-packages-on-your-machine",
    "href": "python/intro_packages.html#installing-packages-on-your-machine",
    "title": "Modules, packages, and libraries",
    "section": "Installing packages on your machine",
    "text": "Installing packages on your machine\nYou can install external packages containing additional functions, constants, and utilities to extend the capabilities of Python.\nThe Python Package Index is a public repository of open source packages contributed by users.\nInstallation of packages can be done via pip.\nInstead of installing packages system wide or for your user, you can create a semi-isolated Python environment in which you install the packages needed for a particular project. This makes reproducibility and collaboration easier. It also helps handle dependency conflicts. Some Linux distributions will not let you use pip outside a virtual environment anymore. It is a great practice to always use virtual environments.\nCreate a Python virtual environment called env:\npython -m venv ~/env\nActivate it:\nsource ~/env/bin/activate\nUpdate pip:\npython -m pip install --upgrade pip\nInstall packages:\npython -m pip install &lt;package&gt;\nOn your local machine, particularly if you are on Windows and want to install a complex software stack, conda can makes things easy by installing from the Anaconda Distribution.",
    "crumbs": [
      "Python",
      "<b><em>Getting started</em></b>",
      "Modules and libraries"
    ]
  },
  {
    "objectID": "python/intro_packages.html#installing-packages-on-the-clusters",
    "href": "python/intro_packages.html#installing-packages-on-the-clusters",
    "title": "Modules, packages, and libraries",
    "section": "Installing packages on the clusters",
    "text": "Installing packages on the clusters\nDon’t use conda or Anaconda on the Alliance clusters. If you really must, do it in a container with Apptainer.\nOn the Alliance clusters, install packages inside a virtual environment and use Python wheels whenever possible.\nYou can see whether a wheel is available with avail_wheels &lt;package&gt; or look at the list of available wheels. To install from wheels instead of downloading from PyPI, add the --no-index flag to the install command.\nAdvantages of wheels:\n\ncompiled for the clusters hardware,\nensures no missing or conflicting dependencies,\nmuch faster installation.\n\nThe workflow thus looks like:\npython -m venv ~/env\nsource ~/env/bin/activate\npython -m pip install --upgrade --no-index pip\npython -m pip install --no-index &lt;package&gt;",
    "crumbs": [
      "Python",
      "<b><em>Getting started</em></b>",
      "Modules and libraries"
    ]
  },
  {
    "objectID": "python/intro_functions.html",
    "href": "python/intro_functions.html",
    "title": "Writing functions",
    "section": "",
    "text": "Python comes with a number of built-in functions. Packages can provide additional ones. In many cases however, you will want to create your own functions to perform exactly the computations that you need.\nIn this section, we will see how to define new functions.",
    "crumbs": [
      "Python",
      "<b><em>Getting started</em></b>",
      "Functions"
    ]
  },
  {
    "objectID": "python/intro_functions.html#syntax",
    "href": "python/intro_functions.html#syntax",
    "title": "Writing functions",
    "section": "Syntax",
    "text": "Syntax\nThe function definition syntax follows:\ndef &lt;name&gt;(&lt;arguments&gt;):\n    &lt;body&gt;\nOnce defined, new functions can be used as any other function.\nLet’s give this a try by creating some greeting functions.",
    "crumbs": [
      "Python",
      "<b><em>Getting started</em></b>",
      "Functions"
    ]
  },
  {
    "objectID": "python/intro_functions.html#function-without-argument",
    "href": "python/intro_functions.html#function-without-argument",
    "title": "Writing functions",
    "section": "Function without argument",
    "text": "Function without argument\nLet’s start with the simple case in which our function does not accept any argument:\n\ndef hello():\n    print('Hello')\n\nThen we call it:\n\nhello()\n\nHello\n\n\nThis was great, but …\n\nhello('Marie')\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[3], line 1\n----&gt; 1 hello('Marie')\n\nTypeError: hello() takes 0 positional arguments but 1 was given\n\n\n\n… it does not accept arguments.",
    "crumbs": [
      "Python",
      "<b><em>Getting started</em></b>",
      "Functions"
    ]
  },
  {
    "objectID": "python/intro_functions.html#function-with-one-argument",
    "href": "python/intro_functions.html#function-with-one-argument",
    "title": "Writing functions",
    "section": "Function with one argument",
    "text": "Function with one argument\nLet’s step this up with a function which can accept an argument:\n\ndef greetings(name):\n    print('Hello ' + name)\n\nThis time, this works:\n\ngreetings('Marie')\n\nHello Marie\n\n\nHowever, this does not work anymore:\n\ngreetings()\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[6], line 1\n----&gt; 1 greetings()\n\nTypeError: greetings() missing 1 required positional argument: 'name'\n\n\n\n:(",
    "crumbs": [
      "Python",
      "<b><em>Getting started</em></b>",
      "Functions"
    ]
  },
  {
    "objectID": "python/intro_functions.html#function-with-a-facultative-argument",
    "href": "python/intro_functions.html#function-with-a-facultative-argument",
    "title": "Writing functions",
    "section": "Function with a facultative argument",
    "text": "Function with a facultative argument\nLet’s make this even more fancy: a function with a facultative argument. That is, a function which accepts an argument, but also has a default value for when we do not provide any argument:\n\ndef howdy(name='you'):\n    print('Hello ' + name)\n\nWe can call it without argument (making use of the default value):\n\nhowdy()\n\nHello you\n\n\nAnd we can call it with an argument:\n\nhowdy('Marie')\n\nHello Marie\n\n\nThis was better, but …\n\nhowdy('Marie', 'Paul')\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[10], line 1\n----&gt; 1 howdy('Marie', 'Paul')\n\nTypeError: howdy() takes from 0 to 1 positional arguments but 2 were given\n\n\n\n… this does not work.",
    "crumbs": [
      "Python",
      "<b><em>Getting started</em></b>",
      "Functions"
    ]
  },
  {
    "objectID": "python/intro_functions.html#function-with-two-arguments",
    "href": "python/intro_functions.html#function-with-two-arguments",
    "title": "Writing functions",
    "section": "Function with two arguments",
    "text": "Function with two arguments\nWe could create a function which takes two arguments:\n\ndef hey(name1, name2):\n    print('Hello ' + name1 + ', ' + name2)\n\nWhich solves our problem:\n\nhey('Marie', 'Paul')\n\nHello Marie, Paul\n\n\nBut it is terribly limiting:\n\n# This doesn't work\nhey()\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[13], line 2\n      1 # This doesn't work\n----&gt; 2 hey()\n\nTypeError: hey() missing 2 required positional arguments: 'name1' and 'name2'\n\n\n\n\n# And neither does this\nhey('Marie')\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[14], line 2\n      1 # And neither does this\n----&gt; 2 hey('Marie')\n\nTypeError: hey() missing 1 required positional argument: 'name2'\n\n\n\n\n# Nor to mention this...\nhey('Marie', 'Paul', 'Alex')\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[15], line 2\n      1 # Nor to mention this...\n----&gt; 2 hey('Marie', 'Paul', 'Alex')\n\nTypeError: hey() takes 2 positional arguments but 3 were given",
    "crumbs": [
      "Python",
      "<b><em>Getting started</em></b>",
      "Functions"
    ]
  },
  {
    "objectID": "python/intro_functions.html#function-with-any-number-of-arguments",
    "href": "python/intro_functions.html#function-with-any-number-of-arguments",
    "title": "Writing functions",
    "section": "Function with any number of arguments",
    "text": "Function with any number of arguments\nLet’s create a truly great function which handles all our cases:\n\ndef hi(name='you', *args):\n    result = ''\n    for i in args:\n        result += (', ' + i)\n    print('Hello ' + name + result)\n\nAnd let’s test it:\n\nhi()\nhi('Marie')\nhi('Marie', 'Paul')\nhi('Marie', 'Paul', 'Alex')\n\nHello you\nHello Marie\nHello Marie, Paul\nHello Marie, Paul, Alex\n\n\nEverything works!",
    "crumbs": [
      "Python",
      "<b><em>Getting started</em></b>",
      "Functions"
    ]
  },
  {
    "objectID": "python/intro_functions.html#documenting-functions",
    "href": "python/intro_functions.html#documenting-functions",
    "title": "Writing functions",
    "section": "Documenting functions",
    "text": "Documenting functions\nIt is a good habit to document what your functions do. As with comments, those “documentation strings” or “docstrings” will help future you or other users of your code.\nPEP 257—docstring conventions—suggests to use single-line docstrings surrounded by triple quotes.\nRemember the function definition syntax we saw at the start of this chapter? To be more exhaustive, we should have written it this way:\ndef &lt;name&gt;(&lt;arguments&gt;):\n    \"\"\"&lt;docstrings&gt;\"\"\"\n    &lt;body&gt;\n\nExample:\n\n\ndef hi(name='you', *args):\n    \"\"\"Print a greeting\"\"\"\n    result = ''\n    for i in args:\n        result += (', ' + i)\n    print('Hello ' + name + result)\n\nPEP 8—the style guide for Python code—suggests a maximum of 72 characters per line for docstrings.\nIf your docstring is longer, you should create a multi-line one. In that case, PEP 257 suggests to have a summary line at the top (right after the opening set of triple quotes), then leave a blank line, then have your long docstrings (which can occupy multiple lines), and finally have the closing set of triple quotes on a line of its own:\ndef &lt;name&gt;(&lt;arguments&gt;):\n    \"\"\"&lt;summary docstrings line&gt;\"\"\"\n\n    &lt;more detailed description&gt;\n    \"\"\"\n    &lt;body&gt;\n\nExample:\n\n\ndef hi(name='you', *args):\n    \"\"\"Print a greeting\n\n    Accepts any number of arguments\n    \"\"\"\n    result = ''\n    for i in args:\n        result += (', ' + i)\n    print('Hello ' + name + result)\n\n\nYou can (and should) document modules, classes, and methods in the same way.\n\nYou can now access the documentation of your function as you would any Python function:\n\nhelp(hi)\n\nHelp on function hi in module __main__:\n\nhi(name='you', *args)\n    Print a greeting\n\n    Accepts any number of arguments\n\n\n\nOr:\n\nprint(hi.__doc__)\n\nPrint a greeting\n\nAccepts any number of arguments",
    "crumbs": [
      "Python",
      "<b><em>Getting started</em></b>",
      "Functions"
    ]
  },
  {
    "objectID": "python/intro_collections.html",
    "href": "python/intro_collections.html",
    "title": "Collections",
    "section": "",
    "text": "Values can be stored in collections. This section introduces tuples, dictionaries, sets, and arrays in Python.",
    "crumbs": [
      "Python",
      "<b><em>Getting started</em></b>",
      "Collections"
    ]
  },
  {
    "objectID": "python/intro_collections.html#lists",
    "href": "python/intro_collections.html#lists",
    "title": "Collections",
    "section": "Lists",
    "text": "Lists\nLists are declared in square brackets:\n\nl = [2, 1, 3]\nl\n\n[2, 1, 3]\n\n\n\ntype(l)\n\nlist\n\n\nThey are mutable:\n\nl.append(0)\nl\n\n[2, 1, 3, 0]\n\n\nLists are ordered:\n\n['b', 'a'] == ['a', 'b']\n\nFalse\n\n\nThey can have repeat values:\n\n['a', 'a', 'a', 't']\n\n['a', 'a', 'a', 't']\n\n\nLists can be homogeneous:\n\n['b', 'a', 'x', 'e']\n\n['b', 'a', 'x', 'e']\n\n\n\ntype('b') == type('a') == type('x') == type('e')\n\nTrue\n\n\nor heterogeneous:\n\n[3, 'some string', 2.9, 'z']\n\n[3, 'some string', 2.9, 'z']\n\n\n\ntype(3) == type('some string') == type(2.9) == type('z')\n\nFalse\n\n\nThey can even be nested:\n\n[3, ['b', 'e', 3.9, ['some string', 9.9]], 8]\n\n[3, ['b', 'e', 3.9, ['some string', 9.9]], 8]\n\n\nThe length of a list is the number of items it contains and can be obtained with the function len:\n\nlen([3, ['b', 'e', 3.9, ['some string', 9.9]], 8])\n\n3\n\n\nTo extract an item from a list, you index it:\n\n[3, ['b', 'e', 3.9, ['some string', 9.9]], 8][0]\n\n3\n\n\n\nPython starts indexing at 0, so what we tend to think of as the “first” element of a list is for Python the “zeroth” element.\n\n\n[3, ['b', 'e', 3.9, ['some string', 9.9]], 8][1]\n\n['b', 'e', 3.9, ['some string', 9.9]]\n\n\n\n[3, ['b', 'e', 3.9, ['some string', 9.9]], 8][2]\n\n8\n\n\n\n# Of course you can't extract items that don't exist\n[3, ['b', 'e', 3.9, ['some string', 9.9]], 8][3]\n\n\n---------------------------------------------------------------------------\nIndexError                                Traceback (most recent call last)\nCell In[15], line 2\n      1 # Of course you can't extract items that don't exist\n----&gt; 2 [3, ['b', 'e', 3.9, ['some string', 9.9]], 8][3]\n\nIndexError: list index out of range\n\n\n\nYou can index from the end of the list with negative values (here you start at -1 for the last element):\n\n[3, ['b', 'e', 3.9, ['some string', 9.9]], 8][-1]\n\n8\n\n\n\n\nYour turn:\n\nHow could you extract the string 'some string' from the list [3, ['b', 'e', 3.9, ['some string', 9.9]], 8]?\n\nYou can also slice a list:\n\n[3, ['b', 'e', 3.9, ['some string', 9.9]], 8][0:1]\n\n[3]\n\n\n\nNotice how slicing returns a list.\nNotice also how the left index is included but the right index excluded.\n\nIf you omit the first index, the slice starts at the beginning of the list:\n\n[1, 2, 3, 4, 5, 6, 7, 8, 9][:6]\n\n[1, 2, 3, 4, 5, 6]\n\n\nIf you omit the second index, the slice goes to the end of the list:\n\n[1, 2, 3, 4, 5, 6, 7, 8, 9][6:]\n\n[7, 8, 9]\n\n\nWhen slicing, you can specify the stride:\n\n[1, 2, 3, 4, 5, 6, 7, 8, 9][2:7:2]\n\n[3, 5, 7]\n\n\n\nThe default stride is 1:\n\n[1, 2, 3, 4, 5, 6, 7, 8, 9][2:7] == [1, 2, 3, 4, 5, 6, 7, 8, 9][2:7:1]\n\nTrue\n\n\n\nYou can reverse the order of a list with a -1 stride applied on the whole list:\n\n[1, 2, 3, 4, 5, 6, 7, 8, 9][::-1]\n\n[9, 8, 7, 6, 5, 4, 3, 2, 1]\n\n\nYou can test whether an item is in a list:\n\n3 in [3, ['b', 'e', 3.9, ['some string', 9.9]], 8]\n\nTrue\n\n\n\n9 in [3, ['b', 'e', 3.9, ['some string', 9.9]], 8]\n\nFalse\n\n\nor not in a list:\n\n3 not in [3, ['b', 'e', 3.9, ['some string', 9.9]], 8]\n\nFalse\n\n\nYou can get the index (position) of an item inside a list:\n\n[3, ['b', 'e', 3.9, ['some string', 9.9]], 8].index(3)\n\n0\n\n\n\nNote that this only returns the index of the first occurrence:\n\n[3, 3, ['b', 'e', 3.9, ['some string', 9.9]], 8].index(3)\n\n0\n\n\n\nLists are mutable (they can be modified). For instance, you can replace items in a list by other items:\n\nL = [3, ['b', 'e', 3.9, ['some string', 9.9]], 8]\nL\n\n[3, ['b', 'e', 3.9, ['some string', 9.9]], 8]\n\n\n\nL[1] = 2\nL\n\n[3, 2, 8]\n\n\nYou can delete items from a list using their indices with list.pop:\n\nL.pop(2)\nL\n\n[3, 2]\n\n\n\nHere, because we are using list.pop, 2 represents the index (the 3rd item).\n\nor with del:\n\ndel L[0]\nL\n\n[2]\n\n\n\nNotice how a list can have a single item:\n\nlen(L)\n\n1\n\n\nIt is then called a “singleton list”.\n\nYou can also delete items from a list using their values with list.remove:\n\nL.remove(2)\nL\n\n[]\n\n\n\nHere, because we are using list.remove, 2 is the value 2.\n\n\nNotice how a list can even be empty:\n\nlen(L)\n\n0\n\n\nYou can actually initialise empty lists:\n\nM = []\ntype(M)\n\nlist\n\n\n\nYou can add items to a list. One at a time:\n\nL.append(7)\nL\n\n[7]\n\n\nAnd if you want to add multiple items at once?\n\n# This doesn't work...\nL.append(3, 6, 9)\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[37], line 2\n      1 # This doesn't work...\n----&gt; 2 L.append(3, 6, 9)\n\nTypeError: list.append() takes exactly one argument (3 given)\n\n\n\n\n# This doesn't work either (that's not what we wanted)\nL.append([3, 6, 9])\nL\n\n[7, [3, 6, 9]]\n\n\n\n\nYour turn:\n\nFix this mistake we just made and remove the nested list [3, 6, 9].\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nOne option is:\n\ndel L[1]\n\n\n\n\nTo add multiple values to a list (and not a nested list), you need to use list.extend:\n\nL.extend([3, 6, 9])\nL\n\n[7, 3, 6, 9]\n\n\nIf you don’t want to add an item at the end of a list, you can use list.insert(&lt;index&gt;, &lt;object&gt;):\n\nL.insert(3, 'test')\nL\n\n[7, 3, 6, 'test', 9]\n\n\n\n\nYour turn:\n\nLet’s have the following list:\n\nL = [7, [3, 6, 9], 3, 'test', 6, 9]\n\nInsert the string 'nested' in the zeroth position of the nested list [3, 6, 9] in L.\n\nYou can sort an homogeneous list:\n\nL = [3, 9, 10, 0]\nL.sort()\nL\n\n[0, 3, 9, 10]\n\n\n\nL = ['some string', 'b', 'a']\nL.sort()\nL\n\n['a', 'b', 'some string']\n\n\n\nHeterogeneous lists cannot be sorted:\n\nL = [3, ['b', 'e', 3.9, ['some string', 9.9]], 8]\nL.sort()\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[45], line 2\n      1 L = [3, ['b', 'e', 3.9, ['some string', 9.9]], 8]\n----&gt; 2 L.sort()\n\nTypeError: '&lt;' not supported between instances of 'list' and 'int'\n\n\n\n\nYou can also get the min and max value of homogeneous lists:\n\nmin([3, 9, 10, 0])\n\n0\n\n\n\nmax(['some string', 'b', 'a'])\n\n'some string'\n\n\n\nFor heterogeneous lists, this also doesn’t work:\n\nmin([3, ['b', 'e', 3.9, ['some string', 9.9]], 8])\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[48], line 1\n----&gt; 1 min([3, ['b', 'e', 3.9, ['some string', 9.9]], 8])\n\nTypeError: '&lt;' not supported between instances of 'list' and 'int'\n\n\n\n\nLists can be concatenated with +:\n\nL + [3, 6, 9]\n\n[3, ['b', 'e', 3.9, ['some string', 9.9]], 8, 3, 6, 9]\n\n\nor repeated with *:\n\nL * 3\n\n[3,\n ['b', 'e', 3.9, ['some string', 9.9]],\n 8,\n 3,\n ['b', 'e', 3.9, ['some string', 9.9]],\n 8,\n 3,\n ['b', 'e', 3.9, ['some string', 9.9]],\n 8]\n\n\n\nTo sum up, lists are declared in square brackets. They are mutable, ordered (thus indexable), and possibly heterogeneous collections of values.",
    "crumbs": [
      "Python",
      "<b><em>Getting started</em></b>",
      "Collections"
    ]
  },
  {
    "objectID": "python/intro_collections.html#strings",
    "href": "python/intro_collections.html#strings",
    "title": "Collections",
    "section": "Strings",
    "text": "Strings\nStrings behave (a little) like lists of characters in that they have a length (the number of characters):\n\nS = 'This is a string.'\nlen(S)\n\n17\n\n\nThey have a min and a max:\n\nmin(S)\n\n' '\n\n\n\nmax(S)\n\n't'\n\n\nYou can index them:\n\nS[3]\n\n's'\n\n\nSlice them:\n\nS[10:16]\n\n'string'\n\n\n\n\nYour turn:\n\nReverse the order of the string S.\n\nThey can also be concatenated with +:\n\nT = 'This is another string.'\nprint(S + ' ' + T)\n\nThis is a string. This is another string.\n\n\nor repeated with *:\n\nprint(S * 3)\n\nThis is a string.This is a string.This is a string.\n\n\n\nprint((S + ' ') * 3)\n\nThis is a string. This is a string. This is a string. \n\n\nThis is where the similarities stop however: methods such as list.sort, list.append, etc. will not work on strings.",
    "crumbs": [
      "Python",
      "<b><em>Getting started</em></b>",
      "Collections"
    ]
  },
  {
    "objectID": "python/intro_collections.html#arrays",
    "href": "python/intro_collections.html#arrays",
    "title": "Collections",
    "section": "Arrays",
    "text": "Arrays\nPython comes with a built-in array module. When you need arrays for storing and retrieving data, this module is perfectly suitable and extremely lightweight. This tutorial covers the syntax in detail.\nWhenever you plan on performing calculations on your data however (which is the vast majority of cases), you should instead use the NumPy package, covered in another section.",
    "crumbs": [
      "Python",
      "<b><em>Getting started</em></b>",
      "Collections"
    ]
  },
  {
    "objectID": "python/intro_collections.html#tuples",
    "href": "python/intro_collections.html#tuples",
    "title": "Collections",
    "section": "Tuples",
    "text": "Tuples\nTuples are defined with parentheses:\n\nt = (3, 1, 4, 2)\nt\n\n(3, 1, 4, 2)\n\n\n\ntype(t)\n\ntuple\n\n\nTuples are ordered:\n\n(2, 3) == (3, 2)\n\nFalse\n\n\nThis means that they are indexable and sliceable:\n\n(2, 4, 6)[2]\n\n6\n\n\n\n(2, 4, 6)[::-1]\n\n(6, 4, 2)\n\n\nThey can be nested:\n\ntype((3, 1, (0, 2)))\n\ntuple\n\n\n\nlen((3, 1, (0, 2)))\n\n3\n\n\n\nmax((3, 1, 2))\n\n3\n\n\nThey can be heterogeneous:\n\ntype(('string', 2, True))\n\ntuple\n\n\nYou can create empty tuples:\n\ntype(())\n\ntuple\n\n\nYou can also create singleton tuples, but the syntax is a bit odd:\n\n# This is not a tuple...\ntype((1))\n\nint\n\n\n\n# This is the weird way to define a singleton tuple\ntype((1,))\n\ntuple\n\n\nHowever, the big difference with lists is that tuples are immutable:\n\nT = (2, 5)\nT[0] = 8\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[71], line 2\n      1 T = (2, 5)\n----&gt; 2 T[0] = 8\n\nTypeError: 'tuple' object does not support item assignment\n\n\n\nTuples are quite fascinating:\n\na, b = 1, 2\na, b\n\n(1, 2)\n\n\n\na, b = b, a\na, b\n\n(2, 1)\n\n\n\nTuples are declared in parentheses. They are immutable, ordered (thus indexable), and possibly heterogeneous collections of values.",
    "crumbs": [
      "Python",
      "<b><em>Getting started</em></b>",
      "Collections"
    ]
  },
  {
    "objectID": "python/intro_collections.html#sets",
    "href": "python/intro_collections.html#sets",
    "title": "Collections",
    "section": "Sets",
    "text": "Sets\nSets are declared in curly braces:\n\ns = {3, 2, 5}\ns\n\n{2, 3, 5}\n\n\n\ntype(s)\n\nset\n\n\nSets are unordered:\n\n{2, 4, 1} == {4, 2, 1}\n\nTrue\n\n\nConsequently, it makes no sense to index a set.\nSets can be heterogeneous:\n\nS = {2, 'a', 'string'}\nisinstance(S, set)\n\nTrue\n\n\n\ntype(2) == type('a') == type('string')\n\nFalse\n\n\nThere are no duplicates in a set:\n\n{2, 2, 'a', 2, 'string', 'a'}\n\n{2, 'a', 'string'}\n\n\nYou can define an empty set, but only with the set function (because empty curly braces define a dictionary):\n\nt = set()\nt\n\nset()\n\n\n\nlen(t)\n\n0\n\n\n\ntype(t)\n\nset\n\n\nSince strings an iterables, you can use set to get a set of the unique characters:\n\nset('abba')\n\n{'a', 'b'}\n\n\n\n\nYour turn:\n\nHow could you create a set with the single element 'abba' in it?\n\n\nSets are declared in curly brackets. They are mutable, unordered (thus non indexable), possibly heterogeneous collections of unique values.",
    "crumbs": [
      "Python",
      "<b><em>Getting started</em></b>",
      "Collections"
    ]
  },
  {
    "objectID": "python/intro_collections.html#dictionaries",
    "href": "python/intro_collections.html#dictionaries",
    "title": "Collections",
    "section": "Dictionaries",
    "text": "Dictionaries\nDictionaries are declared in curly braces. They associate values to keys:\n\nd = {'key1': 'value1', 'key2': 'value2'}\nd\n\n{'key1': 'value1', 'key2': 'value2'}\n\n\n\ntype(d)\n\ndict\n\n\nDictionaries are unordered:\n\n{'a': 1, 'b': 2} == {'b': 2, 'a': 1}\n\nTrue\n\n\nConsequently, the pairs themselves cannot be indexed. However, you can access values in a dictionary from their keys:\n\nD = {'c': 1, 'a': 3, 'b': 2}\nD['b']\n\n2\n\n\n\nD.get('b')\n\n2\n\n\n\nD.items()\n\ndict_items([('c', 1), ('a', 3), ('b', 2)])\n\n\n\nD.values()\n\ndict_values([1, 3, 2])\n\n\n\nD.keys()\n\ndict_keys(['c', 'a', 'b'])\n\n\nTo return a sorted list of keys:\n\nsorted(D)\n\n['a', 'b', 'c']\n\n\nYou can create empty dictionaries:\n\nE = {}\ntype(E)\n\ndict\n\n\nDictionaries are mutable, so you can add, remove, or replace items.\nLet’s add an item to our empty dictionary E:\n\nE['author'] = 'Proust'\nE\n\n{'author': 'Proust'}\n\n\nWe can add another one:\n\nE['title'] = 'In search of lost time'\nE\n\n{'author': 'Proust', 'title': 'In search of lost time'}\n\n\nWe can modify one:\n\nE['author'] = 'Marcel Proust'\nE\n\n{'author': 'Marcel Proust', 'title': 'In search of lost time'}\n\n\n\n\nYour turn:\n\nAdd a third item to E with the number of volumes.\n\nWe can also remove items:\n\nE.pop('author')\nE\n\n{'title': 'In search of lost time'}\n\n\nAnother method to remove items:\n\ndel E['title']\nE\n\n{}\n\n\n\nDictionaries are declared in curly braces. They are mutable and unordered collections of key/value pairs. They play the role of an associative array.",
    "crumbs": [
      "Python",
      "<b><em>Getting started</em></b>",
      "Collections"
    ]
  },
  {
    "objectID": "python/intro_collections.html#conversion-between-collections",
    "href": "python/intro_collections.html#conversion-between-collections",
    "title": "Collections",
    "section": "Conversion between collections",
    "text": "Conversion between collections\nFrom tuple to list:\n\nlist((3, 8, 1))\n\n[3, 8, 1]\n\n\nFrom tuple to set:\n\nset((3, 2, 3, 3))\n\n{2, 3}\n\n\nFrom list to tuple:\n\ntuple([3, 1, 4])\n\n(3, 1, 4)\n\n\nFrom list to set:\n\nset(['a', 2, 4])\n\n{2, 4, 'a'}\n\n\nFrom set to tuple:\n\ntuple({2, 3})\n\n(2, 3)\n\n\nFrom set to list:\n\nlist({2, 3})\n\n[2, 3]",
    "crumbs": [
      "Python",
      "<b><em>Getting started</em></b>",
      "Collections"
    ]
  },
  {
    "objectID": "python/intro_collections.html#collections-module",
    "href": "python/intro_collections.html#collections-module",
    "title": "Collections",
    "section": "Collections module",
    "text": "Collections module\nPython has a built-in collections module providing the additional data structures: deque, defaultdict, namedtuple, OrderedDict, Counter, ChainMap, UserDict, UserList, and UserList.",
    "crumbs": [
      "Python",
      "<b><em>Getting started</em></b>",
      "Collections"
    ]
  },
  {
    "objectID": "python/index.html",
    "href": "python/index.html",
    "title": "Python",
    "section": "",
    "text": "Getting started in  \nAn intro course to Python\n\n\n\n\nFaster  \nSpeeding up Python computations\n\n\n\n\n\n\nText analysis\nAn introduction to NLP using TextBlob\n\n\n\n\nWorkshops\nVarious Python topics\n\n\n\n\n\n\n60 min webinars\nVarious Python topics",
    "crumbs": [
      "Python",
      "<br>&nbsp;<img src=\"img/logo_python.svg\" class=\"img-fluid\" style=\"width:1.55em\" alt=\"noshadow\"><br><br>"
    ]
  },
  {
    "objectID": "python/hpc_jax.html",
    "href": "python/hpc_jax.html",
    "title": "High-performance computing with JAX",
    "section": "",
    "text": "JAX can be used for any high-performance scientific computing with arrays, but because it is a good starting point to build deep learning libraries, please find this course in the AI section.",
    "crumbs": [
      "Python",
      "<b><em>Faster Python</em></b>",
      "JAX: accelerated arrays & AD"
    ]
  },
  {
    "objectID": "julia/wb_makie_slides.html#plotting-in-julia",
    "href": "julia/wb_makie_slides.html#plotting-in-julia",
    "title": "Makie",
    "section": "Plotting in Julia",
    "text": "Plotting in Julia\n\nMany options:\n\nPlots.jl: high-level API for working with different back-ends (GR, Pyplot, Plotly…)\nPyPlot.jl: Julia interface to Matplotlib’s matplotlib.pyplot\nPlotlyJS.jl: Julia interface to plotly.js\nPlotlyLight.jl: the fastest plotting option in Julia by far, but limited features\nGadfly.jl: following the grammar of graphics popularized by Hadley Wickham in R\nVegaLite.jl: grammar of interactive graphics\nPGFPlotsX.jl: Julia interface to the PGFPlots LaTeX package\nUnicodePlots.jl: plots in the terminal 🙂\n\n\n\n\nMakie.jl: powerful plotting ecosystem: animation, 3D, GPU optimization"
  },
  {
    "objectID": "julia/wb_makie_slides.html#makie-ecosystem",
    "href": "julia/wb_makie_slides.html#makie-ecosystem",
    "title": "Makie",
    "section": "Makie ecosystem",
    "text": "Makie ecosystem\n\n\nMain package:\n\nMakie: plots functionalities. Backend needed to render plots into images or vector graphics\n\n\n\n\n\nBackends:\n\nCairoMakie: vector graphics or high-quality 2D plots. Creates, but does not display plots (you need an IDE that does or you can use ElectronDisplay.jl)\nGLMakie: based on OpenGL; 3D rendering and interactivity in GLFW window (no vector graphics)\nWGLMakie: web version of GLMakie (plots rendered in a browser instead of a window)"
  },
  {
    "objectID": "julia/wb_makie_slides.html#extensions",
    "href": "julia/wb_makie_slides.html#extensions",
    "title": "Makie",
    "section": "Extensions",
    "text": "Extensions\n\nGeoMakie.jl add geographical plotting utilities to Makie\nAlgebraOfGraphics.jl turns plotting into a simple algebra of building blocks\nGraphMakie.jl to create network graphs"
  },
  {
    "objectID": "julia/wb_makie_slides.html#cheatsheet-2d",
    "href": "julia/wb_makie_slides.html#cheatsheet-2d",
    "title": "Makie",
    "section": "Cheatsheet 2D",
    "text": "Cheatsheet 2D\n\n\nFrom: Storopoli, Huijzer and Alonso (2021). Julia Data Science. https://juliadatascience.io. ISBN: 97984898"
  },
  {
    "objectID": "julia/wb_makie_slides.html#cheatsheet-3d",
    "href": "julia/wb_makie_slides.html#cheatsheet-3d",
    "title": "Makie",
    "section": "Cheatsheet 3D",
    "text": "Cheatsheet 3D\n\n\nFrom: Storopoli, Huijzer and Alonso (2021). Julia Data Science. https://juliadatascience.io. ISBN: 97984898"
  },
  {
    "objectID": "julia/wb_makie_slides.html#resources",
    "href": "julia/wb_makie_slides.html#resources",
    "title": "Makie",
    "section": "Resources",
    "text": "Resources\n\nOfficial documentation\nJulia Data Science book, chapter 5\nMany examples in the project Beautiful Makie"
  },
  {
    "objectID": "julia/wb_makie_slides.html#troubleshooting",
    "href": "julia/wb_makie_slides.html#troubleshooting",
    "title": "Makie",
    "section": "Troubleshooting",
    "text": "Troubleshooting\nInstalling GLMakie can be challenging. This page may lead you towards solutions\nCairoMakie and WGLMakie should install without issues"
  },
  {
    "objectID": "julia/wb_makie_slides.html#figure",
    "href": "julia/wb_makie_slides.html#figure",
    "title": "Makie",
    "section": "Figure",
    "text": "Figure\nLoad the package (here, we are using CairoMakie):\n\nusing CairoMakie                        # no need to import Makie itself\n\n\n\nCreate a Figure (container object):\n\nfig = Figure()\n\n\n\n\n\n\n\n\n\n\n\n\n\ntypeof(fig)\n\nFigure"
  },
  {
    "objectID": "julia/wb_makie_slides.html#axis",
    "href": "julia/wb_makie_slides.html#axis",
    "title": "Makie",
    "section": "Axis",
    "text": "Axis\n\n\nThen, you can create an Axis:\n\nax = Axis(Figure()[1, 1])\n\nAxis with 0 plots:\n\n\n\n\n\n\n\ntypeof(ax)\n\nAxis"
  },
  {
    "objectID": "julia/wb_makie_slides.html#plot",
    "href": "julia/wb_makie_slides.html#plot",
    "title": "Makie",
    "section": "Plot",
    "text": "Plot\nFinally, we can add a plot:\n\nfig = Figure()\nax = Axis(fig[1, 1])\nx = LinRange(-10, 10, 20)\ny = x\nscatter!(ax, x, y)  # Functions with ! transform their arguments\nfig"
  },
  {
    "objectID": "julia/wb_makie_slides.html#d",
    "href": "julia/wb_makie_slides.html#d",
    "title": "Makie",
    "section": "2D",
    "text": "2D\n\nusing CairoMakie\nusing StatsBase, LinearAlgebra\nusing Interpolations, OnlineStats\nusing Distributions\nCairoMakie.activate!(type = \"png\")\n\nfunction eq_hist(matrix; nbins = 256 * 256)\n    h_eq = fit(Histogram, vec(matrix), nbins = nbins)\n    h_eq = normalize(h_eq, mode = :density)\n    cdf = cumsum(h_eq.weights)\n    cdf = cdf / cdf[end]\n    edg = h_eq.edges[1]\n    interp_linear = LinearInterpolation(edg, [cdf..., cdf[end]])\n    out = reshape(interp_linear(vec(matrix)), size(matrix))\n    return out\nend\n\nfunction getcounts!(h, fn; n = 100)\n    for _ in 1:n\n        vals = eigvals(fn())\n        x0 = real.(vals)\n        y0 = imag.(vals)\n        fit!(h, zip(x0,y0))\n    end\nend\n\nm(;a=10rand()-5, b=10rand()-5) = [0 0 0 a; -1 -1 1 0; b 0 0 0; -1 -1 -1 -1]\n\nh = HeatMap(range(-3.5,3.5,length=1200), range(-3.5,3.5, length=1200))\ngetcounts!(h, m; n=2_000_000)\n\nwith_theme(theme_black()) do\n    fig = Figure(figure_padding=0,resolution=(600,600))\n    ax = Axis(fig[1,1]; aspect = DataAspect())\n    heatmap!(ax,-3.5..3.5, -3.5..3.5, eq_hist(h.counts); colormap = :bone_1)\n    hidedecorations!(ax)\n    hidespines!(ax)\n    fig\nend"
  },
  {
    "objectID": "julia/wb_makie_slides.html#d-output",
    "href": "julia/wb_makie_slides.html#d-output",
    "title": "Makie",
    "section": "2D",
    "text": "2D"
  },
  {
    "objectID": "julia/wb_makie_slides.html#d-1",
    "href": "julia/wb_makie_slides.html#d-1",
    "title": "Makie",
    "section": "3D",
    "text": "3D\nusing GLMakie, Random\nGLMakie.activate!()\n\nRandom.seed!(13)\nx = -6:0.5:6\ny = -6:0.5:6\nz = 6exp.( -(x.^2 .+ y' .^ 2)./4)\n\nbox = Rect3(Point3f(-0.5), Vec3f(1))\nn = 100\ng(x) = x^(1/10)\nalphas = [g(x) for x in range(0,1,length=n)]\ncmap_alpha = resample_cmap(:linear_worb_100_25_c53_n256, n, alpha = alphas)\n\nwith_theme(theme_dark()) do\n    fig, ax, = meshscatter(x, y, z;\n                           marker=box,\n                           markersize = 0.5,\n                           color = vec(z),\n                           colormap = cmap_alpha,\n                           colorrange = (0,6),\n                           axis = (;\n                                   type = Axis3,\n                                   aspect = :data,\n                                   azimuth = 7.3,\n                                   elevation = 0.189,\n            perspectiveness = 0.5),\n        figure = (;\n            resolution =(1200,800)))\n    meshscatter!(ax, x .+ 7, y, z./2;\n        markersize = 0.25,\n        color = vec(z./2),\n        colormap = cmap_alpha,\n        colorrange = (0, 6),\n        ambient = Vec3f(0.85, 0.85, 0.85),\n        backlight = 1.5f0)\n    xlims!(-5.5,10)\n    ylims!(-5.5,5.5)\n    hidedecorations!(ax; grid = false)\n    hidespines!(ax)\n    fig\nend"
  },
  {
    "objectID": "julia/wb_makie_slides.html#d-2",
    "href": "julia/wb_makie_slides.html#d-2",
    "title": "Makie",
    "section": "3D",
    "text": "3D"
  },
  {
    "objectID": "julia/wb_makie_slides.html#compiling-sysimages",
    "href": "julia/wb_makie_slides.html#compiling-sysimages",
    "title": "Makie",
    "section": "Compiling sysimages",
    "text": "Compiling sysimages\nWhile Makie is extremely powerful, its compilation time and its time to first plot are extremely long\nFor this reason, it might save you a lot of time to create a sysimage (a file containing information from a Julia session such as loaded packages, global variables, compiled code, etc.) with PackageCompiler.jl\n\nThe upcoming Julia 1.9 will do this automatically"
  },
  {
    "objectID": "julia/wb_makie_slides.html#cairomakie",
    "href": "julia/wb_makie_slides.html#cairomakie",
    "title": "Makie",
    "section": "CairoMakie",
    "text": "CairoMakie\nCairoMakie will run without problem on the Alliance clusters\nIt is not designed for interactivity, so saving to file is what makes the most sense\n\nExample:\n\nsave(\"graph.png\", fig)\n Remember however that CairoMakie is 2D only (for now)"
  },
  {
    "objectID": "julia/wb_makie_slides.html#glmakie",
    "href": "julia/wb_makie_slides.html#glmakie",
    "title": "Makie",
    "section": "GLMakie",
    "text": "GLMakie\nGLMakie relies on GLFW to create windows with OpenGL\nGLFW doesn’t support creating contexts without an associated window\nThe dependency GLFW.jl will thus not install in the clusters—even with X11 forwarding—unless you use VDI nodes, VNC, or Virtual GL"
  },
  {
    "objectID": "julia/wb_makie_slides.html#wglmakie",
    "href": "julia/wb_makie_slides.html#wglmakie",
    "title": "Makie",
    "section": "WGLMakie",
    "text": "WGLMakie\nYou can setup a server with JSServe.jl as per the documentation\nHowever, this method is intended for the creation of interactive widgets, e.g. for a website\nWhile this is really cool, it isn’t optimized for performance\nThere might also be a way to create an SSH tunnel to your local browser, although there is no documentation on this\nBest probably is to save to file"
  },
  {
    "objectID": "julia/wb_makie_slides.html#conclusion-makie-on-production-clusters",
    "href": "julia/wb_makie_slides.html#conclusion-makie-on-production-clusters",
    "title": "Makie",
    "section": "Conclusion: Makie on production clusters",
    "text": "Conclusion: Makie on production clusters\n\n2D plots: use CairoMakie and save to file\n3D plots: use WGLMakie and save to file"
  },
  {
    "objectID": "julia/wb_flux.html",
    "href": "julia/wb_flux.html",
    "title": "Machine learning in Julia with Flux",
    "section": "",
    "text": "This webinar, aimed at users with no experience in machine learning, is an introduction to the basic concepts of neural networks, followed by a simple example—the classic classification of the MNIST database of handwritten digits—using the Julia package Flux.",
    "crumbs": [
      "Julia",
      "<b><em>Webinars</em></b>",
      "Deep learning with Flux"
    ]
  },
  {
    "objectID": "julia/top_wb.html",
    "href": "julia/top_wb.html",
    "title": "Julia webinars",
    "section": "",
    "text": "Visualization with \n\n\n\n\nFirst gentle dab at  \n\n\n\n\nDeep learning with",
    "crumbs": [
      "Julia",
      "<b><em>Webinars</em></b>"
    ]
  },
  {
    "objectID": "julia/intro_tabular.html",
    "href": "julia/intro_tabular.html",
    "title": "Working with tabular data:",
    "section": "",
    "text": "Requirements:\n1 - The current Julia stable release\nInstallation instructions can be found here.\n2 - The packages: CSV, DataFrames, TimeSeries, Plots\nPackages can be installed with ] add &lt;package&gt;.\n3 - Covid-19 data from the Johns Hopkins University CSSE repository\nClone (git clone &lt;repo url&gt;) or download and unzip the repository.",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Working with tabular data"
    ]
  },
  {
    "objectID": "julia/intro_tabular.html#load-packages",
    "href": "julia/intro_tabular.html#load-packages",
    "title": "Working with tabular data:",
    "section": "Load packages",
    "text": "Load packages\nusing CSV\nusing DataFrames\nusing Dates          # From the standard Julia library\nusing TimeSeries\nusing NamedArrays\nusing Plots\n\nWe will use the GR framework as a backend for Plots.",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Working with tabular data"
    ]
  },
  {
    "objectID": "julia/intro_tabular.html#data-until-march-22-2020",
    "href": "julia/intro_tabular.html#data-until-march-22-2020",
    "title": "Working with tabular data:",
    "section": "Data until March 22, 2020",
    "text": "Data until March 22, 2020\n\n\nfrom xkcd.com\n\nThe files in the Johns Hopkins University CSSE repository have changed over time.\nIn this workshop, we will use 2 sets of files:\n\na first set from January 22, 2020 until March 22, 2020\na second set from January 22, 2020 to the present\n\nBoth sets contain data on confirmed and dead cases for world countries and in some cases their subregions (provinces, states, etc. which I will globally here call “provinces”).\nThe first set also contains numbers of recovered cases which allow to calculate numbers of currently ill persons (of course, keep in mind that all these data represent various degrees of underestimation and are flawed in many ways, amongst which are varying levels of testing efforts both geographically and over time, under-reporting, etc).\nThe second set does not contain recovered cases (many overwhelmed countries stopped monitoring this at some point).\nWe will play with the first set together and you will then try to play with the second set on your own.\n\nLoad the data\nIf you did not clone or download and unzip the Covid-19 data repository in your working directory, adapt the path consequently.\n#= create a variable with the path we are interested in;\nthis makes the code below a bit shorter =#\ndir = \"COVID-19/csse_covid_19_data/csse_covid_19_time_series\"\n\n# create a list of the full paths of all the files in dir\nlist = joinpath.(relpath(dir), readdir(dir))\n\n#= read in the 3 csv files with confirmed, dead, and recovered numbers\ncorresponding to the first set of data (until March 22, 2020) =#\ndat = DataFrame.(CSV.File.(list[collect(2:4)]))\nWe now have a one-dimensional array of 3 DataFrames called dat.\n\n\nTransform data into long format\n# rename some variables to easier names\nDataFrames.rename!.(dat, Dict.(1 =&gt; Symbol(\"province\"),\n                               2 =&gt; Symbol(\"country\")))\n\n# create a one-dimensional array of strings\nvar = [\"total\", \"dead\", \"recovered\"]\n\n#= transform the data into long format in a vectorized fashion\nusing both our one-dimensional arrays of 3 elements =#\ndatlong = map((x, y) -&gt; stack(x, Not(collect(1:4)),\n                              variable_name = Symbol(\"date\"),\n                              value_name = Symbol(\"$y\")),\n              dat, var)\nWe now have a one-dimensional array of 3 DataFrames in long format called datlong.\n# join all elements of this array into a single DataFrame\nall = join(datlong[1], datlong[2], datlong[3],\n           on = [:date, :country, :province, :Lat, :Long])\n\n# get rid of \"Lat\" and \"Long\" and re-order the columns\nselect!(all, [4, 3, 1, 2, 7, 8])\n\n#= turn the year from 2 digits to 4 digits using regular expression\n(in a vectorised fashion by braodcasting with the dot notation);\nthen turn these values into strings, and finally into dates =#\nall.date = Date.(replace.(string.(all[:, 3]),\n                          r\"(.*)(..)$\" =&gt; s\"\\g&lt;1&gt;20\\2\"), \"m/dd/yy\");\n\n#= replace the missing values by the string \"NA\"\n(these are not real missing values, but rather non applicable ones) =#\nreplace!(all.province, missing =&gt; \"NA\");\nWe now have a single DataFrame called all, in long format, with the variables confirmed, dead, recovered, and ill.\nCalculate the number of currently ill individuals (again, in a vectorized fashion, by broadcasting with the dot notation):\nall.current = all.total .- all.dead .- all.recovered;\n\n\nWorld summary\nTo make a single plot with world totals of confirmed, dead, recovered, and ill cases, we want the sums of these variables for each day. We do this by grouping the data by date:\nworld = by(all, :date,\n           total = :total =&gt; sum,\n           dead = :dead =&gt; sum,\n           recovered = :recovered =&gt; sum,\n           current = :current =&gt; sum)\nNow we can plot our new variable world.\nAs our data is a time series, we need to transform it to a TimeArray thanks to the TimeArray() function from the TimeSeries package.\nplot(TimeArray(world, timestamp = :date),\n     title = \"World\",\n     legend = :outertopright,\n     widen = :false)\n Data until March 22, 2020\n\n\nCountries/provinces summaries\nNow, we want to group the data by country:\ncountries = groupby(all, :country)\nWe also need to know how the authors of the dataset decided to label the various countries and their subregions.\nFor example, if you want to see what the data looks like for France, Canada, and India, you can run:\ncountries[findall(x -&gt; \"France\" in x, keys(countries))]\ncountries[findall(x -&gt; \"Canada\" in x, keys(countries))]\ncountries[findall(x -&gt; \"India\" in x, keys(countries))]\nThen you need to subset the data for the countries or provinces you are interested in.\nHere are some examples:\n# countries for which there are data for several provinces\ncanada = all[all[:, :country] .== \"Canada\", :]\nus = all[all[:, :country] .== \"US\", :]\nchina = all[all[:, :country] .== \"China\", :]\n\n# countries with no province data\nskorea = all[all[:, :country] .== \"Korea, South\", :]\ntaiwan = all[all[:, :country] .== \"Taiwan*\", :]\nsingapore = all[all[:, :country] .== \"Singapore\", :]\nitaly = all[all[:, :country] .== \"Italy\", :]\nspain = all[all[:, :country] .== \"Spain\", :]\n\n#= countries wich have subregions spread widely in the world;\nhere, I took the arbitrary decision to only look at the main subregions =#\nfrance = all[all[:, :province] .== \"France\", :]\nuk = all[all[:, :province] .== \"United Kingdom\", :]\n\n# provinces\nbc = all[all[:, :province] .== \"British Columbia\", :]\nny = all[all[:, :province] .== \"New York\", :]\nCalculate the totals for Canada, US, and China which all have data for subregions:\ncanada, us, china = by.([canada, us, china], :date,\n                        total = :total =&gt; sum,\n                        dead = :dead =&gt; sum,\n                        recovered = :recovered =&gt; sum,\n                        current = :current =&gt; sum)\nloclist1 = [canada, us, china]\nloctitles1 = [\"Canada\", \"US\", \"China\"]\n\npcanada, pus, pchina =\n    map((x, y) -&gt; plot(TimeArray(x, timestamp = :date),\n                       title = \"$y\", legend = :outertopright,\n                       widen = :false, dpi = :300),\n        loclist1, loctitles1)\nloclist2 = [france, bc, ny, taiwan, skorea, singapore, spain, italy, uk]\nloctitles2 = [\"France\", \"BC\", \"NY\", \"Taiwan\", \"South Korea\",\n              \"Singapore\", \"Spain\", \"Italy\", \"UK\"]\n\npfrance, pbc, pny, ptaiwan, pskorea,\npsingapore, pspain, pitaly, puk =\n    map((x, y) -&gt; plot(TimeArray(select(x, Not([:country, :province])),\n                                 timestamp = :date),\n                       title = \"$y\", legend = :outertopright,\n                       widen = :false, dpi = :300),\n        loclist2, loctitles2)\nNow, let’s plot a few countries/provinces:\n\nNorth America\nplot(pcanada, pbc, pus, pny,\n     legend = false, titlefontsize = 7, tickfontsize = 6)\n Data until March 22, 2020\n\n\nAsia\nplot(pchina, ptaiwan, pskorea, psingapore,\n     legend = false, titlefontsize = 7, tickfontsize = 6)\n Data until March 22, 2020\n\n\nEurope\nplot(pfrance, pspain, pitaly, puk,\n     legend = false, titlefontsize = 7, tickfontsize = 6)\n Data until March 22, 2020",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Working with tabular data"
    ]
  },
  {
    "objectID": "julia/intro_tabular.html#data-up-to-the-present",
    "href": "julia/intro_tabular.html#data-up-to-the-present",
    "title": "Working with tabular data:",
    "section": "Data up to the present",
    "text": "Data up to the present\n\nSummary graphs\n\n\nYour turn:\n\nWrite the code to create an up-to-date graph for the world using the files: time_series_covid19_confirmed_global.csv and time_series_covid19_deaths_global.csv.\n\nHere is the result:\n Data until March 25, 2020\n\n\nYour turn:\n\nCreate up-to-date graphs for the countries and/or provinces of your choice.\n\nHere are a few possible results:\n Data until March 25, 2020\n\n\nCountries comparison\nOur side by side graphs don’t make comparisons very easy since they vary greatly in their axes scales.\nOf course, we could constrain them to have the same axes, but then, why not plot multiple countries or provinces in the same graph?\ncanada[!, :loc] .= \"Canada\";\nchina[!, :loc] .= \"China\";\n\nall = join(all, canada, china, on = [:date, :total, :dead, :loc],\n           kind = :outer)\n\nconfirmed = unstack(all[:, collect(3:5)], :loc, :total)\n\nconf_sel = select(confirmed,\n                  [:date, :Italy, :Spain, :China, :Iran,\n                   :France, :US, Symbol(\"South Korea\"), :Canada])\n\nplot(TimeArray(conf_sel, timestamp = :date),\n     title = \"Confirmed across a few countries\",\n     legend = :outertopright, widen = :false)\n Data until March 25, 2020\n\n\nYour turn:\n\nWrite the code to make a similar graph with the number of deaths in a few countries of your choice.\n\nHere is a possible result:\n Data until March 25, 2020",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Working with tabular data"
    ]
  },
  {
    "objectID": "julia/intro_plotting.html",
    "href": "julia/intro_plotting.html",
    "title": "Plotting",
    "section": "",
    "text": "It can be convenient to plot directly in the REPL (for instance when using SSH).\n\nusing UnicodePlots\nhistogram(randn(1000), nbins=40)\n\nPrecompiling UnicodePlots\n  ✓ Crayons\n  ✓ MarchingCubes\n  ✓ UnicodePlots\n  3 dependencies successfully precompiled in 35 seconds. 41 already precompiled.\nPrecompiling IntervalSetsExt\n  ✓ UnicodePlots → IntervalSetsExt\n  1 dependency successfully precompiled in 1 seconds. 46 already precompiled.\nPrecompiling FreeTypeExt\n  ✓ UnicodePlots → FreeTypeExt\n  1 dependency successfully precompiled in 1 seconds. 53 already precompiled.\n\n\n\n                ┌                                        ┐ \n   [-3.0, -2.8) ┤█▊ 4                                      \n   [-2.8, -2.6) ┤▌ 1                                       \n   [-2.6, -2.4) ┤▌ 1                                       \n   [-2.4, -2.2) ┤█████▏ 11                                 \n   [-2.2, -2.0) ┤██████▊ 15                                \n   [-2.0, -1.8) ┤█████▌ 12                                 \n   [-1.8, -1.6) ┤█████▏ 11                                 \n   [-1.6, -1.4) ┤██████████▌ 23                            \n   [-1.4, -1.2) ┤████████████████▊ 37                      \n   [-1.2, -1.0) ┤█████████████████████████▉ 57             \n   [-1.0, -0.8) ┤████████████████████████▎ 53              \n   [-0.8, -0.6) ┤█████████████████████▌ 47                 \n   [-0.6, -0.4) ┤█████████████████████████████████▍ 73     \n   [-0.4, -0.2) ┤██████████████████████████████▉ 68        \n   [-0.2, -0.0) ┤████████████████████████████████▊ 72      \n   [-0.0,  0.2) ┤██████████████████████████████▉ 68        \n   [ 0.2,  0.4) ┤████████████████████████████████████  79  \n   [ 0.4,  0.6) ┤████████████████████████████████▊ 72      \n   [ 0.6,  0.8) ┤████████████████████████████▍ 62          \n   [ 0.8,  1.0) ┤██████████████████████▍ 49                \n   [ 1.0,  1.2) ┤██████████████████▋ 41                    \n   [ 1.2,  1.4) ┤██████████████████████▍ 49                \n   [ 1.4,  1.6) ┤██████████████▌ 32                        \n   [ 1.6,  1.8) ┤█████████▌ 21                             \n   [ 1.8,  2.0) ┤█████▉ 13                                 \n   [ 2.0,  2.2) ┤████▌ 10                                  \n   [ 2.2,  2.4) ┤██▋ 6                                     \n   [ 2.4,  2.6) ┤█▍ 3                                      \n   [ 2.6,  2.8) ┤██▋ 6                                     \n   [ 2.8,  3.0) ┤█▍ 3                                      \n   [ 3.0,  3.2) ┤▌ 1                                       \n                └                                        ┘ \n                                 Frequency                 \n\n\n\nMost of the time however, you will want to make nicer looking graphs. There are many options to plot in Julia.\nPlots is a convenient Julia package which allows to use the same code with several graphing backends such as the GR framework (great for speed), Plotly.js (allows interaction with your graphs in a browser), or PyPlot. The default backend is the GR framework.\nStatsPlots is an enhanced version with added stats functionality.\n\nExample:\n\n\n# First run takes time as the package needs to compile\nusing StatsPlots\nStatsPlots.histogram(randn(1000), bins=40)\n\nPrecompiling StatsPlots\n  ✓ Arpack_jll\n  ✓ Distances\n  ✓ Widgets\n  ✓ Arpack\n  ✓ SentinelArrays\n  ✓ Distances → DistancesSparseArraysExt\n  ✓ TableOperations\n  ✓ Distances → DistancesChainRulesCoreExt\n  ✓ MultivariateStats\n  ✓ NearestNeighbors\n  ✓ Clustering\n  ✓ StatsPlots\n  12 dependencies successfully precompiled in 8 seconds. 225 already precompiled.\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere, we need to explicitly run StatsPlots.histogram rather than histogram to prevent a conflict with the function of the same name from the package UnicodePlots.",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Plotting"
    ]
  },
  {
    "objectID": "julia/intro_non_interactive.html",
    "href": "julia/intro_non_interactive.html",
    "title": "Non interactive execution",
    "section": "",
    "text": "Julia scripts have a .jl extension.\nThe include function sources a Julia script (in a REPL session or in another script):\ninclude(\"file.jl\")\nThe code contained in file.jl is thus run non interactively.",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Non interactive execution"
    ]
  },
  {
    "objectID": "julia/intro_non_interactive.html#sourcing-a-file",
    "href": "julia/intro_non_interactive.html#sourcing-a-file",
    "title": "Non interactive execution",
    "section": "",
    "text": "Julia scripts have a .jl extension.\nThe include function sources a Julia script (in a REPL session or in another script):\ninclude(\"file.jl\")\nThe code contained in file.jl is thus run non interactively.",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Non interactive execution"
    ]
  },
  {
    "objectID": "julia/intro_non_interactive.html#running-code-from-the-command-line",
    "href": "julia/intro_non_interactive.html#running-code-from-the-command-line",
    "title": "Non interactive execution",
    "section": "Running code from the command line",
    "text": "Running code from the command line\nYou can run scripts by passing them to the julia command on the command line:\n$ julia script.jl\n\nThis code is run in a terminal, not in Julia, as is indicated by the $ prompt.\n\nYou can also evaluate single expressions in Julia from the command line by using the flag -e:\n$ julia -e 'println(2 + 3)'\n5\n\nPassing arguments\n\nTo the julia command itself\nIf you want to pass arguments to the julia command itself, you need to add them before the script or the Julia expression.\n\nExample:\n\n$ julia -O script.jl\n\n\nTo the script/Julia expression\nTo pass arguments to the script (or Julia expression if you use -e), you add them after the script or expression:\n$ julia script.jl arg1 arg2 arg3\narg1, arg2, arg3 will be passed in the global constant ARGS and interpreted as arguments to the script.\n\nExample passing arguments to an expression:\n\n$ julia -e 'for x in ARGS; println(x); end' 2 3\n2\n3\n\n\nTo both\nTo pass arguments both to the julia command and to the script/expression, you need to add the -- delimiter before the script/expression:\n$ julia [switches] -- [programfile] [args...]\n\nExample:\n\n$ julia -O -- script.jl arg1 arg2",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Non interactive execution"
    ]
  },
  {
    "objectID": "julia/intro_intro.html",
    "href": "julia/intro_intro.html",
    "title": "Introduction to Julia",
    "section": "",
    "text": "Why would I want to learn a new language? I already know R/python.\n\nR and python are interpreted languages: the code is executed directly, without prior-compilation. This is extremely convenient: it is what allows you to run code in an interactive shell. The price to pay is low performance: R and python are simply not good at handling large amounts of data. To overcome this limitation, users often turn to C or C++ for the most computation-intensive parts of their analyses. These are compiled—and extremely efficient—languages, but the need to use multiple languages and the non-interactive nature of compiled languages make this approach tedious.\nJulia uses just-in-time (JIT) compilation: the code is compiled at run time. This combines the interactive advantage of interpreted languages with the efficiency of compiled ones. Basically, it feels like running R or python, while it is almost as fast as C. This makes Julia particularly well suited for big data analyses, machine learning, or heavy modelling.\nIn addition, multiple dispatch (generic functions with multiple methods depending on the types of all the arguments) is at the very core of Julia. This is extremly convenient, cutting on conditionals and repetitions, and allowing for easy extensibility without having to rewrite code.\nFinally, Julia shines by its extremely clean and concise syntax. This last feature makes it easy to learn and really enjoyable to use.\nIn this workshop, which does not require any prior experience in Julia (experience in another language—e.g. R or python—would be best), we will go over the basics of Julia’s syntax and package system; then we will push the performance aspect further by looking at how Julia can make use of clusters for large scale parallel computing.",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Introduction to Julia"
    ]
  },
  {
    "objectID": "julia/intro_intro.html#introducing-julia",
    "href": "julia/intro_intro.html#introducing-julia",
    "title": "Introduction to Julia",
    "section": "Introducing Julia",
    "text": "Introducing Julia\n\nBrief history\nStarted in 2009 by Jeff Bezanson, Stefan Karpinski, Viral B. Shah, and Alan Edelman, the general-purpose programming language Julia was launched in 2012 as free and open source software. Version 1.0 was released in 2018.\nRust developer Graydon Hoare wrote an interesting post which places Julia in a historical context of programming languages.\n\n\nWhy another language?\n\nJIT\nComputer languages mostly fall into two categories: compiled languages and interpreted languages.\n\nCompiled languages\nCompiled languages require two steps:\n\nin a first step the code you write in a human-readable format (the source code, usually in plain text) gets compiled into machine code\nit is then this machine code that is used to process your data\n\nSo you write a script, compile it, then use it.\n\nBecause machine code is a lot easier to process by computers, compiled languages are fast. The two step process however makes prototyping new code less practical, these languages are hard to learn, and debugging compilation errors can be challenging.\n\nExamples of compiled languages include C, C++, Fortran, Go, and Haskell.\n\n\n\nInterpreted languages\nInterpreted languages are executed directly which has many advantages such as dynamic typing and direct feed-back from the code and they are easy to learn, but this comes at the cost of efficiency. The source code can facultatively be bytecompiled into non human-readable, more compact, lower level bytecode which is read by the interpreter more efficiently.\n\n\nExamples of interpreted languages include R, Python, Perl, and JavaScript.\n\n\n\nA common workflow\nSo, with this, what do researchers do?\nA common workflow, with the constraints of either type of languages, consists of:\n\nexploring the data and developing code using a sample of the data or reasonably light computations in an interpreted language,\ntranslating the code into a compiled language,\nfinally throwing the full data and all the heavy duty computation at that optimized code.\n\nThis works and it works well.\nBut, as you can imagine, this roundabout approach is tedious, not to mention the fact that it involves mastering 2 languages.\n\n\nJIT compiled languages\nJulia uses just-in-time compilation or JIT based on LLVM: the source code is compiled at run time. This combines the flexibility of interpretation with the speed of compilation, bringing speed to an interactive language. It also allows for dynamic recompilation, continuous weighing of gains and costs of the compilation of parts of the code, and other on the fly optimizations.\nOf course, there are costs here too. They come in the form of overhead time to compile code the first time it is run and increased memory usage.\n\n\n\nMultiple dispatch\nIn languages with multiple dispatch, functions apply different methods at run time based on the type of the operands. This brings great type stability and improves speed.\nJulia is extremely flexible: type declaration is not required. Out of convenience, you can forego the feature if you want. Specifying types however will greatly optimize your code.\nHere is a good post on type stability, multiple dispatch, and Julia efficiency.",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Introduction to Julia"
    ]
  },
  {
    "objectID": "julia/intro_intro.html#how-to-run-julia",
    "href": "julia/intro_intro.html#how-to-run-julia",
    "title": "Introduction to Julia",
    "section": "How to run Julia?",
    "text": "How to run Julia?\nThere are several ways to run Julia interactively:\n\ndirectly in the REPL (read–eval–print loop: the interactive Julia shell),\nin interactive notebooks (e.g. Jupyter, Pluto),\nin an editor able to run Julia interactively (e.g. Emacs, VS Code, Vim).\n\nLet’s have a look at these interfaces.\n\nThe Julia REPL\nYou can launch the REPL from a terminal directly by typing the julia command.\n\nREPL keybindings\nIn the REPL, you can use standard command line keybindings (Emacs kbd):\nC-c     cancel\nC-d     quit\nC-l     clear console\n\nC-u     kill from the start of line\nC-k     kill until the end of line\n\nC-a     go to start of line\nC-e     go to end of line\n\nC-f     move forward one character\nC-b     move backward one character\n\nM-f     move forward one word\nM-b     move backward one word\n\nC-d     delete forward one character\nC-h     delete backward one character\n\nM-d     delete forward one word\nM-Backspace delete backward one word\n\nC-p     previous command\nC-n     next command\n\nC-r     backward search\nC-s     forward search\n\n\nREPL modes\nThe Julia REPL is unique in that it has four distinct modes:\njulia&gt;     The main mode in which you will be running your code.\nhelp?&gt;     A mode to easily access documentation.\nshell&gt;     A mode in which you can run bash commands from within Julia.\n(env) pkg&gt;   A mode to easily perform actions on packages with Julia package manager.\n(env is the name of your current project environment.\nProject environments are similar to Python’s virtual environments and allow you, for instance, to have different package versions for different projects. By default, it is the current Julia version. So what you will see is (v1.3) pkg&gt;).\nEnter the various modes by typing ?, ;, and ]. Go back to the regular mode with the Backspace key.\n\n\n\nText editors\n\nVS Code\nJulia for Visual Studio Code has become the main Julia IDE.\n\n\nEmacs\n\nthrough the julia-emacs and julia-repl packages\nthrough the ESS package\nthrough the Emacs IPython Notebook package if you want to access Jupyter notebooks in Emacs\n\n\n\nVim\nThrough the julia-vim package.\n\n\n\nInteractive notebooks\n\nJupyter\nProject Jupyter allows to create interactive programming documents through its web-based JupyterLab environment and its Jupyter Notebook.\n\n\nPluto\nThe Julia package Juno is a reactive notebook for Julia.\n\n\n\nQuarto\nQuarto builds interactive documents with code and runs Julia through Jupyter.",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Introduction to Julia"
    ]
  },
  {
    "objectID": "julia/intro_intro.html#startup-options",
    "href": "julia/intro_intro.html#startup-options",
    "title": "Introduction to Julia",
    "section": "Startup options",
    "text": "Startup options\nYou can configure Julia by creating the file ~/.julia/config/startup.jl.",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Introduction to Julia"
    ]
  },
  {
    "objectID": "julia/intro_intro.html#help-and-documentation",
    "href": "julia/intro_intro.html#help-and-documentation",
    "title": "Introduction to Julia",
    "section": "Help and documentation",
    "text": "Help and documentation\nAs we already saw, you can type ? to enter the help mode:\n?sum\nsearch: sum sum! summary cumsum cumsum! isnumeric VersionNumber issubnormal \nget_zero_subnormals set_zero_subnormals\n\n  sum(f, itr; [init])\n\n  Sum the results of calling function f on each element of itr.\n\n  The return type is Int for signed integers of less than system word size, \n  and UInt for unsigned integers of less than system word size. For all other \n  arguments a common return type is found to which all arguments are promoted.\n\n  The value returned for empty itr can be specified by init. It must be the \n  additive identity (i.e. zero) as it is unspecified whether init is used for \n  non-empty collections.\n\nI truncated this output as the documentation also contains many examples.\n\nTo print the list of functions containing a certain word in their description, you can use apropos().\n\nExample:\n\n\napropos(\"truncate\")\n\nBase.IOContext\nBase.IOBuffer\nBase.open\nBase.truncate\nBase.open_flags\nCore.String\nBase.dump\nBase.Broadcast.newindex\nArgTools\nNetworkOptions\nLinearAlgebra.eigen\nTar\nDates.format\nBase.trunc\nDates.Date\nIJulia.watch_stream\nIJulia.set_max_stdio\nAbstractTrees.print_tree\nAbstractTrees.TreeCharSet\nOffsetArrays\nPDMats\nStatsFuns\nDistributions\nDistributions.truncated\nDistributions.Distributions\nDistributions.Truncated\nDistributions.TruncatedNormal\nLazyModules\nSimpleRandom\nMods\nMultisets\nPolynomials.truncate!\nBase.truncate\nSimplePolynomials\nLinearAlgebraX\nPermutations\nDelaunayTriangulation.grow_polygon_outside_of_box\nDelaunayTriangulation.clip_unbounded_polygon_to_bounding_box\nStableHashTraits\nMakie\nMakie.to_vertices",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Introduction to Julia"
    ]
  },
  {
    "objectID": "julia/intro_intro.html#version-information",
    "href": "julia/intro_intro.html#version-information",
    "title": "Introduction to Julia",
    "section": "Version information",
    "text": "Version information\nJulia version only:\n\nversioninfo()\n\nJulia Version 1.10.3\nCommit 0b4590a5507 (2024-04-30 10:59 UTC)\nBuild Info:\n  Official https://julialang.org/ release\nPlatform Info:\n  OS: Linux (x86_64-linux-gnu)\n  CPU: 16 × Intel(R) Core(TM) i7-10875H CPU @ 2.30GHz\n  WORD_SIZE: 64\n  LIBM: libopenlibm\n  LLVM: libLLVM-15.0.7 (ORCJIT, skylake)\nThreads: 1 default, 0 interactive, 1 GC (on 16 virtual cores)\n\n\nMore information, including commit, OS, CPU, and compiler:\n\nVERSION\n\nv\"1.10.3\"",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Introduction to Julia"
    ]
  },
  {
    "objectID": "julia/intro_intro.html#lets-try-a-few-commands",
    "href": "julia/intro_intro.html#lets-try-a-few-commands",
    "title": "Introduction to Julia",
    "section": "Let’s try a few commands",
    "text": "Let’s try a few commands\nx = 10\nx\nx = 2;\nx\ny = x;\ny\nans\nans + 3\n\na, b, c = 1, 2, 3\nb\n\n3 + 2\n+(3, 2)\n\na = 3\n2a\na += 7\na\n\n2\\8\n\na = [1 2; 3 4]\nb = a\na[1, 1] = 0\nb\n\n[1, 2, 3, 4]\n[1 2; 3 4]\n[1 2 3 4]\n[1 2 3 4]'\ncollect(1:4)\ncollect(1:1:4)\n1:4\na = 1:4\ncollect(a)\n\n[1, 2, 3] .* [1, 2, 3]\n\n4//8\n8//1\n1//2 + 3//4\n\na = true\nb = false\na + b\n\n\nYour turn:\n\nWhat does ; at the end of a command do?\nWhat is surprising about 2a?\nWhat does += do?\nWhat does .*do?\n\na = [3, 1, 2]\n\nsort(a)\nprintln(a)\n\nsort!(a)\nprintln(a)\n\n\nYour turn:\n\nWhat does ! at the end of a function name do?",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Introduction to Julia"
    ]
  },
  {
    "objectID": "julia/intro_control_flow.html",
    "href": "julia/intro_control_flow.html",
    "title": "Control flow",
    "section": "",
    "text": "Control flow statements alter the linear execution of code, allowing for one or another section of code to be executed, or for one section of code to be executed multiple times.",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Control flow"
    ]
  },
  {
    "objectID": "julia/intro_control_flow.html#conditional-statements",
    "href": "julia/intro_control_flow.html#conditional-statements",
    "title": "Control flow",
    "section": "Conditional statements",
    "text": "Conditional statements\nConditional statements allow to run instructions based on predicates: different sets of instructions will be executed depending on whether the predicates return true or false.\n\nPredicates\n\nHere are a few examples of predicates with classic operators:\n\noccursin(\"that\", \"this and that\")\n4 &lt; 3\na == b\na != b\n2 in 1:3\n3 &lt;= 4 && 4 &gt; 5\n3 &lt;= 4 || 4 &gt; 5\nIn addition, Julia possesses more exotic operators that can be used in predicates:\n\nThe inexact equality comparator, useful to compare floating-point numbers despite computer rounding.\n\n\nThe function isapprox or the equivalent binary operator ≈ (typed with \\approx&lt;tab&gt;) can be used:\n\n0.1 + 0.2 == 0.3\n\nfalse\n\n\n\n0.1 + 0.2 ≈ 0.3\n\ntrue\n\n\n\nisapprox(0.1 + 0.2, 0.3)\n\ntrue\n\n\nThe negatives are the function !isapprox and ≉ (typed with \\napprox&lt;tab&gt;).\n\n\nThe equivalent or triple equal operator compares objects in deeper ways (address in memory for mutable objects and content at the bit level for immutable objects).\n\n\n=== or ≡ (typed with \\equiv&lt;tab&gt;) can be used:\n\na = [1, 2]; b = [1, 2];\n\n\na == b\n\ntrue\n\n\n\na ≡ b     # This can also be written `a === b`\n\nfalse\n\n\n\na ≡ a\n\ntrue\n\n\n\n\n\nIf statements\nif &lt;predicate&gt;\n    &lt;some action&gt;\nend\n\nIf &lt;predicate&gt; evaluates to true, the body of the if statement gets evaluated (&lt;some action&gt; is run),\nIf &lt;predicate&gt; evaluates to false, nothing happens.\n\n\nExample:\n\n\nfunction testsign1(x)\n    if x &gt;= 0\n        println(\"x is positive\")\n    end\nend\n\ntestsign1 (generic function with 1 method)\n\n\n\ntestsign1(3)\n\nx is positive\n\n\n\ntestsign1(-2)\n\n\nNothing gets returned since the predicate returned false.\n\n\n\nIf else statements\nif &lt;predicate&gt;\n    &lt;some action&gt;\nelse\n    &lt;some other action&gt;\nend\n\nIf &lt;predicate&gt; evaluates to true, &lt;some action&gt; is done,\nIf &lt;predicate&gt; evaluates to false, &lt;some other action&gt; is done.\n\n\nExample:\n\n\nfunction testsign2(x)\n    if x &gt;= 0\n        println(\"x is positive\")\n    else\n        println(\"x is negative\")\n    end\nend\n\ntestsign2 (generic function with 1 method)\n\n\n\ntestsign2(3)\n\nx is positive\n\n\n\ntestsign2(-2)\n\nx is negative\n\n\nIf else statements can be written in a terse format using the ternary operator:\n&lt;predicate&gt; ? &lt;some action&gt; : &lt;some other action&gt;\n\nHere is our function testsign2 written in terse format:\n\n\nfunction testsign2(x)\n    x &gt;= 0 ? println(\"x is positive\") : println(\"x is negative\")\nend\n\ntestsign2(-2)\n\nx is negative\n\n\n\nHere is another example:\n\na = 2\nb = 2.0\n\nif a == b\n    println(\"It's true\")\nelse\n    println(\"It's false\")\nend\nAnd in terse format:\n\na == b ? println(\"It's true\") : println(\"It's false\")\n\nIt's true\n\n\n\n\nIf elseif else statements\nif &lt;predicate1&gt;\n    &lt;some action&gt;\nelseif &lt;predicate2&gt;\n    &lt;some other action&gt;\nelse\n    &lt;yet some other action&gt;\nend\n\nExample:\n\n\nfunction testsign3(x)\n    if x &gt; 0\n        println(\"x is positive\")\n    elseif x == 0\n        println(\"x is zero\")\n    else\n        println(\"x is negative\")\n    end\nend\n\ntestsign3 (generic function with 1 method)\n\n\n\ntestsign3(3)\n\nx is positive\n\n\n\ntestsign3(0)\n\nx is zero\n\n\n\ntestsign3(-2)\n\nx is negative",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Control flow"
    ]
  },
  {
    "objectID": "julia/intro_control_flow.html#loops",
    "href": "julia/intro_control_flow.html#loops",
    "title": "Control flow",
    "section": "Loops",
    "text": "Loops\n\nFor loops\nFor loops run a set of instructions for each element of an iterable:\nfor &lt;iterable&gt;\n    &lt;some action&gt;\nend\n\nExamples:\n\n\nfor name = [\"Paul\", \"Lucie\", \"Sophie\"]\n    println(\"Hello $name\")\nend\n\nHello Paul\nHello Lucie\nHello Sophie\n\n\n\nfor i = 1:3, j = 3:5\n    println(i + j)\nend\n\n4\n5\n6\n5\n6\n7\n6\n7\n8\n\n\n\n\nWhile loops\nWhile loops run as long as a condition remains true:\nwhile &lt;predicate&gt;\n    &lt;some action&gt;\nend\n\nExample:\n\n\ni = 0\n\nwhile i &lt;= 10\n    println(i)\n    i += 1\nend\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Control flow"
    ]
  },
  {
    "objectID": "julia/intro_basics.html",
    "href": "julia/intro_basics.html",
    "title": "Basics of the Julia language",
    "section": "",
    "text": "Comments do not get evaluated by Julia and are for humans only.\n\n# Comments in Julia are identified by hastags\n\n\n#=\nComments can also spread over multiple lines\nif you enclose them with this syntax\n=#\n\n\nx = 2          # Comments can be added at the end of lines\n\n2",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Basics of the Julia language"
    ]
  },
  {
    "objectID": "julia/intro_basics.html#comments",
    "href": "julia/intro_basics.html#comments",
    "title": "Basics of the Julia language",
    "section": "",
    "text": "Comments do not get evaluated by Julia and are for humans only.\n\n# Comments in Julia are identified by hastags\n\n\n#=\nComments can also spread over multiple lines\nif you enclose them with this syntax\n=#\n\n\nx = 2          # Comments can be added at the end of lines\n\n2",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Basics of the Julia language"
    ]
  },
  {
    "objectID": "julia/intro_basics.html#basic-operations",
    "href": "julia/intro_basics.html#basic-operations",
    "title": "Basics of the Julia language",
    "section": "Basic operations",
    "text": "Basic operations\n\n# By default, Julia returns the output\n2 + 3\n\n5\n\n\n\n# Trailing semi-colons suppress the output\n3 + 7;\n\n\n# Alternative syntax that can be used with operators\n+(2, 5)\n\n7\n\n\n\n# Updating operators\na = 3\na += 8    # this is the same as a = a + 8\n\n11\n\n\n\n# Operator precedence follows standard rules\n3 + 2 ^ 3 * 10\n\n83\n\n\n\nMore exotic operators\n\n# Usual division\n6 / 2\n\n3.0\n\n\n\n# Inverse division\n2 \\ 6\n\n3.0\n\n\n\n# Integer division (division truncated to an integer)\n7 ÷ 2\n\n3\n\n\n\n# Remainder\n7 % 2        # equivalent to rem(7, 2)\n\n1\n\n\n\n# Fraction\n4//8\n\n1//2\n\n\n\n# Julia supports fraction operations\n1//2 + 3//4\n\n5//4",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Basics of the Julia language"
    ]
  },
  {
    "objectID": "julia/intro_basics.html#variables",
    "href": "julia/intro_basics.html#variables",
    "title": "Basics of the Julia language",
    "section": "Variables",
    "text": "Variables\n\n\nfrom xkcd.com\n\nA variable is a name bound to a value:\n\na = 3;\n\nIt can be called:\n\na\n\n3\n\n\nOr used in expressions:\n\na + 2\n\n5\n\n\n\nAssignment\nYou can re-assign new values to variables:\n\na = 3;\na = -8.2;\na\n\n-8.2\n\n\nEven values of a different type:\n\na = \"a is now a string\"\n\n\"a is now a string\"\n\n\nYou can define multiple variables at once:\n\na, b, c = 1, 2, 3\nb\n\n2\n\n\n\n\nVariable names\nThese names are extremely flexible and can use Unicode character:\n\\omega       # press TAB\n\\sum         # press TAB\n\\sqrt        # press TAB\n\\in          # press TAB\n\\:phone:     # press TAB\n\nδ = 8.5;\n🐌 = 3;\nδ + 🐌\n\n11.5\n\n\nAdmittedly, using emojis doesn’t seem very useful, but using Greek letters to write equations really makes Julia a great mathematical language:\n\nσ = 3\nδ = π\nϕ = 8\n\n(5σ + 3δ) / ϕ\n\n3.0530972450961724\n\n\n\nNote how the multiplication operator can be omitted when this does not lead to confusion. Also note how the mathematical constant π is available in Julia without having to load any module.\n\nIf you want to know how to type a symbol, ask Julia: type ? and paste it in the REPL.\nThe only hard rules for variable names are:\n\nThey must begin with a letter or an underscore,\nThey cannot take the names of built-in keywords such as if, do, try, else,\nThey cannot take the names of built-in constants (e.g. π) and keywords in use in a session.\n\n\nWe thus get an error here:\n\n\nfalse = 3\n\nLoadError: syntax: invalid assignment location \"false\" around In[24]:1\nsyntax: invalid assignment location \"false\" around In[24]:1\n\nStacktrace:\n [1] top-level scope\n   @ In[24]:1\n\n\nIn addition, the Julia Style Guide recommends to follow these conventions:\n\nUse lower case,\nWord separation can be indicated by underscores, but better not to use them if the names can be read easily enough without them.\n\n\n\nThe ans variable\nThe keyword ans is a variable which, in the REPL, takes the value of the last computation:\na = 3 ^ 2;\nans + 1\n10\n\n\nPrinting\nTo print the value of a variable in an interactive session, you only need to call it:\n\na = 3;\na\n\n3\n\n\nIn non interactive sessions, you have to use the println function:\n\nprintln(a)\n\n3",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Basics of the Julia language"
    ]
  },
  {
    "objectID": "julia/intro_basics.html#quotes",
    "href": "julia/intro_basics.html#quotes",
    "title": "Basics of the Julia language",
    "section": "Quotes",
    "text": "Quotes\nNote the difference between single and double quotes:\n\ntypeof(\"a\")\n\nString\n\n\n\ntypeof('a')\n\nChar\n\n\n\n\"This is a string\"\n\n\"This is a string\"\n\n\n\n'This is not a sring'\n\n\nParseError:\n# Error @ ]8;;file:///home/marie/parvus/prog/mint/julia/In[30]#1:2\\In[30]:1:2]8;;\\\n'This is not a sring'\n#└─────────────────┘ ── character literal contains multiple characters\n\nStacktrace:\n [1] top-level scope\n   @ In[30]:1\n\n\n\n\nWe got an error here since ' is used for the character type and can thus only contain a single character.\n\n\n'a'\n\n'a': ASCII/Unicode U+0061 (category Ll: Letter, lowercase)",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Basics of the Julia language"
    ]
  },
  {
    "objectID": "julia/hpc_performance.html",
    "href": "julia/hpc_performance.html",
    "title": "Performance",
    "section": "",
    "text": "The one thing you need to remember: avoid global variables.\nThis means: avoid variables defined in the global environment.",
    "crumbs": [
      "Julia",
      "<b><em>High-performance Julia</em></b>",
      "Performance"
    ]
  },
  {
    "objectID": "julia/hpc_performance.html#definitions",
    "href": "julia/hpc_performance.html#definitions",
    "title": "Performance",
    "section": "Definitions",
    "text": "Definitions\nScope of variables:   Environment within which a variables exist\nGlobal scope:     Global environment of a module\nLocal scope:      Environment within a function, a loop, a struct, a macro, etc.",
    "crumbs": [
      "Julia",
      "<b><em>High-performance Julia</em></b>",
      "Performance"
    ]
  },
  {
    "objectID": "julia/hpc_performance.html#why-avoid-global-variables",
    "href": "julia/hpc_performance.html#why-avoid-global-variables",
    "title": "Performance",
    "section": "Why avoid global variables?",
    "text": "Why avoid global variables?\nThe Julia compiler is not good at optimizing code using global variables.\nPart of the reason is that their type can change.\n\nExample\nWe will use the @time macro to time a loop:\n\nIn the global environment:\n\n\ntotal = 0\nn = 1e6\n\n@time for i in 1:n\n    global total += i\nend\n\n  0.364778 seconds (4.00 M allocations: 76.360 MiB, 64.25% gc time, 4.36% compilation time)\n\n\n\nNote the garbage collection (gc) time: 14% of total time.\nGarbage collection time is a sign of poor code.\n\n\nIn a local environment (a function):\n\n\nfunction local_loop(total, n)\n    total = total\n    @time for i in 1:n\n        global total += i\n    end\nend\n\nlocal_loop(0, 1e6)\n\n  0.026919 seconds (2.00 M allocations: 30.518 MiB)\n\n\n\nWe get a 7.5 speedup.\nThe memory allocation also decreased by more than half.\n\nFor more accurate performance measurements, you should use the @btime macro from the BenchmarkTools package which excludes compilation time from the timing, averages metrics over multiple runs, and is highly customizable.",
    "crumbs": [
      "Julia",
      "<b><em>High-performance Julia</em></b>",
      "Performance"
    ]
  },
  {
    "objectID": "julia/hpc_intro.html",
    "href": "julia/hpc_intro.html",
    "title": "Introduction to high performance research computing in Julia",
    "section": "",
    "text": "When you launch a Jupyter session from a JupyterHub, you are running a Slurm job on a compute node. If you want to play for 8 hours in Jupyter, you are requesting an 8 hour job. Now, most of the time you spend on Jupyter is spent typing, running bits and pieces of code, or doing nothing at all. If you ask for GPUs, many CPUs, and lots of RAM, all of it will remain idle most of the time. This is a suboptimal use of resources.\nIn addition, if you ask for lots of resources for a long time, you will have to wait for a while before they get allocated to you.\nLastly, you will go through your allocations quickly.\nAll of this applies equally for interactive sessions launched from an SSH session with salloc.",
    "crumbs": [
      "Julia",
      "<b><em>High-performance Julia</em></b>",
      "Intro to HPC in Julia"
    ]
  },
  {
    "objectID": "julia/hpc_intro.html#interactive-sessions-for-high-performance-computing",
    "href": "julia/hpc_intro.html#interactive-sessions-for-high-performance-computing",
    "title": "Introduction to high performance research computing in Julia",
    "section": "",
    "text": "When you launch a Jupyter session from a JupyterHub, you are running a Slurm job on a compute node. If you want to play for 8 hours in Jupyter, you are requesting an 8 hour job. Now, most of the time you spend on Jupyter is spent typing, running bits and pieces of code, or doing nothing at all. If you ask for GPUs, many CPUs, and lots of RAM, all of it will remain idle most of the time. This is a suboptimal use of resources.\nIn addition, if you ask for lots of resources for a long time, you will have to wait for a while before they get allocated to you.\nLastly, you will go through your allocations quickly.\nAll of this applies equally for interactive sessions launched from an SSH session with salloc.",
    "crumbs": [
      "Julia",
      "<b><em>High-performance Julia</em></b>",
      "Intro to HPC in Julia"
    ]
  },
  {
    "objectID": "julia/hpc_intro.html#a-better-approach",
    "href": "julia/hpc_intro.html#a-better-approach",
    "title": "Introduction to high performance research computing in Julia",
    "section": "A better approach",
    "text": "A better approach\nA more efficient strategy is to develop and test your code with small samples, few iterations, etc. in an interactive job (from an SSH session in the cluster with salloc), on your own computer, or in Jupyter. Once you are confident that your code works, launch an sbatch job from an SSH session in the cluster to run the code as a script on all your data. This ensures that heavy duty resources that you requested are actually put to use to run your heavy calculations and not seating idle while you are thinking, typing, etc.",
    "crumbs": [
      "Julia",
      "<b><em>High-performance Julia</em></b>",
      "Intro to HPC in Julia"
    ]
  },
  {
    "objectID": "julia/hpc_intro.html#logging-on-to-the-cluster",
    "href": "julia/hpc_intro.html#logging-on-to-the-cluster",
    "title": "Introduction to high performance research computing in Julia",
    "section": "Logging on to the cluster",
    "text": "Logging on to the cluster\nOpen a terminal emulator:\nWindows users:  launch MobaXTerm.\nmacOS users:   launch Terminal.\nLinux users:     launch xterm or the terminal emulator of your choice.\nThen access the cluster through secure shell:\n$ ssh &lt;username&gt;@&lt;hostname&gt;    # enter password",
    "crumbs": [
      "Julia",
      "<b><em>High-performance Julia</em></b>",
      "Intro to HPC in Julia"
    ]
  },
  {
    "objectID": "julia/hpc_intro.html#accessing-julia",
    "href": "julia/hpc_intro.html#accessing-julia",
    "title": "Introduction to high performance research computing in Julia",
    "section": "Accessing Julia",
    "text": "Accessing Julia\nThis is done with the Lmod tool through the module command. You can find the full documentation here and below are the subcommands you will need:\n# get help on the module command\n$ module help\n$ module --help\n$ module -h\n\n# list modules that are already loaded\n$ module list\n\n# see which modules are available for Julia\n$ module spider julia\n\n# see how to load julia 1.3\n$ module spider julia/1.3.0\n\n# load julia 1.3 with the required gcc module first\n# (the order is important)\n$ module load gcc/7.3.0 julia/1.3.0\n\n# you can see that we now have Julia loaded\n$ module list",
    "crumbs": [
      "Julia",
      "<b><em>High-performance Julia</em></b>",
      "Intro to HPC in Julia"
    ]
  },
  {
    "objectID": "julia/hpc_intro.html#copying-files-to-the-cluster",
    "href": "julia/hpc_intro.html#copying-files-to-the-cluster",
    "title": "Introduction to high performance research computing in Julia",
    "section": "Copying files to the cluster",
    "text": "Copying files to the cluster\nWe will create a julia_workshop directory in ~/scratch, then copy our julia script in it.\n$ mkdir ~/scratch/julia_job\nOpen a new terminal window and from your local terminal (make sure that you are not on the remote terminal by looking at the bash prompt) run:\n$ scp /local/path/to/sort.jl &lt;username&gt;@&lt;hostname&gt;:scratch/julia_job\n$ scp /local/path/to/psort.jl &lt;username&gt;@&lt;hostname&gt;:scratch/julia_job\n\n# enter password",
    "crumbs": [
      "Julia",
      "<b><em>High-performance Julia</em></b>",
      "Intro to HPC in Julia"
    ]
  },
  {
    "objectID": "julia/hpc_intro.html#job-scripts",
    "href": "julia/hpc_intro.html#job-scripts",
    "title": "Introduction to high performance research computing in Julia",
    "section": "Job scripts",
    "text": "Job scripts\nWe will not run an interactive session with Julia on the cluster: we already have julia scripts ready to run. All we need to do is to write job scripts to submit to Slurm, the job scheduler used by the Alliance clusters.\nWe will create 2 scripts: one to run Julia on one core and one on as many cores as are available.\n\n\nYour turn:\n\nHow many processors are there on our training cluster?\n\nWe can run Julia with multiple threads by running:\n$ JULIA_NUM_THREADS=2 julia\nor:\n$ julia -t 2\nOnce in Julia, you can double check that Julia does indeed have access to 2 threads by running:\nThreads.nthreads()\nSave your job scripts in the files ~/scratch/julia_job/job_julia1c.sh and job_julia2c.sh for one and two cores respectively.\nHere is what our single core Slurm script looks like:\n#!/bin/bash\n#SBATCH --job-name=julia1c          # job name\n#SBATCH --time=00:01:00             # max walltime 1 min\n#SBATCH --cpus-per-task=1           # number of cores\n#SBATCH --mem=1000                  # max memory (default unit is megabytes)\n#SBATCH --output=julia1c%j.out      # file name for the output\n#SBATCH --error=julia1c%j.err       # file name for errors\n# %j gets replaced with the job number\n\necho Running NON parallel script\njulia sort.jl\necho Running parallel script on $SLURM_CPUS_PER_TASK core\njulia -t $SLURM_CPUS_PER_TASK psort.jl\n\n\nYour turn:\n\nWrite the script for 2 cores.\n\nNow, we can submit our jobs to the cluster:\n$ cd ~/scratch/julia_job\n$ sbatch job_julia1c.sh\n$ sbatch job_julia2c.sh\nAnd we can check their status with:\n$ sq      # This is an Alliance alias for `squeue -u $USER $@`\n\nPD stands for pending\nR stands for running",
    "crumbs": [
      "Julia",
      "<b><em>High-performance Julia</em></b>",
      "Intro to HPC in Julia"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Training in Research Computing",
    "section": "",
    "text": "AI\nDeep learning frameworks\n\n\n\n\nR\nStatistical programming\n\n\n\n\nPython\nScientific programming\n\n\n\n\nJulia\nJIT-compiled programming\n\n\n\n\n\n\nGit\nVersion control & collaboration\n\n\n\n\nBash/Zsh\nCommand line & Unix shell scripting\n\n\n\n\nEmacs\nPower editor & programming IDE\n\n\n\n\nTools\nTools for research computing\n\n\n\n\n\n\nMain website\nThis site contains content by Marie-Hélène Burle. To view all training material, please visit our main website."
  },
  {
    "objectID": "git/ws_contrib.html",
    "href": "git/ws_contrib.html",
    "title": "Collaborating to projects on GitHub",
    "section": "",
    "text": "There are countless free and open source tools on GitHub and if you use one such tool and find a problem or think that you can improve the project, or if you would like to request a novel feature, how do you go about it?\nIn this workshop, we will learn how to contribute to open source projects hosted on GitHub by opening issues and submitting pull requests.",
    "crumbs": [
      "Git",
      "<b><em>Workshops</em></b>",
      "Contributing to projects"
    ]
  },
  {
    "objectID": "git/ws_contrib.html#opening-issues",
    "href": "git/ws_contrib.html#opening-issues",
    "title": "Collaborating to projects on GitHub",
    "section": "Opening issues",
    "text": "Opening issues\nThe easiest thing to do, if for instance, you are having problems with the tool, found a bug, or want to submit a feature request, is to open an issue.\nGitHub has also now implemented the ability to open “Discussions”. If enabled by the maintainer of a project, this is the place where you want to ask for help.",
    "crumbs": [
      "Git",
      "<b><em>Workshops</em></b>",
      "Contributing to projects"
    ]
  },
  {
    "objectID": "git/ws_contrib.html#forking-a-project",
    "href": "git/ws_contrib.html#forking-a-project",
    "title": "Collaborating to projects on GitHub",
    "section": "Forking a project",
    "text": "Forking a project\nNow, a more advanced approach is to actually make changes to the code of the project.\nIf you want to develop your own version of the project, you can fork the GitHub repository: go to GitHub and fork the project by clicking on the “Fork” button in the top right corner.\nYou have all privileges on the forked project. So you can make any change you want there. You can clone it to your machine and develop the fork. But your fork does not get updated to the improvements made to the initial project. It is an independent project of its own.\n\nKeeping a fork up to date\nIf you want to keep your fork up to date with the initial project, you need to:\n\n1. Clone your fork on your machine\nThis will automatically set your fork on GitHub as the remote called origin:\n# If you have set SSH for your GitHub account\ngit clone git@github.com:&lt;user&gt;/&lt;repo&gt;.git &lt;name&gt;\n\n# If you haven't set SSH\ngit clone https://github.com/&lt;user&gt;/&lt;repo&gt;.git &lt;name&gt;\n\n\n2. Add the initial project as upstream\nAdd a second remote, this one pointing to the initial project. It is usual to call this remote upstream:\n# If you have set SSH for your GitHub account\ngit remote add upstream git@github.com:&lt;user&gt;/&lt;repo&gt;.git\n\n# If you haven't set SSH\ngit remote add upstream https://github.com/&lt;user&gt;/&lt;repo&gt;.git\n\n\n3. Pull from upstream\nYou can now pull from upstream to keep your fork up to date.\nFrom there on, you can pull from and push to origin (your fork) and you can pull from upstream (the initial repo).\nOf course, if your project and the initial one diverge in places, this will lead to conflicts that you will have to resolve as you merge the pulls from upstream.\nMost of the time however, you don’t want to develop your own version of the project. Instead, you want to make the initial project better by contributing to it. But you can’t push changes to upstream directly since you are not part of that project. You don’t have write access to that repository. If anybody could push to any project, that would be utter chaos.\nSo how do you contribute code to someone else’s project?",
    "crumbs": [
      "Git",
      "<b><em>Workshops</em></b>",
      "Contributing to projects"
    ]
  },
  {
    "objectID": "git/ws_contrib.html#creating-pull-requests",
    "href": "git/ws_contrib.html#creating-pull-requests",
    "title": "Collaborating to projects on GitHub",
    "section": "Creating pull requests",
    "text": "Creating pull requests\nHere is the workflow as described in the Git manual:\n\nPull from upstream to make sure that your contributions are made on an up-to-date version of the project\nCreate and checkout a new branch\nMake and commit your changes on that branch\nPush that branch to your fork (i.e. origin—remember that you do not have write access on upstream)\nGo to the original project GitHub’s page and open a pull request from your fork. Note that after you have pushed your branch to origin, GitHub will automatically offer you to do so.\n\nThe maintainer of the initial project may accept or decline the PR. They may also make comments and ask you to make changes. If so, make new changes and push additional commits to that branch until they are happy with the change.\nOnce the PR is merged by the maintainer, you can delete the branch on your fork and pull from upstream to update your local fork with the recently accepted changes.",
    "crumbs": [
      "Git",
      "<b><em>Workshops</em></b>",
      "Contributing to projects"
    ]
  },
  {
    "objectID": "git/wb_lazygit_slides.html#git-interfaces",
    "href": "git/wb_lazygit_slides.html#git-interfaces",
    "title": "A great Git UI: lazygit",
    "section": "Git interfaces",
    "text": "Git interfaces\nThere are 3 main ways to use Git:\n\nThrough a Git GUI\nFrom the command line\nIntegrated within IDE"
  },
  {
    "objectID": "git/wb_lazygit_slides.html#git-interfaces-1",
    "href": "git/wb_lazygit_slides.html#git-interfaces-1",
    "title": "A great Git UI: lazygit",
    "section": "Git interfaces",
    "text": "Git interfaces\nThey all have downsides:\n\nThrough a Git GUI      ➔  Slow and buggy\nFrom the command line   ➔  Austere and unintuitive\nIntegrated within IDE     ➔  Limited"
  },
  {
    "objectID": "git/wb_lazygit_slides.html#on-the-beauty-of-tuis",
    "href": "git/wb_lazygit_slides.html#on-the-beauty-of-tuis",
    "title": "A great Git UI: lazygit",
    "section": "On the beauty of TUIs",
    "text": "On the beauty of TUIs\nTerminal user interfaces (TUIs) were precursors to graphical user interfaces (GUIs), but they did not disappear\nPeople continue to build TUIs because they uniquely provide the speed of the command line and the easy of use of GUIs\nGitHub is full of sleek, modern, open source TUIs for all sorts of applications\nSeveral of them provide an interface to Git\nMy personal TUIs of choice are ranger as file manager and lazygit for Git"
  },
  {
    "objectID": "git/wb_lazygit_slides.html#lazygit",
    "href": "git/wb_lazygit_slides.html#lazygit",
    "title": "A great Git UI: lazygit",
    "section": "lazygit",
    "text": "lazygit\nWith over 52k stars on GitHub, lazygit, created and maintained by Jesse Duffield is probably the most polished Git TUI\nI followed it as it grew and developed over the past 5 years. It was great from the start, but by now, it is a truly beautiful mature tool\nIt is cross-platform. You can find installation instructions in the README"
  },
  {
    "objectID": "git/wb_lazygit_slides.html#lazygit-1",
    "href": "git/wb_lazygit_slides.html#lazygit-1",
    "title": "A great Git UI: lazygit",
    "section": "lazygit",
    "text": "lazygit\nGet command options:\nlazygit -h\nPrint default configurations with:\nlazygit -c\n\nlazygit is fully customizable"
  },
  {
    "objectID": "git/wb_lazygit_slides.html#resources",
    "href": "git/wb_lazygit_slides.html#resources",
    "title": "A great Git UI: lazygit",
    "section": "Resources",
    "text": "Resources\n\nRepo\nDefault kbds\nConfiguration options"
  },
  {
    "objectID": "git/wb_lazygit_slides.html#time-for-a-demo",
    "href": "git/wb_lazygit_slides.html#time-for-a-demo",
    "title": "A great Git UI: lazygit",
    "section": "Time for a demo!",
    "text": "Time for a demo!\nI will spend the rest of this webinar showing you how to use Git through lazygit"
  },
  {
    "objectID": "git/wb_dvc_slides.html#on-version-control",
    "href": "git/wb_dvc_slides.html#on-version-control",
    "title": "Version control for data science & machine learning with DVC",
    "section": "On version control",
    "text": "On version control\nI won’t introduce here the benefits of using a good version control system such as Git\n\n\n\nOn the benefits of VCS"
  },
  {
    "objectID": "git/wb_dvc_slides.html#extending-git-for-data",
    "href": "git/wb_dvc_slides.html#extending-git-for-data",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Extending Git for data",
    "text": "Extending Git for data\nWhile Git is a wonderful tool for text files versioning (code, writings in markup formats), it isn’t a tool to manage changes to datasets\nSeveral open source tools—each with a different structure and functioning—extend Git capabilities to track data: Git LFS, git-annex, lakeFS, Dolt, DataLad"
  },
  {
    "objectID": "git/wb_dvc_slides.html#extending-git-for-models-and-experiments",
    "href": "git/wb_dvc_slides.html#extending-git-for-models-and-experiments",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Extending Git for models and experiments",
    "text": "Extending Git for models and experiments\nReproducible research and collaboration on data science and machine learning projects involve more than datasets management:\nExperiments and the models they produce also need to be tracked"
  },
  {
    "objectID": "git/wb_dvc_slides.html#many-moving-parts",
    "href": "git/wb_dvc_slides.html#many-moving-parts",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Many moving parts",
    "text": "Many moving parts\n\n*hp = hyperparameter\n\n\n\n\n\n\n\n\n\n\ndata1\n\ndata1\n\n\n\nmodel1\n\nmodel1\n\n\n\ndata1-&gt;model1\n\n\n\n\n\nmodel2\n\nmodel2\n\n\n\ndata1-&gt;model2\n\n\n\n\n\nmodel3\n\nmodel3\n\n\n\ndata1-&gt;model3\n\n\n\n\n\ndata2\n\ndata2\n\n\n\ndata2-&gt;model1\n\n\n\n\n\ndata2-&gt;model2\n\n\n\n\n\ndata2-&gt;model3\n\n\n\n\n\ndata3\n\ndata3\n\n\n\ndata3-&gt;model1\n\n\n\n\n\ndata3-&gt;model2\n\n\n\n\n\ndata3-&gt;model3\n\n\n\n\n\nhp1\n\nhp1\n\n\n\nhp1-&gt;model1\n\n\n\n\n\nhp1-&gt;model2\n\n\n\n\n\nhp1-&gt;model3\n\n\n\n\n\nhp2\n\nhp2\n\n\n\nhp2-&gt;model1\n\n\n\n\n\nhp2-&gt;model2\n\n\n\n\n\nhp2-&gt;model3\n\n\n\n\n\nhp3\n\nhp3\n\n\n\nhp3-&gt;model1\n\n\n\n\n\nhp3-&gt;model2\n\n\n\n\n\nhp3-&gt;model3\n\n\n\n\n\nperformance\n\nperformance1 ... performance27\n\n\n\nmodel1-&gt;performance\n\n\n\n\n\nmodel1-&gt;performance\n\n\n\n\n\nmodel1-&gt;performance\n\n\n\n\n\nmodel1-&gt;performance\n\n\n\n\n\nmodel1-&gt;performance\n\n\n\n\n\nmodel1-&gt;performance\n\n\n\n\n\nmodel1-&gt;performance\n\n\n\n\n\nmodel1-&gt;performance\n\n\n\n\n\nmodel1-&gt;performance\n\n\n\n\n\nmodel2-&gt;performance\n\n\n\n\n\nmodel2-&gt;performance\n\n\n\n\n\nmodel2-&gt;performance\n\n\n\n\n\nmodel2-&gt;performance\n\n\n\n\n\nmodel2-&gt;performance\n\n\n\n\n\nmodel2-&gt;performance\n\n\n\n\n\nmodel2-&gt;performance\n\n\n\n\n\nmodel2-&gt;performance\n\n\n\n\n\nmodel2-&gt;performance\n\n\n\n\n\nmodel3-&gt;performance\n\n\n\n\n\nmodel3-&gt;performance\n\n\n\n\n\nmodel3-&gt;performance\n\n\n\n\n\nmodel3-&gt;performance\n\n\n\n\n\nmodel3-&gt;performance\n\n\n\n\n\nmodel3-&gt;performance\n\n\n\n\n\nmodel3-&gt;performance\n\n\n\n\n\nmodel3-&gt;performance\n\n\n\n\n\nmodel3-&gt;performance\n\n\n\n\n\n\n\n\n\n\n\nHow did we get performance17 again? 🤯"
  },
  {
    "objectID": "git/wb_dvc_slides.html#dvc-principles",
    "href": "git/wb_dvc_slides.html#dvc-principles",
    "title": "Version control for data science & machine learning with DVC",
    "section": "DVC principles",
    "text": "DVC principles\nLarge files (datasets, models…) are kept outside Git\nEach large file or directory put under DVC tracking has an associated .dvc file\nGit only tracks the .dvc files (metadata)\n\nWorkflows can be tracked for collaboration and reproducibility\n\n\nDVC functions as a Makefile and allows to only rerun what is necessary"
  },
  {
    "objectID": "git/wb_dvc_slides.html#installation",
    "href": "git/wb_dvc_slides.html#installation",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Installation",
    "text": "Installation\nFor Linux (other OSes, refer to the doc):\n\npip:\npip install dvc\nconda\npipx (if you want dvc available everywhere without having to activate virtual envs):\npipx install dvc\n\n\nOptional dependencies [s3], [gdrive], etc. for remote storage"
  },
  {
    "objectID": "git/wb_dvc_slides.html#how-to-run",
    "href": "git/wb_dvc_slides.html#how-to-run",
    "title": "Version control for data science & machine learning with DVC",
    "section": "How to run",
    "text": "How to run\n\nTerminal\ndvc ...\nVS Code extension\nPython library if installed via pip or conda\nimport dvc.api\n\n\nIn this webinar, I will use DVC through the command line"
  },
  {
    "objectID": "git/wb_dvc_slides.html#acknowledgements",
    "href": "git/wb_dvc_slides.html#acknowledgements",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nCode and data for this webinar modified from:\n\nReal Python\nDataLad handbook\nDVC documentation"
  },
  {
    "objectID": "git/wb_dvc_slides.html#the-project",
    "href": "git/wb_dvc_slides.html#the-project",
    "title": "Version control for data science & machine learning with DVC",
    "section": "The project",
    "text": "The project\ntree -L 3\n├── LICENSE\n├── data\n│   ├── prepared\n│   └── raw\n│       ├── train\n│       └── val\n├── metrics\n├── model\n├── requirements.txt\n└── src\n    ├── evaluate.py\n    ├── prepare.py\n    └── train.py"
  },
  {
    "objectID": "git/wb_dvc_slides.html#initialize-git-repo",
    "href": "git/wb_dvc_slides.html#initialize-git-repo",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Initialize Git repo",
    "text": "Initialize Git repo\ngit init\nInitialized empty Git repository in dvc/.git/\n\nThis creates the .git directory\n\n\ngit status\nOn branch main\n\nNo commits yet\n\nUntracked files:\n    LICENSE\n    data/\n    requirements.txt\n    src/"
  },
  {
    "objectID": "git/wb_dvc_slides.html#initialize-dvc-project",
    "href": "git/wb_dvc_slides.html#initialize-dvc-project",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Initialize DVC project",
    "text": "Initialize DVC project\ndvc init\nInitialized DVC repository.\n\nYou can now commit the changes to git.\n\nYou will also see a note about usage analytics collection and info on how to opt out\n\n\nA .dvc directory and a .dvcignore file got created"
  },
  {
    "objectID": "git/wb_dvc_slides.html#commit-dvc-system-files",
    "href": "git/wb_dvc_slides.html#commit-dvc-system-files",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Commit DVC system files",
    "text": "Commit DVC system files\n\nDVC automatically staged its system file for us:\ngit status\nOn branch main\n\nNo commits yet\n\nChanges to be committed:\n    new file:   .dvc/.gitignore\n    new file:   .dvc/config\n    new file:   .dvcignore\n\nUntracked files:\n    LICENSE\n    data/\n    requirements.txt\n    src/\n\nSo we can directly commit:\ngit commit -m \"Initialize DVC\""
  },
  {
    "objectID": "git/wb_dvc_slides.html#prepare-repo",
    "href": "git/wb_dvc_slides.html#prepare-repo",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Prepare repo",
    "text": "Prepare repo\nLet’s work in a virtual environment:\n# Create venv and add to .gitignore\npython -m venv venv && echo venv &gt; .gitignore\n\n# Activate venv\nsource venv/bin/activate\n\n# Update pip\npython -m pip install --upgrade pip\n\n# Install packages needed\npython -m pip install -r requirements.txt"
  },
  {
    "objectID": "git/wb_dvc_slides.html#clean-working-tree",
    "href": "git/wb_dvc_slides.html#clean-working-tree",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Clean working tree",
    "text": "Clean working tree\ngit add .gitignore LICENSE requirements.txt\ngit commit -m \"Add general files\"\ngit add src\ngit commit -m \"Add scripts\"\ngit status\nOn branch main\nUntracked files:\n    data/\n\n\nNow, it is time to deal with the data"
  },
  {
    "objectID": "git/wb_dvc_slides.html#put-data-under-dvc-tracking",
    "href": "git/wb_dvc_slides.html#put-data-under-dvc-tracking",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Put data under DVC tracking",
    "text": "Put data under DVC tracking\nWe are still not tracking any data:\ndvc status\nThere are no data or pipelines tracked in this project yet.\nYou can choose what to track as a unit (i.e. each picture individually, the whole data directory as a unit)\nLet’s break it down by set:\ndvc add data/raw/train\ndvc add data/raw/val"
  },
  {
    "objectID": "git/wb_dvc_slides.html#section",
    "href": "git/wb_dvc_slides.html#section",
    "title": "Version control for data science & machine learning with DVC",
    "section": "",
    "text": "This adds data to .dvc/cache/files and created 3 files in data/raw:\n\n.gitignore\ntrain.dvc\nval.dvc\n\nThe .gitignore tells Git not to track the data:\ncat data/raw/.gitignore\n/train\n/val\nThe .dvc files contain the metadata for the cached directories"
  },
  {
    "objectID": "git/wb_dvc_slides.html#tracked-data",
    "href": "git/wb_dvc_slides.html#tracked-data",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Tracked data",
    "text": "Tracked data\nWe are all good:\ndvc status\nData and pipelines are up to date."
  },
  {
    "objectID": "git/wb_dvc_slides.html#data-deduplication",
    "href": "git/wb_dvc_slides.html#data-deduplication",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Data (de)duplication",
    "text": "Data (de)duplication\nLink between checked-out version of a file/directory and the cache:\n\nCache ⟷ working directory\n\n\n\n\n\n\n\n\nDuplication\nEditable\n\n\n\n\nReflinks*\nOnly when needed\nYes\n\n\nHardlinks/Symlinks\nNo\nNo\n\n\nCopies\nYes\nYes\n\n\n\n*Reflinks only available for a few file systems (Btrfs, XFS, OCFS2, or APFS)"
  },
  {
    "objectID": "git/wb_dvc_slides.html#commit-the-metafiles",
    "href": "git/wb_dvc_slides.html#commit-the-metafiles",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Commit the metafiles",
    "text": "Commit the metafiles\nThe metafiles should be put under Git version control\n\nYou can configure DVC to automatically stage its newly created system files:\ndvc config [--system] [--global] core.autostage true\n\nYou can then commit directly:\ngit commit -m \"Initial version of data\"\ngit status\nOn branch main\nnothing to commit, working tree clean"
  },
  {
    "objectID": "git/wb_dvc_slides.html#track-changes-to-the-data",
    "href": "git/wb_dvc_slides.html#track-changes-to-the-data",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Track changes to the data",
    "text": "Track changes to the data\nLet’s make some change to the data:\nrm data/raw/val/n03445777/ILSVRC2012_val*\n\nRemember that Git is not tracking the data:\ngit status\nOn branch main\nnothing to commit, working tree clean\n\n\nBut DVC is:\ndvc status\ndata/raw/val.dvc:\n    changed outs:\n            modified:           data/raw/val"
  },
  {
    "objectID": "git/wb_dvc_slides.html#add-changes-to-dvc",
    "href": "git/wb_dvc_slides.html#add-changes-to-dvc",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Add changes to DVC",
    "text": "Add changes to DVC\ndvc add data/raw/val\ndvc status\nData and pipelines are up to date.\n\nNow we need to commit the changes to the .dvc file to Git:\ngit status\nOn branch main\nChanges to be committed:\n    modified:   data/raw/val.dvc\n\nStaging happened automatically because I have set the autostage option to true on my system\n\ngit commit -m \"Delete data/raw/val/n03445777/ILSVRC2012_val*\""
  },
  {
    "objectID": "git/wb_dvc_slides.html#check-out-older-versions",
    "href": "git/wb_dvc_slides.html#check-out-older-versions",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Check out older versions",
    "text": "Check out older versions\nWhat if we want to go back to the 1st version of our data?\nFor this, we first use Git to checkout the proper commit, then run dvc checkout to have the data catch up to the .dvc file\nTo avoid forgetting to run the commands that will make DVC catch up to Git, we can automate this process by installing Git hooks:\ndvc install"
  },
  {
    "objectID": "git/wb_dvc_slides.html#git-workflows",
    "href": "git/wb_dvc_slides.html#git-workflows",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Git workflows",
    "text": "Git workflows\ngit checkout is ok to have a look, but a detached HEAD is not a good place to create new commits\nLet’s create a new branch and switch to it:\ngit switch -c alternative\nSwitched to a new branch 'alternative'\nGoing back and forth between both versions of our data is now as simple as switching branch:\ngit switch main\ngit switch alternative"
  },
  {
    "objectID": "git/wb_dvc_slides.html#classic-workflow",
    "href": "git/wb_dvc_slides.html#classic-workflow",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Classic workflow",
    "text": "Classic workflow\nThe Git project (including .dvc files) go to a Git remote (GitHub/GitLab/Bitbucket/server)\nThe data go to a DVC remote (AWS/Azure/Google Drive/server/etc.)"
  },
  {
    "objectID": "git/wb_dvc_slides.html#dvc-remotes",
    "href": "git/wb_dvc_slides.html#dvc-remotes",
    "title": "Version control for data science & machine learning with DVC",
    "section": "DVC remotes",
    "text": "DVC remotes\nDVC can use many cloud storage or remote machines/server via SSH, WebDAV, etc.\nLet’s create a local remote here:\n# Create a directory outside the project\nmkdir ../remote\n\n# Setup default (-d) remote\ndvc remote add -d local_remote ../remote\nSetting 'local_remote' as a default remote.\ncat .dvc/config\n[core]\n    remote = local_remote\n['remote \"local_remote\"']\n    url = ../../remote"
  },
  {
    "objectID": "git/wb_dvc_slides.html#commit-remote-config",
    "href": "git/wb_dvc_slides.html#commit-remote-config",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Commit remote config",
    "text": "Commit remote config\nThe new remote configuration should be committed:\ngit status\nOn branch alternative\n\nChanges not staged for commit:\n    modified:   .dvc/config\ngit add .\ngit commit -m \"Config remote\""
  },
  {
    "objectID": "git/wb_dvc_slides.html#push-to-remotes",
    "href": "git/wb_dvc_slides.html#push-to-remotes",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Push to remotes",
    "text": "Push to remotes\nLet’s push the data from the cache (.dvc/cache) to the remote:\ndvc push\n2702 files pushed\n\nWith Git hooks installed, dvc push is automatically run after git push\n(But the data is pushed to the DVC remote while the files tracked by Git get pushed to the Git remote)\n\nBy default, the entire data cache gets pushed to the remote, but there are many options\n\nExample: only push data corresponding to a certain .dvc files\ndvc push data/raw/val.dvc"
  },
  {
    "objectID": "git/wb_dvc_slides.html#pull-from-remotes",
    "href": "git/wb_dvc_slides.html#pull-from-remotes",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Pull from remotes",
    "text": "Pull from remotes\ndvc fetch downloads data from the remote into the cache. To have it update the working directory, follow by dvc checkout\nYou can do these 2 commands at the same time with dvc pull"
  },
  {
    "objectID": "git/wb_dvc_slides.html#dvc-pipelines",
    "href": "git/wb_dvc_slides.html#dvc-pipelines",
    "title": "Version control for data science & machine learning with DVC",
    "section": "DVC pipelines",
    "text": "DVC pipelines\nDVC pipelines create reproducible workflows and are functionally similar to Makefiles\nEach step in a pipeline is created with dvc stage add and add an entry to a dvc.yaml file\n\ndvc stage add options:\n-n: name of stage\n-d: dependency\n-o: output\n\n\nEach stage contains:\n\ncmd: the command executed\ndeps: the dependencies\nouts: the outputs\n\nThe file is then used to visualize the pipeline and run it"
  },
  {
    "objectID": "git/wb_dvc_slides.html#example",
    "href": "git/wb_dvc_slides.html#example",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Example",
    "text": "Example\nLet’s create a pipeline to run a classifier on our data\nThe pipeline contains 3 steps:\n\nprepare\ntrain\nevaluate"
  },
  {
    "objectID": "git/wb_dvc_slides.html#create-a-pipeline",
    "href": "git/wb_dvc_slides.html#create-a-pipeline",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Create a pipeline",
    "text": "Create a pipeline\n1st stage (data preparation):\ndvc stage add -n prepare -d src/prepare.py -d data/raw \\\n    -o data/prepared/train.csv -o data/prepared/test.csv \\\n    python src/prepare.py\nAdded stage 'prepare' in 'dvc.yaml'\n\n2nd stage (training)\ndvc stage add -n train -d src/train.py -d data/prepared/train.csv \\\n    -o model/model.joblib \\\n    python src/train.py\nAdded stage `train` in 'dvc.yaml'\n\n\n3rd stage (evaluation)\ndvc stage add -n evaluate -d src/evaluate.py -d model/model.joblib \\\n    -M metrics/accuracy.json \\\n    python src/evaluate.py\nAdded stage `evaluate` in 'dvc.yaml'"
  },
  {
    "objectID": "git/wb_dvc_slides.html#commit-pipeline",
    "href": "git/wb_dvc_slides.html#commit-pipeline",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Commit pipeline",
    "text": "Commit pipeline\ngit commit -m \"Define pipeline\"\nprepare:\n    changed deps:\n            modified:           data/raw\n            modified:           src/prepare.py\n    changed outs:\n            deleted:            data/prepared/test.csv\n            deleted:            data/prepared/train.csv\ntrain:\n    changed deps:\n            deleted:            data/prepared/train.csv\n            modified:           src/train.py\n    changed outs:\n            deleted:            model/model.joblib\nevaluate:\n    changed deps:\n            deleted:            model/model.joblib\n            modified:           src/evaluate.py\n    changed outs:\n            deleted:            metrics/accuracy.json\n[main 4aa331b] Define pipeline\n 3 files changed, 27 insertions(+)\n create mode 100644 data/prepared/.gitignore\n create mode 100644 dvc.yaml\n create mode 100644 model/.gitignore"
  },
  {
    "objectID": "git/wb_dvc_slides.html#visualize-pipeline-in-a-dag",
    "href": "git/wb_dvc_slides.html#visualize-pipeline-in-a-dag",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Visualize pipeline in a DAG",
    "text": "Visualize pipeline in a DAG\ndvc dag\n+--------------------+         +------------------+\n| data/raw/train.dvc |         | data/raw/val.dvc |\n+--------------------+         +------------------+\n                  ***           ***\n                     **       **\n                       **   **\n                    +---------+\n                    | prepare |\n                    +---------+\n                          *\n                          *\n                          *\n                      +-------+\n                      | train |\n                      +-------+\n                          *\n                          *\n                          *\n                    +----------+\n                    | evaluate |\n                    +----------+"
  },
  {
    "objectID": "git/wb_dvc_slides.html#run-pipeline",
    "href": "git/wb_dvc_slides.html#run-pipeline",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Run pipeline",
    "text": "Run pipeline\ndvc repro\n'data/raw/train.dvc' didn't change, skipping\n'data/raw/val.dvc' didn't change, skipping\nRunning stage 'prepare':\n&gt; python src/prepare.py\nGenerating lock file 'dvc.lock'\nUpdating lock file 'dvc.lock'\n\nRunning stage 'train':\n&gt; python src/train.py\nUpdating lock file 'dvc.lock'\n\nRunning stage 'evaluate':\n&gt; python src/evaluate.py\nUpdating lock file 'dvc.lock'\nUse `dvc push` to send your updates to remote storage."
  },
  {
    "objectID": "git/wb_dvc_slides.html#dvc-repro-breakdown",
    "href": "git/wb_dvc_slides.html#dvc-repro-breakdown",
    "title": "Version control for data science & machine learning with DVC",
    "section": "dvc repro breakdown",
    "text": "dvc repro breakdown\n\ndvc repro runs the dvc.yaml file in a Makefile fashion\nFirst, it looks at the dependencies: the data didn’t change\nThen it ran the commands to produce the outputs (since it is our first run, we had no outputs)\nWhen the 1st stage is run, a dvc.lock is created with information on that part of the run\nWhen the 2nd and 3rd stages are run, dvc.lock is updated\nAt the end of the run dvc.lock contains all the info about the run we just did (version of the data used, etc.)\nA new directory called runs is created in .dvc/cache with cached data for this run"
  },
  {
    "objectID": "git/wb_dvc_slides.html#results-of-the-run",
    "href": "git/wb_dvc_slides.html#results-of-the-run",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Results of the run",
    "text": "Results of the run\n\nThe prepared data was created in data/prepared (with a .gitignore to exclude it from Git—you don’t want to track results in Git, but the scripts that can reproduce them)\nA model was saved in model (with another .gitignore file)\nThe accuracy of this run was created in metrics"
  },
  {
    "objectID": "git/wb_dvc_slides.html#clean-working-tree-1",
    "href": "git/wb_dvc_slides.html#clean-working-tree-1",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Clean working tree",
    "text": "Clean working tree\nNow, we definitely want to create a commit with the dvc.lock\nWe could add the metrics resulting from this run in the same commit:\ngit add metrics\ngit commit -m \"First pipeline run and results\"\n\nOur working tree is now clean and our data/pipeline up to date:\ngit status\nOn branch alternative\nnothing to commit, working tree clean\ndvc status\nData and pipelines are up to date."
  },
  {
    "objectID": "git/wb_dvc_slides.html#modify-pipeline",
    "href": "git/wb_dvc_slides.html#modify-pipeline",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Modify pipeline",
    "text": "Modify pipeline\nFrom now on, if we edit one of the scripts, or one of the dependencies, dvc status will tell us what changed and dvc repro will only rerun the parts of the pipeline to update the result, pretty much as a Makefile would"
  },
  {
    "objectID": "git/wb_dvc_slides.html#going-further-next-time",
    "href": "git/wb_dvc_slides.html#going-further-next-time",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Going further … next time",
    "text": "Going further … next time\n DVC is a sophisticated tool with many additional features:\n\nCreation of data registries\nDVCLive\nA Python library to log experiment metrics\nVisualize the performance logs as plots\nContinuous integration\nWith the sister project CML (Continuous Machine Learning)"
  },
  {
    "objectID": "git/top_ws.html",
    "href": "git/top_ws.html",
    "title": "Git workshops",
    "section": "",
    "text": "Searching a Git project\n\n\n\n\nCollaborating through Git\n\n\n\n\nContributing to projects",
    "crumbs": [
      "Git",
      "<b><em>Workshops</em></b>"
    ]
  },
  {
    "objectID": "git/top_intro.html",
    "href": "git/top_intro.html",
    "title": "Getting started with Git",
    "section": "",
    "text": "This introductory course to Git will allow you to put your projects under version control, create commits with important versions of your files, revisit these versions, and add remotes on services such as GitHub or GitLab.\nYou will get a deep understanding of the functioning of Git and learn to use it from the command line.\n\n Start course ➤",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>"
    ]
  },
  {
    "objectID": "git/intro_undo.html",
    "href": "git/intro_undo.html",
    "title": "Undoing",
    "section": "",
    "text": "This section covers a few of the ways actions can be undone in Git.",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Undoing"
    ]
  },
  {
    "objectID": "git/intro_undo.html#amending-the-last-commit",
    "href": "git/intro_undo.html#amending-the-last-commit",
    "title": "Undoing",
    "section": "Amending the last commit",
    "text": "Amending the last commit\nHere is a common scenario: you make a commit, then realize that you forgot to include some changes in that commit; or you aren’t happy with the commit message; or both. You can edit your latest commit with the --amend flag:\ngit commit --amend\nThis will hide your last commit (as if it had never happened), add the changes in the staging area (if any) to the changes in that last commit, open a text editor showing the message of the last commit (you can keep or edit that message), and create a new commit which replaces your last commit.\nSo if you only want to change the commit message, run that command with an empty staging area. If you want to add changes to the last commit, stage them, then run the command.\nIn short, what this does is to replace your last commit with a new commit with the added changes and/or edited message. This prevents having a messy history with commits of the type “add missing file to last commit” or “better message for last commit: bla bla bla”. If you made a typo in your last commit message (and if you care about having a nice, clean history), you can fix it easily this way.\n\n\nYour turn:\n\n\nRun git log --oneline (notice the hash of the last commit)\nEdit your last commit message\nRun git log --oneline again to see that your last commit now has a new hash (so it is a different commit) and a new message\nNow, make some change in your project (add a file, or edit a file… any change you want)\nThen add that new change to your last commit without changing the message",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Undoing"
    ]
  },
  {
    "objectID": "git/intro_undo.html#unstaging",
    "href": "git/intro_undo.html#unstaging",
    "title": "Undoing",
    "section": "Unstaging",
    "text": "Unstaging\nYou know how to add changes to the staging area. But what if you want to unstage changes? You don’t want to loose those changes. But you staged them and then realized that you don’t want to include them in your next commit after all.\nHere is the command for this:\ngit restore --staged &lt;file&gt;\n\nNote that Git will remind you about the existence of this command when you run git status and have staged files ready to be committed.\n\n\n\nYour turn:\n\n\nMake changes to one of your existing files\nStage that file\nRun git status and notice Git’s reminder about this command\nUnstage the changes on that file",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Undoing"
    ]
  },
  {
    "objectID": "git/intro_undo.html#erasing-modifications",
    "href": "git/intro_undo.html#erasing-modifications",
    "title": "Undoing",
    "section": "Erasing modifications",
    "text": "Erasing modifications\nNow, what if you made changes to a file, then decide that they were no good? You can easily get rid of these edits and restore the file to its last committed version:\ngit restore &lt;file&gt;\n\nNote that Git will tell you about this command when you run git status and have unstaged changes in tracked files.\n\n\n\nYour turn:\n\n\nRun git status again and notice Git’s reminder about the existence of this command\nErase that last change of yours\nOpen your file and notice that your edits are gone\n\n\n\nAs you just experienced, this command leads to data loss.\nThose last edits are gone and unrecoverable. Be very careful when using this!",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Undoing"
    ]
  },
  {
    "objectID": "git/intro_undo.html#reverting",
    "href": "git/intro_undo.html#reverting",
    "title": "Undoing",
    "section": "Reverting",
    "text": "Reverting\n\nThe working directory must be clean before you can use git revert.\n\ngit revert creates a new commit which reverses the effect of past commit(s).\n\nTo revert the last commit (current location of HEAD):\n\ngit revert HEAD\n\nYou can use the hash of the last commit instead of HEAD.\n\n\nTo revert the last two commits:\n\ngit revert HEAD~\n\nHEAD~ is equivalent to HEAD~1 and means the commit before the one HEAD is on.\nHere too of course, you can use the hash of the commit before last instead of HEAD~.\n\n\nTo revert the last three commits:\n\ngit revert HEAD~2",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Undoing"
    ]
  },
  {
    "objectID": "git/intro_time_travel.html",
    "href": "git/intro_time_travel.html",
    "title": "Revisiting old commits",
    "section": "",
    "text": "It’s great to record history, but if we don’t know how to make use of it, it isn’t exactly useful.\nIn this workshop, you will travel through the history of a project.",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Revisiting old commits"
    ]
  },
  {
    "objectID": "git/intro_time_travel.html#looking-at-the-past-without-travelling",
    "href": "git/intro_time_travel.html#looking-at-the-past-without-travelling",
    "title": "Revisiting old commits",
    "section": "Looking at the past without travelling",
    "text": "Looking at the past without travelling\nHEAD is a little file in the .git directory which points to our current location in the Git history.\nYou already saw multiple ways to have a glimpse at your project history without moving HEAD:\n\ngit log and its many variations shows a list or a tree of your commits\ngit show displays information about a Git object such as a past commit\n\nThose are useful options, but Git allows you to really travel in your project history: HEAD can be moved around with the command git checkout to point to any branch, tag, or commit.",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Revisiting old commits"
    ]
  },
  {
    "objectID": "git/intro_time_travel.html#travelling-through-history",
    "href": "git/intro_time_travel.html#travelling-through-history",
    "title": "Revisiting old commits",
    "section": "Travelling through history",
    "text": "Travelling through history\nAs soon as you move HEAD to a new Git object with git checkout, the working directory and the index get updated to match the snapshot that Git object is pointing to. That means that your project will suddenly be back to the state in which it was when you committed that snapshot.\n\nMoving HEAD\nLet’s give this a try and move HEAD to a past commit.\n\nIdentifying the commit we want to move HEAD to\nYou can use the ~ notation:\n\nHEAD~ or HEAD~1 means the commit which precedes the one HEAD is pointing to.\nHEAD~2 means the commit before that.\nHEAD~3 refers to the 3rd commit before the current commit.\netc.\n\nYou can also run git log to find the hash of your commit of interest.\n\n\n\nDetached HEAD\nLet’s look at a hypothetical scenario to see what happens when you checkout a commit.\nThis is our starting point:\n\nNow, we move HEAD to the commit 31fukv1:\ngit checkout 31fukv1\n\nNotice that HEAD is not pointing at a branch anymore: it is pointing directly at a commit. This is called a detached HEAD state and Git will give you plenty of warnings about it.\nIf you look at your files, you will see that they match their state when you committed 31fukv1: your working directory got updated to match the current position of HEAD.\nYou can look at your project at that point in its history, then go back to your main branch (here main) with:\ngit checkout main\n\nAnd that’s that. You took a little trip into the past just to have a look, then came back to “the present” and all went well.\n\n\nCreating commits from a detached HEAD\nNow, when you are at commit 31fukv1, maybe you wanted to try something.\n\nYou can safely try anything you want: when you checkout main to come back to “the present”, those experimental changes will get lost.\nBut what if you want to keep those changes you made at 31fukv1?\nIn that case, as you always do, you create a commit to archive those changes into the project history:\n\nYou can make more commits:\n\nThe thing is that you are still in this detached HEAD state. HEAD is not pointing to a branch as it normally is. Is this a problem?\n\nBad workflow\nWell, it becomes a problem if you checkout main from there:\n\nIf you decide that you don’t care about those commits after all, then all is good. But if you care about them, this is a bad situation because those commits you created when you were in a detached HEAD state are now left behind: they are not in the history of any branch or tag.\nThis is bad for three reasons:\n\nThose commits will not show when you run git log, so it is easy to forget about them.\nIt is not easy to go back to them because there aren’t any tag or branch that you can checkout.\nThe garbage collection (which runs every 30 days by default) will delete those commits which are not on any branch or tag. So you will ultimately loose them.\n\n\n\nGood workflow\nA good workflow would have been to create a new branch on 31fukv1 (let’s call it alternative) and switch to it. That way, the commits created from 31fukv1 are on a branch and they will not be deleted by the next garbage collection:\n\nIn this good workflow, it is totally safe to switch back to main:\n\n\nIf you want to list the commits 23f481q and rthy7wg when you are back on main, you need to run git log with the --all flag.\n\n\n\nRecovering commits left behind\nWhat if you left commits behind (not on a branch)?\nYou can retrieve their hash by running:\ngit reflog\nThis tracks the position of HEAD over time.\nYou can then checkout the commit you care about (so you are going back to a detached HEAD state):\ngit checkout &lt;hash-abandonned-commit&gt;\nThis puts you back into a situation where you can rescue the commit(s) by creating a branch:\nDo this as soon as you can since those commits will be deleted at the next garbage collection (and finding their hash with git reflog will become increasingly complicated as you wait).",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Revisiting old commits"
    ]
  },
  {
    "objectID": "git/intro_tags.html",
    "href": "git/intro_tags.html",
    "title": "Tags",
    "section": "",
    "text": "When you reach an important point in the development of a project, it is convenient to be able to identify the next commit easily. Rather than having to look for it through date, commit message, or hash, you can create a tag: a pointer to that commit.",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Tags"
    ]
  },
  {
    "objectID": "git/intro_tags.html#leightweight-tag",
    "href": "git/intro_tags.html#leightweight-tag",
    "title": "Tags",
    "section": "Leightweight tag",
    "text": "Leightweight tag\nYou create a tag with:\ngit tag &lt;tag-name&gt;\n\nExample:\n\ngit tag J_Climate_2009\n\nAs you keep developing the project and create new commits, the branch and HEAD pointers will move along, but the tag remains on your important commit.\n\nAt any time, you can get info on the commit thus tagged with:\ngit show J_Climate_2009\nOr you can check it out with:\ngit checkout J_Climate_2009",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Tags"
    ]
  },
  {
    "objectID": "git/intro_tags.html#annotated-tag",
    "href": "git/intro_tags.html#annotated-tag",
    "title": "Tags",
    "section": "Annotated tag",
    "text": "Annotated tag\nA more sophisticated form of tag comes with a message:\ngit tag -a &lt;tag-name&gt; -m \"&lt;message&gt;\"\ngit tag -a J_Climate_2009 -m \"State of project at the publication of paper\"",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Tags"
    ]
  },
  {
    "objectID": "git/intro_tags.html#list-tags",
    "href": "git/intro_tags.html#list-tags",
    "title": "Tags",
    "section": "List tags",
    "text": "List tags\ngit tag",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Tags"
    ]
  },
  {
    "objectID": "git/intro_tags.html#deleting-tags",
    "href": "git/intro_tags.html#deleting-tags",
    "title": "Tags",
    "section": "Deleting tags",
    "text": "Deleting tags\ngit tag -d &lt;tag-name&gt;\ngit tag -d J_Climate_2009",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Tags"
    ]
  },
  {
    "objectID": "git/intro_resources.html",
    "href": "git/intro_resources.html",
    "title": "Resources",
    "section": "",
    "text": "This section contains useful resources on Git as well as some advice.\n\n\nOnline documentation\n\nOfficial Git manual\nOpen source Pro Git book\n\n\n\nCourses & workshops\n\nWestern Canada Research Computing Git workshops\nWestGrid Summer School 2020 Git course\nWestGrid Autumn School 2020 Git course\nSoftware Carpentry Git lesson\n\n\n\nQ & A\n\nStack Overflow [git] tag\n\n\n\nTroubleshooting\n\n“Listen” to Git!\nGit is extremely verbose: by default, it will return lots of information. Read it!\nThese messages may feel overwhelming at first, but:\n\nthey will make more and more sense as you gain expertise,\nthey often give you clues as to what the problem is,\neven if you don’t understand them, you can use them as Google search terms.\n\n\n\n(Re-read) the doc\nAs I have no memory, I need to check the man pages all the time. That’s ok! It is quick and easy.\nFor more detailed information and examples, I really like the Official Git manual.\n\n\nSearch online\n\nGoogle\nStack Overflow [git] tag\n\n\n\nDon’t panic\nBe analytical. It is easy to panic and feel lost if something doesn’t work as expected. Take a breath and start with the basis:\n\nmake sure you are in the repo (pwd) and the files are where you think they are (ls -a),\ninspect the repository (git status, git diff, git log). Make sure not to overlook what Git is “telling” you there.",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Resources"
    ]
  },
  {
    "objectID": "git/intro_logs.html",
    "href": "git/intro_logs.html",
    "title": "Getting information on commits",
    "section": "",
    "text": "Before we can make use of old commits, we need to be able to get information about them. This is critical to know how to navigate the history of a project.",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Getting information on commits"
    ]
  },
  {
    "objectID": "git/intro_logs.html#displaying-the-commit-history",
    "href": "git/intro_logs.html#displaying-the-commit-history",
    "title": "Getting information on commits",
    "section": "Displaying the commit history",
    "text": "Displaying the commit history\nDo you remember this diagram from the first section of this course?\n\nThis is very useful to get a map of the history of our project. But how can we get a visual for it? The command here is git log and its many options.\nIn its basic form, it outputs the logs of our past commits:\ngit log\ncommit ca3c0360bd8ab961117671dd1f8fb2d2c3d5d7a1 (HEAD -&gt; main)\nAuthor: Marie-Helene Burle &lt;xxx@xxx&gt;\nDate:   Tue Jul 11 23:08:59 2023 -0700\n\n    Add first draft of script\n\ncommit c4ab5e755179a7b28a09c0ca587551bdac504d35\nAuthor: Marie-Helene Burle &lt;xxx@xxx&gt;\nDate:   Tue Jul 11 23:06:40 2023 -0700\n\n    Add .gitignore file with data and results\n\ncommit 61abf96298b54baf6d48cdea2ab1477db1075b5e\nAuthor: Marie-Helene Burle &lt;xxx@xxx&gt;\nDate:   Mon Jul 10 23:23:25 2023 -0700\n\n    Initial commit\nAs you can see, commits are listed from the bottom up.\nCommits can be displayed as one-liners:\ngit log --oneline\nca3c036 (HEAD -&gt; main) Add first draft of script\nc4ab5e7 Add .gitignore file with data and results\n61abf96 Initial commit\nOr as a graph:\ngit log --graph\n* commit ca3c0360bd8ab961117671dd1f8fb2d2c3d5d7a1 (HEAD -&gt; main)\n| Author: Marie-Helene Burle &lt;xxx@xxx&gt;\n| Date:   Tue Jul 11 23:08:59 2023 -0700\n|\n|     Add first draft of script\n|\n* commit c4ab5e755179a7b28a09c0ca587551bdac504d35\n| Author: Marie-Helene Burle &lt;xxx@xxx&gt;\n| Date:   Tue Jul 11 23:06:40 2023 -0700\n|\n|     Add .gitignore file with data and results\n|\n* commit 61abf96298b54baf6d48cdea2ab1477db1075b5e\n  Author: Marie-Helene Burle &lt;xxx@xxx&gt;\n  Date:   Mon Jul 10 23:23:25 2023 -0700\n\n      Initial commit\nOr in any fancy way you like:\ngit log \\\n    --graph \\\n    --date=short \\\n    --pretty=format:'%C(cyan)%h %C(blue)%ar %C(auto)%d'`\n                   `'%C(yellow)%s%+b %C(magenta)%ae'\n* ca3c036 31 minutes ago  (HEAD -&gt; main)Add first draft of script xxx@xxx\n* c4ab5e7 34 minutes ago Add .gitignore file with data and results xxx@xxx\n* 61abf96 24 hours ago Initial commit xxx@xxx\nRun man git-log for a full list of options.",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Getting information on commits"
    ]
  },
  {
    "objectID": "git/intro_logs.html#information-about-a-commit",
    "href": "git/intro_logs.html#information-about-a-commit",
    "title": "Getting information on commits",
    "section": "Information about a commit",
    "text": "Information about a commit\ngit log is useful to get an overview of our project history, but the information we get about each commit is limited. To get additional information about a particular commit, you can use git show followed by the hash of the commit you are interested in.\nFor instance, let’s explore our last commit\ngit show\ncommit ca3c0360bd8ab961117671dd1f8fb2d2c3d5d7a1 (HEAD -&gt; main)\nAuthor: Marie-Helene Burle &lt;xxx@xxx&gt;\nDate:   Tue Jul 11 23:08:59 2023 -0700\n\n    Add first draft of script\n\ndiff --git a/src/script.py b/src/script.py\nnew file mode 100644\nindex 0000000..263ef67\n--- /dev/null\n+++ b/src/script.py\n@@ -0,0 +1,7 @@\n+import pandas as pd\n+from matplotlib import pyplot as plt\n+\n+df = pd.read_csv('../data/dataset.csv')\n+\n+df.plot()\n+plt.savefig('../results/plot.png', dpi=300)\nOr our second commit:\ngit show c4ab5e7  # Replace the hash by the hash of your second commit\ncommit c4ab5e755179a7b28a09c0ca587551bdac504d35\nAuthor: Marie-Helene Burle &lt;xxx@xxx&gt;\nDate:   Tue Jul 11 23:06:40 2023 -0700\n\n    Add .gitignore file with data and results\n\ndiff --git a/.gitignore b/.gitignore\nnew file mode 100644\nindex 0000000..e85f44a\n--- /dev/null\n+++ b/.gitignore\n@@ -0,0 +1,2 @@\n+/data/\n+/results/\nIn addition to displaying the commit metadata, git show also displays the diff of that commit with its parent commit.",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Getting information on commits"
    ]
  },
  {
    "objectID": "git/intro_intro.html",
    "href": "git/intro_intro.html",
    "title": "What is Git?",
    "section": "",
    "text": "Git is a free and open source version control system—a software that tracks changes to your files, allowing you to revisit or revert to older versions.\n\nSlides (Click and wait: the presentation might take a few instants to load)",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "What is Git?"
    ]
  },
  {
    "objectID": "git/intro_ignore.html",
    "href": "git/intro_ignore.html",
    "title": "Excluding from version control",
    "section": "",
    "text": "Not everything should be under version control, yet we don’t want a cluttered working directory. The solution: a list of files or patterns that Git disregards.",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Excluding from version control"
    ]
  },
  {
    "objectID": "git/intro_ignore.html#what-to-exclude",
    "href": "git/intro_ignore.html#what-to-exclude",
    "title": "Excluding from version control",
    "section": "What to exclude",
    "text": "What to exclude\nThere are files you really want to put under version control, but there are files you shouldn’t.\nPut under version control:\n\nScripts\nManuscripts and notes\nMakefile & similar\n\nDo NOT put under version control:\n\nNon-text files (e.g. images, office documents)\nYour initial data\nOutputs that can be recreated by running code (e.g. graphs, results)\n\nHowever, you don’t want to have such documents constantly showing up when you run git status. In order to have a clean working directory while keeping them out of version control, you can create a file called .gitignore and add to it a list of files or patterns that you want Git to disregard.\n\n\nYour turn:\n\nIn the case of our mock project,\n\nwhat should we put under version control?\nwhat should we ignore?",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Excluding from version control"
    ]
  },
  {
    "objectID": "git/intro_ignore.html#how-to-exclude",
    "href": "git/intro_ignore.html#how-to-exclude",
    "title": "Excluding from version control",
    "section": "How to exclude",
    "text": "How to exclude\n\nThe .gitignore file\nTo exclude files from version control, create a file called .gitignore in the root of your project and add those files to it, one per line.\n\nExample:\n\n# Create .gitignore and add 'graph.png' to it\necho graph.png &gt; .gitignore\n\n# `&gt;` would overwrite the content. `&gt;&gt;` appends\necho output.txt &gt;&gt; .gitignore\nYou can also ignore entire directories.\n\nExample:\n\necho /results/ &gt;&gt; .gitignore\nFinally, you can use globbing patterns to ignore all files matching a certain pattern.\n\nExample:\n\n# Exclude all .png files\necho *.png &gt;&gt; .gitignore\n\n.gitignore syntax\nEach line in a .gitignore file specifies a pattern.\nBlank lines are ignored and can serve as separators for readability.\nLines starting with # are comments.\nTo add patterns starting with a special character (e.g. #, !), that character needs to be escaped with \\.\nTrailing spaces are ignored unless they are escaped with \\.\n! negates patterns.\nPatterns ending with / match directories. Otherwise patterns match both files and directories.\n/ at the beginning or within a search pattern indicates that the pattern is relative to the directory level of the .gitignore file (usually the root of the project). Otherwise the pattern matches anywhere below the .gitignore level.\n\nExamples:\n/foo/bar/ matches the directory foo/bar, but not the directory a/foo/bar\nfoo/bar/ matches both the directories foo/bar and a/foo/bar\n\n* matches anything except /.\n? matches any one character except /.\nThe range notation (e.g. [a-zA-Z]) can be used to match one of the characters in a range.\nA leading **/ matches all directories.\n\nExample:\n**/foo matches file or directory foo anywhere. This is the same as foo.\n\nA trailing /** matches everything inside what it precedes.\n\nExample:\nabc/** matches all files (recursively) inside directory abc\n\n/**/ matches zero or more directories.\n\nExample:\na/**/b matches a/b, a/x/b, and a/x/y/b\n\n\n\n\nYour turn:\n\nCreate a .gitignore file suitable for our mock project.\n\nThe .gitignore is a file like any other file, so you’ll want to stage and commit it:\ngit status\nOn branch main\nUntracked files:\n  (use \"git add &lt;file&gt;...\" to include in what will be committed)\n    .gitignore\n    src/\n\nnothing added to commit but untracked files present (use \"git add\" to track)\n\nNotice how data/ is not listed in the untracked files anymore.\n\nWe stage our .gitignore file:\ngit add .gitignore\ngit status\nOn branch main\nChanges to be committed:\n  (use \"git restore --staged &lt;file&gt;...\" to unstage)\n    new file:   .gitignore\n\nUntracked files:\n  (use \"git add &lt;file&gt;...\" to include in what will be committed)\n    src/\nAnd we create a new commit:\ngit commit -m \"Add .gitignore file with data and results\"\ngit status\n[main a1df8e5] Add .gitignore file with data and results\n 1 file changed, 2 insertions(+)\n create mode 100644 .gitignore\ngit status\nOn branch main\nUntracked files:\n  (use \"git add &lt;file&gt;...\" to include in what will be committed)\n    src/\n\nnothing added to commit but untracked files present (use \"git add\" to track)\nLet’s create a third commit with the Python script:\ngit add src/script.py\ngit commit -m \"Add first draft of script\"\n[main ca3c036] Add first draft of script\n 1 file changed, 7 insertions(+)\n create mode 100644 src/script.py\ngit status\nOn branch main\nnothing to commit, working tree clean\nWhat does “working tree clean” mean? In the next section, we will talk about the three file trees of Git.",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Excluding from version control"
    ]
  },
  {
    "objectID": "git/intro_documentation.html",
    "href": "git/intro_documentation.html",
    "title": "Documentation",
    "section": "",
    "text": "Git comes with internal documentation. This section covers how to access it.",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Documentation"
    ]
  },
  {
    "objectID": "git/intro_documentation.html#manual-pages",
    "href": "git/intro_documentation.html#manual-pages",
    "title": "Documentation",
    "section": "Manual pages",
    "text": "Manual pages\nThe manual page for any Git command can be open with:\ngit help &lt;command&gt;\n\nExample:\n\ngit help commit\nOn Unix systems (Linux and macOS), you can alternatively use the man command this way:\nman git-&lt;command&gt;\n\nExample:\n\nman git-commit\nFinally, many commands come with an help flag:\ngit &lt;command&gt; --help\n\nExample:\n\ngit commit --help\nAll these methods lead to the same thing: the manual page corresponding to the command will open in a pager (usually less). A pager is a program which makes it easier to read documents in the command line.\n\nUseful keybindings when you are in the pager:\nSPACE      scroll one screen down\nb          backa one screen\nq          quit the pager\ng          go to the top of the document\n7g         go to line 7 from the top\nG          go to the bottom of the document\n/          search for a term\n           n will take you to the next result\n           N to the previous result",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Documentation"
    ]
  },
  {
    "objectID": "git/intro_documentation.html#command-options",
    "href": "git/intro_documentation.html#command-options",
    "title": "Documentation",
    "section": "Command options",
    "text": "Command options\nInstead of opening the full manual in the pager, if you want to simply output the various flags (options) for a command directly in the terminal, you can use:\ngit &lt;command&gt; -h\n\nExample:\n\ngit commit -h",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Documentation"
    ]
  },
  {
    "objectID": "git/intro_branches.html",
    "href": "git/intro_branches.html",
    "title": "Branches",
    "section": "",
    "text": "One of the reasons Git has become so popular is its branching system: unlike in other version control tools in which creating branches is a lengthy and expensive process involving heavy copies, a branch in Git is just a lightweight pointer to a commit. This makes creating branches extremely quick and cheap.",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Branches"
    ]
  },
  {
    "objectID": "git/intro_branches.html#what-is-a-branch",
    "href": "git/intro_branches.html#what-is-a-branch",
    "title": "Branches",
    "section": "What is a branch?",
    "text": "What is a branch?\nA branch is a pointer to a commit (under the hood, it is a small file containing the 40 character hash checksum of the commit it points to).\nRemember that little pointer called main? That’s our main branch: the one Git creates automatically when we create our first commit.\nWhen you run git status and get “On branch main” in the output, or when you run git log and see “(HEAD -&gt; main)” in the log, it means that the HEAD pointer (your position in the Git history) points to the branch main (which itself points to a commit).\n\nI know that is a lot of pointers … but this is really what makes Git so nimble, powerful, and fantastic. Because these pointers are very cheap (tiny files) and so useful.",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Branches"
    ]
  },
  {
    "objectID": "git/intro_branches.html#why-use-multiple-branches",
    "href": "git/intro_branches.html#why-use-multiple-branches",
    "title": "Branches",
    "section": "Why use multiple branches?",
    "text": "Why use multiple branches?\nBranches are useful in so many situations:\n\nIf your changes break code, you still have a fully functional branch to go back to if needed.\nIf you develop a tool being used, this allows you to experiment with new features until they are ready without messing up with the working project.\nYou can create a branch for each alternative approach. This allows you to jump back and forth between various alternatives.\nYou can work on different aspects of the project on different branches. This prevents having messy incomplete work all over the place on the same branch.\nIf you want to revisit an old commit, you can create a branch there and switch to it instead of moving HEAD (creating a detached HEAD situation). This way, if you decide to create new commits from that old one, you don’t risk loosing them.\nBranches are great for collaboration: each person can work on their own branch and merge it back to the main branch when they are done with one section of a project.\n\nAnd since branches are so cheap to create, there is no downside to their creation.",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Branches"
    ]
  },
  {
    "objectID": "git/intro_branches.html#creating-branches-and-switching-between-branches",
    "href": "git/intro_branches.html#creating-branches-and-switching-between-branches",
    "title": "Branches",
    "section": "Creating branches and switching between branches",
    "text": "Creating branches and switching between branches\nYou can create a new branch with:\ngit branch &lt;new-branch-name&gt;\n\nExample:\n\ngit branch test\n\nand you can then switch to it with:\ngit switch &lt;new-branch-name&gt;\n\nExample:\n\ngit switch test\n\nAlternatively, you can do both at once with the convenient:\ngit switch -c &lt;new-branch-name&gt;\n\n-c flag for “create”. So you create a branch and switch to it directly.\n\nI find this last command most useful as it is all too easy otherwise to create a new branch, forget to switch to it, and create commits on the wrong branch …",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Branches"
    ]
  },
  {
    "objectID": "git/intro_branches.html#listing-branches",
    "href": "git/intro_branches.html#listing-branches",
    "title": "Branches",
    "section": "Listing branches",
    "text": "Listing branches\ngit branch\n  main\n* test\nThe * shows the branch you are currently on (i.e. the branch to which HEAD points to). In our example, the project has two branches and we are on the branch test.",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Branches"
    ]
  },
  {
    "objectID": "git/intro_branches.html#comparing-branches",
    "href": "git/intro_branches.html#comparing-branches",
    "title": "Branches",
    "section": "Comparing branches",
    "text": "Comparing branches\nYou can use git diff to compare branches:\ngit diff main test\nThis shows all the lines that have been modified (added or deleted) between the commits both branches point to.",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Branches"
    ]
  },
  {
    "objectID": "git/intro_branches.html#merging-branches",
    "href": "git/intro_branches.html#merging-branches",
    "title": "Branches",
    "section": "Merging branches",
    "text": "Merging branches\nWhen you are happy with the changes you made on your test branch, you can merge it into main.\n\nFast-forward merge\nIf you have only created new commits on the branch test, the merge is called a “fast-forward merge” because main and test have not diverged: it is simply a question of having main catch up to test.\n\nFirst, you switch to main:\ngit switch main\n\nThen you do the fast-forward merge:\ngit merge test\n\nThen, usually, you delete the branch test as it has served its purpose:\ngit branch -d test\n\nAlternatively, you can switch back to test and do the next bit of experimental work on it. This allows to keep main free of mishaps and bad developments.\n\n\nThree-way merge\nIf the branches have diverged (you created commits on both branches), the merge requires the creation of an additional commit called a “merge commit”.\nLet’s go back to our situation before we created the branch test:\n\nThis time, you create a branch called test2:\n\nand you switch to it:\n\nThen you create some commits:\n\n\nNow you switch back to main:\n\nAnd you create commits from main too:\n\n\nTo merge your branch test2 into main, a new commit is now required. Git will create this new commit automatically. As long as there is no conflict, it is just as easy as a fast-forward merge:\ngit merge test2\n\nAfter which, you can delete the (now useless) test branch (with git branch -d test2):",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Branches"
    ]
  },
  {
    "objectID": "git/intro_branches.html#resolving-conflicts",
    "href": "git/intro_branches.html#resolving-conflicts",
    "title": "Branches",
    "section": "Resolving conflicts",
    "text": "Resolving conflicts\nGit works line by line. As long as you aren’t working on the same line(s) of the same file(s) on different branches, there will not be any merging difficulty. If however you modified one or more of the same line(s) of the same file(s) on different branches, Git has no way to decide which version should be kept and will thus not be able to complete the merge. It will then ask you to resolve the conflict(s). Conveniently, it will list the file(s) containing the conflict(s).\nThere are fancy tools to resolve conflicts, but you can do it in any text editor: simply open the file(s) listed by Git as having conflicts and look for the following markers:\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\nThis is your version.\n=======\nThis is the alternative version of the same section of the file.\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; alternative version\nIn our case, it could look something like:\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\nGreat sentence.\n=======\nGreat sentence with some variations.\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; test2\nThese markers are added by Git to signal the areas of conflict. It is up to you to choose between the two versions (or create a third one) and remove the conflict markers. After that, you can stage the file(s) which contained the conflicts to finish the merge (and then you can commit).",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Branches"
    ]
  },
  {
    "objectID": "git/index.html",
    "href": "git/index.html",
    "title": "Git",
    "section": "",
    "text": "Getting started with  \nA course in version control\n\n\n\n\nWorkshops\nVarious Git topics\n\n\n\n\n\n\n60 min webinars\nVarious Git topics",
    "crumbs": [
      "Git",
      "<br>&nbsp;<img src=\"img/logo_git.png\" class=\"img-fluid\" style=\"width:1.5em\" alt=\"noshadow\"><br><br>"
    ]
  },
  {
    "objectID": "emacs/wb_python.html",
    "href": "emacs/wb_python.html",
    "title": "Full Python IDE in Emacs",
    "section": "",
    "text": "There are quite a few packages that can turn Emacs into a Python IDE (a classic example is elpy and since version 29, eglot—an LSP client—comes shipped with Emacs).\nAfter playing with many of the options, I settled on a selection of packages that turn Emacs into a truly impressive Python IDE:\n\nlsp-mode, lsp-ui, and lsp-pyright provide astounding code completion, debugging, code navigation, and many helpers,\npy-vterm-interaction.el runs your favourite Python shell (Python REPL, ipython, ptpython, or ptipython) in a much improved Emacs terminal emulator,\nemacs-reformatter reformats your code with the linter of your choice (e.g. black or the much faster ruff).\n\n\nComing up in fall 2025.",
    "crumbs": [
      "Emacs",
      "<b><em>Webinars</em></b>",
      "Full Python IDE in Emacs"
    ]
  },
  {
    "objectID": "emacs/wb_new_tools.html",
    "href": "emacs/wb_new_tools.html",
    "title": "Modern Emacs",
    "section": "",
    "text": "Emacs might have been created in the 70s, but development is alive and well:\n\n10 years ago version 24 brought huge speedups with lexical binding.\nIn 2022, version 28 added—among other things—just-in-time native compilation for elisp code for improved performance.\nVersion 29 last year brought countless exciting new additions such as official tree-sitter support and built-in Eglot and use-package.\nIn addition to Emacs itself, a profusion of modern packages have emerged over the past few years (e.g. the vertico/consult/orderless/marginalia/embark completion system; corfu and cape for at point completion) bringing great speed and sleekness to the user experience.\n\nAll these features are optional however and you need to learn about them to take advantage of their huge benefits. This webinar intends to get you started making Emacs really fast.\n\nSlides (Click and wait: this reveal.js presentation may take a little time to load.)",
    "crumbs": [
      "Emacs",
      "<b><em>Webinars</em></b>",
      "Modern, faster & better Emacs"
    ]
  },
  {
    "objectID": "emacs/wb_modes.html",
    "href": "emacs/wb_modes.html",
    "title": "Understanding Emacs modes",
    "section": "",
    "text": "At the core of Emacs functioning are modes: major modes set the appearance and behaviour of Emacs for various types of texts (e.g. a Python script and a Markdown document will display different syntax highlighting and have different functions activated), while minor modes provide additional features that can be turned on or off (e.g. spell checking). Understanding how modes work is key to customizing Emacs and exploiting its strengths.\nIn this webinar, I will explain the functioning of Emacs modes and show how to manage and even customize them. Finally, I will demo how the package Polymode allows to embed sections of a type of text in another type (e.g. snippets of code in a Markdown document).\n\nSlides (Click and wait: this reveal.js presentation may take a little time to load.)",
    "crumbs": [
      "Emacs",
      "<b><em>Webinars</em></b>",
      "Understanding Emacs modes"
    ]
  },
  {
    "objectID": "emacs/wb_llms.html",
    "href": "emacs/wb_llms.html",
    "title": "Using LLMs in Emacs",
    "section": "",
    "text": "Large language models (LLMs) have become powerful tools for coding,writing, and research. Not surprisingly, they are increasingly becoming integrated into many software.\nFor the Emacs users out there, I will show you how to use the latest OpenAI model via GitHub Copilot from within Emacs for code completion (thanks to copilot.el) and for chatting, code review, code correction, code optimization, code transformation, and code explanation (thanks to copilot-chat.el).\nThen I will show how to use any number of models in Emacs with gptel.\n\nComing up in fall 2025.",
    "crumbs": [
      "Emacs",
      "<b><em>Webinars</em></b>",
      "Using LLMs in Emacs"
    ]
  },
  {
    "objectID": "emacs/wb_ide.html",
    "href": "emacs/wb_ide.html",
    "title": "Emacs as a programming IDE",
    "section": "",
    "text": "Once upon a time (not that long ago), powerful text editors such as Vim and Emacs were the only nice interfaces to work with code. Nowadays, there are countless sleek and more GUI-oriented tools such as VS Code, RStudio, or JupyterLab that provide amazing IDEs, without the learning curve.\nSo why would one still use Emacs as a programming IDE?\nWhat does that even look like?\nIn this webinar, I will show some of the many reasons why I can’t let go of Emacs, then show how it can be used as a programming IDE for Python, R, and Julia.\n\nSlides (Click and wait: this reveal.js presentation may take a little time to load.)",
    "crumbs": [
      "Emacs",
      "<b><em>Webinars</em></b>",
      "Emacs as a programming IDE"
    ]
  },
  {
    "objectID": "emacs/top_intro.html",
    "href": "emacs/top_intro.html",
    "title": "Getting started with Emacs",
    "section": "",
    "text": "Emacs is more than ever a very powerful text editor with many exciting new developments.\nThis course will show you what makes Emacs such a fantastic tool and get you started in a smooth and gentle way. You will learn the basic concepts of Emacs, how to customize it, how to manage packages efficiently, and how to use it remotely.\n\n Start course ➤",
    "crumbs": [
      "Emacs",
      "<em><b>Getting started</b></em>"
    ]
  },
  {
    "objectID": "emacs/intro_undo.html",
    "href": "emacs/intro_undo.html",
    "title": "Undoing and redoing",
    "section": "",
    "text": "Undoing and redoing are operations so common while editing files that we don’t think about them much. Most software however have a poor undo/redo system in which edits get lost all the time.\nEmacs’ undos never loses edits and undo-tree brings a wonderful undo/redo system to it.",
    "crumbs": [
      "Emacs",
      "<em><b>Getting started</b></em>",
      "Undoing and redoing"
    ]
  },
  {
    "objectID": "emacs/intro_undo.html#undo-systems",
    "href": "emacs/intro_undo.html#undo-systems",
    "title": "Undoing and redoing",
    "section": "Undo systems",
    "text": "Undo systems\n\nLinear systems: classic undo/redo\n\nYou have some file:\n\n\n\n\n\n\n%%{init: { 'flowchart': {'rankSpacing':20} } }%%\n\nflowchart TD\n   1((\" \")):::current\n   classDef current stroke: #f96, stroke-width: 2px\n\n\n\n\n\n\n\nYou make some edit:\n\n\n\n\n\n\n%%{init: { 'flowchart': {'rankSpacing':20} } }%%\n\nflowchart TD\n   1((\" \"))---2((\" \")):::current\n   classDef current stroke: #f96, stroke-width: 2px\n\n\n\n\n\n\n\nYou make another edit:\n\n\n\n\n\n\n%%{init: { 'flowchart': {'rankSpacing':20} } }%%\n\nflowchart TD\n   1((\" \"))---2((\" \"))---3((\" \")):::current\n   classDef current stroke: #f96, stroke-width: 2px\n\n\n\n\n\n\n\nAnd another one:\n\n\n\n\n\n\n%%{init: { 'flowchart': {'rankSpacing':20} } }%%\n\nflowchart TD\n   1((\" \"))---2((\" \"))---3((\" \"))---4((\" \")):::current\n   classDef current stroke: #f96, stroke-width: 2px\n\n\n\n\n\n\n\nYou can undo:\n\n\n\n\n\n\n%%{init: { 'flowchart': {'rankSpacing':20} } }%%\n\nflowchart TD\n   1((\" \"))---2((\" \"))---3((\" \")):::current---4((\" \"))\n   classDef current stroke: #f96, stroke-width: 2px\n\n\n\n\n\n\n\nYou can undo some more:\n\n\n\n\n\n\n%%{init: { 'flowchart': {'rankSpacing':20} } }%%\n\nflowchart TD\n   1((\" \"))---2((\" \")):::current---3((\" \"))---4((\" \"))\n   classDef current stroke: #f96, stroke-width: 2px\n\n\n\n\n\n\n\nYou can also redo:\n\n\n\n\n\n\n%%{init: { 'flowchart': {'rankSpacing':20} } }%%\n\nflowchart TD\n   1((\" \"))---2((\" \"))---3((\" \")):::current---4((\" \"))\n   classDef current stroke: #f96, stroke-width: 2px\n\n\n\n\n\n\n\nNow, you make some new edit. From this point on, some edits are lost:\n\n\n\n\n\n\n%%{init: { 'flowchart': {'rankSpacing':20} } }%%\n\nflowchart TD\n   1((\" \"))---2((\" \"))---3((\" \"))-.-4((\" \")):::lost\n   3((\" \"))---5((\" \")):::current\n   classDef current stroke: #f96, stroke-width: 2px\n   classDef lost stroke-dasharray: 3 4\n\n\n\n\n\n\n\nYou can still undo:\n\n\n\n\n\n\n%%{init: { 'flowchart': {'rankSpacing':20} } }%%\n\nflowchart TD\n   1((\" \"))---2((\" \"))---3((\" \")):::current-.-4((\" \")):::lost\n   3((\" \"))---5((\" \"))\n   classDef current stroke: #f96, stroke-width: 2px\n   classDef lost stroke-dasharray: 3 4\n\n\n\n\n\n\n\nAnd you can redo your last undo, but you can’t access all previous states of the file:\n\n\n\n\n\n\n%%{init: { 'flowchart': {'rankSpacing':20} } }%%\n\nflowchart TD\n   1((\" \"))---2((\" \"))---3((\" \"))-.-4((\" \")):::lost\n   3((\" \"))---5((\" \")):::current\n   classDef current stroke: #f96, stroke-width: 2px\n   classDef lost stroke-dasharray: 3 4\n\n\n\n\n\n\n\n\nLinear systems: Emacs\n\nYou have some file:\n\n\n\n\n\n\n%%{init: { 'flowchart': {'rankSpacing':20} } }%%\n\nflowchart TD\n   1((1)):::current\n   classDef current stroke: #f96, stroke-width: 2px\n\n\n\n\n\n\n\nYou make some edit:\n\n\n\n\n\n\n%%{init: { 'flowchart': {'rankSpacing':20} } }%%\n\nflowchart TD\n   1((1))---2((2)):::current\n   classDef current stroke: #f96, stroke-width: 2px\n\n\n\n\n\n\n\nYou make another edit:\n\n\n\n\n\n\n%%{init: { 'flowchart': {'rankSpacing':20} } }%%\n\nflowchart TD\n   1((1))---2((2))---3((3)):::current\n   classDef current stroke: #f96, stroke-width: 2px\n\n\n\n\n\n\n\nAnd another one:\n\n\n\n\n\n\n%%{init: { 'flowchart': {'rankSpacing':20} } }%%\n\nflowchart TD\n   1((1))---2((2))---3((3))---4((4)):::current\n   classDef current stroke: #f96, stroke-width: 2px\n\n\n\n\n\n\n\nThe first undo adds a new point to the chain of edits, reversing the effects of the last edit:\n\n\n\n\n\n\n%%{init: { 'flowchart': {'rankSpacing':20} } }%%\n\nflowchart TD\n   1((1))---2((2))---3((3))---4((4))---5((3)):::current\n   classDef current stroke: #f96, stroke-width: 2px\n\n\n\n\n\n\n\nMore undoing keeps adding points to the chain:\n\n\n\n\n\n\n%%{init: { 'flowchart': {'rankSpacing':20} } }%%\n\nflowchart TD\n   1((1))---2((2))---3((3))---4((4))---5((3))---6((2)):::current\n   classDef current stroke: #f96, stroke-width: 2px\n\n\n\n\n\n\n\nThere is no proper redo. Instead, you stop undoing, then start again to undo the undo:\n\n\n\n\n\n\n%%{init: { 'flowchart': {'rankSpacing':20} } }%%\n\nflowchart TD\n   1((1))---2((2))---3((3))---4((4))---5((3))---6((2))---7((3)):::current\n   classDef current stroke: #f96, stroke-width: 2px\n\n\n\n\n\n\n\nYou can make new edits\n\n\n\n\n\n\n%%{init: { 'flowchart': {'rankSpacing':20} } }%%\n\nflowchart TD\n   1((1))---2((2))---3((3))---4((4))---5((3))---6((2))---7((3))---8((5)):::current\n   classDef current stroke: #f96, stroke-width: 2px\n\n\n\n\n\n\n\nNothing ever gets lost, but you might get headaches. For instance, to go back to the beginning, you have to do:\n\n\n\n\n\n\n%%{init: { 'flowchart': {'rankSpacing':20} } }%%\n\nflowchart TD\n   1((1))---2((2))---3((3))---4((4))---5((3))---6((2))---7((3))---8((5))---9((3))---10((2))---11((3))---12((4))---13((3))---14((2))---15((1)):::current\n   classDef current stroke: #f96, stroke-width: 2px\n\n\n\n\n\n\n\n\nNon linear system: undo-tree\n\nYou have some file:\n\n\n\n\n\n\n%%{init: { 'flowchart': {'rankSpacing':20} } }%%\n\nflowchart TD\n   1((\" \")):::current\n   classDef current stroke: #f96, stroke-width: 2px\n\n\n\n\n\n\n\nYou make some edit:\n\n\n\n\n\n\n%%{init: { 'flowchart': {'rankSpacing':20} } }%%\n\nflowchart TD\n   1((\" \"))---2((\" \")):::current\n   classDef current stroke: #f96, stroke-width: 2px\n\n\n\n\n\n\n\nYou make another edit:\n\n\n\n\n\n\n%%{init: { 'flowchart': {'rankSpacing':20} } }%%\n\nflowchart TD\n   1((\" \"))---2((\" \"))---3((\" \")):::current\n   classDef current stroke: #f96, stroke-width: 2px\n\n\n\n\n\n\n\nAnd another one:\n\n\n\n\n\n\n%%{init: { 'flowchart': {'rankSpacing':20} } }%%\n\nflowchart TD\n   1((\" \"))---2((\" \"))---3((\" \"))---4((\" \")):::current\n   classDef current stroke: #f96, stroke-width: 2px\n\n\n\n\n\n\n\nYou can undo:\n\n\n\n\n\n\n%%{init: { 'flowchart': {'rankSpacing':20} } }%%\n\nflowchart TD\n   1((\" \"))---2((\" \"))---3((\" \")):::current---4((\" \"))\n   classDef current stroke: #f96, stroke-width: 2px\n\n\n\n\n\n\n\nYou can undo some more:\n\n\n\n\n\n\n%%{init: { 'flowchart': {'rankSpacing':20} } }%%\n\nflowchart TD\n   1((\" \"))---2((\" \")):::current---3((\" \"))---4((\" \"))\n   classDef current stroke: #f96, stroke-width: 2px\n\n\n\n\n\n\n\nYou can also redo:\n\n\n\n\n\n\n%%{init: { 'flowchart': {'rankSpacing':20} } }%%\n\nflowchart TD\n   1((\" \"))---2((\" \"))---3((\" \")):::current---4((\" \"))\n   classDef current stroke: #f96, stroke-width: 2px\n\n\n\n\n\n\n\nNow, you make some new edit:\n\n\n\n\n\n\n%%{init: { 'flowchart': {'rankSpacing':20} } }%%\n\nflowchart TD\n   1((\" \"))---2((\" \"))---3((\" \"))---4((\" \"))\n   3((\" \"))---5((\" \")):::current\n   classDef current stroke: #f96, stroke-width: 2px\n\n\n\n\n\n\n\nYou can still undo:\n\n\n\n\n\n\n%%{init: { 'flowchart': {'rankSpacing':20} } }%%\n\nflowchart TD\n   1((\" \"))---2((\" \"))---3((\" \"))---4((\" \"))\n   3((\" \")):::current---5((\" \"))\n   classDef current stroke: #f96, stroke-width: 2px\n\n\n\n\n\n\n\nYou can switch branch and redo the old version:\n\n\n\n\n\n\n%%{init: { 'flowchart': {'rankSpacing':20} } }%%\n\nflowchart TD\n   1((\" \"))---2((\" \"))---3((\" \"))---4((\" \")):::current\n   3((\" \"))---5((\" \"))\n   classDef current stroke: #f96, stroke-width: 2px\n\n\n\n\n\n\n\nNothing ever gets lost and it is a lot more sane to navigate the history.\n\n\nTo to back to the beginning, you only have to do:\n\n\n\n\n\n\n%%{init: { 'flowchart': {'rankSpacing':20} } }%%\n\nflowchart TD\n   1((\" \")):::current---2((\" \"))---3((\" \"))---4((\" \"))\n   3((\" \"))---5((\" \"))\n   classDef current stroke: #f96, stroke-width: 2px\n\n\n\n\n\n\n\nCompare this with the insane Emacs default system:\n\n\n\n\n\n\n%%{init: { 'flowchart': {'rankSpacing':20} } }%%\n\nflowchart TD\n   1((1))---2((2))---3((3))---4((4))---5((3))---6((2))---7((3))---8((5))---9((3))---10((2))---11((3))---12((4))---13((3))---14((2))---15((1)):::current\n   classDef current stroke: #f96, stroke-width: 2px\n\n\n\n\n\n\nAnd this is an exceedingly simple example only involving 5 different file states. I let you imagine how it quickly explodes in complexity in real life situations 🙂\nNow, the default Emacs system has the huge benefit to never lose any edit. It is already a huge improvement over the default system on most software! The thing is that when we undo and redo changes, linear systems are not ideal. A tree structure that can be fully navigated is just a more sensible solution.\n\nUndo-tree was initially developed for Vim, so Vim can also use an ideal undo/redo system.",
    "crumbs": [
      "Emacs",
      "<em><b>Getting started</b></em>",
      "Undoing and redoing"
    ]
  },
  {
    "objectID": "emacs/intro_undo.html#installing-and-customizing-undo-tree",
    "href": "emacs/intro_undo.html#installing-and-customizing-undo-tree",
    "title": "Undoing and redoing",
    "section": "Installing and customizing undo-tree",
    "text": "Installing and customizing undo-tree\nThis is a personal affair.\nThe minimal configuration when using straight (to download the package) and use-package to load it and customize it, looks like this:\n(use-package undo-tree\n  :straight t)\nMy personal configuration looks like this:\n(use-package undo-tree\n    :straight t\n    :init\n    (global-undo-tree-mode 1)\n    :bind ((\"C-l\" . undo-tree-undo)\n           (\"C-r\" . undo-tree-redo)\n           (\"s-t\" . undo-tree-visualize)\n           :map undo-tree-visualizer-mode-map\n           ;; go to selected undo state\n           (\"&lt;return&gt;\" . undo-tree-visualizer-quit)\n           ;; cancel (return to state before calling undo-tree-visualize)\n           (\"q\" . undo-tree-visualizer-abort)))",
    "crumbs": [
      "Emacs",
      "<em><b>Getting started</b></em>",
      "Undoing and redoing"
    ]
  },
  {
    "objectID": "emacs/intro_shells.html",
    "href": "emacs/intro_shells.html",
    "title": "Other functionalities",
    "section": "",
    "text": "In the previous section, we saw that, besides text editing, Emacs can be used as a file manager. In this section, we will see that Emacs can be used for many other tasks.",
    "crumbs": [
      "Emacs",
      "<em><b>Getting started</b></em>",
      "Other functionalities"
    ]
  },
  {
    "objectID": "emacs/intro_shells.html#calendar",
    "href": "emacs/intro_shells.html#calendar",
    "title": "Other functionalities",
    "section": "Calendar",
    "text": "Calendar\nM-x calendar opens a little calendar that can be navigated and to which events can be added.",
    "crumbs": [
      "Emacs",
      "<em><b>Getting started</b></em>",
      "Other functionalities"
    ]
  },
  {
    "objectID": "emacs/intro_shells.html#calculator",
    "href": "emacs/intro_shells.html#calculator",
    "title": "Other functionalities",
    "section": "Calculator",
    "text": "Calculator\nM-x calculator launches well … a calculator.",
    "crumbs": [
      "Emacs",
      "<em><b>Getting started</b></em>",
      "Other functionalities"
    ]
  },
  {
    "objectID": "emacs/intro_shells.html#man-pages",
    "href": "emacs/intro_shells.html#man-pages",
    "title": "Other functionalities",
    "section": "Man pages",
    "text": "Man pages\nMan pages can be open in Emacs with M-x man.",
    "crumbs": [
      "Emacs",
      "<em><b>Getting started</b></em>",
      "Other functionalities"
    ]
  },
  {
    "objectID": "emacs/intro_shells.html#running-processes",
    "href": "emacs/intro_shells.html#running-processes",
    "title": "Other functionalities",
    "section": "Running processes",
    "text": "Running processes\nEmacs can run processes in what are called inferior modes.\n\nScripting shells and terminals\nEmacs can talk to external shells installed on your system (e.g. Bash, Zsh) or play the role of a terminal emulator.\n\nTerminal emulators\nIf you want to run a terminal emulator directly in Emacs, you can use the pre-installed term (M-x term) or the similar ansi-term (M-x ansi-term).\nThe package multi-term allows to run multiple terminal buffers at the same time (similar to terminal multiplexers such as tmux).\nFor a better terminal emulator, you can install vterm.\n\n\nShells\nAny scripting shell installed on your system can be run directly in Emacs with M-x shell (it’ll use your default shell if you don’t customize it). The documentation will give you the list of kbds.\nOr you can run Emacs’ own shell Eshell with M-x eshell.\nEshell doesn’t talk to Bash or Zsh. It is its own shell, written entirely in Emacs Lisp. Consequently, it’ll provide you a Bash-like scripting shell on Windows.\nIt can also accept commands in Elisp (even mixed in with classic shell commands).\nFor more information on Eshell, you can read this excellent demo (also available as a lightning talk) or this article of Mastering Emacs.\nIf you want to run a single shell command, you can use the minibuffer with M-x shell-command or M-!.\n\n\n\nREPL and interpreter shells\nProgramming shells such as Python, R, Julia… can also be run in Emacs. Some come out of the box, while others require specific packages to be installed.\nPython runs in Emacs out of the box: M-x run-python.\nYou can also use Emacs as a fully-fledged IDE for programming languages. See our webinar for use with R, Julia, and Python. Later in this course, we will try it with R.",
    "crumbs": [
      "Emacs",
      "<em><b>Getting started</b></em>",
      "Other functionalities"
    ]
  },
  {
    "objectID": "emacs/intro_resources.html",
    "href": "emacs/intro_resources.html",
    "title": "Resources",
    "section": "",
    "text": "Here is a list of resources to get started with Emacs.\n\n\nInternal documentation\nYou can access Emacs internal documentation with the following kbds:\n\nC-h r: Emacs manual\nC-h t: Emacs tutorial\nC-h i: Emacs info directory\nC-h k: info on kbd\nC-h f: info on function (command)\nC-h v: info on variable\nC-h a: info on command matching regexp\n\n\n\nOfficial documentation\n\nGNU Emacs website\nGuided tour of Emacs\nGNU Emacs manual\nManuals for specific features\nEmacs wiki\n\n\n\nQ&A\n\nStack Overflow [emacs] tag\nEmacs Stack Exchange\n\n\n\nGNU Emacs Lisp\n\nOfficial Reference manual",
    "crumbs": [
      "Emacs",
      "<em><b>Getting started</b></em>",
      "Resources"
    ]
  },
  {
    "objectID": "emacs/intro_packages.html",
    "href": "emacs/intro_packages.html",
    "title": "Packages",
    "section": "",
    "text": "Emacs is a huge and endlessly customizable toolkit out of the box. In addition, countless external packages have been (and continue to be) developed to add yet more functionality. This section will cover the basics of package installation and customization.",
    "crumbs": [
      "Emacs",
      "<em><b>Getting started</b></em>",
      "Packages"
    ]
  },
  {
    "objectID": "emacs/intro_packages.html#package-manager",
    "href": "emacs/intro_packages.html#package-manager",
    "title": "Packages",
    "section": "Package manager",
    "text": "Package manager\nThere are multiple ways to manage external Emacs packages. package.el is the built-in package manager. Several packages provide alternative package management systems. My favourite by far is straight. It allows to install packages from anywhere (MELPA, ELPA, Emacsmirror, local server, GitLab, GitHub…). Packages are cloned as Git repos instead of tarballs, making it easy to revert to an old version, edit, etc. Packages are also compiled natively for better efficiency.\nTo install straight, you need to put the following in your init file:\n;; Install straight\n(defvar bootstrap-version)\n(let ((bootstrap-file\n       (expand-file-name \"straight/repos/straight.el/bootstrap.el\" user-emacs-directory))\n      (bootstrap-version 6))\n  (unless (file-exists-p bootstrap-file)\n    (with-current-buffer\n        (url-retrieve-synchronously\n         \"https://raw.githubusercontent.com/radian-software/straight.el/develop/install.el\"\n         'silent 'inhibit-cookies)\n      (goto-char (point-max))\n      (eval-print-last-sexp)))\n  (load bootstrap-file nil 'nomessage))\nThen you need to evaluate this code. For this, you can close and re-open Emacs. Alternatively, you can select the paragraph and run M-x eval-region.",
    "crumbs": [
      "Emacs",
      "<em><b>Getting started</b></em>",
      "Packages"
    ]
  },
  {
    "objectID": "emacs/intro_packages.html#package-location",
    "href": "emacs/intro_packages.html#package-location",
    "title": "Packages",
    "section": "Package location",
    "text": "Package location\nExcept for the init file which, by default, lives directly in your home directory, all Emacs configuration files get created in a directory called .emacs.d located in your home directory. This is where Emacs will store your installed packages.\nIf you use straight to manage your packages, a straight directory will be created in ~/.emacs.d and in it, you will see two subdirectories:\n\nrepos which holds the cloned Git repos of the packages and\nbuild which holds the built packages.",
    "crumbs": [
      "Emacs",
      "<em><b>Getting started</b></em>",
      "Packages"
    ]
  },
  {
    "objectID": "emacs/intro_packages.html#package-loading-and-configuration",
    "href": "emacs/intro_packages.html#package-loading-and-configuration",
    "title": "Packages",
    "section": "Package loading and configuration",
    "text": "Package loading and configuration\nUse-package is a modern package that allows lazy loading of packages for a speedy startup and a neat way to configure Emacs package by package.\nDue to the huge popularity of this package, starting with Emacs 29, use-package ships with Emacs and doesn’t need to be installed. Prior to Emacs 29, it can be installed (using straight) with:\n;; Install use-package (unnecessary for Emacs &gt;= 29)\n(straight-use-package 'use-package)",
    "crumbs": [
      "Emacs",
      "<em><b>Getting started</b></em>",
      "Packages"
    ]
  },
  {
    "objectID": "emacs/intro_packages.html#installing-packages",
    "href": "emacs/intro_packages.html#installing-packages",
    "title": "Packages",
    "section": "Installing packages",
    "text": "Installing packages\nWhen you install a new package, the best thing to do is to read the README carefully and start with minimal configuration. A growing number of packages will give you configuration instructions using use-package.\nWith usage, you can add more configurations, either in your use-package declaration or using the easy customization interface.\n\nExample: ESS package\nFirst, let’s create a file called test.R with the following R code in it:\na &lt;- c(1, 2, 3)\n\nb &lt;- 5L\n\n\nYour turn:\n\n\nWhat is the major mode used by Emacs?\n\nWhy do you think that is?\n\n\nTo get the proper major mode which will give us syntax highlighting and indentation for R, as well as a lot of additional functionality, we need to install the package ESS (Emacs Speaks Statistics).\nTo install it using straight, you can put the following in your init file:\n(straight-use-package 'ess)\nOr you can use the perfectly equivalent expression:\n(use-package ess\n    :straight t)\nThe advantage of this second syntax is that you can now add any customization you want to the use-package declaration.\n\n\nYour turn:\n\nAfter evaluating this snippet of code in your init file, re-open test.R.\n- What is the major mode now?\n- Notice that we now also have syntax highlighting for R.\n\nNow that we have a proper mode for R, we can even use Emacs as an IDE.\nFirst, of course, we will need to have R available. Send Emacs to the background with C-z and load the R module:\nmodule load r/4.3.1\nThen bring the test.R file back to the foreground by typing fg and Enter in the terminal.\nNow, you can use the kbd C-c C-c, bound to ess-eval-region-or-function-or-paragraph-and-step, to send sections of code from the script to a buffer containing a running R console.\nIf you want to have two windows, one with your script on the left and one with your running R process on the right, you need to split windows, select the proper buffers to display in each window, and move the cursor to the script. This is easy to do, but a bit annoying to have to do each time you want to run R from script.\nInstead, you could save a keyboard macro with all these commands and set a kbd for it. Or you can define a function doing all this and set a kbd. Let’s do it as an example of configuration using use-package:\n(use-package ess-r-mode\n    :straight (ess)\n    :config\n    (defun my-start-r ()\n      (interactive)\n      (split-window-right)\n      (R)\n      (other-window 1))\n    :bind (:map ess-r-mode-map\n                ;; start R process from script\n                (\"C-c r\" . my-start-r)))\n\nNote that we had to edit our use-package declaration a little because ESS provides modes for both R and Julia. This is a weird case. Usually, you don’t have to make any such change when you add configuration to the use-package declaration.\n\nAfter evaluating this declaration, you can now launch an R process from any R script, in a window to the right, with the kbd C-c r (after which you can evaluate your R script chunk by chunk with C-c C-c).",
    "crumbs": [
      "Emacs",
      "<em><b>Getting started</b></em>",
      "Packages"
    ]
  },
  {
    "objectID": "emacs/intro_kbd.html",
    "href": "emacs/intro_kbd.html",
    "title": "Emacs keybindings",
    "section": "",
    "text": "In this section, we will explore the endlessly humoristic topic of Emacs keybindings.",
    "crumbs": [
      "Emacs",
      "<em><b>Getting started</b></em>",
      "Key bindings"
    ]
  },
  {
    "objectID": "emacs/intro_kbd.html#on-emacs-keybindings",
    "href": "emacs/intro_kbd.html#on-emacs-keybindings",
    "title": "Emacs keybindings",
    "section": "On Emacs keybindings",
    "text": "On Emacs keybindings\nOne of the strengths and weaknesses of Emacs are its keybindings (kbd). Strength because everything can be bound to a kbd and kbd are—as everything else in Emacs—fully customizable. This means that you can make Emacs truly your own and work on text very quickly from the keyboard. Weakness because the default kbds are overwhelming to new users and the gymnastics they involve has lead to a lot of jokes.\n\n\n\nFrom Ecol LG #134 by Javier Malonda.",
    "crumbs": [
      "Emacs",
      "<em><b>Getting started</b></em>",
      "Key bindings"
    ]
  },
  {
    "objectID": "emacs/intro_kbd.html#kbd-notations",
    "href": "emacs/intro_kbd.html#kbd-notations",
    "title": "Emacs keybindings",
    "section": "Kbd notations",
    "text": "Kbd notations\nFirst of all, a note about notations:\n\nC-c means press the Control key and the C key together,\nM-x means press the Alt (Windows) or Option (macOS) key and the X key together,\nC-c m means press the Control key and the C key together, then press the M key,\nC-c C-x m means press Ctl+C, then Ctl+X, then M,\nDEL means press the Backspace key,\nSPC means press the Space bar,\nS-SPC means press Shift and the Space bar together,\nESC means press the Escape key,\ns-t means press the Window key (Windows) or Command key (macOS) and the T key together,\nC-x C-c M-w C-m M-v M-t M-u means that you probably should choose another kbd.",
    "crumbs": [
      "Emacs",
      "<em><b>Getting started</b></em>",
      "Key bindings"
    ]
  },
  {
    "objectID": "emacs/intro_kbd.html#main-kbds",
    "href": "emacs/intro_kbd.html#main-kbds",
    "title": "Emacs keybindings",
    "section": "Main kbds",
    "text": "Main kbds\nFor the rest of this course, you should have this page open in a separate tab (or even monitor if you can) as the reference you will need to look at constantly as we work in Emacs. Within a few days though, you will know these and they will become second nature—so much so that you will start truly missing them when you work outside Emacs!\nMost important\n\nC-g         Cancel beginning of command\nC-x C-c     Save file and exit Emacs\nC-z         Send Emacs to the background to get back to the terminal\n            The Emacs window can be brought back from the terminal with: fg (foreground)\nESC ESC ESC Get out command (exit minibuffer, close other windows, etc.)\n\nBuffers\n\nC-x b       Switch buffer\nC-x C-b     List buffers\nC-x k       Kill buffer\n\nWindows\n\nC-x 1       Delete other windows\nC-x 2       Split window below\nC-x 3       Split window right\nC-x o       Jump to other window\nC-M-v       Scroll other window\n\nFiles\n\nC-x C-f     Find file\nC-x C-s     Save file\n\nNavigation\n\nC-f         Move forward one character\nC-b         Move backward one character\n\nC-p         Move to previous line\nC-n         Move to next line\n\nC-a         Move to beginning of line\nC-e         Move to end of line\n\nM-a         Move to beginning of sentence\nM-e         Move to end of sentence\n\nM-f         Move forward one word\nM-b         Move backward one word\n\nC-v         Move forward one screenful\nM-v         Move backward one screenful\n\nC-l         Center text around cursor\n\nM-&lt;         Move to beginning of buffer\nM-&gt;         Move to end of buffer\n\nC-u         Universal argument:\nC-u 3 C-f   Move forward three characters\nC-u -4 C-f  Move backward four characters\nC-u 5 M-e   Move forward five sentences\nC-u 6 t     Type six characters t\n\nEditing\n\nDEL         Delete character before cursor\nC-d         Delete character after cursor\n\nM-DEL       Kill word before cursor\nM-d         Kill word after cursor\n\nC-k         Kill to end of line\nM-k         Kill to end of sentence\n\nC-SPC       Set mark to select region\nC-x h       Select all\n\nC-w         Kill region (cut)\nM-w         Copy region\nC-y         Yank killed text (paste)\nM-y         Following C-y: go back in kill ring to yank\n\nC-/         Undo (to redo, use C-g followed by another C-/)\n\nSearching\n\nC-s         Increamental search forward (repeat for next occurance)\nC-r         Incremental search backward (repeat for previous occurance)\nM-n         While in search: go forward in search history\nM-p         While in search: go backward in search history\n\nEmacs was so influential in the early days of computing that many other software actually use Emacs kbds. This is the case for instance of all shells, REPLs, terminals, and consoles.\nNext time you are in Bash, or in your Python/Julia/R shell, try the commands above and you will see that many of them will work.\n\nAll of these kbds are of course customizable.\nThis list is not exhaustive. You can also associate a new kbd to any command.",
    "crumbs": [
      "Emacs",
      "<em><b>Getting started</b></em>",
      "Key bindings"
    ]
  },
  {
    "objectID": "emacs/intro_kbd.html#emacs-pinky",
    "href": "emacs/intro_kbd.html#emacs-pinky",
    "title": "Emacs keybindings",
    "section": "Emacs pinky",
    "text": "Emacs pinky\nBecause so many Emacs kbds involve the Control key, it can be very tiresome for the pinky finger. Most Emacs users remap their keyboard to have the Caps lock key into another Control key. This page gives information on how to do this with most operating systems.",
    "crumbs": [
      "Emacs",
      "<em><b>Getting started</b></em>",
      "Key bindings"
    ]
  },
  {
    "objectID": "emacs/intro_kbd.html#practice",
    "href": "emacs/intro_kbd.html#practice",
    "title": "Emacs keybindings",
    "section": "Practice",
    "text": "Practice\nLet’s practice those common kbds thanks to the Emacs tutorial: launch Emacs, navigate to the link “Emacs tutorial” (use C-n four times for that), and press Enter. This will open a buffer with the Emacs tutorial. The tutorial covers the kbds above and provides an opportunity to play with an Emacs text.",
    "crumbs": [
      "Emacs",
      "<em><b>Getting started</b></em>",
      "Key bindings"
    ]
  },
  {
    "objectID": "emacs/intro_kbd.html#executing-commands-without-kbds",
    "href": "emacs/intro_kbd.html#executing-commands-without-kbds",
    "title": "Emacs keybindings",
    "section": "Executing commands without kbds",
    "text": "Executing commands without kbds\nAnother way to execute commands interactively is to type M-x (this brings up the minibuffer, a place in which to type inputs) followed by the command name. This is useful if a command is not bound to a kbd or if you don’t remember its kbd.\n\nFor example, M-x count-words will output the number of lines, sentences, words, and characters of the current buffer in the echo area.\n\n\nCommands are series of words separated by hyphens (-), but you can type spaces instead: Emacs will add the hyphens automatically for you.\nAfter you have entered M-x, you can use the tab key for autocompletion and you can use the kbds M-p and M-n to go back and forth in the command history.",
    "crumbs": [
      "Emacs",
      "<em><b>Getting started</b></em>",
      "Key bindings"
    ]
  },
  {
    "objectID": "emacs/intro_dired.html",
    "href": "emacs/intro_dired.html",
    "title": "Directory editor",
    "section": "",
    "text": "Once you are used to the Emacs environment and you have made it your own with customizations, it can be very comfortable to work in it. For this reason, many people do much more than text editing in Emacs. One of its strengths is actually that it can replace many other tools, letting you do most of your work with the same kbds and habits.\nAmong many other things, Emacs is is a powerful file manager.",
    "crumbs": [
      "Emacs",
      "<em><b>Getting started</b></em>",
      "Directory editor"
    ]
  },
  {
    "objectID": "emacs/intro_dired.html#what-is-dired",
    "href": "emacs/intro_dired.html#what-is-dired",
    "title": "Directory editor",
    "section": "What is Dired?",
    "text": "What is Dired?\nDired (directory editor) can be launched with M-x dired or C-x d and choosing the directory to open in the Dired buffer. You can also use wildcards to select a subset of files matching some pattern.\nYou can quit Dired with the usual C-x k, but also simply with q.\nMuch can be done with it and we won’t have time to cover it all, but if you want to learn more, you can go over the Dired manual. You will see that you can really do a lot in Dired and configure the ls flags launched by default. The sections below only cover a subset of commands.\nLet’s launch Dired.",
    "crumbs": [
      "Emacs",
      "<em><b>Getting started</b></em>",
      "Directory editor"
    ]
  },
  {
    "objectID": "emacs/intro_dired.html#navigation",
    "href": "emacs/intro_dired.html#navigation",
    "title": "Directory editor",
    "section": "Navigation",
    "text": "Navigation\nTo up or down a line in the Dired buffer, you can use the classic C-n and C-p, but you can also simply use n or SPC and p.\nYou can jump to a file with j followed by the file name.",
    "crumbs": [
      "Emacs",
      "<em><b>Getting started</b></em>",
      "Directory editor"
    ]
  },
  {
    "objectID": "emacs/intro_dired.html#opening-files",
    "href": "emacs/intro_dired.html#opening-files",
    "title": "Directory editor",
    "section": "Opening files",
    "text": "Opening files\nFiles can be opened with f or RET (the Enter key), they can be opened in another window with o, opened in another window but without jumping to it with C-o, or they can simply be viewed with v.\n“Viewing a file” means that the minor mode View Mode is enabled. The file cannot be edited, but it can be read quickly by scrolling up and down by entire screen-full with SPC and S-SPC or DEL. You can quit the file with q or turn off View Mode while keeping the position in the file with e.\n\n\nYour turn:\n\nNavigate to your .bashrc file and view it.",
    "crumbs": [
      "Emacs",
      "<em><b>Getting started</b></em>",
      "Directory editor"
    ]
  },
  {
    "objectID": "emacs/intro_dired.html#deleting-files",
    "href": "emacs/intro_dired.html#deleting-files",
    "title": "Directory editor",
    "section": "Deleting files",
    "text": "Deleting files\nd flags a file for deletion, u unflags it, and x deletes the files flagged for deletion.\n# deletes all auto-save files and ~ deletes all backup files (see section on Backup files).\n% d regexp RET flags for deletion files matching regexp.\n\nBe careful that deleting files this way is equivalent to running rm from the command line: files don’t go to a garbage bin, but are truly deleted.",
    "crumbs": [
      "Emacs",
      "<em><b>Getting started</b></em>",
      "Directory editor"
    ]
  },
  {
    "objectID": "emacs/intro_dired.html#other-operations",
    "href": "emacs/intro_dired.html#other-operations",
    "title": "Directory editor",
    "section": "Other operations",
    "text": "Other operations\nYou can mark several files with m (you will see a star * appear at the start of the line) and perform the operations below on all marked files at once. To remove all marks, press U. If no file is marked, these operations are performed on the current file (file where the cursor is).\nYou can copy files with C, delete them with D, rename them with R, create hard links with H or symlinks with S.\nYou can also change mode with M, change group with G, or change owner with O.\nYou can run touch (change the timestamp) with T or compress the file with Z.\nA will search files for a regexp and Q will replace regexp with whatever expression you provide.\nYou can also apply shell commands with !.\n% u turns names to upper case, % l to lower case, % R, % C, % H, and % S will rename, copy, create hard links and symlinks of selected files based on a regexp. This is extremely convenient to quickly renaming many files.\nFinally, you can compare files with = (this will run diff on both files).",
    "crumbs": [
      "Emacs",
      "<em><b>Getting started</b></em>",
      "Directory editor"
    ]
  },
  {
    "objectID": "emacs/intro_dired.html#subdirectories",
    "href": "emacs/intro_dired.html#subdirectories",
    "title": "Directory editor",
    "section": "Subdirectories",
    "text": "Subdirectories\nThe content of subdirectories can be viewed in a section below with i. Sections can then be contracted or extended with $.\n\n\nYour turn:\n\nDisplay the content of the subdirectory projects.",
    "crumbs": [
      "Emacs",
      "<em><b>Getting started</b></em>",
      "Directory editor"
    ]
  },
  {
    "objectID": "emacs/intro_dired.html#editing-files",
    "href": "emacs/intro_dired.html#editing-files",
    "title": "Directory editor",
    "section": "Editing files",
    "text": "Editing files\nC-x C-q toggles Wdired mode—a mode in which you can directly edit file and directory names. Once you have edited what you wanted, save the changes with C-c C-c.\nTo edit permissions, you need to set the variable wdired-allow-to-change-permissions to 1 or 2. For this, run M-x customize variable wdired-allow-to-change-permissions, navigate to “Value Menu”, press Enter, type 1 or 2, press Enter, then navigate to “State”, press Enter, and save the change.\nYou can now edit the file permissions simply by typing r, w, or x directly in the WDired buffer.\n\n\nYour turn:\n\nChange the permission of the .bash_logout file to rw-rw----.",
    "crumbs": [
      "Emacs",
      "<em><b>Getting started</b></em>",
      "Directory editor"
    ]
  },
  {
    "objectID": "emacs/intro_completion.html",
    "href": "emacs/intro_completion.html",
    "title": "Selection and completion frameworks",
    "section": "",
    "text": "One of the reasons why I love working in Emacs is the ease to find and jump to files or specific locations in files. Whether it is reopening a recent document, jumping to a bookmark, hopping to a specific header, looking for an expression, it can all be done smoothly and with previews thanks to a powerful modern selection framework.\nAnother strength is the countless options to auto-complete text. The same modern framework can also be used here.",
    "crumbs": [
      "Emacs",
      "<em><b>Getting started</b></em>",
      "Completion frameworks"
    ]
  },
  {
    "objectID": "emacs/intro_completion.html#the-backends",
    "href": "emacs/intro_completion.html#the-backends",
    "title": "Selection and completion frameworks",
    "section": "The backends",
    "text": "The backends\nEmacs has a bookmark system, it can open files with M-x find-file, look for recently opened files with M-x recentf, search in a buffer with M-x isearch, switch to another open buffer with M-x switch-to-buffer, jump to previous positions in the mark ring, yank text from the kill ring, and countless other functionalities.\nUsing such functions directly works, but it doesn’t make for the best user experience. Accessing them via a frontend that expands the minibuffer, shows available options, narrows them down through incremental search, and offers previews is a huge improvement.\nMultiple such frontends, increasingly powerful and/or efficient, have been developed over time. All of them are still available.",
    "crumbs": [
      "Emacs",
      "<em><b>Getting started</b></em>",
      "Completion frameworks"
    ]
  },
  {
    "objectID": "emacs/intro_completion.html#a-history-of-completion-frameworks-in-emacs",
    "href": "emacs/intro_completion.html#a-history-of-completion-frameworks-in-emacs",
    "title": "Selection and completion frameworks",
    "section": "A history of completion frameworks in Emacs",
    "text": "A history of completion frameworks in Emacs\n\nIn the minibuffer\nA number of completions in Emacs happen in the minibuffer. Those are governed by the completing-read function.\nBy default, the minibuffer is a single line with no offer of available options. It is quite dry… but many packages have improved it.\nFirst, came IDO (“Interactively DO things”), part of Emacs. It expands the minibuffer and shows options to choose from.\n\n\n\nFrom Xah Emacs Blog\n\n\nThen, the IDO vertical package made the list of options in the minibuffer vertical, which is a big visual improvement.\n\n\n\nFrom oremacs\n\n\nHELM came and revolutionized the Emacs world. It became so popular that replacements for many basic Emacs functions got written to work with the HELM frontend.\nHELM doesn’t just expands the minibuffer, it turns it into a fully-fledged buffer for much improved functionality.\n\n\n\nFrom oracleyue\n\n\nBecause HELM is such a heavy duty tool, it tends to be slow. It also requires rewrites for all of the common function. Ivy came about to bring the snappiness of IDO back. Optional Counsel & Swiper make it nicer with function rewrites.\n\n\n\nFrom abo-abo/swiper\n\n\n\n\nIn the editing buffer\nIn-buffer completions, governed by completion-at-point are completions that happen in the buffer itself.\nBy default, the available options are displayed in a *Completions* buffer that is quite clunky to navigate. A number of packages have instead allowed them to happen in small pop-ups.\nFirst came auto-complete.\n\n\n\nFrom auto-complete\n\n\nThen came company-mode.\n\n\n\nFrom company-mode website",
    "crumbs": [
      "Emacs",
      "<em><b>Getting started</b></em>",
      "Completion frameworks"
    ]
  },
  {
    "objectID": "emacs/intro_completion.html#a-modern-framework",
    "href": "emacs/intro_completion.html#a-modern-framework",
    "title": "Selection and completion frameworks",
    "section": "A modern framework",
    "text": "A modern framework\nIn recent years, a new set of packages came about which integrate closely Emacs internal functions. Lightweight, they are incredibly fast. Each package works on its own and you can pick and choose which functionality you want.\n\nList of packages\n\n\nMinibuffer\n\nvertico    Frontend completion UI\norderless   Backend completion style\nconsult   Backend completion functions\nmarginalia   Annotations\nembark   Actions on completion buffer\n\n\nIn-buffer\n\ncorfu    Frontend completion UI\norderless   Backend completion style\ncape    Backend completion functions\n\n\n\nI will demo usage of these packages in class.\n\n\nConfiguration\nThese packages are extremely well documented and you will find in the READMEs all the information you need to install and configure them to your liking.\nAs an example, I am sharing here my own configurations (menus most kbds which rely on an exotic self-made system), including those for eglot—Emacs client for Language Server Protocol servers, Abbrevs—Emacs abbreviation system, yasnippet—a template system for Emacs, and copilot, an Emacs plug-in to GitHub Copilot.\nIt wouldn’t make much sense to copy-paste this to your init file blindly: instead, read careful the README of the various packages, decide which you want to use, and start with a minimal configuration based on the packages’ authors suggestions.\n\n\n\n\n\n\nExample\n\n\n\n\n\n;; completion utilities\n\n;; orderless\n;; backend completion matching style (regexp, flex, initialism...)\n(use-package orderless\n    :straight t\n    :custom\n    (completion-styles '(orderless basic))\n    (completion-category-overrides '((file (styles basic partial-completion))\n                                     (eglot (styles orderless)))))  ; use orderless with eglot\n\n;; embark\n(use-package embark\n    :straight t\n    :init\n    (setq prefix-help-command #'embark-prefix-help-command)\n    :config\n    (defun my-embark-bindings-global ()\n      (interactive)\n      (embark-bindings t))\n    ;; hide the mode line of the embark live/completions buffers\n    (add-to-list 'display-buffer-alist\n                 '(\"\\\\`\\\\*Embark Collect \\\\(Live\\\\|Completions\\\\)\\\\*\"\n                   nil\n                   (window-parameters (mode-line-format . none))))\n    (defun my-embark-kill (&optional arg)\n      (interactive \"P\")\n      (require 'embark)\n      (if-let ((targets (embark--targets)))\n          (let* ((target\n                  (or (nth\n                       (if (or (null arg) (minibufferp))\n                           0\n                           (mod (prefix-numeric-value arg) (length targets)))\n                       targets)))\n                 (type (plist-get target :type)))\n            (cond\n              ((eq type 'buffer)\n               (let ((embark-pre-action-hooks))\n                 (embark--act 'kill-buffer target)))))))\n    :bind ((\"&lt;f1&gt; b\" . my-embark-bindings-global)\n           :map minibuffer-local-map\n           (\"C-;\" . embark-dwim)\n           (\"C-SPC\" . embark-act-all)\n           (\"C-,\" . embark-act)\n           (\"M-k\" . my-embark-kill)))\n\n(use-package embark-consult\n  :hook\n  (embark-collect-mode . consult-preview-at-point-mode))\n\n;; marginalia\n(use-package marginalia\n    :straight t\n    :init\n    (marginalia-mode 1)\n    :bind (:map minibuffer-local-map\n                (\"M-a\" . marginalia-cycle)))\n\n;; minibuffer completion\n\n;; vertico\n;; frontend for completion in minibuffer\n(use-package vertico\n    :straight t\n    :init\n    (vertico-mode 1)\n    ;; config of display for each function\n    (vertico-multiform-mode 1)\n    :config\n    (setq vertico-multiform-commands\n          '((consult-line buffer)\n            (consult-line-thing-at-point buffer)\n            (consult-recent-file buffer)\n            (consult-mode-command buffer)\n            (consult-complex-command buffer)\n            (consult-locate buffer)\n            (consult-project-buffer buffer)\n            (consult-ripgrep buffer)\n            (consult-fd buffer)\n            (telega-msg-add-reaction buffer)))\n    (defun my-exit-keeping-point ()\n      (interactive)\n      (let ((location (with-minibuffer-selected-window (point-marker))))\n        (run-at-time 0 nil #'consult--jump location)\n        (exit-minibuffer)))\n    :bind (:map vertico-map\n                (\"C-k\" . kill-whole-line)\n                (\"C-u\" . kill-whole-line)\n                (\"C-o\" . vertico-next-group)\n                (\"&lt;tab&gt;\" . minibuffer-complete)\n                (\"M-&lt;return&gt;\" . minibuffer-force-complete-and-exit)\n                (\"C-&lt;return&gt;\" . my-exit-keeping-point)))\n\n;; save search history\n(use-package savehist\n    :init\n    (savehist-mode 1))\n\n;; consult\n;; backend completion functions\n(use-package consult\n    :straight t\n    ;; buffers, files, etc\n    :config\n    (defun my-get-major-mode-name ()\n      (interactive)\n      (message \"`%s'\" major-mode))\n    (defalias 'consult-line-thing-at-point 'consult-line)\n    (consult-customize\n     consult-line-thing-at-point\n     :initial (thing-at-point 'symbol)))\n    \n;; completion at point\n\n;; corfu\n(use-package corfu\n    :straight t\n    :init\n    (global-corfu-mode 1)\n    :bind (:map corfu-map\n                ;; Configure SPC for separator insertion\n                (\"SPC\" . corfu-insert-separator)\n                (\"&lt;tab&gt;\" . corfu-next)\n                (\"C-n\" . corfu-next)\n                (\"C-p\" . corfu-previous)))\n\n;; dabbrev\n(use-package dabbrev\n    :custom\n    (dabbrev-ignored-buffer-regexps '(\"\\\\.\\\\(?:pdf\\\\|jpe?g\\\\|png\\\\)\\\\'\"))\n    :bind ((\"&lt;tab&gt;\" . dabbrev-expand)\n           (\"; &lt;tab&gt;\" . dabbrev-completion)))\n\n;; cape\n(use-package cape\n    :straight t\n    :bind (\"C-'\" . completion-at-point))\n\n;; abbrev\n(use-package abbrev\n  :straight nil\n  :config\n  (setq-default abbrev-mode t)\n  :bind (\"C-c &lt;tab&gt;\" . add-global-abbrev))\n\n;; yasnippet\n(use-package yasnippet\n    :straight t\n    :init\n    (yas-global-mode 1)\n    :config\n    (setq yas-snippet-dirs '(\"~/.emacs.d/snippets\"))\n    :bind ((\"C-c y n\" . yas-new-snippet)         ; y=yas, n=new\n           (\"C-c y e\" . yas-visit-snippet-file)  ; y=yas, e=edit\n           (\"C-c y r\" . yas-reload-all)          ; y=yas, r=reload\n           ;; and rebind open-line (C-o)\n           (\"; C-o\" . open-line)\n           ;; when yas-minor-mode-map is active\n           :map yas-minor-mode-map\n           (\"&lt;tab&gt;\" . nil)\n           (\"C-o\" . yas-expand)\n           ;; during snippet completion\n           :map yas-keymap\n           (\"&lt;tab&gt;\" . nil)\n           (\"C-o\" . yas-next-field-or-maybe-expand)))\n\n;; yasnippet-capf\n(use-package yasnippet-capf\n    :straight t\n    :after cape)\n\n;; auto-yasnippet\n(use-package auto-yasnippet\n    :straight t\n    :bind (\"C-c y a\" . aya-create))\n\n;; eglot\n(straight-use-package 'eglot-jl)\n\n;; copilot\n;; copilot dependency\n(straight-use-package 'editorconfig)\n\n(use-package copilot\n    :straight (:host github :repo \"copilot-emacs/copilot.el\" :files (\"dist\" \"*.el\"))\n    :bind ((\"C-8\" . copilot-complete)\n           :map copilot-completion-map\n           (\"C-j\" . copilot-accept-completion)\n           (\"C-f\" . copilot-accept-completion-by-word)\n           (\"C-t\" . copilot-accept-completion-by-line)\n           (\"M-n\" . copilot-next-completion)\n           (\"M-p\" . copilot-previous-completion)))",
    "crumbs": [
      "Emacs",
      "<em><b>Getting started</b></em>",
      "Completion frameworks"
    ]
  },
  {
    "objectID": "emacs/intro_automation.html",
    "href": "emacs/intro_automation.html",
    "title": "Automation",
    "section": "",
    "text": "A good text editor should make your work easier and more efficient. At the core of efficiency is automation which can be achieved by various techniques such as powerful search and replace, usage of regular expressions, definition of functions, or keyboard macros.",
    "crumbs": [
      "Emacs",
      "<em><b>Getting started</b></em>",
      "Automation"
    ]
  },
  {
    "objectID": "emacs/intro_automation.html#search-and-replace",
    "href": "emacs/intro_automation.html#search-and-replace",
    "title": "Automation",
    "section": "Search and replace",
    "text": "Search and replace\nOpen a new file (you can call it exercise if you want).\n\nRemember that one option is to launch Emacs (or emacsclient if you have an Emacs server running), then find a new file with C-x C-f. Another option is to launch Emacs (or emacsclient) directly with a new file open (with emacs exercise or emacsclient exercise).\n\nNow, yank (C-y) the following text in it:\nThis is a list:\n\n- Item one\n- Item two\n- Item three\n- Item four\n- Item five\n- Item six\n- Item seven\n- Item eight\n- Item nine\n- Item ten\nWe want to turn this list into a different style, with each item in lower case and a coma at the end.\nIn this particular example, because all items start with the same word, we can use a search and replace method.\n\nfirst, go to the start of the buffer (with M-&lt;),\nthen use the query-replace command. You can access it with M-x query-replace or with the kbd M-%,\nnow, enter what you want to look for (“Item”) and press RET (the &lt;return&gt; key labelled “Enter” on your keyboard),\nfinally type what you want to replace the query by (“item”) and press RET again.\n\nEmacs tells you that you can press ? for help. If you do so, you will see how to navigate the search and replace interface. Here are the most important kbds to remember:\nSPC to replace one match\nn to skip to next match\nRET to exit\n! to replace all remaining matches in this buffer\nSince we want to replace all instances, we can press !.\nNow, for the addition of comas at the end of items:\n\nmove back to the top of the buffer again,\nlaunch another query-replace,\nfor the query, type C-q C-j then `RET,\nfor the replacement, type , C-q C-j and RET.\n\nC-q is a way to quote the next character, in effect entering it in the search query rather than applying its effect. C-j inserts a new line (what we want since we want to add , before new lines), but if pressed unquoted, it has the same effect as pressing RET, which would move the query-replace questions along—not what we want.\n\nTry pressing C-j to see that it will insert a new line in your buffer.\n\nNow, we don’t want to add , at the end of the first or second lines, so press n (for “no”) twice. Then you can either press the space bar until the end of the list or press ! to replace the remaining matches.",
    "crumbs": [
      "Emacs",
      "<em><b>Getting started</b></em>",
      "Automation"
    ]
  },
  {
    "objectID": "emacs/intro_automation.html#search-and-replace-with-regexp",
    "href": "emacs/intro_automation.html#search-and-replace-with-regexp",
    "title": "Automation",
    "section": "Search and replace with regexp",
    "text": "Search and replace with regexp\nLet’s consider a different list. Copy it, go back to your buffer, press C-x h to select all its content, press DEL (backspace key) to erase the highlighted region, then yank what you had copied below with C-y.\nThis is a list:\n\n- First item\n- Second item\n- Third item\n- Fourth item\n- Fifth item\n- Sixth item\n- Seventh item\n- Eighth item\n- Ninth item\n- Tenth item\nIn this case, we can’t replace the first item with its lower case version with a simple search and replace. We can however do it using a regular expression:\n\ngo to the top of the buffer,\nenter the command query-replace-regexp or C-M-%,\nfor the query entry type: - \\(.\\),\nand for the replacement entry type: - \\,(downcase \\1).\n\nWhat we are doing here is to replace the first character after “-” (which we place in a group) with “-” followed by its lower case version. \\, enables us to use an Elisp expression as part of the transformation. The Elisp expression is between parenthesis and uses the command downcase. \\1 replaces the grouped expression.\n\nIf you have never used regular expression, I recommend having a look at this site which covers the topic in a very clear fashion. For Emacs specific regexps, you can find information in the manual.\n\nFor the end of line coma, we can replace the regexp $ with ,.\n\n\nYour turn:\n\nTry this yourself.",
    "crumbs": [
      "Emacs",
      "<em><b>Getting started</b></em>",
      "Automation"
    ]
  },
  {
    "objectID": "emacs/intro_automation.html#keyboard-macros",
    "href": "emacs/intro_automation.html#keyboard-macros",
    "title": "Automation",
    "section": "Keyboard macros",
    "text": "Keyboard macros\n\nWhat are keyboard macros?\nKeyboard macros are recordings of key presses. They are extremely convenient because they allow to automate any task, on the fly, without having to define functions or use complex regular expressions.\nThis is how it works:\n\nyou initiate the recording with C-x ( or &lt;f3&gt;,\nthen you do whatever task you wish to repeat,\nfinally you end the recording with C-x ) or &lt;f4&gt;.\n\nNow, to repeat the task you recorded, you just have to press C-x e or &lt;f4&gt;.\nYou can use the macro any number of times, either in succession or at any later time in your editing session. You can apply them on any buffer and you can apply them multiple times in a row with the usual repeat kbds (e.g. C-u 10 &lt;f4&gt;).\n\n\nNaming macros\nIf you define a new macro, it will replace the previously recorded one. If you want to use multiple macros at the same time, you can give them names using M-x name-last-kbd-macro. That way, there is no limit in how many you have access to simultaneously. To access any of these macros, call with as you would a command with M-x.\n\n\nSaving macros\nIf you want to save named macros for future sessions, you can add them to your init file by running M-x insert-kbd-macro.\nYou can even give them a kbd with (global-set-key (kbd \"&lt;your kbd&gt;\") '&lt;your macro&gt;).\n\n\nYour turn:\n\nGo back to your buffer, undo the changes with C-/ until the comas are gone and the items are capitalized again, then try using a keyboard macro to re-edit it efficiently.",
    "crumbs": [
      "Emacs",
      "<em><b>Getting started</b></em>",
      "Automation"
    ]
  },
  {
    "objectID": "emacs/advanced_customize.html#evaluation-order",
    "href": "emacs/advanced_customize.html#evaluation-order",
    "title": "Advanced Emacs customizations",
    "section": "Evaluation order",
    "text": "Evaluation order\nIf you write your own Emacs code, be careful that functions and variables take the value of their last loaded version. The order in which Emacs code is evaluated thus matters.\nYou want to evaluate as little as possible when you launch Emacs to speed up start-up time (lazy evaluation): you don’t want to load every single package that you have installed.\nThis means that if you overwrite a function or variable of a mode in your init file, the init file is read at start-up, but when that mode is launched, the default function/variable will overwrite the custom one you wrote in your init file.\nTo by-pass this problem, you can use eval-after-load.\n\nExample:\n(eval-after-load\n \"markdown\"\n '(defun markdown-demote ()\n    ...))\n\n\nuse-package has the :init and :config keyword symbols that ensure that the following expressions are evaluated respectively before or after the loading of a package."
  },
  {
    "objectID": "emacs/advanced_customize.html#customizing-kbd",
    "href": "emacs/advanced_customize.html#customizing-kbd",
    "title": "Advanced Emacs customizations",
    "section": "Customizing kbd",
    "text": "Customizing kbd\nLike everything else, kbds can be customized.\n\nGlobal kbd\nMost modes come with specific keymaps: sets of kbd only active when the mode is enabled.\n\nFor example, to modify the kbd for the function markdown-outline-previous in the markdown-mode-map:\n\n(define-key markdown-mode-map (kbd \"M-p\") 'markdown-outline-previous)\n\nOr, using use-package:\n\n(use-package markdown-mode\n    :bind (:map markdown-mode-map\n                (\"M-p\" . markdown-outline-previous)))"
  },
  {
    "objectID": "calendar.html",
    "href": "calendar.html",
    "title": "Upcoming training events",
    "section": "",
    "text": "Our training events also get posted on our main site."
  },
  {
    "objectID": "bash/ws_scripting.html",
    "href": "bash/ws_scripting.html",
    "title": "Automation & scripting in bash for beginners",
    "section": "",
    "text": "This workshop will demystify the command line and get you started using Bash and Bash scripting.\nWarning: you might find that working in the command line is actually really fun and addictive!",
    "crumbs": [
      "Bash/Zsh",
      "<b><em>Workshops</em></b>",
      "Scripting for beginners"
    ]
  },
  {
    "objectID": "bash/ws_scripting.html#background",
    "href": "bash/ws_scripting.html#background",
    "title": "Automation & scripting in bash for beginners",
    "section": "Background",
    "text": "Background\n\nWhat are Unix shells?\nA Unix shell is a command line interpreter: the user enters commands as text, either interactively in the command line or in a script, and the shell passes them to the operating system.\n\nBash\nBash (Bourne Again SHell), released in 1989, is part of the GNU Project and is the default Unix shell on many systems (macOS recently changed its default to zsh).\n\n\nOther shells\nPrior to Bash, the default was the Bourne shell (sh).\nA new and popular shell (backward compatible with Bash) is zsh. It extends Bash’s capabilities.\nAnother shell in the same family is the KornShell (ksh).\nAll these shells are quite similar. The C shell (csh) however was modeled on the C programming language.\nBash is the most common shell and the one which makes the most sense to learn as a first Unix shell.\n\n\n\nWhy use a shell?\nWhile automating GUI operations is really difficult, it is easy to rerun a script (a file with a number of commands). Unix shells thus allow the creation of reproducible workflows and the automation of repetitive tasks.\nThey are powerful to launch tools, modify files, search text, or combine commands.\nThey also allow to work on remote machines and HPC systems.",
    "crumbs": [
      "Bash/Zsh",
      "<b><em>Workshops</em></b>",
      "Scripting for beginners"
    ]
  },
  {
    "objectID": "bash/ws_scripting.html#how-we-will-use-bash-today",
    "href": "bash/ws_scripting.html#how-we-will-use-bash-today",
    "title": "Automation & scripting in bash for beginners",
    "section": "How we will use Bash today",
    "text": "How we will use Bash today\nBash is a Unix shell. You thus need a Unix or Unix-like operating system.\nWe will connect to a remote HPC system via SSH (secure shell). HPC systems always run Linux.\nThose on Linux or macOS can alternatively use Bash directly on their machine. On macOS, the default is now zsh (you can see that by typing echo $SHELL in Terminal), but zsh is fully compatible with Bash commands, so it is totally fine to use it instead. If you really want to use Bash, simply launch it by typing in Terminal: bash.\n\nConnecting to a remote HPC system via SSH\n\nUsernames and password\nWe will give you a link to an etherpad during the workshop. Add your name next to a free username to claim it.\nWe will also give you the password for our training cluster. When prompted, enter it.\n\nNote that you will not see any character as you type the password: this is called blind typing and is a Linux safety feature. Type slowly and make sure not to make typos. It can be unsettling at first not to get any feed-back while typing.\n\n\n\nLinux and macOS users\nLinux users: open the terminal emulator of your choice.\nmacOS users: open “Terminal”.\nThen type:\nssh userxx@bashworkshop.c3.ca  # Replace userxx by your username (e.g. user09)\n\n\nWindows users\nWe suggest using the free version of MobaXterm.\nMobaXterm comes with a terminal emulator and a GUI interface for SSH sessions.\nOpen MobaXterm, click on “Session”, then “SSH”, and fill in the Remote host name and your username. Here is a live demo.",
    "crumbs": [
      "Bash/Zsh",
      "<b><em>Workshops</em></b>",
      "Scripting for beginners"
    ]
  },
  {
    "objectID": "bash/ws_scripting.html#bash-the-basics",
    "href": "bash/ws_scripting.html#bash-the-basics",
    "title": "Automation & scripting in bash for beginners",
    "section": "Bash: the basics",
    "text": "Bash: the basics\n\nThe prompt\nIn command-line interfaces, a command prompt is a sequence of characters indicating that the interpreter is ready to accept input. It can also provide some information (e.g. time, error types, username and hostname, etc.)\nThe Bash prompt is customizable. By default, it often gives the username and the hostname, and it typically ends with $.\n\n\nHelp on commands\nMan pages:\nman &lt;command&gt;\n\nMan pages open in a pager (usually less).\nNavigate up/down with the space bar and the b key.\nQuit the pager with the q key.\n\nHelp pages:\n&lt;command&gt; --help\nInspect commands:\ncommand -V &lt;command&gt;\n\n\nExamples of commands\n\nPrint working directory: pwd\nChange directory: cd\nPrint: echo\nPrint content of a file: cat\nList: ls\nCopy: cp\nMove or rename: mv\nCreate a new directory: mkdir\nCreate a new file: touch\n\n\n\nKeybindings\nClear the terminal (command clear) with C-l (this means: press the Ctrl and L keys at the same time).\nNavigate command history with C-p and C-n (or up and down arrows).\nYou can auto-complete commands by pressing the tab key.",
    "crumbs": [
      "Bash/Zsh",
      "<b><em>Workshops</em></b>",
      "Scripting for beginners"
    ]
  },
  {
    "objectID": "bash/ws_scripting.html#bash-scripting-the-basics",
    "href": "bash/ws_scripting.html#bash-scripting-the-basics",
    "title": "Automation & scripting in bash for beginners",
    "section": "Bash scripting: the basics",
    "text": "Bash scripting: the basics\nInstead of typing commands one at a time directly in a terminal, you can write them down, one per line, in a text file called a script.\nThey will be run in the order in which they are written when you execute the script.\nThis is a great way to automate tasks: to rerun this sequence of commands, you simply have to rerun the script.\n\nFile name\nShell scripts, including Bash scripts, are usually given the extension sh (e.g. my_script.sh).\nYou can store scripts anywhere, but a common practice is to store them in a ~/bin directory.\n\n\nSyntax\n\nShebang\nScripts can be written for any interpreter (e.g. Bash, Python, R, etc.) The way to tell the system which one to use is to use a shebang (#!) followed by the path of the interpreter on the first line of the script.\nTo use Bash, start your scripts with:\n#!/bin/bash\nYou may also encounter this notation:\n#!/usr/bin/env bash\nIf you are curious, you can read the answers to this Stack Overflow question for the differences between the two.\n\n\nComments\nAnything to the left of # is ignored by the interpreter and is for human consumption only.\n# You can write full-line comments\n\npwd       # You can also write comments after commands\n\n\n\nExecuting scripts\nThere are two ways to execute a script:\nbash my_script.sh\n./my_script.sh  # The dot represents the current directory\nIn the latter case, you need to make sure that your script is executable by first running:\nchmod u+x my_script.sh  # This makes the script executable by the user (i.e. you)\n\n\nOur first script\nOpen a text editor (e.g. nano) and type:\n#!/bin/bash\n\necho \"This is our first script.\"\nSave and close the file.\n\n\nYour turn:\n\nNow run the script with one, then the other method.\nWhat does this script do?",
    "crumbs": [
      "Bash/Zsh",
      "<b><em>Workshops</em></b>",
      "Scripting for beginners"
    ]
  },
  {
    "objectID": "bash/ws_scripting.html#variables",
    "href": "bash/ws_scripting.html#variables",
    "title": "Automation & scripting in bash for beginners",
    "section": "Variables",
    "text": "Variables\n\nDeclaring variables\nYou can declare a variable (i.e. a name that holds a value) with the = sign.\n!! Make sure not to put spaces around the equal sign.\nvariable=Test\n\n\nQuotes\nLet’s experiment with quotes:\n\nvariable=This string is the value of the variable\necho $variable\n\nbash: line 1: string: command not found\n\n\nOops…\n\nvariable=\"This string is the value of the variable\"\necho $variable\n\nThis string is the value of the variable\n\n\n\nvariable='This string is the value of the variable'\necho $variable\n\nThis string is the value of the variable\n\n\n\nvariable='This string's the value of the variable'\necho $variable\n\nbash: -c: line 1: unexpected EOF while looking for matching `''\n\n\nOops…\nOne solution to this is to use double quotes:\n\nvariable=\"This string's the value of the variable\"\necho $variable\n\nThis string's the value of the variable\n\n\nAlternatively, single quotes can be escaped:\n\nvariable='This string'\"'\"'s the value of the variable'\necho $variable\n\nThis string's the value of the variable\n\n\n\nAdmittedly, this last one is a little crazy. It is the way to escape single quotes in single-quoted strings.\nThe first ' ends the first string, both \" create a double-quoted string with ' (escaped) in it, then the last ' starts the second string.\nEscaping double quotes is a lot easier and simply requires \\\".\n\n\n\nExpanding a variable’s value\nTo expand a variable (to access its value), you need to prepend its name with $:\n\nvariable=Test\necho variable\n\nvariable\n\n\nMmmm… not really want we want!\n\nvariable=Test\necho $variable\n\nTest\n\n\n\nvariable=Test; echo \"$variable\"\n\nTest\n\n\n!! Single quotes don’t expand variables.\n\nvariable=Test; echo '$variable'\n\n$variable\n\n\n\n\nPassing variables to a Bash script\nCreate a script called name.sh with the following content:\n#!/bin/bash\n\necho \"My name is $1.\"  # $1 refers to the first variable passed to the script\nYou can now pass a variable to this script with:\nbash name.sh Marie\nMy name is Marie.\nYou can pass several variables to a script. Copy name.sh to name2.sh and edit name2.sh to look like the following:\n#!/bin/bash\n\necho \"My name is $1 and I am $2 years old.\"\nbash name2.sh Marie 43\nMy name is Marie and I am 43 years old.\nYou can also pass any number of variables to a script:\n#!/bin/bash\n\necho $@\nbash script.sh argument1 argument2 argument3 argument4\nargument1 argument2 argument3 argument4\n\n\nBrace expansion\n\necho {1..5}\n\n1 2 3 4 5\n\n\n\necho {01..10}\n\n01 02 03 04 05 06 07 08 09 10\n\n\n\necho {1..5}.txt\n\n1.txt 2.txt 3.txt 4.txt 5.txt\n\n\n\necho {r..v}\n\nr s t u v\n\n\n\necho {file1,file2}.sh\n\nfile1.sh file2.sh\n\n\n!! Make sure not to add a space after the comma.\ntouch {file1,file2}.sh\ntouch file{3..6}.sh\n\necho {list,of,strings}\n\nlist of strings\n\n\n\n\nWildcards\nWildcards are really powerful to apply a command to all the elements having a common pattern.\nFor instance, we can delete all the files we created earlier (file1.sh, file2.sh, etc.) with a single command:\nrm file*.sh\n!! Be very careful that rm is irreversible. Deleted files do not go to the trash: they are gone.",
    "crumbs": [
      "Bash/Zsh",
      "<b><em>Workshops</em></b>",
      "Scripting for beginners"
    ]
  },
  {
    "objectID": "bash/ws_scripting.html#loops",
    "href": "bash/ws_scripting.html#loops",
    "title": "Automation & scripting in bash for beginners",
    "section": "Loops",
    "text": "Loops\nTo apply a set of commands to all the elements of a list, you can use for loops. The general structure is as follows:\nfor &lt;iterable&gt; in &lt;list&gt;\ndo\n    &lt;statement1&gt;\n    &lt;statement2&gt;\n    ...\ndone\nLet’s create the script names.sh:\n#!/bin/bash\n\nfor name in $@\ndo\n    echo $name\ndone\nNow let’s run it with a list of arguments:\nbash names.sh Patrick Paul Marie Alex\nPatrick\nPaul\nMarie\nAlex\n\n\nYour turn:\n\nCompare the outputs of the following 2 scripts:\n\nscript1.sh:\n\n#!/bin/bash\n\necho $@\n\nscript2.sh:\n\n#!/bin/bash\n\nfor i in $@\ndo\n    echo $i\ndone\nHow do you explain the difference between running:\nbash script1.sh arg1 arg2 arg3\nand running:\nbash script2.sh arg1 arg2 arg3",
    "crumbs": [
      "Bash/Zsh",
      "<b><em>Workshops</em></b>",
      "Scripting for beginners"
    ]
  },
  {
    "objectID": "bash/ws_scripting.html#lets-put-it-all-together-to-automate-some-task",
    "href": "bash/ws_scripting.html#lets-put-it-all-together-to-automate-some-task",
    "title": "Automation & scripting in bash for beginners",
    "section": "Let’s put it all together to automate some task",
    "text": "Let’s put it all together to automate some task\nThis is a rather silly example, but bear with me and let’s imagine that it actually makes sense (of course, you don’t write that many thesis chapters so you would probably never automate these tasks…)\nSo… let’s imagine that each time you write a thesis chapter, you do the same things:\n\nyou create a directory with the name of the chapter,\nyou create a number of subdirectories (for your source code, your manuscript, your data, and your results),\nyou create a Python script in the source code directory,\nyou create a markdown document in your manuscript directory,\nyou put the whole thing under version control with Git,\nyou create a .gitignore file in which you put the data subdirectory.\n\n\n\nYour turn:\n\nWrite a script that would do all this, then test the script.\nGive it a try on your own before looking at the solution below…\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nHere is what the script looks like (let’s call it chapter.sh):\n#!/bin/bash\n\nmkdir $1\ncd $1\nmkdir src data results ms\ntouch src/$1.py ms/$1.md\ngit init\necho data/ &gt; .gitignore\nYou then run the script:\nbash chapter.sh chapter1\nYou can verify that all the files and directories got created with:\ntree chapter1\nchapter1/\n├── data\n├── ms\n│   └── chapter1.md\n├── results\n└── src\n    └── chapter1.py\nand:\nls -aF chapter1\n./  ../  data/  .git/  .gitignore  ms/  results/  src/\nYou can also verify the content of your .gitignore file with:\ncat chapter1/.gitignore\ndata/",
    "crumbs": [
      "Bash/Zsh",
      "<b><em>Workshops</em></b>",
      "Scripting for beginners"
    ]
  },
  {
    "objectID": "bash/ws_scripting.html#resources",
    "href": "bash/ws_scripting.html#resources",
    "title": "Automation & scripting in bash for beginners",
    "section": "Resources",
    "text": "Resources\nOne very useful (although very dense) resource is the Bash manual.\nYou can also get information on Bash from within Bash with:\ninfo bash\nand:\nman bash\nThere are also countless resources online and don’t forget to Google anything you don’t know how to do: you will almost certainly find the answer on StackOverflow or some Stack Exchange site.",
    "crumbs": [
      "Bash/Zsh",
      "<b><em>Workshops</em></b>",
      "Scripting for beginners"
    ]
  },
  {
    "objectID": "bash/wb_tools3.html",
    "href": "bash/wb_tools3.html",
    "title": "Modern shell utilities",
    "section": "",
    "text": "In recent years, a number of open-source utilities for the Unix shell have emerged. Some are meant as replacements for classic tools with improved performance, better defaults, or nicer-looking outputs; others add novel functionality. Several of them were recently installed on the Alliance clusters.\nIn this webinar I will cover a selection of tools that are very popular, well-maintained, and that have served me well in my daily workflows:\n\nls in colours: eza,\nsmart cd: zoxide,\na cat with wings: bat,\nRIP grep: ripgrep,\nfaster find: fd,\nfuzzy finder: fzf,\nfile system TUIs.\n\nI will also talk about three useful Zsh plugins:\n\na syntax highlighter,\nautosuggestions,\nan improved history searcher.\n\nFor each tool/plugin, I will talk about installation on a personal computer and on the Alliance clusters and I will give live demos.\n\nSlides (Click and wait: this reveal.js presentation may take a little time to load.)",
    "crumbs": [
      "Bash/Zsh",
      "<b><em>Webinars</em></b>",
      "Modern shell utilities"
    ]
  },
  {
    "objectID": "bash/wb_tools1.html",
    "href": "bash/wb_tools1.html",
    "title": "Fun tools to simplify your life in the shell",
    "section": "",
    "text": "Working in the command-line has many advantages and it is often necessary, but can it be fun?\nIn this webinar, aimed at any command-line user, I intend to demonstrate that yes, it can! by introducing three free and open source utilities which make navigating your system and your outputs a lot easier:\n\nfzf is a simple, yet extremely powerful interactive fuzzy finder allowing for incremental completion and narrowing selection of any command line output. I will show you how to build simple shell functions which harvest its power to instantly refresh your memory on your custom keybindings or aliases, navigate your command history, find and kill processes, and explore and checkout your git commits. After this, you will be able to use fzf for any number of other applications in your work in the command-line.\nautojump lets you jump anywhere you want in your directories in just a few keystrokes (no more of this painful navigation writing down long paths).\nWith the ranger file manager, you can browse (with preview!), open, copy, move, delete, etc. your files and directories in a friendly way from the command line. Added bonus: you can use fzf and autojump within ranger!\n\nWarning: too much fun in the command-line can lead to addiction and geek behaviours. Use in moderation.",
    "crumbs": [
      "Bash/Zsh",
      "<b><em>Webinars</em></b>",
      "Fun tools for the command line"
    ]
  },
  {
    "objectID": "bash/top_wb.html",
    "href": "bash/top_wb.html",
    "title": "Bash webinars",
    "section": "",
    "text": "Fun tools for the command line\n\n\n\n\nMore fun tools for the command line\n\n\n\n\nModern shell utilities",
    "crumbs": [
      "Bash/Zsh",
      "<b><em>Webinars</em></b>"
    ]
  },
  {
    "objectID": "bash/molecules/intro_wildcards.html",
    "href": "bash/molecules/intro_wildcards.html",
    "title": "Wildcards",
    "section": "",
    "text": "Wildcards are a convenient way to select items matching patterns.\n\n\n\n\n\n\n\nData for this section\n\n\n\n\n\nFor this section, we will play with files created by The Carpentries.\nYou can download them into a zip file called bash.zip with:\nwget https://bit.ly/bashfile -O bash.zip\nYou can then unzip that file with:\nunzip bash.zip\nYou should now have a data-shell directory with a molecules subdirectory.\ncd into it:\ncd data-shell/molecules\n\n\n\nLet’s list the files in the molecules directory:\nls\ncubane.pdb  ethane.pdb  methane.pdb  octane.pdb  pentane.pdb  propane.pdb\nYou could do the same with:\nls *\nThe star expands to all files/directories matching any pattern. It is a wildcard.\nOf course, you can match more interesting patterns.\nFor instance, to list all files starting with the letter o, we can run:\nls o*\noctane.pdb\nTo list all files containing the letter o anywhere in their name, you can use:\nls *o*\noctane.pdb  propane.pdb\nThis saves a lot of typing and is a powerful way to apply a command to a subset of files/directories.\n\n\nYour turn:\n\nWildcards are often used to select all files with a certain extension.\nLet’s create 3 new files:\ntouch file1.txt file2.txt file3.md\nHow would you list all files with the .txt extension and only those?",
    "crumbs": [
      "Bash/Zsh",
      "<b><em>Getting started</em></b>",
      "Wildcards"
    ]
  },
  {
    "objectID": "bash/molecules/intro_redirections.html",
    "href": "bash/molecules/intro_redirections.html",
    "title": "Redirections & pipes",
    "section": "",
    "text": "By default, commands that produce an output print it to the terminal. This output can however be redirected to be printed elsewhere (e.g. to a file) or to be passed as the argument of another command.",
    "crumbs": [
      "Bash/Zsh",
      "<b><em>Getting started</em></b>",
      "Redirections & pipes"
    ]
  },
  {
    "objectID": "bash/molecules/intro_redirections.html#redirections",
    "href": "bash/molecules/intro_redirections.html#redirections",
    "title": "Redirections & pipes",
    "section": "Redirections",
    "text": "Redirections\nBy default, commands that produce an output print it to standard output—that is, the terminal. This is what we have been doing so far.\nThe output can however be redirected with the &gt; sign. For instance, it can be redirected to a file, which is very handy if you want to save the result.\n\nExample:\n\nLet’s print the number of lines in each .pdb file in the molecules directory:\n\nwc -l *.pdb\n\n  20 gas_cubane.pdb\n  12 gas_ethane.pdb\n   9 gas_methane.pdb\n  30 gas_octane.pdb\n  21 gas_pentane.pdb\n  15 gas_propane.pdb\n 107 total\n\n\n\n\nYour turn:\n\n\nWhat does the wc command do?\nWhat does the -l flag for this command do?\nHow did you find out?\n\n\nTo save this result into a file called lengths.txt, we run:\n\nwc -l *.pdb &gt; lengths.txt\n\n\nNote that &gt; always creates a new file. If a file called lengths.txt already exists, it will be overwritten. Be careful not to lose data this way!\nIf you don’t want to lose the content of the old file, you can append the output to the existing file with &gt;&gt; (&gt;&gt; will create a file lengths.txt if it doesn’t exist yet, but if it exists, it will append the new content below the old one).\n\n\n\nYour turn:\n\nHow can you make sure that you did create a file called lengths.txt?\n\nLet’s print its content to the terminal:\n\ncat lengths.txt\n\n  20 gas_cubane.pdb\n  12 gas_ethane.pdb\n   9 gas_methane.pdb\n  30 gas_octane.pdb\n  21 gas_pentane.pdb\n  15 gas_propane.pdb\n 107 total\n\n\nAs you can see, it contains the output of the command wc -l *.pdb.\nOf course, we can print the content of the file with modification. For instance, we can sort it:\n\nsort -n lengths.txt\n\n   9 gas_methane.pdb\n  12 gas_ethane.pdb\n  15 gas_propane.pdb\n  20 gas_cubane.pdb\n  21 gas_pentane.pdb\n  30 gas_octane.pdb\n 107 total\n\n\nAnd we can redirect this new output to a new file:\n\nsort -n lengths.txt &gt; sorted.txt\n\nInstead of printing an entire file to the terminal, you can print only part of it.\nLet’s print the first line of the new file sorted.txt:\n\nhead -1 sorted.txt\n\n   9 gas_methane.pdb",
    "crumbs": [
      "Bash/Zsh",
      "<b><em>Getting started</em></b>",
      "Redirections & pipes"
    ]
  },
  {
    "objectID": "bash/molecules/intro_redirections.html#pipes",
    "href": "bash/molecules/intro_redirections.html#pipes",
    "title": "Redirections & pipes",
    "section": "Pipes",
    "text": "Pipes\nAnother form of redirection is the Bash pipe. Instead of redirecting the output to a different stream for printing, the output is passed as an argument to another command. This is very convenient because it allows to chain multiple commands without having to create files or variables to save intermediate results.\nFor instance, we could run the three commands we ran previously at once, without the creation of the two intermediate files:\n\nwc -l *.pdb | sort -n | head -1\n\n   9 gas_methane.pdb\n\n\nIn each case, the output of the command on the left-hand side (LHS) is passed as the input of the command on the right-hand side (RHS).\n\n\nYour turn:\n\nIn a directory we want to find the 3 files that have the least number of lines. Which command would work for this?\n\nwc -l * &gt; sort -n &gt; head -3\nwc -l * | sort -n | head 1-3\nwc -l * | sort -n | head -3\nwc -l * | head -3 | sort -n\n\n\nHere is a video of a previous version of this workshop.",
    "crumbs": [
      "Bash/Zsh",
      "<b><em>Getting started</em></b>",
      "Redirections & pipes"
    ]
  },
  {
    "objectID": "bash/molecules/intro_control_flow.html",
    "href": "bash/molecules/intro_control_flow.html",
    "title": "Control flow",
    "section": "",
    "text": "By default, scripts get executed linearly from top to bottom. Often however, you will want to control what gets executed when.\nThis section covers various ways to control the flow of execution through a script.",
    "crumbs": [
      "Bash/Zsh",
      "<b><em>Getting started</em></b>",
      "Control flow"
    ]
  },
  {
    "objectID": "bash/molecules/intro_control_flow.html#next-command-conditional-on-success",
    "href": "bash/molecules/intro_control_flow.html#next-command-conditional-on-success",
    "title": "Control flow",
    "section": "Next command conditional on success",
    "text": "Next command conditional on success\nCommands can be limited to running only if the previous commands ran successfully thanks to &&.\n\nExample:\n\nLook at the following commands:\nunzip bash.zip\nrm bash.zip\nThis is equivalent to:\nunzip bash.zip;\nrm bash.zip\nand to:\nunzip bash.zip; rm bash.zip\nThis is what we did to get the data for the past few sessions.\nIn both cases, both commands will try to run. Now, if for some reason, the unzipping fails, we have deleted the zip file and we have to re-download it. Not a big deal here, but in some situations, executing a command if the one before fails can be a real bummer.\nTo prevent this, we can use the double-ampersand (&&) operator, which plays the role of a logical AND statement:\nunzip bash.zip &&\nrm bash.zip\nThis is equivalent to:\nunzip bash.zip && rm bash.zip\nIf the unzipping works (if it returns a zero exit status), then the Zip file gets deleted. If however, the unzipping fails (if it returns a non-zero exit status), the script aborts and we haven’t lost our Zip file.",
    "crumbs": [
      "Bash/Zsh",
      "<b><em>Getting started</em></b>",
      "Control flow"
    ]
  },
  {
    "objectID": "bash/molecules/intro_control_flow.html#next-command-conditional-on-failure",
    "href": "bash/molecules/intro_control_flow.html#next-command-conditional-on-failure",
    "title": "Control flow",
    "section": "Next command conditional on failure",
    "text": "Next command conditional on failure\nThe opposite of && is || which plays the role of a logical OR statement: the following command only gets executed if the first one fails.\nExample:\n[ -e file ] || echo File does not exist",
    "crumbs": [
      "Bash/Zsh",
      "<b><em>Getting started</em></b>",
      "Control flow"
    ]
  },
  {
    "objectID": "bash/molecules/intro_control_flow.html#conditional-executions",
    "href": "bash/molecules/intro_control_flow.html#conditional-executions",
    "title": "Control flow",
    "section": "Conditional executions",
    "text": "Conditional executions\nSections of scripts can be executed (or not) based on conditions thanks to if statements.\n\nSyntax\nif [ predicate1 ]\nthen\n    command1\n    command2\n    ...\nelif [ predicate2 ]\nthen\n    command3\n    command4\n    ...\nelse\n    command5\n    command6\n    ...\nfi\n\n\nExample\nLet’s create a file called check.sh with the following if statement:\nfor f in $@\ndo\n    if [ -e $f ]      # Make sure to have spaces around each bracket\n    then\n        echo $f exists\n    else\n        echo $f does not exist\n    fi\ndone\nNow, let’s make it executable:\nchmod u+x check.sh\nAnd let’s run this:\n./check.sh file1 file2 check.sh file3\n\n\nPredicates\nPredicates are expressions that return an exit status of 0 when they are evaluated if they are true and an exit status of 1 if they are false.\nHere are a few examples of predicates:\n[ $var == 'text' ] checks whether var is equal to 'text'.\n[ $var == number ] checks whether var is equal to number.\n[ -e name ] checks whether name exists.\n\nLet’s create a directory and a file:\nmkdir d1\ntouch f1\nls\nIf we test for the existence of our new directory, we do not get any output nor visible signal:\n[ -e d1 ]\nIn fact, we do get a response 0, which means “yes” or “success”. Since all is good, the shell doesn’t bother us with a signal.\nBy contrast, if we test for the existence of a non existing file or directory, we get a signal 1, which means “no” or “failure”:\n[ -e d2 ]\n\n[ -d name ] checks whether name is a directory.\n[ -f name ] checks whether name is a file.\n\n\nYour turn:\n\n\nTest whether d1 is a file\nTest whether d1 is a directory\nTest whether f1 is a file\nTest whether f1 is a directory",
    "crumbs": [
      "Bash/Zsh",
      "<b><em>Getting started</em></b>",
      "Control flow"
    ]
  },
  {
    "objectID": "bash/molecules/intro_control_flow.html#conditionally-repeated-executions",
    "href": "bash/molecules/intro_control_flow.html#conditionally-repeated-executions",
    "title": "Control flow",
    "section": "Conditionally repeated executions",
    "text": "Conditionally repeated executions\nSections of scripts can be repeated as long as a condition returns True thanks to while loops.\n\nSyntax\nThe syntax of a while loop in Bash is:\nwhile predicate\ndo\n    command1\n    command2\n    ...\ndone\nThe set of commands in the body of the while loop are executed as long as the predicate returns true.\nBe careful that while loop can lead to infinite loops. Such loops need to be manually interrupted (by pressing &lt;Ctrl+C&gt;).\n\nExample of infinite loop:\n\nwhile true\ndo\n    echo \"Press &lt;Ctrl+C&gt; to stop\"\n    sleep 1\ndone\nHere is a video of a previous version of this workshop.",
    "crumbs": [
      "Bash/Zsh",
      "<b><em>Getting started</em></b>",
      "Control flow"
    ]
  },
  {
    "objectID": "bash/molecules/intro_control_flow.html#repeated-executions",
    "href": "bash/molecules/intro_control_flow.html#repeated-executions",
    "title": "Control flow",
    "section": "Repeated executions",
    "text": "Repeated executions\nSections of scripts can be repeated for each element of a list thanks to for loops.\n\nSyntax\nThe general structure of a for loop is as follows:\nfor &lt;iterable&gt; in &lt;list&gt;\ndo\n    &lt;command1&gt;\n    &lt;command2&gt;\n    ...\ndone\n\n\nExample\nThe molecules directory contains the following .pdb files:\nls *.pdb\ncubane.pdb  ethane.pdb  methane.pdb  octane.pdb  pentane.pdb  propane.pdb\nWe want to rename these files by prepending “gas_” to their current names.\nWildcards don’t work here:\n\nmv *.pdb gas_*.pdb\n\nmv: target 'gas_propane.pdb': Not a directory\n\n\nThe solution is to use a for loop:\nfor file in *.pdb\ndo\n    mv $file gas_$file\ndone\nThis can also be written as a one-liner, although it is harder to read:\nfor file in *.pdb; do mv $file gas_$file; done\n\n\nYour turn:\n\nUsing what we learnt in the string manipulation section, how could you remove the gas_ prefix to all these files?\n\n\n\nCollections\nFor loops run a set of commands for each item of a collection. How do you create those collections?\n\nListing items one by one\nThe least efficient method is to list all the items one by one:\n\nExample:\n\nfor i in file1 file2 file3\ndo\n    echo $i\ndone\nfile1\nfile2\nfile3\n\n\nWildcards\nAs we have already seen, wildcards are very useful to build for loops.\n\n\nBrace expansion\nCollections can also be created with brace expansion.\n\nExamples:\n\n\necho {1,2,5}\n\n1 2 5\n\n\n\nMake sure not to add a space after the commas.\n\n\necho {list,of,strings}\n\nlist of strings\n\n\n\necho {file1,file2}.sh\n\nfile1.sh file2.sh\n\n\n\nls -l {ethane,methane,pentane}.pdb\n\nls: cannot access 'ethane.pdb': No such file or directory\nls: cannot access 'methane.pdb': No such file or directory\nls: cannot access 'pentane.pdb': No such file or directory\n\n\n\necho {1..5}\n\n1 2 3 4 5\n\n\n\necho {01..10}\n\n01 02 03 04 05 06 07 08 09 10\n\n\n\necho {r..v}\n\nr s t u v\n\n\n\necho {v..r}\n\nv u t s r\n\n\n\necho {a..e}{1..3}\n\na1 a2 a3 b1 b2 b3 c1 c2 c3 d1 d2 d3 e1 e2 e3\n\n\n\necho {a..c}{a..c}\n\naa ab ac ba bb bc ca cb cc\n\n\n\necho {1..5}.txt\n\n1.txt 2.txt 3.txt 4.txt 5.txt\n\n\n\necho file{3..6}.sh\n\nfile3.sh file4.sh file5.sh file6.sh\n\n\nBrace expansion can be used to create lists iterated over in loops, but also to apply commands to files or directories.\n\n\nSequences\nCollections can also be sequences:\n\nseq 1 2 10\n\n1\n3\n5\n7\n9\n\n\n\nHere, 1 is the start of the sequence, 10 is the end, and 2 is the step.\n\nSuch a sequence could be used in a loop this way:\n\nfor i in $(seq 1 2 10)\ndo\n    echo file$i.txt\ndone\n\nfile1.txt\nfile3.txt\nfile5.txt\nfile7.txt\nfile9.txt\n\n\n\n\n\nYour turn:\n\nIn a directory the command ls returns:\nfructose.dat  glucose.dat  sucrose.dat  maltose.txt\nWhat would be the output of the following loop?\nfor datafile in *.dat\ndo\n  cat $datafile &gt;&gt; sugar.dat\ndone\n\nAll of the text from fructose.dat, glucose.dat and sucrose.dat would be concatenated and saved to a file called sugar.dat.\nThe text from sucrose.dat will be saved to a file called sugar.dat.\nAll of the text from fructose.dat, glucose.dat, sucrose.dat, and maltose.txt would be concatenated and saved to a file called sugar.dat.\nAll of the text from fructose.dat, glucose.dat and sucrose.dat will be printed to the screen and saved into a file called sugar.dat.",
    "crumbs": [
      "Bash/Zsh",
      "<b><em>Getting started</em></b>",
      "Control flow"
    ]
  },
  {
    "objectID": "bash/intro_wildcards.html",
    "href": "bash/intro_wildcards.html",
    "title": "Wildcards",
    "section": "",
    "text": "Wildcards are a convenient way to select items matching patterns.\n\n\n\n\n\n\n\nData for this section\n\n\n\n\n\nFor this section, we will play with files created by The Carpentries.\nYou can download them into a zip file called bash.zip with:\nwget https://bit.ly/bashfile -O bash.zip\nYou can then unzip that file with:\nunzip bash.zip\nYou should now have a data-shell directory with a molecules subdirectory.\ncd into it:\ncd data-shell/molecules\n\n\n\nLet’s list the files in the molecules directory:\nls\ncubane.pdb  ethane.pdb  methane.pdb  octane.pdb  pentane.pdb  propane.pdb\nYou could do the same with:\nls *\nThe star expands to all files/directories matching any pattern. It is a wildcard.\nOf course, you can match more interesting patterns.\nFor instance, to list all files starting with the letter o, we can run:\nls o*\noctane.pdb\nTo list all files containing the letter o anywhere in their name, you can use:\nls *o*\noctane.pdb  propane.pdb\nThis saves a lot of typing and is a powerful way to apply a command to a subset of files/directories.\n\n\nYour turn:\n\nWildcards are often used to select all files with a certain extension.\nLet’s create 3 new files:\ntouch file1.txt file2.txt file3.md\nHow would you list all files with the .txt extension and only those?"
  },
  {
    "objectID": "bash/intro_transfer.html",
    "href": "bash/intro_transfer.html",
    "title": "Transferring files",
    "section": "",
    "text": "If you want to use the Alliance clusters to run some of your heavy computations, you will have to move files back and forth between your machine and the clusters.\nThis section covers various ways to do this.",
    "crumbs": [
      "Bash/Zsh",
      "<b><em>Getting started</em></b>",
      "Transferring files"
    ]
  },
  {
    "objectID": "bash/intro_transfer.html#remote-copies-with-scp",
    "href": "bash/intro_transfer.html#remote-copies-with-scp",
    "title": "Transferring files",
    "section": "Remote copies with scp",
    "text": "Remote copies with scp\nSecure copy protocol (SCP) allows to copy files over the Secure Shell Protocol (SSH) with the scp utility. scp follows a syntax similar to that of the cp command.\nNote that you need to run it from your local machines (not from the cluster).\n\nCopy from your machine to the cluster\n# Copy a local file to your home directory on the cluster\nscp /local/path/file userxx@spring.c3.ca:\n# Copy a local file to some path on the cluster\nscp /local/path/file userxx@spring.c3.ca:/remote/path\n\n\nCopy from the cluster to your machine\n# Copy a file from the cluster to some path on your machine\nscp userxx@spring.c3.ca:/remote/path/file /local/path\n# Copy a file from the cluster to your current location on your machine\nscp userxx@spring.c3.ca:/remote/path/file .\nYou can also use wildcards to transfer multiple files:\n# Copy all the Bash scripts from your cluster home dir to some local path\nscp userxx@spring.c3.ca:*.sh /local/path\n\n\nCopying directories\nTo copy a directory, you need to add the -r (recursive) flag:\nscp -r /local/path/folder userxx@spring.c3.ca:/remote/path\n\n\nCopying for Windows users\nMobaXterm users (on Windows) can copy files by dragging them between the local and remote machines in the GUI. Alternatively, they can use the download and upload buttons.\n\n\nYour turn:\n\nCopy a file from your local computer to your home directory in the training cluster.\n\nHere is a video of a previous version of this workshop.",
    "crumbs": [
      "Bash/Zsh",
      "<b><em>Getting started</em></b>",
      "Transferring files"
    ]
  },
  {
    "objectID": "bash/intro_transfer.html#interactive-transfers-with-sftp",
    "href": "bash/intro_transfer.html#interactive-transfers-with-sftp",
    "title": "Transferring files",
    "section": "Interactive transfers with sftp",
    "text": "Interactive transfers with sftp\nThe Secure File Transfer Protocol (SFTP) is more sophisticated and allows additional operations. The sftp command provided by OpenSSH and other packages launches an SFTP client:\nsftp userxx@spring.c3.ca\n\nLook at your prompt: your usual Bash/Zsh prompt has been replaced with sftp&gt;.\n\nFrom this prompt, you can access a number of SFTP commands. Type help for a list:\nsftp&gt; help\nAvailable commands:\nbye                                Quit sftp\ncd path                            Change remote directory to 'path'\nchgrp [-h] grp path                Change group of file 'path' to 'grp'\nchmod [-h] mode path               Change permissions of file 'path' to 'mode'\nchown [-h] own path                Change owner of file 'path' to 'own'\ncopy oldpath newpath               Copy remote file\ncp oldpath newpath                 Copy remote file\ndf [-hi] [path]                    Display statistics for current directory or\n                                   filesystem containing 'path'\nexit                               Quit sftp\nget [-afpR] remote [local]         Download file\nhelp                               Display this help text\nlcd path                           Change local directory to 'path'\nlls [ls-options [path]]            Display local directory listing\nlmkdir path                        Create local directory\nln [-s] oldpath newpath            Link remote file (-s for symlink)\nlpwd                               Print local working directory\nls [-1afhlnrSt] [path]             Display remote directory listing\nlumask umask                       Set local umask to 'umask'\nmkdir path                         Create remote directory\nprogress                           Toggle display of progress meter\nput [-afpR] local [remote]         Upload file\npwd                                Display remote working directory\nquit                               Quit sftp\nreget [-fpR] remote [local]        Resume download file\nrename oldpath newpath             Rename remote file\nreput [-fpR] local [remote]        Resume upload file\nrm path                            Delete remote file\nrmdir path                         Remove remote directory\nsymlink oldpath newpath            Symlink remote file\nversion                            Show SFTP version\n!command                           Execute 'command' in local shell\n!                                  Escape to local shell\n?                                  Synonym for help\nAs this list shows, you have access to a number of classic Unix command such as cd, pwd, ls, etc. These commands will be executed on the remote machine.\nIn addition, there are a number of commands of the form l&lt;command&gt;. “l” stands for “local”.\nThese commands will be executed on your local machine.\nFor instance, ls will list the files in your current directory in the remote machine while lls (“local ls”) will list the files in your current directory on your computer.\nThis means that you are now able to navigate two file systems at once: your local machine and the remote machine.\n\nHere are a few examples:\n\nsftp&gt; pwd              # print remote working directory\nsftp&gt; lpwd             # print local working directory\nsftp&gt; ls               # list files in remote working directory\nsftp&gt; lls              # list files in local working directory\nsftp&gt; cd               # change the remote directory\nsftp&gt; lcd              # change the local directory\nsftp&gt; put local_file   # upload a file\nsftp&gt; get remote_file  # download a file\n\nCopying directories\nTo upload/download directories, you first need to create them in the destination, then copy the content with the -r (recursive) flag.\n\nIf you have a local directory called dir and you want to copy it to the cluster you need to run:\n\nsftp&gt; mkdir dir    # First create the directory\nsftp&gt; put -r dir   # Then copy the content\nTo terminate the session, press &lt;Ctrl+D&gt;.\n\n\nYour turn:\n\nIn an SFTP session:\n\nList the content of projects (projects is in your home on the training cluster).\nCopy a file from the training cluster to your local computer.",
    "crumbs": [
      "Bash/Zsh",
      "<b><em>Getting started</em></b>",
      "Transferring files"
    ]
  },
  {
    "objectID": "bash/intro_transfer.html#syncing",
    "href": "bash/intro_transfer.html#syncing",
    "title": "Transferring files",
    "section": "Syncing",
    "text": "Syncing\nIf, instead of an occasional copying of files between your machine and the cluster, you want to keep a directory in sync between both machines, you might want to use rsync instead. You can look at the Alliance wiki page on rsync for complete instructions.",
    "crumbs": [
      "Bash/Zsh",
      "<b><em>Getting started</em></b>",
      "Transferring files"
    ]
  },
  {
    "objectID": "bash/intro_transfer.html#heavy-transfers",
    "href": "bash/intro_transfer.html#heavy-transfers",
    "title": "Transferring files",
    "section": "Heavy transfers",
    "text": "Heavy transfers\nWhile the methods covered above work very well for limited amounts of data, if you need to make large transfers, you should use globus instead, following the instructions in the Alliance wiki page on this service.",
    "crumbs": [
      "Bash/Zsh",
      "<b><em>Getting started</em></b>",
      "Transferring files"
    ]
  },
  {
    "objectID": "bash/intro_transfer.html#windows-line-endings",
    "href": "bash/intro_transfer.html#windows-line-endings",
    "title": "Transferring files",
    "section": "Windows line endings",
    "text": "Windows line endings\nOn modern Mac operating systems and on Linux, lines in files are terminated with a newline (\\n). On Windows, they are terminated with a carriage return + newline (\\r\\n).\nWhen you transfer files between Windows and Linux (the cluster uses Linux), this creates a mismatch. Most modern software handle this correctly, but you may occasionally run into problems.\nThe solution is to convert a file from Windows encoding to Unix encoding with:\ndos2unix file\nTo convert a file back to Windows encoding, run:\nunix2dos file",
    "crumbs": [
      "Bash/Zsh",
      "<b><em>Getting started</em></b>",
      "Transferring files"
    ]
  },
  {
    "objectID": "bash/intro_script.html",
    "href": "bash/intro_script.html",
    "title": "Writing scripts",
    "section": "",
    "text": "There are series of commands that you need to run regularly. Instead of having to type them each time, you can write them in a text file (called a script) with a .sh extension and execute that file whenever you want to run that set of commands. This is a great way to automate work.\nThis section covers scripts syntax and execution."
  },
  {
    "objectID": "bash/intro_script.html#writing-and-executing-scripts",
    "href": "bash/intro_script.html#writing-and-executing-scripts",
    "title": "Writing scripts",
    "section": "Writing and executing scripts",
    "text": "Writing and executing scripts\n\nScripts as arguments to bash\nA shell script is simply a text file. You can create it with a text editor such as nano which is installed on most systems.\nLet’s try to create one that we will call test.sh:\nnano test.sh\nIn the file, write the command: echo This is my first script.\nThis is the content of our test.sh file:\n\n\ntest.sh\n\necho This is my first script\n\nNow, how do we run this?\nWe simply pass it as an argument to the bash command:\nbash test.sh\nThis is my first script\nAnd it worked!\n\n\nShebang\nThere is another way to write and execute scripts: we can use a shebang.\nA shebang consists of the characters #! followed by the path of an executable. Here, the executable we want is bash and its path is /bin/bash.\nSo our script becomes:\n\n\ntest.sh\n\n#!/bin/bash\n\necho This is my first script.\n\nNow, the cool thing about this is that we don’t need to pass the script as an argument of the bash command anymore since the information that this should be executed by Bash is already written in the shebang. Instead, we can execute it with ./test.sh.\nBut there is a little twist:\n./test.sh\nbash: ./test.sh: Permission denied\nWe first need to make the file executable by changing its permissions.\n\n\nUnix permissions\nUnix systems such as Linux use POSIX permissions.\nTo add an executable permission to a file, you need to run:\nchmod u+x test.sh\nNow that our script is executable, we can run:\n./test.sh\nThis is my first script\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere and here are two videos of a previous version of this workshop."
  },
  {
    "objectID": "bash/intro_script.html#comments",
    "href": "bash/intro_script.html#comments",
    "title": "Writing scripts",
    "section": "Comments",
    "text": "Comments\nAnything to the right of the symbol # is ignored by the interpreter and is for human consumption only.\n# You can write full-line comments\n\npwd       # You can also write comments after a command\nComments are used to document scripts. DO USE THEM: future you will thank you."
  },
  {
    "objectID": "bash/intro_regexp.html",
    "href": "bash/intro_regexp.html",
    "title": "Regular expressions",
    "section": "",
    "text": "Regular expressions (regex or regexp) are a more powerful system than globing patterns to look for matches of a particular pattern. They are extremely useful and implemented in most programming and scripting languages.\n\nWe do not have time to cover regexp in this course, but there is an excellent site that covers them in detail in a clear fashion.\nThere are many sites such as this that allow you to test your regexps.\nIf you find the syntax daunting, don’t despair: any LLM (large language model) will tell you what syntax to use. We offered a webinar on this in 2023 with an early version of ChatGPT:",
    "crumbs": [
      "Bash/Zsh",
      "<b><em>Getting started</em></b>",
      "Regular expressions"
    ]
  },
  {
    "objectID": "bash/intro_intro.html",
    "href": "bash/intro_intro.html",
    "title": "Introduction",
    "section": "",
    "text": "What are Unix shells?\nDo I need to use them?\nWhich one should I use?\nThis section answers these questions and covers how we are going to run a shell for this course.",
    "crumbs": [
      "Bash/Zsh",
      "<b><em>Getting started</em></b>",
      "Introduction"
    ]
  },
  {
    "objectID": "bash/intro_intro.html#unix-shells",
    "href": "bash/intro_intro.html#unix-shells",
    "title": "Introduction",
    "section": "Unix shells",
    "text": "Unix shells\nUnix shells are command line interpreters for Unix-like operating systems1: the user enters commands as text—interactively in a terminal or in scripts—and the shell passes them to the operating system.\nIt is thus a way to give instructions to the machine through text instead of using a graphical user interface (GUI).\n\nTypes of Unix shells\nBash, the Bourne Again SHell, released in 1989 as part of the GNU Project, is the default Unix shell on most systems, including the Alliance clusters. The executable to use it is bash. It replaces the initial Bourne shell—executable: sh. We will mostly learn Bash in this course.\nA newer and very popular shell, almost fully backward compatible with Bash and extending its capabilities, is Z shell (or Zsh)—executable: zsh. macOS recently changed its default shell from Bash to Zsh. We will talk about it at the end of this course.\nBoth Bash and Zsh are POSIX compliant, meaning that they respect the Portable Operating System Interface standards, making them compatible between operating systems.\nThere are several other Unix shells, some of which are also POSIX compliant and with a very similar syntax such as KornShell (ksh), others with a more different syntax and not POSIX compliant such as fish or C shell (csh).\n\n\nWhy use a shell?\nWhile automating GUI operations is really difficult, it is easy to rerun a script (a file with a number of commands). It is also very easy to apply the same command to any number of files. Unix shells thus allow the creation of reproducible workflows and the automation of repetitive tasks.\n\nImagine you had 1000 files in a directory and you wanted to rename them all.\nUsing Windows Explorer or macOS Finder, you could right click on every file one by one to rename it, but it would take hours. Using a Unix shell, this is done by a very simple command and takes an instant.\n\nShells are particularly powerful to launch tools, modify files, search text, or combine commands.\n\nBecause shells are powerful, you can easily make consequential mistakes (e.g. deleting a lot of files). For this reason, it is a good idea to make backups of your data before you start experimenting with novel shell commands (it is a very good idea to make frequent backups of your data anyway!).\n\nFinally, shells allow to access remote machines and HPC clusters.",
    "crumbs": [
      "Bash/Zsh",
      "<b><em>Getting started</em></b>",
      "Introduction"
    ]
  },
  {
    "objectID": "bash/intro_intro.html#running-bash",
    "href": "bash/intro_intro.html#running-bash",
    "title": "Introduction",
    "section": "Running Bash",
    "text": "Running Bash\nSince Bash is a Unix shell, you need a Unix or Unix-like operating system. This means that people on Linux or macOS can use Bash directly on their machine.\nFor Windows users, there are various options:\n\nusing Windows Subsystem for Linux (WSL),\nusing a Bash emulator (e.g. Git BASH), but those only have a subset of the usual Bash utilities,\nusing a Unix-like environment for Windows (e.g. Cygwin),\nusing a Unix Virtual machine,\naccessing a remote Unix machine.\n\n\nHow we will use Bash today\nToday, we will connect to a remote HPC cluster (supercomputer) via SSH (secure shell). HPC systems always run Linux.\n\nNote that this cluster is virtual and temporary. It will only be available for the duration of this course.\n\nThose on Linux or macOS can use Bash directly on their machine if they prefer, but using our remote system will give you an opportunity to practice using SSH—something you will have to do if you ever want to use the Alliance supercomputers.\n\nOn macOS, the default is now Zsh (you can see that by launching the application called “Terminal” and typing echo $SHELL followed by the &lt;enter&gt; key), but Zsh is almost fully compatible with Bash commands, so it is fine to use it instead. If you really want to use Bash, simply launch it by typing in “Terminal”: bash, then pressing the &lt;enter&gt; key.",
    "crumbs": [
      "Bash/Zsh",
      "<b><em>Getting started</em></b>",
      "Introduction"
    ]
  },
  {
    "objectID": "bash/intro_intro.html#remote-connection-to-the-cluster-via-ssh",
    "href": "bash/intro_intro.html#remote-connection-to-the-cluster-via-ssh",
    "title": "Introduction",
    "section": "Remote connection to the cluster via SSH",
    "text": "Remote connection to the cluster via SSH\n\nStep one: get a username and the password\nWe will give you a link to an etherpad during the workshop. Add your name next to a free username to claim it.\nWe will also give you the password for our training cluster.\n\n\nStep two: run the ssh command\n\n •  Linux and macOS users\nLinux users:   open the terminal emulator of your choice.\nmacOS users:   open “Terminal”.\nThen type:\nssh userxx@hostname\n\n\nReplace userxx by your username (e.g. user09).\nReplace hostname by the hostname we will give you the day of the workshop.\n\n\n\n\n •  Windows users\nWe suggest using the free version of MobaXterm. MobaXterm comes with a terminal emulator and a GUI interface for SSH sessions.\nOpen MobaXterm, click on “Session”, then “SSH”, and fill in the Remote host name and your username.\n\nHere is a live demo.\n\n\n\n\nStep three: enter the password\nWhen prompted, enter the password we gave you.\nYou will not see any character as you type the password: this is called blind typing and is a Linux safety feature. It can be unsettling at first not to get any feed-back while typing as it really looks like it is not working. Type slowly and make sure not to make typos.\nYou will be asked whether you are sure that you want to continue connecting. Answer “yes”.\nYou are now logged in and your prompt should look like the following (with your actual username):\n[userxx@login1 ~]$\n\n\nTroubleshooting\nProblems logging in are almost always due to typos. If you cannot log in, retry slowly, entering your password carefully.",
    "crumbs": [
      "Bash/Zsh",
      "<b><em>Getting started</em></b>",
      "Introduction"
    ]
  },
  {
    "objectID": "bash/intro_intro.html#footnotes",
    "href": "bash/intro_intro.html#footnotes",
    "title": "Introduction",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nUnix-like systems include Linux, macOS, and a few others.↩︎",
    "crumbs": [
      "Bash/Zsh",
      "<b><em>Getting started</em></b>",
      "Introduction"
    ]
  },
  {
    "objectID": "bash/intro_find.html",
    "href": "bash/intro_find.html",
    "title": "Finding files",
    "section": "",
    "text": "For this section, we will play with files created by The Carpentries.\nYou can download them into a zip file called bash.zip with:\nwget http://bit.ly/bashfile -O bash.zip\nYou can then unzip that file with:\nunzip bash.zip\nFinally, you can delete the zip file:\nrm bash.zip\nYou should now have a data-shell directory with a molecules subdirectory.\ncd into it:\ncd data-shell/molecules"
  },
  {
    "objectID": "bash/intro_find.html#data-for-this-section",
    "href": "bash/intro_find.html#data-for-this-section",
    "title": "Finding files",
    "section": "",
    "text": "For this section, we will play with files created by The Carpentries.\nYou can download them into a zip file called bash.zip with:\nwget http://bit.ly/bashfile -O bash.zip\nYou can then unzip that file with:\nunzip bash.zip\nFinally, you can delete the zip file:\nrm bash.zip\nYou should now have a data-shell directory with a molecules subdirectory.\ncd into it:\ncd data-shell/molecules"
  },
  {
    "objectID": "bash/intro_find.html#command-find",
    "href": "bash/intro_find.html#command-find",
    "title": "Finding files",
    "section": "Command find",
    "text": "Command find\nSearch for files inside the current working directory:\nfind . -type f\n./methane.pdb\n./pentane.pdb\n./sorted.txt\n./propane.pdb\n./lengths.txt\n./cubane.pdb\n./ethane.pdb\n./octane.pdb\nfind . -type d will instead search for directories inside the current working directory.\nHere are other examples:\nfind . -maxdepth 1 -type f     # depth 1 is the current directory\nfind . -mindepth 2 -type f     # current directory and one level down\nfind . -name haiku.txt      # finds specific file\nls data       # shows one.txt two.txt\nfind . -name *.txt      # still finds one file -- why? answer: expands *.txt to haiku.txt\nfind . -name '*.txt'    # finds all three files -- good!\nLet’s wrap the last command into $()—called command substitution—as if it were a variable:\necho $(find . -name '*.txt')   # will print ./data/one.txt ./data/two.txt ./haiku.txt\nls -l $(find . -name '*.txt')   # will expand to ls -l ./data/one.txt ./data/two.txt ./haiku.txt\nwc -l $(find . -name '*.txt')   # will expand to wc -l ./data/one.txt ./data/two.txt ./haiku.txt\ngrep elegant $(find . -name '*.txt')   # will look for 'elegant' inside all *.txt files\n\n\nYour turn:\n\ngrep’s -v flag inverts pattern matching, so that only lines that do not match the pattern are printed.\nGiven that, which of the following commands will find all files in /data whose names end in ose.dat (e.g. sucrose.dat or maltose.dat), but do not contain the word temp?\n\nfind /data -name '*.dat' | grep ose | grep -v temp\nfind /data -name ose.dat | grep -v temp\ngrep -v temp $(find /data -name '*ose.dat')\nNone of the above\n\n\nHere is a video of a previous version of this workshop."
  },
  {
    "objectID": "bash/intro_find.html#running-a-command-on-the-results-of-find",
    "href": "bash/intro_find.html#running-a-command-on-the-results-of-find",
    "title": "Finding files",
    "section": "Running a command on the results of find",
    "text": "Running a command on the results of find\nLet’s say that you want to run a command on each of the files in the output of find. You can always do something using command substitution like this:\nfor f in $(find . -name \"*.txt\")\ndo\n    command on $f\ndone\nAlternatively, you can make it a one-liner:\nfind . -name \"*.txt\" -exec command {} \\;\nAnother—perhaps more elegant—one-line alternative is to use xargs. In its simplest usage, xargs command lets you construct a list of arguments:\nfind . -name \"*.txt\"                   # returns multiple lines\nfind . -name \"*.txt\" | xargs           # use those lines to construct a list\nfind . -name \"*.txt\" | xargs command   # pass this list as arguments to `command`\ncommand $(find . -name \"*.txt\")        # command substitution, achieving the same result (this is riskier!)\ncommand `(find . -name \"*.txt\")`       # alternative syntax for command substitution\nIn these examples, xargs achieves the same result as command substitution, but it is safer in terms of memory usage and the length of lists you can pass.\nWhen would you need to use this? A good example is with the command grep. grep takes a search stream (and not a list of files) as its standard input:\ncat filename | grep pattern\nTo pass a list of files to grep, you can use xargs that takes that list from its standard input and converts it into a list of arguments that is then passed to grep:\nfind . -name \"*.txt\" | xargs grep pattern   # search for `pattern` inside all those files (`grep` does not take a list of files as standard input)\n\n\nHere is a video of a previous version of this workshop."
  },
  {
    "objectID": "bash/intro_control_flow.html",
    "href": "bash/intro_control_flow.html",
    "title": "Control flow",
    "section": "",
    "text": "By default, scripts get executed linearly from top to bottom. Often however, you will want to control what gets executed when.\nThis section covers various ways to control the flow of execution through a script."
  },
  {
    "objectID": "bash/intro_control_flow.html#next-command-conditional-on-success",
    "href": "bash/intro_control_flow.html#next-command-conditional-on-success",
    "title": "Control flow",
    "section": "Next command conditional on success",
    "text": "Next command conditional on success\nCommands can be limited to running only if the previous commands ran successfully thanks to &&.\n\nExample:\n\nLook at the following commands:\nunzip bash.zip\nrm bash.zip\nThis is equivalent to:\nunzip bash.zip;\nrm bash.zip\nand to:\nunzip bash.zip; rm bash.zip\nThis is what we did to get the data for the past few sessions.\nIn both cases, both commands will try to run. Now, if for some reason, the unzipping fails, we have deleted the zip file and we have to re-download it. Not a big deal here, but in some situations, executing a command if the one before fails can be a real bummer.\nTo prevent this, we can use the double-ampersand (&&) operator, which plays the role of a logical AND statement:\nunzip bash.zip &&\nrm bash.zip\nThis is equivalent to:\nunzip bash.zip && rm bash.zip\nIf the unzipping works (if it returns a zero exit status), then the Zip file gets deleted. If however, the unzipping fails (if it returns a non-zero exit status), the script aborts and we haven’t lost our Zip file."
  },
  {
    "objectID": "bash/intro_control_flow.html#next-command-conditional-on-failure",
    "href": "bash/intro_control_flow.html#next-command-conditional-on-failure",
    "title": "Control flow",
    "section": "Next command conditional on failure",
    "text": "Next command conditional on failure\nThe opposite of && is || which plays the role of a logical OR statement: the following command only gets executed if the first one fails.\nExample:\n[ -e file ] || echo File does not exist"
  },
  {
    "objectID": "bash/intro_control_flow.html#conditional-executions",
    "href": "bash/intro_control_flow.html#conditional-executions",
    "title": "Control flow",
    "section": "Conditional executions",
    "text": "Conditional executions\nSections of scripts can be executed (or not) based on conditions thanks to if statements.\n\nSyntax\nif [ predicate1 ]\nthen\n    command1\n    command2\n    ...\nelif [ predicate2 ]\nthen\n    command3\n    command4\n    ...\nelse\n    command5\n    command6\n    ...\nfi\n\n\nExample\nLet’s create a file called check.sh with the following if statement:\nfor f in $@\ndo\n    if [ -e $f ]      # Make sure to have spaces around each bracket\n    then\n        echo $f exists\n    else\n        echo $f does not exist\n    fi\ndone\nNow, let’s make it executable:\nchmod u+x check.sh\nAnd let’s run this:\n./check.sh file1 file2 check.sh file3\n\n\nPredicates\nPredicates are expressions that return an exit status of 0 when they are evaluated if they are true and an exit status of 1 if they are false.\nHere are a few examples of predicates:\n[ $var == 'text' ] checks whether var is equal to 'text'.\n[ $var == number ] checks whether var is equal to number.\n[ -e name ] checks whether name exists.\n\nLet’s create a directory and a file:\nmkdir d1\ntouch f1\nls\nIf we test for the existence of our new directory, we do not get any output nor visible signal:\n[ -e d1 ]\nIn fact, we do get a response 0, which means “yes” or “success”. Since all is good, the shell doesn’t bother us with a signal.\nBy contrast, if we test for the existence of a non existing file or directory, we get a signal 1, which means “no” or “failure”:\n[ -e d2 ]\n\n[ -d name ] checks whether name is a directory.\n[ -f name ] checks whether name is a file.\n\n\nYour turn:\n\n\nTest whether d1 is a file\nTest whether d1 is a directory\nTest whether f1 is a file\nTest whether f1 is a directory"
  },
  {
    "objectID": "bash/intro_control_flow.html#conditionally-repeated-executions",
    "href": "bash/intro_control_flow.html#conditionally-repeated-executions",
    "title": "Control flow",
    "section": "Conditionally repeated executions",
    "text": "Conditionally repeated executions\nSections of scripts can be repeated as long as a condition returns True thanks to while loops.\n\nSyntax\nThe syntax of a while loop in Bash is:\nwhile predicate\ndo\n    command1\n    command2\n    ...\ndone\nThe set of commands in the body of the while loop are executed as long as the predicate returns true.\nBe careful that while loop can lead to infinite loops. Such loops need to be manually interrupted (by pressing &lt;Ctrl+C&gt;).\n\nExample of infinite loop:\n\nwhile true\ndo\n    echo \"Press &lt;Ctrl+C&gt; to stop\"\n    sleep 1\ndone\nHere is a video of a previous version of this workshop."
  },
  {
    "objectID": "bash/intro_control_flow.html#repeated-executions",
    "href": "bash/intro_control_flow.html#repeated-executions",
    "title": "Control flow",
    "section": "Repeated executions",
    "text": "Repeated executions\nSections of scripts can be repeated for each element of a list thanks to for loops.\n\nSyntax\nThe general structure of a for loop is as follows:\nfor &lt;iterable&gt; in &lt;list&gt;\ndo\n    &lt;command1&gt;\n    &lt;command2&gt;\n    ...\ndone\n\n\nExample\nThe molecules directory contains the following .pdb files:\nls *.pdb\ncubane.pdb  ethane.pdb  methane.pdb  octane.pdb  pentane.pdb  propane.pdb\nWe want to rename these files by prepending “gas_” to their current names.\nWildcards don’t work here:\n\nmv *.pdb gas_*.pdb\n\nmv: cannot stat '*.pdb': No such file or directory\n\n\nThe solution is to use a for loop:\nfor file in *.pdb\ndo\n    mv $file gas_$file\ndone\nThis can also be written as a one-liner, although it is harder to read:\nfor file in *.pdb; do mv $file gas_$file; done\n\n\nYour turn:\n\nUsing what we learnt in the string manipulation section, how could you remove the gas_ prefix to all these files?\n\n\n\nCollections\nFor loops run a set of commands for each item of a collection. How do you create those collections?\n\nListing items one by one\nThe least efficient method is to list all the items one by one:\n\nExample:\n\nfor i in file1 file2 file3\ndo\n    echo $i\ndone\nfile1\nfile2\nfile3\n\n\nWildcards\nAs we have already seen, wildcards are very useful to build for loops.\n\n\nBrace expansion\nCollections can also be created with brace expansion.\n\nExamples:\n\n\necho {1,2,5}\n\n1 2 5\n\n\n\nMake sure not to add a space after the commas.\n\n\necho {list,of,strings}\n\nlist of strings\n\n\n\necho {file1,file2}.sh\n\nfile1.sh file2.sh\n\n\n\nls -l {ethane,methane,pentane}.pdb\n\nls: cannot access 'ethane.pdb': No such file or directory\nls: cannot access 'methane.pdb': No such file or directory\nls: cannot access 'pentane.pdb': No such file or directory\n\n\n\necho {1..5}\n\n1 2 3 4 5\n\n\n\necho {01..10}\n\n01 02 03 04 05 06 07 08 09 10\n\n\n\necho {r..v}\n\nr s t u v\n\n\n\necho {v..r}\n\nv u t s r\n\n\n\necho {a..e}{1..3}\n\na1 a2 a3 b1 b2 b3 c1 c2 c3 d1 d2 d3 e1 e2 e3\n\n\n\necho {a..c}{a..c}\n\naa ab ac ba bb bc ca cb cc\n\n\n\necho {1..5}.txt\n\n1.txt 2.txt 3.txt 4.txt 5.txt\n\n\n\necho file{3..6}.sh\n\nfile3.sh file4.sh file5.sh file6.sh\n\n\nBrace expansion can be used to create lists iterated over in loops, but also to apply commands to files or directories.\n\n\nSequences\nCollections can also be sequences:\n\nseq 1 2 10\n\n1\n3\n5\n7\n9\n\n\n\nHere, 1 is the start of the sequence, 10 is the end, and 2 is the step.\n\nSuch a sequence could be used in a loop this way:\n\nfor i in $(seq 1 2 10)\ndo\n    echo file$i.txt\ndone\n\nfile1.txt\nfile3.txt\nfile5.txt\nfile7.txt\nfile9.txt\n\n\n\n\n\nYour turn:\n\nIn a directory the command ls returns:\nfructose.dat  glucose.dat  sucrose.dat  maltose.txt\nWhat would be the output of the following loop?\nfor datafile in *.dat\ndo\n  cat $datafile &gt;&gt; sugar.dat\ndone\n\nAll of the text from fructose.dat, glucose.dat and sucrose.dat would be concatenated and saved to a file called sugar.dat.\nThe text from sucrose.dat will be saved to a file called sugar.dat.\nAll of the text from fructose.dat, glucose.dat, sucrose.dat, and maltose.txt would be concatenated and saved to a file called sugar.dat.\nAll of the text from fructose.dat, glucose.dat and sucrose.dat will be printed to the screen and saved into a file called sugar.dat."
  },
  {
    "objectID": "bash/intro_aliases.html",
    "href": "bash/intro_aliases.html",
    "title": "Aliases",
    "section": "",
    "text": "Aliases are a convenient way to assign a custom command to a name. You can use new names or re-assign existing command names.\n\nalias myip=\"ip -json route get 8.8.8.8 | jq -r '.[].prefsrc'\"\nalias ls='ls -F'\nYou can retrieve the definition of an alias by running the alias command without argument. To remove an alias, use unalias.\nYou can use the non-aliased version of a command with \\ (e.g. \\ls).",
    "crumbs": [
      "Bash/Zsh",
      "<b><em>Getting started</em></b>",
      "Aliases"
    ]
  },
  {
    "objectID": "ai/ws_hss_intro_slides.html#ml-allows-to-achieve-previously-impossible-tasks",
    "href": "ai/ws_hss_intro_slides.html#ml-allows-to-achieve-previously-impossible-tasks",
    "title": "Introduction to machine learning for the humanities",
    "section": "ML allows to achieve previously impossible tasks",
    "text": "ML allows to achieve previously impossible tasks\n\nLet’s take the example of image recognition:\n\nIn typical computing, a programmer writes code that gives a computer detailed instructions of what to do\nCoding all the possible ways—pixel by pixel—that an image can represent, say, a dog is an impossibly large task: there are many breeds of dogs, the image can be a picture, a blurred picture, a drawing, a cartoon, the dog can be in all sorts of positions, wearing clothes, etc.\nThere just aren’t enough resources to make the traditional programming approach able to create a computer program that can identify a dog in images\nBy feeding a very large number of dog images to a neural network however, we can train that network to recognize dogs in images that it has never seen (without explicitly programming how it does this!)"
  },
  {
    "objectID": "ai/ws_hss_intro_slides.html#old-concept-new-computing-power",
    "href": "ai/ws_hss_intro_slides.html#old-concept-new-computing-power",
    "title": "Introduction to machine learning for the humanities",
    "section": "Old concept … new computing power",
    "text": "Old concept … new computing power\nThe concept is everything but new: Arthur Samuel came up with it in 1949 and built a self-learning Checkers-playing program in 1959\n\n\nMachine learning consists of feeding vast amounts of data to algorithms to strengthen pathways, so the excitement for the approach became somewhat dormant due to the lack of computing power and the lack of training data at the time\nThe advent of powerful computers, GPUs, and massive amounts of data have brought the old concept to the forefront\n\n\n\n\n\n\nFrom xkcd.com"
  },
  {
    "objectID": "ai/ws_hss_intro_slides.html#supervised-learning",
    "href": "ai/ws_hss_intro_slides.html#supervised-learning",
    "title": "Introduction to machine learning for the humanities",
    "section": "Supervised learning",
    "text": "Supervised learning\n\nRegression is a form of supervised learning with continuous outputs\nClassification is supervised learning with discrete outputs\n\nSupervised learning uses training data in the form of example input/output pairs\nGoal\nFind the relationship between inputs and outputs"
  },
  {
    "objectID": "ai/ws_hss_intro_slides.html#unsupervised-learning",
    "href": "ai/ws_hss_intro_slides.html#unsupervised-learning",
    "title": "Introduction to machine learning for the humanities",
    "section": "Unsupervised learning",
    "text": "Unsupervised learning\nClustering, social network analysis, market segmentation, PCA … are all forms of unsupervised learning\nUnsupervised learning uses unlabelled data\nGoal\nFind structure within the data"
  },
  {
    "objectID": "ai/ws_hss_intro_slides.html#reinforcement-learning",
    "href": "ai/ws_hss_intro_slides.html#reinforcement-learning",
    "title": "Introduction to machine learning for the humanities",
    "section": "Reinforcement learning",
    "text": "Reinforcement learning\nThe algorithm explores by performing random actions and these actions are rewarded or punished (bonus points or penalties)\nThis is how algorithms learn to play games"
  },
  {
    "objectID": "ai/ws_hss_intro_slides.html#decide-on-an-architecture",
    "href": "ai/ws_hss_intro_slides.html#decide-on-an-architecture",
    "title": "Introduction to machine learning for the humanities",
    "section": "Decide on an architecture",
    "text": "Decide on an architecture\n\nThe architecture won’t change during training\nThe type of architecture you choose (e.g. CNN, Transformer) depends on the type of data you have (e.g. vision, textual). The depth and breadth of your network depend on the amount of data and computing resource you have"
  },
  {
    "objectID": "ai/ws_hss_intro_slides.html#set-some-initial-parameters",
    "href": "ai/ws_hss_intro_slides.html#set-some-initial-parameters",
    "title": "Introduction to machine learning for the humanities",
    "section": "Set some initial parameters",
    "text": "Set some initial parameters\n\nYou can initialize them randomly or get much better ones through transfer learning\nWhile the parameters are also part of the model, those will change during training"
  },
  {
    "objectID": "ai/ws_hss_intro_slides.html#get-some-labelled-data",
    "href": "ai/ws_hss_intro_slides.html#get-some-labelled-data",
    "title": "Introduction to machine learning for the humanities",
    "section": "Get some labelled data",
    "text": "Get some labelled data\n\nWhen we say that we need a lot of data for machine learning, we mean “lots of labelled data” as this is what gets used for training models"
  },
  {
    "objectID": "ai/ws_hss_intro_slides.html#make-sure-to-keep-some-data-for-testing",
    "href": "ai/ws_hss_intro_slides.html#make-sure-to-keep-some-data-for-testing",
    "title": "Introduction to machine learning for the humanities",
    "section": "Make sure to keep some data for testing",
    "text": "Make sure to keep some data for testing\n\nThose data won’t be used for training the model. Often people keep around 20% of their data for testing"
  },
  {
    "objectID": "ai/ws_hss_intro_slides.html#pass-data-and-parameters-through-the-architecture",
    "href": "ai/ws_hss_intro_slides.html#pass-data-and-parameters-through-the-architecture",
    "title": "Introduction to machine learning for the humanities",
    "section": "Pass data and parameters through the architecture",
    "text": "Pass data and parameters through the architecture\n\nThe train data are the inputs and the process of calculating the outputs is the forward pass"
  },
  {
    "objectID": "ai/ws_hss_intro_slides.html#the-outputs-of-the-model-are-predictions",
    "href": "ai/ws_hss_intro_slides.html#the-outputs-of-the-model-are-predictions",
    "title": "Introduction to machine learning for the humanities",
    "section": "The outputs of the model are predictions",
    "text": "The outputs of the model are predictions"
  },
  {
    "objectID": "ai/ws_hss_intro_slides.html#compare-those-predictions-to-the-train-labels",
    "href": "ai/ws_hss_intro_slides.html#compare-those-predictions-to-the-train-labels",
    "title": "Introduction to machine learning for the humanities",
    "section": "Compare those predictions to the train labels",
    "text": "Compare those predictions to the train labels\n\nSince our data was labelled, we know what the true outputs are"
  },
  {
    "objectID": "ai/ws_hss_intro_slides.html#calculate-train-loss",
    "href": "ai/ws_hss_intro_slides.html#calculate-train-loss",
    "title": "Introduction to machine learning for the humanities",
    "section": "Calculate train loss",
    "text": "Calculate train loss\n\nThe deviation of our predictions from the true outputs gives us a measure of training loss"
  },
  {
    "objectID": "ai/ws_hss_intro_slides.html#adjust-parameters",
    "href": "ai/ws_hss_intro_slides.html#adjust-parameters",
    "title": "Introduction to machine learning for the humanities",
    "section": "Adjust parameters",
    "text": "Adjust parameters\n\nThe parameters get automatically adjusted to reduce the training loss through the mechanism of backpropagation. This is the actual training part\nThis process is repeated many times. Training models is pretty much a giant for loop"
  },
  {
    "objectID": "ai/ws_hss_intro_slides.html#from-model-to-program",
    "href": "ai/ws_hss_intro_slides.html#from-model-to-program",
    "title": "Introduction to machine learning for the humanities",
    "section": "From model to program",
    "text": "From model to program\n\nRemember that the model architecture is fixed, but that the parameters change at each iteration of the training process"
  },
  {
    "objectID": "ai/ws_hss_intro_slides.html#section",
    "href": "ai/ws_hss_intro_slides.html#section",
    "title": "Introduction to machine learning for the humanities",
    "section": " ",
    "text": "While the labelled data are key to training, what we are really interested in is the combination of architecture + final parameters"
  },
  {
    "objectID": "ai/ws_hss_intro_slides.html#section-1",
    "href": "ai/ws_hss_intro_slides.html#section-1",
    "title": "Introduction to machine learning for the humanities",
    "section": " ",
    "text": "When the training is over, the parameters become fixed. Which means that our model now behaves like a classic program"
  },
  {
    "objectID": "ai/ws_hss_intro_slides.html#evaluate-the-model",
    "href": "ai/ws_hss_intro_slides.html#evaluate-the-model",
    "title": "Introduction to machine learning for the humanities",
    "section": "Evaluate the model",
    "text": "Evaluate the model\n\nWe can now use the testing set (which was never used to train the model) to evaluate our model: if we pass the test inputs through our program, we get some predictions that we can compare to the test labels (which are the true outputs)\nThis gives us the test loss: a measure of how well our model performs"
  },
  {
    "objectID": "ai/ws_hss_intro_slides.html#use-the-model",
    "href": "ai/ws_hss_intro_slides.html#use-the-model",
    "title": "Introduction to machine learning for the humanities",
    "section": "Use the model",
    "text": "Use the model\n\nNow that we have a program, we can use it on unlabelled inputs to get what people ultimately want: unknown outputs\nThis is when we put our model to actual use to solve some problem"
  },
  {
    "objectID": "ai/ws_hss_intro_slides.html#learning",
    "href": "ai/ws_hss_intro_slides.html#learning",
    "title": "Introduction to machine learning for the humanities",
    "section": "Learning",
    "text": "Learning\n\n\nThe process of learning in biological NN happens through neuron death or growth and the creation or loss of synaptic connections between neurons\n\n\n\nIn ANN, learning happens through optimization algorithms such as gradient descent which minimize cross entropy loss functions by adjusting the weights and biases connecting each layer of neurons over many iterations"
  },
  {
    "objectID": "ai/ws_hss_intro_slides.html#fully-connected-neural-networks",
    "href": "ai/ws_hss_intro_slides.html#fully-connected-neural-networks",
    "title": "Introduction to machine learning for the humanities",
    "section": "Fully connected neural networks",
    "text": "Fully connected neural networks\n\n\n\n\n\nFrom Glosser.ca, Wikipedia\n\n\n\nEach neuron receives inputs from every neuron of the previous layer and passes its output to every neuron of the next layer"
  },
  {
    "objectID": "ai/ws_hss_intro_slides.html#convolutional-neural-networks",
    "href": "ai/ws_hss_intro_slides.html#convolutional-neural-networks",
    "title": "Introduction to machine learning for the humanities",
    "section": "Convolutional neural networks",
    "text": "Convolutional neural networks\n\n\nFrom Programming Journeys by Rensu Theart\n\nConvolutional neural networks (CNN) are used for spatially structured data (e.g. images)\nImages have huge input sizes and would require a very large number of neurons in a fully connected neural net. In convolutional layers, neurons receive input from a subarea (called local receptive field) of the previous layer. This greatly reduces the number of parameters. Optionally, pooling (combining the outputs of neurons in a subarea) reduces the data dimensions"
  },
  {
    "objectID": "ai/ws_hss_intro_slides.html#recurrent-neural-networks",
    "href": "ai/ws_hss_intro_slides.html#recurrent-neural-networks",
    "title": "Introduction to machine learning for the humanities",
    "section": "Recurrent neural networks",
    "text": "Recurrent neural networks\n\n\nFrom fdeloche, Wikipedia\n\nRecurrent neural networks (RNN) such as Long Short-Term Memory (LSTM) are used for chain structured data (e.g. text)\nThey are not feedforward networks (i.e. networks for which the information moves only in the forward direction without any loop)"
  },
  {
    "objectID": "ai/ws_hss_intro_slides.html#transformers",
    "href": "ai/ws_hss_intro_slides.html#transformers",
    "title": "Introduction to machine learning for the humanities",
    "section": "Transformers",
    "text": "Transformers\nA combination of two RNNs (the encoder and the decoder) is used in sequence to sequence models for translation or picture captioning\nIn 2014 the concept of attention (giving added weight to important words) was developed, greatly improving the ability of such models to process a lot of data\nThe problem with recurrence is that it is not easily to parallelize (and thus to run fast on GPUs)\nIn 2017, a new model—the transformer—was proposed: by using only attention mechanisms and no recurrence, the transformer achieves better results in an easily parallelizable fashion\nWith the addition of transfer learning, powerful transformers emerged in the field of NLP (e.g. Bidirectional Encoder Representations from Transformers (BERT) from Google and Generative Pre-trained Transformer-3 (GPT-3) from OpenAI)"
  },
  {
    "objectID": "ai/ws_hss_intro_slides.html#data-bias",
    "href": "ai/ws_hss_intro_slides.html#data-bias",
    "title": "Introduction to machine learning for the humanities",
    "section": "Data bias",
    "text": "Data bias\nBias is always present in data\nDocument the limitations and scope of your data as best as possible\nProblems to watch for:\n\nOut of domain data: data used for training are not relevant to the model application\nDomain shift: model becoming inadapted as conditions evolve\nFeedback loop: initial bias exacerbated over the time"
  },
  {
    "objectID": "ai/ws_hss_intro_slides.html#transformation-of-subjects",
    "href": "ai/ws_hss_intro_slides.html#transformation-of-subjects",
    "title": "Introduction to machine learning for the humanities",
    "section": "Transformation of subjects",
    "text": "Transformation of subjects\nAlgorithms are supposed to help us, not transform us (e.g. YouTube recommendation algorithms)"
  },
  {
    "objectID": "ai/ws_hss_intro_slides.html#bugs",
    "href": "ai/ws_hss_intro_slides.html#bugs",
    "title": "Introduction to machine learning for the humanities",
    "section": "Bugs",
    "text": "Bugs\nExample of bug with real life consequences"
  },
  {
    "objectID": "ai/ws_hss_intro_slides.html#many-options",
    "href": "ai/ws_hss_intro_slides.html#many-options",
    "title": "Introduction to machine learning for the humanities",
    "section": "Many options",
    "text": "Many options\nHere are just a few:\n\nscikit-learn: a Python ML library built on top of SciPy\nNatural Language Toolkit (NLTK): a suite of Python libraries geared towards teaching and research\nspaCy: Python library geared towards production\ntorchtext, part of the PyTorch project (and many options of added layers on top such as PyTorch-NLP): Python library\nGenSim: Python library\nStanford CoreNLP: Java library\nMany libraries in the Julia programming language"
  },
  {
    "objectID": "ai/ws_hss_intro_slides.html#which-one-to-choose",
    "href": "ai/ws_hss_intro_slides.html#which-one-to-choose",
    "title": "Introduction to machine learning for the humanities",
    "section": "Which one to choose?",
    "text": "Which one to choose?\nChoose an open source tool (i.e. stay away from proprietary software such as MATLAB)\n\nResearchers who do not have access to the tool cannot reproduce your methods (open tools = open equitable research)\nOnce you graduate, you may not have access to the tool anymore\nYour university may stop paying for a license\nYou may get locked-in\nProprietary tools are often black boxes\nLong-term access is not guaranty (problem to replicate studies)\nThe licenses you have access to may be limiting and a cause of headache\nProprietary tools fall behind popular open-source tools\nProprietary tools often fail to address specialized edge cases needed in research"
  },
  {
    "objectID": "ai/ws_hss_intro_slides.html#neural-nets",
    "href": "ai/ws_hss_intro_slides.html#neural-nets",
    "title": "Introduction to machine learning for the humanities",
    "section": "Neural nets",
    "text": "Neural nets\n3Blue1Brown by Grant Sanderson has a series of 4 videos on neural networks which is easy to watch, fun, and does an excellent job at introducing the functioning of a simple neural network\n\nWhat are NN? (19 min)\nHow do NN learn? (21 min)\nWhat is backpropagation? (14 min)\nHow does backpropagation work? (10 min)\n\n\nAs you develop your own ML models, if you find that your mathematical background is shaky, 3blue1brown also has an excellent series of videos on linear algebra and an equally great one on calculus"
  },
  {
    "objectID": "ai/ws_hss_intro_slides.html#open-access-preprints",
    "href": "ai/ws_hss_intro_slides.html#open-access-preprints",
    "title": "Introduction to machine learning for the humanities",
    "section": "Open-access preprints",
    "text": "Open-access preprints\n\nArxiv Sanity Preserver by Andrej Karpathy\nML papers in the computer science category on arXiv\nML papers in the stats category on arXiv\nDistill ML research online journal"
  },
  {
    "objectID": "ai/ws_hss_intro_slides.html#advice-and-sources",
    "href": "ai/ws_hss_intro_slides.html#advice-and-sources",
    "title": "Introduction to machine learning for the humanities",
    "section": "Advice and sources",
    "text": "Advice and sources\n\nAdvice and sources from ML research student"
  },
  {
    "objectID": "ai/ws_hss_intro_slides.html#getting-help",
    "href": "ai/ws_hss_intro_slides.html#getting-help",
    "title": "Introduction to machine learning for the humanities",
    "section": "Getting help",
    "text": "Getting help\nStack Overflow …\n\n[machine-learning] tag\n[deep-learning] tag\n[supervised-learning] tag\n[unsupervised-learning] tag\n[semisupervised-learning] tag\n[reinforcement-learning] tag\n[transfer-learning] tag\n[machine-learning-model] tag\n[learning-rate] tag"
  },
  {
    "objectID": "ai/ws_hss_intro_slides.html#open-datasets",
    "href": "ai/ws_hss_intro_slides.html#open-datasets",
    "title": "Introduction to machine learning for the humanities",
    "section": "Open datasets",
    "text": "Open datasets\n\nbenchmarks.ai\nAIBench\nkaggle\nWikipedia"
  },
  {
    "objectID": "ai/ws_hss_intro_slides.html#pytorch",
    "href": "ai/ws_hss_intro_slides.html#pytorch",
    "title": "Introduction to machine learning for the humanities",
    "section": "PyTorch",
    "text": "PyTorch\n\nDocumentation\nTutorials\nExamples\n\nGetting help\n\nPyTorch Discourse forum\nStack Overflow [pytorch] tag\n\nPre-trained models\n\nPyTorch Hub"
  },
  {
    "objectID": "ai/ws_hss_intro_slides.html#python",
    "href": "ai/ws_hss_intro_slides.html#python",
    "title": "Introduction to machine learning for the humanities",
    "section": "Python",
    "text": "Python\nIDE\n\nProject Jupyter\nList of IDEs with description\nComparison of IDEs\nEmacs Python IDE\n\nGetting help\n\nStack Overflow [python] tag"
  },
  {
    "objectID": "ai/ws_dl_nlp_llm_slides.html#ml-allows-to-achieve-previously-impossible-tasks",
    "href": "ai/ws_dl_nlp_llm_slides.html#ml-allows-to-achieve-previously-impossible-tasks",
    "title": "A quick introduction to deep learning, NLP, and LLMs ",
    "section": "ML allows to achieve previously impossible tasks",
    "text": "ML allows to achieve previously impossible tasks\n\nLet’s take the example of image recognition:\n\nIn typical computing, a programmer writes code that gives a computer detailed instructions of what to do\nCoding all the possible ways—pixel by pixel—that an image can represent, say, a dog is an impossibly large task: there are many breeds of dogs, the image can be a picture, a blurred picture, a drawing, a cartoon, the dog can be in all sorts of positions, wearing clothes, etc.\nThere just aren’t enough resources to make the traditional programming approach able to create a computer program that can identify a dog in images\nBy feeding a very large number of dog images to a neural network however, we can train that network to recognize dogs in images that it has never seen (without explicitly programming how it does this!)"
  },
  {
    "objectID": "ai/ws_dl_nlp_llm_slides.html#old-concept-new-computing-power",
    "href": "ai/ws_dl_nlp_llm_slides.html#old-concept-new-computing-power",
    "title": "A quick introduction to deep learning, NLP, and LLMs ",
    "section": "Old concept … new computing power",
    "text": "Old concept … new computing power\nThe concept is everything but new: Arthur Samuel came up with it in 1949 and built a self-learning Checkers-playing program in 1959\n\n\nMachine learning consists of feeding vast amounts of data to algorithms to strengthen pathways, so the excitement for the approach became somewhat dormant due to the lack of computing power and the lack of training data at the time\nThe advent of powerful computers, GPUs, and massive amounts of data have brought the old concept to the forefront\n\n\n\n\n\n\nFrom xkcd.com"
  },
  {
    "objectID": "ai/ws_dl_nlp_llm_slides.html#supervised-learning",
    "href": "ai/ws_dl_nlp_llm_slides.html#supervised-learning",
    "title": "A quick introduction to deep learning, NLP, and LLMs ",
    "section": "Supervised learning",
    "text": "Supervised learning\n\nRegression is a form of supervised learning with continuous outputs\nClassification is supervised learning with discrete outputs\n\nSupervised learning uses training data in the form of example input/output pairs\nGoal\nFind the relationship between inputs and outputs"
  },
  {
    "objectID": "ai/ws_dl_nlp_llm_slides.html#unsupervised-learning",
    "href": "ai/ws_dl_nlp_llm_slides.html#unsupervised-learning",
    "title": "A quick introduction to deep learning, NLP, and LLMs ",
    "section": "Unsupervised learning",
    "text": "Unsupervised learning\nClustering, social network analysis, market segmentation, PCA … are all forms of unsupervised learning\nUnsupervised learning uses unlabelled data\nGoal\nFind structure within the data"
  },
  {
    "objectID": "ai/ws_dl_nlp_llm_slides.html#reinforcement-learning",
    "href": "ai/ws_dl_nlp_llm_slides.html#reinforcement-learning",
    "title": "A quick introduction to deep learning, NLP, and LLMs ",
    "section": "Reinforcement learning",
    "text": "Reinforcement learning\nThe algorithm explores by performing random actions and these actions are rewarded or punished (bonus points or penalties)\nThis is how algorithms learn to play games"
  },
  {
    "objectID": "ai/ws_dl_nlp_llm_slides.html#decide-on-an-architecture",
    "href": "ai/ws_dl_nlp_llm_slides.html#decide-on-an-architecture",
    "title": "A quick introduction to deep learning, NLP, and LLMs ",
    "section": "Decide on an architecture",
    "text": "Decide on an architecture\n\nThe architecture won’t change during training\nThe type of architecture you choose (e.g. CNN, Transformer) depends on the type of data you have (e.g. vision, textual). The depth and breadth of your network depend on the amount of data and computing resource you have"
  },
  {
    "objectID": "ai/ws_dl_nlp_llm_slides.html#set-some-initial-parameters",
    "href": "ai/ws_dl_nlp_llm_slides.html#set-some-initial-parameters",
    "title": "A quick introduction to deep learning, NLP, and LLMs ",
    "section": "Set some initial parameters",
    "text": "Set some initial parameters\n\nYou can initialize them randomly or get much better ones through transfer learning\nWhile the parameters are also part of the model, those will change during training"
  },
  {
    "objectID": "ai/ws_dl_nlp_llm_slides.html#get-some-labelled-data",
    "href": "ai/ws_dl_nlp_llm_slides.html#get-some-labelled-data",
    "title": "A quick introduction to deep learning, NLP, and LLMs ",
    "section": "Get some labelled data",
    "text": "Get some labelled data\n\nWhen we say that we need a lot of data for machine learning, we mean “lots of labelled data” as this is what gets used for training models"
  },
  {
    "objectID": "ai/ws_dl_nlp_llm_slides.html#make-sure-to-keep-some-data-for-testing",
    "href": "ai/ws_dl_nlp_llm_slides.html#make-sure-to-keep-some-data-for-testing",
    "title": "A quick introduction to deep learning, NLP, and LLMs ",
    "section": "Make sure to keep some data for testing",
    "text": "Make sure to keep some data for testing\n\nThose data won’t be used for training the model. Often people keep around 20% of their data for testing"
  },
  {
    "objectID": "ai/ws_dl_nlp_llm_slides.html#pass-data-and-parameters-through-the-architecture",
    "href": "ai/ws_dl_nlp_llm_slides.html#pass-data-and-parameters-through-the-architecture",
    "title": "A quick introduction to deep learning, NLP, and LLMs ",
    "section": "Pass data and parameters through the architecture",
    "text": "Pass data and parameters through the architecture\n\nThe train data are the inputs and the process of calculating the outputs is the forward pass"
  },
  {
    "objectID": "ai/ws_dl_nlp_llm_slides.html#the-outputs-of-the-model-are-predictions",
    "href": "ai/ws_dl_nlp_llm_slides.html#the-outputs-of-the-model-are-predictions",
    "title": "A quick introduction to deep learning, NLP, and LLMs ",
    "section": "The outputs of the model are predictions",
    "text": "The outputs of the model are predictions"
  },
  {
    "objectID": "ai/ws_dl_nlp_llm_slides.html#compare-those-predictions-to-the-train-labels",
    "href": "ai/ws_dl_nlp_llm_slides.html#compare-those-predictions-to-the-train-labels",
    "title": "A quick introduction to deep learning, NLP, and LLMs ",
    "section": "Compare those predictions to the train labels",
    "text": "Compare those predictions to the train labels\n\nSince our data was labelled, we know what the true outputs are"
  },
  {
    "objectID": "ai/ws_dl_nlp_llm_slides.html#calculate-train-loss",
    "href": "ai/ws_dl_nlp_llm_slides.html#calculate-train-loss",
    "title": "A quick introduction to deep learning, NLP, and LLMs ",
    "section": "Calculate train loss",
    "text": "Calculate train loss\n\nThe deviation of our predictions from the true outputs gives us a measure of training loss"
  },
  {
    "objectID": "ai/ws_dl_nlp_llm_slides.html#adjust-parameters",
    "href": "ai/ws_dl_nlp_llm_slides.html#adjust-parameters",
    "title": "A quick introduction to deep learning, NLP, and LLMs ",
    "section": "Adjust parameters",
    "text": "Adjust parameters\n\nThe parameters get automatically adjusted to reduce the training loss through the mechanism of backpropagation. This is the actual training part\nThis process is repeated many times. Training models is pretty much a giant for loop"
  },
  {
    "objectID": "ai/ws_dl_nlp_llm_slides.html#from-model-to-program",
    "href": "ai/ws_dl_nlp_llm_slides.html#from-model-to-program",
    "title": "A quick introduction to deep learning, NLP, and LLMs ",
    "section": "From model to program",
    "text": "From model to program\n\nRemember that the model architecture is fixed, but that the parameters change at each iteration of the training process"
  },
  {
    "objectID": "ai/ws_dl_nlp_llm_slides.html#section",
    "href": "ai/ws_dl_nlp_llm_slides.html#section",
    "title": "A quick introduction to deep learning, NLP, and LLMs ",
    "section": " ",
    "text": "While the labelled data are key to training, what we are really interested in is the combination of architecture + final parameters"
  },
  {
    "objectID": "ai/ws_dl_nlp_llm_slides.html#section-1",
    "href": "ai/ws_dl_nlp_llm_slides.html#section-1",
    "title": "A quick introduction to deep learning, NLP, and LLMs ",
    "section": " ",
    "text": "When the training is over, the parameters become fixed. Which means that our model now behaves like a classic program"
  },
  {
    "objectID": "ai/ws_dl_nlp_llm_slides.html#evaluate-the-model",
    "href": "ai/ws_dl_nlp_llm_slides.html#evaluate-the-model",
    "title": "A quick introduction to deep learning, NLP, and LLMs ",
    "section": "Evaluate the model",
    "text": "Evaluate the model\n\nWe can now use the testing set (which was never used to train the model) to evaluate our model: if we pass the test inputs through our program, we get some predictions that we can compare to the test labels (which are the true outputs)\nThis gives us the test loss: a measure of how well our model performs"
  },
  {
    "objectID": "ai/ws_dl_nlp_llm_slides.html#use-the-model",
    "href": "ai/ws_dl_nlp_llm_slides.html#use-the-model",
    "title": "A quick introduction to deep learning, NLP, and LLMs ",
    "section": "Use the model",
    "text": "Use the model\n\nNow that we have a program, we can use it on unlabelled inputs to get what people ultimately want: unknown outputs\nThis is when we put our model to actual use to solve some problem"
  },
  {
    "objectID": "ai/ws_dl_nlp_llm_slides.html#learning",
    "href": "ai/ws_dl_nlp_llm_slides.html#learning",
    "title": "A quick introduction to deep learning, NLP, and LLMs ",
    "section": "Learning",
    "text": "Learning\n\n\nThe process of learning in biological NN happens through neuron death or growth and the creation or loss of synaptic connections between neurons\n\n\n\nIn ANN, learning happens through optimization algorithms such as gradient descent which minimize cross entropy loss functions by adjusting the weights and biases connecting each layer of neurons over many iterations"
  },
  {
    "objectID": "ai/ws_dl_nlp_llm_slides.html#fully-connected-neural-networks",
    "href": "ai/ws_dl_nlp_llm_slides.html#fully-connected-neural-networks",
    "title": "A quick introduction to deep learning, NLP, and LLMs ",
    "section": "Fully connected neural networks",
    "text": "Fully connected neural networks\n\n\n\n\n\nFrom Glosser.ca, Wikipedia\n\n\n\nEach neuron receives inputs from every neuron of the previous layer and passes its output to every neuron of the next layer"
  },
  {
    "objectID": "ai/ws_dl_nlp_llm_slides.html#convolutional-neural-networks",
    "href": "ai/ws_dl_nlp_llm_slides.html#convolutional-neural-networks",
    "title": "A quick introduction to deep learning, NLP, and LLMs ",
    "section": "Convolutional neural networks",
    "text": "Convolutional neural networks\n\n\nFrom Programming Journeys by Rensu Theart\n\nConvolutional neural networks (CNN) are used for spatially structured data (e.g. images)\nImages have huge input sizes and would require a very large number of neurons in a fully connected neural net. In convolutional layers, neurons receive input from a subarea (called local receptive field) of the previous layer. This greatly reduces the number of parameters. Optionally, pooling (combining the outputs of neurons in a subarea) reduces the data dimensions"
  },
  {
    "objectID": "ai/ws_dl_nlp_llm_slides.html#recurrent-neural-networks",
    "href": "ai/ws_dl_nlp_llm_slides.html#recurrent-neural-networks",
    "title": "A quick introduction to deep learning, NLP, and LLMs ",
    "section": "Recurrent neural networks",
    "text": "Recurrent neural networks\n\n\nFrom fdeloche, Wikipedia\n\nRecurrent neural networks (RNN) such as Long Short-Term Memory (LSTM) are used for chain structured data (e.g. text)\nThey are not feedforward networks (i.e. networks for which the information moves only in the forward direction without any loop)"
  },
  {
    "objectID": "ai/ws_dl_nlp_llm_slides.html#transformers",
    "href": "ai/ws_dl_nlp_llm_slides.html#transformers",
    "title": "A quick introduction to deep learning, NLP, and LLMs ",
    "section": "Transformers",
    "text": "Transformers\nA combination of two RNNs (the encoder and the decoder) is used in sequence to sequence models for translation or picture captioning\nIn 2014 the concept of attention (giving added weight to important words) was developed, greatly improving the ability of such models to process a lot of data\nThe problem with recurrence is that it is not easily to parallelize (and thus to run fast on GPUs)\nIn 2017, a new model—the transformer—was proposed: by using only attention mechanisms and no recurrence, the transformer achieves better results in an easily parallelizable fashion\nWith the addition of transfer learning, powerful transformers emerged in the field of NLP (e.g. Bidirectional Encoder Representations from Transformers (BERT) from Google and Generative Pre-trained Transformer-3 (GPT-3) from OpenAI)"
  },
  {
    "objectID": "ai/ws_dl_nlp_llm_slides.html#data-bias",
    "href": "ai/ws_dl_nlp_llm_slides.html#data-bias",
    "title": "A quick introduction to deep learning, NLP, and LLMs ",
    "section": "Data bias",
    "text": "Data bias\nBias is always present in data\nDocument the limitations and scope of your data as best as possible\nProblems to watch for:\n\nOut of domain data: data used for training are not relevant to the model application\nDomain shift: model becoming inadapted as conditions evolve\nFeedback loop: initial bias exacerbated over the time"
  },
  {
    "objectID": "ai/ws_dl_nlp_llm_slides.html#transformation-of-subjects",
    "href": "ai/ws_dl_nlp_llm_slides.html#transformation-of-subjects",
    "title": "A quick introduction to deep learning, NLP, and LLMs ",
    "section": "Transformation of subjects",
    "text": "Transformation of subjects\nAlgorithms are supposed to help us, not transform us (e.g. YouTube recommendation algorithms)"
  },
  {
    "objectID": "ai/ws_dl_nlp_llm_slides.html#bugs",
    "href": "ai/ws_dl_nlp_llm_slides.html#bugs",
    "title": "A quick introduction to deep learning, NLP, and LLMs ",
    "section": "Bugs",
    "text": "Bugs\nExample of bug with real life consequences"
  },
  {
    "objectID": "ai/ws_dl_nlp_llm_slides.html#what-is-nlp",
    "href": "ai/ws_dl_nlp_llm_slides.html#what-is-nlp",
    "title": "A quick introduction to deep learning, NLP, and LLMs ",
    "section": "What is NLP?",
    "text": "What is NLP?\nNatural language processing is simply the application of machine learning to human (natural) language"
  },
  {
    "objectID": "ai/ws_dl_nlp_llm_slides.html#applications",
    "href": "ai/ws_dl_nlp_llm_slides.html#applications",
    "title": "A quick introduction to deep learning, NLP, and LLMs ",
    "section": "Applications",
    "text": "Applications\n\nSpam detection\nTranslation\nSentiment analysis\nPredictive text\nText classification\nSpeech recognition\nNatural language generation\nChatbots\nSearch results"
  },
  {
    "objectID": "ai/ws_dl_nlp_llm_slides.html#data-processing",
    "href": "ai/ws_dl_nlp_llm_slides.html#data-processing",
    "title": "A quick introduction to deep learning, NLP, and LLMs ",
    "section": "Data processing",
    "text": "Data processing\n\nTokenization: split text into sentences (sentence tokenization) and words (word tokenization)\nRemove punctuation and stopwords (e.g. “the”, “a”, “and”, “is”, “are”)\nTurn all words to lower case\nKeep only the lemma of words (lemmatization)\n\n\nAn alternative and simpler method is stemming\n\n\nIdentify word collocations (groups of words that often occur together, such as the bigrams “United States” or “open source”)\nTagging\nModel training"
  },
  {
    "objectID": "ai/ws_dl_nlp_llm_slides.html#many-options",
    "href": "ai/ws_dl_nlp_llm_slides.html#many-options",
    "title": "A quick introduction to deep learning, NLP, and LLMs ",
    "section": "Many options",
    "text": "Many options\nHere are just a few:\n\nscikit-learn: a Python ML library built on top of SciPy\nNatural Language Toolkit (NLTK): a suite of Python libraries geared towards teaching and research\nspaCy: Python library geared towards production\ntorchtext, part of the PyTorch project (and many options of added layers on top such as PyTorch-NLP): Python library\nGenSim: Python library\nStanford CoreNLP: Java library\nMany libraries in the Julia programming language"
  },
  {
    "objectID": "ai/ws_dl_nlp_llm_slides.html#which-one-to-choose",
    "href": "ai/ws_dl_nlp_llm_slides.html#which-one-to-choose",
    "title": "A quick introduction to deep learning, NLP, and LLMs ",
    "section": "Which one to choose?",
    "text": "Which one to choose?\nChoose an open source tool (i.e. stay away from proprietary software such as MATLAB)\n\nResearchers who do not have access to the tool cannot reproduce your methods (open tools = open equitable research)\nOnce you graduate, you may not have access to the tool anymore\nYour university may stop paying for a license\nYou may get locked-in\nProprietary tools are often black boxes\nLong-term access is not guaranty (problem to replicate studies)\nThe licenses you have access to may be limiting and a cause of headache\nProprietary tools fall behind popular open-source tools\nProprietary tools often fail to address specialized edge cases needed in research"
  },
  {
    "objectID": "ai/ws_dl_nlp_llm_slides.html#neural-nets",
    "href": "ai/ws_dl_nlp_llm_slides.html#neural-nets",
    "title": "A quick introduction to deep learning, NLP, and LLMs ",
    "section": "Neural nets",
    "text": "Neural nets\n3Blue1Brown by Grant Sanderson has a series of 4 videos on neural networks which is easy to watch, fun, and does an excellent job at introducing the functioning of a simple neural network\n\nWhat are NN? (19 min)\nHow do NN learn? (21 min)\nWhat is backpropagation? (14 min)\nHow does backpropagation work? (10 min)\n\n\nAs you develop your own ML models, if you find that your mathematical background is shaky, 3blue1brown also has an excellent series of videos on linear algebra and an equally great one on calculus"
  },
  {
    "objectID": "ai/ws_dl_nlp_llm_slides.html#open-access-preprints",
    "href": "ai/ws_dl_nlp_llm_slides.html#open-access-preprints",
    "title": "A quick introduction to deep learning, NLP, and LLMs ",
    "section": "Open-access preprints",
    "text": "Open-access preprints\n\nArxiv Sanity Preserver by Andrej Karpathy\nML papers in the computer science category on arXiv\nML papers in the stats category on arXiv\nDistill ML research online journal"
  },
  {
    "objectID": "ai/ws_dl_nlp_llm_slides.html#advice-and-sources",
    "href": "ai/ws_dl_nlp_llm_slides.html#advice-and-sources",
    "title": "A quick introduction to deep learning, NLP, and LLMs ",
    "section": "Advice and sources",
    "text": "Advice and sources\n\nAdvice and sources from ML research student"
  },
  {
    "objectID": "ai/ws_dl_nlp_llm_slides.html#getting-help",
    "href": "ai/ws_dl_nlp_llm_slides.html#getting-help",
    "title": "A quick introduction to deep learning, NLP, and LLMs ",
    "section": "Getting help",
    "text": "Getting help\nStack Overflow:\n\n[machine-learning] tag\n[deep-learning] tag\n[supervised-learning] tag\n[unsupervised-learning] tag\n[semisupervised-learning] tag\n[reinforcement-learning] tag\n[transfer-learning] tag\n[machine-learning-model] tag\n[learning-rate] tag"
  },
  {
    "objectID": "ai/ws_dl_nlp_llm_slides.html#open-datasets",
    "href": "ai/ws_dl_nlp_llm_slides.html#open-datasets",
    "title": "A quick introduction to deep learning, NLP, and LLMs ",
    "section": "Open datasets",
    "text": "Open datasets\n\nbenchmarks.ai\nAIBench\nkaggle\nWikipedia"
  },
  {
    "objectID": "ai/ws_dl_nlp_llm_slides.html#pytorch",
    "href": "ai/ws_dl_nlp_llm_slides.html#pytorch",
    "title": "A quick introduction to deep learning, NLP, and LLMs ",
    "section": "PyTorch",
    "text": "PyTorch\n\nDocumentation\nTutorials\nExamples\n\nGetting help\n\nPyTorch Discourse forum\nStack Overflow [pytorch] tag\n\nPre-trained models\n\nPyTorch Hub"
  },
  {
    "objectID": "ai/ws_dl_nlp_llm_slides.html#python",
    "href": "ai/ws_dl_nlp_llm_slides.html#python",
    "title": "A quick introduction to deep learning, NLP, and LLMs ",
    "section": "Python",
    "text": "Python\nIDE\n\nProject Jupyter\nList of IDEs with description\nComparison of IDEs\nEmacs Python IDE\n\nGetting help\n\nStack Overflow [python] tag"
  },
  {
    "objectID": "ai/wb_frameworks_slides.html#disclaimers",
    "href": "ai/wb_frameworks_slides.html#disclaimers",
    "title": "A map of current machine learning frameworks",
    "section": "Disclaimers",
    "text": "Disclaimers\nKeeping up with the ever evolving field of machine learning is daunting. This webinar aims to bring a little help in this navigation and maybe inspire you to explore new tools. It is however flawed in several ways:\n\nit is not exhaustive and focuses on the most used frameworks accessible through 3 popular languages,\na lot of the content could be accused of being biased and subjective,\nit will be outdated quickly.\n\nUltimately, the best frameworks are the ones that work best for you and your needs. The only piece of advice I would give is to always use open source tools."
  },
  {
    "objectID": "ai/wb_flux.html",
    "href": "ai/wb_flux.html",
    "title": "Machine learning in Julia with Flux",
    "section": "",
    "text": "This webinar, aimed at users with no experience in machine learning, is an introduction to the basic concepts of neural networks, followed by a simple example—the classic classification of the MNIST database of handwritten digits—using the Julia package Flux.",
    "crumbs": [
      "AI",
      "<b><em>Webinars</em></b>",
      "DL in Julia with Flux"
    ]
  },
  {
    "objectID": "ai/wb_dvc.html",
    "href": "ai/wb_dvc.html",
    "title": "Version control for data science and machine learning with DVC",
    "section": "",
    "text": "Data version control (DVC) is an open source tool that brings all the versioning and collaboration capabilities you use on your code with Git to your data and machine learning workflow.\nIf you use datasets in your work, it makes it easy to track their evolution.\nIf you are in the field of machine learning, it additionally allows you to track your models, manage your pipelines from parameters to metrics, collaborate on your experiments, and integrate with the continuous integration tool for machine learning projects CML.\nThis webinar will show you how to get started with DVC, first in the simple case where you just want to put your data under version control, then in the more complex situation where you want to manage your machine learning workflow in a more organized and reproducible fashion.\n\nSlides (Click and wait: this reveal.js presentation may take a little time to load.)",
    "crumbs": [
      "AI",
      "<b><em>Webinars</em></b>",
      "Data & model version control"
    ]
  },
  {
    "objectID": "ai/wb_copilot.html",
    "href": "ai/wb_copilot.html",
    "title": "AI-powered programming with Copilot",
    "section": "",
    "text": "The recent advances in generative AI have brought about a number of code generators and code-completion assistants. This webinar will give an overview of the state of the field, briefly explain the functioning of various types of tools, then focus on GitHub Copilot.\nCopilot is developed by GitHub and OpenAI. It is a cloud-based service requiring a subscription, but students and teachers can apply for free access. It can be used directly in the command line or as an extension to text editors such as VS Code, Emacs, or Neovim.\nI will demo Copilot’s main features:\n\nprovide live code-completion,\nturn comments into code,\ntranslate from one programming language to another.\n\n\nSlides (Click and wait: this reveal.js presentation may take a little time to load.)",
    "crumbs": [
      "AI",
      "<b><em>Webinars</em></b>",
      "AI-powered coding with Copilot"
    ]
  },
  {
    "objectID": "ai/top_wb.html",
    "href": "ai/top_wb.html",
    "title": "AI webinars",
    "section": "",
    "text": "Map of current ML frameworks\n\n\n\n\nModel version control with\n\n\n\n\nAccelerated array & autodiff with \n\n\n\n\nImage upscaling (super-resolution)\n\n\n\n\n\n\nAI-powered coding with  \n\n\n\n\nEasier    with fastai\n\n\n\n\nDL in  with \n\n\n\n\n  tensors in depth\n\n\n\n\n\n\nBayesian inference in",
    "crumbs": [
      "AI",
      "<b><em>Webinars</em></b>"
    ]
  },
  {
    "objectID": "ai/top_jx.html",
    "href": "ai/top_jx.html",
    "title": "An introduction to JAX",
    "section": "",
    "text": "JAX is an open source Python library for high-performance array computing and flexible automatic differentiation.\nHigh-performance computing is achieved by asynchronous dispatch, just-in-time compilation, the XLA compiler for linear algebra, and full compatibility with accelerators (GPUs and TPUs).\nAutomatic differentiation uses Autograd and works with complex control flows (conditions, recursions), second and third-order derivatives, forward and reverse modes. This makes JAX ideal for machine learning and neural network libraries such as Flax are built on it.\n\n Start course ➤",
    "crumbs": [
      "AI",
      "<b><em>The JAX library</em></b>"
    ]
  },
  {
    "objectID": "ai/sk_workflow.html",
    "href": "ai/sk_workflow.html",
    "title": "Sklearn workflow",
    "section": "",
    "text": "Scikit-learn has a very clean and consistent API, making it very easy to use: a similar workflow can be applied to most techniques. Let’s go over two examples.\nThis code was modified from Matthew Greenberg.",
    "crumbs": [
      "AI",
      "<b><em>ML with Scikit-learn</em></b>",
      "Sklearn workflow"
    ]
  },
  {
    "objectID": "ai/sk_workflow.html#load-packages",
    "href": "ai/sk_workflow.html#load-packages",
    "title": "Sklearn workflow",
    "section": "Load packages",
    "text": "Load packages\n\nfrom sklearn.datasets import fetch_california_housing, load_breast_cancer\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import (\n    mean_squared_error,\n    mean_absolute_percentage_error,\n    accuracy_score\n)\n\nimport pandas as pd\n\nimport matplotlib\nfrom matplotlib import pyplot as plt\n\nimport numpy as np\n\nfrom collections import Counter",
    "crumbs": [
      "AI",
      "<b><em>ML with Scikit-learn</em></b>",
      "Sklearn workflow"
    ]
  },
  {
    "objectID": "ai/sk_workflow.html#example-1-california-housing-dataset",
    "href": "ai/sk_workflow.html#example-1-california-housing-dataset",
    "title": "Sklearn workflow",
    "section": "Example 1: California housing dataset",
    "text": "Example 1: California housing dataset\n\nLoad and explore the data\n\ncal_housing = fetch_california_housing()\ntype(cal_housing)\n\nsklearn.utils._bunch.Bunch\n\n\nLet’s look at the attributes of cal_housing:\n\ndir(cal_housing)\n\n['DESCR', 'data', 'feature_names', 'frame', 'target', 'target_names']\n\n\n\ncal_housing.feature_names\n\n['MedInc',\n 'HouseAge',\n 'AveRooms',\n 'AveBedrms',\n 'Population',\n 'AveOccup',\n 'Latitude',\n 'Longitude']\n\n\n\nprint(cal_housing.DESCR)\n\n.. _california_housing_dataset:\n\nCalifornia Housing dataset\n--------------------------\n\n**Data Set Characteristics:**\n\n:Number of Instances: 20640\n\n:Number of Attributes: 8 numeric, predictive attributes and the target\n\n:Attribute Information:\n    - MedInc        median income in block group\n    - HouseAge      median house age in block group\n    - AveRooms      average number of rooms per household\n    - AveBedrms     average number of bedrooms per household\n    - Population    block group population\n    - AveOccup      average number of household members\n    - Latitude      block group latitude\n    - Longitude     block group longitude\n\n:Missing Attribute Values: None\n\nThis dataset was obtained from the StatLib repository.\nhttps://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html\n\nThe target variable is the median house value for California districts,\nexpressed in hundreds of thousands of dollars ($100,000).\n\nThis dataset was derived from the 1990 U.S. census, using one row per census\nblock group. A block group is the smallest geographical unit for which the U.S.\nCensus Bureau publishes sample data (a block group typically has a population\nof 600 to 3,000 people).\n\nA household is a group of people residing within a home. Since the average\nnumber of rooms and bedrooms in this dataset are provided per household, these\ncolumns may take surprisingly large values for block groups with few households\nand many empty houses, such as vacation resorts.\n\nIt can be downloaded/loaded using the\n:func:`sklearn.datasets.fetch_california_housing` function.\n\n.. rubric:: References\n\n- Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\n  Statistics and Probability Letters, 33 (1997) 291-297\n\n\n\n\nX = cal_housing.data\ny = cal_housing.target\n\n\nThis can also be obtained with X, y = fetch_california_housing(return_X_y=True).\n\nLet’s have a look at the shape of X and y:\n\nX.shape\n\n(20640, 8)\n\n\n\ny.shape\n\n(20640,)\n\n\nWhile not at all necessary, we can turn this bunch object into a more familiar data frame to explore the data further:\n\ncal_housing_df = pd.DataFrame(cal_housing.data, columns=cal_housing.feature_names)\n\n\ncal_housing_df.head()\n\n\n\n\n\n\n\n\nMedInc\nHouseAge\nAveRooms\nAveBedrms\nPopulation\nAveOccup\nLatitude\nLongitude\n\n\n\n\n0\n8.3252\n41.0\n6.984127\n1.023810\n322.0\n2.555556\n37.88\n-122.23\n\n\n1\n8.3014\n21.0\n6.238137\n0.971880\n2401.0\n2.109842\n37.86\n-122.22\n\n\n2\n7.2574\n52.0\n8.288136\n1.073446\n496.0\n2.802260\n37.85\n-122.24\n\n\n3\n5.6431\n52.0\n5.817352\n1.073059\n558.0\n2.547945\n37.85\n-122.25\n\n\n4\n3.8462\n52.0\n6.281853\n1.081081\n565.0\n2.181467\n37.85\n-122.25\n\n\n\n\n\n\n\n\ncal_housing_df.tail()\n\n\n\n\n\n\n\n\nMedInc\nHouseAge\nAveRooms\nAveBedrms\nPopulation\nAveOccup\nLatitude\nLongitude\n\n\n\n\n20635\n1.5603\n25.0\n5.045455\n1.133333\n845.0\n2.560606\n39.48\n-121.09\n\n\n20636\n2.5568\n18.0\n6.114035\n1.315789\n356.0\n3.122807\n39.49\n-121.21\n\n\n20637\n1.7000\n17.0\n5.205543\n1.120092\n1007.0\n2.325635\n39.43\n-121.22\n\n\n20638\n1.8672\n18.0\n5.329513\n1.171920\n741.0\n2.123209\n39.43\n-121.32\n\n\n20639\n2.3886\n16.0\n5.254717\n1.162264\n1387.0\n2.616981\n39.37\n-121.24\n\n\n\n\n\n\n\n\ncal_housing_df.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 20640 entries, 0 to 20639\nData columns (total 8 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   MedInc      20640 non-null  float64\n 1   HouseAge    20640 non-null  float64\n 2   AveRooms    20640 non-null  float64\n 3   AveBedrms   20640 non-null  float64\n 4   Population  20640 non-null  float64\n 5   AveOccup    20640 non-null  float64\n 6   Latitude    20640 non-null  float64\n 7   Longitude   20640 non-null  float64\ndtypes: float64(8)\nmemory usage: 1.3 MB\n\n\n\ncal_housing_df.describe() \n\n\n\n\n\n\n\n\nMedInc\nHouseAge\nAveRooms\nAveBedrms\nPopulation\nAveOccup\nLatitude\nLongitude\n\n\n\n\ncount\n20640.000000\n20640.000000\n20640.000000\n20640.000000\n20640.000000\n20640.000000\n20640.000000\n20640.000000\n\n\nmean\n3.870671\n28.639486\n5.429000\n1.096675\n1425.476744\n3.070655\n35.631861\n-119.569704\n\n\nstd\n1.899822\n12.585558\n2.474173\n0.473911\n1132.462122\n10.386050\n2.135952\n2.003532\n\n\nmin\n0.499900\n1.000000\n0.846154\n0.333333\n3.000000\n0.692308\n32.540000\n-124.350000\n\n\n25%\n2.563400\n18.000000\n4.440716\n1.006079\n787.000000\n2.429741\n33.930000\n-121.800000\n\n\n50%\n3.534800\n29.000000\n5.229129\n1.048780\n1166.000000\n2.818116\n34.260000\n-118.490000\n\n\n75%\n4.743250\n37.000000\n6.052381\n1.099526\n1725.000000\n3.282261\n37.710000\n-118.010000\n\n\nmax\n15.000100\n52.000000\n141.909091\n34.066667\n35682.000000\n1243.333333\n41.950000\n-114.310000\n\n\n\n\n\n\n\nWe can even plot it:\n\nplt.hist(y)\n\n(array([ 877., 3612., 4099., 3771., 2799., 1769., 1239.,  752.,  479.,\n        1243.]),\n array([0.14999 , 0.634992, 1.119994, 1.604996, 2.089998, 2.575   ,\n        3.060002, 3.545004, 4.030006, 4.515008, 5.00001 ]),\n &lt;BarContainer object of 10 artists&gt;)\n\n\n\n\n\n\n\n\n\n\n\nCreate and fit a model\nLet’s start with a very simple model: linear regression.\n\nmodel = LinearRegression().fit(X, y)\n\n\nThis is equivalent to:\nmodel = LinearRegression()\nmodel.fit(X, y)\nFirst, we create an instance of the class LinearRegression, then we call .fit() on it to fit the model.\n\n\nmodel.coef_\n\narray([ 4.36693293e-01,  9.43577803e-03, -1.07322041e-01,  6.45065694e-01,\n       -3.97638942e-06, -3.78654265e-03, -4.21314378e-01, -4.34513755e-01])\n\n\n\nTrailing underscores indicate that an attribute is estimated. .coef_ here is an estimated value.\n\n\nmodel.coef_.shape\n\n(8,)\n\n\n\nmodel.intercept_\n\nnp.float64(-36.94192020718422)\n\n\nWe can now get our predictions:\n\ny_hat = model.predict(X)\n\nAnd calculate some measures of error:\n\nSum of squared errors\n\n\nnp.sum((y - y_hat) ** 2)\n\nnp.float64(10821.985154850292)\n\n\n\nMean squared error\n\n\nmean_squared_error(y, y_hat)\n\n0.5243209861846072\n\n\n\nMSE could also be calculated with np.mean((y - y_hat)**2).\n\n\nmean_absolute_percentage_error(y, y_hat)\n\n0.31715404597233515\n\n\nIndex of minimum value:\n\nmodel.coef_.argmin()\n\nnp.int64(7)\n\n\nIndex of maximum value:\n\nmodel.coef_.argmax()\n\nnp.int64(3)\n\n\n\nXX = np.concatenate([np.ones((len(X), 1)), X], axis=1)\n\nbeta = np.linalg.lstsq(XX, y, rcond=None)[0]\nintercept_, *coef_ = beta\n\nintercept_, model.intercept_\n\n(np.float64(-36.94192020718429), np.float64(-36.94192020718422))\n\n\n\nnp.allclose(coef_, model.coef_)\n\nTrue\n\n\n\nThis means that the two arrays are equal element-wise, within a certain tolerance.\n\n\nX_test = np.random.normal(size=(10, X.shape[1]))\nX_test.shape\n\n(10, 8)\n\n\n\ny_test = X_test @ coef_ + intercept_\ny_test\n\narray([-37.7624766 , -35.43989534, -37.62776124, -36.16743807,\n       -36.70282941, -36.91231298, -36.84916   , -37.72509389,\n       -35.90083768, -36.00753365])\n\n\n\nmodel.predict(X_test)\n\narray([-37.7624766 , -35.43989534, -37.62776124, -36.16743807,\n       -36.70282941, -36.91231298, -36.84916   , -37.72509389,\n       -35.90083768, -36.00753365])\n\n\nOf course, instead of LinearRegression(), we could have used another model such as a random forest regressor (a meta estimator that fits a number of classifying decision trees on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting) for instance:\n\nmodel = RandomForestRegressor().fit(X, y).predict(X_test)\nmodel\n\narray([1.4199201, 1.2526101, 1.5110801, 1.4515   , 1.5110801, 1.4446701,\n       1.4515   , 1.5110801, 1.4479301, 1.1729801])\n\n\n\nWhich is equivalent to:\nmodel = RandomForestRegressor()\nmodel.fit(X, y).predict(X_test)",
    "crumbs": [
      "AI",
      "<b><em>ML with Scikit-learn</em></b>",
      "Sklearn workflow"
    ]
  },
  {
    "objectID": "ai/sk_workflow.html#example-2-breast-cancer",
    "href": "ai/sk_workflow.html#example-2-breast-cancer",
    "title": "Sklearn workflow",
    "section": "Example 2: breast cancer",
    "text": "Example 2: breast cancer\n\nLoad and explore the data\n\nb_cancer = load_breast_cancer()\n\nLet’s print the description of this dataset:\n\nprint(b_cancer.DESCR)\n\n.. _breast_cancer_dataset:\n\nBreast cancer wisconsin (diagnostic) dataset\n--------------------------------------------\n\n**Data Set Characteristics:**\n\n:Number of Instances: 569\n\n:Number of Attributes: 30 numeric, predictive attributes and the class\n\n:Attribute Information:\n    - radius (mean of distances from center to points on the perimeter)\n    - texture (standard deviation of gray-scale values)\n    - perimeter\n    - area\n    - smoothness (local variation in radius lengths)\n    - compactness (perimeter^2 / area - 1.0)\n    - concavity (severity of concave portions of the contour)\n    - concave points (number of concave portions of the contour)\n    - symmetry\n    - fractal dimension (\"coastline approximation\" - 1)\n\n    The mean, standard error, and \"worst\" or largest (mean of the three\n    worst/largest values) of these features were computed for each image,\n    resulting in 30 features.  For instance, field 0 is Mean Radius, field\n    10 is Radius SE, field 20 is Worst Radius.\n\n    - class:\n            - WDBC-Malignant\n            - WDBC-Benign\n\n:Summary Statistics:\n\n===================================== ====== ======\n                                        Min    Max\n===================================== ====== ======\nradius (mean):                        6.981  28.11\ntexture (mean):                       9.71   39.28\nperimeter (mean):                     43.79  188.5\narea (mean):                          143.5  2501.0\nsmoothness (mean):                    0.053  0.163\ncompactness (mean):                   0.019  0.345\nconcavity (mean):                     0.0    0.427\nconcave points (mean):                0.0    0.201\nsymmetry (mean):                      0.106  0.304\nfractal dimension (mean):             0.05   0.097\nradius (standard error):              0.112  2.873\ntexture (standard error):             0.36   4.885\nperimeter (standard error):           0.757  21.98\narea (standard error):                6.802  542.2\nsmoothness (standard error):          0.002  0.031\ncompactness (standard error):         0.002  0.135\nconcavity (standard error):           0.0    0.396\nconcave points (standard error):      0.0    0.053\nsymmetry (standard error):            0.008  0.079\nfractal dimension (standard error):   0.001  0.03\nradius (worst):                       7.93   36.04\ntexture (worst):                      12.02  49.54\nperimeter (worst):                    50.41  251.2\narea (worst):                         185.2  4254.0\nsmoothness (worst):                   0.071  0.223\ncompactness (worst):                  0.027  1.058\nconcavity (worst):                    0.0    1.252\nconcave points (worst):               0.0    0.291\nsymmetry (worst):                     0.156  0.664\nfractal dimension (worst):            0.055  0.208\n===================================== ====== ======\n\n:Missing Attribute Values: None\n\n:Class Distribution: 212 - Malignant, 357 - Benign\n\n:Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\n\n:Donor: Nick Street\n\n:Date: November, 1995\n\nThis is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\nhttps://goo.gl/U2Uwz2\n\nFeatures are computed from a digitized image of a fine needle\naspirate (FNA) of a breast mass.  They describe\ncharacteristics of the cell nuclei present in the image.\n\nSeparating plane described above was obtained using\nMultisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\nConstruction Via Linear Programming.\" Proceedings of the 4th\nMidwest Artificial Intelligence and Cognitive Science Society,\npp. 97-101, 1992], a classification method which uses linear\nprogramming to construct a decision tree.  Relevant features\nwere selected using an exhaustive search in the space of 1-4\nfeatures and 1-3 separating planes.\n\nThe actual linear program used to obtain the separating plane\nin the 3-dimensional space is that described in:\n[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\nProgramming Discrimination of Two Linearly Inseparable Sets\",\nOptimization Methods and Software 1, 1992, 23-34].\n\nThis database is also available through the UW CS ftp server:\n\nftp ftp.cs.wisc.edu\ncd math-prog/cpo-dataset/machine-learn/WDBC/\n\n.. dropdown:: References\n\n  - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction\n    for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on\n    Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\n    San Jose, CA, 1993.\n  - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and\n    prognosis via linear programming. Operations Research, 43(4), pages 570-577,\n    July-August 1995.\n  - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\n    to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994)\n    163-171.\n\n\n\n\nb_cancer.feature_names\n\narray(['mean radius', 'mean texture', 'mean perimeter', 'mean area',\n       'mean smoothness', 'mean compactness', 'mean concavity',\n       'mean concave points', 'mean symmetry', 'mean fractal dimension',\n       'radius error', 'texture error', 'perimeter error', 'area error',\n       'smoothness error', 'compactness error', 'concavity error',\n       'concave points error', 'symmetry error',\n       'fractal dimension error', 'worst radius', 'worst texture',\n       'worst perimeter', 'worst area', 'worst smoothness',\n       'worst compactness', 'worst concavity', 'worst concave points',\n       'worst symmetry', 'worst fractal dimension'], dtype='&lt;U23')\n\n\n\nb_cancer.target_names\n\narray(['malignant', 'benign'], dtype='&lt;U9')\n\n\n\nX = b_cancer.data\ny = b_cancer.target\n\n\nHere again, we could have used instead X, y = load_breast_cancer(return_X_y=True).\n\n\nX.shape\n\n(569, 30)\n\n\n\ny.shape\n\n(569,)\n\n\n\nset(y)\n\n{np.int64(0), np.int64(1)}\n\n\n\nCounter(y)\n\nCounter({np.int64(1): 357, np.int64(0): 212})\n\n\n\n\nCreate and fit a first model\n\nmodel = LogisticRegression(max_iter=10000)\ny_hat = model.fit(X, y).predict(X)\n\nGet some measure of accuracy:\n\naccuracy_score(y, y_hat)\n\n0.9578207381370826\n\n\n\nThis can also be obtained with:\nnp.mean(y_hat == y)\n\n\ndef sigmoid(x):\n  return 1/(1 + np.exp(-x))\n\nx = np.linspace(-10, 10, 100)\nplt.plot(x, sigmoid(x), lw=3)\nplt.title(\"The Sigmoid Function $\\\\sigma(x)$\")\n\nText(0.5, 1.0, 'The Sigmoid Function $\\\\sigma(x)$')\n\n\n\n\n\n\n\n\n\n\ny_pred = 1*(sigmoid(X @ model.coef_.squeeze() + model.intercept_) &gt; 0.5)\nassert np.all(y_pred == model.predict(X))\n\nnp.allclose(\n    model.predict_proba(X)[:, 1],\n    sigmoid(X @ model.coef_.squeeze() + model.intercept_)\n)\n\nTrue\n\n\n\ndef make_spirals(k=20, s=1.0, n=2000):\n    X = np.zeros((n, 2))\n    y = np.round(np.random.uniform(size=n)).astype(int)\n    r = np.random.uniform(size=n)*k*np.pi\n    rr = r**0.5\n    theta = rr + np.random.normal(loc=0, scale=s, size=n)\n    theta[y == 1] = theta[y == 1] + np.pi\n    X[:,0] = rr*np.cos(theta)\n    X[:,1] = rr*np.sin(theta)\n    return X, y\n\nX, y = make_spirals()\ncmap = matplotlib.colormaps[\"viridis\"]\n\na = cmap(0)\na = [*a[:3], 0.3]\nb = cmap(0.99)\nb = [*b[:3], 0.3]\n\nplt.figure(figsize=(7,7))\nax = plt.gca()\nax.set_aspect(\"equal\")\nax.plot(X[y == 0, 0], X[y == 0, 1], 'o', color=a, ms=8, label=\"$y=0$\")\nax.plot(X[y == 1, 0], X[y == 1, 1], 'o', color=b, ms=8, label=\"$y=1$\")\nplt.title(\"Spirals\")\nplt.legend()\n\n\n\n\n\n\n\n\n\n\nCreate and fit a second model\nHere, we use a logistic regression:\n\nmodel = LogisticRegression()\ny_hat = model.fit(X, y).predict(X)\naccuracy_score(y, y_hat)\n\n0.5785\n\n\n\nu = np.linspace(-8, 8, 100)\nv = np.linspace(-8, 8, 100)\nU, V = np.meshgrid(u, v)\nUV = np.array([U.ravel(), V.ravel()]).T\nU.shape, V.shape, UV.shape\n\n((100, 100), (100, 100), (10000, 2))\n\n\n\nnp.ravel returns a contiguous flattened array.\n\n\nW = model.predict(UV).reshape(U.shape)\nW.shape\n\n(100, 100)\n\n\n\nplt.pcolormesh(U, V, W)\n\n\n\n\n\n\n\n\n\n\nCreate and fit a third model\nLet’s use a k-nearest neighbours classifier this time:\n\nmodel = KNeighborsClassifier(n_neighbors=5)\ny_hat = model.fit(X, y).predict(X)\naccuracy_score(y, y_hat)\n\n0.8965\n\n\n\nu = np.linspace(-8, 8, 100)\nv = np.linspace(-8, 8, 100)\nU, V = np.meshgrid(u, v)\nUV = np.array([U.ravel(), V.ravel()]).T\nU.shape, V.shape, UV.shape\n\n((100, 100), (100, 100), (10000, 2))\n\n\n\nW = model.predict(UV).reshape(U.shape)\nW.shape\n\n(100, 100)\n\n\n\nplt.pcolormesh(U, V, W)\n\n\n\n\n\n\n\n\nWe can iterate over various values of k to see how the accuracy and pseudocolor plot evolve:\n\nfig, axes = plt.subplots(2, 4, figsize=(9.8, 5))\nfig.suptitle(\"Decision Regions\")\n\nu = np.linspace(-8, 8, 100)\nv = np.linspace(-8, 8, 100)\nU, V = np.meshgrid(u, v)\nUV = np.array([U.ravel(), V.ravel()]).T\n\nks = np.arange(1, 16, 2)\n\nfor k, ax in zip(ks, axes.ravel()):\n  model = KNeighborsClassifier(n_neighbors=k)\n  model.fit(X, y)\n  acc = accuracy_score(y, model.predict(X))\n  W = model.predict(UV).reshape(U.shape)\n  ax.imshow(W, origin=\"lower\", cmap=cmap)\n  ax.set_axis_off()\n  ax.set_title(f\"$k$={k}, acc={acc:.2f}\")",
    "crumbs": [
      "AI",
      "<b><em>ML with Scikit-learn</em></b>",
      "Sklearn workflow"
    ]
  },
  {
    "objectID": "ai/pt/ws_pretrained_models.html",
    "href": "ai/pt/ws_pretrained_models.html",
    "title": "Finding pretrained models for transfer learning",
    "section": "",
    "text": "Training models from scratch requires way too much data, time, and computing power (or money) to be a practical option. This is why transfer learning has become such a common practice: by starting with models trained on related problems, you are saving time and achieving good results with little data.\nNow, where do you find such models?\nIn this workshop, we will see how to use pre-trained models included in PyTorch libraries, have a look at some of the most popular pre-trained models repositories, and learn how to search models in the literature and on GitHub.",
    "crumbs": [
      "AI",
      "<b><em>Workshops</em></b>",
      "Finding pre-trained models"
    ]
  },
  {
    "objectID": "ai/pt/ws_pretrained_models.html#what-are-pre-trained-models",
    "href": "ai/pt/ws_pretrained_models.html#what-are-pre-trained-models",
    "title": "Finding pretrained models for transfer learning",
    "section": "What are pre-trained models?",
    "text": "What are pre-trained models?\n\nTransfer learning\nIf you build models from scratch, expect their performance to be mediocre. Totally naive models with random weights and biases usually need to be trained for a long time on very large datasets, using vast amounts of computing resources, before they produce competitive results. You may not even have enough data to train a model from scratch.\nInstead of starting from zero however, you can use a model that has been trained on a similar task. For instance, if your goal is to create a model able to identify bird species from pictures, you could look for a model developed for image recognition tasks trained on a classic dataset such as ImageNet. Classic such models include AlexNet (2012) and ResNet (2015). These models will already have features that are useful to you and you will get better performance with less training time and fewer data. This is called transfer learning.\n\n\nHow transfer learning works\nTypically, you remove the last layer (for instance, with AlexNet, you would remove the classification layer), replace it with a layer suitable to your task, then, optionally, you can fine tune the model.\nFine tuning a model consists of freezing the first layers (fixing their weights and biases) while retraining the model with data specific to the new task. This will only train the last few layers, greatly reducing the size of the model actually being trained and taking advantage of the early features from the source model.\nI will talk about transfer learning in another workshop, but today, we are focusing on finding a suitable pre-trained model.\nNote that the most powerful recent transformers such as GPT-3 and 4 and their competitors perform well in different tasks without the need for re-training.\n\n\nHow to find a pre-trained model\nKey to transfer learning is the search for an appropriate source model. The great news is that the world of machine learning research is incredibly open: many teams make their papers and models available online. But you need a way to navigate this abundance of resource.\nThings you should probably care about when looking for a pre-trained model include:\n\nHow pertinent is the model relative to your task?\nDoes the model have an open license?\nIs the performance good?\nIs the model size suitable for the resources I have?",
    "crumbs": [
      "AI",
      "<b><em>Workshops</em></b>",
      "Finding pre-trained models"
    ]
  },
  {
    "objectID": "ai/pt/ws_pretrained_models.html#models-in-pytorch-libraries",
    "href": "ai/pt/ws_pretrained_models.html#models-in-pytorch-libraries",
    "title": "Finding pretrained models for transfer learning",
    "section": "Models in PyTorch libraries",
    "text": "Models in PyTorch libraries\nThe PyTorch ecosystem contains domain specific libraries (e.g. torchvision, torchtext, torchaudio). Among many domain specific utilities, these libraries contain many pretrained models in vision, text, and audio.\nThese models benefit from optimum convenience since they are entirely integrated into PyTorch.\n\nLoading ResNet-18 is as simple as:\n\nimport torchvision\nmodel = torchvision.models.resnet18()\n\nInitializing a pretrained ResNet-50 model with the best currently available weights is as simple as:\n\nfrom torchvision.models import resnet50, ResNet50_Weights\nmodel = resnet50(weights=ResNet50_Weights.DEFAULT)",
    "crumbs": [
      "AI",
      "<b><em>Workshops</em></b>",
      "Finding pre-trained models"
    ]
  },
  {
    "objectID": "ai/pt/ws_pretrained_models.html#pytorch-hub",
    "href": "ai/pt/ws_pretrained_models.html#pytorch-hub",
    "title": "Finding pretrained models for transfer learning",
    "section": "PyTorch Hub",
    "text": "PyTorch Hub\nPyTorch Hub is a repository of pretrained models.\n\nLoading ResNet-18 from the hub is done with:\n\nimport torch\nmodel = torch.hub.load('pytorch/vision', 'resnet18', pretrained=True)\n\n\nYour turn:\n\nLook for a small image classification model in the PyTorch Hub.",
    "crumbs": [
      "AI",
      "<b><em>Workshops</em></b>",
      "Finding pre-trained models"
    ]
  },
  {
    "objectID": "ai/pt/ws_pretrained_models.html#hugging-face",
    "href": "ai/pt/ws_pretrained_models.html#hugging-face",
    "title": "Finding pretrained models for transfer learning",
    "section": "Hugging Face",
    "text": "Hugging Face\nHugging Face, launched in 2016, provides a Model Hub. Let’s explore it together.\n\nNote that Hugging Face also has a Dataset Hub.\n\n\n\nYour turn:\n\nFind a pre-trained model for image classification in PyTorch, trained on ImageNet, with an open license, and less than 100MB in size.\n\n\ntimm\nFor computer vision specifically, the timm (PyTorch Image Models) library contains more than 700 pretrained models, as well as scripts, utilities, optimizers, data-loaders, etc. The repo can be found here.\nYou can load models from the Hugging Face Hub with:\nimport timm\nmodel = timm.create_model('hf_hub:author/model', pretrained=True)",
    "crumbs": [
      "AI",
      "<b><em>Workshops</em></b>",
      "Finding pre-trained models"
    ]
  },
  {
    "objectID": "ai/pt/ws_pretrained_models.html#github",
    "href": "ai/pt/ws_pretrained_models.html#github",
    "title": "Finding pretrained models for transfer learning",
    "section": "GitHub",
    "text": "GitHub\nA large number of open source models are hosted on GitHub and the platform can be searched directly for specific models.\n\n\nYour turn:\n\nDo a search on GitHub, trying to find pre-trained models in PyTorch for image classification.",
    "crumbs": [
      "AI",
      "<b><em>Workshops</em></b>",
      "Finding pre-trained models"
    ]
  },
  {
    "objectID": "ai/pt/ws_pretrained_models.html#literature",
    "href": "ai/pt/ws_pretrained_models.html#literature",
    "title": "Finding pretrained models for transfer learning",
    "section": "Literature",
    "text": "Literature\nWhile a less direct way to find pre-trained models, the literature is invaluable to (try to) keep up with what people are doing in the field.\nPapers With Code gathers machine learning papers with open source code.\narXiv is an open-source repository of scientific preprints created by Paul Ginsparg from Cornell University in 1991. It contains a huge number of e-prints on machine learning in the computer science and the statistics fields. arxiv-sanity, created by Andrej Karpathy, tracks arXiv machine learning papers and is easier to browse.",
    "crumbs": [
      "AI",
      "<b><em>Workshops</em></b>",
      "Finding pre-trained models"
    ]
  },
  {
    "objectID": "ai/pt/wb_upscaling_slides.html#can-be-broken-down-into-2-main-periods",
    "href": "ai/pt/wb_upscaling_slides.html#can-be-broken-down-into-2-main-periods",
    "title": "Super-resolution with PyTorch",
    "section": "Can be broken down into 2 main periods:",
    "text": "Can be broken down into 2 main periods:\n\nA rather slow history with various interpolation algorithms of increasing complexity before deep neural networks\nAn incredibly fast evolution since the advent of deep learning (DL)"
  },
  {
    "objectID": "ai/pt/wb_upscaling_slides.html#sr-history-pre-dl",
    "href": "ai/pt/wb_upscaling_slides.html#sr-history-pre-dl",
    "title": "Super-resolution with PyTorch",
    "section": "SR history Pre-DL",
    "text": "SR history Pre-DL\nPixel-wise interpolation prior to DL\nVarious methods ranging from simple (e.g. nearest-neighbour, bicubic) to complex (e.g. Gaussian process regression, iterative FIR Wiener filter) algorithms"
  },
  {
    "objectID": "ai/pt/wb_upscaling_slides.html#sr-history-pre-dl-1",
    "href": "ai/pt/wb_upscaling_slides.html#sr-history-pre-dl-1",
    "title": "Super-resolution with PyTorch",
    "section": "SR history Pre-DL",
    "text": "SR history Pre-DL\nNearest-neighbour interpolation\nSimplest method of interpolation\nSimply uses the value of the nearest pixel\nBicubic interpolation\nConsists of determining the 16 coefficients \\(a_{ij}\\) in:\n\\[p(x, y) = \\sum_{i=0}^3\\sum_{i=0}^3 a\\_{ij} x^i y^j\\]"
  },
  {
    "objectID": "ai/pt/wb_upscaling_slides.html#sr-history-with-dl",
    "href": "ai/pt/wb_upscaling_slides.html#sr-history-with-dl",
    "title": "Super-resolution with PyTorch",
    "section": "SR history with DL",
    "text": "SR history with DL\nDeep learning has seen a fast evolution marked by the successive emergence of various frameworks and architectures over the past 10 years\nSome key network architectures and frameworks:\n\nCNN\nGAN\nTransformers\n\nThese have all been applied to SR"
  },
  {
    "objectID": "ai/pt/wb_upscaling_slides.html#srcnn",
    "href": "ai/pt/wb_upscaling_slides.html#srcnn",
    "title": "Super-resolution with PyTorch",
    "section": "SRCNN",
    "text": "SRCNN\n\n\nDong, C., Loy, C. C., He, K., & Tang, X. (2015). Image super-resolution using deep convolutional networks. IEEE transactions on pattern analysis and machine intelligence, 38(2), 295-307\n\n\nGiven a low-resolution image Y, the first convolutional layer of the SRCNN extracts a set of feature maps. The second layer maps these feature maps nonlinearly to high-resolution patch representations. The last layer combines the predictions within a spatial neighbourhood to produce the final high-resolution image F(Y)"
  },
  {
    "objectID": "ai/pt/wb_upscaling_slides.html#srcnn-1",
    "href": "ai/pt/wb_upscaling_slides.html#srcnn-1",
    "title": "Super-resolution with PyTorch",
    "section": "SRCNN",
    "text": "SRCNN\nCan use sparse-coding-based methods\n\n\nDong, C., Loy, C. C., He, K., & Tang, X. (2015). Image super-resolution using deep convolutional networks. IEEE transactions on pattern analysis and machine intelligence, 38(2), 295-307"
  },
  {
    "objectID": "ai/pt/wb_upscaling_slides.html#srgan",
    "href": "ai/pt/wb_upscaling_slides.html#srgan",
    "title": "Super-resolution with PyTorch",
    "section": "SRGAN",
    "text": "SRGAN\nDo not provide the best PSNR, but can give more realistic results by providing more texture (less smoothing)"
  },
  {
    "objectID": "ai/pt/wb_upscaling_slides.html#gan",
    "href": "ai/pt/wb_upscaling_slides.html#gan",
    "title": "Super-resolution with PyTorch",
    "section": "GAN",
    "text": "GAN\n\n\nStevens E., Antiga L., & Viehmann T. (2020). Deep Learning with PyTorch"
  },
  {
    "objectID": "ai/pt/wb_upscaling_slides.html#srgan-1",
    "href": "ai/pt/wb_upscaling_slides.html#srgan-1",
    "title": "Super-resolution with PyTorch",
    "section": "SRGAN",
    "text": "SRGAN\n\n\nLedig, C., Theis, L., Huszár, F., Caballero, J., Cunningham, A., Acosta, A., … & Shi, W. (2017). Photo-realistic single image super-resolution using a generative adversarial network. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 4681-4690)"
  },
  {
    "objectID": "ai/pt/wb_upscaling_slides.html#srgan-2",
    "href": "ai/pt/wb_upscaling_slides.html#srgan-2",
    "title": "Super-resolution with PyTorch",
    "section": "SRGAN",
    "text": "SRGAN\nFollowed by the ESRGAN and many other flavours of SRGANs"
  },
  {
    "objectID": "ai/pt/wb_upscaling_slides.html#attention",
    "href": "ai/pt/wb_upscaling_slides.html#attention",
    "title": "Super-resolution with PyTorch",
    "section": "Attention",
    "text": "Attention\n\nMnih, V., Heess, N., & Graves, A. (2014). Recurrent models of visual attention. In Advances in neural information processing systems (pp. 2204-2212)\n\n(cited 2769 times)\n\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., … & Polosukhin, I. (2017). Attention is all you need. In Advances in neural information processing systems (pp. 5998-6008)\n\n(cited 30999 times…)"
  },
  {
    "objectID": "ai/pt/wb_upscaling_slides.html#transformers",
    "href": "ai/pt/wb_upscaling_slides.html#transformers",
    "title": "Super-resolution with PyTorch",
    "section": "Transformers",
    "text": "Transformers\n\n\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., … & Polosukhin, I. (2017). Attention is all you need. In Advances in neural information processing systems (pp. 5998-6008)"
  },
  {
    "objectID": "ai/pt/wb_upscaling_slides.html#transformers-1",
    "href": "ai/pt/wb_upscaling_slides.html#transformers-1",
    "title": "Super-resolution with PyTorch",
    "section": "Transformers",
    "text": "Transformers\nInitially used for NLP to replace RNN as they allow parallelization Now entering the domain of vision and others Very performant with relatively few parameters"
  },
  {
    "objectID": "ai/pt/wb_upscaling_slides.html#swin-transformer",
    "href": "ai/pt/wb_upscaling_slides.html#swin-transformer",
    "title": "Super-resolution with PyTorch",
    "section": "Swin Transformer",
    "text": "Swin Transformer\nThe Swin Transformer improved the use of transformers to the vision domain\nSwin = Shifted WINdows"
  },
  {
    "objectID": "ai/pt/wb_upscaling_slides.html#swin-transformer-1",
    "href": "ai/pt/wb_upscaling_slides.html#swin-transformer-1",
    "title": "Super-resolution with PyTorch",
    "section": "Swin Transformer",
    "text": "Swin Transformer\nSwin transformer (left) vs transformer as initially applied to vision (right):\n\n\nLiu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., … & Guo, B. (2021). Swin transformer: Hierarchical vision transformer using shifted windows. arXiv preprint arXiv:2103.14030"
  },
  {
    "objectID": "ai/pt/wb_upscaling_slides.html#swinir-1",
    "href": "ai/pt/wb_upscaling_slides.html#swinir-1",
    "title": "Super-resolution with PyTorch",
    "section": "SwinIR",
    "text": "SwinIR\n\n\nLiang, J., Cao, J., Sun, G., Zhang, K., Van Gool, L., & Timofte, R. (2021). SwinIR: Image restoration using swin transformer. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 1833-1844)"
  },
  {
    "objectID": "ai/pt/wb_upscaling_slides.html#training-sets-used",
    "href": "ai/pt/wb_upscaling_slides.html#training-sets-used",
    "title": "Super-resolution with PyTorch",
    "section": "Training sets used",
    "text": "Training sets used\nDIV2K, Flickr2K, and other datasets"
  },
  {
    "objectID": "ai/pt/wb_upscaling_slides.html#models-assessment",
    "href": "ai/pt/wb_upscaling_slides.html#models-assessment",
    "title": "Super-resolution with PyTorch",
    "section": "Models assessment",
    "text": "Models assessment\n3 metrics commonly used:\nPeak sign-to-noise ratio (PSNR) measured in dB\n\\(\\frac{\\text{Maximum possible power of signal}}{\\text{Power of noise (calculated as the mean squared error)}}\\)\nCalculated at the pixel level\nStructural similarity index measure (SSIM)\nPrediction of perceived image quality based on a “perfect” reference image\nMean opinion score (MOS)\nMean of subjective quality ratings"
  },
  {
    "objectID": "ai/pt/wb_upscaling_slides.html#models-assessment-1",
    "href": "ai/pt/wb_upscaling_slides.html#models-assessment-1",
    "title": "Super-resolution with PyTorch",
    "section": "Models assessment",
    "text": "Models assessment\nPeak sign-to-noise ratio (PSNR) measured in dB\n\\[PSNR = 10\\,\\cdot\\,log_{10}\\,\\left(\\frac{MAX_I^2}{MSE}\\right)\\]\nStructural similarity index measure (SSIM)\n\\[SSIM(x,y) = \\frac{(2\\mu_x\\mu_y + c_1) + (2 \\sigma _{xy} + c_2)}\n    {(\\mu_x^2 + \\mu_y^2+c_1) (\\sigma_x^2 + \\sigma_y^2+c_2)}\\]\nMean opinion score (MOS)\n\\[MOS = \\frac{\\sum_{n=1}^N R\\_n}{N}\\]"
  },
  {
    "objectID": "ai/pt/wb_upscaling_slides.html#metrics-implementation",
    "href": "ai/pt/wb_upscaling_slides.html#metrics-implementation",
    "title": "Super-resolution with PyTorch",
    "section": "Metrics implementation",
    "text": "Metrics implementation\n\nImplement them yourself (using torch.log10, etc.)\nUse some library that implements them (e.g. kornia)\nUse code of open source project with good implementation (e.g. SwinIR)\nUse some higher level library that provides them (e.g. ignite)"
  },
  {
    "objectID": "ai/pt/wb_upscaling_slides.html#metrics-implementation-1",
    "href": "ai/pt/wb_upscaling_slides.html#metrics-implementation-1",
    "title": "Super-resolution with PyTorch",
    "section": "Metrics implementation",
    "text": "Metrics implementation\n\nImplement them yourself (using torch.log10, etc.)\nUse some library that implements them (e.g. kornia)\nUse code of open source project with good implementation (e.g. SwinIR)\nUse some higher level library that provides them (e.g. ignite)"
  },
  {
    "objectID": "ai/pt/wb_upscaling_slides.html#metrics-implementation-2",
    "href": "ai/pt/wb_upscaling_slides.html#metrics-implementation-2",
    "title": "Super-resolution with PyTorch",
    "section": "Metrics implementation",
    "text": "Metrics implementation\nimport kornia\n\npsnr_value = kornia.metrics.psnr(input, target, max_val)\nssim_value = kornia.metrics.ssim(img1, img2, window_size, max_val=1.0, eps=1e-12)\nSee the Kornia documentation for more info on kornia.metrics.psnr & kornia.metrics.ssim"
  },
  {
    "objectID": "ai/pt/wb_upscaling_slides.html#benchmark-datasets",
    "href": "ai/pt/wb_upscaling_slides.html#benchmark-datasets",
    "title": "Super-resolution with PyTorch",
    "section": "Benchmark datasets",
    "text": "Benchmark datasets\nSet5\n\nSet14\n\nBSD100 (Berkeley Segmentation Dataset)"
  },
  {
    "objectID": "ai/pt/wb_upscaling_slides.html#benchmark-datasets-1",
    "href": "ai/pt/wb_upscaling_slides.html#benchmark-datasets-1",
    "title": "Super-resolution with PyTorch",
    "section": "Benchmark datasets",
    "text": "Benchmark datasets\nSet5\n\nSet14\n\nBSD100 (Berkeley Segmentation Dataset)"
  },
  {
    "objectID": "ai/pt/wb_upscaling_slides.html#the-set5-dataset",
    "href": "ai/pt/wb_upscaling_slides.html#the-set5-dataset",
    "title": "Super-resolution with PyTorch",
    "section": "The Set5 dataset",
    "text": "The Set5 dataset\nA dataset consisting of 5 images which has been used for at least 18 years to assess SR methods"
  },
  {
    "objectID": "ai/pt/wb_upscaling_slides.html#how-to-get-the-dataset",
    "href": "ai/pt/wb_upscaling_slides.html#how-to-get-the-dataset",
    "title": "Super-resolution with PyTorch",
    "section": "How to get the dataset?",
    "text": "How to get the dataset?\nFrom the HuggingFace Datasets Hub with the HuggingFace datasets package:\nfrom datasets import load_dataset\n\nset5 = load_dataset('eugenesiow/Set5', 'bicubic_x4', split='validation')"
  },
  {
    "objectID": "ai/pt/wb_upscaling_slides.html#dataset-exploration",
    "href": "ai/pt/wb_upscaling_slides.html#dataset-exploration",
    "title": "Super-resolution with PyTorch",
    "section": "Dataset exploration",
    "text": "Dataset exploration\nprint(set5)\nlen(set5)\nset5[0]\nset5.shape\nset5.column_names\nset5.features\nset5.set_format('torch', columns=['hr', 'lr'])\nset5.format"
  },
  {
    "objectID": "ai/pt/wb_upscaling_slides.html#benchmarks",
    "href": "ai/pt/wb_upscaling_slides.html#benchmarks",
    "title": "Super-resolution with PyTorch",
    "section": "Benchmarks",
    "text": "Benchmarks\nA 2012 review of interpolation methods for SR gives the metrics for a series of interpolation methods (using other datasets)"
  },
  {
    "objectID": "ai/pt/wb_upscaling_slides.html#interpolation-methods",
    "href": "ai/pt/wb_upscaling_slides.html#interpolation-methods",
    "title": "Super-resolution with PyTorch",
    "section": "Interpolation methods",
    "text": "Interpolation methods"
  },
  {
    "objectID": "ai/pt/wb_upscaling_slides.html#dl-methods",
    "href": "ai/pt/wb_upscaling_slides.html#dl-methods",
    "title": "Super-resolution with PyTorch",
    "section": "DL methods",
    "text": "DL methods\nThe Papers with Code website lists available benchmarks on Set5"
  },
  {
    "objectID": "ai/pt/wb_upscaling_slides.html#lets-use-swinir",
    "href": "ai/pt/wb_upscaling_slides.html#lets-use-swinir",
    "title": "Super-resolution with PyTorch",
    "section": "Let’s use SwinIR",
    "text": "Let’s use SwinIR\n# Get the model\ngit clone git@github.com:JingyunLiang/SwinIR.git\ncd SwinIR\n\n# Copy our test images in the repo\ncp -r &lt;some/path&gt;/my_tests /testsets/my_tests\n\n# Run the model on our images\npython main_test_swinir.py --tile 400 --task real_sr --scale 4 --large_model --model_path model_zoo/swinir/003_realSR_BSRGAN_DFOWMFC_s64w8_SwinIR-L_x4_GAN.pth --folder_lq testsets/my_tests\nRan in 9 min on my machine with one GPU and 32GB of RAM"
  },
  {
    "objectID": "ai/pt/wb_upscaling_slides.html#results",
    "href": "ai/pt/wb_upscaling_slides.html#results",
    "title": "Super-resolution with PyTorch",
    "section": "Results",
    "text": "Results"
  },
  {
    "objectID": "ai/pt/wb_upscaling_slides.html#results-1",
    "href": "ai/pt/wb_upscaling_slides.html#results-1",
    "title": "Super-resolution with PyTorch",
    "section": "Results",
    "text": "Results"
  },
  {
    "objectID": "ai/pt/wb_upscaling_slides.html#results-2",
    "href": "ai/pt/wb_upscaling_slides.html#results-2",
    "title": "Super-resolution with PyTorch",
    "section": "Results",
    "text": "Results"
  },
  {
    "objectID": "ai/pt/wb_upscaling_slides.html#results-3",
    "href": "ai/pt/wb_upscaling_slides.html#results-3",
    "title": "Super-resolution with PyTorch",
    "section": "Results",
    "text": "Results"
  },
  {
    "objectID": "ai/pt/wb_upscaling_slides.html#results-4",
    "href": "ai/pt/wb_upscaling_slides.html#results-4",
    "title": "Super-resolution with PyTorch",
    "section": "Results",
    "text": "Results"
  },
  {
    "objectID": "ai/pt/wb_upscaling_slides.html#results-5",
    "href": "ai/pt/wb_upscaling_slides.html#results-5",
    "title": "Super-resolution with PyTorch",
    "section": "Results",
    "text": "Results"
  },
  {
    "objectID": "ai/pt/wb_upscaling_slides.html#results-6",
    "href": "ai/pt/wb_upscaling_slides.html#results-6",
    "title": "Super-resolution with PyTorch",
    "section": "Results",
    "text": "Results"
  },
  {
    "objectID": "ai/pt/wb_upscaling_slides.html#results-7",
    "href": "ai/pt/wb_upscaling_slides.html#results-7",
    "title": "Super-resolution with PyTorch",
    "section": "Results",
    "text": "Results"
  },
  {
    "objectID": "ai/pt/wb_upscaling_slides.html#results-8",
    "href": "ai/pt/wb_upscaling_slides.html#results-8",
    "title": "Super-resolution with PyTorch",
    "section": "Results",
    "text": "Results"
  },
  {
    "objectID": "ai/pt/wb_upscaling_slides.html#results-9",
    "href": "ai/pt/wb_upscaling_slides.html#results-9",
    "title": "Super-resolution with PyTorch",
    "section": "Results",
    "text": "Results"
  },
  {
    "objectID": "ai/pt/wb_upscaling_slides.html#metrics",
    "href": "ai/pt/wb_upscaling_slides.html#metrics",
    "title": "Super-resolution with PyTorch",
    "section": "Metrics",
    "text": "Metrics\nWe could use the PSNR and SSIM implementations from SwinIR, but let’s try the Kornia functions we mentioned earlier:\n\nkornia.metrics.psnr\nkornia.metrics.ssim"
  },
  {
    "objectID": "ai/pt/wb_upscaling_slides.html#metrics-1",
    "href": "ai/pt/wb_upscaling_slides.html#metrics-1",
    "title": "Super-resolution with PyTorch",
    "section": "Metrics",
    "text": "Metrics\nLet’s load the libraries we need:\nimport kornia\nfrom PIL import Image\nimport torch\nfrom torchvision import transforms"
  },
  {
    "objectID": "ai/pt/wb_upscaling_slides.html#metrics-2",
    "href": "ai/pt/wb_upscaling_slides.html#metrics-2",
    "title": "Super-resolution with PyTorch",
    "section": "Metrics",
    "text": "Metrics\nThen, we load one pair images (LR and HR):\nberlin1_lr = Image.open(\"&lt;some/path&gt;/lr/berlin_1945_1.jpg\")\nberlin1_hr = Image.open(\"&lt;some/path&gt;/hr/berlin_1945_1.png\")\n We can display these images with:\nberlin1_lr.show()\nberlin1_hr.show()"
  },
  {
    "objectID": "ai/pt/wb_upscaling_slides.html#metrics-3",
    "href": "ai/pt/wb_upscaling_slides.html#metrics-3",
    "title": "Super-resolution with PyTorch",
    "section": "Metrics",
    "text": "Metrics\nNow, we need to resize them so that they have identical dimensions and turn them into tensors:\npreprocess = transforms.Compose([\n        transforms.Resize(256),\n        transforms.ToTensor()\n        ])\n\nberlin1_lr_t = preprocess(berlin1_lr)\nberlin1_hr_t = preprocess(berlin1_hr)"
  },
  {
    "objectID": "ai/pt/wb_upscaling_slides.html#metrics-4",
    "href": "ai/pt/wb_upscaling_slides.html#metrics-4",
    "title": "Super-resolution with PyTorch",
    "section": "Metrics",
    "text": "Metrics\nberlin1_lr_t.shape\nberlin1_hr_t.shape\ntorch.Size([3, 267, 256])\ntorch.Size([3, 267, 256])\nWe now have tensors with 3 dimensions:\n\nthe channels (RGB)\nthe height of the image (in pixels)\nthe width of the image (in pixels)"
  },
  {
    "objectID": "ai/pt/wb_upscaling_slides.html#metrics-5",
    "href": "ai/pt/wb_upscaling_slides.html#metrics-5",
    "title": "Super-resolution with PyTorch",
    "section": "Metrics",
    "text": "Metrics\nAs data processing is done in batch in ML, we need to add a 4th dimension: the batch size\n(It will be equal to 1 since we have a batch size of a single image)\nbatch_berlin1_lr_t = torch.unsqueeze(berlin1_lr_t, 0)\nbatch_berlin1_hr_t = torch.unsqueeze(berlin1_hr_t, 0)"
  },
  {
    "objectID": "ai/pt/wb_upscaling_slides.html#metrics-6",
    "href": "ai/pt/wb_upscaling_slides.html#metrics-6",
    "title": "Super-resolution with PyTorch",
    "section": "Metrics",
    "text": "Metrics\nOur new tensors are now ready:\nbatch_berlin1_lr_t.shape\nbatch_berlin1_hr_t.shape\ntorch.Size([1, 3, 267, 256])\ntorch.Size([1, 3, 267, 256])"
  },
  {
    "objectID": "ai/pt/wb_upscaling_slides.html#psnr",
    "href": "ai/pt/wb_upscaling_slides.html#psnr",
    "title": "Super-resolution with PyTorch",
    "section": "PSNR",
    "text": "PSNR\npsnr_value = kornia.metrics.psnr(batch_berlin1_lr_t, batch_berlin1_hr_t, max_val=1.0)\npsnr_value.item()\n33.379642486572266"
  },
  {
    "objectID": "ai/pt/wb_upscaling_slides.html#ssim",
    "href": "ai/pt/wb_upscaling_slides.html#ssim",
    "title": "Super-resolution with PyTorch",
    "section": "SSIM",
    "text": "SSIM\nssim_map = kornia.metrics.ssim(\n    batch_berlin1_lr_t, batch_berlin1_hr_t, window_size=5, max_val=1.0, eps=1e-12)\n\nssim_map.mean().item()\n0.9868119359016418"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#acknowledgements",
    "href": "ai/pt/wb_torchtensors_slides.html#acknowledgements",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nMany drawings in this webinar come from the book:\n\nThe section on storage is also highly inspired by it"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#using-tensors-locally",
    "href": "ai/pt/wb_torchtensors_slides.html#using-tensors-locally",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Using tensors locally",
    "text": "Using tensors locally\nYou need to have Python and PyTorch installed\nAdditionally, you might want to use an IDE such as elpy if you are an Emacs user, JupyterLab, etc.\n\nNote that PyTorch does not yet support Python 3.10 except in some Linux distributions or on systems where a wheel has been built For the time being, you might have to use it with Python 3.9"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#using-tensors-on-cc-clusters",
    "href": "ai/pt/wb_torchtensors_slides.html#using-tensors-on-cc-clusters",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Using tensors on CC clusters",
    "text": "Using tensors on CC clusters\n(In the terminal)\nList available wheels and compatible Python versions:\navail_wheels \"torch*\"\nList available Python versions:\nmodule avail python\nGet setup:\nmodule load python/3.9.6             # Load a sensible Python version\nvirtualenv --no-download env         # Create a virtual env\nsource env/bin/activate              # Activate the virtual env\npip install --no-index --upgrade pip # Update pip\npip install --no-index torch         # Install PyTorch\nYou can then launch jobs with sbatch or salloc\nLeave the virtual env with the command: deactivate"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#outline",
    "href": "ai/pt/wb_torchtensors_slides.html#outline",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Outline",
    "text": "Outline\n\nWhat is a PyTorch tensor?\nMemory storage\nData type (dtype)\nBasic operations\nWorking with NumPy\nLinear algebra\nHarvesting the power of GPUs\nDistributed operations"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#outline-1",
    "href": "ai/pt/wb_torchtensors_slides.html#outline-1",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Outline",
    "text": "Outline\n\nWhat is a PyTorch tensor?\nMemory storage\nData type (dtype)\nBasic operations\nWorking with NumPy\nLinear algebra\nHarvesting the power of GPUs\nDistributed operations"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#ann-do-not-process-information-directly",
    "href": "ai/pt/wb_torchtensors_slides.html#ann-do-not-process-information-directly",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "ANN do not process information directly",
    "text": "ANN do not process information directly\n\n\nModified from Stevens, E., Antiga, L., & Viehmann, T. (2020). Deep learning with PyTorch. Manning Publications"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#it-needs-to-be-converted-to-numbers",
    "href": "ai/pt/wb_torchtensors_slides.html#it-needs-to-be-converted-to-numbers",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "It needs to be converted to numbers",
    "text": "It needs to be converted to numbers\n\n\nModified from Stevens, E., Antiga, L., & Viehmann, T. (2020). Deep learning with PyTorch. Manning Publications"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#these-numbers-must-be-stored-in-a-data-structure",
    "href": "ai/pt/wb_torchtensors_slides.html#these-numbers-must-be-stored-in-a-data-structure",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "These numbers must be stored in a data structure",
    "text": "These numbers must be stored in a data structure\n\nPyTorch tensors are Python objects holding multidimensional arrays\n\n\n\nStevens, E., Antiga, L., & Viehmann, T. (2020). Deep learning with PyTorch. Manning Publications"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#why-a-new-object-when-numpy-already-exists",
    "href": "ai/pt/wb_torchtensors_slides.html#why-a-new-object-when-numpy-already-exists",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Why a new object when NumPy already exists?",
    "text": "Why a new object when NumPy already exists?\n\n\nCan run on accelerators (GPUs, TPUs…)\nKeep track of computation graphs, allowing automatic differentiation\nFuture plan for sharded tensors to run distributed computations"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#what-is-a-pytorch-tensor-2",
    "href": "ai/pt/wb_torchtensors_slides.html#what-is-a-pytorch-tensor-2",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "What is a PyTorch tensor?",
    "text": "What is a PyTorch tensor?\nPyTorch is foremost a deep learning library\nIn deep learning, the information contained in objects of interest (e.g. images, texts, sounds) is converted to floating-point numbers (e.g. pixel values, token values, frequencies)\nAs this information is complex, multiple dimensions are required (e.g. two dimensions for the width and height of an image, plus one dimension for the RGB colour channels)\nAdditionally, items are grouped into batches to be processed together, adding yet another dimension\nMultidimensional arrays are thus particularly well suited for deep learning"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#what-is-a-pytorch-tensor-3",
    "href": "ai/pt/wb_torchtensors_slides.html#what-is-a-pytorch-tensor-3",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "What is a PyTorch tensor?",
    "text": "What is a PyTorch tensor?\nArtificial neurons perform basic computations on these tensors\nTheir number however is huge and computing efficiency is paramount\nGPUs/TPUs are particularly well suited to perform many simple operations in parallel\nThe very popular NumPy library has, at its core, a mature multidimensional array object well integrated into the scientific Python ecosystem\nBut the PyTorch tensor has additional efficiency characteristics ideal for machine learning and it can be converted to/from NumPy’s ndarray if needed"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#outline-2",
    "href": "ai/pt/wb_torchtensors_slides.html#outline-2",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Outline",
    "text": "Outline\n\nWhat is a PyTorch tensor?\nMemory storage\nData type (dtype)\nBasic operations\nWorking with NumPy\nLinear algebra\nHarvesting the power of GPUs\nDistributed operations"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#efficient-memory-storage",
    "href": "ai/pt/wb_torchtensors_slides.html#efficient-memory-storage",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Efficient memory storage",
    "text": "Efficient memory storage\nIn Python, collections (lists, tuples) are groupings of boxed Python objects\nPyTorch tensors and NumPy ndarrays are made of unboxed C numeric types\n\n\nStevens, E., Antiga, L., & Viehmann, T. (2020). Deep learning with PyTorch. Manning Publications"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#efficient-memory-storage-1",
    "href": "ai/pt/wb_torchtensors_slides.html#efficient-memory-storage-1",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Efficient memory storage",
    "text": "Efficient memory storage\nThey are usually contiguous memory blocks, but the main difference is that they are unboxed: floats will thus take 4 (32-bit) or 8 (64-bit) bytes each\nBoxed values take up more memory (memory for the pointer + memory for the primitive)\n\n\nStevens, E., Antiga, L., & Viehmann, T. (2020). Deep learning with PyTorch. Manning Publications"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#implementation",
    "href": "ai/pt/wb_torchtensors_slides.html#implementation",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Implementation",
    "text": "Implementation\nUnder the hood, the values of a PyTorch tensor are stored as a torch.Storage instance which is a one-dimensional array\n\nimport torch\nt = torch.arange(10.).view(2, 5); print(t) # Functions explained later\ntensor([[ 0.,  1.,  2., 3.,  4.],\n        [ 5.,  6.,  7.,  8.,  9.]])"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#implementation-1",
    "href": "ai/pt/wb_torchtensors_slides.html#implementation-1",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Implementation",
    "text": "Implementation\nstorage = t.storage(); print(storage)\n 0.0\n 1.0\n 2.0\n 3.0\n 4.0\n 5.0\n 6.0\n 7.0\n 8.0\n 9.0\n[torch.FloatStorage of size 10]"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#implementation-2",
    "href": "ai/pt/wb_torchtensors_slides.html#implementation-2",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Implementation",
    "text": "Implementation\nThe storage can be indexed\nstorage[3]\n3.0"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#implementation-3",
    "href": "ai/pt/wb_torchtensors_slides.html#implementation-3",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Implementation",
    "text": "Implementation\nstorage[3] = 10.0; print(storage)\n 0.0\n 1.0\n 2.0\n 10.0\n 4.0\n 5.0\n 6.0\n 7.0\n 8.0\n 9.0\n[torch.FloatStorage of size 10]"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#implementation-4",
    "href": "ai/pt/wb_torchtensors_slides.html#implementation-4",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Implementation",
    "text": "Implementation\nTo view a multidimensional array from storage, we need metadata:\n\nthe size (shape in NumPy) sets the number of elements in each dimension\nthe offset indicates where the first element of the tensor is in the storage\nthe stride establishes the increment between each element"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#storage-metadata",
    "href": "ai/pt/wb_torchtensors_slides.html#storage-metadata",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Storage metadata",
    "text": "Storage metadata\n\n\nStevens, E., Antiga, L., & Viehmann, T. (2020). Deep learning with PyTorch. Manning Publications"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#storage-metadata-1",
    "href": "ai/pt/wb_torchtensors_slides.html#storage-metadata-1",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Storage metadata",
    "text": "Storage metadata\nt.size()\nt.storage_offset()\nt.stride()\ntorch.Size([2, 5])\n0\n(5, 1)"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#storage-metadata-2",
    "href": "ai/pt/wb_torchtensors_slides.html#storage-metadata-2",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Storage metadata",
    "text": "Storage metadata"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#sharing-storage",
    "href": "ai/pt/wb_torchtensors_slides.html#sharing-storage",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Sharing storage",
    "text": "Sharing storage\nMultiple tensors can use the same storage, saving a lot of memory since the metadata is a lot lighter than a whole new array\n\n\nStevens, E., Antiga, L., & Viehmann, T. (2020). Deep learning with PyTorch. Manning Publications"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#transposing-in-2-dimensions",
    "href": "ai/pt/wb_torchtensors_slides.html#transposing-in-2-dimensions",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Transposing in 2 dimensions",
    "text": "Transposing in 2 dimensions\nt = torch.tensor([[3, 1, 2], [4, 1, 7]]); print(t)\nt.size()\nt.t()\nt.t().size()\ntensor([[3, 1, 2],\n        [4, 1, 7]])\ntorch.Size([2, 3])\ntensor([[3, 4],\n        [1, 1],\n        [2, 7]])\ntorch.Size([3, 2])"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#transposing-in-2-dimensions-1",
    "href": "ai/pt/wb_torchtensors_slides.html#transposing-in-2-dimensions-1",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Transposing in 2 dimensions",
    "text": "Transposing in 2 dimensions\n= flipping the stride elements around\n\n\nStevens, E., Antiga, L., & Viehmann, T. (2020). Deep learning with PyTorch. Manning Publications"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#transposing-in-higher-dimensions",
    "href": "ai/pt/wb_torchtensors_slides.html#transposing-in-higher-dimensions",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Transposing in higher dimensions",
    "text": "Transposing in higher dimensions\ntorch.t() is a shorthand for torch.transpose(0, 1):\ntorch.equal(t.t(), t.transpose(0, 1))\nTrue\nWhile torch.t() only works for 2D tensors, torch.transpose() can be used to transpose 2 dimensions in tensors of any number of dimensions"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#transposing-in-higher-dimensions-1",
    "href": "ai/pt/wb_torchtensors_slides.html#transposing-in-higher-dimensions-1",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Transposing in higher dimensions",
    "text": "Transposing in higher dimensions\nt = torch.zeros(1, 2, 3); print(t)\n\nt.size()\nt.stride()\ntensor([[[0., 0., 0.],\n         [0., 0., 0.]]])\n\ntorch.Size([1, 2, 3])\n(6, 3, 1)"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#transposing-in-higher-dimensions-2",
    "href": "ai/pt/wb_torchtensors_slides.html#transposing-in-higher-dimensions-2",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Transposing in higher dimensions",
    "text": "Transposing in higher dimensions\nt.transpose(0, 1)\n\nt.transpose(0, 1).size()\nt.transpose(0, 1).stride()\ntensor([[[0., 0., 0.]],\n        [[0., 0., 0.]]])\n\ntorch.Size([2, 1, 3])\n(3, 6, 1)  # Notice how transposing flipped 2 elements of the stride"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#transposing-in-higher-dimensions-3",
    "href": "ai/pt/wb_torchtensors_slides.html#transposing-in-higher-dimensions-3",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Transposing in higher dimensions",
    "text": "Transposing in higher dimensions\nt.transpose(0, 2)\n\nt.transpose(0, 2).size()\nt.transpose(0, 2).stride()\ntensor([[[0.],\n         [0.]],\n        [[0.],\n         [0.]],\n        [[0.],\n         [0.]]])\n\ntorch.Size([3, 2, 1])\n(1, 3, 6)"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#transposing-in-higher-dimensions-4",
    "href": "ai/pt/wb_torchtensors_slides.html#transposing-in-higher-dimensions-4",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Transposing in higher dimensions",
    "text": "Transposing in higher dimensions\nt.transpose(1, 2)\n\nt.transpose(1, 2).size()\nt.transpose(1, 2).stride()\ntensor([[[0., 0.],\n         [0., 0.],\n         [0., 0.]]])\n\ntorch.Size([1, 3, 2])\n(6, 1, 3)"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#outline-3",
    "href": "ai/pt/wb_torchtensors_slides.html#outline-3",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Outline",
    "text": "Outline\n\nWhat is a PyTorch tensor?\nMemory storage\nData type (dtype)\nBasic operations\nWorking with NumPy\nLinear algebra\nHarvesting the power of GPUs\nDistributed operations"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#default-dtype",
    "href": "ai/pt/wb_torchtensors_slides.html#default-dtype",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Default dtype",
    "text": "Default dtype\nSince PyTorch tensors were built with utmost efficiency in mind for neural networks, the default data type is 32-bit floating points\nThis is sufficient for accuracy and much faster than 64-bit floating points\n\nNote that, by contrast, NumPy ndarrays use 64-bit as their default"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#list-of-pytorch-tensor-dtypes",
    "href": "ai/pt/wb_torchtensors_slides.html#list-of-pytorch-tensor-dtypes",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "List of PyTorch tensor dtypes",
    "text": "List of PyTorch tensor dtypes\n\n\n\ntorch.float16 / torch.half\n\n\n  \n\n\n16-bit / half-precision floating-point\n\n\n\n\ntorch.float32 / torch.float\n\n\n\n\n32-bit / single-precision floating-point\n\n\n\n\ntorch.float64 / torch.double\n\n\n\n\n64-bit / double-precision floating-point\n\n\n\n\n\n\n\n\n\n\ntorch.uint8\n\n\n\n\nunsigned 8-bit integers\n\n\n\n\ntorch.int8\n\n\n\n\nsigned 8-bit integers\n\n\n\n\ntorch.int16 / torch.short\n\n\n\n\nsigned 16-bit integers\n\n\n\n\ntorch.int32 / torch.int\n\n\n\n\nsigned 32-bit integers\n\n\n\n\ntorch.int64 / torch.long\n\n\n\n\nsigned 64-bit integers\n\n\n\n\n\n\n\n\n\n\n\n\ntorch.bool\n\n\n\n\nboolean"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#checking-and-changing-dtype",
    "href": "ai/pt/wb_torchtensors_slides.html#checking-and-changing-dtype",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Checking and changing dtype",
    "text": "Checking and changing dtype\nt = torch.rand(2, 3)\nprint(t)\n\n# Remember that the default dtype for PyTorch tensors is float32\nt.dtype\n\n# If dtype ≠ default, it is printed\nt2 = t.type(torch.float64)\nprint(t2)\n\nt2.dtype\ntensor([[0.8130, 0.3757, 0.7682],\n        [0.3482, 0.0516, 0.3772]])\n\ntorch.float32\n\ntensor([[0.8130, 0.3757, 0.7682],\n        [0.3482, 0.0516, 0.3772]], dtype=torch.float64)\n\ntorch.float64"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#outline-4",
    "href": "ai/pt/wb_torchtensors_slides.html#outline-4",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Outline",
    "text": "Outline\n\nWhat is a PyTorch tensor?\nMemory storage\nData type (dtype)\nBasic operations\nWorking with NumPy\nLinear algebra\nHarvesting the power of GPUs\nDistributed operations"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#creating-tensors",
    "href": "ai/pt/wb_torchtensors_slides.html#creating-tensors",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Creating tensors",
    "text": "Creating tensors\n\ntorch.tensor:    Input individual values\ntorch.arange:    Similar to range but creates a 1D tensor\ntorch.linspace:   1D linear scale tensor\ntorch.logspace:   1D log scale tensor\ntorch.rand:     Random numbers from a uniform distribution on [0, 1)\ntorch.randn:    Numbers from the standard normal distribution\ntorch.randperm:   Random permutation of integers\ntorch.empty:    Uninitialized tensor\ntorch.zeros:    Tensor filled with 0\ntorch.ones:     Tensor filled with 1\ntorch.eye:      Identity matrix"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#creating-tensors-1",
    "href": "ai/pt/wb_torchtensors_slides.html#creating-tensors-1",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Creating tensors",
    "text": "Creating tensors\ntorch.manual_seed(0)  # If you want to reproduce the result\ntorch.rand(1)\n\ntorch.manual_seed(0)  # Run before each operation to get the same result\ntorch.rand(1).item()  # Extract the value from a tensor\ntensor([0.4963])\n\n0.49625658988952637"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#creating-tensors-2",
    "href": "ai/pt/wb_torchtensors_slides.html#creating-tensors-2",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Creating tensors",
    "text": "Creating tensors\ntorch.rand(1)\ntorch.rand(1, 1)\ntorch.rand(1, 1, 1)\ntorch.rand(1, 1, 1, 1)\ntensor([0.6984])\ntensor([[0.5675]])\ntensor([[[0.8352]]])\ntensor([[[[0.2056]]]])"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#creating-tensors-3",
    "href": "ai/pt/wb_torchtensors_slides.html#creating-tensors-3",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Creating tensors",
    "text": "Creating tensors\ntorch.rand(2)\ntorch.rand(2, 2, 2, 2)\ntensor([0.5932, 0.1123])\ntensor([[[[0.1147, 0.3168],\n          [0.6965, 0.9143]],\n         [[0.9351, 0.9412],\n          [0.5995, 0.0652]]],\n        [[[0.5460, 0.1872],\n          [0.0340, 0.9442]],\n         [[0.8802, 0.0012],\n          [0.5936, 0.4158]]]])"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#creating-tensors-4",
    "href": "ai/pt/wb_torchtensors_slides.html#creating-tensors-4",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Creating tensors",
    "text": "Creating tensors\ntorch.rand(2)\ntorch.rand(3)\ntorch.rand(1, 1)\ntorch.rand(1, 1, 1)\ntorch.rand(2, 6)\ntensor([0.7682, 0.0885])\ntensor([0.1320, 0.3074, 0.6341])\ntensor([[0.4901]])\ntensor([[[0.8964]]])\ntensor([[0.4556, 0.6323, 0.3489, 0.4017, 0.0223, 0.1689],\n        [0.2939, 0.5185, 0.6977, 0.8000, 0.1610, 0.2823]])"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#creating-tensors-5",
    "href": "ai/pt/wb_torchtensors_slides.html#creating-tensors-5",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Creating tensors",
    "text": "Creating tensors\ntorch.rand(2, 4, dtype=torch.float64)  # You can set dtype\ntorch.ones(2, 1, 4, 5)\ntensor([[0.6650, 0.7849, 0.2104, 0.6767],\n        [0.1097, 0.5238, 0.2260, 0.5582]], dtype=torch.float64)\ntensor([[[[1., 1., 1., 1., 1.],\n          [1., 1., 1., 1., 1.],\n          [1., 1., 1., 1., 1.],\n          [1., 1., 1., 1., 1.]]],\n        [[[1., 1., 1., 1., 1.],\n          [1., 1., 1., 1., 1.],\n          [1., 1., 1., 1., 1.],\n          [1., 1., 1., 1., 1.]]]])"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#creating-tensors-6",
    "href": "ai/pt/wb_torchtensors_slides.html#creating-tensors-6",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Creating tensors",
    "text": "Creating tensors\nt = torch.rand(2, 3); print(t)\ntorch.zeros_like(t)             # Matches the size of t\ntorch.ones_like(t)\ntorch.randn_like(t)\ntensor([[0.4051, 0.6394, 0.0871],\n        [0.4509, 0.5255, 0.5057]])\ntensor([[0., 0., 0.],\n        [0., 0., 0.]])\ntensor([[1., 1., 1.],\n        [1., 1., 1.]])\ntensor([[-0.3088, -0.0104,  1.0461],\n        [ 0.9233,  0.0236, -2.1217]])"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#creating-tensors-7",
    "href": "ai/pt/wb_torchtensors_slides.html#creating-tensors-7",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Creating tensors",
    "text": "Creating tensors\ntorch.arange(2, 10, 4)    # From 2 to 10 in increments of 4\ntorch.linspace(2, 10, 4)  # 4 elements from 2 to 10 on the linear scale\ntorch.logspace(2, 10, 4)  # Same on the log scale\ntorch.randperm(4)\ntorch.eye(3)\ntensor([2, 6])\ntensor([2.0000,  4.6667,  7.3333, 10.0000])\ntensor([1.0000e+02, 4.6416e+04, 2.1544e+07, 1.0000e+10])\ntensor([1, 3, 2, 0])\ntensor([[1., 0., 0.],\n        [0., 1., 0.],\n        [0., 0., 1.]])"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#tensor-information",
    "href": "ai/pt/wb_torchtensors_slides.html#tensor-information",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Tensor information",
    "text": "Tensor information\nt = torch.rand(2, 3); print(t)\nt.size()\nt.dim()\nt.numel()\ntensor([[0.5885, 0.7005, 0.1048],\n        [0.1115, 0.7526, 0.0658]])\ntorch.Size([2, 3])\n2\n6"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#tensor-indexing",
    "href": "ai/pt/wb_torchtensors_slides.html#tensor-indexing",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Tensor indexing",
    "text": "Tensor indexing\nx = torch.rand(3, 4)\nx[:]                 # With a range, the comma is implicit: same as x[:, ]\nx[:, 2]\nx[1, :]\nx[2, 3]\ntensor([[0.6575, 0.4017, 0.7391, 0.6268],\n        [0.2835, 0.0993, 0.7707, 0.1996],\n        [0.4447, 0.5684, 0.2090, 0.7724]])\ntensor([0.7391, 0.7707, 0.2090])\ntensor([0.2835, 0.0993, 0.7707, 0.1996])\ntensor(0.7724)"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#tensor-indexing-1",
    "href": "ai/pt/wb_torchtensors_slides.html#tensor-indexing-1",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Tensor indexing",
    "text": "Tensor indexing\nx[-1:]        # Last element (implicit comma, so all columns)\n\n# No range, no implicit comma\n# Indexing from a list of tensors, so the result is a one dimensional tensor\n# (Each dimension is a list of tensors of the previous dimension)\nx[-1]\n\nx[-1].size()  # Same number of dimensions than x (2 dimensions)\n\nx[-1:].size() # We dropped one dimension\ntensor([[0.8168, 0.0879, 0.2642, 0.3777]])\n\ntensor([0.8168, 0.0879, 0.2642, 0.3777])\n\ntorch.Size([4])\n\ntorch.Size([1, 4])"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#tensor-indexing-2",
    "href": "ai/pt/wb_torchtensors_slides.html#tensor-indexing-2",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Tensor indexing",
    "text": "Tensor indexing\nx[0:1]     # Python ranges are inclusive to the left, not the right\nx[:-1]     # From start to one before last (and implicit comma)\nx[0:3:2]   # From 0th (included) to 3rd (excluded) in increment of 2\ntensor([[0.5873, 0.0225, 0.7234, 0.4538]])\ntensor([[0.5873, 0.0225, 0.7234, 0.4538],\n        [0.9525, 0.0111, 0.6421, 0.4647]])\ntensor([[0.5873, 0.0225, 0.7234, 0.4538],\n        [0.8168, 0.0879, 0.2642, 0.3777]])"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#tensor-indexing-3",
    "href": "ai/pt/wb_torchtensors_slides.html#tensor-indexing-3",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Tensor indexing",
    "text": "Tensor indexing\nx[None]          # Adds a dimension of size one as the 1st dimension\nx.size()\nx[None].size()\ntensor([[[0.5873, 0.0225, 0.7234, 0.4538],\n         [0.9525, 0.0111, 0.6421, 0.4647],\n         [0.8168, 0.0879, 0.2642, 0.3777]]])\ntorch.Size([3, 4])\ntorch.Size([1, 3, 4])"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#a-word-of-caution-about-indexing",
    "href": "ai/pt/wb_torchtensors_slides.html#a-word-of-caution-about-indexing",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "A word of caution about indexing",
    "text": "A word of caution about indexing\nWhile indexing elements of a tensor to extract some of the data as a final step of some computation is fine, you should not use indexing to run operations on tensor elements in a loop as this would be extremely inefficient\nInstead, you want to use vectorized operations"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#vectorized-operations",
    "href": "ai/pt/wb_torchtensors_slides.html#vectorized-operations",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Vectorized operations",
    "text": "Vectorized operations\nSince PyTorch tensors are homogeneous (i.e. made of a single data type), as with NumPy’s ndarrays, operations are vectorized and thus staggeringly fast\nNumPy is mostly written in C and PyTorch in C++. With either library, when you run vectorized operations on arrays/tensors, you don’t use raw Python (slow) but compiled C/C++ code (much faster)\nHere is an excellent post explaining Python vectorization and why it makes such a big difference"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#vectorized-operations-comparison",
    "href": "ai/pt/wb_torchtensors_slides.html#vectorized-operations-comparison",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Vectorized operations: comparison",
    "text": "Vectorized operations: comparison\nRaw Python method\n# Create tensor. We use float64 here to avoid truncation errors\nt = torch.rand(10**6, dtype=torch.float64)\n\n# Initialize sum\nsum = 0\n\n# Run loop\nfor i in range(len(t)): sum += t[i]\n\n# Print result\nprint(sum)\nVectorized function\nt.sum()"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#vectorized-operations-comparison-1",
    "href": "ai/pt/wb_torchtensors_slides.html#vectorized-operations-comparison-1",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Vectorized operations: comparison",
    "text": "Vectorized operations: comparison\nBoth methods give the same result\n\nThis is why we used float64:\nWhile the accuracy remains excellent with float32 if we use the PyTorch function torch.sum(), the raw Python loop gives a fairly inaccurate result\n\ntensor(500023.0789, dtype=torch.float64)\ntensor(500023.0789, dtype=torch.float64)"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#vectorized-operations-timing",
    "href": "ai/pt/wb_torchtensors_slides.html#vectorized-operations-timing",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Vectorized operations: timing",
    "text": "Vectorized operations: timing\nLet’s compare the timing with PyTorch built-in benchmark utility\n# Load utility\nimport torch.utils.benchmark as benchmark\n\n# Create a function for our loop\ndef sum_loop(t, sum):\n    for i in range(len(t)): sum += t[i]"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#vectorized-operations-timing-1",
    "href": "ai/pt/wb_torchtensors_slides.html#vectorized-operations-timing-1",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Vectorized operations: timing",
    "text": "Vectorized operations: timing\nNow we can create the timers\nt0 = benchmark.Timer(\n    stmt='sum_loop(t, sum)',\n    setup='from __main__ import sum_loop',\n    globals={'t': t, 'sum': sum})\n\nt1 = benchmark.Timer(\n    stmt='t.sum()',\n    globals={'t': t})"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#vectorized-operations-timing-2",
    "href": "ai/pt/wb_torchtensors_slides.html#vectorized-operations-timing-2",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Vectorized operations: timing",
    "text": "Vectorized operations: timing\nLet’s time 100 runs to have a reliable benchmark\nprint(t0.timeit(100))\nprint(t1.timeit(100))\n\nI ran the code on my laptop with a dedicated GPU and 32GB RAM"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#vectorized-operations-timing-3",
    "href": "ai/pt/wb_torchtensors_slides.html#vectorized-operations-timing-3",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Vectorized operations: timing",
    "text": "Vectorized operations: timing\nTiming of raw Python loop\nsum_loop(t, sum)\nsetup: from __main__ import sum_loop\n  1.37 s\n  1 measurement, 100 runs , 1 thread\nTiming of vectorized function\nt.sum()\n  191.26 us\n  1 measurement, 100 runs , 1 thread"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#vectorized-operations-timing-4",
    "href": "ai/pt/wb_torchtensors_slides.html#vectorized-operations-timing-4",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Vectorized operations: timing",
    "text": "Vectorized operations: timing\nSpeedup:\n1.37/(191.26 * 10**-6) = 7163\n\nThe vectorized function runs more than 7,000 times faster!!!"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#even-more-important-on-gpus",
    "href": "ai/pt/wb_torchtensors_slides.html#even-more-important-on-gpus",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Even more important on GPUs",
    "text": "Even more important on GPUs\nWe will talk about GPUs in detail later\nTiming of raw Python loop on GPU (actually slower on GPU!)\nsum_loop(t, sum)\nsetup: from __main__ import sum_loop\n  4.54 s\n  1 measurement, 100 runs , 1 thread\nTiming of vectorized function on GPU (here we do get a speedup)\nt.sum()\n  50.62 us\n  1 measurement, 100 runs , 1 thread"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#even-more-important-on-gpus-1",
    "href": "ai/pt/wb_torchtensors_slides.html#even-more-important-on-gpus-1",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Even more important on GPUs",
    "text": "Even more important on GPUs\nSpeedup:\n4.54/(50.62 * 10**-6) = 89688\n\nOn GPUs, it is even more important not to index repeatedly from a tensor\n\n\nOn GPUs, the vectorized function runs almost 90,000 times faster!!!"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#simple-mathematical-operations",
    "href": "ai/pt/wb_torchtensors_slides.html#simple-mathematical-operations",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Simple mathematical operations",
    "text": "Simple mathematical operations\nt1 = torch.arange(1, 5).view(2, 2); print(t1)\nt2 = torch.tensor([[1, 1], [0, 0]]); print(t2)\n\nt1 + t2 # Operation performed between elements at corresponding locations\nt1 + 1  # Operation applied to each element of the tensor\ntensor([[1, 2],\n        [3, 4]])\ntensor([[1, 1],\n        [0, 0]])\n\ntensor([[2, 3],\n        [3, 4]])\ntensor([[2, 3],\n        [4, 5]])"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#reduction",
    "href": "ai/pt/wb_torchtensors_slides.html#reduction",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Reduction",
    "text": "Reduction\nt = torch.ones(2, 3, 4); print(t)\nt.sum()   # Reduction over all entries\ntensor([[[1., 1., 1., 1.],\n         [1., 1., 1., 1.],\n         [1., 1., 1., 1.]],\n        [[1., 1., 1., 1.],\n         [1., 1., 1., 1.],\n         [1., 1., 1., 1.]]])\ntensor(24.)\n\nOther reduction functions (e.g. mean) behave the same way"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#reduction-1",
    "href": "ai/pt/wb_torchtensors_slides.html#reduction-1",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Reduction",
    "text": "Reduction\n# Reduction over a specific dimension\nt.sum(0)\nt.sum(1)\nt.sum(2)\ntensor([[2., 2., 2., 2.],\n        [2., 2., 2., 2.],\n        [2., 2., 2., 2.]])\ntensor([[3., 3., 3., 3.],\n        [3., 3., 3., 3.]])\ntensor([[4., 4., 4.],\n        [4., 4., 4.]])"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#reduction-2",
    "href": "ai/pt/wb_torchtensors_slides.html#reduction-2",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Reduction",
    "text": "Reduction\n# Reduction over multiple dimensions\nt.sum((0, 1))\nt.sum((0, 2))\nt.sum((1, 2))\ntensor([6., 6., 6., 6.])\ntensor([8., 8., 8.])\ntensor([12., 12.])"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#in-place-operations",
    "href": "ai/pt/wb_torchtensors_slides.html#in-place-operations",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "In-place operations",
    "text": "In-place operations\nWith operators post-fixed with _:\nt1 = torch.tensor([1, 2]); print(t1)\nt2 = torch.tensor([1, 1]); print(t2)\nt1.add_(t2); print(t1)\nt1.zero_(); print(t1)\ntensor([1, 2])\ntensor([1, 1])\ntensor([2, 3])\ntensor([0, 0])"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#in-place-operations-vs-reassignments",
    "href": "ai/pt/wb_torchtensors_slides.html#in-place-operations-vs-reassignments",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "In-place operations vs reassignments",
    "text": "In-place operations vs reassignments\nt1 = torch.ones(1); t1, hex(id(t1))\nt1.add_(1); t1, hex(id(t1))        # In-place operation: same address\nt1 = t1.add(1); t1, hex(id(t1))    # Reassignment: new address in memory\nt1 = t1 + 1; t1, hex(id(t1))       # Reassignment: new address in memory\n(tensor([1.]), '0x7fc61accc3b0')\n(tensor([2.]), '0x7fc61accc3b0')\n(tensor([3.]), '0x7fc61accc5e0')\n(tensor([4.]), '0x7fc61accc6d0')"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#tensor-views",
    "href": "ai/pt/wb_torchtensors_slides.html#tensor-views",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Tensor views",
    "text": "Tensor views\nt = torch.tensor([[1, 2, 3], [4, 5, 6]]); print(t)\nt.size()\nt.view(6)\nt.view(3, 2)\nt.view(3, -1) # Same: with -1, the size is inferred from other dimensions\ntensor([[1, 2, 3],\n        [4, 5, 6]])\ntorch.Size([2, 3])\ntensor([1, 2, 3, 4, 5, 6])\ntensor([[1, 2],\n        [3, 4],\n        [5, 6]])"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#note-the-difference",
    "href": "ai/pt/wb_torchtensors_slides.html#note-the-difference",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Note the difference",
    "text": "Note the difference\nt1 = torch.tensor([[1, 2, 3], [4, 5, 6]]); print(t1)\nt2 = t1.t(); print(t2)\nt3 = t1.view(3, 2); print(t3)\ntensor([[1, 2, 3],\n        [4, 5, 6]])\ntensor([[1, 4],\n        [2, 5],\n        [3, 6]])\ntensor([[1, 2],\n        [3, 4],\n        [5, 6]])"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#logical-operations",
    "href": "ai/pt/wb_torchtensors_slides.html#logical-operations",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Logical operations",
    "text": "Logical operations\nt1 = torch.randperm(5); print(t1)\nt2 = torch.randperm(5); print(t2)\nt1 &gt; 3                            # Test each element\nt1 &lt; t2                           # Test corresponding pairs of elements\ntensor([4, 1, 0, 2, 3])\ntensor([0, 4, 2, 1, 3])\ntensor([ True, False, False, False, False])\ntensor([False,  True,  True, False, False])"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#outline-5",
    "href": "ai/pt/wb_torchtensors_slides.html#outline-5",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Outline",
    "text": "Outline\n\nWhat is a PyTorch tensor?\nMemory storage\nData type (dtype)\nBasic operations\nWorking with NumPy\nLinear algebra\nHarvesting the power of GPUs\nDistributed operations"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#conversion-without-copy",
    "href": "ai/pt/wb_torchtensors_slides.html#conversion-without-copy",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Conversion without copy",
    "text": "Conversion without copy\nPyTorch tensors can be converted to NumPy ndarrays and vice-versa in a very efficient manner as both objects share the same memory\nt = torch.rand(2, 3); print(t)     # PyTorch Tensor\nt_np = t.numpy(); print(t_np)      # NumPy ndarray\ntensor([[0.8434, 0.0876, 0.7507],\n        [0.1457, 0.3638, 0.0563]])   \n\n[[0.84344184 0.08764815 0.7506627 ]\n [0.14567494 0.36384273 0.05629885]]"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#mind-the-different-defaults",
    "href": "ai/pt/wb_torchtensors_slides.html#mind-the-different-defaults",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Mind the different defaults",
    "text": "Mind the different defaults\nt_np.dtype\ndtype('float32')\n\nRemember that PyTorch tensors use 32-bit floating points by default\n(because this is what you want in neural networks)\n\n\nBut NumPy defaults to 64-bit\nDepending on your workflow, you might have to change dtype"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#from-numpy-to-pytorch",
    "href": "ai/pt/wb_torchtensors_slides.html#from-numpy-to-pytorch",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "From NumPy to PyTorch",
    "text": "From NumPy to PyTorch\nimport numpy as np\na = np.random.rand(2, 3); print(a)\na_pt = torch.from_numpy(a); print(a_pt)    # From ndarray to tensor\n[[0.55892276 0.06026952 0.72496545]\n [0.65659463 0.27697739 0.29141587]]\n\ntensor([[0.5589, 0.0603, 0.7250],\n        [0.6566, 0.2770, 0.2914]], dtype=torch.float64)\n\nHere again, you might have to change dtype"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#notes-about-conversion-without-copy",
    "href": "ai/pt/wb_torchtensors_slides.html#notes-about-conversion-without-copy",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Notes about conversion without copy",
    "text": "Notes about conversion without copy\nt and t_np are objects of different Python types, so, as far as Python is concerned,\nthey have different addresses\nid(t) == id(t_np)\nFalse"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#notes-about-conversion-without-copy-1",
    "href": "ai/pt/wb_torchtensors_slides.html#notes-about-conversion-without-copy-1",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Notes about conversion without copy",
    "text": "Notes about conversion without copy\nHowever—that’s quite confusing—they share an underlying C array in memory and modifying one in-place also modifies the other\nt.zero_()\nprint(t_np)\ntensor([[0., 0., 0.],\n        [0., 0., 0.]])\n\n[[0. 0. 0.]\n [0. 0. 0.]]"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#notes-about-conversion-without-copy-2",
    "href": "ai/pt/wb_torchtensors_slides.html#notes-about-conversion-without-copy-2",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Notes about conversion without copy",
    "text": "Notes about conversion without copy\nLastly, as NumPy only works on CPU, to convert a PyTorch tensor allocated to the GPU, the content will have to be copied to the CPU first"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#outline-6",
    "href": "ai/pt/wb_torchtensors_slides.html#outline-6",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Outline",
    "text": "Outline\n\nWhat is a PyTorch tensor?\nMemory storage\nData type (dtype)\nBasic operations\nWorking with NumPy\nLinear algebra\nHarvesting the power of GPUs\nDistributed operations"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#torch.linalg-module",
    "href": "ai/pt/wb_torchtensors_slides.html#torch.linalg-module",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "torch.linalg module",
    "text": "torch.linalg module\nAll functions from numpy.linalg implemented (with accelerator and automatic differentiation support) + additional functions\n\nRequires torch &gt;= 1.9\nLinear algebra support was less developed before the introduction of this module"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#system-of-linear-equations-solver",
    "href": "ai/pt/wb_torchtensors_slides.html#system-of-linear-equations-solver",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "System of linear equations solver",
    "text": "System of linear equations solver\nLet’s have a look at an extremely basic example:\n2x + 3y - z = 5\nx - 2y + 8z = 21\n6x + y - 3z = -1\nWe are looking for the values of x, y, and z that would satisfy this system"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#system-of-linear-equations-solver-1",
    "href": "ai/pt/wb_torchtensors_slides.html#system-of-linear-equations-solver-1",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "System of linear equations solver",
    "text": "System of linear equations solver\nWe create a 2D tensor A of size (3, 3) with the coefficients of the equations\nand a 1D tensor b of size 3 with the right hand sides values of the equations\nA = torch.tensor([[2., 3., -1.], [1., -2., 8.], [6., 1., -3.]]); print(A)\nb = torch.tensor([5., 21., -1.]); print(b)\ntensor([[ 2.,  3., -1.],\n        [ 1., -2.,  8.],\n        [ 6.,  1., -3.]])\ntensor([ 5., 21., -1.])"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#system-of-linear-equations-solver-2",
    "href": "ai/pt/wb_torchtensors_slides.html#system-of-linear-equations-solver-2",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "System of linear equations solver",
    "text": "System of linear equations solver\nSolving this system is as simple as running the torch.linalg.solve function:\nx = torch.linalg.solve(A, b); print(x)\ntensor([1., 2., 3.])\nOur solution is:\nx = 1\ny = 2\nz = 3"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#verify-our-result",
    "href": "ai/pt/wb_torchtensors_slides.html#verify-our-result",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Verify our result",
    "text": "Verify our result\ntorch.allclose(A @ x, b)\nTrue"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#system-of-linear-equations-solver-3",
    "href": "ai/pt/wb_torchtensors_slides.html#system-of-linear-equations-solver-3",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "System of linear equations solver",
    "text": "System of linear equations solver\nHere is another simple example:\n# Create a square normal random matrix\nA = torch.randn(4, 4); print(A)\n# Create a tensor of right hand side values\nb = torch.randn(4); print(b)\n\n# Solve the system\nx = torch.linalg.solve(A, b); print(x)\n\n# Verify\ntorch.allclose(A @ x, b)"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#system-of-linear-equations-solver-4",
    "href": "ai/pt/wb_torchtensors_slides.html#system-of-linear-equations-solver-4",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "System of linear equations solver",
    "text": "System of linear equations solver\n(Results)\nA (coefficients):\ntensor([[ 1.5091,  2.0820,  1.7067,  2.3804],\n        [-1.1256, -0.3170, -1.0925, -0.0852],\n        [ 0.3276, -0.7607, -1.5991,  0.0185],\n        [-0.7504,  0.1854,  0.6211,  0.6382]])\nb (right hand side values):\ntensor([-1.0886, -0.2666,  0.1894, -0.2190])\nx (our solution):\ntensor([ 0.1992, -0.7011,  0.2541, -0.1526])\nVerification:\nTrue"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#with-2-multidimensional-tensors",
    "href": "ai/pt/wb_torchtensors_slides.html#with-2-multidimensional-tensors",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "With 2 multidimensional tensors",
    "text": "With 2 multidimensional tensors\nA = torch.randn(2, 3, 3)              # Must be batches of square matrices\nB = torch.randn(2, 3, 5)              # Dimensions must be compatible\nX = torch.linalg.solve(A, B); print(X)\ntorch.allclose(A @ X, B)\ntensor([[[-0.0545, -0.1012,  0.7863, -0.0806, -0.0191],\n         [-0.9846, -0.0137, -1.7521, -0.4579, -0.8178],\n         [-1.9142, -0.6225, -1.9239, -0.6972,  0.7011]],\n        [[ 3.2094,  0.3432, -1.6604, -0.7885,  0.0088],\n         [ 7.9852,  1.4605, -1.7037, -0.7713,  2.7319],\n         [-4.1979,  0.0849,  1.0864,  0.3098, -1.0347]]])\nTrue"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#matrix-inversions",
    "href": "ai/pt/wb_torchtensors_slides.html#matrix-inversions",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Matrix inversions",
    "text": "Matrix inversions\n\n\nIt is faster and more numerically stable to solve a system of linear equations directly than to compute the inverse matrix first\n\n\n\nLimit matrix inversions to situations where it is truly necessary"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#matrix-inversions-1",
    "href": "ai/pt/wb_torchtensors_slides.html#matrix-inversions-1",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Matrix inversions",
    "text": "Matrix inversions\nA = torch.rand(2, 3, 3)      # Batch of square matrices\nA_inv = torch.linalg.inv(A)  # Batch of inverse matrices\nA @ A_inv                    # Batch of identity matrices\ntensor([[[ 1.0000e+00, -6.0486e-07,  1.3859e-06],\n         [ 5.5627e-08,  1.0000e+00,  1.0795e-06],\n         [-1.4133e-07,  7.9992e-08,  1.0000e+00]],\n        [[ 1.0000e+00,  4.3329e-08, -3.6741e-09],\n         [-7.4627e-08,  1.0000e+00,  1.4579e-07],\n         [-6.3580e-08,  8.2354e-08,  1.0000e+00]]])"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#other-linear-algebra-functions",
    "href": "ai/pt/wb_torchtensors_slides.html#other-linear-algebra-functions",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Other linear algebra functions",
    "text": "Other linear algebra functions\ntorch.linalg contains many more functions:\n\ntorch.tensordot which generalizes matrix products\ntorch.linalg.tensorsolve which computes the solution X to the system torch.tensordot(A, X) = B\ntorch.linalg.eigvals which computes the eigenvalues of a square matrix\n…"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#outline-7",
    "href": "ai/pt/wb_torchtensors_slides.html#outline-7",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Outline",
    "text": "Outline\n\nWhat is a PyTorch tensor?\nMemory storage\nData type (dtype)\nBasic operations\nWorking with NumPy\nLinear algebra\nHarvesting the power of GPUs\nDistributed operations"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#device-attribute",
    "href": "ai/pt/wb_torchtensors_slides.html#device-attribute",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Device attribute",
    "text": "Device attribute\nTensor data can be placed in the memory of various processor types:\n\nthe RAM of CPU\nthe RAM of a GPU with CUDA support\nthe RAM of a GPU with AMD’s ROCm support\nthe RAM of an XLA device (e.g. Cloud TPU) with the torch_xla package"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#device-attribute-1",
    "href": "ai/pt/wb_torchtensors_slides.html#device-attribute-1",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Device attribute",
    "text": "Device attribute\nThe values for the device attributes are:\n\nCPU:  'cpu'\nGPU (CUDA and AMD’s ROCm):  'cuda'\nXLA:  xm.xla_device()\n\nThis last option requires to load the torch_xla package first:\nimport torch_xla\nimport torch_xla.core.xla_model as xm"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#creating-a-tensor-on-a-specific-device",
    "href": "ai/pt/wb_torchtensors_slides.html#creating-a-tensor-on-a-specific-device",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Creating a tensor on a specific device",
    "text": "Creating a tensor on a specific device\nBy default, tensors are created on the CPU\nt1 = torch.rand(2); print(t1)\ntensor([0.1606, 0.9771])  # Implicit: device='cpu'\n\nPrinted tensors only display attributes with values ≠ default values"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#creating-a-tensor-on-a-specific-device-1",
    "href": "ai/pt/wb_torchtensors_slides.html#creating-a-tensor-on-a-specific-device-1",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Creating a tensor on a specific device",
    "text": "Creating a tensor on a specific device\nYou can create a tensor on an accelerator by specifying the device attribute\nt2_gpu = torch.rand(2, device='cuda'); print(t2_gpu)\ntensor([0.0664, 0.7829], device='cuda:0')  # :0 means the 1st GPU"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#copying-a-tensor-to-a-specific-device",
    "href": "ai/pt/wb_torchtensors_slides.html#copying-a-tensor-to-a-specific-device",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Copying a tensor to a specific device",
    "text": "Copying a tensor to a specific device\nYou can also make copies of a tensor on other devices\n# Make a copy of t1 on the GPU\nt1_gpu = t1.to(device='cuda'); print(t1_gpu)\nt1_gpu = t1.cuda()  # Same as above written differently\n\n# Make a copy of t2_gpu on the CPU\nt2 = t2_gpu.to(device='cpu'); print(t2)\nt2 = t2_gpu.cpu()   # For the altenative form\ntensor([0.1606, 0.9771], device='cuda:0')\ntensor([0.0664, 0.7829]) # Implicit: device='cpu'"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#multiple-gpus",
    "href": "ai/pt/wb_torchtensors_slides.html#multiple-gpus",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Multiple GPUs",
    "text": "Multiple GPUs\nIf you have multiple GPUs, you can optionally specify which one a tensor should be created on or copied to\nt3_gpu = torch.rand(2, device='cuda:0')  # Create a tensor on 1st GPU\nt4_gpu = t1.to(device='cuda:0')          # Make a copy of t1 on 1st GPU\nt5_gpu = t1.to(device='cuda:1')          # Make a copy of t1 on 2nd GPU\n\nOr the equivalent short forms for the last two:\nt4_gpu = t1.cuda(0)\nt5_gpu = t1.cuda(1)"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#timing",
    "href": "ai/pt/wb_torchtensors_slides.html#timing",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Timing",
    "text": "Timing\nLet’s compare the timing of some matrix multiplications on CPU and GPU with PyTorch built-in benchmark utility\n# Load utility\nimport torch.utils.benchmark as benchmark\n# Define tensors on the CPU\nA = torch.randn(500, 500)\nB = torch.randn(500, 500)\n# Define tensors on the GPU\nA_gpu = torch.randn(500, 500, device='cuda')\nB_gpu = torch.randn(500, 500, device='cuda')\n\nI ran the code on my laptop with a dedicated GPU and 32GB RAM"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#timing-1",
    "href": "ai/pt/wb_torchtensors_slides.html#timing-1",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Timing",
    "text": "Timing\nLet’s time 100 runs to have a reliable benchmark\nt0 = benchmark.Timer(\n    stmt='A @ B',\n    globals={'A': A, 'B': B})\n\nt1 = benchmark.Timer(\n    stmt='A_gpu @ B_gpu',\n    globals={'A_gpu': A_gpu, 'B_gpu': B_gpu})\n\nprint(t0.timeit(100))\nprint(t1.timeit(100))"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#timing-2",
    "href": "ai/pt/wb_torchtensors_slides.html#timing-2",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Timing",
    "text": "Timing\nA @ B\n  2.29 ms\n  1 measurement, 100 runs , 1 thread\n\nA_gpu @ B_gpu\n  108.02 us\n  1 measurement, 100 runs , 1 thread\nSpeedup:\n(2.29 * 10**-3)/(108.02 * 10**-6) = 21\nThis computation was 21 times faster on my GPU than on CPU"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#timing-3",
    "href": "ai/pt/wb_torchtensors_slides.html#timing-3",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Timing",
    "text": "Timing\nBy replacing 500 with 5000, we get:\nA @ B\n  2.21 s\n  1 measurement, 100 runs , 1 thread\n\nA_gpu @ B_gpu\n  57.88 ms\n  1 measurement, 100 runs , 1 thread\nSpeedup:\n2.21/(57.88 * 10**-3) = 38\nThe larger the computation, the greater the benefit: now 38 times faster"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#outline-8",
    "href": "ai/pt/wb_torchtensors_slides.html#outline-8",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Outline",
    "text": "Outline\n\nWhat is a PyTorch tensor?\nMemory storage\nData type (dtype)\nBasic operations\nWorking with NumPy\nLinear algebra\nHarvesting the power of GPUs\nDistributed operations"
  },
  {
    "objectID": "ai/pt/wb_torchtensors_slides.html#parallel-tensor-operations",
    "href": "ai/pt/wb_torchtensors_slides.html#parallel-tensor-operations",
    "title": "Everything you wanted to know(and more!)about PyTorch tensors",
    "section": "Parallel tensor operations",
    "text": "Parallel tensor operations\nPyTorch already allows for distributed training of ML models\nThe implementation of distributed tensor operations—for instance for linear algebra—is in the work through the use of a ShardedTensor primitive that can be sharded across nodes\nSee also this issue for more comments about upcoming developments on (among other things) tensor sharding"
  },
  {
    "objectID": "ai/pt/wb_fastai.html",
    "href": "ai/pt/wb_fastai.html",
    "title": "fastai",
    "section": "",
    "text": "fastai is a deep learning library which provides practitioners with high-level components that can quickly and easily provide state-of-the-art results in standard deep learning domains. This webinar will take a closer look at the features and functionality of fastai.",
    "crumbs": [
      "AI",
      "<b><em>Webinars</em></b>",
      "Easier PyTorch with fastai"
    ]
  },
  {
    "objectID": "ai/pt/pt_training.html",
    "href": "ai/pt/pt_training.html",
    "title": "Training",
    "section": "",
    "text": "After you have created the data loaders and defined your model, it is time to improve the weights and biases through training."
  },
  {
    "objectID": "ai/pt/pt_training.html#chose-hyperparameters",
    "href": "ai/pt/pt_training.html#chose-hyperparameters",
    "title": "Training",
    "section": "Chose hyperparameters",
    "text": "Chose hyperparameters\nWhile the learning parameters of a model (weights and biases) are the values that get adjusted through training (and they will become part of the final program, along with the model architecture, once training is over), hyperparameters control the training process.\nThey include:\n\nthe batch size: number of samples passed through the model before the parameters are updated,\nthe number of epochs: number iterations,\nthe learning rate (lr): size of the incremental changes to model parameters at each iteration. Smaller values yield slow learning speed, while large values may miss minima.\n\nLet’s define them here:\nlearning_rate = 1e-3\nbatch_size = 64\nepochs = 5"
  },
  {
    "objectID": "ai/pt/pt_training.html#define-the-loss-function",
    "href": "ai/pt/pt_training.html#define-the-loss-function",
    "title": "Training",
    "section": "Define the loss function",
    "text": "Define the loss function\nTo assess the predicted outputs of our model against the true values from the labels, we also need a loss function (e.g. mean square error for regressions: nn.MSELoss or negative log likelihood for classification: nn.NLLLoss)\nThe machine learning literature is rich in information about various loss functions.\nHere is an example with nn.CrossEntropyLoss which combines nn.LogSoftmax and nn.NLLLoss:\nloss_fn = nn.CrossEntropyLoss()"
  },
  {
    "objectID": "ai/pt/pt_training.html#initialize-the-optimizer",
    "href": "ai/pt/pt_training.html#initialize-the-optimizer",
    "title": "Training",
    "section": "Initialize the optimizer",
    "text": "Initialize the optimizer\nThe optimization algorithm determines how the model parameters get adjusted at each iteration.\nThere are many optimizers and you need to search in the literature which one performs best for your time of model and data.\nBelow is an example with stochastic gradient descent:\noptimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n\nlr is the learning rate.\nmomentum is a method increasing convergence rate and reducing oscillation for SDG."
  },
  {
    "objectID": "ai/pt/pt_training.html#define-the-train-and-test-loops",
    "href": "ai/pt/pt_training.html#define-the-train-and-test-loops",
    "title": "Training",
    "section": "Define the train and test loops",
    "text": "Define the train and test loops\nFinally, we need to define the train and test loops.\nThe train loop:\n\ngets a batch of training data from the DataLoader,\nresets the gradients of model parameters with optimizer.zero_grad(),\ncalculates predictions from the model for an input batch,\ncalculates the loss for that set of predictions vs. the labels on the dataset,\ncalculates the backward gradients over the learning parameters (that’s the backpropagation) with loss.backward(),\nadjusts the parameters by the gradients collected in the backward pass with optimizer.step().\n\nThe test loop evaluates the model’s performance against the test data.\ndef train(dataloader, model, loss_fn, optimizer):\n    size = len(dataloader.dataset)\n    for batch, (X, y) in enumerate(dataloader):\n        # Compute prediction and loss\n        pred = model(X)\n        loss = loss_fn(pred, y)\n\n        # Backpropagation\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if batch % 100 == 0:\n            loss, current = loss.item(), batch * len(X)\n            print(f\"loss: {loss:&gt;7f}  [{current:&gt;5d}/{size:&gt;5d}]\")\n\n\ndef test(dataloader, model, loss_fn):\n    size = len(dataloader.dataset)\n    num_batches = len(dataloader)\n    test_loss, correct = 0, 0\n\n    with torch.no_grad():\n        for X, y in dataloader:\n            pred = model(X)\n            test_loss += loss_fn(pred, y).item()\n            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n\n    test_loss /= num_batches\n    correct /= size\n    print(f\"Test Error: \\n Accuracy: {(100*correct):&gt;0.1f}%, Avg loss: {test_loss:&gt;8f} \\n\")"
  },
  {
    "objectID": "ai/pt/pt_training.html#train",
    "href": "ai/pt/pt_training.html#train",
    "title": "Training",
    "section": "Train",
    "text": "Train\nTo train our model, we run the loop over the epochs:\nfor t in range(epochs):\n    print(f\"Epoch {t+1}\\n-------------------------------\")\n    train(train_dataloader, model, loss_fn, optimizer)\n    test(test_dataloader, model, loss_fn)\nprint(\"Training completed\")"
  },
  {
    "objectID": "ai/pt/pt_supervised_learning.html",
    "href": "ai/pt/pt_supervised_learning.html",
    "title": "Understanding supervised learning",
    "section": "",
    "text": "In supervised learning, neural networks learn by adjusting their parameters automatically in an iterative manner. This is derived from Arthur Samuel’s concept.\nIt is important to get a good understanding of this process, so let’s go over it step by step."
  },
  {
    "objectID": "ai/pt/pt_supervised_learning.html#decide-on-an-architecture",
    "href": "ai/pt/pt_supervised_learning.html#decide-on-an-architecture",
    "title": "Understanding supervised learning",
    "section": "Decide on an architecture",
    "text": "Decide on an architecture\n\nThe architecture won’t change during training. This is set. The type of architecture you choose (e.g. CNN, Transformer, etc.) depends on the type of data you have (e.g. vision, textual, etc.). The depth and breadth of your network depend on the amount of data and computing resource you have."
  },
  {
    "objectID": "ai/pt/pt_supervised_learning.html#set-some-initial-parameters",
    "href": "ai/pt/pt_supervised_learning.html#set-some-initial-parameters",
    "title": "Understanding supervised learning",
    "section": "Set some initial parameters",
    "text": "Set some initial parameters\n\nYou can initialize them randomly or get much better ones through transfer learning.\nWhile the parameters are also part of the model, those will change during training."
  },
  {
    "objectID": "ai/pt/pt_supervised_learning.html#get-some-labelled-data",
    "href": "ai/pt/pt_supervised_learning.html#get-some-labelled-data",
    "title": "Understanding supervised learning",
    "section": "Get some labelled data",
    "text": "Get some labelled data\n\nWhen we say that we need a lot of data for machine learning, we mean “lots of labelled data” as this is what gets used for training models."
  },
  {
    "objectID": "ai/pt/pt_supervised_learning.html#make-sure-to-keep-some-data-for-testing",
    "href": "ai/pt/pt_supervised_learning.html#make-sure-to-keep-some-data-for-testing",
    "title": "Understanding supervised learning",
    "section": "Make sure to keep some data for testing",
    "text": "Make sure to keep some data for testing\n\nThose data won’t be used for training the model. Often people keep around 20% of their data for testing."
  },
  {
    "objectID": "ai/pt/pt_supervised_learning.html#pass-data-and-parameters-through-the-architecture",
    "href": "ai/pt/pt_supervised_learning.html#pass-data-and-parameters-through-the-architecture",
    "title": "Understanding supervised learning",
    "section": "Pass data and parameters through the architecture",
    "text": "Pass data and parameters through the architecture\n\nThe train data are the inputs and the process of calculating the outputs is the forward pass."
  },
  {
    "objectID": "ai/pt/pt_supervised_learning.html#the-outputs-of-the-model-are-predictions",
    "href": "ai/pt/pt_supervised_learning.html#the-outputs-of-the-model-are-predictions",
    "title": "Understanding supervised learning",
    "section": "The outputs of the model are predictions",
    "text": "The outputs of the model are predictions"
  },
  {
    "objectID": "ai/pt/pt_supervised_learning.html#compare-those-predictions-to-the-train-labels",
    "href": "ai/pt/pt_supervised_learning.html#compare-those-predictions-to-the-train-labels",
    "title": "Understanding supervised learning",
    "section": "Compare those predictions to the train labels",
    "text": "Compare those predictions to the train labels\n\nSince our data was labelled, we know what the true outputs are."
  },
  {
    "objectID": "ai/pt/pt_supervised_learning.html#calculate-train-loss",
    "href": "ai/pt/pt_supervised_learning.html#calculate-train-loss",
    "title": "Understanding supervised learning",
    "section": "Calculate train loss",
    "text": "Calculate train loss\n\nThe deviation of our predictions from the true outputs gives us a measure of training loss."
  },
  {
    "objectID": "ai/pt/pt_supervised_learning.html#adjust-parameters",
    "href": "ai/pt/pt_supervised_learning.html#adjust-parameters",
    "title": "Understanding supervised learning",
    "section": "Adjust parameters",
    "text": "Adjust parameters\n\nThe parameters get automatically adjusted to reduce the training loss through the mechanism of backpropagation.\nThis is the actual training part.\nThis process is repeated many times. Training models is pretty much a giant for loop."
  },
  {
    "objectID": "ai/pt/pt_supervised_learning.html#from-model-to-program",
    "href": "ai/pt/pt_supervised_learning.html#from-model-to-program",
    "title": "Understanding supervised learning",
    "section": "From model to program",
    "text": "From model to program\n\nRemember that the model architecture is fixed, but that the parameters change at each iteration of the training process.\nWhile the labelled data are key to training, what we are really interested in is the combination of architecture + final parameters.\n\nWhen the training is over, the parameters become fixed. Which means that our model now behaves like a classic program."
  },
  {
    "objectID": "ai/pt/pt_supervised_learning.html#evaluate-the-model",
    "href": "ai/pt/pt_supervised_learning.html#evaluate-the-model",
    "title": "Understanding supervised learning",
    "section": "Evaluate the model",
    "text": "Evaluate the model\n\nWe can now use the testing set (which was never used to train the model) to evaluate our model: if we pass the test inputs through our program, we get some predictions that we can compare to the test labels (which are the true outputs).\nThis gives us the test loss: a measure of how well our model performs."
  },
  {
    "objectID": "ai/pt/pt_supervised_learning.html#use-the-model",
    "href": "ai/pt/pt_supervised_learning.html#use-the-model",
    "title": "Understanding supervised learning",
    "section": "Use the model",
    "text": "Use the model\n\nNow that we have a program, we can use it on unlabelled inputs to get what people ultimately want: unknown outputs. This is when we put our model to actual use to solve some problem."
  },
  {
    "objectID": "ai/pt/pt_pytorch.html",
    "href": "ai/pt/pt_pytorch.html",
    "title": "The PyTorch API",
    "section": "",
    "text": "PyTorch is a free and open-source machine learning and scientific computing framework based on Torch. While Torch uses a scripting language based on Lua, PyTorch has a Python and a C++ interface.\nCreated by Meta AI (formerly Facebook, Inc.) in 2017, it is now a project of The Linux Foundation.\nPyTorch is widely used in academia and research. Part of its popularity stems from the fact that the Python interface is truly pythonic in nature, making it easier to learn than other popular frameworks such as TensorFlow.\nThe PyTorch API is vast and complex. This section links to the key components to get you started.",
    "crumbs": [
      "AI",
      "<b><em>Intro DL with PyTorch</em></b>",
      "The PyTorch API"
    ]
  },
  {
    "objectID": "ai/pt/pt_pytorch.html#domain-specific-libraries",
    "href": "ai/pt/pt_pytorch.html#domain-specific-libraries",
    "title": "The PyTorch API",
    "section": "Domain-specific libraries",
    "text": "Domain-specific libraries\nPyTorch is a large framework with domain-specific libraries:\n\nTorchVision for computer vision,\nTorchText for natural languages,\nTorchAudio for audio and signal processing.\n\nThese libraries contain standard datasets and utilities specific to the data in those fields.",
    "crumbs": [
      "AI",
      "<b><em>Intro DL with PyTorch</em></b>",
      "The PyTorch API"
    ]
  },
  {
    "objectID": "ai/pt/pt_pytorch.html#loading-data",
    "href": "ai/pt/pt_pytorch.html#loading-data",
    "title": "The PyTorch API",
    "section": "Loading data",
    "text": "Loading data\ntorch.utils.data contains everything you need create data loaders (iterables that present the data to a model).",
    "crumbs": [
      "AI",
      "<b><em>Intro DL with PyTorch</em></b>",
      "The PyTorch API"
    ]
  },
  {
    "objectID": "ai/pt/pt_pytorch.html#building-models",
    "href": "ai/pt/pt_pytorch.html#building-models",
    "title": "The PyTorch API",
    "section": "Building models",
    "text": "Building models\ntorch.nn contains the elements you need to build your model architecture and chose a loss function.",
    "crumbs": [
      "AI",
      "<b><em>Intro DL with PyTorch</em></b>",
      "The PyTorch API"
    ]
  },
  {
    "objectID": "ai/pt/pt_pytorch.html#training",
    "href": "ai/pt/pt_pytorch.html#training",
    "title": "The PyTorch API",
    "section": "Training",
    "text": "Training\nTraining a model consists of optimizing the model parameters.\ntorch.autograd contains the tools for automatic differentiation (to compute the gradients, that is the tensors containing the partial derivatives of the error with respect to the parameters of the functions in the model) and torch.optim contains optimization algorithms that can be used for gradient descent.",
    "crumbs": [
      "AI",
      "<b><em>Intro DL with PyTorch</em></b>",
      "The PyTorch API"
    ]
  },
  {
    "objectID": "ai/pt/pt_nn.html",
    "href": "ai/pt/pt_nn.html",
    "title": "Introduction to neural networks",
    "section": "",
    "text": "3Blue1Brown by Grant Sanderson has a series of 4 videos on neural networks which is easy to watch, fun, and does an excellent job at explaining the functioning of a simple neural network. In this section, we will watch the first 2 videos as an introduction to what neural networks are and how they learn.",
    "crumbs": [
      "AI",
      "<b><em>Intro DL with PyTorch</em></b>",
      "Introduction to NN"
    ]
  },
  {
    "objectID": "ai/pt/pt_nn.html#what-are-nn-19-min",
    "href": "ai/pt/pt_nn.html#what-are-nn-19-min",
    "title": "Introduction to neural networks",
    "section": "What are NN? (19 min)",
    "text": "What are NN? (19 min)\n\n\nAs you develop your own ML models, if you find that your mathematical background is shaky, 3blue1brown also has an excellent series of videos on linear algebra and an equally great one on calculus.",
    "crumbs": [
      "AI",
      "<b><em>Intro DL with PyTorch</em></b>",
      "Introduction to NN"
    ]
  },
  {
    "objectID": "ai/pt/pt_nn.html#how-do-nn-learn-21-min",
    "href": "ai/pt/pt_nn.html#how-do-nn-learn-21-min",
    "title": "Introduction to neural networks",
    "section": "How do NN learn? (21 min)",
    "text": "How do NN learn? (21 min)",
    "crumbs": [
      "AI",
      "<b><em>Intro DL with PyTorch</em></b>",
      "Introduction to NN"
    ]
  },
  {
    "objectID": "ai/pt/pt_nn.html#nn-vs-biological-neurons-and-types-of-nn",
    "href": "ai/pt/pt_nn.html#nn-vs-biological-neurons-and-types-of-nn",
    "title": "Introduction to neural networks",
    "section": "NN vs biological neurons and types of NN",
    "text": "NN vs biological neurons and types of NN\nSlides (Click and wait: the presentation might take a few instants to load)",
    "crumbs": [
      "AI",
      "<b><em>Intro DL with PyTorch</em></b>",
      "Introduction to NN"
    ]
  },
  {
    "objectID": "ai/pt/pt_mnist.html",
    "href": "ai/pt/pt_mnist.html",
    "title": "Example: classifying the MNIST dataset",
    "section": "",
    "text": "Here is an example you can try on your own after the workshop: the classification of the MNIST dataset—a classic of machine learning.",
    "crumbs": [
      "AI",
      "<b><em>Intro DL with PyTorch</em></b>",
      "Example: the MNIST"
    ]
  },
  {
    "objectID": "ai/pt/pt_mnist.html#the-mnist-dataset",
    "href": "ai/pt/pt_mnist.html#the-mnist-dataset",
    "title": "Example: classifying the MNIST dataset",
    "section": "The MNIST dataset",
    "text": "The MNIST dataset\nThe MNIST is a classic dataset commonly used for testing machine learning systems. It consists of pairs of images of handwritten digits and their corresponding labels.\nThe images are composed of 28x28 pixels of greyscale RGB codes ranging from 0 to 255 and the labels are the digits from 0 to 9 that each image represents.\nThere are 60,000 training pairs and 10,000 testing pairs.\nThe goal is to build a neural network which can learn from the training set to properly identify the handwritten digits and which will perform well when presented with the testing set that it has never seen. This is a typical case of supervised learning.\n\nNow, let’s explore the MNIST with PyTorch.",
    "crumbs": [
      "AI",
      "<b><em>Intro DL with PyTorch</em></b>",
      "Example: the MNIST"
    ]
  },
  {
    "objectID": "ai/pt/pt_mnist.html#download-unzip-and-transform-the-data",
    "href": "ai/pt/pt_mnist.html#download-unzip-and-transform-the-data",
    "title": "Example: classifying the MNIST dataset",
    "section": "Download, unzip, and transform the data",
    "text": "Download, unzip, and transform the data\n\nWhere to store the data in the cluster\nWe will all use the same data. It would make little sense to all download it in our home directory.\nOn the Alliance clusters, a good place to store data shared amongst members of a project is in the /project file system.\nYou usually belong to /project/def-&lt;group&gt;, where &lt;group&gt; is the name of your PI. You can access it from your home directory through the symbolic link ~/projects/def-&lt;group&gt;.\nIn our training cluster, we are all part of the group def-sponsor00, accessible through /project/def-sponsor00 (or the hyperlink ~/projects/def-sponsor00).\nWe will thus all access the MNIST data in ~/projects/def-sponsor00/data.\n\n\nHow to obtain the data?\nThe dataset can be downloaded directly from the MNIST website, but the PyTorch package TorchVision has tools to download and transform several classic vision datasets, including the MNIST.\nhelp(torchvision.datasets.MNIST)\nHelp on class MNIST in module torchvision.datasets.mnist:\n\nclass MNIST(torchvision.datasets.vision.VisionDataset)\n\n |  MNIST(root: str, train: bool = True, \n |    transform: Optional[Callable] = None,\n |    target_transform: Optional[Callable] = None, \n |    download: bool = False) -&gt; None\n |   \n |  Args:\n |    root (string): Root directory of dataset where \n |      MNIST/raw/train-images-idx3-ubyte and \n |      MNIST/raw/t10k-images-idx3-ubyte exists.\n |    train (bool, optional): If True, creates dataset from \n |      train-images-idx3-ubyte, otherwise from t10k-images-idx3-ubyte.\n |    download (bool, optional): If True, downloads the dataset from the \n |      internet and puts it in root directory. If dataset is already \n |      downloaded, it is not downloaded again.\n |    transform (callable, optional): A function/transform that takes in \n |      an PIL image and returns a transformed version.\n |      E.g, transforms.RandomCrop\n |    target_transform (callable, optional): A function/transform that \n |      takes in the target and transforms it.\nNote that here too, the root argument sets the location of the downloaded data and we will use /project/def-sponsor00/data/.\n\n\nPrepare the data\nFirst, let’s load the needed libraries:\n\nimport torch\nfrom torchvision import datasets, transforms\nfrom matplotlib import pyplot as plt\n\nThe MNIST dataset already consists of a training and a testing sets, so we don’t have to split the data manually. Instead, we can directly create 2 different objects with the same function (train=True selects the train set and train=False selects the test set).\nWe will transform the raw data to tensors and normalize them using the mean and standard deviation of the MNIST training set: 0.1307 and 0.3081 respectively (even though the mean and standard deviation of the test data are slightly different, it is important to normalize the test data with the values of the training data to apply the same treatment to both sets).\nSo we first need to define a transformation:\n\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.1307,), (0.3081,))\n])\n\n\n\nWe can now create our data objects\n\nTraining data\n\nRemember that train=True selects the training set of the MNIST.\n\n\ntrain_data = datasets.MNIST(\n    '~/projects/def-sponsor00/data',\n    train=True, download=True, transform=transform)\n\n\n\nTest data\n\ntrain=False selects the test set.\n\n\ntest_data = datasets.MNIST(\n    '~/projects/def-sponsor00/data',\n    train=False, transform=transform)",
    "crumbs": [
      "AI",
      "<b><em>Intro DL with PyTorch</em></b>",
      "Example: the MNIST"
    ]
  },
  {
    "objectID": "ai/pt/pt_mnist.html#exploring-the-data",
    "href": "ai/pt/pt_mnist.html#exploring-the-data",
    "title": "Example: classifying the MNIST dataset",
    "section": "Exploring the data",
    "text": "Exploring the data\n\nData inspection\nFirst, let’s check the size of train_data:\n\nprint(len(train_data))\n\n60000\n\n\nThat makes sense since the MNIST’s training set has 60,000 pairs. train_data has 60,000 elements and we should expect each element to be of size 2 since it is a pair. Let’s double-check with the first element:\n\nprint(len(train_data[0]))\n\n2\n\n\nSo far, so good. We can print that first pair:\n\nprint(train_data[0])\n\n(tensor([[[-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.3860, -0.1951,\n          -0.1951, -0.1951,  1.1795,  1.3068,  1.8032, -0.0933,  1.6887,\n           2.8215,  2.7197,  1.1923, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.0424,  0.0340,  0.7722,  1.5359,  1.7396,  2.7960,\n           2.7960,  2.7960,  2.7960,  2.7960,  2.4396,  1.7650,  2.7960,\n           2.6560,  2.0578,  0.3904, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n           0.1995,  2.6051,  2.7960,  2.7960,  2.7960,  2.7960,  2.7960,\n           2.7960,  2.7960,  2.7960,  2.7706,  0.7595,  0.6195,  0.6195,\n           0.2886,  0.0722, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.1951,  2.3633,  2.7960,  2.7960,  2.7960,  2.7960,  2.7960,\n           2.0960,  1.8923,  2.7197,  2.6433, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242,  0.5940,  1.5614,  0.9377,  2.7960,  2.7960,  2.1851,\n          -0.2842, -0.4242,  0.1231,  1.5359, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.2460, -0.4115,  1.5359,  2.7960,  0.7213,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242,  1.3450,  2.7960,  1.9942,\n          -0.3988, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.2842,  1.9942,  2.7960,\n           0.4668, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.0213,  2.6433,\n           2.4396,  1.6123,  0.9504, -0.4115, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.6068,\n           2.6306,  2.7960,  2.7960,  1.0904, -0.1060, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n           0.1486,  1.9432,  2.7960,  2.7960,  1.4850, -0.0806, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.2206,  0.7595,  2.7833,  2.7960,  1.9560, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242,  2.7451,  2.7960,  2.7451,  0.3904,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n           0.1613,  1.2305,  1.9051,  2.7960,  2.7960,  2.2105, -0.3988,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.0722,  1.4596,\n           2.4906,  2.7960,  2.7960,  2.7960,  2.7578,  1.8923, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.1187,  1.0268,  2.3887,  2.7960,\n           2.7960,  2.7960,  2.7960,  2.1342,  0.5686, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.1315,  0.4159,  2.2869,  2.7960,  2.7960,  2.7960,\n           2.7960,  2.0960,  0.6068, -0.3988, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.1951,\n           1.7523,  2.3633,  2.7960,  2.7960,  2.7960,  2.7960,  2.0578,\n           0.5940, -0.3097, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242,  0.2758,  1.7650,  2.4524,\n           2.7960,  2.7960,  2.7960,  2.7960,  2.6815,  1.2686, -0.2842,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242,  1.3068,  2.7960,  2.7960,\n           2.7960,  2.2742,  1.2941,  1.2559, -0.2206, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242]]]), 5)\n\n\nAnd you can see that it is a tuple with:\n\nprint(type(train_data[0]))\n\n&lt;class 'tuple'&gt;\n\n\nWhat is that tuple made of?\n\nprint(type(train_data[0][0]))\nprint(type(train_data[0][1]))\n\n&lt;class 'torch.Tensor'&gt;\n&lt;class 'int'&gt;\n\n\nIt is made of the tensor for the first image (remember that we transformed the images into tensors when we created the objects train_data and test_data) and the integer of the first label (which you can see is 5 when you print train_data[0][1]).\nSo since train_data[0][0] is the tensor representing the image of the first pair, let’s check its size:\n\nprint(train_data[0][0].size())\n\ntorch.Size([1, 28, 28])\n\n\nThat makes sense: a color image would have 3 layers of RGB values (so the size in the first dimension would be 3), but because the MNIST has black and white images, there is a single layer of values—the values of each pixel on a gray scale—so the first dimension has a size of 1. The 2nd and 3rd dimensions correspond to the width and length of the image in pixels, hence 28 and 28.\n\n\nYour turn:\n\nRun the following:\nprint(train_data[0][0][0])\nprint(train_data[0][0][0][0])\nprint(train_data[0][0][0][0][0])\nAnd think about what each of them represents.\nThen explore the test_data object.\n\n\n\nPlotting an image from the data\nFor this, we will use pyplot from matplotlib.\nFirst, we select the image of the first pair and we resize it from 3 to 2 dimensions by removing its dimension of size 1 with torch.squeeze:\nimg = torch.squeeze(train_data[0][0])\nThen, we plot it with pyplot, but since we are in a cluster, instead of showing it to screen with plt.show(), we save it to file:\nplt.imshow(img, cmap='gray')\nThis is what that first image looks like:\n\nAnd indeed, it matches the first label we explored earlier (train_data[0][1]).\n\n\nPlotting an image with its pixel values\nWe can plot it with more details by showing the value of each pixel in the image. One little twist is that we need to pick a threshold value below which we print the pixel values in white otherwise they would not be visible (black on near black background). We also round the pixel values to one decimal digit so as not to clutter the result.\nimgplot = plt.figure(figsize = (12, 12))\nsub = imgplot.add_subplot(111)\nsub.imshow(img, cmap='gray')\nwidth, height = img.shape\nthresh = img.max() / 2.5\nfor x in range(width):\n    for y in range(height):\n        val = round(img[x][y].item(), 1)\n        sub.annotate(str(val), xy=(y, x),\n                     horizontalalignment='center',\n                     verticalalignment='center',\n                     color='white' if img[x][y].item() &lt; thresh else 'black')",
    "crumbs": [
      "AI",
      "<b><em>Intro DL with PyTorch</em></b>",
      "Example: the MNIST"
    ]
  },
  {
    "objectID": "ai/pt/pt_mnist.html#batch-processing",
    "href": "ai/pt/pt_mnist.html#batch-processing",
    "title": "Example: classifying the MNIST dataset",
    "section": "Batch processing",
    "text": "Batch processing\nPyTorch provides the torch.utils.data.DataLoader class which combines a dataset and an optional sampler and provides an iterable (while training or testing our neural network, we will iterate over that object). It allows, among many other things, to set the batch size and shuffle the data.\nSo our last step in preparing the data is to pass it through DataLoader.\n\nCreate DataLoaders\n\nTraining data\ntrain_loader = torch.utils.data.DataLoader(\n    train_data, batch_size=20, shuffle=True)\n\n\nTest data\ntest_loader = torch.utils.data.DataLoader(\n    test_data, batch_size=20, shuffle=False)\n\n\n\nPlot a full batch of images with their labels\nNow that we have passed our data through DataLoader, it is easy to select one batch from it. Let’s plot an entire batch of images with their labels.\nFirst, we need to get one batch of training images and their labels:\ndataiter = iter(train_loader)\nbatchimg, batchlabel = dataiter.next()\nThen, we can plot them:\nbatchplot = plt.figure(figsize=(20, 5))\nfor i in torch.arange(20):\n    sub = batchplot.add_subplot(2, 10, i+1, xticks=[], yticks=[])\n    sub.imshow(torch.squeeze(batchimg[i]), cmap='gray')\n    sub.set_title(str(batchlabel[i].item()), fontsize=25)",
    "crumbs": [
      "AI",
      "<b><em>Intro DL with PyTorch</em></b>",
      "Example: the MNIST"
    ]
  },
  {
    "objectID": "ai/pt/pt_mnist.html#time-to-build-a-nn-to-classify-the-mnist",
    "href": "ai/pt/pt_mnist.html#time-to-build-a-nn-to-classify-the-mnist",
    "title": "Example: classifying the MNIST dataset",
    "section": "Time to build a NN to classify the MNIST",
    "text": "Time to build a NN to classify the MNIST\nLet’s build a multi-layer perceptron (MLP): the simplest neural network. It is a feed-forward (i.e. no loop), fully-connected (i.e. each neuron of one layer is connected to all the neurons of the adjacent layers) neural network with a single hidden layer.\n\n\nLoad packages\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torchvision import datasets, transforms\nfrom torch.optim.lr_scheduler import StepLR\nThe torch.nn.functional module contains all the functions of the torch.nn package.\nThese functions include loss functions, activation functions, pooling functions…\n\n\nCreate a SummaryWriter instance for TensorBoard\nwriter = SummaryWriter()\n\n\nDefine the architecture of the network\n# To build a model, create a subclass of torch.nn.Module:\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.fc1 = nn.Linear(784, 128)\n        self.fc2 = nn.Linear(128, 10)\n\n    # Method for the forward pass:\n    def forward(self, x):\n        x = torch.flatten(x, 1)\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.fc2(x)\n        output = F.log_softmax(x, dim=1)\n        return output\nPython’s class inheritance gives our subclass all the functionality of torch.nn.Module while allowing us to customize it.\n\n\nDefine a training function\ndef train(model, device, train_loader, optimizer, epoch):\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n        optimizer.zero_grad()  # reset the gradients to 0\n        output = model(data)\n        loss = F.nll_loss(output, target)  # negative log likelihood\n        writer.add_scalar(\"Loss/train\", loss, epoch)\n        loss.backward()\n        optimizer.step()\n        if batch_idx % 10 == 0:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(data), len(train_loader.dataset),\n                100. * batch_idx / len(train_loader), loss.item()))\n\n\nDefine a testing function\ndef test(model, device, test_loader):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            # Sum up batch loss:\n            test_loss += F.nll_loss(output, target, reduction='sum').item()\n            # Get the index of the max log-probability:\n            pred = output.argmax(dim=1, keepdim=True)\n            correct += pred.eq(target.view_as(pred)).sum().item()\n\n    test_loss /= len(test_loader.dataset)\n\n    # Print a summary\n    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n        test_loss, correct, len(test_loader.dataset),\n        100. * correct / len(test_loader.dataset)))\n\n\nDefine a function main() which runs our network\ndef main():\n    epochs = 1\n    torch.manual_seed(1)\n    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n\n    transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,))\n    ])\n\n    train_data = datasets.MNIST(\n        '~/projects/def-sponsor00/data',\n        train=True, download=True, transform=transform)\n\n    test_data = datasets.MNIST(\n        '~/projects/def-sponsor00/data',\n        train=False, transform=transform)\n\n    train_loader = torch.utils.data.DataLoader(train_data, batch_size=64)\n    test_loader = torch.utils.data.DataLoader(test_data, batch_size=1000)\n    model = Net().to(device)  # create instance of our network and send it to device\n    optimizer = optim.Adadelta(model.parameters(), lr=1.0)\n    scheduler = StepLR(optimizer, step_size=1, gamma=0.7)\n\n    for epoch in range(1, epochs + 1):\n        train(model, device, train_loader, optimizer, epoch)\n        test(model, device, test_loader)\n        scheduler.step()\n\n\nRun the network\nmain()\n\n\nWrite pending events to disk and close the TensorBoard\nwriter.flush()\nwriter.close()\nThe code is working. Time to actually train our model!\nJupyter is a fantastic tool. It has a major downside however: when you launch a Jupyter server, you are running a job on a compute node. If you want to play for 8 hours in Jupyter, you are requesting an 8 hour job. Now, most of the time you spend on Jupyter is spent typing, running bits and pieces of code, or doing nothing at all. If you ask for GPUs, many CPUs, and lots of RAM, all of it will remain idle almost all of the time. It is a really suboptimal use of the Alliance resources.\nIn addition, if you ask for lots of resources for a long time, you will have to wait a long time in the queue before they get allocated to you.\nLastly, you will go through your allocation quickly.\nA much better strategy is to develop and test your code (with very little data, few epochs, etc.) in an interactive job (with salloc) or in Jupyter, then, launch an sbatch job to actually train your model. This ensures that heavy duty resources such as GPU(s) are only allocated to you when you are actually needing and using them.",
    "crumbs": [
      "AI",
      "<b><em>Intro DL with PyTorch</em></b>",
      "Example: the MNIST"
    ]
  },
  {
    "objectID": "ai/pt/pt_mnist.html#lets-train-and-test-our-model",
    "href": "ai/pt/pt_mnist.html#lets-train-and-test-our-model",
    "title": "Example: classifying the MNIST dataset",
    "section": "Let’s train and test our model",
    "text": "Let’s train and test our model\n\nLog in a cluster\nOpen a terminal and SSH to a cluster.\n\n\nLoad necessary modules\nFirst, we need to load the Python and CUDA modules. This is done with the Lmod tool through the module command. Here are some key Lmod commands:\n# Get help on the module command\n$ module help\n\n# List modules that are already loaded\n$ module list\n\n# See which modules are available for a tool\n$ module avail &lt;tool&gt;\n\n# Load a module\n$ module load &lt;module&gt;[/&lt;version&gt;]\nHere are the modules we need:\n$ module load nixpkgs/16.09 gcc/7.3.0 cuda/10.0.130 cudnn/7.6 python/3.8.2\n\n\nInstall Python packages\nYou also need the Python packages matplotlib, torch, torchvision, and tensorboard.\nOn the Alliance clusters, you need to create a virtual environment in which you install packages with pip, then activate that virtual environment.\n\nDo not use Anaconda.\nWhile Anaconda is a great tool on personal computers, it is not an appropriate tool when working on the Alliance clusters: binaries are unoptimized for those clusters and library paths are inconsistent with their architecture. Anaconda installs packages in $HOME where it creates a very large number of small files. It can also create conflicts by modifying .bashrc.\n\nCreate a virtual environment:\n$ virtualenv --no-download ~/env\nActivate the virtual environment:\n$ source ~/env/bin/activate\nUpdate pip:\n(env) $ pip install --no-index --upgrade pip\nInstall the packages you need in the virtual environment:\n(env) $ pip install --no-cache-dir --no-index matplotlib torch torchvision tensorboard\nIf you want to exit the virtual environment, you can press Ctrl-D or run:\n(env) $ deactivate\n\n\nWrite a Python script\nCreate a directory for this project and cd into it:\nmkdir mnist\ncd mnist\nStart a Python script with the text editor of your choice:\nnano nn.py\nIn it, copy-paste the code we played with in Jupyter, but this time have it run for 10 epochs:\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torchvision import datasets, transforms\nfrom torch.optim.lr_scheduler import StepLR\n\nwriter = SummaryWriter()\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.fc1 = nn.Linear(784, 128)\n        self.fc2 = nn.Linear(128, 10)\n\n    def forward(self, x):\n        x = torch.flatten(x, 1)\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.fc2(x)\n        output = F.log_softmax(x, dim=1)\n        return output\n\ndef train(model, device, train_loader, optimizer, epoch):\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = F.nll_loss(output, target)\n        writer.add_scalar(\"Loss/train\", loss, epoch)\n        loss.backward()\n        optimizer.step()\n        if batch_idx % 10 == 0:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(data), len(train_loader.dataset),\n                100. * batch_idx / len(train_loader), loss.item()))\n\ndef test(model, device, test_loader):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            test_loss += F.nll_loss(output, target, reduction='sum').item()\n            pred = output.argmax(dim=1, keepdim=True)\n            correct += pred.eq(target.view_as(pred)).sum().item()\n\n    test_loss /= len(test_loader.dataset)\n\n    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n        test_loss, correct, len(test_loader.dataset),\n        100. * correct / len(test_loader.dataset)))\n\ndef main():\n    epochs = 10  # don't forget to change the number of epochs\n    torch.manual_seed(1)\n    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n\n    transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,))\n    ])\n\n    train_data = datasets.MNIST(\n        '~/projects/def-sponsor00/data',\n        train=True, download=True, transform=transform)\n\n    test_data = datasets.MNIST(\n        '~/projects/def-sponsor00/data',\n        train=False, transform=transform)\n\n    train_loader = torch.utils.data.DataLoader(train_data, batch_size=64)\n    test_loader = torch.utils.data.DataLoader(test_data, batch_size=1000)\n    model = Net().to(device)\n    optimizer = optim.Adadelta(model.parameters(), lr=1.0)\n    scheduler = StepLR(optimizer, step_size=1, gamma=0.7)\n\n    for epoch in range(1, epochs + 1):\n        train(model, device, train_loader, optimizer, epoch)\n        test(model, device, test_loader)\n        scheduler.step()\n\nmain()\n\nwriter.flush()\nwriter.close()\n\n\nWrite a Slurm script\nWrite a shell script with the text editor of your choice:\nnano nn.sh\nThis is what you want in that script:\n#!/bin/bash\n#SBATCH --time=5:0\n#SBATCH --cpus-per-task=1\n#SBATCH --gres=gpu:1\n#SBATCH --mem=4G\n#SBATCH --output=%x_%j.out\n#SBATCH --error=%x_%j.err\n\npython ~/mnist/nn.py\n\n--time accepts these formats: “min”, “min:s”, “h:min:s”, “d-h”, “d-h:min” & “d-h:min:s”\n%x will get replaced by the script name & %j by the job number\n\n\n\nSubmit a job\nFinally, you need to submit your job to Slurm:\n$ sbatch ~/mnist/nn.sh\nYou can check the status of your job with:\n$ sq\n\nPD = pending\nR = running\nCG = completing (Slurm is doing the closing processes)\nNo information = your job has finished running\n\nYou can cancel it with:\n$ scancel &lt;jobid&gt;\nOnce your job has finished running, you can display efficiency measures with:\n$ seff &lt;jobid&gt;",
    "crumbs": [
      "AI",
      "<b><em>Intro DL with PyTorch</em></b>",
      "Example: the MNIST"
    ]
  },
  {
    "objectID": "ai/pt/pt_mnist.html#lets-explore-our-models-metrics-with-tensorboard",
    "href": "ai/pt/pt_mnist.html#lets-explore-our-models-metrics-with-tensorboard",
    "title": "Example: classifying the MNIST dataset",
    "section": "Let’s explore our model’s metrics with TensorBoard",
    "text": "Let’s explore our model’s metrics with TensorBoard\nTensorBoard is a web visualization toolkit developed by TensorFlow which can be used with PyTorch.\nBecause we have sent our model’s metrics logs to TensorBoard as part of our code, a directory called runs with those logs was created in our ~/mnist directory.\n\nLaunch TensorBoard\nTensorBoard requires too much processing power to be run on the login node. When you run long jobs, the best strategy is to launch it in the background as part of the job. This allows you to monitor your model as it is running (and cancel it if things don’t look right).\n\nExample:\n\n#!/bin/bash\n#SBATCH ...\n#SBATCH ...\n\ntensorboard --logdir=runs --host 0.0.0.0 &\npython ~/mnist/nn.py\nBecause we only have 1 GPU and are taking turns running our jobs, we need to keep our jobs very short here. So we will launch a separate job for TensorBoard. This time, we will launch an interactive job:\nsalloc --time=1:0:0 --mem=2000M\nTo launch TensorBoard, we need to activate our Python virtual environment (TensorBoard was installed by pip):\nsource ~/projects/def-sponsor00/env/bin/activate\nThen we can launch TensorBoard in the background:\ntensorboard --logdir=~/mnist/runs --host 0.0.0.0 &\nNow, we need to create a connection with SSH tunnelling between your computer and the compute note running your TensorBoard job.\n\n\nConnect to TensorBoard from your computer\nFrom a new terminal on your computer, run:\nssh -NfL localhost:6006:&lt;hostname&gt;:6006 userxxx@uu.c3.ca\n\nReplace &lt;hostname&gt; by the name of the compute node running your salloc job. You can find it by looking at your prompt (your prompt shows &lt;username&gt;@&lt;hostname&gt;).\nReplace &lt;userxxx&gt; by your user name.\n\nNow, you can open a browser on your computer and access TensorBoard at http://localhost:6006.",
    "crumbs": [
      "AI",
      "<b><em>Intro DL with PyTorch</em></b>",
      "Example: the MNIST"
    ]
  },
  {
    "objectID": "ai/pt/pt_intro.html",
    "href": "ai/pt/pt_intro.html",
    "title": "Introduction to machine learning",
    "section": "",
    "text": "This presentation gives a high-level overview of machine learning.\nI will define concepts, present the different types of learning, and explain the basic functioning of neural networks.\n\nSlides (Click and wait: the presentation might take a few instants to load)",
    "crumbs": [
      "AI",
      "<b><em>Intro DL with PyTorch</em></b>",
      "Introduction"
    ]
  },
  {
    "objectID": "ai/pt/pt_high_level_frameworks.html",
    "href": "ai/pt/pt_high_level_frameworks.html",
    "title": "High-level frameworks for PyTorch",
    "section": "",
    "text": "Several popular higher-level frameworks are built on top of PyTorch and make the code easier to write and run:\nThe following tag trends on Stack Overflow might give an idea of the popularity of these frameworks over time (catalyst doesn’t have any Stack Overflow tag):\nIf this data is to be believed, ignite never really took off (it also has a lower number of stars on GitHub), fast-ai was extremely popular when it came out, but its usage is going down, and PyTorch-lightning is currently the most popular.",
    "crumbs": [
      "AI",
      "<b><em>Intro DL with PyTorch</em></b>",
      "High-level frameworks"
    ]
  },
  {
    "objectID": "ai/pt/pt_high_level_frameworks.html#should-you-use-one-and-which-one",
    "href": "ai/pt/pt_high_level_frameworks.html#should-you-use-one-and-which-one",
    "title": "High-level frameworks for PyTorch",
    "section": "Should you use one (and which one)?",
    "text": "Should you use one (and which one)?\nLearning raw PyTorch is probably the best option for research. PyTorch is stable and here to stay. Higher-level frameworks may rise and drop in popularity and today’s popular one may see little usage tomorrow.\nRaw PyTorch is also the most flexible, the closest to the actual computations happening in your model, and probably the easiest to debug.\nDepending on your deep learning trajectory, you might find some of these tools useful though:\n\nIf you work in industry, you might want or need to get results quickly\nSome operations (e.g. parallel execution on multiple GPUs) can be tricky in raw PyTorch, while being extremely streamlined when using e.g. PyTorch-lightning\nEven in research, it might make sense to spend more time thinking about the structure of your model and the functioning of a network instead of getting bogged down in writing code\n\n\nBefore moving to any of these tools, it is probably a good idea to get a good knowledge of raw PyTorch: use these tools to simplify your workflow, not cloud your understanding of what your code is doing.",
    "crumbs": [
      "AI",
      "<b><em>Intro DL with PyTorch</em></b>",
      "High-level frameworks"
    ]
  },
  {
    "objectID": "ai/pt/pt_concepts.html",
    "href": "ai/pt/pt_concepts.html",
    "title": "Concepts:",
    "section": "",
    "text": "Artificial intelligence is a vast field: any system mimicking animal intelligence falls in its scope.\nMachine learning (ML) is a subfield of artificial intelligence that can be defined as computer programs whose performance at a task improves with experience.\nSince this experience comes in the form of data, ML consists of feeding vast amounts of data to algorithms to strengthen pathways.\n\n\nFrom xkcd.com\n\n\n\nCoding all the possible ways—pixel by pixel—that a picture can represent a certain object is an impossibly large task. By feeding examples of images of that object to a neural network however, we can train it to recognize that object in images that it has never seen (without explicitly programming how it does this!).\n\n\nFrom xkcd.com"
  },
  {
    "objectID": "ai/pt/pt_concepts.html#what-is-machine-learning",
    "href": "ai/pt/pt_concepts.html#what-is-machine-learning",
    "title": "Concepts:",
    "section": "",
    "text": "Artificial intelligence is a vast field: any system mimicking animal intelligence falls in its scope.\nMachine learning (ML) is a subfield of artificial intelligence that can be defined as computer programs whose performance at a task improves with experience.\nSince this experience comes in the form of data, ML consists of feeding vast amounts of data to algorithms to strengthen pathways.\n\n\nFrom xkcd.com\n\n\n\nCoding all the possible ways—pixel by pixel—that a picture can represent a certain object is an impossibly large task. By feeding examples of images of that object to a neural network however, we can train it to recognize that object in images that it has never seen (without explicitly programming how it does this!).\n\n\nFrom xkcd.com"
  },
  {
    "objectID": "ai/pt/pt_concepts.html#types-of-learning",
    "href": "ai/pt/pt_concepts.html#types-of-learning",
    "title": "Concepts:",
    "section": "Types of learning",
    "text": "Types of learning\nThere are now more types of learning than those presented here. But these initial types are interesting because they will already be familiar to you.\n\nSupervised learning\nYou have been doing supervised machine learning for years without looking at it in the framework of machine learning:\n\nRegression is a form of supervised learning with continuous outputs\nClassification is supervised learning with discrete outputs\n\nSupervised learning uses training data in the form of example input/output \\((x_i, y_i)\\) pairs.\nGoal:\nIf \\(X\\) is the space of inputs and \\(Y\\) the space of outputs, the goal is to find a function \\(h\\) so that\nfor each \\(x_i \\in X\\):\n\n\\(h_\\theta(x_i)\\) is a predictor for the corresponding value \\(y_i\\)\n\n(\\(\\theta\\) represents the set of parameters of \\(h_\\theta\\)).\n\n→ i.e. we want to find the relationship between inputs and outputs.\n\n\nUnsupervised learning\nHere too, you are familiar with some forms of unsupervised learning that you weren’t thinking about in such terms:\nClustering, social network analysis, market segmentation, PCA … are all forms of unsupervised learning.\nUnsupervised learning uses unlabelled data (training set of \\(x_i\\)).\nGoal:\nFind structure within the data.\n\n\nFrom xkcd.com"
  },
  {
    "objectID": "ai/pt/pt_concepts.html#artificial-neural-networks",
    "href": "ai/pt/pt_concepts.html#artificial-neural-networks",
    "title": "Concepts:",
    "section": "Artificial neural networks",
    "text": "Artificial neural networks\nArtificial neural networks (ANN) are one of the machine learning models (other models include decision trees or Bayesian networks). Their potential and popularity has truly exploded in recent years and this is what we will focus on in this course.\nArtificial neural networks are a series of layered units mimicking the concept of biological neurons: inputs are received by every unit of a layer, computed, then transmitted to units of the next layer. In the process of learning, experience strengthens some connections between units and weakens others.\nIn biological networks, the information consists of action potentials (neuron membrane rapid depolarizations) propagating through the network. In artificial ones, the information consists of tensors (multidimensional arrays) of weights and biases: each unit passes a weighted sum of an input tensor with an additional—possibly weighted—bias through an activation function before passing on the output tensor to the next layer of units.\n\n\nThe bias allows to shift the output of the activation function to the right or to the left (i.e. it creates an offset).\n\nSchematic of a biological neuron:\n\n\nFrom Dhp1080, Wikipedia\n\nSchematic of an artificial neuron:\n\n\nModified from O.C. Akgun & J. Mei 2019\n\nWhile biological neurons are connected in extremely intricate patterns, artificial ones follow a layered structure. Another difference in complexity is in the number of units: the human brain has 65–90 billion neurons. ANN have much fewer units.\nNeurons in mouse cortex:\n\n\nNeurons are in green, the dark branches are blood vessels. Image by Na Ji, UC Berkeley\n\nNeural network with 2 hidden layers:\n\n\nFrom The Maverick Meerkat\n\nThe information in biological neurons is an all-or-nothing electrochemical pulse or action potential. Greater stimuli don’t produce stronger signals but increase firing frequency. In contrast, artificial neurons pass the computation of their inputs through an activation function and the output can take any of the values possible with that function.\nThreshold potential in biological neurons:\n\n\nModified from Blacktc, Wikimedia\n\nSome of the most common activation functions in artificial neurons:\n\n\nFrom Diganta Misra 2019\n\nWhich activation function to use depends on the type of problem and the available computing budget. Some early functions have fallen out of use while new ones have emerged (e.g. sigmoid got replaced by ReLU which is easier to train).\n\nLearning\nThe process of learning in biological NN happens through neuron death or growth and through the creation or loss of synaptic connections between neurons. In ANN, learning happens through optimization algorithms such as gradient descent which minimize cross entropy loss functions by adjusting the weights and biases connecting each layer of neurons over many iterations (cross entropy is the difference between the predicted and the real distributions).\n\n\nFrom xkcd.com\n\n\n\nGradient descent\nThere are several gradient descent methods:\nBatch gradient descent uses all examples in each iteration and is thus slow for large datasets (the parameters are adjusted only after all the samples have been processed).\nMini-batch gradient descent is an intermediate approach: it uses mini-batch sized examples in each iteration. This allows a vectorized approach (and hence parallelization).\nThe Adam optimization algorithm is a popular variation of mini-batch gradient descent.\nStochastic gradient descent uses one example in each iteration. It is thus much faster than batch gradient descent (the parameters are adjusted after each example). But it does not allow any vectorization.\n\n\nFrom Imad Dabbura\n\n\n\n3Blue1Brown by Grant Sanderson videos\n3Blue1Brown by Grant Sanderson has a series of 4 videos on neural networks which is easy to watch, fun, and does an excellent job at introducing the functioning of a simple neural network.\n\nWhat are NN? (19 min)\n\nWatch this video beyond the acknowledgement as the function ReLU (a really important function in modern neural networks) is introduced at the very end.\n\n\n\nAs you develop your own ML models, if you find that your mathematical background is shaky, 3blue1brown also has an excellent series of videos on linear algebra and an equally great one on calculus.\n\n\n\nHow do NN learn? (21 min)\n\n\n\nWhat is backpropagation? (14 min)\n\n\nThere is one minor terminological error in this video: they call the use of mini-batches stochastic gradient descent. In fact, this is called mini-batch gradient descent. Stochastic gradient descent uses a single example at each iteration.\n\n\n\nHow does backpropagation work? (10 min)\n\n\n\n\nTypes of ANN\n\nFully connected neural networks\n\n\nFrom Glosser.ca, Wikipedia\n\nEach neuron receives inputs from every neuron of the previous layer and passes its output to every neuron of the next layer.\n\n\nConvolutional neural networks\n\n\nFrom Programming Journeys by Rensu Theart\n\nConvolutional neural networks (CNN) are used for spatially structured data (e.g. in image recognition).\nImages have huge input sizes and would require a very large number of neurons in a fully connected neural net. In convolutional layers, neurons receive input from a subarea (called local receptive field) of the previous layer. This greatly reduces the number of parameters.\nOptionally, pooling (combining the outputs of neurons in a subarea) reduces the data dimensions. The stride then dictates how the subarea is moved across the image. Max-pooling is one of the forms of pooling which uses the maximum for each subarea.\n\n\nRecurrent neural networks\n\n\nFrom fdeloche, Wikipedia\n\nRecurrent neural networks (RNN) such as Long Short-Term Memory (LSTM) are used for chain structured data (e.g. in speech recognition).\nThey are not feedforward networks (i.e. networks for which the information moves only in the forward direction without any loop).\n\n\nTransformers\nA combination of two RNNs or sets of RNNs (the encoder and the decoder) is used in sequence to sequence models for translation or picture captioning. Such models were slow to process a lot of data.\nIn 2014 and 2015, the concept of attention (giving added weight to important words) was developed, greatly improving the ability of such models to process a lot of data.\nThis blog post by Jay Alammar—a blogger whose high-quality posts have been referenced in MIT and Stanford courses—explains this in a high-level visual fashion.\nThe problem with recurrence is that it is not easily to parallelize (and thus to run fast on GPUs).\nIn 2017, a new model—the transformer—was proposed: by using only attention mechanisms and no recurrence, the transformer achieves better results in an easily parallelizable fashion.\nJay Alammar has also a blog post on the transformer. The post includes a 30 min video.\nWith the addition of transfer learning, powerful transformers emerged in the field of NLP (Natural Language Processing). Examples include BERT (Bidirectional Encoder Representations from Transformers) from Google and GPT-3 (Generative Pre-trained Transformer-3) from OpenAI.\nJay Alammar has yet another great blog post on these advanced NLP models.\n\n\n\nDeep learning\nThe first layer of a neural net is the input layer. The last one is the output layer. All the layers in-between are called hidden layers. Shallow neural networks have only one hidden layer and deep networks have two or more hidden layers. When an ANN is deep, we talk about Deep Learning (DL).\n\n\nFrom xkcd.com"
  },
  {
    "objectID": "ai/pt/pt_checkpoints.html",
    "href": "ai/pt/pt_checkpoints.html",
    "title": "Saving/loading models and checkpointing",
    "section": "",
    "text": "After you have trained your model, obviously you will want to save it to use it thereafter. You will then need to load it in any session in which you want to use it.\nIn addition to saving or loading a fully trained model, it is important to know how to create regular checkpoints: training ML models takes a long time and a cluster crash or countless other issues might interrupt the training process. You don’t want to have to restart from scratch if that happens.",
    "crumbs": [
      "AI",
      "<b><em>Intro DL with PyTorch</em></b>",
      "Creating checkpoints"
    ]
  },
  {
    "objectID": "ai/pt/pt_checkpoints.html#savingloading-models",
    "href": "ai/pt/pt_checkpoints.html#savingloading-models",
    "title": "Saving/loading models and checkpointing",
    "section": "Saving/loading models",
    "text": "Saving/loading models\n\nSaving models\nYou can save a model by serializing its internal state dictionary. The state dictionary is a Python dictionary that contains the learnable parameters of your model (weights and biases).\ntorch.save(model.state_dict(), \"model.pt\")\n\n\nLoading models\nTo recreate your model, you first need to recreate its structure:\nmodel = TheModelClass(*args, **kwargs)\nThen you can load the state dictionary containing the parameters values into it:\nmodel.load_state_dict(torch.load(\"model.pt\"))\nAssuming you want to use your model for inference, you also must run:\nmodel.eval()\n\nIf instead you want to do more training on your model, you would of course run model.train() instead.",
    "crumbs": [
      "AI",
      "<b><em>Intro DL with PyTorch</em></b>",
      "Creating checkpoints"
    ]
  },
  {
    "objectID": "ai/pt/pt_checkpoints.html#checkpointing",
    "href": "ai/pt/pt_checkpoints.html#checkpointing",
    "title": "Saving/loading models and checkpointing",
    "section": "Checkpointing",
    "text": "Checkpointing\n\nCreating a checkpoint\ntorch.save({\n    'epoch': epoch,\n    'model_state_dict': model.state_dict(),\n    'optimizer_state_dict': optimizer.state_dict(),\n    'loss': loss,\n    ...\n}, \"model.pt\")\n\n\nResuming training from a checkpoint\nRecreate the state of your model from the checkpoint:\nmodel = TheModelClass(*args, **kwargs)\noptimizer = TheOptimizerClass(*args, **kwargs)\n\ncheckpoint = torch.load(\"model.pt\")\nmodel.load_state_dict(checkpoint['model_state_dict'])\noptimizer.load_state_dict(checkpoint['optimizer_state_dict'])\nepoch = checkpoint['epoch']\nloss = checkpoint['loss']\nResume training:\nmodel.train()",
    "crumbs": [
      "AI",
      "<b><em>Intro DL with PyTorch</em></b>",
      "Creating checkpoints"
    ]
  },
  {
    "objectID": "ai/jxbayesian/wb_bayesian_slides.html#two-interpretations-of-probabilities",
    "href": "ai/jxbayesian/wb_bayesian_slides.html#two-interpretations-of-probabilities",
    "title": "Bayesian inference in",
    "section": "Two interpretations of probabilities",
    "text": "Two interpretations of probabilities\n\n\n\n\nFrequentist\n\n\n\n\nImage source\n\n\n\n\n\n\n\nBayesian\n\n\n\n\nImage source"
  },
  {
    "objectID": "ai/jxbayesian/wb_bayesian_slides.html#frequentist",
    "href": "ai/jxbayesian/wb_bayesian_slides.html#frequentist",
    "title": "Bayesian inference in",
    "section": "Frequentist",
    "text": "Frequentist\nFrequentist approach to probabilities: assigns probabilities to the long-run frequency of events\nIt doesn’t assign probabilities to non-random variables such as hypotheses or parameters\nInstead, the probability is assigned to the limit of the relative frequencies of events in infinite trials and we can assign a probability to the fact that a new random sample would produce a confidence interval that contains the unknown parameter\nThis is not how we intuitively think and the results are hard to interpret. This approach is also often artificially constrained and limits the integration of various forms of information\nIt is however computationally simple and fast: samples are randomly selected from the sample space and it returns test statistics such as p-values and confidence intervals. This is why it was the dominant approach for a long time: we knew how to do it"
  },
  {
    "objectID": "ai/jxbayesian/wb_bayesian_slides.html#bayesian",
    "href": "ai/jxbayesian/wb_bayesian_slides.html#bayesian",
    "title": "Bayesian inference in",
    "section": "Bayesian",
    "text": "Bayesian\nBayesian approach: assigns probabilities to our beliefs about an event\nBased on Bayes’ theorem of conditional probabilities which allows to calculate the probability of a cause given its effect:\n\\[ P(A \\vert X) = \\frac{P(X \\vert A) P(A)}{P(X)} \\]\nwhere:\n\n\\(P(A)\\) is the prior probability of \\(A\\)—our belief about event \\(A\\)\n\\(P(X)\\) is the marginal probability of event \\(X\\) (some observed data)\n\\(P(X \\vert A)\\) is the likelihood or conditional probability of observing \\(X\\) given \\(A\\)\n\\(P(A \\vert X)\\) is the posterior probability—our updated belief about \\(A\\) given the data"
  },
  {
    "objectID": "ai/jxbayesian/wb_bayesian_slides.html#which-approach-to-choose",
    "href": "ai/jxbayesian/wb_bayesian_slides.html#which-approach-to-choose",
    "title": "Bayesian inference in",
    "section": "Which approach to choose?",
    "text": "Which approach to choose?\nBayesian statistics:\n\nis more intuitive to the way we think about the world (easier to interpret)\nallows for the incorporation of prior information and diverse data\nis more informative as it provides a measure of uncertainty (returns probabilities)\nis extremely valuable when there is little data (the inference is unstable and frequentist estimates have large variance and confidence intervals)\n\nBut beyond extremely simple examples, Bayesian inference is mathematically extremely arduous\nIt is also much more computationally heavy and only became possible to apply widely with the advent of powerful computers and new algorithms such as Markov chain Monte Carlo (MCMC)"
  },
  {
    "objectID": "ai/jxbayesian/wb_bayesian_slides.html#algorithms",
    "href": "ai/jxbayesian/wb_bayesian_slides.html#algorithms",
    "title": "Bayesian inference in",
    "section": "Algorithms",
    "text": "Algorithms\nA Bayesian approach to statistics often leads to posterior probability distributions that are too complex or too highly dimensional to be studied by analytical techniques\nMarkov chain Monte Carlo (MCMC) is a class of sampling algorithms which explore such distributions\nDifferent algorithms move in different ways across the N-dimensional space of the parameters, accepting or rejecting each new position based on its adherence to the prior distribution and the data\nThe sequence of accepted positions constitute the traces"
  },
  {
    "objectID": "ai/jxbayesian/wb_bayesian_slides.html#probabilistic-programming-language",
    "href": "ai/jxbayesian/wb_bayesian_slides.html#probabilistic-programming-language",
    "title": "Bayesian inference in",
    "section": "Probabilistic Programming Language",
    "text": "Probabilistic Programming Language\nProbabilistic programming language (PPL), explained simply in this (a bit outdated) blog post, are computer languages specialized in creating probabilistic models and making inference\nModel components are first-class primitives\nThey can be based on a general programming language (e.g. Python, Julia) or domain specific"
  },
  {
    "objectID": "ai/jxbayesian/wb_bayesian_slides.html#first-bayesian-ppls",
    "href": "ai/jxbayesian/wb_bayesian_slides.html#first-bayesian-ppls",
    "title": "Bayesian inference in",
    "section": "First Bayesian PPLs",
    "text": "First Bayesian PPLs\nRelied on Gibbs sampling:\n\nWinBUGS replaced by OpenBUGS, written in Component Pascal\nJAGS, written in C++\n\nBUGS = Bayesian inference Using Gibbs Sampling\nJAGS = Just Another Gibbs Sampler"
  },
  {
    "objectID": "ai/jxbayesian/wb_bayesian_slides.html#stan",
    "href": "ai/jxbayesian/wb_bayesian_slides.html#stan",
    "title": "Bayesian inference in",
    "section": "Stan",
    "text": "Stan\nStan (see also website and paper) is a domain-specific language\nStan scripts can be executed from R, Python, or the shell via RStan, PyStan, etc.\nAlso used as the backend for the R package brms which doesn’t require learning Stan but only works for simple models\nRelies on No-U-Turn sampler (NUTS), a variant of Hamiltonian Monte Carlo (HMC) (see also HMC paper)\nHMC and variants require burdensome calculations of derivatives. Stan solved that by creating its own reverse-mode automatic differentiation engine\nSuperior to Gibbs sampler ➔ made Stan a very popular PPL for years"
  },
  {
    "objectID": "ai/jxbayesian/wb_bayesian_slides.html#ppls-based-on-deep-learning-frameworks",
    "href": "ai/jxbayesian/wb_bayesian_slides.html#ppls-based-on-deep-learning-frameworks",
    "title": "Bayesian inference in",
    "section": "PPLs based on deep learning frameworks",
    "text": "PPLs based on deep learning frameworks\nSince HMC and NUTS require autodiff, many Python PPLs have emerged in recent years, following the explosion of deep learning\nExamples:\n\nPyro based on PyTorch\nEdward, then Edward2 as well as TensorFlow Probability based on TensorFlow"
  },
  {
    "objectID": "ai/jxbayesian/wb_bayesian_slides.html#enters-jax",
    "href": "ai/jxbayesian/wb_bayesian_slides.html#enters-jax",
    "title": "Bayesian inference in",
    "section": "Enters JAX",
    "text": "Enters JAX\n\n\nHad JAX existed when we started coding Stan in 2011, we would’ve used that rather than rolling our own autodiff system.\n\n\nBob Carpenter, one of Stan’s creators, in a recent blog post"
  },
  {
    "objectID": "ai/jxbayesian/wb_bayesian_slides.html#what-is-jax",
    "href": "ai/jxbayesian/wb_bayesian_slides.html#what-is-jax",
    "title": "Bayesian inference in",
    "section": "What is JAX?",
    "text": "What is JAX?\nJAX is a library for Python that:\n\nmakes use of the extremely performant XLA compiler\nruns on accelerators (GPUs/TPUs)\nprovides automatic differentiation\nuses just-in-time compilation\nallows batching and parallelization\n\n\n⇒ perfect tool for Bayesian statistics\n\n See our introductory JAX course and webinar for more details"
  },
  {
    "objectID": "ai/jxbayesian/wb_bayesian_slides.html#jax-idiosyncrasies",
    "href": "ai/jxbayesian/wb_bayesian_slides.html#jax-idiosyncrasies",
    "title": "Bayesian inference in",
    "section": "JAX idiosyncrasies",
    "text": "JAX idiosyncrasies\nJAX is sublanguage of Python requiring pure functions instead of Python’s object-oriented style\nIt has other quirks\nThe only one you really need to understand for use in PPLs is the pseudorandom number generation"
  },
  {
    "objectID": "ai/jxbayesian/wb_bayesian_slides.html#prng-keys",
    "href": "ai/jxbayesian/wb_bayesian_slides.html#prng-keys",
    "title": "Bayesian inference in",
    "section": "PRNG keys",
    "text": "PRNG keys\nTraditional pseudorandom number generators are based on nondeterministic state of the OS. This is slow and problematic for parallel executions\nJAX relies on an explicitly-set random state called a key:\nfrom jax import random\nkey = random.key(18)\nEach key can only be used for one random function, but it can be split into new keys:\nkey, subkey = random.split(key)\n\nThe first key can’t be used anymore. We overwrote it with a new key to ensure we don’t accidentally reuse it\n\nWe can now use subkey in random functions in our code (and keep key to generate new subkeys as needed)"
  },
  {
    "objectID": "ai/jxbayesian/wb_bayesian_slides.html#jax-use-cases",
    "href": "ai/jxbayesian/wb_bayesian_slides.html#jax-use-cases",
    "title": "Bayesian inference in",
    "section": "JAX use cases",
    "text": "JAX use cases"
  },
  {
    "objectID": "ai/jxbayesian/wb_bayesian_slides.html#new-jax-backends-added-to-many-ppls",
    "href": "ai/jxbayesian/wb_bayesian_slides.html#new-jax-backends-added-to-many-ppls",
    "title": "Bayesian inference in",
    "section": "New JAX backends added to many PPLs",
    "text": "New JAX backends added to many PPLs\nEdward2 and TensorFlow Probability can now use JAX as backend\nPyMC relies on building a static graph. It is based on PyTensor which provides JAX compilation (PyTensor is a fork of aesara, itself a fork of Theano)"
  },
  {
    "objectID": "ai/jxbayesian/wb_bayesian_slides.html#numpyro",
    "href": "ai/jxbayesian/wb_bayesian_slides.html#numpyro",
    "title": "Bayesian inference in",
    "section": "NumPyro",
    "text": "NumPyro\nNumPyro is a library based on Pyro but using NumPy and JAX"
  },
  {
    "objectID": "ai/jxbayesian/wb_bayesian_slides.html#blackjax",
    "href": "ai/jxbayesian/wb_bayesian_slides.html#blackjax",
    "title": "Bayesian inference in",
    "section": "Blackjax",
    "text": "Blackjax\nNot a PPL but a library of MCMC samplers built on JAX\nCan be used directly if you want to define your own log-probability density functions or can be used with several PPLs to define your model (make sure to translate it to a log-probability function)\nAlso provides building blocks for experimentation with new algorithms"
  },
  {
    "objectID": "ai/jxbayesian/wb_bayesian_slides.html#blackjax-1",
    "href": "ai/jxbayesian/wb_bayesian_slides.html#blackjax-1",
    "title": "Bayesian inference in",
    "section": "Blackjax",
    "text": "Blackjax"
  },
  {
    "objectID": "ai/jxbayesian/wb_bayesian_slides.html#example-blackjax-sampler-hmc",
    "href": "ai/jxbayesian/wb_bayesian_slides.html#example-blackjax-sampler-hmc",
    "title": "Bayesian inference in",
    "section": "Example Blackjax sampler: HMC",
    "text": "Example Blackjax sampler: HMC"
  },
  {
    "objectID": "ai/jxbayesian/wb_bayesian_slides.html#example-blackjax-sampler-nuts",
    "href": "ai/jxbayesian/wb_bayesian_slides.html#example-blackjax-sampler-nuts",
    "title": "Bayesian inference in",
    "section": "Example Blackjax sampler: NUTS",
    "text": "Example Blackjax sampler: NUTS"
  },
  {
    "objectID": "ai/jxbayesian/wb_bayesian_slides.html#which-tool-to-choose",
    "href": "ai/jxbayesian/wb_bayesian_slides.html#which-tool-to-choose",
    "title": "Bayesian inference in",
    "section": "Which tool to choose?",
    "text": "Which tool to choose?\nAll these tools are in active development (JAX was released and started shaking the field in 2018). Things are fast evolving. Reading blogs of main developers, posts on Hacker News, discourse forums, etc. helps to keep an eye on evolutions in the field\nThis recent conversation between Bob Carpenter (Stan core developer) and Ricardo Vieira (PyMC core developer) in the PyMC discourse forum is interesting\nA lot of it also comes down to user preferences"
  },
  {
    "objectID": "ai/jxbayesian/wb_bayesian_slides.html#how-to-get-started-with-bayesian-computing",
    "href": "ai/jxbayesian/wb_bayesian_slides.html#how-to-get-started-with-bayesian-computing",
    "title": "Bayesian inference in",
    "section": "How to get started with Bayesian computing?",
    "text": "How to get started with Bayesian computing?\nThe book Probabilistic Programming & Bayesian Methods for Hackers by Cameron Davidson-Pilon provides a code-based (using PyMC) and math-free introduction to Bayesian methods for the real beginner\nSeveral resources on the PyMC website including intro Bayesian with PyMC\nNumPyro tutorials\nMore advanced: tutorials from Blackjax Sampling Book Project"
  },
  {
    "objectID": "ai/jxbayesian/wb_bayesian_slides.html#how-to-transition-from-stan-to-a-jax-based-ppl",
    "href": "ai/jxbayesian/wb_bayesian_slides.html#how-to-transition-from-stan-to-a-jax-based-ppl",
    "title": "Bayesian inference in",
    "section": "How to transition from Stan to a JAX-based PPL?",
    "text": "How to transition from Stan to a JAX-based PPL?\nThe code to the classic Bayesian textbook Statistical Rethinking by Richard McElreath got translated by various people to modern JAX-based PPLs:"
  },
  {
    "objectID": "ai/jxai/fl_training.html",
    "href": "ai/jxai/fl_training.html",
    "title": "Training models",
    "section": "",
    "text": "We talked about how Flax handles state, about loading data, and about model architecture. It is now time to talk about training.\nTraining models is the crux of deep learning. This is the part that requires a lot of time and resources (and money if you use commercial cloud services). This is also where issues with convergence, underfitting or overfitting, and vanishing or exploding gradients can come in.\nConsequently, this is where optimizations and JAX’s performance tricks (e.g. JIT compilation) matter the most. This is also where understanding of deep learning theory is important.\nIn this section, we will point to strategies and resources to navigate training. We will also see how to use the Alliance clusters to train your models."
  },
  {
    "objectID": "ai/jxai/fl_training.html#fundamental-functioning",
    "href": "ai/jxai/fl_training.html#fundamental-functioning",
    "title": "Training models",
    "section": "Fundamental functioning",
    "text": "Fundamental functioning\n\nCalculate predictions\nWe can create some random inputs:\nkey, x_key = jax.random.split(key)\n\nx = jax.random.normal(x_key, (1, 28, 28, 1))\nThe predictions of our model based on these inputs are obtained by:\ny = cnn.apply(params, x)\nprint(y)\n\n\nUpdate parameters\nOptax—another library built on JAX—is a full toolkit for gradient processing and optimization. It contains all the classic optimizers and loss functions and makes it easy to create your own optimizers and optimizer schedulers. Flax initially used its own optimizers but has now fully adopted use of Optax.\nHere is the most basic case:\nimport optax\n\nlearning_rate = 1e-1\noptimiser = optax.sgd(learning_rate)\nprint(optimiser)\nThe optimizer is a gradient transformation. It is a tuple of an init and an update methods. Those are pure functions following the model of JAX and Flax. This means that they are stateless and that a state needs to be initialized and passed as input, exactly as we saw for Flax models.\nLet’s initialize the optimizer state:\noptimiser_state = optimiser.init(params)\nThe update method returns a gradient transformation (that we can later apply to the gradients) and an updated optimizer state.\nThe gradients are calculated by passing a loss function to jax.grad and passing the parameters, the inputs, and the predictions to this transformed function (the derivative):\ngrads = jax.grad(&lt;some-loss-function&gt;)(params, x, y)\nThe loss function can be built from a large array of Optax loss methods.\nHere is how to use optimizer.update:\nupdates, optimiser_state = optimiser.update(grads, optimiser_state, params)"
  },
  {
    "objectID": "ai/jxai/fl_training.html#key-regularizations",
    "href": "ai/jxai/fl_training.html#key-regularizations",
    "title": "Training models",
    "section": "Key regularizations",
    "text": "Key regularizations\nFlax makes it easy to apply classic regularizations and optimization techniques.\nBatch normalization improves convergence speed and has been a classic regularization technique since the publication of Sergey Ioffe and Christian Szegedy’s key paper in 2015. You can use it by adding a flax.linen.BatchNorm layer to your model.\nSimilarly, dropout techniques are implemented with a flax.linen.Dropout layer."
  },
  {
    "objectID": "ai/jxai/fl_training.html#getting-started",
    "href": "ai/jxai/fl_training.html#getting-started",
    "title": "Training models",
    "section": "Getting started",
    "text": "Getting started\nThe best way to get started building your own model is to go over the examples provided as template by Flax. They all follow the same format, making it easy to clone and modify them. You can even modify them directly in Google Colab for some of them, making experimentation easy without having to install anything.\n\nNote however that things are not as simple as the documentation makes it to appear and some of the examples will not run for various reasons (dependency problem, error in code, etc.)\n\nLet’s check this structure and look at a few models.\nThen let’s run the ogbg-molpcba example together in Google Colab to have access to a free GPU."
  },
  {
    "objectID": "ai/jxai/fl_training.html#running-flax-examples-in-the-alliance-clusters",
    "href": "ai/jxai/fl_training.html#running-flax-examples-in-the-alliance-clusters",
    "title": "Training models",
    "section": "Running Flax examples in the Alliance clusters",
    "text": "Running Flax examples in the Alliance clusters\nInstead of running these examples in Google Colab, you might want to run them on the Alliance clusters, particularly as you start developing your own model (rather than just run examples to learn techniques).\nFirst, you need to get the model you are interested in to the cluster.\nThere are many ways you could go about it, but one option is to download the directory of that particular model to your machine as a zip file using one of several sites making this easy.\nFor the ogbg-molpcba example, you paste the link “https://github.com/google/flax/tree/main/examples/ogbg_molpcba” in the site.\nYou can then copy it to the cluster with:\nscp &lt;path-to-zip-file-on-your-machine&gt; &lt;user-name&gt;@&lt;hostname&gt;:\nIt will look something like this (make sure to rename the zip file to remove the spaces or to quote the path):\nscp examples-ogbg_molpcba.zip userxx@xxx.c3.ca:\nThen you could run it using JupyterLab, but a more efficient method is to use sbatch.\nCreate a script:\n\n\n&lt;your_job&gt;.sh\n\n#!/bin/bash\n#SBATCH --account=def-&lt;your_account&gt;\n#SBATCH --time=xxx\n#SBATCH --mem-per-cpu=xxx\n#SBATCH --cpus-per-task=xxx\n#SBATCH --job-name=\"&lt;your_job&gt;\"\n\n# Setup\nmodule load python/3.11.5\nsource ~/env/bin/activate\npython -m pip install --upgrade pip --no-index\npython -m pip install -r requirements.txt --no-index\n\n# Run example\npython main.py --workdir=./ogbg_molpcba --config=configs/default.py\n\nAnd run the script:\nsbatch &lt;your_job&gt;.sh"
  },
  {
    "objectID": "ai/jxai/fl_run.html",
    "href": "ai/jxai/fl_run.html",
    "title": "Running JAX",
    "section": "",
    "text": "This sections covers important considerations on how to use resources efficiently when training neural networks and instructions on how to run code during this course.",
    "crumbs": [
      "AI",
      "<b><em>Intro DL with JAX</em></b>",
      "Running JAX"
    ]
  },
  {
    "objectID": "ai/jxai/fl_run.html#resource-efficient-workflow",
    "href": "ai/jxai/fl_run.html#resource-efficient-workflow",
    "title": "Running JAX",
    "section": "Resource efficient workflow",
    "text": "Resource efficient workflow\n\nCode prototyping\nPython being an interpreted language, it makes sense to prototype code in an interactive fashion. There are many options of this, including:\n\nlaunching the Python REPL (which finally saw a little refresh with Python 3.13 but still remains very austere)\nusing the much more powerful IPython shell,\nusing the even more powerful ptpython (prompt toolkit) shell,\nusing the previous two combined (ptpython integrates with IPython thanks to its ptipython executable),\nusing Emacs as a Python IDE,\nusing JupyterLab.\n\nTraining a model requires a lot of resources. You might need multiple GPUs or entire nodes. It would be silly to have so much resource sit idle for hours while you are typing in a Jupyter notebook or thinking about your code.\nThe answer is to prototype code in an interactive environment at a very small scale (e.g. on a tiny subsample of data) until you have a program (a script) that works.\n\n\nScaling things up\nOnce you are confident that your code is good, you can scale it up on more hardware (not all at once, do multiple tests at increasingly larger scale so that you don’t wait for three weeks for results that don’t work).\nFor this part, it is best to SSH into a cluster and launch a batch Slurm job.\nOne of the great things about JAX is that the same code runs on any device so you can test the code on your machine on CPUs and then run it as is on a clusters on GPU.",
    "crumbs": [
      "AI",
      "<b><em>Intro DL with JAX</em></b>",
      "Running JAX"
    ]
  },
  {
    "objectID": "ai/jxai/fl_run.html#our-workflow-for-this-course",
    "href": "ai/jxai/fl_run.html#our-workflow-for-this-course",
    "title": "Running JAX",
    "section": "Our workflow for this course",
    "text": "Our workflow for this course\nWe will mostly use a JupyterHub during this course to play with snippets of code. When we get to really trying to train a model, we will log in to our training cluster via SSH and submit jobs to the Slurm scheduler.\n\nAccessing our temporary JupyterHub\n\nGo to the etherpad shared during the course to claim a username,\ngo to the URL of the JupyterHub for this course,\nsign in with the username you claimed and the password we gave you,\nleave OTP blank,\nthe server options are good as they are unless you want to bump the time a little (e.g. to 1.5h),\npress start,\nstart a Python notebook: click on the button “Python 3” in the “Notebook” section (top row of buttons).\n\nThe packages for this course are already installed in the JupyterHub.\nIf you don’t need all the time you asked for after all, you should log out (the resources you are using on a cluster are shared amongst many people and when resources are allocated to you, they aren’t available to other people. So it is a good thing not to ask for unnecessary resources and have them sit idle when others could be using them).\nTo log out, click on “File” in the top menu and select “Log out” at the very bottom.\nIf you would like to make a change to the information you entered on the server option page after you have pressed “start”, log out in the same way, log back in, edit the server options, and press start again.\n\nNote that this JupyterHub will be destroyed at the end of the course.\n\n\n\nLogging in through SSH\n\nOpen a terminal emulator\nWindows users:  Install the free version of MobaXTerm and launch it.\nmacOS users:   Launch Terminal.\nLinux users:     Open the terminal emulator of your choice.\n\n\nAccess the cluster through secure shell\n\nWindows users\nFollow the first 18% of this demo.\nFor “Remote host”, use the hostname we gave you.\nSelect the box “Specify username” and provide your username.\n\nNote that the password is entered through blind typing, meaning that you will not see anything happening as you type it. This is a Linux feature. While it is a little disturbing at first, do know that it is working. Make sure to type it slowly to avoid typos, then press the “enter” key on your keyboard.\n\n\n\nmacOS and Linux users\nIn the terminal, run:\nssh &lt;username&gt;@&lt;hostname&gt;\n\nReplace the username and hostname by their values. For instance:\nssh user21@somecluster.c3.ca\n\nYou will be asked a question, answer “Yes”.\nWhen prompted, type the password.\n\nNote that the password is entered through blind typing, meaning that you will not see anything happening as you type it. This is a Linux feature. While it is a little disturbing at first, do know that it is working. Make sure to type it slowly to avoid typos, then press the “enter” key on your keyboard.\n\n\n\n\nTroubleshooting\nProblems logging in are almost always due to typos. If you cannot log in, retry slowly, entering your password carefully.",
    "crumbs": [
      "AI",
      "<b><em>Intro DL with JAX</em></b>",
      "Running JAX"
    ]
  },
  {
    "objectID": "ai/jxai/fl_regularization.html",
    "href": "ai/jxai/fl_regularization.html",
    "title": "Regularization",
    "section": "",
    "text": "Regularization"
  },
  {
    "objectID": "ai/jxai/fl_regularization.html#batch-normalization",
    "href": "ai/jxai/fl_regularization.html#batch-normalization",
    "title": "Regularization",
    "section": "Batch normalization",
    "text": "Batch normalization\nBatch normalization"
  },
  {
    "objectID": "ai/jxai/fl_regularization.html#dropout",
    "href": "ai/jxai/fl_regularization.html#dropout",
    "title": "Regularization",
    "section": "Dropout",
    "text": "Dropout\nDropout"
  },
  {
    "objectID": "ai/jxai/fl_numpy.html",
    "href": "ai/jxai/fl_numpy.html",
    "title": "Relation to NumPy",
    "section": "",
    "text": "NumPy is a popular Python scientific API at the core of many libraries. JAX uses a NumPy-inspired API. There are however important differences that we will explore in this section.",
    "crumbs": [
      "AI",
      "<b><em>Intro DL with JAX</em></b>",
      "Relation to NumPy"
    ]
  },
  {
    "objectID": "ai/jxai/fl_numpy.html#a-numpy-inspired-api",
    "href": "ai/jxai/fl_numpy.html#a-numpy-inspired-api",
    "title": "Relation to NumPy",
    "section": "A NumPy-inspired API",
    "text": "A NumPy-inspired API\nNumPy being so popular, JAX comes with a convenient high-level wrapper to NumPy: jax.numpy.\n\nBeing familiar with NumPy is thus an advantage to get started with JAX. The NumPy quickstart is a useful resource.\n\n\nFor a more efficient usage, JAX also comes with a lower-level API: jax.lax.\n\n\nNumPyJAX NumPy\n\n\n\nimport numpy as np\n\n\nprint(np.array([(1, 2, 3), (4, 5, 6)]))\n\n[[1 2 3]\n [4 5 6]]\n\n\n\nprint(np.zeros((2, 3)))\n\n[[0. 0. 0.]\n [0. 0. 0.]]\n\n\n\nprint(np.ones((2, 3, 2)))\n\n[[[1. 1.]\n  [1. 1.]\n  [1. 1.]]\n\n [[1. 1.]\n  [1. 1.]\n  [1. 1.]]]\n\n\n\nprint(np.arange(24).reshape(2, 3, 4))\n\n[[[ 0  1  2  3]\n  [ 4  5  6  7]\n  [ 8  9 10 11]]\n\n [[12 13 14 15]\n  [16 17 18 19]\n  [20 21 22 23]]]\n\n\n\nprint(np.linspace(0, 2, 9))\n\n[0.   0.25 0.5  0.75 1.   1.25 1.5  1.75 2.  ]\n\n\n\nprint(np.linspace(0, 2, 9)[::-1])\n\n[2.   1.75 1.5  1.25 1.   0.75 0.5  0.25 0.  ]\n\n\n\n\nimport jax.numpy as jnp\nprint(jnp.array([(1, 2, 3), (4, 5, 6)]))\n[[1 2 3]\n [4 5 6]]\nprint(jnp.zeros((2, 3)))\n[[0. 0. 0.]\n [0. 0. 0.]]\nprint(jnp.ones((2, 3, 2)))\n[[[1. 1.]\n  [1. 1.]\n  [1. 1.]]\n\n [[1. 1.]\n  [1. 1.]\n  [1. 1.]]]\nprint(jnp.arange(24).reshape(2, 3, 4))\n[[[ 0  1  2  3]\n  [ 4  5  6  7]\n  [ 8  9 10 11]]\n\n [[12 13 14 15]\n  [16 17 18 19]\n  [20 21 22 23]]]\nprint(jnp.linspace(0, 2, 9))\n[0.   0.25 0.5  0.75 1.   1.25 1.5  1.75 2.  ]\nprint(jnp.linspace(0, 2, 9)[::-1])\n[2.   1.75 1.5  1.25 1.   0.75 0.5  0.25 0.  ]\n\n\n\nDespite the similarities, there are important differences between JAX and NumPy.",
    "crumbs": [
      "AI",
      "<b><em>Intro DL with JAX</em></b>",
      "Relation to NumPy"
    ]
  },
  {
    "objectID": "ai/jxai/fl_numpy.html#differences-with-numpy",
    "href": "ai/jxai/fl_numpy.html#differences-with-numpy",
    "title": "Relation to NumPy",
    "section": "Differences with NumPy",
    "text": "Differences with NumPy\n\nDifferent types\ntype(np.zeros((2, 3))) == type(jnp.zeros((2, 3)))\nFalse\n\nNumpyJAX NumPy\n\n\n\ntype(np.zeros((2, 3)))\n\nnumpy.ndarray\n\n\n\n\ntype(jnp.zeros((2, 3)))\njaxlib.xla_extension.ArrayImpl\n\n\n\n\n\nDifferent default data types\n\nNumpyJAX NumPy\n\n\n\nnp.zeros((2, 3)).dtype\n\ndtype('float64')\n\n\n\n\njnp.zeros((2, 3)).dtype\ndtype('float32')\n\n\n\n\nLower numerical precision improves speed and reduces memory usage at no cost while training neural networks and is thus a net benefit. Having been built with deep learning in mind, JAX defaults align with that of other DL libraries (e.g. PyTorch, TensorFlow).\n\n\n\nImmutable arrays\n\nNumpyJAX NumPy\n\n\nIn NumPy, you can modify ndarrays:\n\na = np.arange(5)\na[0] = 9\nprint(a)\n\n[9 1 2 3 4]\n\n\n\n\nJAX arrays are immutable:\na = jnp.arange(5)\na[0] = 9\nTypeError: '&lt;class 'jaxlib.xla_extension.ArrayImpl'&gt;' object does not support item assignment. JAX arrays are immutable. Instead of ``x[idx] = y``, use ``x = x.at[idx].set(y)`` or another .at[] method: https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.ndarray.at.html\nInstead, you need to create a copy of the array with the mutation. This is done with:\nb = a.at[0].set(9)\nprint(b)\n[9 1 2 3 4]\nOf course, you can overwrite a:\na = a.at[0].set(9)\n\n\n\n\n\nPseudorandom number generation\nProgramming languages usually come with automated pseudorandom number generator (PRNG) based on nondeterministic data from the operating system. They are extremely convenient, but slow, based on repeats, and problematic in parallel executions.\nJAX relies on an explicitly set random state called a key.\nfrom jax import random\n\nkey = random.key(18)\nprint(key)\n[ 0 18]\nEach time you call a random function, you need a subkey split from your key. Keys should only ever be used once in your code. The key is what makes your code reproducible, but you don’t want to reuse it within your code as it would create spurious correlations.\nHere is the workflow:\n\nyou split your key into a new key and one or multiple subkeys,\nyou discard the old key (because it was used to do the split—so its entropy budget, so to speak, has been used),\nyou use the subkey(s) to run your random function(s) and keep the new key for a future split.\n\n\nSubkeys are of the same nature as keys. This is just a terminology.\n\nTo make sure not to reuse the old key, you can overwrite it by the new one:\nkey, subkey = random.split(key)\nprint(key)\n[4197003906 1654466292]\n\nThat’s the value of our new key for future splits.\n\nprint(subkey)\n[1685972163 1654824463]\n\nThis is the value of the subkey that we can use to call a random function.\n\nLet’s use that subkey now:\nprint(random.normal(subkey))\n1.1437175\n\nTo split your key into more subkeys, pass an argument to random.split:\nkey, subkey1, subkey2, subkey3 = random.split(key, 4)\n\n\n\nStrict input control\n\nNumpyJAX NumPy\n\n\nNumPy’s fundamental object is the ndarray, but NumPy is very tolerant as to the type of input.\n\nnp.sum([1.0, 2.0])  # here we are using a list\n\nnp.float64(3.0)\n\n\n\nnp.sum((1.0, 2.0))  # here is a tuple\n\nnp.float64(3.0)\n\n\n\n\nTo avoid inefficiencies, JAX will only accept arrays.\njnp.sum([1.0, 2.0])\nTypeError: sum requires ndarray or scalar arguments, got &lt;class 'list'&gt; at position 0.\njnp.sum((1.0, 2.0))\nTypeError: sum requires ndarray or scalar arguments, got &lt;class 'tuple'&gt; at position 0.\n\n\n\n\n\nOut of bounds indexing\n\nNumpyJAX NumPy\n\n\nNumPy will warn you with an error message if you try to index out of bounds:\n\nprint(np.arange(5)[10])\n\n\n---------------------------------------------------------------------------\nIndexError                                Traceback (most recent call last)\nCell In[13], line 1\n----&gt; 1 print(np.arange(5)[10])\n\nIndexError: index 10 is out of bounds for axis 0 with size 5\n\n\n\n\n\nBe aware that JAX will not raise an error. Instead, it will silently return the closest boundary:\nprint(jnp.arange(5)[10])\n4\n\n\n\n\n\nFunctionally pure functions\nMore importantly, only functionally pure functions—that is, functions for which the outputs are only based on the inputs and which have no side effects—can be used with JAX.\n\nOutputs only based on inputs\nConsider the function:\ndef f(x):\n    return a + x\nwhich uses the variable a from the global environment.\nThis function is not functionally pure because the outputs (the results of the function) do not solely depend on the arguments (the values given to x) passed to it. They also depend on the value of a.\nRemember how tracing works: new inputs with the same shape and dtype use the cached compiled program directly. If the value of a changes in the global environment, a new tracing is not triggered and the cached compiled program uses the old value of a (the one that was used during tracing).\nIt is only if the code is run on an input x with a different shape and/or dtype that tracing happens again and that the new value for a takes effect.\nfrom jax import jit\n\na = jnp.ones(3)\nprint(a)\n[1. 1. 1.]\ndef f(x):\n    return a + x\n\nprint(jit(f)(jnp.ones(3)))\n[2. 2. 2.]\n\nAll good here because this is the first run (tracing).\n\nNow, let’s change the value of a to an array of zeros:\na = jnp.zeros(3)\nprint(a)\n[0. 0. 0.]\nAnd rerun the same code:\nprint(jit(f)(jnp.ones(3)))\n[2. 2. 2.]\nWe should have an array of ones, but we get the same result we got earlier. Why? because we are running a cached program with the value that a had during tracing.\nThe new value for a will only take effect if we re-trigger tracing by changing the shape and/or dtype of x:\na = jnp.zeros(4)\nprint(a)\n[0. 0. 0. 0.]\nprint(jit(f)(jnp.ones(4)))\n[1. 1. 1. 1.]\nPassing an argument of a different shape to f forced recompilation. Using a different data type (e.g. with jnp.arange(3)) would have done the same.\n\n\nNo side effects\nA function is said to have a side effect if it changes something outside of its local environment (if it does anything beside returning an output).\nExamples of side effects include:\n\nprinting to standard output/shell,\nreading from file/writing to file,\nmodifying a global variable.\n\nIn JAX, the side effects will happen during the first run (tracing), but will not happen on subsequent runs. You thus cannot rely on side effects in your code.\ndef f(a, b):\n    print(\"Calculating sum\")\n    return a + b\n\nprint(jit(f)(jnp.arange(3), jnp.arange(3)))\nCalculating sum\n[0 2 4]\n\nPrinting (the side effect) happened here because this is the first run.\n\nLet’s rerun the function:\nprint(jit(f)(jnp.arange(3), jnp.arange(3)))\n[0 2 4]\nThis time, no printing.\n\nUnderstanding jaxprs\nJaxprs are created by tracers wrapping the Python code during compilation (the first run). They contain information on the shape and data type of arrays as well as the operations performed on these arrays. Jaxprs do not however contain information on values: this allows the compiled program to be general enough to be rerun with any new arrays of the same shape and data type without having to rerun the slow Python code and recompile.\nJaxprs also do not contain any information on elements that are not part of the inputs such as external variables, nor do they contain information on side effects.\nJaxprs can be visualized with the jax.make_jaxpr function:\nimport jax\n\nx = jnp.array([1., 4., 3.])\ny = jnp.array([8., 1., 2.])\n\ndef f(x, y):\n    return 2 * x**2 + y\n\njax.make_jaxpr(f)(x, y) \n{ lambda ; a:f32[3] b:f32[3]. let\n    c:f32[3] = integer_pow[y=2] a\n    d:f32[3] = mul 2.0 c\n    e:f32[3] = add d b\n  in (e,) }\nLet’s add a print function to f:\ndef f(x, y):\n    print(\"This is a function with side-effect\")\n    return 2 * x**2 + y\n\njax.make_jaxpr(f)(x, y)\n{ lambda ; a:f32[3] b:f32[3]. let\n    c:f32[3] = integer_pow[y=2] a\n    d:f32[3] = mul 2.0 c\n    e:f32[3] = add d b\n  in (e,) }\nThe jaxpr is exactly the same. This is why printing will happen during tracing (when the Python code is run), but not afterwards (when the compiled code using the jaxpr is run).",
    "crumbs": [
      "AI",
      "<b><em>Intro DL with JAX</em></b>",
      "Relation to NumPy"
    ]
  },
  {
    "objectID": "ai/jxai/fl_numpy.html#why-the-constraints",
    "href": "ai/jxai/fl_numpy.html#why-the-constraints",
    "title": "Relation to NumPy",
    "section": "Why the constraints?",
    "text": "Why the constraints?\nThe more constraints you add to a programming language, the more optimization you can get from the compiler. Speed comes at the cost of convenience.\nFor instance, consider a Python list. It is an extremely convenient and flexible object: heterogeneous, mutable… You can do anything with it. But computations on lists are extremely slow.\nNumPy’s ndarrays are more constrained (homogeneous), but the type constraint permits the creation of a much faster language (NumPy is written in C and Fortran as well as Python) with vectorization, optimizations, and a greatly improved performance.\nJAX takes it further: by using an intermediate representation and very strict constraints on type, pure functional programming, etc., yet more optimizations can be achieved and you can optimize your own functions with JIT compilation and the XLA. Ultimately, this is what makes JAX so fast.",
    "crumbs": [
      "AI",
      "<b><em>Intro DL with JAX</em></b>",
      "Relation to NumPy"
    ]
  },
  {
    "objectID": "ai/jxai/fl_numpy.html#the-good-news",
    "href": "ai/jxai/fl_numpy.html#the-good-news",
    "title": "Relation to NumPy",
    "section": "The good news",
    "text": "The good news\nThe good news is that Flax used to rely on the Linen API which followed JAX closely. It was very elegant and respected JAX extremely closely: updating model parameters and optimizer state could not be done as a side-effect and the models were thus stateless. Stateless models frameworks follow a functional programming approach in which the parameters are separate from the model and passed as inputs to the forward pass along with the data. This is also the case of the Julia package Lux (a modern rewrite of Flux with explicit model parameters and a philosophy similar to JAX’s).\nElegant, yes, but nobody was using Flax because it was just too obscure. People were using libraries such as Equinox instead because they were a lot easier and more familiar to PyTorch users.\nFlax entirely changed its API. The new API (NNX) now deals with stateful models à la PyTorch. JAX’s idiosyncrasies are mostly dealt with by Flax under the hood (you still need to be aware of them though to prevent you from making mistakes when dealing with jnp.array) and Flax code is now not so dissimilar from PyTorch while still making use of the great AD, JIT, XLA, and same code on all devices.",
    "crumbs": [
      "AI",
      "<b><em>Intro DL with JAX</em></b>",
      "Relation to NumPy"
    ]
  },
  {
    "objectID": "ai/jxai/fl_linen.html",
    "href": "ai/jxai/fl_linen.html",
    "title": "The Linen API",
    "section": "",
    "text": "In Flax, the base class for neural networks is the flax.linen.Module. Linen is a new API replacing the initial flax.nn API and taking full advantage of JAX transformations while automating the initialization and handling of the parameters.\nLinen is imported this way:\nfrom flax import linen as nn\nTo define a model, you create a subclass. The syntax closely resembles that of PyTorch torch.nn:\nclass Net(nn.Module):\n    ...\nBut unlike in PyTorch, the parameters are passed through the model in the form of Pytrees (nested containers such as dictionaries, lists, and tuples)."
  },
  {
    "objectID": "ai/jxai/fl_linen.html#the-linen-api",
    "href": "ai/jxai/fl_linen.html#the-linen-api",
    "title": "The Linen API",
    "section": "",
    "text": "In Flax, the base class for neural networks is the flax.linen.Module. Linen is a new API replacing the initial flax.nn API and taking full advantage of JAX transformations while automating the initialization and handling of the parameters.\nLinen is imported this way:\nfrom flax import linen as nn\nTo define a model, you create a subclass. The syntax closely resembles that of PyTorch torch.nn:\nclass Net(nn.Module):\n    ...\nBut unlike in PyTorch, the parameters are passed through the model in the form of Pytrees (nested containers such as dictionaries, lists, and tuples)."
  },
  {
    "objectID": "ai/jxai/fl_jax_slides.html#what-is-jax",
    "href": "ai/jxai/fl_jax_slides.html#what-is-jax",
    "title": "A brief intro to",
    "section": "What is JAX?",
    "text": "What is JAX?\n\nLibrary for Python developed by Google\n\n\nKey data structure: Array\n\n\nComposition, transformation, and differentiation of numerical programs\n\n\nCompilation for CPUs, GPUs, and TPUs\n\n\nNumPy-like and lower-level APIs\n\n\nRequires strict functional programming"
  },
  {
    "objectID": "ai/jxai/fl_jax_slides.html#fast",
    "href": "ai/jxai/fl_jax_slides.html#fast",
    "title": "A brief intro to",
    "section": "Fast",
    "text": "Fast\n\nDefault data type suited for deep learning\nLike PyTorch, uses float32 as default. This level of precision is suitable for deep learning and increases efficiency (by contrast, NumPy defaults to float64)\nJIT compilation\nThe same code can run on CPUs or on accelerators (GPUs and TPUs)\nXLA (Accelerated Linear Algebra) optimization\nAsynchronous dispatch\nVectorization, data parallelism, and sharding\nAll levels of shared and distributed memory parallelism are supported"
  },
  {
    "objectID": "ai/jxai/fl_jax_slides.html#great-ad",
    "href": "ai/jxai/fl_jax_slides.html#great-ad",
    "title": "A brief intro to",
    "section": "Great AD",
    "text": "Great AD\n\n\n\n\n\n\n\n\n\n01\n\n\nAutodiff method\n\n\n\n1\nStatic graph\nand XLA\n\n\n\n\n02\n\n\nFramework\n\n\n\n\n2\nDynamic graph\n\n\n\n1-&gt;2\n\n\n\n\n\na\n\nTensorFlow\n\n\n\n\n4\nDynamic graph\nand XLA\n\n\n\n2-&gt;4\n\n\n\n\n\nb\n\nPyTorch\n\n\n\n\n5\nPseudo-dynamic\nand XLA\n\n\n\n4-&gt;5\n\n\n\n\n\nd\n\nTensorFlow2\n\n\n\n\ne\n\nJAX\n\n\n\n\n\n03\n\n\nAdvantage\n\n\n\n\n\n7\nMostly\noptimized AD\n\n\n\n\n\n8\nConvenient\n\n\n\n\n\n9\nConvenient\n\n\n\n\n10\nConvenient and\nmostly optimized AD\n\n\n\n\n\n04\n\n\nDisadvantage\n\n\n\n\n\nA\nManual writing of IR\n\n\n\n\n\nB\nLimited AD optimization\n\n\n\n\n\nD\nDisappointing speed\n\n\n\n\nE\nPure functions\n\n\n\n\n\n\n\n\n\n\n\n\n\n  Summarized from a blog post by Chris Rackauckas"
  },
  {
    "objectID": "ai/jxai/fl_jax_slides.html#close-to-the-math",
    "href": "ai/jxai/fl_jax_slides.html#close-to-the-math",
    "title": "A brief intro to",
    "section": "Close to the math",
    "text": "Close to the math\nConsidering the function f:\nf = lambda x: x**3 + 2*x**2 - 3*x + 8\nWe can create a new function dfdx that computes the gradient of f w.r.t. x:\nfrom jax import grad\n\ndfdx = grad(f)\ndfdx returns the derivatives:\nprint(dfdx(1.))\n4.0"
  },
  {
    "objectID": "ai/jxai/fl_jax_slides.html#forward-and-reverse-modes",
    "href": "ai/jxai/fl_jax_slides.html#forward-and-reverse-modes",
    "title": "A brief intro to",
    "section": "Forward and reverse modes",
    "text": "Forward and reverse modes\n\nreverse-mode vector-Jacobian products: jax.vjp\nforward-mode Jacobian-vector products: jax.jvp"
  },
  {
    "objectID": "ai/jxai/fl_jax_slides.html#higher-order-differentiation",
    "href": "ai/jxai/fl_jax_slides.html#higher-order-differentiation",
    "title": "A brief intro to",
    "section": "Higher-order differentiation",
    "text": "Higher-order differentiation\nWith a single variable, the grad function calls can be nested:\nd2fdx = grad(dfdx)   # function to compute 2nd order derivatives\nd3fdx = grad(d2fdx)  # function to compute 3rd order derivatives\n...\nWith several variables, you have to use the functions:\n\njax.jacfwd for forward-mode,\njax.jacrev for reverse-mode."
  },
  {
    "objectID": "ai/jxai/fl_jax_slides.html#how-does-it-work",
    "href": "ai/jxai/fl_jax_slides.html#how-does-it-work",
    "title": "A brief intro to",
    "section": "How does it work?",
    "text": "How does it work?\n\n\n\n\n\n\n\n\n\ntracer\n\nTracing\n\n\n\njaxpr\n\nJaxprs\n(JAX expressions)\nintermediate\nrepresentation\n(IR)\n\n\n\ntracer-&gt;jaxpr\n\n\n\n\n\njit\n\nTransformation\n\n\n\nhlo\n\nHigh-level\noptimized (HLO)\nprogram\n\n\n\njit-&gt;hlo\n\n\n\n\n\nxla\n\nAccelerated\n Linear Algebra \n(XLA)\n\n\n\nCPU\n\nCPU\n\n\n\nxla-&gt;CPU\n\n\n\n\n\nGPU\n\nGPU\n\n\n\nxla-&gt;GPU\n\n\n\n\n\nTPU\n\nTPU\n\n\n\nxla-&gt;TPU\n\n\n\n\n\ntransform\n\n Transformations \n\n\n\npy\n\nPure Python\nfunctions\n\n\n\npy-&gt;tracer\n\n\n\n\njaxpr-&gt;jit\n\n\n\n\njaxpr-&gt;transform\n\n\n\n\n\n\nhlo-&gt;xla"
  },
  {
    "objectID": "ai/jxai/fl_jax_slides.html#how-does-it-work-1",
    "href": "ai/jxai/fl_jax_slides.html#how-does-it-work-1",
    "title": "A brief intro to",
    "section": "How does it work?",
    "text": "How does it work?\n\n\n\n\n\n\n\n\n\ntracer\n\nTracing\n\n\n\njaxpr\n\nJaxprs\n(JAX expressions)\nintermediate\nrepresentation\n(IR)\n\n\n\ntracer-&gt;jaxpr\n\n\n\n\n\njit\n\n Just-in-time \n(JIT)\ncompilation\n\n\n\nhlo\n\nHigh-level\noptimized (HLO)\nprogram\n\n\n\njit-&gt;hlo\n\n\n\n\n\nxla\n\nAccelerated\n Linear Algebra \n(XLA)\n\n\n\nCPU\n\nCPU\n\n\n\nxla-&gt;CPU\n\n\n\n\n\nGPU\n\nGPU\n\n\n\nxla-&gt;GPU\n\n\n\n\n\nTPU\n\nTPU\n\n\n\nxla-&gt;TPU\n\n\n\n\n\ntransform\n\nVectorization\nParallelization\n   Differentiation  \n\n\n\npy\n\nPure Python\nfunctions\n\n\n\npy-&gt;tracer\n\n\n\n\njaxpr-&gt;jit\n\n\n\n\njaxpr-&gt;transform\n\n\n\n\n\n\nhlo-&gt;xla"
  },
  {
    "objectID": "ai/jxai/fl_jax_slides.html#how-does-it-work-2",
    "href": "ai/jxai/fl_jax_slides.html#how-does-it-work-2",
    "title": "A brief intro to",
    "section": "How does it work?",
    "text": "How does it work?\n\n\n\n\n\n\n\n\n\ntracer\n\nTracing\n\n\n\njaxpr\n\nJaxprs\n(JAX expressions)\nintermediate\nrepresentation\n(IR)\n\n\n\ntracer-&gt;jaxpr\n\n\n\n\n\njit\n\njax.jit\n\n\n\nhlo\n\nHigh-level\noptimized (HLO)\nprogram\n\n\n\njit-&gt;hlo\n\n\n\n\n\nxla\n\nAccelerated\n Linear Algebra \n(XLA)\n\n\n\nCPU\n\nCPU\n\n\n\nxla-&gt;CPU\n\n\n\n\n\nGPU\n\nGPU\n\n\n\nxla-&gt;GPU\n\n\n\n\n\nTPU\n\nTPU\n\n\n\nxla-&gt;TPU\n\n\n\n\n\ntransform\n\njax.vmap\njax.pmap\njax.grad\n\n\n\npy\n\nPure Python\nfunctions\n\n\n\npy-&gt;tracer\n\n\n\n\njaxpr-&gt;jit\n\n\n\n\njaxpr-&gt;transform\n\n\n\n\n\n\nhlo-&gt;xla"
  },
  {
    "objectID": "ai/jxai/fl_jax_slides.html#not-a-deep-learning-library",
    "href": "ai/jxai/fl_jax_slides.html#not-a-deep-learning-library",
    "title": "A brief intro to",
    "section": "Not a deep learning library",
    "text": "Not a deep learning library\n\n\n\n\n\n\n\n\n\njx\n\nJAX\n\n\n\ndl\nDeep learning\n\n\n\njx-&gt;dl\n\n\n\n\n\nop\nOptimizers\n\n\n\njx-&gt;op\n\n\n\n\n\npp\nProbabilistic\nprogramming\n\n\n\njx-&gt;pp\n\n\n\n\n\npm\nProbabilistic\nmodeling\n\n\n\njx-&gt;pm\n\n\n\n\n\nll\nLLMs\n\n\n\nll-&gt;jx\n\n\n\n\n\nso\nSolvers\n\n\n\nso-&gt;jx\n\n\n\n\n\nph\nPhysics\nsimulations\n\n\n\nph-&gt;jx"
  },
  {
    "objectID": "ai/jxai/fl_jax_slides.html#a-python-sublanguage-ideal-for-deep-learning",
    "href": "ai/jxai/fl_jax_slides.html#a-python-sublanguage-ideal-for-deep-learning",
    "title": "A brief intro to",
    "section": "A Python sublanguage ideal for deep learning",
    "text": "A Python sublanguage ideal for deep learning\n\n\n\n\n\n\n\n\n\njx\n\nJAX\n\n\n\ndl\nDeep learning\n\n\n\njx-&gt;dl\n\n\n\n\n\nop\nOptimizers\n\n\n\njx-&gt;op\n\n\n\n\n\npp\nProbabilistic\nprogramming\n\n\n\njx-&gt;pp\n\n\n\n\n\npm\nProbabilistic\nmodeling\n\n\n\njx-&gt;pm\n\n\n\n\n\nll\nLLMs\n\n\n\nll-&gt;jx\n\n\n\n\n\nso\nSolvers\n\n\n\nso-&gt;jx\n\n\n\n\n\nph\nPhysics\nsimulations\n\n\n\nph-&gt;jx"
  },
  {
    "objectID": "ai/jxai/fl_jax_slides.html#deep-learning-libraries",
    "href": "ai/jxai/fl_jax_slides.html#deep-learning-libraries",
    "title": "A brief intro to",
    "section": "Deep learning libraries",
    "text": "Deep learning libraries\n\n\n\n\n\n\n\n\n\njx\n\nJAX\n\n\n\ndl\nDeep learning\n\n\n\njx-&gt;dl\n\n\n\n\n\nop\nOptimizers\n\n\n\njx-&gt;op\n\n\n\n\n\nfl\n\nFlax\n\n\n\ndl-&gt;fl\n\n\n\n\neq\n\nEquinox\n\n\n\ndl-&gt;eq\n\n\n\n\nke\n\nKeras\n\n\n\ndl-&gt;ke\n\n\n\n\noa\n\nOptax\n\n\n\nop-&gt;oa\n\n\n\n\noi\n\nOptimix\n\n\n\nop-&gt;oi"
  },
  {
    "objectID": "ai/jxai/fl_jax_slides.html#this-course",
    "href": "ai/jxai/fl_jax_slides.html#this-course",
    "title": "A brief intro to",
    "section": "This course",
    "text": "This course\n\n\n\n\n\n\n\n\n\njx\n\nJAX\n\n\n\ndl\nDeep learning\n\n\n\njx-&gt;dl\n\n\n\n\n\nop\nOptimizers\n\n\n\njx-&gt;op\n\n\n\n\n\nfl\n\nFlax\n\n\n\ndl-&gt;fl\n\n\n\n\neq\n\nEquinox\n\n\n\ndl-&gt;eq\n\n\n\n\nke\n\nKeras\n\n\n\ndl-&gt;ke\n\n\n\n\noa\n\nOptax\n\n\n\nop-&gt;oa\n\n\n\n\noi\n\nOptimix\n\n\n\nop-&gt;oi"
  },
  {
    "objectID": "ai/jxai/fl_jax_slides.html#data-loaders",
    "href": "ai/jxai/fl_jax_slides.html#data-loaders",
    "title": "A brief intro to",
    "section": "Data loaders",
    "text": "Data loaders\n\n\n\n\n\n\n\n\n\nload\nLoad data\n\n\n\npt\n\ntorchdata\n\n\n\npt-&gt;load\n\n\n\n\n\ntfds\n\ntfds\n\n\n\ntfds-&gt;load\n\n\n\n\n\ndt\n\ndatasets\n\n\n\ndt-&gt;load"
  },
  {
    "objectID": "ai/jxai/fl_jax_slides.html#data-transformations",
    "href": "ai/jxai/fl_jax_slides.html#data-transformations",
    "title": "A brief intro to",
    "section": "Data transformations",
    "text": "Data transformations\n\n\n\n\n\n\n\n\n\nload\nLoad data\n\n\n\nproc\nProcess data\n\n\n\nload-&gt;proc\n\n\n\n\ntv\n\ntorchvision\n\n\n\n\npt\n\ntorchdata\n\n\n\npt-&gt;load\n\n\n\n\n\ntfds\n\ntfds\n\n\n\ntfds-&gt;load\n\n\n\n\n\ndt\n\ndatasets\n\n\n\ndt-&gt;load\n\n\n\n\n\ngr\n\ngrain\n\n\n\n\ngr-&gt;proc\n\n\n\n\n\ntv-&gt;proc"
  },
  {
    "objectID": "ai/jxai/fl_jax_slides.html#core-deep-learning-library",
    "href": "ai/jxai/fl_jax_slides.html#core-deep-learning-library",
    "title": "A brief intro to",
    "section": "Core deep learning library",
    "text": "Core deep learning library\n\n\n\n\n\n\n\n\n\nload\nLoad data\n\n\n\nproc\nProcess data\n\n\n\nload-&gt;proc\n\n\n\n\ntv\n\ntorchvision\n\n\n\n\nnn\nDefine architecture\n\n\n\nproc-&gt;nn\n\n\n\n\npt\n\ntorchdata\n\n\n\npt-&gt;load\n\n\n\n\n\ntfds\n\ntfds\n\n\n\ntfds-&gt;load\n\n\n\n\n\ndt\n\ndatasets\n\n\n\ndt-&gt;load\n\n\n\n\n\ngr\n\ngrain\n\n\n\n\ngr-&gt;proc\n\n\n\n\n\ntv-&gt;proc\n\n\n\n\n\nfl\n\nflax\n\n\n\nfl-&gt;nn"
  },
  {
    "objectID": "ai/jxai/fl_jax_slides.html#optimizer-and-loss-functions",
    "href": "ai/jxai/fl_jax_slides.html#optimizer-and-loss-functions",
    "title": "A brief intro to",
    "section": "Optimizer and loss functions",
    "text": "Optimizer and loss functions\n\n\n\n\n\n\n\n\n\nload\nLoad data\n\n\n\nproc\nProcess data\n\n\n\nload-&gt;proc\n\n\n\n\ntv\n\ntorchvision\n\n\n\n\nnn\nDefine architecture\n\n\n\nproc-&gt;nn\n\n\n\n\nopt\nOptimize\n\n\n\nnn-&gt;opt\n\n\n\n\npt\n\ntorchdata\n\n\n\npt-&gt;load\n\n\n\n\n\ntfds\n\ntfds\n\n\n\ntfds-&gt;load\n\n\n\n\n\ndt\n\ndatasets\n\n\n\ndt-&gt;load\n\n\n\n\n\ngr\n\ngrain\n\n\n\n\ngr-&gt;proc\n\n\n\n\n\ntv-&gt;proc\n\n\n\n\n\nfl\n\nflax\n\n\n\nfl-&gt;nn\n\n\n\n\n\noa\n\noptax\n\n\n\noa-&gt;opt"
  },
  {
    "objectID": "ai/jxai/fl_jax_slides.html#checkpointing",
    "href": "ai/jxai/fl_jax_slides.html#checkpointing",
    "title": "A brief intro to",
    "section": "Checkpointing",
    "text": "Checkpointing\n\n\n\n\n\n\n\n\n\nload\nLoad data\n\n\n\nproc\nProcess data\n\n\n\nload-&gt;proc\n\n\n\n\ntv\n\ntorchvision\n\n\n\n\nnn\nDefine architecture\n\n\n\nproc-&gt;nn\n\n\n\n\nopt\nOptimize\n\n\n\nnn-&gt;opt\n\n\n\n\ncp\nCheckpoint\n\n\n\nopt-&gt;cp\n\n\n\n\npt\n\ntorchdata\n\n\n\npt-&gt;load\n\n\n\n\n\ntfds\n\ntfds\n\n\n\ntfds-&gt;load\n\n\n\n\n\ndt\n\ndatasets\n\n\n\ndt-&gt;load\n\n\n\n\n\ngr\n\ngrain\n\n\n\n\ngr-&gt;proc\n\n\n\n\n\ntv-&gt;proc\n\n\n\n\n\nfl\n\nflax\n\n\n\nfl-&gt;nn\n\n\n\n\n\noa\n\noptax\n\n\n\noa-&gt;opt\n\n\n\n\n\nob\n\norbax\n\n\n\nob-&gt;cp"
  },
  {
    "objectID": "ai/jxai/fl_jax_slides.html#fine-tuning",
    "href": "ai/jxai/fl_jax_slides.html#fine-tuning",
    "title": "A brief intro to",
    "section": "Fine-tuning",
    "text": "Fine-tuning\n\n\n\n\n\n\n\n\n\nload\nLoad data\n\n\n\nproc\nProcess data\n\n\n\nload-&gt;proc\n\n\n\n\ntv\n\ntorchvision\n\n\n\n\nnn\nDefine architecture\n\n\n\nproc-&gt;nn\n\n\n\n\npretr\nPre-trained model\n\n\n\n\nopt\nOptimize\n\n\n\nnn-&gt;opt\n\n\n\n\npretr-&gt;nn\n\n\n\n\ncp\nCheckpoint\n\n\n\nopt-&gt;cp\n\n\n\n\npt\n\ntorchdata\n\n\n\npt-&gt;load\n\n\n\n\n\ntfds\n\ntfds\n\n\n\ntfds-&gt;load\n\n\n\n\n\ndt\n\ndatasets\n\n\n\ndt-&gt;load\n\n\n\n\n\ngr\n\ngrain\n\n\n\n\ngr-&gt;proc\n\n\n\n\n\ntv-&gt;proc\n\n\n\n\n\ntr\n\ntransformers\n\n\n\ntr-&gt;pretr\n\n\n\n\n\nfl\n\nflax\n\n\n\n\nfl-&gt;nn\n\n\n\n\n\noa\n\noptax\n\n\n\noa-&gt;opt\n\n\n\n\n\nob\n\norbax\n\n\n\nob-&gt;cp"
  },
  {
    "objectID": "ai/jxai/fl_jax_slides.html#installing-jax",
    "href": "ai/jxai/fl_jax_slides.html#installing-jax",
    "title": "A brief intro to",
    "section": "Installing JAX",
    "text": "Installing JAX\n\n\n\n\n\n\n\n\n\n\n\n\n\nLinux x86_64\nLinux aarch64\nMac x86_64\nMac aarch64\nWindows x86_64\nWindows WSL2 x86_64\n\n\n\n\nCPU\nyes\nyes\nyes\nyes\nyes\nyes\n\n\nNVIDIA GPU\nyes\nyes\nno\nn/a\nno\nexperimental\n\n\nGoogle TPU\nyes\nn/a\nn/a\nn/a\nn/a\nn/a\n\n\nAMD GPU\nyes\nno\nexperimental\nn/a\nno\nno\n\n\nApple GPU\nn/a\nno\nn/a\nexperimental\nn/a\nn/a\n\n\nIntel GPU\nexperimental\nn/a\nn/a\nn/a\nno\nno\n\n\n\nFrom JAX documentation"
  },
  {
    "objectID": "ai/jxai/fl_jax_slides.html#installing-jax-1",
    "href": "ai/jxai/fl_jax_slides.html#installing-jax-1",
    "title": "A brief intro to",
    "section": "Installing JAX",
    "text": "Installing JAX\nIf you install packages which depend on JAX (e.g. Flax), they will by default install the CPU version of JAX. If you want to run JAX on GPUs, make sure to first install jax[cuda12]\nYou can install the CPU version on your machine to prototype and use a GPU version on the clusters (we have wheels)"
  },
  {
    "objectID": "ai/jxai/fl_jax_slides.html#installing-other-modules",
    "href": "ai/jxai/fl_jax_slides.html#installing-other-modules",
    "title": "A brief intro to",
    "section": "Installing other “modules”",
    "text": "Installing other “modules”\nThe modular approach has the downside that several libraries are required and conflicts between dependencies can be a problem\nThe meta-library jax-ai-stack makes this easier to manage (install jax[cuda12] first for GPU)\nNote that for now TensorFlow and packages which depend on it (e.g. TFDS, grain) are still stuck at Python 3.12, so you can’t use a newer Python version if you want to use some of them\n\nOn your machine (and your machine only), a great tool to manage Python versions and all those packages is uv. (Webinar coming soon). On the clusters, you have to use module to load the Python version you want and pip to install packages"
  },
  {
    "objectID": "ai/jxai/fl_install.html#on-an-alliance-cluster",
    "href": "ai/jxai/fl_install.html#on-an-alliance-cluster",
    "title": "Installing JAX",
    "section": "On an Alliance cluster",
    "text": "On an Alliance cluster\n\nLogging in through SSH\n\nOpen a terminal emulator\nWindows users:  Install the free version of MobaXTerm and launch it.\nmacOS users:   Launch Terminal.\nLinux users:     Open the terminal emulator of your choice.\n\n\nAccess the cluster through secure shell\n\nWindows users\nFollow the first 18% of this demo.\nFor “Remote host”, use the hostname we gave you.\nSelect the box “Specify username” and provide your username.\n\nNote that the password is entered through blind typing, meaning that you will not see anything happening as you type it. This is a Linux feature. While it is a little disturbing at first, do know that it is working. Make sure to type it slowly to avoid typos, then press the “enter” key on your keyboard.\n\n\n\nmacOS and Linux users\nIn the terminal, run:\nssh &lt;username&gt;@&lt;hostname&gt;\n\nReplace the username and hostname by their values. For instance:\nssh user21@somecluster.c3.ca\n\nYou will be asked a question, answer “Yes”.\nWhen prompted, type the password.\n\nNote that the password is entered through blind typing, meaning that you will not see anything happening as you type it. This is a Linux feature. While it is a little disturbing at first, do know that it is working. Make sure to type it slowly to avoid typos, then press the “enter” key on your keyboard.\n\n\n\n\nTroubleshooting\nProblems logging in are almost always due to typos. If you cannot log in, retry slowly, entering your password carefully.\n\n\n\nInstall Flax\n\nWe already created a Python virtual environment and installed Flax to save time. The instructions for today thus differ from what you would normally do, but I include the normal instructions in a separate tab for your future reference.\n\n\nTodayProduction cluster\n\n\nI already created a virtual Python environment under /project and installed Flax in it to save time and space. All you have to do is activate it:\nsource /project/60055/env/bin/activate\n\n\nLook for available Python modules:\nmodule spider python\nLoad the version of your choice:\nmodule load python/3.11.5\nCreate a Python virtual environment:\npython -m venv ~/env\nActivate it:\nsource ~/env/bin/activate\nUpdate pip from wheel:\npython -m pip install --upgrade pip --no-index\n\nWhenever a Python wheel for a package is available on the Alliance clusters, you should use it instead of downloading the package from PyPI. To do this, simply add the --no-index flag to the install command.\nYou can see whether a wheel is available with avail_wheels &lt;package&gt; or look at the list of available wheels.\nAdvantages of wheels:\n\ncompiled for the clusters hardware,\nensures no missing or conflicting dependencies,\nmuch faster installation.\n\n\nInstall libraries from wheel:\npython -m pip install --no-index # install from wheels \\\n    flax                         # NN library \\\n    clu                          # training loop helpers \\\n    optax                        # optimizers & loss functions \\\n    orbax-checkpoint             # checkpointing \\\n    # only install the library you want to use to load datasets\n    datasets torchvision tensorflow-datasets\n\nDon’t forget the --no-index flag here: the wheel will save you from having to deal with the CUDA and CUDNN dependencies, making your life a lot easier.\n\n\nDon’t mindlessly install all the datasets libraries: it is pointless to clutter your virtual environment with things you don’t need and—because they rely on similar dependencies—it can lead to dependency conflicts."
  },
  {
    "objectID": "ai/jxai/fl_data.html",
    "href": "ai/jxai/fl_data.html",
    "title": "Loading data",
    "section": "",
    "text": "In this section, we will download the Food-101 (Bossard, Guillaumin, and Van Gool 2014) dataset that we will later use to train and fine-tune models.",
    "crumbs": [
      "AI",
      "<b><em>Intro DL with JAX</em></b>",
      "Loading data"
    ]
  },
  {
    "objectID": "ai/jxai/fl_data.html#choosing-a-library",
    "href": "ai/jxai/fl_data.html#choosing-a-library",
    "title": "Loading data",
    "section": "Choosing a library",
    "text": "Choosing a library\nData can be downloaded and processed manually, but many datasets are available via Hugging Face datasets, torchvision, and TensorFlow datasets. Remember that JAX does not implement domain-specific utilities and is not a deep learning library. Flax is a deep learning library, but, because there are already so many good options to load and process data, they did not implement a method of their own.\nChoose the library you are the most familiar with, or the one for which you found code somewhere, or the one that seems the easiest to you, or provides the exact functionality that you want for your project.\nThe Food-101 dataset for instance can be accessed with torchvision.datasets.Food101 since it is one of TorchVision datasets or with tfds.image_classification.Food101 since it is also one of TFDS datasets.\nIt is also in the Hugging Face Hub and that’s the method that we will use here.",
    "crumbs": [
      "AI",
      "<b><em>Intro DL with JAX</em></b>",
      "Loading data"
    ]
  },
  {
    "objectID": "ai/jxai/fl_data.html#hugging-face-datasets",
    "href": "ai/jxai/fl_data.html#hugging-face-datasets",
    "title": "Loading data",
    "section": "Hugging Face datasets",
    "text": "Hugging Face datasets\nThe Datasets library from Hugging Face is a lightweight, framework-agnostic, and easy to use API to download datasets from the Hugging Face Hub. It uses Apache Arrow’s efficient caching system, allowing large datasets to be used on machines with small memory (Lhoest et al. 2021).\n\nSearch dataset\nGo to the Hugging Face Hub and search through thousands of open source datasets provided by the community.\n\n\nInspect dataset\nYou can get information on a dataset before downloading it.\nLoad the dataset builder for the dataset you are interested in:\nfrom datasets import load_dataset_builder\nds_builder = load_dataset_builder(\"food101\")\nGet a description of the dataset:\nds_builder.info.description\nGet information on the features:\nds_builder.info.features\n\n\nDownload dataset\nWe will only use the first 3 classes of food (instead of 101) to test our code. To prevent us from all downloading the data (by default in ~/.cache/huggingface), we will use a joint cache directory at /project/60055/data.\nfrom datasets import load_dataset\n\ntrain_size = 3 * 750\nval_size = 3 * 250\n\ntrain_dataset = load_dataset(\"food101\",\n                             split=f\"train[:{train_size}]\",\n                             cache_dir=\"/project/60055/data\")\n\nval_dataset = load_dataset(\"food101\",\n                           split=f\"validation[:{val_size}]\",\n                           cache_dir=\"/project/60055/data\")",
    "crumbs": [
      "AI",
      "<b><em>Intro DL with JAX</em></b>",
      "Loading data"
    ]
  },
  {
    "objectID": "ai/jx/wb_jax_slides.html#what-is-jax",
    "href": "ai/jx/wb_jax_slides.html#what-is-jax",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "What is JAX?",
    "text": "What is JAX?\n\nLibrary for Python developed by Google\n\n\nKey data structure: Array\n\n\nComposition, transformation, and differentiation of numerical programs\n\n\nCompilation for CPUs, GPUs, and TPUs\n\n\nNumPy-like and lower-level APIs\n\n\nRequires strict functional programming"
  },
  {
    "objectID": "ai/jx/wb_jax_slides.html#why-jax",
    "href": "ai/jx/wb_jax_slides.html#why-jax",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "Why JAX?",
    "text": "Why JAX?\n\n\n\n\n\n\n\n\n\n01\n\n\nAutodiff method\n\n\n\n1\nStatic graph\nand XLA\n\n\n\n\n02\n\n\nFramework\n\n\n\n\n2\nDynamic graph\n\n\n\n1-&gt;2\n\n\n\n\n\na\n\nTensorFlow\n\n\n\n\n4\nDynamic graph\nand XLA\n\n\n\n2-&gt;4\n\n\n\n\n\nb\n\nPyTorch\n\n\n\n\n5\nPseudo-dynamic\nand XLA\n\n\n\n4-&gt;5\n\n\n\n\n\nd\n\nTensorFlow2\n\n\n\n\ne\n\nJAX\n\n\n\n\n\n03\n\n\nAdvantage\n\n\n\n\n\n7\nMostly\noptimized AD\n\n\n\n\n\n8\nConvenient\n\n\n\n\n\n9\nConvenient\n\n\n\n\n10\nConvenient and\nmostly optimized AD\n\n\n\n\n\n04\n\n\nDisadvantage\n\n\n\n\n\nA\nManual writing of IR\n\n\n\n\n\nB\nLimited AD optimization\n\n\n\n\n\nD\nDisappointing speed\n\n\n\n\nE\nPure functions\n\n\n\n\n\n\n\n\n\n\n\n\n\n  Summarized from a blog post by Chris Rackauckas"
  },
  {
    "objectID": "ai/jx/wb_jax_slides.html#installation",
    "href": "ai/jx/wb_jax_slides.html#installation",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "Installation",
    "text": "Installation\n Install from pip wheels:\n\nPersonal computer: use wheels installation commands from official site\nAlliance clusters: python -m pip install jax --no-index \n\n\nWindows: GPU support only via WSL"
  },
  {
    "objectID": "ai/jx/wb_jax_slides.html#the-numpy-api",
    "href": "ai/jx/wb_jax_slides.html#the-numpy-api",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "The NumPy API",
    "text": "The NumPy API\n\nNumPyJAX NumPy\n\n\n\nimport numpy as np\n\nprint(np.array([(1, 2, 3), (4, 5, 6)]))\n\n[[1 2 3]\n [4 5 6]]\n\n\n\nprint(np.arange(5))\n\n[0 1 2 3 4]\n\n\n\nprint(np.zeros(2))\n\n[0. 0.]\n\n\n\nprint(np.linspace(0, 2, 9))\n\n[0.   0.25 0.5  0.75 1.   1.25 1.5  1.75 2.  ]\n\n\n\n\nimport jax.numpy as jnp\n\nprint(jnp.array([(1, 2, 3), (4, 5, 6)]))\n[[1 2 3]\n [4 5 6]]\nprint(jnp.arange(5))\n[0 1 2 3 4]\nprint(jnp.zeros(2))\n[0. 0.]\nprint(jnp.linspace(0, 2, 9))\n[0.   0.25 0.5  0.75 1.   1.25 1.5  1.75 2.  ]"
  },
  {
    "objectID": "ai/jx/wb_jax_slides.html#different-types",
    "href": "ai/jx/wb_jax_slides.html#different-types",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "Different types",
    "text": "Different types\n\nNumpyJAX NumPy\n\n\n\ntype(np.zeros((2, 3)))\n\nnumpy.ndarray\n\n\n\n\ntype(jnp.zeros((2, 3)))\njaxlib.xla_extension.ArrayImpl"
  },
  {
    "objectID": "ai/jx/wb_jax_slides.html#different-default-data-types",
    "href": "ai/jx/wb_jax_slides.html#different-default-data-types",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "Different default data types",
    "text": "Different default data types\n\nNumpyJAX NumPy\n\n\n\nnp.zeros((2, 3)).dtype\n\ndtype('float64')\n\n\n\n\njnp.zeros((2, 3)).dtype\ndtype('float32')\n\nStandard for DL and libraries built for accelerators\nFloat64 are very slow on GPUs and not supported on TPUs"
  },
  {
    "objectID": "ai/jx/wb_jax_slides.html#immutable-arrays",
    "href": "ai/jx/wb_jax_slides.html#immutable-arrays",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "Immutable arrays",
    "text": "Immutable arrays\n\nNumpyJAX NumPy\n\n\n\na = np.arange(5)\na[0] = 9\nprint(a)\n\n[9 1 2 3 4]\n\n\n\n\na = jnp.arange(5)\na[0] = 9\nTypeError: '&lt;class 'jaxlib.xla_extension.ArrayImpl'&gt;' object does not support item assignment. JAX arrays are immutable.\nb = a.at[0].set(9)\nprint(b)\n[9 1 2 3 4]"
  },
  {
    "objectID": "ai/jx/wb_jax_slides.html#strict-input-control",
    "href": "ai/jx/wb_jax_slides.html#strict-input-control",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "Strict input control",
    "text": "Strict input control\n\nNumpyJAX NumPy\n\n\nNumPy is easy-going:\n\nnp.sum([1.0, 2.0])  # argument is a list\n\nnp.float64(3.0)\n\n\n\nnp.sum((1.0, 2.0))  # argument is a tuple\n\nnp.float64(3.0)\n\n\n\n\nTo avoid inefficiencies, JAX will only accept arrays:\njnp.sum([1.0, 2.0])\nTypeError: sum requires ndarray or scalar arguments, got &lt;class 'list'&gt;\njnp.sum((1.0, 2.0))\nTypeError: sum requires ndarray or scalar arguments, got &lt;class 'tuple'&gt;"
  },
  {
    "objectID": "ai/jx/wb_jax_slides.html#out-of-bounds-indexing",
    "href": "ai/jx/wb_jax_slides.html#out-of-bounds-indexing",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "Out of bounds indexing",
    "text": "Out of bounds indexing\n\nNumpyJAX NumPy\n\n\nNumPy will error if you index out of bounds:\n\nprint(np.arange(5)[10])\n\n\n---------------------------------------------------------------------------\nIndexError                                Traceback (most recent call last)\nCell In[10], line 1\n----&gt; 1 print(np.arange(5)[10])\n\nIndexError: index 10 is out of bounds for axis 0 with size 5\n\n\n\n\n\nJAX will silently return the closest boundary:\nprint(jnp.arange(5)[10])\n4"
  },
  {
    "objectID": "ai/jx/wb_jax_slides.html#prng-key",
    "href": "ai/jx/wb_jax_slides.html#prng-key",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "PRNG key",
    "text": "PRNG key\nTraditional pseudorandom number generators are based on nondeterministic state of OS\nSlow and problematic for parallel executions\nJAX relies on explicitly-set random state called a key:\nfrom jax import random\n\ninitial_key = random.key(18)\nprint(initial_key)\n[ 0 18]"
  },
  {
    "objectID": "ai/jx/wb_jax_slides.html#prng-key-1",
    "href": "ai/jx/wb_jax_slides.html#prng-key-1",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "PRNG key",
    "text": "PRNG key\nEach key can only be used for one random function, but it can be split into new keys:\nnew_key1, new_key2 = random.split(initial_key)\n\ninitial_key can’t be used anymore now\n\nprint(new_key1)\n[4197003906 1654466292]\nprint(new_key2)\n[1685972163 1654824463]\nWe need to keep one key to split whenever we need and we can use the other one"
  },
  {
    "objectID": "ai/jx/wb_jax_slides.html#prng-key-2",
    "href": "ai/jx/wb_jax_slides.html#prng-key-2",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "PRNG key",
    "text": "PRNG key\nTo make sure we don’t reuse a key by accident, it is best to overwrite the initial key with one of the new ones\nHere are easier names:\nkey = random.key(18)\nkey, subkey = random.split(key)\nWe can now use subkey to generate a random array:\nx = random.normal(subkey, (3, 2))"
  },
  {
    "objectID": "ai/jx/wb_jax_slides.html#benchmarking",
    "href": "ai/jx/wb_jax_slides.html#benchmarking",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "Benchmarking",
    "text": "Benchmarking\nJAX uses asynchronous dispatch\nInstead of waiting for a computation to complete before control returns to Python, the computation is dispatched to an accelerator and a future is created\nTo get proper timings, we need to make sure the future is resolved by using the block_until_ready() method"
  },
  {
    "objectID": "ai/jx/wb_jax_slides.html#jit-syntax",
    "href": "ai/jx/wb_jax_slides.html#jit-syntax",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "JIT syntax",
    "text": "JIT syntax\nfrom jax import jit\n\nkey = random.key(8)\nkey, subkey1, subkey2 = random.split(key, 3)\n\na = random.normal(subkey1, (500, 500))\nb = random.normal(subkey2, (500, 500))\n\ndef sum_squared_error(a, b):\n    return jnp.sum((a-b)**2)\nOur function could simply be used as:\nsse = sum_squared_error(a, b)"
  },
  {
    "objectID": "ai/jx/wb_jax_slides.html#jit-syntax-1",
    "href": "ai/jx/wb_jax_slides.html#jit-syntax-1",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "JIT syntax",
    "text": "JIT syntax\nOur code will run faster if we create a JIT compiled version and use that instead:\nsum_squared_error_jit = jit(sum_squared_error)\n\nsse = sum_squared_error_jit(a, b)\nAlternatively, this can be written as:\nsse = jit(sum_squared_error)(a, b)\nOr with the @jit decorator:\n@jit\ndef sum_squared_error(a, b):\n    return jnp.sum((a - b) ** 2)\n\nsse = sum_squared_error(a, b)"
  },
  {
    "objectID": "ai/jx/wb_jax_slides.html#static-vs-traced-variables",
    "href": "ai/jx/wb_jax_slides.html#static-vs-traced-variables",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "Static vs traced variables",
    "text": "Static vs traced variables\n@jit\ndef cond_func(x):\n    if x &lt; 0.0:\n        return x ** 2.0\n    else:\n        return x ** 3.0\n\nprint(cond_func(1.0))\njax.errors.TracerBoolConversionError: Attempted boolean conversion of traced array with shape bool[]\nJIT compilation uses tracing of the code based on shape and dtype so that the same compiled code can be reused for new values with the same characteristics\nTracer objects are not real values but abstract representation that are more general\nHere, an abstract general value does not work as it wouldn’t know which branch to take"
  },
  {
    "objectID": "ai/jx/wb_jax_slides.html#static-vs-traced-variables-1",
    "href": "ai/jx/wb_jax_slides.html#static-vs-traced-variables-1",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "Static vs traced variables",
    "text": "Static vs traced variables\nOne solution is to tell jit() to exclude the problematic arguments from tracing\nwith arguments positions:\ndef cond_func(x):\n    if x &lt; 0.0:\n        return x ** 2.0\n    else:\n        return x ** 3.0\n\ncond_func_jit = jit(cond_func, static_argnums=(0,))\n\nprint(cond_func_jit(2.0))\nprint(cond_func_jit(-2.0))\n8.0\n4.0"
  },
  {
    "objectID": "ai/jx/wb_jax_slides.html#static-vs-traced-variables-2",
    "href": "ai/jx/wb_jax_slides.html#static-vs-traced-variables-2",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "Static vs traced variables",
    "text": "Static vs traced variables\nOne solution is to tell jit() to exclude the problematic arguments from tracing\nwith arguments names:\ndef cond_func(x):\n    if x &lt; 0.0:\n        return x ** 2.0\n    else:\n        return x ** 3.0\n\ncond_func_jit_alt = jit(cond_func, static_argnames=\"x\")\n\nprint(cond_func_jit_alt(2.0))\nprint(cond_func_jit_alt(-2.0))\n8.0\n4.0"
  },
  {
    "objectID": "ai/jx/wb_jax_slides.html#control-flow-primitives",
    "href": "ai/jx/wb_jax_slides.html#control-flow-primitives",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "Control flow primitives",
    "text": "Control flow primitives\nAnother solution, is to use one of the structured control flow primitives:\nfrom jax import lax\n\nlax.cond(False, lambda x: x ** 2.0, lambda x: x ** 3.0, jnp.array([2.]))\nArray([8.], dtype=float32)\nlax.cond(True, lambda x: x ** 2.0, lambda x: x ** 3.0, jnp.array([-2.]))\nArray([4.], dtype=float32)"
  },
  {
    "objectID": "ai/jx/wb_jax_slides.html#control-flow-primitives-1",
    "href": "ai/jx/wb_jax_slides.html#control-flow-primitives-1",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "Control flow primitives",
    "text": "Control flow primitives\nOther control flow primitives:\n\nlax.while_loop\nlax.fori_loop\nlax.scan\n\nOther pseudo dynamic control flow functions:\n\nlax.select (NumPy API jnp.where and jnp.select)\nlax.switch (NumPy API jnp.piecewise)"
  },
  {
    "objectID": "ai/jx/wb_jax_slides.html#static-vs-traced-operations",
    "href": "ai/jx/wb_jax_slides.html#static-vs-traced-operations",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "Static vs traced operations",
    "text": "Static vs traced operations\nSimilarly, you can mark problematic operations as static so that they don’t get traced during JIT compilation:\n@jit\ndef f(x):\n    return x.reshape(jnp.array(x.shape).prod())\n\nx = jnp.ones((2, 3))\nprint(f(x))\nTypeError: Shapes must be 1D sequences of concrete values of integer type, got [Traced&lt;ShapedArray(int32[])&gt;with&lt;DynamicJaxprTrace(level=1/0)&gt;]"
  },
  {
    "objectID": "ai/jx/wb_jax_slides.html#static-vs-traced-operations-1",
    "href": "ai/jx/wb_jax_slides.html#static-vs-traced-operations-1",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "Static vs traced operations",
    "text": "Static vs traced operations\nThe problem here is that the shape of the argument to prod() depends on the value of x which is unknown at compilation time\nOne solution is to use the NumPy version of prod():\nimport numpy as np\n\n@jit\ndef f(x):\n    return x.reshape((np.prod(x.shape)))\n\nprint(f(x))\n[1. 1. 1. 1. 1. 1.]"
  },
  {
    "objectID": "ai/jx/wb_jax_slides.html#jaxprs",
    "href": "ai/jx/wb_jax_slides.html#jaxprs",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "Jaxprs",
    "text": "Jaxprs\nimport jax\n\nx = jnp.array([1., 4., 3.])\ny = jnp.array([8., 1., 2.])\n\ndef f(x, y):\n    return 2 * x**2 + y\n\njax.make_jaxpr(f)(x, y) \n{ lambda ; a:f32[3] b:f32[3]. let\n    c:f32[3] = integer_pow[y=2] a\n    d:f32[3] = mul 2.0 c\n    e:f32[3] = add d b\n  in (e,) }"
  },
  {
    "objectID": "ai/jx/wb_jax_slides.html#outputs-only-based-on-inputs",
    "href": "ai/jx/wb_jax_slides.html#outputs-only-based-on-inputs",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "Outputs only based on inputs",
    "text": "Outputs only based on inputs\ndef f(x):\n    return a + x\nf uses the variable a from the global environment\nThe output does not solely depend on the inputs: not a pure function"
  },
  {
    "objectID": "ai/jx/wb_jax_slides.html#outputs-only-based-on-inputs-1",
    "href": "ai/jx/wb_jax_slides.html#outputs-only-based-on-inputs-1",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "Outputs only based on inputs",
    "text": "Outputs only based on inputs\na = jnp.ones(3)\nprint(a)\n[1. 1. 1.]\ndef f(x):\n    return a + x\n\nprint(jit(f)(jnp.ones(3)))\n[2. 2. 2.]\n\nThings seem ok here because this is the first run (tracing)"
  },
  {
    "objectID": "ai/jx/wb_jax_slides.html#outputs-only-based-on-inputs-2",
    "href": "ai/jx/wb_jax_slides.html#outputs-only-based-on-inputs-2",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "Outputs only based on inputs",
    "text": "Outputs only based on inputs\nNow, let’s change the value of a to an array of zeros:\na = jnp.zeros(3)\nprint(a)\n[0. 0. 0.]\nAnd rerun the same code:\nprint(jit(f)(jnp.ones(3)))\n[2. 2. 2.]\n\nOur cached compiled program is run and we get a wrong result"
  },
  {
    "objectID": "ai/jx/wb_jax_slides.html#outputs-only-based-on-inputs-3",
    "href": "ai/jx/wb_jax_slides.html#outputs-only-based-on-inputs-3",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "Outputs only based on inputs",
    "text": "Outputs only based on inputs\nThe new value for a will only take effect if we re-trigger tracing by changing the shape and/or dtype of x:\na = jnp.zeros(4)\nprint(a)\n[0. 0. 0. 0.]\nprint(jit(f)(jnp.ones(4)))\n[1. 1. 1. 1.]\nPassing to f() an argument of a different shape forced retracing"
  },
  {
    "objectID": "ai/jx/wb_jax_slides.html#no-side-effects",
    "href": "ai/jx/wb_jax_slides.html#no-side-effects",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "No side effects",
    "text": "No side effects\nSide effects: anything beside returned output\nExamples:\n\nPrinting to standard output\nReading from file/writing to file\nModifying a global variable"
  },
  {
    "objectID": "ai/jx/wb_jax_slides.html#no-side-effects-1",
    "href": "ai/jx/wb_jax_slides.html#no-side-effects-1",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "No side effects",
    "text": "No side effects\nThe side effects will happen during tracing, but not on subsequent runs. You cannot rely on side effects in your code\ndef f(a, b):\n    print(\"Calculating sum\")\n    return a + b\n\nprint(jit(f)(jnp.arange(3), jnp.arange(3)))\nCalculating sum\n[0 2 4]\n\nPrinting happened here because this is the first run"
  },
  {
    "objectID": "ai/jx/wb_jax_slides.html#no-side-effects-2",
    "href": "ai/jx/wb_jax_slides.html#no-side-effects-2",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "No side effects",
    "text": "No side effects\nLet’s rerun the function:\nprint(jit(f)(jnp.arange(3), jnp.arange(3)))\n[0 2 4]\nThis time, no printing"
  },
  {
    "objectID": "ai/jx/wb_jax_slides.html#automatic-differentiation",
    "href": "ai/jx/wb_jax_slides.html#automatic-differentiation",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "Automatic differentiation",
    "text": "Automatic differentiation\nConsidering the function f:\nf = lambda x: x**3 + 2*x**2 - 3*x + 8\nWe can create a new function dfdx that computes the gradient of f w.r.t. x:\nfrom jax import grad\n\ndfdx = grad(f)\ndfdx returns the derivatives\nprint(dfdx(1.))\n4.0"
  },
  {
    "objectID": "ai/jx/wb_jax_slides.html#composing-transformations",
    "href": "ai/jx/wb_jax_slides.html#composing-transformations",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "Composing transformations",
    "text": "Composing transformations\nTransformations can be composed:\nprint(jit(grad(f))(1.))\n4.0\nprint(grad(jit(f))(1.))\n4.0"
  },
  {
    "objectID": "ai/jx/wb_jax_slides.html#forward-and-reverse-modes",
    "href": "ai/jx/wb_jax_slides.html#forward-and-reverse-modes",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "Forward and reverse modes",
    "text": "Forward and reverse modes\nOther autodiff methods:\n\nReverse-mode vector-Jacobian products: jax.vjp\nForward-mode Jacobian-vector products: jax.jvp"
  },
  {
    "objectID": "ai/jx/wb_jax_slides.html#higher-order-differentiation",
    "href": "ai/jx/wb_jax_slides.html#higher-order-differentiation",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "Higher-order differentiation",
    "text": "Higher-order differentiation\n With a single variable, the grad function calls can be nested:\nd2fdx = grad(dfdx)   # function to compute 2nd order derivatives\nd3fdx = grad(d2fdx)  # function to compute 3rd order derivatives\n...\n With several variables:\n\njax.jacfwd for forward-mode\njax.jacrev for reverse-mode"
  },
  {
    "objectID": "ai/jx/wb_jax_slides.html#pytrees",
    "href": "ai/jx/wb_jax_slides.html#pytrees",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "Pytrees",
    "text": "Pytrees\nJAX has a nested container structure: pytree extremely useful for DNN"
  },
  {
    "objectID": "ai/jx/wb_jax_slides.html#vectorization-and-parallelization",
    "href": "ai/jx/wb_jax_slides.html#vectorization-and-parallelization",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "Vectorization and parallelization",
    "text": "Vectorization and parallelization\nOther transformations for parallel run of computations across batches of arrays:\n\nAutomatic vectorization with jax.vmap\nParallelization across devices with jax.pmap"
  },
  {
    "objectID": "ai/jx/wb_jax_slides.html#lax-api",
    "href": "ai/jx/wb_jax_slides.html#lax-api",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "Lax API",
    "text": "Lax API\njax.numpy is a high-level NumPy-like API wrapped around jax.lax\njax.lax is a more efficient lower-level API itself wrapped around XLA"
  },
  {
    "objectID": "ai/jx/wb_jax_slides.html#pallas-extension-to-write-gpu-and-tpu-kernels",
    "href": "ai/jx/wb_jax_slides.html#pallas-extension-to-write-gpu-and-tpu-kernels",
    "title": "Accelerated array computing and flexible differentiation with",
    "section": "Pallas: extension to write GPU and TPU kernels",
    "text": "Pallas: extension to write GPU and TPU kernels\n\n\n\n\n\n\n\n\n\ntracer\n\nTracing\n\n\n\njaxpr\n\nJaxprs\n(JAX expressions)\nintermediate\nrepresentation\n(IR)\n\n\n\ntracer-&gt;jaxpr\n\n\n\n\n\njit\n\n Just-in-time \n(JIT)\ncompilation\n\n\n\nhlo\n\nHigh-level\noptimized (HLO)\nprogram\n\n\n\njit-&gt;hlo\n\n\n\n\n\ntriton\n\nTriton\n\n\n\nGPU\n\nGPU\n\n\n\ntriton-&gt;GPU\n\n\n\n\n\nmosaic\n\nMosaic\n\n\n\nTPU\n\nTPU\n\n\n\nmosaic-&gt;TPU\n\n\n\n\n\ntransform\n\nVectorization\nParallelization\n   Differentiation  \n\n\n\npy\n\nPure Python\nfunctions\n\n\n\npy-&gt;tracer\n\n\n\n\njaxpr-&gt;jit\n\n\n\n\njaxpr-&gt;transform\n\n\n\n\n\n\nhlo-&gt;triton\n\n\n\n\nhlo-&gt;mosaic"
  },
  {
    "objectID": "ai/jx/jx_why.html",
    "href": "ai/jx/jx_why.html",
    "title": "Why JAX?",
    "section": "",
    "text": "There are many excellent and popular deep learning frameworks already (e.g. PyTorch). So why did Google—already behind the successful TensorFlow project—start developing JAX?\nIn this section, we will look at the advantages brought by JAX—namely speed and flexible automatic differentiation.",
    "crumbs": [
      "AI",
      "<b><em>The JAX library</em></b>",
      "Why JAX?"
    ]
  },
  {
    "objectID": "ai/jx/jx_why.html#what-is-jax",
    "href": "ai/jx/jx_why.html#what-is-jax",
    "title": "Why JAX?",
    "section": "What is JAX?",
    "text": "What is JAX?\nJAX is a library for Python developed by Google. Its key data structure is the array. It can perform composition, transformation, and differentiation of numerical programs as well as compilation for CPUs, GPUs, and TPUs.\nIt comes with a NumPy-like API as well as a lower-level API called lax. While the NumPy-like API looks familiar to NumPy users, JAX requires strict functional programming (i.e. functions should only depend on their inputs and should only return outputs).",
    "crumbs": [
      "AI",
      "<b><em>The JAX library</em></b>",
      "Why JAX?"
    ]
  },
  {
    "objectID": "ai/jx/jx_why.html#a-relatively-new-project",
    "href": "ai/jx/jx_why.html#a-relatively-new-project",
    "title": "Why JAX?",
    "section": "A relatively new project",
    "text": "A relatively new project\nIt is clear that JAX is not a widely adopted project yet.\n\nTrends of Google searches\n\n\n\nAs of October 16, 2023.\n\n\n\n\n\nTrends of Stack Overflow tags\n\n\n\nAs of October 16, 2023.\n\n\n\nSo why JAX?",
    "crumbs": [
      "AI",
      "<b><em>The JAX library</em></b>",
      "Why JAX?"
    ]
  },
  {
    "objectID": "ai/jx/jx_why.html#jax-is-fast",
    "href": "ai/jx/jx_why.html#jax-is-fast",
    "title": "Why JAX?",
    "section": "JAX is fast",
    "text": "JAX is fast\nJAX was built with performance in mind. Its speed relies on design decisions at all levels.\n\nDefault data type\nLike PyTorch—a popular deep learning library—JAX uses float32 as its default data type. This level of precision is perfectly suitable for deep learning and increases efficiency (by contrast, NumPy defaults to float64).\nJIT compilation\nJIT compilation combines computations, avoids the allocation of memory to temporary objects, and more generally optimizes code for the XLA.\nAccelerators\nThe same code can run on CPUs or on accelerators (GPUs and TPUs).\nXLA optimization\nXLA (Accelerated Linear Algebra) is a domain-specific compiler for linear algebra that takes JIT-compiled JAX programs and optimizes them for the available hardware (CPUs, GPUs, or TPUs).\nAsynchronous dispatch\nComputations are executed on the accelerators asynchronously.\nVectorization, data parallelism, and sharding\nAll levels of shared and distributed memory parallelism are supported in JAX.",
    "crumbs": [
      "AI",
      "<b><em>The JAX library</em></b>",
      "Why JAX?"
    ]
  },
  {
    "objectID": "ai/jx/jx_why.html#flexible-differentiation",
    "href": "ai/jx/jx_why.html#flexible-differentiation",
    "title": "Why JAX?",
    "section": "Flexible differentiation",
    "text": "Flexible differentiation\nAutomatic differentiation (autodiff or AD) is the evaluation by computer programs of the partial derivatives of functions. It is a key part of deep learning since training a model mostly consists of updating its weights and biases to decrease some loss function and this is done thanks to various gradient-based optimizations.\nSeveral implementations have been developed by different teams over time. This post by Chris Rackauckas summarizes the trade-offs of the various strategies.\nRemoving Julia (which by the way has a lot to offer in the field of AD) and PyTorch’s stale attempt at JIT compilation, Chris Rackauckas’ post can be summarized this way:\n\n\n\n\n\n\n\n\n\n01\n\n\nAutodiff method\n\n\n\n1\nStatic graph\nand XLA\n\n\n\n\n02\n\n\nFramework\n\n\n\n\n2\nDynamic graph\n\n\n\n1-&gt;2\n\n\n\n\n\na\n\nTensorFlow\n\n\n\n\n4\nDynamic graph\nand XLA\n\n\n\n2-&gt;4\n\n\n\n\n\nb\n\nPyTorch\n\n\n\n\n5\nPseudo-dynamic\nand XLA\n\n\n\n4-&gt;5\n\n\n\n\n\nd\n\nTensorFlow2\n\n\n\n\ne\n\nJAX\n\n\n\n\n\n03\n\n\nAdvantage\n\n\n\n\n\n7\nMostly\noptimized AD\n\n\n\n\n\n8\nConvenient\n\n\n\n\n\n9\nConvenient\n\n\n\n\n10\nConvenient and\nmostly optimized AD\n\n\n\n\n\n04\n\n\nDisadvantage\n\n\n\n\n\nA\nManual writing of IR\n\n\n\n\n\nB\nLimited AD optimization\n\n\n\n\n\nD\nDisappointing speed\n\n\n\n\nE\nPure functions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTensorFlow’s initial approach with static computational graphs in a domain-specific language—while efficient thanks to the intermediate representation (IR) and XLA—was inconvenient, limited, and hard to debug. Mostly, users had to write the IR themselves.\nPyTorch came with dynamic graphs—an approach so much more convenient that it marked the beginning of the decline of TensorFlow. The operations are stored during the forward pass which allows for easy automatic differentiation. However this “per value” AD does not allow for a lot of optimizations.\nTensorFlow2 tried to bring dynamic graphs, but it was a poor match for the XLA.\nThis leaves room for new strategies. Julia offers several promising approaches, but implementations are not straightforward and projects are not always mature. It is an exciting avenue for developers, not necessarily an easy one for end users.\nJAX is another attempt at bringing both optimization and flexibility to autodiff. With Google behind it, it is a new but fast growing project.",
    "crumbs": [
      "AI",
      "<b><em>The JAX library</em></b>",
      "Why JAX?"
    ]
  },
  {
    "objectID": "ai/jx/jx_pytree.html",
    "href": "ai/jx/jx_pytree.html",
    "title": "Pytrees",
    "section": "",
    "text": "It is convenient to store data, model parameters, gradients, etc. in container structures such as lists or dicts. JAX has a container-like structure, the pytree that is flexible, can be nested, and is supported by many JAX functions, making for convenient workflows.\nThis section introduces pytrees and their functioning.",
    "crumbs": [
      "AI",
      "<b><em>The JAX library</em></b>",
      "Pytrees"
    ]
  },
  {
    "objectID": "ai/jx/jx_pytree.html#a-tree-like-structure",
    "href": "ai/jx/jx_pytree.html#a-tree-like-structure",
    "title": "Pytrees",
    "section": "A tree-like structure",
    "text": "A tree-like structure\nThe pytree container registry contains, by default, lists, tuples, and dicts. It can be extended to other containers.\nObjects in the pytree container registry are pytrees. Other objects are leaf pytrees (so pytrees are recursive).\nPytrees are great for holding data and parameters, keeping everything organized, even for complex models. The leaves are usually made of arrays. Many JAX functions can be applied to pytrees.\n\nExamples of pytrees:\n\n(1, 2, 3),\n[1, 1., \"string\", True],\njnp.arange(2),\n{'key1': 3.4, 'key2': 6.},\n[3., (1, 2), {'key1': 'val1', 'key2': 'val2', 'key3': 'val3'}],\n(3, 2, (6, 0), 2, ()),\njnp.zeros(3)\n\n\n\n\n\n\nCluster setup\n\n\n\n\n\nLet’s kill our previous interactive job with a GPU:\nexit\nThen start an interactive job with a CPU:\nsalloc --time=2:0:0 --mem-per-cpu=5500M\nLoad the ipython module:\nmodule load ipython-kernel/3.11\nActivate the virtual python environment:\nsource /project/60055/env/bin/activate\nLaunch IPython:\nipython",
    "crumbs": [
      "AI",
      "<b><em>The JAX library</em></b>",
      "Pytrees"
    ]
  },
  {
    "objectID": "ai/jx/jx_pytree.html#extracting-leaves",
    "href": "ai/jx/jx_pytree.html#extracting-leaves",
    "title": "Pytrees",
    "section": "Extracting leaves",
    "text": "Extracting leaves\nTrees can be flattened and their leaves extracted into a list with jax.tree.leaves:\njax.tree.leaves([3., (1, 2), {'key1': 'val1', 'key2': 'val2', 'key3': 'val3'}])\n[3.0, 1, 2, 'val1', 'val2', 'val3']\nLet’s create a list of pytrees and extract their leaves to look at more examples:\nimport jax\nimport jax.numpy as jnp\n\nlist_trees = [\n    (1, 2, 3),\n    [1, 1., \"string\", True],\n    jnp.arange(2),\n    {'key1': 3.4, 'key2': 6.},\n    [3., (1, 2), {'key1': 'val1', 'key2': 'val2', 'key3': 'val3'}],\n    (3, 2, (6, 0, 9), 2, ()),\n    jnp.zeros(3)\n    ]\n\nfor pytree in list_trees:\n  leaves = jax.tree.leaves(pytree)\n  print(f\"{len(leaves)} leaves: {leaves}\")\n3 leaves: [1, 2, 3]\n4 leaves: [1, 1.0, 'string', True]\n1 leaves: [Array([0, 1], dtype=int32)]\n2 leaves: [3.4, 6.0]\n6 leaves: [3.0, 1, 2, 'val1', 'val2', 'val3']\n5 leaves: [3, 2, 6, 0, 9, 2]\n1 leaves: [Array([0., 0., 0.], dtype=float32)]\n\nBe careful that leaves are not the same as container elements:\n\nwhile an array contains many elements, it is a single leaf,\nwhile a nested list or tuple represent a single element of the parent container, all the elements of nested tuples and lists are leaves,\nan empty tuple or list is a pytree without children and is not counted as a leaf.\n\nContrast this with the length (i.e. the number of elements of containers):\nfor pytree in list_trees:\n  print(f\"{len(pytree)} elements\")\n3 elements\n4 elements\n2 elements\n2 elements\n3 elements\n5 elements\n3 elements",
    "crumbs": [
      "AI",
      "<b><em>The JAX library</em></b>",
      "Pytrees"
    ]
  },
  {
    "objectID": "ai/jx/jx_pytree.html#structure-of-pytrees",
    "href": "ai/jx/jx_pytree.html#structure-of-pytrees",
    "title": "Pytrees",
    "section": "Structure of pytrees",
    "text": "Structure of pytrees\nAs we just saw, JAX can extract the leaves of pytrees. This is useful to run functions on them. But JAX also records their structure and is able to recreate them. The structure can be obtained with jax.tree.structure:\njax.tree.structure([3., (1, 2), {'key1': 'val1', 'key2': 'val2', 'key3': 'val3'}])\nPyTreeDef([*, (*, *), {'key1': *, 'key2': *, 'key3': *}])\nSo each pytree can be turned into a tuple of the list of its leaves and its structure and that tuple can be turned back into the pytree.\njax.tree.flatten([3., (1, 2), {'key1': 'val1', 'key2': 'val2', 'key3': 'val3'}])\n([3.0, 1, 2, 'val1', 'val2', 'val3'],\n PyTreeDef([*, (*, *), {'key1': *, 'key2': *, 'key3': *}]))\nvalues, structure = jax.tree.flatten(\n    [3., (1, 2), {'key1': 'val1', 'key2': 'val2', 'key3': 'val3'}]\n)\njax.tree.unflatten(structure, values)\n[3.0, (1, 2), {'key1': 'val1', 'key2': 'val2', 'key3': 'val3'}]\nThe path to each leaf can be obtained with jax.tree_util.tree_flatten_with_path:\njax.tree_util.tree_flatten_with_path(\n    [3., (1, 2), {'key1': 'val1', 'key2': 'val2', 'key3': 'val3'}]\n)\n([((SequenceKey(idx=0),), 3.0),\n  ((SequenceKey(idx=1), SequenceKey(idx=0)), 1),\n  ((SequenceKey(idx=1), SequenceKey(idx=1)), 2),\n  ((SequenceKey(idx=2), DictKey(key='key1')), 'val1'),\n  ((SequenceKey(idx=2), DictKey(key='key2')), 'val2'),\n  ((SequenceKey(idx=2), DictKey(key='key3')), 'val3')],\n PyTreeDef([*, (*, *), {'key1': *, 'key2': *, 'key3': *}]))",
    "crumbs": [
      "AI",
      "<b><em>The JAX library</em></b>",
      "Pytrees"
    ]
  },
  {
    "objectID": "ai/jx/jx_pytree.html#pytree-operations",
    "href": "ai/jx/jx_pytree.html#pytree-operations",
    "title": "Pytrees",
    "section": "Pytree operations",
    "text": "Pytree operations\nJAX can run operations on pytrees. Let’s create a few pytrees to play with:\ntree1 = {'key1': 1., 'key2': 2., 'key3': 3.}\ntree2 = {'key1': 4., 'key2': 5., 'key3': 6.}\ntree3 = {'key1': 7., 'key2': 8., 'key3': 9.}\njax.tree.map allows to apply functions to each leaf of a tree:\njax.tree.map(lambda x: 3 * x, tree1)\n{'key1': 3.0, 'key2': 6.0, 'key3': 9.0}\nAs long as pytrees share the same structure (including the same dicts keys), operations combining multiple pytrees also work:\njax.tree.map(lambda x, y, z: x * y + z, tree1, tree2, tree3)\n{'key1': 11.0, 'key2': 18.0, 'key3': 27.0}\nHere are a few more examples:\ntree4 = [[1, 1, 1], (2, 2, 2, 2), 3]\ntree5 = [[0, 5, 1], (2, 2, 2, 2), 3]\ntree6 = [[0, 5, 1, 2], (2, 2, 2), 3]\njax.tree.map(lambda x, y: x + y, tree4, tree5)\n[[1, 6, 2], (4, 4, 4, 4), 6]\nThis won’t work though as the structures are different:\njax.tree.map(lambda x, y: x + y, tree5, tree6)\nValueError: Tuple arity mismatch: 3 != 4; tuple: (2, 2, 2).",
    "crumbs": [
      "AI",
      "<b><em>The JAX library</em></b>",
      "Pytrees"
    ]
  },
  {
    "objectID": "ai/jx/jx_pytree.html#pytree-transposition",
    "href": "ai/jx/jx_pytree.html#pytree-transposition",
    "title": "Pytrees",
    "section": "Pytree transposition",
    "text": "Pytree transposition\nA list of pytrees can be transposed into a pytree of lists.\nLet’s create a list with a few of our previous pytrees:\ntrees = [tree1, tree2, tree3]\nprint(trees)\n[{'key1': 1.0, 'key2': 2.0, 'key3': 3.0}, {'key1': 1.0, 'key2': 2.0, 'key3': 3.0}, {'key1': 1.0, 'key2': 2.0, 'key3': 3.0}]\nHere is how to transpose this list of pytrees:\njax.tree.map(lambda *x: list(x), *trees)\n{'key1': [1.0, 1.0, 1.0], 'key2': [2.0, 2.0, 2.0], 'key3': [3.0, 3.0, 3.0]}",
    "crumbs": [
      "AI",
      "<b><em>The JAX library</em></b>",
      "Pytrees"
    ]
  },
  {
    "objectID": "ai/jx/jx_pytree.html#pytrees-in-nn",
    "href": "ai/jx/jx_pytree.html#pytrees-in-nn",
    "title": "Pytrees",
    "section": "Pytrees in NN",
    "text": "Pytrees in NN\nPytrees are very useful when using JAX for deep learning. Our course on DL with Flax will show this, but below is a basic example modified from the JAX documentation.\nimport jax\nimport jax.numpy as jnp\nfrom jax import random\nThe parameters of a multi-layer perceptron can be initialized with:\ndef init_params(layer_width):\n  params = []\n  key = random.PRNGKey(11)\n  key, subkey = random.split(key)\n  for n_in, n_out in zip(layer_width[:-1], layer_width[1:]):\n    params.append(\n        dict(weights=random.normal(subkey, (n_in, n_out)) * jnp.sqrt(2/n_in),\n             biases=jnp.ones(n_out)\n            )\n    )\n  return params\n\nparams = init_params([1, 128, 128, 1])\nparams is a pytree:\njax.tree.map(lambda x: x.shape, params)\n[{'biases': (128,), 'weights': (1, 128)},\n {'biases': (128,), 'weights': (128, 128)},\n {'biases': (1,), 'weights': (128, 1)}]\nTo train our MLP, we need to define a function for the forward pass:\n@jax.jit\ndef forward(params, x):\n  *hidden, last = params\n  for layer in hidden:\n    x = jax.nn.relu(x @ layer['weights'] + layer['biases'])\n  return x @ last['weights'] + last['biases']\nAnd a loss function:\n@jax.jit\ndef loss_fn(params, x, y):\n  return jnp.mean((forward(params, x) - y) ** 2)\nThen we choose a learning rate and define a function for the backpropagation:\nlr = 0.0001\n\n@jax.jit\ndef update(params, x, y):\n  grads = jax.grad(loss_fn)(params, x, y)\n  return jax.tree.map(\n      lambda p, g: p - lr * g, params, grads\n  )\n\nBecause jax.grad can accept pytrees, we can create a new pytree grads by passing the params pytree to it.\nThe gradient descent can be applied using both pytrees thanks to jax.tree.map.\n\nThen of course we could train our model:\nkey = random.PRNGKey(3)\nkey, subkey = random.split(key)\n\nx = random.normal(subkey, (128, 1))\ny = x ** 2\n\nfor _ in range(1000):\n  params = update(params, x, y)",
    "crumbs": [
      "AI",
      "<b><em>The JAX library</em></b>",
      "Pytrees"
    ]
  },
  {
    "objectID": "ai/jx/jx_pallas.html",
    "href": "ai/jx/jx_pallas.html",
    "title": "Pallas",
    "section": "",
    "text": "tracer\n\nTracing\n\n\n\njaxpr\n\nJaxprs\n(JAX expressions)\nintermediate\nrepresentation\n(IR)\n\n\n\ntracer-&gt;jaxpr\n\n\n\n\n\njit\n\n Just-in-time \n(JIT)\ncompilation\n\n\n\nhlo\n\nHigh-level\noptimized (HLO)\nprogram\n\n\n\njit-&gt;hlo\n\n\n\n\n\ntriton\n\nTriton\n\n\n\nGPU\n\nGPU\n\n\n\ntriton-&gt;GPU\n\n\n\n\n\nmosaic\n\nMosaic\n\n\n\nTPU\n\nTPU\n\n\n\nmosaic-&gt;TPU\n\n\n\n\n\ntransform\n\nVectorization\nParallelization\n   Differentiation  \n\n\n\npy\n\nPure Python\nfunctions\n\n\n\npy-&gt;tracer\n\n\n\n\njaxpr-&gt;jit\n\n\n\n\njaxpr-&gt;transform\n\n\n\n\n\n\nhlo-&gt;triton\n\n\n\n\nhlo-&gt;mosaic"
  },
  {
    "objectID": "ai/jx/jx_numpy.html",
    "href": "ai/jx/jx_numpy.html",
    "title": "Relation to NumPy",
    "section": "",
    "text": "NumPy is a popular Python scientific API at the core of many libraries. JAX uses a NumPy-inspired API. There are however important differences that we will explore in this section.",
    "crumbs": [
      "AI",
      "<b><em>The JAX library</em></b>",
      "Relation to NumPy"
    ]
  },
  {
    "objectID": "ai/jx/jx_numpy.html#a-numpy-inspired-api",
    "href": "ai/jx/jx_numpy.html#a-numpy-inspired-api",
    "title": "Relation to NumPy",
    "section": "A NumPy-inspired API",
    "text": "A NumPy-inspired API\nNumPy being so popular, JAX comes with a convenient high-level wrapper to NumPy: jax.numpy.\n\nBeing familiar with NumPy is thus an advantage to get started with JAX. The NumPy quickstart is a useful resource.\n\n\nFor a more efficient usage, JAX also comes with a lower-level API: jax.lax.\n\n\nNumPyJAX NumPy\n\n\n\nimport numpy as np\n\n\nprint(np.array([(1, 2, 3), (4, 5, 6)]))\n\n[[1 2 3]\n [4 5 6]]\n\n\n\nprint(np.zeros((2, 3)))\n\n[[0. 0. 0.]\n [0. 0. 0.]]\n\n\n\nprint(np.ones((2, 3, 2)))\n\n[[[1. 1.]\n  [1. 1.]\n  [1. 1.]]\n\n [[1. 1.]\n  [1. 1.]\n  [1. 1.]]]\n\n\n\nprint(np.arange(24).reshape(2, 3, 4))\n\n[[[ 0  1  2  3]\n  [ 4  5  6  7]\n  [ 8  9 10 11]]\n\n [[12 13 14 15]\n  [16 17 18 19]\n  [20 21 22 23]]]\n\n\n\nprint(np.linspace(0, 2, 9))\n\n[0.   0.25 0.5  0.75 1.   1.25 1.5  1.75 2.  ]\n\n\n\nprint(np.linspace(0, 2, 9)[::-1])\n\n[2.   1.75 1.5  1.25 1.   0.75 0.5  0.25 0.  ]\n\n\n\n\nimport jax.numpy as jnp\nprint(jnp.array([(1, 2, 3), (4, 5, 6)]))\n[[1 2 3]\n [4 5 6]]\nprint(jnp.zeros((2, 3)))\n[[0. 0. 0.]\n [0. 0. 0.]]\nprint(jnp.ones((2, 3, 2)))\n[[[1. 1.]\n  [1. 1.]\n  [1. 1.]]\n\n [[1. 1.]\n  [1. 1.]\n  [1. 1.]]]\nprint(jnp.arange(24).reshape(2, 3, 4))\n[[[ 0  1  2  3]\n  [ 4  5  6  7]\n  [ 8  9 10 11]]\n\n [[12 13 14 15]\n  [16 17 18 19]\n  [20 21 22 23]]]\nprint(jnp.linspace(0, 2, 9))\n[0.   0.25 0.5  0.75 1.   1.25 1.5  1.75 2.  ]\nprint(jnp.linspace(0, 2, 9)[::-1])\n[2.   1.75 1.5  1.25 1.   0.75 0.5  0.25 0.  ]\n\n\n\nDespite the similarities, there are important differences between JAX and NumPy.",
    "crumbs": [
      "AI",
      "<b><em>The JAX library</em></b>",
      "Relation to NumPy"
    ]
  },
  {
    "objectID": "ai/jx/jx_numpy.html#differences-with-numpy",
    "href": "ai/jx/jx_numpy.html#differences-with-numpy",
    "title": "Relation to NumPy",
    "section": "Differences with NumPy",
    "text": "Differences with NumPy\n\nDifferent types\ntype(np.zeros((2, 3))) == type(jnp.zeros((2, 3)))\nFalse\n\nNumpyJAX NumPy\n\n\n\ntype(np.zeros((2, 3)))\n\nnumpy.ndarray\n\n\n\n\ntype(jnp.zeros((2, 3)))\njaxlib.xla_extension.ArrayImpl\n\n\n\n\n\nDifferent default data types\n\nNumpyJAX NumPy\n\n\n\nnp.zeros((2, 3)).dtype\n\ndtype('float64')\n\n\n\n\njnp.zeros((2, 3)).dtype\ndtype('float32')\n\n\n\n\nLower numerical precision improves speed and reduces memory usage at no cost while training neural networks and is thus a net benefit. Having been built with deep learning in mind, JAX defaults align with that of other DL libraries (e.g. PyTorch, TensorFlow).\n\n\n\nImmutable arrays\n\nNumpyJAX NumPy\n\n\nIn NumPy, you can modify ndarrays:\n\na = np.arange(5)\na[0] = 9\nprint(a)\n\n[9 1 2 3 4]\n\n\n\n\nJAX arrays are immutable:\na = jnp.arange(5)\na[0] = 9\nTypeError: '&lt;class 'jaxlib.xla_extension.ArrayImpl'&gt;' object does not support item assignment. JAX arrays are immutable. Instead of ``x[idx] = y``, use ``x = x.at[idx].set(y)`` or another .at[] method: https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.ndarray.at.html\nInstead, you need to create a copy of the array with the mutation. This is done with:\nb = a.at[0].set(9)\nprint(b)\n[9 1 2 3 4]\nOf course, you can overwrite a:\na = a.at[0].set(9)\n\n\n\n\n\nPseudorandom number generation\nProgramming languages usually come with automated pseudorandom number generator (PRNG) based on nondeterministic data from the operating system. They are extremely convenient, but slow, based on repeats, and problematic in parallel executions.\nJAX relies on an explicitly set random state called a key.\nfrom jax import random\n\nkey = random.key(18)\nprint(key)\n[ 0 18]\nEach time you call a random function, you need a subkey split from your key. Keys should only ever be used once in your code. The key is what makes your code reproducible, but you don’t want to reuse it within your code as it would create spurious correlations.\nHere is the workflow:\n\nyou split your key into a new key and one or multiple subkeys,\nyou discard the old key (because it was used to do the split—so its entropy budget, so to speak, has been used),\nyou use the subkey(s) to run your random function(s) and keep the new key for a future split.\n\n\nSubkeys are of the same nature as keys. This is just a terminology.\n\nTo make sure not to reuse the old key, you can overwrite it by the new one:\nkey, subkey = random.split(key)\nprint(key)\n[4197003906 1654466292]\n\nThat’s the value of our new key for future splits.\n\nprint(subkey)\n[1685972163 1654824463]\n\nThis is the value of the subkey that we can use to call a random function.\n\nLet’s use that subkey now:\nprint(random.normal(subkey))\n1.1437175\n\nTo split your key into more subkeys, pass an argument to random.split:\nkey, subkey1, subkey2, subkey3 = random.split(key, 4)\n\n\n\nStrict input control\n\nNumpyJAX NumPy\n\n\nNumPy’s fundamental object is the ndarray, but NumPy is very tolerant as to the type of input.\n\nnp.sum([1.0, 2.0])  # here we are using a list\n\nnp.float64(3.0)\n\n\n\nnp.sum((1.0, 2.0))  # here is a tuple\n\nnp.float64(3.0)\n\n\n\n\nTo avoid inefficiencies, JAX will only accept arrays.\njnp.sum([1.0, 2.0])\nTypeError: sum requires ndarray or scalar arguments, got &lt;class 'list'&gt; at position 0.\njnp.sum((1.0, 2.0))\nTypeError: sum requires ndarray or scalar arguments, got &lt;class 'tuple'&gt; at position 0.\n\n\n\n\n\nOut of bounds indexing\n\nNumpyJAX NumPy\n\n\nNumPy will warn you with an error message if you try to index out of bounds:\n\nprint(np.arange(5)[10])\n\n\n---------------------------------------------------------------------------\nIndexError                                Traceback (most recent call last)\nCell In[13], line 1\n----&gt; 1 print(np.arange(5)[10])\n\nIndexError: index 10 is out of bounds for axis 0 with size 5\n\n\n\n\n\nBe aware that JAX will not raise an error. Instead, it will silently return the closest boundary:\nprint(jnp.arange(5)[10])\n4\n\n\n\n\n\nFunctionally pure functions\nMore importantly, only functionally pure functions—that is, functions for which the outputs are only based on the inputs and which have no side effects—can be used with JAX.\n\nOutputs only based on inputs\nConsider the function:\ndef f(x):\n    return a + x\nwhich uses the variable a from the global environment.\nThis function is not functionally pure because the outputs (the results of the function) do not solely depend on the arguments (the values given to x) passed to it. They also depend on the value of a.\nRemember how tracing works: new inputs with the same shape and dtype use the cached compiled program directly. If the value of a changes in the global environment, a new tracing is not triggered and the cached compiled program uses the old value of a (the one that was used during tracing).\nIt is only if the code is run on an input x with a different shape and/or dtype that tracing happens again and that the new value for a takes effect.\n\nTo demo this, we need to use JIT compilation that we will explain in a later section.\n\nfrom jax import jit\n\na = jnp.ones(3)\nprint(a)\n[1. 1. 1.]\ndef f(x):\n    return a + x\n\nprint(jit(f)(jnp.ones(3)))\n[2. 2. 2.]\n\nAll good here because this is the first run (tracing).\n\nNow, let’s change the value of a to an array of zeros:\na = jnp.zeros(3)\nprint(a)\n[0. 0. 0.]\nAnd rerun the same code:\nprint(jit(f)(jnp.ones(3)))\n[2. 2. 2.]\nWe should have an array of ones, but we get the same result we got earlier. Why? because we are running a cached program with the value that a had during tracing.\nThe new value for a will only take effect if we re-trigger tracing by changing the shape and/or dtype of x:\na = jnp.zeros(4)\nprint(a)\n[0. 0. 0. 0.]\nprint(jit(f)(jnp.ones(4)))\n[1. 1. 1. 1.]\nPassing an argument of a different shape to f forced recompilation. Using a different data type (e.g. with jnp.arange(3)) would have done the same.\n\n\nNo side effects\nA function is said to have a side effect if it changes something outside of its local environment (if it does anything beside returning an output).\nExamples of side effects include:\n\nprinting to standard output/shell,\nreading from file/writing to file,\nmodifying a global variable.\n\nIn JAX, the side effects will happen during the first run (tracing), but will not happen on subsequent runs. You thus cannot rely on side effects in your code.\ndef f(a, b):\n    print(\"Calculating sum\")\n    return a + b\n\nprint(jit(f)(jnp.arange(3), jnp.arange(3)))\nCalculating sum\n[0 2 4]\n\nPrinting (the side effect) happened here because this is the first run.\n\nLet’s rerun the function:\nprint(jit(f)(jnp.arange(3), jnp.arange(3)))\n[0 2 4]\nThis time, no printing.\n\nUnderstanding jaxprs\nJaxprs are created by tracers wrapping the Python code during compilation (the first run). They contain information on the shape and data type of arrays as well as the operations performed on these arrays. Jaxprs do not however contain information on values: this allows the compiled program to be general enough to be rerun with any new arrays of the same shape and data type without having to rerun the slow Python code and recompile.\nJaxprs also do not contain any information on elements that are not part of the inputs such as external variables, nor do they contain information on side effects.\nJaxprs can be visualized with the jax.make_jaxpr function:\nimport jax\n\nx = jnp.array([1., 4., 3.])\ny = jnp.array([8., 1., 2.])\n\ndef f(x, y):\n    return 2 * x**2 + y\n\njax.make_jaxpr(f)(x, y) \n{ lambda ; a:f32[3] b:f32[3]. let\n    c:f32[3] = integer_pow[y=2] a\n    d:f32[3] = mul 2.0 c\n    e:f32[3] = add d b\n  in (e,) }\nLet’s add a print function to f:\ndef f(x, y):\n    print(\"This is a function with side-effect\")\n    return 2 * x**2 + y\n\njax.make_jaxpr(f)(x, y)\n{ lambda ; a:f32[3] b:f32[3]. let\n    c:f32[3] = integer_pow[y=2] a\n    d:f32[3] = mul 2.0 c\n    e:f32[3] = add d b\n  in (e,) }\nThe jaxpr is exactly the same. This is why printing will happen during tracing (when the Python code is run), but not afterwards (when the compiled code using the jaxpr is run).",
    "crumbs": [
      "AI",
      "<b><em>The JAX library</em></b>",
      "Relation to NumPy"
    ]
  },
  {
    "objectID": "ai/jx/jx_numpy.html#why-the-constraints",
    "href": "ai/jx/jx_numpy.html#why-the-constraints",
    "title": "Relation to NumPy",
    "section": "Why the constraints?",
    "text": "Why the constraints?\nThe more constraints you add to a programming language, the more optimization you can get from the compiler. Speed comes at the cost of convenience.\nFor instance, consider a Python list. It is an extremely convenient and flexible object: heterogeneous, mutable… You can do anything with it. But computations on lists are extremely slow.\nNumPy’s ndarrays are more constrained (homogeneous), but the type constraint permits the creation of a much faster language (NumPy is written in C and Fortran as well as Python) with vectorization, optimizations, and a greatly improved performance.\nJAX takes it further: by using an intermediate representation and very strict constraints on type, pure functional programming, etc., yet more optimizations can be achieved and you can optimize your own functions with JIT compilation and the XLA. Ultimately, this is what makes JAX so fast.",
    "crumbs": [
      "AI",
      "<b><em>The JAX library</em></b>",
      "Relation to NumPy"
    ]
  },
  {
    "objectID": "ai/jx/jx_libraries.html",
    "href": "ai/jx/jx_libraries.html",
    "title": "Libraries built on JAX",
    "section": "",
    "text": "JAX is an efficient and flexible framework for array operations and program transformations (including automatic differentiation) built to run on accelerators. Its goal is not to develop specialized applications, but to focus on these chore tasks.\nWhile it is possible to use JAX directly in applications (e.g. to build a NN from scratch), it makes sense to use specialized libraries that are built on top of JAX, make use of its characteristics, and provide convenience functions for specialized tasks.\nThe list of libraries built on JAX keeps growing, but here are a few of the currently important ones.",
    "crumbs": [
      "AI",
      "<b><em>The JAX library</em></b>",
      "Libraries built on JAX"
    ]
  },
  {
    "objectID": "ai/jx/jx_libraries.html#neural-networks",
    "href": "ai/jx/jx_libraries.html#neural-networks",
    "title": "Libraries built on JAX",
    "section": "Neural networks",
    "text": "Neural networks\nFlax is an NN library initially developed by Google Brain and now by Google DeepMind. It is the deep learning library officially recommended by the JAX developers. This is the library that we will use in this course.\nEquinox is another DL library, relying on models as pytrees. While its syntax is a lot more user-friendly and familiar to PyTorch users, it has limitations.\nKeras can now use JAX as a backend.\n\nIt is worth noting that PyTorch is attempting to incorporate JAX’s ideas with a new library under development, functorch.\nHaiku was the initial library developed by Google DeepMind. While it is still maintained, development has been stopped in favour of Flax and it is thus not advisable to get started with it unless you are already using it.\n\nOptax is a gradient manipulation and optimization library developed by Google DeepMind.",
    "crumbs": [
      "AI",
      "<b><em>The JAX library</em></b>",
      "Libraries built on JAX"
    ]
  },
  {
    "objectID": "ai/jx/jx_libraries.html#bayesian-statistics",
    "href": "ai/jx/jx_libraries.html#bayesian-statistics",
    "title": "Libraries built on JAX",
    "section": "Bayesian statistics",
    "text": "Bayesian statistics\nNumPyro and PyMC are probabilistic programming languages.\nBlackJAX is a library of samples.\nFor a basic and high-level introduction, you can have a look at our webinar on Bayesian inference in JAX.",
    "crumbs": [
      "AI",
      "<b><em>The JAX library</em></b>",
      "Libraries built on JAX"
    ]
  },
  {
    "objectID": "ai/jx/jx_libraries.html#probabilistic-state-space-models",
    "href": "ai/jx/jx_libraries.html#probabilistic-state-space-models",
    "title": "Libraries built on JAX",
    "section": "Probabilistic state space models",
    "text": "Probabilistic state space models\nDynamax provides state and parameter estimation for, among others:\n\nhidden markov models,\nlinear gaussian state space models,\nnonlinear gaussian state space models,\ngeneralized gaussian state space models.",
    "crumbs": [
      "AI",
      "<b><em>The JAX library</em></b>",
      "Libraries built on JAX"
    ]
  },
  {
    "objectID": "ai/jx/jx_jit.html",
    "href": "ai/jx/jx_jit.html",
    "title": "JIT compilation",
    "section": "",
    "text": "JIT compilation is a key component to JAX efficiency. For the most part, it is very easy to use, but there are subtleties to be aware of.",
    "crumbs": [
      "AI",
      "<b><em>The JAX library</em></b>",
      "JIT compilation"
    ]
  },
  {
    "objectID": "ai/jx/jx_jit.html#jit",
    "href": "ai/jx/jx_jit.html#jit",
    "title": "JIT compilation",
    "section": "JIT",
    "text": "JIT\nJAX functions are already compiled and optimized, but user functions can also be optimized for the XLA by JIT compilation which will combine computations.\nRemember the map of JAX functioning:\n\n\n\n\n\n\n\n\n\ntracer\n\nTracing\n\n\n\njaxpr\n\nJaxpr\n(JAX expression)\nintermediate\nrepresentation\n(IR)\n\n\n\ntracer-&gt;jaxpr\n\n\n\n\n\njit\n\nJIT\n compilation \n\n\n\nhlo\n\nHigh-level\noptimized (HLO)\nprogram\n\n\n\njit-&gt;hlo\n\n\n\n\n\nxla\n\nAccelerated\n Linear Algebra \n(XLA)\n\n\n\nCPU\n\nCPU\n\n\n\nxla-&gt;CPU\n\n\n\n\n\nGPU\n\nGPU\n\n\n\nxla-&gt;GPU\n\n\n\n\n\nTPU\n\nTPU\n\n\n\nxla-&gt;TPU\n\n\n\n\n\ntransform\n\n Transformations \n\n\n\npy\n\nPure Python\nfunctions\n\n\n\npy-&gt;tracer\n\n\n\n\njaxpr-&gt;jit\n\n\n\n\njaxpr-&gt;transform\n\n\n\n\n\n\nhlo-&gt;xla\n\n\n\n\n\n\n\n\n\nThis is done by the jax.jit() function or the equivalent decorator @jit.\nLet’s consider this code:\nimport jax.numpy as jnp\nfrom jax import jit\nfrom jax import random\n\nkey = random.PRNGKey(8)\nkey, subkey1, subkey2 = random.split(key, 3)\n\na = random.normal(subkey1, (500, 500))\nb = random.normal(subkey2, (500, 500))\n\ndef sum_squared_error(a, b):\n    return jnp.sum((a-b)**2)\nOur function can simply be used as:\nprint(sum_squared_error(a, b))\nOur code will run faster if we create a JIT compiled version and use that instead (we will see how to benchmark JAX code later in the course. There are some subtleties for that too, so for now, just believe that it is faster. You will be able to test it later):\nsum_squared_error_jit = jit(sum_squared_error)\nprint(sum_squared_error_jit(a, b))\n502084.75\nAlternatively, this can be written as:\nprint(jit(sum_squared_error)(a, b))\n502084.75\nOr as:\n@jit\ndef sum_squared_error(a, b):\n    return jnp.sum((a - b) ** 2)\n\nprint(sum_squared_error(a, b))\n502084.75\n\nUnderstanding jaxprs\nLet’s have a look at the jaxpr of a jit-compiled function.\nThis is what the jaxpr of the non-jit-compiled function looks like:\nimport jax\n\ndef sum_squared_error(a, b):\n    return jnp.sum((a - b) ** 2)\n\njax.make_jaxpr(sum_squared_error)(x, y)\n{ lambda ; a:f32[3] b:f32[3]. let\n    c:f32[3] = sub a b\n    d:f32[3] = integer_pow[y=2] c\n    e:f32[] = reduce_sum[axes=(0,)] d\n  in (e,) }\nThe jaxpr of the jit-compiled function looks like this:\n@jit\ndef sum_squared_error(a, b):\n    return jnp.sum((a - b) ** 2)\n\njax.make_jaxpr(sum_squared_error)(x, y)\n{ lambda ; a:f32[3] b:f32[3]. let\n    c:f32[] = pjit[\n      name=sum_squared_error\n      jaxpr={ lambda ; d:f32[3] e:f32[3]. let\n          f:f32[3] = sub d e\n          g:f32[3] = integer_pow[y=2] f\n          h:f32[] = reduce_sum[axes=(0,)] g\n        in (h,) }\n    ] a b\n  in (c,) }",
    "crumbs": [
      "AI",
      "<b><em>The JAX library</em></b>",
      "JIT compilation"
    ]
  },
  {
    "objectID": "ai/jx/jx_jit.html#jit-constraints",
    "href": "ai/jx/jx_jit.html#jit-constraints",
    "title": "JIT compilation",
    "section": "JIT constraints",
    "text": "JIT constraints\nUsing jit in the example above was very easy. There are situations however in which tracing will fail.\n\nControl flow\nOne example can arise with control flow:\n@jit\ndef cond_func(x):\n    if x &lt; 0.0:\n        return x ** 2.0\n    else:\n        return x ** 3.0\n\nprint(cond_func(1.0))\njax.errors.TracerBoolConversionError: Attempted boolean conversion of traced array with shape bool[]..\nThe error occurred while tracing the function cond_func at jx_jit.qmd:85 for jit. This concrete value was not available in Python because it depends on the value of the argument x.\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.TracerBoolConversionError\nJIT compilation uses tracing of the code based on shape and dtype so that the same compiled code can be reused for new values with the same characteristics. The tracer objects are not real values but abstract representation that are more general. In control flow situations such as the one we have here, an abstract general value does not work as it wouldn’t know which branch to take.\n\nStatic variables\nOne solution is to tell jit() to exclude the problematic arguments (in our example the argument: x) from tracing (i.e. to consider them as static). Of course, those elements will not be optimized, but the rest of the code will, so it is a lot better than not JIT compiling the function at all.\nYou can either use the static_argnums parameter which takes an integer or a collection of integers to specify the position of the arguments to treat as static:\ndef cond_func(x):\n    if x &lt; 0.0:\n        return x ** 2.0\n    else:\n        return x ** 3.0\n\ncond_func_jit = jit(cond_func, static_argnums=(0,))\n\nprint(cond_func_jit(2.0))\nprint(cond_func_jit(-2.0))\n8.0\n4.0\nOr you can use static_argnames which accepts argument names:\ndef cond_func(x):\n    if x &lt; 0.0:\n        return x ** 2.0\n    else:\n        return x ** 3.0\n\ncond_func_jit_alt = jit(cond_func, static_argnames='x')\n\nprint(cond_func_jit_alt(2.0))\nprint(cond_func_jit_alt(-2.0))\n8.0\n4.0\nYou cannot use the @jit decorator when you need to pass arguments to the jit function, but you can still use a decorator:\nfrom functools import partial\n\n@partial(jit, static_argnums=(0,))\ndef cond_func(x):\n    if x &lt; 0.0:\n        return x ** 2.0\n    else:\n        return x ** 3.0\nor:\n@partial(jit, static_argnames=['x'])\ndef cond_func(x):\n    if x &lt; 0.0:\n        return x ** 2.0\n    else:\n        return x ** 3.0\n\n\nControl flow primitives\nIf you don’t want the code to recompile for each new value, another solution, is to use one of the structured control flow primitives:\nfrom jax import lax\n\nlax.cond(False, lambda x: x ** 2.0, lambda x: x ** 3.0, jnp.array([2.]))\nArray([8.], dtype=float32)\nlax.cond(True, lambda x: x ** 2.0, lambda x: x ** 3.0, jnp.array([-2.]))\nArray([4.], dtype=float32)\nThere are other control flow primitives:\n\nlax.while_loop\nlax.fori_loop\nlax.scan\n\nand other pseudo dynamic control flow functions:\n\nlax.select (NumPy API jnp.where and jnp.select)\nlax.switch (NumPy API jnp.piecewise)\n\n\n\n\nStatic operations\nSimilarly, you will need to mark problematic operations as static so that they don’t get traced during JIT compilation:\n@jit\ndef f(x):\n    return x.reshape(jnp.array(x.shape).prod())\n\nx = jnp.ones((2, 3))\nprint(f(x))\nTypeError: Shapes must be 1D sequences of concrete values of integer type, got [Traced&lt;ShapedArray(int32[])&gt;with&lt;DynamicJaxprTrace(level=1/0)&gt;].\nIf using `jit`, try using `static_argnums` or applying `jit` to smaller subfunctions.\nThe problem here is that the shape of the argument to jnp.reshape is traced while it needs to be static.\nOne solution is to use the NumPy version of prod which will not create a traced result:\nimport numpy as np\n\n@jit\ndef f(x):\n    return x.reshape(np.prod(x.shape))\n\nprint(f(x))\n[1. 1. 1. 1. 1. 1.]",
    "crumbs": [
      "AI",
      "<b><em>The JAX library</em></b>",
      "JIT compilation"
    ]
  },
  {
    "objectID": "ai/jx/jx_benchmark.html",
    "href": "ai/jx/jx_benchmark.html",
    "title": "Benchmarking JAX code",
    "section": "",
    "text": "You have to be careful when benchmarking JAX code to actually measure the computation time and not the dispatch time.",
    "crumbs": [
      "AI",
      "<b><em>The JAX library</em></b>",
      "Benchmarking JAX code"
    ]
  },
  {
    "objectID": "ai/jx/jx_benchmark.html#asynchronous-dispatch",
    "href": "ai/jx/jx_benchmark.html#asynchronous-dispatch",
    "title": "Benchmarking JAX code",
    "section": "Asynchronous dispatch",
    "text": "Asynchronous dispatch\nOne of the efficiencies of JAX is its use of asynchronous execution.\nLet’s consider the code:\nimport jax.numpy as jnp\nfrom jax import random\n\nkey = random.PRNGKey(11)\nkey, subkey1, subkey2 = random.split(key, 3)\n\nx = random.normal(subkey1, (1000, 1000))\ny = random.normal(subkey2, (1000, 1000))\n\nz = jnp.dot(x, y)\nInstead of having to wait for the computation to complete before control returns to Python, this computation is dispatched to an accelerator and a future is created. This future is a jax.Array and can be passed to further computations immediately.\nOf course, if you print the result or convert it to a NumPy ndarray, JAX forces Python to wait for the result of the computation.",
    "crumbs": [
      "AI",
      "<b><em>The JAX library</em></b>",
      "Benchmarking JAX code"
    ]
  },
  {
    "objectID": "ai/jx/jx_benchmark.html#consequence-for-benchmarking",
    "href": "ai/jx/jx_benchmark.html#consequence-for-benchmarking",
    "title": "Benchmarking JAX code",
    "section": "Consequence for benchmarking",
    "text": "Consequence for benchmarking\nTiming jnp.dot(x, y) would not give us the time it takes for the computation to take place, but the time it takes to dispatch the computation.\nOn my laptop which has one dedicated GPU I get:\n%timeit jnp.dot(x, y)\n496 µs ± 948 ns per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n\n%timeit is an IPython built-in magic command. In Python, you would have to use the timeit module.\n\nTo get a proper timing, we need to make sure that the future is resolved using the block_until_ready method:\nOn the same machine:\n%timeit jnp.dot(x, y).block_until_ready()\n598 µs ± 10.4 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\nThe difference here is not huge because the GPU executes the matrix multiplication rapidly. Nevertheless, this is the true timing. If you benchmark your JAX code, make sure to do it this way.\n\nIf you are running small computations such as this one without accelerator, the computation will be dispatched on the thread running the Python process because the overhead of the asynchronous execution would be larger than the speedup you would gain from it. This means that, if you are running the above code on CPUs, you should get the same time with and without block_until_ready.\nNevertheless, because it is difficult to predict when the dispatch will be asynchronous, you should always use block_until_ready in your benchmarks.",
    "crumbs": [
      "AI",
      "<b><em>The JAX library</em></b>",
      "Benchmarking JAX code"
    ]
  },
  {
    "objectID": "ai/jx/jx_accelerator.html",
    "href": "ai/jx/jx_accelerator.html",
    "title": "Accelerators",
    "section": "",
    "text": "One of the efficiencies of JAX is its use of accelerators. In this section, we can see how easily this is done.",
    "crumbs": [
      "AI",
      "<b><em>The JAX library</em></b>",
      "Accelerators"
    ]
  },
  {
    "objectID": "ai/jx/jx_accelerator.html#auto-detection",
    "href": "ai/jx/jx_accelerator.html#auto-detection",
    "title": "Accelerators",
    "section": "Auto-detection",
    "text": "Auto-detection\nOne of the convenience of the XLA used by JAX (and TensorFlow) is that the same code runs on any device without modification.\n\nThis is in contrast with PyTorch where tensors are created on the CPU by default and can be moved to the GPU using the .to method.\nOr tensors need to be created explicitly on a device (e.g. for GPU, x = torch.ones(2, 4, device='cuda')).\nAlternatively, the code can be made more robust with the creation of a device handle which will allow it to run without modification on CPU or GPU:\nimport torch\n\ndevice = (\n    \"cuda\"\n    if torch.cuda.is_available()\n    else \"mps\"\n    if torch.backends.mps.is_available()\n    else \"cpu\"\n)\n\nx = torch.ones(2, 4, device=device)\n\nMPS = Apple Metal Performance Shaders (GPU on macOS).\n\nAnd there are methods to run PyTorch on TPU with the torch_xla package, with tricks of scalability.",
    "crumbs": [
      "AI",
      "<b><em>The JAX library</em></b>",
      "Accelerators"
    ]
  },
  {
    "objectID": "ai/jx/jx_accelerator.html#interactive-job-with-a-gpu",
    "href": "ai/jx/jx_accelerator.html#interactive-job-with-a-gpu",
    "title": "Accelerators",
    "section": "Interactive job with a GPU",
    "text": "Interactive job with a GPU\nThe Alliance wiki documents how to use GPUs on Alliance clusters.\nFor now, let’s relinquish our current interactive job. It is important not to run nested jobs by running salloc inside a running job.\nKill the current job by running (from Bash, not from ipython):\nexit\nThen, start an interactive job with a GPU:\nsalloc --time=1:0:0 --gpus-per-node=1 --mem=22000M\nReload the ipython module:\nmodule load ipython-kernel/3.11\nRe-activate the virtual python environment:\nsource /project/60055/env/bin/activate\nFinally, relaunch IPython:\nipython",
    "crumbs": [
      "AI",
      "<b><em>The JAX library</em></b>",
      "Accelerators"
    ]
  },
  {
    "objectID": "ai/jx/jx_accelerator.html#effect-on-timing",
    "href": "ai/jx/jx_accelerator.html#effect-on-timing",
    "title": "Accelerators",
    "section": "Effect on timing",
    "text": "Effect on timing\nHere is an example of the difference that a GPU makes compared to CPUs for a simple computation.\nThe following times are on my laptop which has one dedicated GPU.\nFirst, let’s set things up:\nimport jax. numpy as jnp\nfrom jax import random, device_put\nimport numpy as np\n\nseed = 0\nkey = random.PRNGKey(seed)\nkey, subkey = random.split(key)\n\nsize = 3000\nNow, let’s time a dot product of two arrays using NumPy (which only uses CPUs):\nx_np = np.random.normal(size=(size, size)).astype(np.float32)\n%timeit np.dot(x_np, x_np.T)\n58.6 ms ± 2.67 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\nWe can use the NumPy ndarrays in a JAX dot product function:\n%timeit jnp.dot(x_np, x_np.T).block_until_ready()\n31.1 ms ± 1.82 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\nRemember that whenever you benchmark JAX computations, you need to use the block_until_ready method, due to asynchronous dispatch, to ensure that you are timing the computation and not the creation of a future.\n\nIf you want to use NumPy ndarrays in JAX and you have accelerators available, a much better approach is to transfer them to the accelerators with the device_put method:\nx_to_gpu = device_put(x_np)\n%timeit jnp.dot(x_to_gpu, x_to_gpu.T).block_until_ready()\n13.2 ms ± 27.9 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\nThis is much faster and similar to the full JAX code would be:\nx_jx = random.normal(key, (size, size), dtype=jnp.float32)\n\n%timeit jnp.dot(x_jx, x_jx.T).block_until_ready()\n13.3 ms ± 33.3 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)",
    "crumbs": [
      "AI",
      "<b><em>The JAX library</em></b>",
      "Accelerators"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About this site",
    "section": "",
    "text": "This site contains Marie-Hélène Burle’s latest content.\nHer older training material can be found on the archived sites:"
  },
  {
    "objectID": "about.html#main-training-website",
    "href": "about.html#main-training-website",
    "title": "About this site",
    "section": "Main training website",
    "text": "Main training website\nThis is the mint (“mint is not training”) website.\nTo view all our training material, please visit our main training website."
  },
  {
    "objectID": "about.html#other-websites",
    "href": "about.html#other-websites",
    "title": "About this site",
    "section": "Other websites",
    "text": "Other websites\nIn addition, here are a few of our websites for various training events:\n\nAutumn School 2022\nTraining Modules 2022\nTraining Modules 2021\nSummer School 2020\nCoding Fundamentals for Humanists 2022 for the Digital Humanities Summer Institute\nCoding Fundamentals for Humanists 2021 for the Digital Humanities Summer Institute\nHSS Winter Series 2023\nHSS Winter Series 2022"
  },
  {
    "objectID": "ai/index.html",
    "href": "ai/index.html",
    "title": "AI",
    "section": "",
    "text": "Getting started with  \nAn intro course to DL with PyTorch\n\n\n\n\nFast computing with \nAn intro course to JAX\n\n\n\n\n\n\nJAX NN with \nA DL course with Flax\n\n\n\n\nA brief overview of  \nTraditional ML with scikit-learn\n\n\n\n\n\n\nWorkshops\nVarious DL topics\n\n\n\n\n60 min webinars\nVarious DL topics",
    "crumbs": [
      "AI",
      "<br>&nbsp;<em><b>AI</b></em><br><br>"
    ]
  },
  {
    "objectID": "ai/jx/jx_ad.html",
    "href": "ai/jx/jx_ad.html",
    "title": "Automatic differentiation",
    "section": "",
    "text": "One of the transformations that can be applied to array computations is the calculation of gradients which is crucial to the backpropagation through deep neural networks.\nConsidering the function f:\nWe can create a new function dfdx that computes the gradient of f w.r.t. x:\ndfdx returns the derivatives:",
    "crumbs": [
      "AI",
      "<b><em>The JAX library</em></b>",
      "Automatic differentiation"
    ]
  },
  {
    "objectID": "ai/jx/jx_ad.html#composing-transformations",
    "href": "ai/jx/jx_ad.html#composing-transformations",
    "title": "Automatic differentiation",
    "section": "Composing transformations",
    "text": "Composing transformations\nTransformations can be composed:\nprint(jit(grad(f))(1.))\n4.0\nprint(grad(jit(f))(1.))\n4.0",
    "crumbs": [
      "AI",
      "<b><em>The JAX library</em></b>",
      "Automatic differentiation"
    ]
  },
  {
    "objectID": "ai/jx/jx_ad.html#forward-and-reverse-modes",
    "href": "ai/jx/jx_ad.html#forward-and-reverse-modes",
    "title": "Automatic differentiation",
    "section": "Forward and reverse modes",
    "text": "Forward and reverse modes\nJAX offers other autodiff methods:\n\nreverse-mode vector-Jacobian products: jax.vjp,\nforward-mode Jacobian-vector products: jax.jvp.",
    "crumbs": [
      "AI",
      "<b><em>The JAX library</em></b>",
      "Automatic differentiation"
    ]
  },
  {
    "objectID": "ai/jx/jx_ad.html#higher-order-differentiation",
    "href": "ai/jx/jx_ad.html#higher-order-differentiation",
    "title": "Automatic differentiation",
    "section": "Higher-order differentiation",
    "text": "Higher-order differentiation\nWith a single variable, the grad function calls can be nested:\nd2fdx = grad(dfdx)   # function to compute 2nd order derivatives\nd3fdx = grad(d2fdx)  # function to compute 3rd order derivatives\n...\nWith several variables, you have to use the functions:\n\njax.jacfwd for forward-mode,\njax.jacrev for reverse-mode.",
    "crumbs": [
      "AI",
      "<b><em>The JAX library</em></b>",
      "Automatic differentiation"
    ]
  },
  {
    "objectID": "ai/jx/jx_install.html",
    "href": "ai/jx/jx_install.html",
    "title": "Installing JAX",
    "section": "",
    "text": "In this section, we will access a virtual training cluster through SSH and make JAX accessible.\nWe will also cover how to install JAX in the Alliance production clusters.\nUnless you aren’t planning to use accelerators, JAX relies on GPUs/TPUs dependencies determined by your OS and hardware (e.g. CUDA and CUDNN). Making sure that the dependencies are installed, compatible, and working with JAX can be finicky, so it is a lot easier to install JAX from pip wheels.",
    "crumbs": [
      "AI",
      "<b><em>The JAX library</em></b>",
      "Installation"
    ]
  },
  {
    "objectID": "ai/jx/jx_install.html#on-your-computer",
    "href": "ai/jx/jx_install.html#on-your-computer",
    "title": "Installing JAX",
    "section": "On your computer",
    "text": "On your computer\nOn your personal computer, use the wheel installation command from the official JAX site corresponding to your system.\n\nOn Windows, GPUs are only supported via Windows Subsystem for Linux 2.\n\nBecause JAX is designed for large array computations and machine learning, you will most likely want to use it on supercomputers. In this course, we will thus use a virtual Alliance cluster.",
    "crumbs": [
      "AI",
      "<b><em>The JAX library</em></b>",
      "Installation"
    ]
  },
  {
    "objectID": "ai/jx/jx_install.html#on-an-alliance-cluster",
    "href": "ai/jx/jx_install.html#on-an-alliance-cluster",
    "title": "Installing JAX",
    "section": "On an Alliance cluster",
    "text": "On an Alliance cluster\n\nLogging in through SSH\n\nOpen a terminal emulator\nWindows users:  Install the free version of MobaXTerm and launch it.\nmacOS users:   Launch Terminal.\nLinux users:     Open the terminal emulator of your choice.\n\n\nAccess the cluster through secure shell\n\nWindows users\nFollow the first 18% of this demo.\nFor “Remote host”, use the hostname we gave you.\nSelect the box “Specify username” and provide your username.\n\nNote that the password is entered through blind typing, meaning that you will not see anything happening as you type it. This is a Linux feature. While it is a little disturbing at first, do know that it is working. Make sure to type it slowly to avoid typos, then press the “enter” key on your keyboard.\n\n\n\nmacOS and Linux users\nIn the terminal, run:\nssh &lt;username&gt;@&lt;hostname&gt;\n\nReplace the username and hostname by their values. For instance:\nssh user21@somecluster.c3.ca\n\nYou will be asked a question, answer “Yes”.\nWhen prompted, type the password.\n\nNote that the password is entered through blind typing, meaning that you will not see anything happening as you type it. This is a Linux feature. While it is a little disturbing at first, do know that it is working. Make sure to type it slowly to avoid typos, then press the “enter” key on your keyboard.\n\n\n\n\nTroubleshooting\nProblems logging in are almost always due to typos. If you cannot log in, retry slowly, entering your password carefully.\n\n\n\nInstall JAX\n\nWe already created a Python virtual environment and installed JAX to save time. The instructions for today thus differ from what you would normally do, but I include the normal instructions in a separate tab for your future reference.\n\n\nTodayProduction cluster\n\n\nI already created a virtual Python environment under /project and installed JAX in it to save time and space. All you have to do is activate it:\nsource /project/60055/env/bin/activate\n\n\nLook for available Python modules:\nmodule spider python\nLoad the version of your choice:\nmodule load python/3.11.5\nCreate a Python virtual environment:\npython -m venv ~/env\nActivate it:\nsource ~/env/bin/activate\nUpdate pip from wheel:\npython -m pip install --upgrade pip --no-index\n\nWhenever a Python wheel for a package is available on the Alliance clusters, you should use it instead of downloading the package from PyPI. To do this, simply add the --no-index flag to the install command.\nYou can see whether a wheel is available with avail_wheels &lt;package&gt; or look at the list of available wheels.\nAdvantages of wheels:\n\ncompiled for the clusters hardware,\nensures no missing or conflicting dependencies,\nmuch faster installation.\n\n\nInstall JAX from wheel:\npython -m pip install jax --no-index\n\nDon’t forget the --no-index flag here: the wheel will save you from having to deal with the CUDA and CUDNN dependencies, making your life a lot easier.\n\n\nIf you want to install a particular version of JAX, you first need to see what wheel is available:\navail_wheels \"jax*\"\nThen load the wheel of your choice:\npython -m pip install jax==0.4.26 --no-index",
    "crumbs": [
      "AI",
      "<b><em>The JAX library</em></b>",
      "Installation"
    ]
  },
  {
    "objectID": "ai/jx/jx_jobs.html",
    "href": "ai/jx/jx_jobs.html",
    "title": "Running jobs",
    "section": "",
    "text": "This section is a quick review on how to submit jobs to the scheduler (Slurm) on the Alliance clusters. For more detailed information, please visit our wiki.\n\nThere are two types of jobs that can be launched on an Alliance cluster: interactive jobs and batch jobs.\n\nDon’t run computations on the login node: those are very small nodes not designed to handle anything heavy.\n\n\nInteractive jobs\nTo run Python interactively, you should launch an salloc session.\n\nExample:\n\nsalloc --time=xxx --mem-per-cpu=xxx --cpus-per-task=xxx\nThis takes you to a compute node where you can now launch Python (or even better IPython) to run computations:\nipython\n\nNote that while interactive jobs are great for code development, they are not resource efficient: all the resources that you requested are blocked for you while your job is running, whether you are making use of them (running heavy computations) or not (thinking, typing code, running computations that use only a fraction of the requested resources).\nBest to use this on sample data using few resources.\n\n\n\nScripts\nOnce you have a working and tested program, you should run a batch job on the resources you need to get your results. To run a Python script called &lt;your_script&gt;.py, you first need to write a job script:\n\nExample:\n\n\n&lt;your_job&gt;.sh\n\n#!/bin/bash\n#SBATCH --account=def-&lt;your_account&gt;\n#SBATCH --time=xxx\n#SBATCH --mem-per-cpu=xxx\n#SBATCH --cpus-per-task=xxx\n#SBATCH --job-name=\"&lt;your_job&gt;\"\n\nsource ~/env/bin/activate\npython &lt;your_script&gt;.py\n\n\nThen launch your job with:\nsbatch &lt;your_job&gt;.sh\nYou can monitor your job with sq (an alias for squeue -u $USER $@).\n\nBatch jobs are the best approach to run heavy computations requiring a lot of hardware.\nIt will save you lots of waiting time (Alliance clusters) or money (commercial clusters).",
    "crumbs": [
      "AI",
      "<b><em>The JAX library</em></b>",
      "Running jobs"
    ]
  },
  {
    "objectID": "ai/jx/jx_map.html",
    "href": "ai/jx/jx_map.html",
    "title": "How does it work?",
    "section": "",
    "text": "Before using JAX, it is critical to understand its functioning: JAX architecture is at the core of its efficiency and flexibility, but also the cause of a number of constraints.",
    "crumbs": [
      "AI",
      "<b><em>The JAX library</em></b>",
      "How does it work?"
    ]
  },
  {
    "objectID": "ai/jx/jx_map.html#map",
    "href": "ai/jx/jx_map.html#map",
    "title": "How does it work?",
    "section": "Map",
    "text": "Map\nHere is a schematic of JAX’s functioning:\n\n\n\n\n\n\n\n\n\ntracer\n\nTracing\n\n\n\njaxpr\n\nJaxprs\n(JAX expressions)\nintermediate\nrepresentation\n(IR)\n\n\n\ntracer-&gt;jaxpr\n\n\n\n\n\njit\n\n Just-in-time \n(JIT)\ncompilation\n\n\n\nhlo\n\nHigh-level\noptimized (HLO)\nprogram\n\n\n\njit-&gt;hlo\n\n\n\n\n\nxla\n\nAccelerated\n Linear Algebra \n(XLA)\n\n\n\nCPU\n\nCPU\n\n\n\nxla-&gt;CPU\n\n\n\n\n\nGPU\n\nGPU\n\n\n\nxla-&gt;GPU\n\n\n\n\n\nTPU\n\nTPU\n\n\n\nxla-&gt;TPU\n\n\n\n\n\ntransform\n\n Transformations \n\n\n\npy\n\nPure Python\nfunctions\n\n\n\npy-&gt;tracer\n\n\n\n\njaxpr-&gt;jit\n\n\n\n\njaxpr-&gt;transform\n\n\n\n\n\n\nhlo-&gt;xla",
    "crumbs": [
      "AI",
      "<b><em>The JAX library</em></b>",
      "How does it work?"
    ]
  },
  {
    "objectID": "ai/jx/jx_map.html#tracing",
    "href": "ai/jx/jx_map.html#tracing",
    "title": "How does it work?",
    "section": "Tracing",
    "text": "Tracing\nTracing happens during the first call of a function. Tracer objects are wrapped around each argument and record all operations performed on them, creating a Jaxpr (JAX expression). It is this intermediate representation—rather than the Python code—that JAX then uses.\nThe tracer objects used to create the Jaxpr contain information about the shape and dtype of the initial Python arguments, but not their values. This means that new inputs with the same shape and dtype will use the cached compiled program directly, skipping the Python code entirely. Inputs with new shape and/or dtype will trigger tracing again (so the Python function gets executed again).\nFunction side-effects are not recorded by the tracers, which means that they are not part of the Jaxprs. They will be executed once (during tracing), but are thereafter absent from the cached compiled program.\nFunctions which use values outside of their arguments (e.g. values from the global environment) will not update the cache if such values change.\nFor these reasons, only functionally pure functions (functions without side effects and which do not rely on values outside their arguments) should be used with JAX.",
    "crumbs": [
      "AI",
      "<b><em>The JAX library</em></b>",
      "How does it work?"
    ]
  },
  {
    "objectID": "ai/jx/jx_map.html#transformations",
    "href": "ai/jx/jx_map.html#transformations",
    "title": "How does it work?",
    "section": "Transformations",
    "text": "Transformations\nJAX is essentially a functional programming framework. Transformations are higher-order functions transforming Jaxprs.\nTransformations are composable and include:\n\njax.grad(): creates a function that evaluates the gradient of the input function,\njax.vmap(): implementation of automatic vectorization,\njax.pmap(): implementation of data parallelism across processing units,\n\nand finally, once other necessary transformations have been performed:\n\njax.jit(): just-in-time compilation for the XLA.",
    "crumbs": [
      "AI",
      "<b><em>The JAX library</em></b>",
      "How does it work?"
    ]
  },
  {
    "objectID": "ai/jx/jx_map.html#xla",
    "href": "ai/jx/jx_map.html#xla",
    "title": "How does it work?",
    "section": "XLA",
    "text": "XLA\nThe XLA (Accelerated Linear Algebra) compiler takes JIT-compiled JAX programs and optimizes them for the available hardware (CPUs, GPUs, or TPUs).",
    "crumbs": [
      "AI",
      "<b><em>The JAX library</em></b>",
      "How does it work?"
    ]
  },
  {
    "objectID": "ai/jx/jx_optimizations.html",
    "href": "ai/jx/jx_optimizations.html",
    "title": "Pushing optimizations further",
    "section": "",
    "text": "JAX feels lower level than other libraries (more constraints, more performance). This can be pushed further for additional speedups (but with additional code complexity).",
    "crumbs": [
      "AI",
      "<b><em>The JAX library</em></b>",
      "Pushing optimizations further"
    ]
  },
  {
    "objectID": "ai/jx/jx_optimizations.html#the-lax-api",
    "href": "ai/jx/jx_optimizations.html#the-lax-api",
    "title": "Pushing optimizations further",
    "section": "The lax API",
    "text": "The lax API\njax.numpy is a high-level NumPy-like API wrapped around jax.lax. jax.lax is a more efficient lower-level API itself wrapped around XLA. It is more powerful, but even stricter and requires many more lines of code.",
    "crumbs": [
      "AI",
      "<b><em>The JAX library</em></b>",
      "Pushing optimizations further"
    ]
  },
  {
    "objectID": "ai/jx/jx_optimizations.html#pallas-extension-to-write-gpu-and-tpu-kernels",
    "href": "ai/jx/jx_optimizations.html#pallas-extension-to-write-gpu-and-tpu-kernels",
    "title": "Pushing optimizations further",
    "section": "Pallas: extension to write GPU and TPU kernels",
    "text": "Pallas: extension to write GPU and TPU kernels\nWith the success of Triton, JAX built the Pallas extension that allows JAX users to write GPU kernels.\nIt also allows to write kernels for the TPU with moisaic.\n\n\n\n\n\n\n\n\n\ntracer\n\nTracing\n\n\n\njaxpr\n\nJaxprs\n(JAX expressions)\nintermediate\nrepresentation\n(IR)\n\n\n\ntracer-&gt;jaxpr\n\n\n\n\n\njit\n\n Just-in-time \n(JIT)\ncompilation\n\n\n\nhlo\n\nHigh-level\noptimized (HLO)\nprogram\n\n\n\njit-&gt;hlo\n\n\n\n\n\ntriton\n\nTriton\n\n\n\nGPU\n\nGPU\n\n\n\ntriton-&gt;GPU\n\n\n\n\n\nmosaic\n\nMosaic\n\n\n\nTPU\n\nTPU\n\n\n\nmosaic-&gt;TPU\n\n\n\n\n\ntransform\n\nVectorization\nParallelization\n   Differentiation  \n\n\n\npy\n\nPure Python\nfunctions\n\n\n\npy-&gt;tracer\n\n\n\n\njaxpr-&gt;jit\n\n\n\n\njaxpr-&gt;transform\n\n\n\n\n\n\nhlo-&gt;triton\n\n\n\n\nhlo-&gt;mosaic",
    "crumbs": [
      "AI",
      "<b><em>The JAX library</em></b>",
      "Pushing optimizations further"
    ]
  },
  {
    "objectID": "ai/jx/jx_parallel.html",
    "href": "ai/jx/jx_parallel.html",
    "title": "Parallel computing",
    "section": "",
    "text": "JAX is designed for DNN and linear algebra at scale. Processing vast amounts of data in parallel is crucial to its goal. Two of JAX’s transformations allow to turn linear code into parallel code very easily.",
    "crumbs": [
      "AI",
      "<b><em>The JAX library</em></b>",
      "Parallel computing"
    ]
  },
  {
    "objectID": "ai/jx/jx_parallel.html#vectorization",
    "href": "ai/jx/jx_parallel.html#vectorization",
    "title": "Parallel computing",
    "section": "Vectorization",
    "text": "Vectorization\nRemember how a number of transformations are applied to jaxprs. We already saw two of JAX’s main transformations: JIT compilation with jax.jit and automatic differentiation with jax.grad. Vectorization with jax.vmap is another one.\nIt automates the vectorization of complex functions (operations on arrays are naturally executed in a vectorized fashion—as is the case in R, in NumPy, etc.—but more complex functions are not).\nHere is an example from JAX 101 commonly encountered in deep learning:\nimport jax\nimport jax.numpy as jnp\n\nx = jnp.arange(5)\nw = jnp.array([2., 3., 4.])\n\ndef convolve(x, w):\n    output = []\n    for i in range(1, len(x)-1):\n        output.append(jnp.dot(x[i-1:i+2], w))\n    return jnp.array(output)\n\nconvolve(x, w)\nArray([11., 20., 29.], dtype=float32)\n\nSee this great post for explanations of convolutions.\n\nYou will probably want to apply the function convolve() to a batch of weights w and vectors x.\nxs = jnp.stack([x, x, x])\nws = jnp.stack([w, w, w])\nWe apply the jax.vmap() transformation to the convolve() function and pass the batches to it:\nvconvolve = jax.vmap(convolve)\nvconvolve(xs, ws)\nArray([[11., 20., 29.],\n       [11., 20., 29.],\n       [11., 20., 29.]], dtype=float32)\n\nAs we already saw, transformations can be composed:\nvconvolve_jit = jax.jit(vconvolve)\nvconvolve_jit(xs, ws)\nArray([[11., 20., 29.],\n       [11., 20., 29.],\n       [11., 20., 29.]], dtype=float32)",
    "crumbs": [
      "AI",
      "<b><em>The JAX library</em></b>",
      "Parallel computing"
    ]
  },
  {
    "objectID": "ai/jx/jx_parallel.html#parallel-runs-across-devices",
    "href": "ai/jx/jx_parallel.html#parallel-runs-across-devices",
    "title": "Parallel computing",
    "section": "Parallel runs across devices",
    "text": "Parallel runs across devices\nThe jax.pmap transformation does the same thing but each computation runs on a different device (e.g. a different GPU) on the same node, allowing to scale things up further:\njax.pmap(convolve)(xs, ws)\njax.pmap automatically JIT compiles the code, so it is unnecessary to pass this to jax.jit.\n\nJAX is also capable of running distributed arrays across multiple devices through sharding.\nJAX does not have the ability to scale things up to the level of multi-node clusters, but the mpi4jax extension provides multi-host communication for distributed parallelism.",
    "crumbs": [
      "AI",
      "<b><em>The JAX library</em></b>",
      "Parallel computing"
    ]
  },
  {
    "objectID": "ai/jx/jx_resources.html",
    "href": "ai/jx/jx_resources.html",
    "title": "Resources",
    "section": "",
    "text": "Here is a list of resources to get started with JAX.\n\n\nOfficial documentation\n\nJAX GitHub repo\nOfficial documentation\nJAX API\n\n\n\nOther resources\nAwesome JAX is a great list of resources on JAX.\n\n\nQ&A\n\nJAX GitHub Discussions\nStack Overflow [jax] tag\n\n\n\nAlliance wiki\nThere is currently no page on JAX, but there is a page on Flax.",
    "crumbs": [
      "AI",
      "<b><em>The JAX library</em></b>",
      "Resources"
    ]
  },
  {
    "objectID": "ai/jx/wb_jax.html",
    "href": "ai/jx/wb_jax.html",
    "title": "Accelerated array computing and flexible differentiation with JAX",
    "section": "",
    "text": "JAX is an open source Python library for high-performance array computing and flexible automatic differentiation.\nHigh-performance computing is achieved by asynchronous dispatch, just-in-time compilation, the XLA compiler for linear algebra, and full compatibility with accelerators (GPUs and TPUs).\nAutomatic differentiation uses Autograd and works with complex control flows (conditions, recursions), second and third-order derivatives, forward and reverse modes. This makes JAX ideal for machine learning and neural network libraries such as Flax are built on it.\nThis webinar will give an overview of JAX’s principles and functioning.\n\nSlides (Click and wait: this reveal.js presentation may take a little time to load.)",
    "crumbs": [
      "AI",
      "<b><em>Webinars</em></b>",
      "Accelerated array & AD with JAX"
    ]
  },
  {
    "objectID": "ai/jxai/fl_data_bits.html",
    "href": "ai/jxai/fl_data_bits.html",
    "title": "Loading data",
    "section": "",
    "text": "Transform the data\nWe use PyTorch v2:\nimport numpy as np\nfrom torchvision.transforms import v2 as T\n\nimg_size = 224\n\ndef to_np_array(pil_image):\n  return np.asarray(pil_image.convert(\"RGB\"))\n\ndef normalize(image):\n    # Image preprocessing matches the one of pretrained ViT\n    mean = np.array([0.5, 0.5, 0.5], dtype=np.float32)\n    std = np.array([0.5, 0.5, 0.5], dtype=np.float32)\n    image = image.astype(np.float32) / 255.0\n    return (image - mean) / std\n\ntv_train_transforms = T.Compose([\n    T.RandomResizedCrop((img_size, img_size), scale=(0.7, 1.0)),\n    T.RandomHorizontalFlip(),\n    T.ColorJitter(0.2, 0.2, 0.2),\n    T.Lambda(to_np_array),\n    T.Lambda(normalize),\n])\n\ntv_test_transforms = T.Compose([\n    T.Resize((img_size, img_size)),\n    T.Lambda(to_np_array),\n    T.Lambda(normalize),\n])\n\ndef get_transform(fn):\n    def wrapper(batch):\n        batch[\"image\"] = [\n            fn(pil_image) for pil_image in batch[\"image\"]\n        ]\n        # map label index between 0 - 19\n        batch[\"label\"] = [\n            labels_mapping[label] for label in batch[\"label\"]\n        ]\n        return batch\n    return wrapper\n\ntrain_transforms = get_transform(tv_train_transforms)\nval_transforms = get_transform(tv_test_transforms)\n\ntrain_dataset = train_dataset.with_transform(train_transforms)\nval_dataset = val_dataset.with_transform(val_transforms)\n\n\nVisualize a few samples\nimport matplotlib.pyplot as plt\n\n\ndef display_datapoints(*datapoints, tag=\"\", names_map=None):\n    num_samples = len(datapoints)\n\n    fig, axs = plt.subplots(1, num_samples, figsize=(20, 10))\n    for i, datapoint in enumerate(datapoints):\n        if isinstance(datapoint, dict):\n            img, label = datapoint[\"image\"], datapoint[\"label\"]\n        else:\n            img, label = datapoint\n\n        if hasattr(img, \"dtype\") and img.dtype in (np.float32, ):\n            img = ((img - img.min()) / (img.max() - img.min()) * 255.0).astype(np.uint8)\n\n        label_str = f\" ({names_map[label]})\" if names_map is not None else \"\"\n        axs[i].set_title(f\"{tag}Label: {label}{label_str}\")\n        axs[i].imshow(img)\n\ndisplay_datapoints(\n    train_dataset[0],\n    train_dataset[1000],\n    train_dataset[2000],\n    train_dataset[3000],\n    tag=\"(Training) \",\n    names_map=train_dataset.features[\"label\"].names\n)\n\ndisplay_datapoints(\n    val_dataset[0],\n    val_dataset[1000],\n    val_dataset[2000],\n    val_dataset[-1],\n    tag=\"(Validation) \",\n    names_map=val_dataset.features[\"label\"].names\n)"
  },
  {
    "objectID": "ai/jxai/fl_jax.html",
    "href": "ai/jxai/fl_jax.html",
    "title": "Introduction: JAX",
    "section": "",
    "text": "A brief introduction to JAX and to the JAX deep learning stack.\n\nSlides (Click and wait: the presentation might take a few instants to load)",
    "crumbs": [
      "AI",
      "<b><em>Intro DL with JAX</em></b>",
      "Introduction: JAX"
    ]
  },
  {
    "objectID": "ai/jxai/fl_jaxdl.html",
    "href": "ai/jxai/fl_jaxdl.html",
    "title": "Deep learning with JAX",
    "section": "",
    "text": "JAX is perfect for developing deep learning models:\n\nit deals with multi-dimensional arrays,\nit is extremely fast,\nit is optimized for accelerators,\nand it is capable of flexible automatic differentiation.\n\nJAX is however not a DL library. While it is possible to create neural networks directly in JAX, it makes more sense to use libraries built on JAX that provide the toolkit necessary to build and train neural networks.\nxxx info from https://github.com/jax-ml/jax-ai-stack?tab=readme-ov-file"
  },
  {
    "objectID": "ai/jxai/fl_jaxdl.html#deep-learning-workflow",
    "href": "ai/jxai/fl_jaxdl.html#deep-learning-workflow",
    "title": "Deep learning with JAX",
    "section": "Deep learning workflow",
    "text": "Deep learning workflow\nTraining a neural network from scratch requires a number of steps:\n\n\n\n\n\n\n\n\n\nLoad\\ndataset\n\nLoad\ndataset\n\n\n\nDefine\\narchitecture\n\nDefine\narchitecture\n\n\n\nLoad\\ndataset-&gt;Define\\narchitecture\n\n\n\n\n\nTrain\n\nTrain\n\n\n\nDefine\\narchitecture-&gt;Train\n\n\n\n\n\nTest\n\nTest\n\n\n\nTrain-&gt;Test\n\n\n\n\n\nSave\\nmodel\n\nSave\nmodel\n\n\n\nTest-&gt;Save\\nmodel\n\n\n\n\n\n\n\n\n\n\n Pretrained models can also be used for feature extraction or transfer learning."
  },
  {
    "objectID": "ai/jxai/fl_jaxdl.html#deep-learning-ecosystem-for-jax",
    "href": "ai/jxai/fl_jaxdl.html#deep-learning-ecosystem-for-jax",
    "title": "Deep learning with JAX",
    "section": "Deep learning ecosystem for JAX",
    "text": "Deep learning ecosystem for JAX\nHere is a classic ecosystem of libraries for deep learning with JAX:\n\nLoad datasets\nThere are already good tools to load datasets (e.g. torchvision, TensorFlow datasets, Hugging Face datasets, Grain), so JAX did not worry about creating its own implementation.\nDefine network architecture\nNeural networks can be build in JAX from scratch, but a number of packages built on JAX provide the necessary toolkit. Flax is the option recommended by the JAX developers and the one we will use in this course.\nTrain\nThe package CLU (Common Loop Utils) is a set of helpers to write shorter training loops. Optax provides loss and optimization functions. Orbax brings checkpointing utilities.\nTest\nTesting a model is easy to do directly in JAX.\nSave model\nFlax provides methods to save a model.\n\n To sum up, here is an ecosystem of libraries to use JAX for neural networks:\n\n\n\n\n\n\n\n\n\nload\nLoad data\n\n\n\nnn\nDefine network\n\n\n\n\nPyTorch\n\nPyTorch\n\n\n\n\ntrain\nTrain\nOptimize\nCheckpoint\n\n\n\n\nflax1\n\nFlax\n\n\n\n\ntest\nTest\n\n\n\n\nsave\nSave model\n\n\n\n\nTensorFlow\n\nTensorFlow\n\n\n\n\nPyTorch-&gt;flax1\n\n\n\n\n\nHugging Face\n\nHugging Face\n\n\n\n\nTensorFlow-&gt;flax1\n\n\n\n\n\nGrain\n\nGrain\n\n\n\n\nHugging Face-&gt;flax1\n\n\n\n\n\nGrain-&gt;flax1\n\n\n\n\n\njax1\n\nJAX\nCLU\nOptax\nOrbax\n\n\n\nflax1-&gt;jax1\n\n\n\n\n\nflax2\n\nFlax\n\n\n\njax2\n\nJAX\n\n\n\njax1-&gt;jax2\n\n\n\n\n\njax2-&gt;flax2\n\n\n\n\n\n\n\n\n\n\n When working from pretrained models, Hugging Face also provides a great API to download from thousands of pretrained models."
  },
  {
    "objectID": "ai/jxai/fl_jaxdl.html#how-to-get-started",
    "href": "ai/jxai/fl_jaxdl.html#how-to-get-started",
    "title": "Deep learning with JAX",
    "section": "How to get started?",
    "text": "How to get started?\nA common approach is to start from one of the example projects and use it as a template."
  },
  {
    "objectID": "ai/jxai/fl_model.html",
    "href": "ai/jxai/fl_model.html",
    "title": "Defining a model architecture",
    "section": "",
    "text": "Defining a model architecture in Flax is currently done with the flax.linen API and is quite straightforward."
  },
  {
    "objectID": "ai/jxai/fl_model.html#the-linen-api",
    "href": "ai/jxai/fl_model.html#the-linen-api",
    "title": "Defining a model architecture",
    "section": "The Linen API",
    "text": "The Linen API\nLinen is Flax’s current 1 NN API. Let’s load it, along with JAX and the JAX NumPy API:\nimport jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\nTo define a model, we create a subclass of the nn.Module which inherits all the characteristics of that module, saving us from defining all the behaviours a neural network should have (exactly as in PyTorch).\nWhat we do need to define of course, is the architecture and the flow of data through it.\nLinen contains all the classic elements to define a NN model:\n\nmodules to define standard layers (e.g. fully connected layer with Dense\nconvolution layer with Conv, pooling layers with max_pool or avg_pool)\na set of activation functions (e.g. relu, softmax, sigmoid)\nJAX transformations (e.g. jit, vmap, jvp, vjp)\n\nLinen makes use of JAX’s shape inference so there is no need for “in” features (e.g. nn.Dense only requires one argument: the “out” feature; this is in contrast with PyTorch’s nn.Linear which requires both an “in” and an “out” features).\n\nExample\nAs we already saw, Linen provides two syntaxes:\n\na longer syntax using setup: more redundant, more PyTorch-like, allows to define multiple methods,\na compact one with the @nn.compact decorator that can only use a single method and avoids duplication between setup and call.\n\nLet’s use the latter here:\nclass CNN(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n      x = nn.Conv(features=32, kernel_size=(3, 3))(x)\n      x = nn.relu(x)\n      x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n      x = nn.Conv(features=64, kernel_size=(3, 3))(x)\n      x = nn.relu(x)\n      x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n      x = x.reshape((x.shape[0], -1))\n      x = nn.Dense(features=256)(x)\n      x = nn.relu(x)\n      x = nn.Dense(features=10)(x)\n      x = nn.log_softmax(x)\n      return x\nNow we can create an instance of the model:\ncnn = CNN()\nRemember that in Linen the parameters are not part of the model: we need to initialize them as a method of the model by passing a random key and JAX array setting up their shape:\ndef get_initial_params(key):\n    init_shape = jnp.ones((1, 28, 28, 1))\n    initial_params = cnn.init(key, init_shape)\n    return initial_params\n\nkey = jax.random.key(0)\nkey, model_key = jax.random.split(key)\n\nparams = get_initial_params(model_key)"
  },
  {
    "objectID": "ai/jxai/fl_model.html#model-inspection",
    "href": "ai/jxai/fl_model.html#model-inspection",
    "title": "Defining a model architecture",
    "section": "Model inspection",
    "text": "Model inspection\nBefore using a model, it is a good idea to inspect it and make sure that everything is ok.\n\nInspect model layers\nThe tabulate method prints a summary table of each module (layer) in our model:\nprint(cnn.tabulate(\n    jax.random.key(0), \n    jnp.ones((1, 28, 28, 1)),\n    compute_flops=True, \n    compute_vjp_flops=True)\n      )\n                                                    CNN Summary\n┏━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ path    ┃ module ┃ inputs              ┃ outputs             ┃ flops   ┃ vjp_flops ┃ params                     ┃\n┡━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         │ CNN    │ float32[1,28,28,1]  │ float32[1,10]       │ 8708144 │ 26957634  │                            │\n├─────────┼────────┼─────────────────────┼─────────────────────┼─────────┼───────────┼────────────────────────────┤\n│ Conv_0  │ Conv   │ float32[1,28,28,1]  │ float32[1,28,28,32] │ 455424  │ 1341472   │ bias: float32[32]          │\n│         │        │                     │                     │         │           │ kernel: float32[3,3,1,32]  │\n│         │        │                     │                     │         │           │                            │\n│         │        │                     │                     │         │           │ 320 (1.3 KB)               │\n├─────────┼────────┼─────────────────────┼─────────────────────┼─────────┼───────────┼────────────────────────────┤\n│ Conv_1  │ Conv   │ float32[1,14,14,32] │ float32[1,14,14,64] │ 6566144 │ 19704320  │ bias: float32[64]          │\n│         │        │                     │                     │         │           │ kernel: float32[3,3,32,64] │\n│         │        │                     │                     │         │           │                            │\n│         │        │                     │                     │         │           │ 18,496 (74.0 KB)           │\n├─────────┼────────┼─────────────────────┼─────────────────────┼─────────┼───────────┼────────────────────────────┤\n│ Dense_0 │ Dense  │ float32[1,3136]     │ float32[1,256]      │ 1605888 │ 5620224   │ bias: float32[256]         │\n│         │        │                     │                     │         │           │ kernel: float32[3136,256]  │\n│         │        │                     │                     │         │           │                            │\n│         │        │                     │                     │         │           │ 803,072 (3.2 MB)           │\n├─────────┼────────┼─────────────────────┼─────────────────────┼─────────┼───────────┼────────────────────────────┤\n│ Dense_1 │ Dense  │ float32[1,256]      │ float32[1,10]       │ 5130    │ 17940     │ bias: float32[10]          │\n│         │        │                     │                     │         │           │ kernel: float32[256,10]    │\n│         │        │                     │                     │         │           │                            │\n│         │        │                     │                     │         │           │ 2,570 (10.3 KB)            │\n├─────────┼────────┼─────────────────────┼─────────────────────┼─────────┼───────────┼────────────────────────────┤\n│         │        │                     │                     │         │     Total │ 824,458 (3.3 MB)           │\n└─────────┴────────┴─────────────────────┴─────────────────────┴─────────┴───────────┴────────────────────────────┘\n\n                                        Total Parameters: 824,458 (3.3 MB)\n\n\nflops: estimated FLOPs (floating point operations per second) cost of forward pass\nvjp_flops: estimated FLOPs cost of backward pass (vjp stands for vector-Jacobian product)\n\n\n\n\nInspect initial parameters\n\n\nYour turn:\n\nThe summary table includes the shape of the parameters.\nBased on what we already learned, what is another way to get this information?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nPrinting the pytree of the initial parameters we created would make it fairly hard to get a sense of their structure, but we know that JAX can operate on pytrees thanks to the tree_util module, so let’s make use of this:\njax.tree.map(jnp.shape, params)\n{'Conv_0': {'bias': (32,), 'kernel': (3, 3, 1, 32)},\n 'Conv_1': {'bias': (64,), 'kernel': (3, 3, 32, 64)},\n 'Dense_0': {'bias': (256,), 'kernel': (3136, 256)},\n 'Dense_1': {'bias': (10,), 'kernel': (256, 10)}}"
  },
  {
    "objectID": "ai/jxai/fl_model.html#footnotes",
    "href": "ai/jxai/fl_model.html#footnotes",
    "title": "Defining a model architecture",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFlax had an initial flax.nn API, now retired. Linen made things easier and more PyTorch-like.\nA new API called NNX is being developed and may replace Linen in the future. It makes Flax even more similar to PyTorch by breaking with JAX’s functionally pure functions requirements and bringing the parameters back into the model.↩︎"
  },
  {
    "objectID": "ai/jxai/fl_optimization.html#data-padding",
    "href": "ai/jxai/fl_optimization.html#data-padding",
    "title": "Optimizations",
    "section": "Data padding",
    "text": "Data padding\n\nPrevent recompilation for the last batch that is smaller (different shape)."
  },
  {
    "objectID": "ai/jxai/fl_optimization.html#learning-rate-scheduling",
    "href": "ai/jxai/fl_optimization.html#learning-rate-scheduling",
    "title": "Optimizations",
    "section": "Learning rate scheduling",
    "text": "Learning rate scheduling"
  },
  {
    "objectID": "ai/jxai/fl_optimization.html#using-multiple-accelerators",
    "href": "ai/jxai/fl_optimization.html#using-multiple-accelerators",
    "title": "Optimizations",
    "section": "Using multiple accelerators",
    "text": "Using multiple accelerators\nParallel runs on multiple GPUs/TPUs"
  },
  {
    "objectID": "ai/jxai/fl_resources.html",
    "href": "ai/jxai/fl_resources.html",
    "title": "Resources",
    "section": "",
    "text": "Here is a list of resources to get started with deep learning using JAX."
  },
  {
    "objectID": "ai/jxai/fl_resources.html#jax",
    "href": "ai/jxai/fl_resources.html#jax",
    "title": "Resources",
    "section": "JAX",
    "text": "JAX"
  },
  {
    "objectID": "ai/jxai/fl_resources.html#data-loaders",
    "href": "ai/jxai/fl_resources.html#data-loaders",
    "title": "Resources",
    "section": "Data loaders",
    "text": "Data loaders"
  },
  {
    "objectID": "ai/jxai/fl_resources.html#flax",
    "href": "ai/jxai/fl_resources.html#flax",
    "title": "Resources",
    "section": "Flax",
    "text": "Flax\n\nOfficial documentation\n\nFlax GitHub repo\nOfficial documentation\nFlax API\n\n\n\nOther resources\nHere is a list of projects using Flax.\n\n\nQ&A\n\nFlax GitHub Discussions\nStack Overflow [flax] tag\n\n\n\nAlliance\nThe Alliance wiki has a page on Flax."
  },
  {
    "objectID": "ai/jxai/fl_resources.html#optax",
    "href": "ai/jxai/fl_resources.html#optax",
    "title": "Resources",
    "section": "Optax",
    "text": "Optax"
  },
  {
    "objectID": "ai/jxai/fl_resources.html#orbax",
    "href": "ai/jxai/fl_resources.html#orbax",
    "title": "Resources",
    "section": "Orbax",
    "text": "Orbax"
  },
  {
    "objectID": "ai/jxai/fl_state.html",
    "href": "ai/jxai/fl_state.html",
    "title": "Flax’s handling of model states",
    "section": "",
    "text": "Deep learning models can be split into two categories depending on the framework used to train them: stateful and stateless models. Flax—being built on top of JAX—falls in the latter category.\nIn this section, we will see what all of this means and how Flax handles model states."
  },
  {
    "objectID": "ai/jxai/fl_state.html#dealing-with-state-in-jax",
    "href": "ai/jxai/fl_state.html#dealing-with-state-in-jax",
    "title": "Flax’s handling of model states",
    "section": "Dealing with state in JAX",
    "text": "Dealing with state in JAX\nJAX JIT compilation requires that functions be without side effects since side effects are only executed once, during tracing.\nUpdating model parameters and optimizer state thus cannot be done as a side-effect. The state cannot be part of the model instance—it needs to be explicit, that is, separated from the model. During instantiation, no memory is allocated for the parameters. During the forward pass, the parameters will be part of the inputs, along with the data. The model is thus stateless and the constrains of pure functional programming are met (inputs lead to outputs without external influence or side effects).\nLet’s see why a stateful approach doesn’t work with JAX1: instead of defining a neural network class, we will define a very simple Counter class, following the PyTorch approach, that just adds 1. This allows us to see right away what is going on.\nimport jax\nimport jax.numpy as jnp\n\nclass Counter:\n    def __init__(self):\n        self.n = 0\n      \n    def count(self) -&gt; int:\n        \"\"\"Adds one to the counter and returns the new value.\"\"\"\n        self.n += 1\n        return self.n\n  \n    def reset(self):\n        \"\"\"Resets the counter to zero.\"\"\"\n        self.n = 0\nNow we can create an instance called counter of the Counter class.\ncounter = Counter()\nWe can use the counter:\nfor _ in range(3):\n    print(counter.count())\n1\n2\n3\nNow, let’s try with a JIT compiled version of count():\ncount_jit = jax.jit(counter.count)\n\ncounter.reset()\n\nfor _ in range(3):\n    print(count_jit())\n1\n1\n1\nThis is because count is not a functionally pure function. The tracing happens for the first run of the function (first iteration of the loop). Thereafter, the compiled version will rerun without taking into account the modifications of the attributes of counter.\nFor this to work, we need to initialize an explicit state and pass it as an argument to the count function:\nState = int\n\nclass Counter:\n    def count(self, n: State) -&gt; tuple[int, State]:\n        return n+1, n+1\n    \n    def reset(self) -&gt; State:\n        return 0\n\ncounter = Counter()\nstate = counter.reset()\n\nfor _ in range(3):\n    value, state = counter.count(state)\n    print(value)\n1\n2\n3\ncount_jit = jax.jit(counter.count)\n\nstate = counter.reset()\n\nfor _ in range(3):\n    value, state = count_jit(state)\n    print(value)\n1\n2\n3\n\nAs explained in JAX’s documentation, we turned a function of the type:\nclass StatefulClass\n  state: State\n  def stateful_method(*args, **kwargs) -&gt; Output:\nInto:\nclass StatelessClass\n  def stateless_method(state: State, *args, **kwargs) -&gt; (Output, State):"
  },
  {
    "objectID": "ai/jxai/fl_state.html#stateful-vs-stateless-models",
    "href": "ai/jxai/fl_state.html#stateful-vs-stateless-models",
    "title": "Flax’s handling of model states",
    "section": "Stateful vs stateless models",
    "text": "Stateful vs stateless models\n\nStateful models\nIn frameworks such as PyTorch or the Julia package Flux, model parameters and optimizer state are stored within the model instance. Instantiating a PyTorch model allocates memory for the model parameters. The model can then be described as stateful.\n\n\nStateless models\nFrameworks based on JAX such as Flax but also the Julia package Lux (a modern rewrite of Flux with explicit model parameters and a philosophy similar to JAX’s) are stateless: they follow a functional programming approach in which the parameters are separate from the model and are passed as inputs to the forward pass along with the data.\n\n\nExample: PyTorch vs Flax\nFlax, being built on JAX, it requires functionally pure functions and thus stateless models.\nHere is a comparison of the approach taken by PyTorch (stateful) vs Flax (stateless) to define and initialize a model (simplified model and workflow to show the principle):\n\nPyTorchFlax\n\n\nThis is how PyTorch works:\nimport torch\nimport torch.nn as nn\n\n# we create a subclass of torch.nn.Module\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.dense1 = nn.Linear(4, 144)\n        self.dense2 = nn.Linear(144, 4)\n\n    def forward(self, x):\n        x = self.dense1(x)\n        x = F.relu(x)\n        x = self.dense2(x)\n        return x\n\n# Create model instance\nmodel = Net()\n\n# Random data and labels\ndata = torch.empty((4, 12, 12, 1))\nlabels = torch.randn((4, 12, 12, 1))\nDuring the forward pass, only the inputs are passed through the model, but of course the outputs depend on the inputs and on the state of the model.\n\n\nHere is the Flax approach:\nimport jax\nimport jax.numpy as jnp\nfrom jax import random\nfrom flax import linen as nn\nimport optax\n\nSetup syntaxCompact syntax\n\n\nFlax provides a setup syntax of model definition which will look more familiar to PyTorch users:\n# Create a subclass of torch.nn.Module\nclass Net(nn.Module):\n  def setup(self):\n    self.dense1 = nn.Dense(12)\n    self.dense2 = nn.Dense(1)\n\n  def __call__(self, x):\n    x = self.dense1(x)\n    x = nn.relu(x)\n    x = self.dense2(x)\n    return x\n\n\nFlax comes with a compact syntax of model definition which is equivalent to the setup syntax in all respect except style:\n# Create a subclass of torch.nn.Module\nclass Net(nn.Module):\n  @nn.compact\n  def __call__(self, x):\n    x = nn.Dense(12, name=\"dense1\")(x)\n    x = nn.relu(x)\n    x = nn.Dense(12, name=\"dense2\")(x)\n    return x\n\n\n\nThe parameters are not part of the model. You initialize them afterwards and create a parameter object:\n# Create model instance\nmodel = Net()\n\n# Random data and labels\nkey, subkey1, subkey2 = random.split(random.key(13), 3)\ndata = jnp.empty((4, 12, 12, 1))\nlabels = random.normal(subkey1, (4, 12, 12, 1))\n\n# Initialize model parameters\nparams = model.init(subkey2, data)\n\n\n\nSimilarly, here are the stateful and stateless approaches to train the model:\n\nPyTorchFlax\n\n\n# Forward pass\nlogits = model(data)\n\nloss = nn.CrossEntropyLoss(logits, labels)\n\n# Calculate gradients\nloss.backward()\n\n# Optimze parameters\noptimizer.step()\n\n\n# Forward pass\ndef loss_func(params, data):\n    logits = model.apply(params, data)\n    loss = optax.softmax_cross_entropy(logits, labels).mean()\n    return loss\n\n# Calculate gradients\ngrads = jax.grad(loss_func)(params)\n\n# Optimze parameters\nparams = state.apply_gradients(grads)\nThe parameters are passed as inputs, along with the data, during the forward pass."
  },
  {
    "objectID": "ai/jxai/fl_state.html#flax-training-state",
    "href": "ai/jxai/fl_state.html#flax-training-state",
    "title": "Flax’s handling of model states",
    "section": "Flax training state",
    "text": "Flax training state\nThe demo above is stripped of any complexity to show the principle, but it is not realistic.\nTo handle every changing state during training (training step, state of the parameters, state of the optimizer), you can create a Flax training state.\nFlax provides a dataclass that you can subclass to create a new training state class:\nfrom flax.training import train_state\n\nclass TrainState(train_state.TrainState):\n    batch_stats: flax.core.FrozenDict\nThen you can define the Flax training step with TrainState.create:\nstate = TrainState.create(\n    apply_fn = model.apply,\n    params = modulel.init(subkey2, data),\n    tx = optax.sgd(0.01),\n    batch_stats = params['batch_stats'],\n)"
  },
  {
    "objectID": "ai/jxai/fl_state.html#nnx",
    "href": "ai/jxai/fl_state.html#nnx",
    "title": "Flax’s handling of model states",
    "section": "NNX",
    "text": "NNX\nA new Flax API is under development and might replace Linen at some point.\nIt provides transformations similar to JAX’s but which work on non-pure functions. This would bring Flax much closer to PyTorch and turn it into a stateful NN library by re-adding the parameters inside the model."
  },
  {
    "objectID": "ai/jxai/fl_state.html#footnotes",
    "href": "ai/jxai/fl_state.html#footnotes",
    "title": "Flax’s handling of model states",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nModified from JAX’s documentation.↩︎"
  },
  {
    "objectID": "ai/jxbayesian/wb_bayesian.html",
    "href": "ai/jxbayesian/wb_bayesian.html",
    "title": "Bayesian inference in JAX",
    "section": "",
    "text": "Bayesian statistics is more intuitive to the way we (humans) think about the world and easier to interpret than the traditional frequentist approach. Moreover, it allows for the incorporation of prior information and diverse data, it provides a measure of uncertainty, and it is extremely valuable in difficult situations with little data. The downside is that it is computationally complex and intensive. In addition, the best performing algorithms require burdensome calculations of derivatives. This explains its initial limitations.\nWith the advent of increasingly performant probabilistic programming languages (PPLs), algorithms, compilers, automatic differentiation engines, and computer hardware, Bayesian approaches are now fast growing in popularity in many fields.\nJAX is a library for Python that makes use of the extremely performant XLA compiler, runs on accelerators (GPUs/TPUs), provides automatic differentiation, just-in-time compilation, batching, and parallelization. In short, it is a perfect tool for Bayesian statistics. Not surprisingly, many PPLs now use it as a backend and Stan users (and developers!) are turning to it.\nIn this webinar, I will give a brief and very high-level introduction to Bayesian inference and JAX, then talk about the various PPLs and samplers which use JAX.\n\nSlides (Click and wait: this reveal.js presentation may take a little time to load.)",
    "crumbs": [
      "AI",
      "<b><em>Webinars</em></b>",
      "Bayesian inference in JAX"
    ]
  },
  {
    "objectID": "ai/pt/pt_autograd.html",
    "href": "ai/pt/pt_autograd.html",
    "title": "Automatic differentiation",
    "section": "",
    "text": "PyTorch has automatic differentiation capabilities—meaning that it can track all the operations performed on tensors during the forward pass and compute all the gradients automatically for the backpropagation—thanks to its package torch.autograd.\nLet’s have a look at this.",
    "crumbs": [
      "AI",
      "<b><em>Intro DL with PyTorch</em></b>",
      "Automatic differentiation"
    ]
  },
  {
    "objectID": "ai/pt/pt_autograd.html#some-definitions",
    "href": "ai/pt/pt_autograd.html#some-definitions",
    "title": "Automatic differentiation",
    "section": "Some definitions",
    "text": "Some definitions\nDerivative of a function:\nRate of change of a function with a single variable w.r.t. its variable.\nPartial derivative:\nRate of change of a function with multiple variables w.r.t. one variable while other variables are considered as constants.\nGradient:\nVector of partial derivatives of function with several variables.\nDifferentiation:\nCalculation of the derivatives of a function.\nChain rule:\nFormula to calculate the derivatives of composite functions.\nAutomatic differentiation:\nAutomatic computation of partial derivatives by algorithms.",
    "crumbs": [
      "AI",
      "<b><em>Intro DL with PyTorch</em></b>",
      "Automatic differentiation"
    ]
  },
  {
    "objectID": "ai/pt/pt_autograd.html#backpropagation",
    "href": "ai/pt/pt_autograd.html#backpropagation",
    "title": "Automatic differentiation",
    "section": "Backpropagation",
    "text": "Backpropagation\nFirst, we need to talk about backpropagation: the backward pass following each forward pass and which adjusts the model’s parameters to minimize the output of the loss function.\nThe last 2 videos of 3Blue1Brown neural network series explains backpropagation and its manual calculation very well.\n\nWhat is backpropagation? (14 min)\n\n\nThere is one minor terminological error in this video: they call the use of mini-batches stochastic gradient descent. In fact, this is called mini-batch gradient descent. Stochastic gradient descent uses a single example at each iteration.\n\n\n\nHow does backpropagation work? (10 min)",
    "crumbs": [
      "AI",
      "<b><em>Intro DL with PyTorch</em></b>",
      "Automatic differentiation"
    ]
  },
  {
    "objectID": "ai/pt/pt_autograd.html#automatic-differentiation",
    "href": "ai/pt/pt_autograd.html#automatic-differentiation",
    "title": "Automatic differentiation",
    "section": "Automatic differentiation",
    "text": "Automatic differentiation\nIf we had to do all this manually, it would be absolute hell. Thankfully, many tools—including PyTorch—can do this automatically.\n\nTracking computations\nFor the automation of the calculation of all those derivatives through chain rules, PyTorch needs to track computations during the forward pass.\nPyTorch does not however track all the computations on all the tensors (this would be extremely memory intensive!). To start tracking computations on a vector, set the requires_grad attribute to True:\n\nimport torch\n\nx = torch.ones(2, 4, requires_grad=True)\nx\n\ntensor([[1., 1., 1., 1.],\n        [1., 1., 1., 1.]], requires_grad=True)\n\n\n\nThe grad_fun attribute\nWhenever a tensor is created by an operation involving a tracked tensor, it has a grad_fun attribute:\n\ny = x + 1\ny\n\ntensor([[2., 2., 2., 2.],\n        [2., 2., 2., 2.]], grad_fn=&lt;AddBackward0&gt;)\n\n\n\ny.grad_fn\n\n&lt;AddBackward0 at 0x738632169690&gt;\n\n\n\n\nJudicious tracking\nYou don’t want to track more than is necessary. There are multiple ways to avoid tracking what you don’t want.\nYou can stop tracking computations on a tensor with the method detach:\n\nx\n\ntensor([[1., 1., 1., 1.],\n        [1., 1., 1., 1.]], requires_grad=True)\n\n\n\nx.detach_()\n\ntensor([[1., 1., 1., 1.],\n        [1., 1., 1., 1.]])\n\n\nYou can change its requires_grad flag:\n\nx = torch.zeros(2, 3, requires_grad=True)\nx\n\ntensor([[0., 0., 0.],\n        [0., 0., 0.]], requires_grad=True)\n\n\n\nx.requires_grad_(False)\n\ntensor([[0., 0., 0.],\n        [0., 0., 0.]])\n\n\nAlternatively, you can wrap any code you don’t want to track under with torch.no_grad():\n\nx = torch.ones(2, 4, requires_grad=True)\n\nwith torch.no_grad():\n    y = x + 1\n\ny\n\ntensor([[2., 2., 2., 2.],\n        [2., 2., 2., 2.]])\n\n\n\nCompare this with what we just did above.\n\n\n\n\nCalculating gradients\nLet’s calculate gradients manually, then use autograd, in a very simple case: imagine that \\(x\\), \\(y\\), and \\(z\\) are tensors containing the parameters of a model and that the error \\(e\\) could be calculated with the equation:\n\\[e=2x^4-y^3+3z^2\\]\n\nManual derivative calculation\nLet’s see how we would do this manually.\nFirst, we need the model parameters tensors:\n\nx = torch.tensor([1., 2.])\ny = torch.tensor([3., 4.])\nz = torch.tensor([5., 6.])\n\nWe calculate \\(e\\) following the above equation:\n\ne = 2*x**4 - y**3 + 3*z**2\n\nThe gradients of the error \\(e\\) w.r.t. the parameters \\(x\\), \\(y\\), and \\(z\\) are:\n\\[\\frac{de}{dx}=8x^3\\] \\[\\frac{de}{dy}=-3y^2\\] \\[\\frac{de}{dz}=6z\\]\nWe can calculate them with:\n\ngradient_x = 8*x**3\ngradient_x\n\ntensor([ 8., 64.])\n\n\n\ngradient_y = -3*y**2\ngradient_y\n\ntensor([-27., -48.])\n\n\n\ngradient_z = 6*z\ngradient_z\n\ntensor([30., 36.])\n\n\n\n\nAutomatic derivative calculation\nFor this method, we need to define our model parameters with requires_grad set to True:\n\nx = torch.tensor([1., 2.], requires_grad=True)\ny = torch.tensor([3., 4.], requires_grad=True)\nz = torch.tensor([5., 6.], requires_grad=True)\n\n\\(e\\) is calculated in the same fashion (except that here, all the computations on \\(x\\), \\(y\\), and \\(z\\) are tracked):\n\ne = 2*x**4 - y**3 + 3*z**2\n\nThe backward propagation is done automatically with:\n\ne.backward(torch.tensor([1., 1.]))\n\nAnd we have our 3 partial derivatives:\n\nprint(x.grad)\nprint(y.grad)\nprint(z.grad)\n\ntensor([ 8., 64.])\ntensor([-27., -48.])\ntensor([30., 36.])\n\n\n\n\nComparison\nThe result is the same, as can be tested with:\n\n8*x**3 == x.grad\n\ntensor([True, True])\n\n\n\n-3*y**2 == y.grad\n\ntensor([True, True])\n\n\n\n6*z == z.grad\n\ntensor([True, True])\n\n\nOf course, calculating the gradients manually here was extremely easy, but imagine how tedious and lengthy it would be to write the chain rules to calculate the gradients of all the composite functions in a neural network manually…",
    "crumbs": [
      "AI",
      "<b><em>Intro DL with PyTorch</em></b>",
      "Automatic differentiation"
    ]
  },
  {
    "objectID": "ai/pt/pt_choosing_frameworks.html",
    "href": "ai/pt/pt_choosing_frameworks.html",
    "title": "Which framework to choose?",
    "section": "",
    "text": "With the growing popularity of machine learning, many frameworks have appeared in various languages. One of the questions you might be facing is: which tool should I choose?\nThe main focus here is on the downsides of proprietary tools.",
    "crumbs": [
      "AI",
      "<b><em>Intro DL with PyTorch</em></b>",
      "Which framework to choose?"
    ]
  },
  {
    "objectID": "ai/pt/pt_choosing_frameworks.html#points-worth-considering",
    "href": "ai/pt/pt_choosing_frameworks.html#points-worth-considering",
    "title": "Which framework to choose?",
    "section": "Points worth considering",
    "text": "Points worth considering\nThere are a several points you might want to consider in making that choice. For instance, what tools do people use in your field? (what tools are used in the papers you read?) What tools are your colleagues and collaborators using?\nLooking a bit further into the future, whether you are considering staying in academia or working in industry could also influence your choice. If the former, paying attention to the literature is key, if the latter, it may be good to have a look at the trends in job postings.\nSome frameworks offer collections of already-made toolkits. They are thus easy to start using and do not require a lot of programming experience. On the other hand, they may feel like black boxes and they are not very customizable. Scikit-learn and Keras—usually run on top of TensorFlow—fall in that category. Lower level tools allow you full control and tuning of your models, but can come with a steeper learning curve.\nPyTorch, developed by Facebook’s AI Research lab, has seen a huge increase in popularity in research in recent years due to its highly pythonic syntax, very convenient tensors, just-in-time (JIT) compilation, dynamic computation graphs, and because it is free and open-source.\nSeveral libraries are now adding a higher level on top of PyTorch: fastai, which we will use in this course, PyTorch Lightning, and PyTorch Ignite. fastai, in addition to the convenience of being able to write a model in a few lines of code, allows to dive as low as you choose into the PyTorch code, thus making it unconstrained by the optional ease of use. It also adds countless functionality. The downside of using this added layer is that it can make it less straightforward to install on a machine or to tweak and customize.\nThe most popular machine learning library currently remains TensorFlow, developed by the Google Brain Team. While it has a Python API, its syntax can be more obscure.\nJulia’s syntax is well suited for the implementation of mathematical models, GPU kernels can be written directly in Julia, and Julia’s speed is attractive in computation hungry fields. So Julia has also seen the development of many ML packages such as Flux or Knet. The user base of Julia remains quite small however.\nMy main motivation in writing this section however is to raise awareness about one question that should really be considered: whether the tool you decide to learn and use in your research is open-source or proprietary.",
    "crumbs": [
      "AI",
      "<b><em>Intro DL with PyTorch</em></b>",
      "Which framework to choose?"
    ]
  },
  {
    "objectID": "ai/pt/pt_choosing_frameworks.html#proprietary-tools-a-word-of-caution",
    "href": "ai/pt/pt_choosing_frameworks.html#proprietary-tools-a-word-of-caution",
    "title": "Which framework to choose?",
    "section": "Proprietary tools: a word of caution",
    "text": "Proprietary tools: a word of caution\nAs a student, it is tempting to have the following perspective:\n\nMy university pays for this very expensive license. I have free access to this very expensive tool. It would be foolish not to make use of it while I can!\n\nWhen there are no equivalent or better open-source tools, that might be true. But when superior open-source tools exist, these university licenses are more of a trap than a gift.\nHere are some of the reasons you should be wary of proprietary tools:\n\nResearchers who do not have access to the tool cannot reproduce your methods\nLarge Canadian universities may offer a license for the tool, but grad students in other countries, independent researchers, researchers in small organizations, etc. may not have access to a free license (or tens of thousands of dollars to pay for it).\n\n\nOnce you graduate, you may not have access to the tool anymore\nOnce you leave your Canadian institution, you may become one of those researchers who do not have access to that tool. This means that you will not be able to re-run your thesis analyses, re-use your methods, and apply the skills you learnt. The time you spent learning that expensive tool you could play with for free may feel a lot less like a gift then.\n\n\nYour university may stop paying for a license\nAs commercial tools fall behind successful open-source ones, some universities may decide to stop purchasing a license. It happened during my years at SFU with an expensive and clunky citation manager which, after having been promoted for years by the library through countless workshops, got abandoned in favour of a much better free and open-source one.\n\n\nYou may get locked-in\nProprietary tools often come with proprietary formats and, depending on the tool, it may be painful (or impossible) to convert your work to another format. When that happens, you are locked-in.\n\n\nProprietary tools are often black boxes\nIt is often impossible to see the source code of proprietary software.\n\n\nLong-term access\nIt is often very difficult to have access to old versions of proprietary tools (and this can be necessary to reproduce old studies). When companies disappear, the tools they produced usually disappear with them. open-source tools, particularly those who have their code under version control in repositories such as GitHub, remain fully accessible (including all stages of development), and if they get abandoned, their maintenance can be taken over or restarted by others.\n\n\nThe licenses you have access to may be limiting and a cause of headache\nFor instance, the Alliance does not have an unlimited number of MATLAB licenses. Since these licenses come with complex rules (one license needed for each node, additional licenses for each toolbox, additional licenses for newer tools, etc.), it can quickly become a nightmare to navigate through it all. You may want to have a look at some of the comments in this thread.\n\n\nProprietary tools fall behind popular open-source tools\nEven large teams of software engineers cannot compete against an active community of researchers developing open-source tools. When open-source tools become really popular, the number of users contributing to their development vastly outnumbers what any company can provide. The testing, licensing, and production of proprietary tools are also too slow to keep up with quickly evolving fields of research. (Of course, open-source tools which do not take off and remain absolutely obscure do not see the benefit of a vast community.)\n\n\nProprietary tools often fail to address specialized edge cases needed in research\nIt is not commercially sound to develop cutting edge capabilities so specialized in a narrow subfield that they can only target a minuscule number of customers. But this is often what research needs. With open-source tools, researchers can develop the capabilities that fit their very specific needs. So while commercial tools are good and reliable for large audiences, they are often not the best in research. This explains the success of R over tools such as SASS or Stata in the past decade.",
    "crumbs": [
      "AI",
      "<b><em>Intro DL with PyTorch</em></b>",
      "Which framework to choose?"
    ]
  },
  {
    "objectID": "ai/pt/pt_choosing_frameworks.html#conclusion",
    "href": "ai/pt/pt_choosing_frameworks.html#conclusion",
    "title": "Which framework to choose?",
    "section": "Conclusion",
    "text": "Conclusion\nAll that said, sometimes you don’t have a choice over the tool to use for your research as this may be dictated by the culture in your field or by your supervisor. But if you are free to choose and if superior or equal open-source alternatives exist and are popular, do not fall in the trap of thinking that because your university and the Alliance pay for a license, you should make use of it. It may be free for you—for now—but it can have hidden costs.",
    "crumbs": [
      "AI",
      "<b><em>Intro DL with PyTorch</em></b>",
      "Which framework to choose?"
    ]
  },
  {
    "objectID": "ai/pt/pt_data.html",
    "href": "ai/pt/pt_data.html",
    "title": "Loading data",
    "section": "",
    "text": "import torch\nimport torchvision\nimport torchvision.transforms as transforms\ntransform = transforms.Compose(\n    [transforms.ToTensor(),\n     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n\nbatch_size = 4\n\ntrainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n                                        download=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n                                          shuffle=True, num_workers=2)\n\ntestset = torchvision.datasets.CIFAR10(root='./data', train=False,\n                                       download=True, transform=transform)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n                                         shuffle=False, num_workers=2)\n\nclasses = ('plane', 'car', 'bird', 'cat',\n           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n\n0.0%0.0%0.1%0.1%0.1%0.1%0.1%0.2%0.2%0.2%0.2%0.2%0.2%0.3%0.3%0.3%0.3%0.3%0.4%0.4%0.4%0.4%0.4%0.5%0.5%0.5%0.5%0.5%0.6%0.6%0.6%0.6%0.6%0.7%0.7%0.7%0.7%0.7%0.7%0.8%0.8%0.8%0.8%0.8%0.9%0.9%0.9%0.9%0.9%1.0%1.0%1.0%1.0%1.0%1.1%1.1%1.1%1.1%1.1%1.2%1.2%1.2%1.2%1.2%1.2%1.3%1.3%1.3%1.3%1.3%1.4%1.4%1.4%1.4%1.4%1.5%1.5%1.5%1.5%1.5%1.6%1.6%1.6%1.6%1.6%1.7%1.7%1.7%1.7%1.7%1.7%1.8%1.8%1.8%1.8%1.8%1.9%1.9%1.9%1.9%1.9%2.0%2.0%2.0%2.0%2.0%2.1%2.1%2.1%2.1%2.1%2.2%2.2%2.2%2.2%2.2%2.2%2.3%2.3%2.3%2.3%2.3%2.4%2.4%2.4%2.4%2.4%2.5%2.5%2.5%2.5%2.5%2.6%2.6%2.6%2.6%2.6%2.7%2.7%2.7%2.7%2.7%2.7%2.8%2.8%2.8%2.8%2.8%2.9%2.9%2.9%2.9%2.9%3.0%3.0%3.0%3.0%3.0%3.1%3.1%3.1%3.1%3.1%3.2%3.2%3.2%3.2%3.2%3.2%3.3%3.3%3.3%3.3%3.3%3.4%3.4%3.4%3.4%3.4%3.5%3.5%3.5%3.5%3.5%3.6%3.6%3.6%3.6%3.6%3.7%3.7%3.7%3.7%3.7%3.7%3.8%3.8%3.8%3.8%3.8%3.9%3.9%3.9%3.9%3.9%4.0%4.0%4.0%4.0%4.0%4.1%4.1%4.1%4.1%4.1%4.2%4.2%4.2%4.2%4.2%4.2%4.3%4.3%4.3%4.3%4.3%4.4%4.4%4.4%4.4%4.4%4.5%4.5%4.5%4.5%4.5%4.6%4.6%4.6%4.6%4.6%4.7%4.7%4.7%4.7%4.7%4.7%4.8%4.8%4.8%4.8%4.8%4.9%4.9%4.9%4.9%4.9%5.0%5.0%5.0%5.0%5.0%5.1%5.1%5.1%5.1%5.1%5.2%5.2%5.2%5.2%5.2%5.2%5.3%5.3%5.3%5.3%5.3%5.4%5.4%5.4%5.4%5.4%5.5%5.5%5.5%5.5%5.5%5.6%5.6%5.6%5.6%5.6%5.7%5.7%5.7%5.7%5.7%5.7%5.8%5.8%5.8%5.8%5.8%5.9%5.9%5.9%5.9%5.9%6.0%6.0%6.0%6.0%6.0%6.1%6.1%6.1%6.1%6.1%6.2%6.2%6.2%6.2%6.2%6.2%6.3%6.3%6.3%6.3%6.3%6.4%6.4%6.4%6.4%6.4%6.5%6.5%6.5%6.5%6.5%6.6%6.6%6.6%6.6%6.6%6.6%6.7%6.7%6.7%6.7%6.7%6.8%6.8%6.8%6.8%6.8%6.9%6.9%6.9%6.9%6.9%7.0%7.0%7.0%7.0%7.0%7.1%7.1%7.1%7.1%7.1%7.1%7.2%7.2%7.2%7.2%7.2%7.3%7.3%7.3%7.3%7.3%7.4%7.4%7.4%7.4%7.4%7.5%7.5%7.5%7.5%7.5%7.6%7.6%7.6%7.6%7.6%7.6%7.7%7.7%7.7%7.7%7.7%7.8%7.8%7.8%7.8%7.8%7.9%7.9%7.9%7.9%7.9%8.0%8.0%8.0%8.0%8.0%8.1%8.1%8.1%8.1%8.1%8.1%8.2%8.2%8.2%8.2%8.2%8.3%8.3%8.3%8.3%8.3%8.4%8.4%8.4%8.4%8.4%8.5%8.5%8.5%8.5%8.5%8.6%8.6%8.6%8.6%8.6%8.6%8.7%8.7%8.7%8.7%8.7%8.8%8.8%8.8%8.8%8.8%8.9%8.9%8.9%8.9%8.9%9.0%9.0%9.0%9.0%9.0%9.1%9.1%9.1%9.1%9.1%9.1%9.2%9.2%9.2%9.2%9.2%9.3%9.3%9.3%9.3%9.3%9.4%9.4%9.4%9.4%9.4%9.5%9.5%9.5%9.5%9.5%9.6%9.6%9.6%9.6%9.6%9.6%9.7%9.7%9.7%9.7%9.7%9.8%9.8%9.8%9.8%9.8%9.9%9.9%9.9%9.9%9.9%10.0%10.0%10.0%10.0%10.0%10.1%10.1%10.1%10.1%10.1%10.1%10.2%10.2%10.2%10.2%10.2%10.3%10.3%10.3%10.3%10.3%10.4%10.4%10.4%10.4%10.4%10.5%10.5%10.5%10.5%10.5%10.6%10.6%10.6%10.6%10.6%10.6%10.7%10.7%10.7%10.7%10.7%10.8%10.8%10.8%10.8%10.8%10.9%10.9%10.9%10.9%10.9%11.0%11.0%11.0%11.0%11.0%11.1%11.1%11.1%11.1%11.1%11.1%11.2%11.2%11.2%11.2%11.2%11.3%11.3%11.3%11.3%11.3%11.4%11.4%11.4%11.4%11.4%11.5%11.5%11.5%11.5%11.5%11.6%11.6%11.6%11.6%11.6%11.6%11.7%11.7%11.7%11.7%11.7%11.8%11.8%11.8%11.8%11.8%11.9%11.9%11.9%11.9%11.9%12.0%12.0%12.0%12.0%12.0%12.1%12.1%12.1%12.1%12.1%12.1%12.2%12.2%12.2%12.2%12.2%12.3%12.3%12.3%12.3%12.3%12.4%12.4%12.4%12.4%12.4%12.5%12.5%12.5%12.5%12.5%12.5%12.6%12.6%12.6%12.6%12.6%12.7%12.7%12.7%12.7%12.7%12.8%12.8%12.8%12.8%12.8%12.9%12.9%12.9%12.9%12.9%13.0%13.0%13.0%13.0%13.0%13.0%13.1%13.1%13.1%13.1%13.1%13.2%13.2%13.2%13.2%13.2%13.3%13.3%13.3%13.3%13.3%13.4%13.4%13.4%13.4%13.4%13.5%13.5%13.5%13.5%13.5%13.5%13.6%13.6%13.6%13.6%13.6%13.7%13.7%13.7%13.7%13.7%13.8%13.8%13.8%13.8%13.8%13.9%13.9%13.9%13.9%13.9%14.0%14.0%14.0%14.0%14.0%14.0%14.1%14.1%14.1%14.1%14.1%14.2%14.2%14.2%14.2%14.2%14.3%14.3%14.3%14.3%14.3%14.4%14.4%14.4%14.4%14.4%14.5%14.5%14.5%14.5%14.5%14.5%14.6%14.6%14.6%14.6%14.6%14.7%14.7%14.7%14.7%14.7%14.8%14.8%14.8%14.8%14.8%14.9%14.9%14.9%14.9%14.9%15.0%15.0%15.0%15.0%15.0%15.0%15.1%15.1%15.1%15.1%15.1%15.2%15.2%15.2%15.2%15.2%15.3%15.3%15.3%15.3%15.3%15.4%15.4%15.4%15.4%15.4%15.5%15.5%15.5%15.5%15.5%15.5%15.6%15.6%15.6%15.6%15.6%15.7%15.7%15.7%15.7%15.7%15.8%15.8%15.8%15.8%15.8%15.9%15.9%15.9%15.9%15.9%16.0%16.0%16.0%16.0%16.0%16.0%16.1%16.1%16.1%16.1%16.1%16.2%16.2%16.2%16.2%16.2%16.3%16.3%16.3%16.3%16.3%16.4%16.4%16.4%16.4%16.4%16.5%16.5%16.5%16.5%16.5%16.5%16.6%16.6%16.6%16.6%16.6%16.7%16.7%16.7%16.7%16.7%16.8%16.8%16.8%16.8%16.8%16.9%16.9%16.9%16.9%16.9%17.0%17.0%17.0%17.0%17.0%17.0%17.1%17.1%17.1%17.1%17.1%17.2%17.2%17.2%17.2%17.2%17.3%17.3%17.3%17.3%17.3%17.4%17.4%17.4%17.4%17.4%17.5%17.5%17.5%17.5%17.5%17.5%17.6%17.6%17.6%17.6%17.6%17.7%17.7%17.7%17.7%17.7%17.8%17.8%17.8%17.8%17.8%17.9%17.9%17.9%17.9%17.9%18.0%18.0%18.0%18.0%18.0%18.0%18.1%18.1%18.1%18.1%18.1%18.2%18.2%18.2%18.2%18.2%18.3%18.3%18.3%18.3%18.3%18.4%18.4%18.4%18.4%18.4%18.5%18.5%18.5%18.5%18.5%18.5%18.6%18.6%18.6%18.6%18.6%18.7%18.7%18.7%18.7%18.7%18.8%18.8%18.8%18.8%18.8%18.9%18.9%18.9%18.9%18.9%18.9%19.0%19.0%19.0%19.0%19.0%19.1%19.1%19.1%19.1%19.1%19.2%19.2%19.2%19.2%19.2%19.3%19.3%19.3%19.3%19.3%19.4%19.4%19.4%19.4%19.4%19.4%19.5%19.5%19.5%19.5%19.5%19.6%19.6%19.6%19.6%19.6%19.7%19.7%19.7%19.7%19.7%19.8%19.8%19.8%19.8%19.8%19.9%19.9%19.9%19.9%19.9%19.9%20.0%20.0%20.0%20.0%20.0%20.1%20.1%20.1%20.1%20.1%20.2%20.2%20.2%20.2%20.2%20.3%20.3%20.3%20.3%20.3%20.4%20.4%20.4%20.4%20.4%20.4%20.5%20.5%20.5%20.5%20.5%20.6%20.6%20.6%20.6%20.6%20.7%20.7%20.7%20.7%20.7%20.8%20.8%20.8%20.8%20.8%20.9%20.9%20.9%20.9%20.9%20.9%21.0%21.0%21.0%21.0%21.0%21.1%21.1%21.1%21.1%21.1%21.2%21.2%21.2%21.2%21.2%21.3%21.3%21.3%21.3%21.3%21.4%21.4%21.4%21.4%21.4%21.4%21.5%21.5%21.5%21.5%21.5%21.6%21.6%21.6%21.6%21.6%21.7%21.7%21.7%21.7%21.7%21.8%21.8%21.8%21.8%21.8%21.9%21.9%21.9%21.9%21.9%21.9%22.0%22.0%22.0%22.0%22.0%22.1%22.1%22.1%22.1%22.1%22.2%22.2%22.2%22.2%22.2%22.3%22.3%22.3%22.3%22.3%22.4%22.4%22.4%22.4%22.4%22.4%22.5%22.5%22.5%22.5%22.5%22.6%22.6%22.6%22.6%22.6%22.7%22.7%22.7%22.7%22.7%22.8%22.8%22.8%22.8%22.8%22.9%22.9%22.9%22.9%22.9%22.9%23.0%23.0%23.0%23.0%23.0%23.1%23.1%23.1%23.1%23.1%23.2%23.2%23.2%23.2%23.2%23.3%23.3%23.3%23.3%23.3%23.4%23.4%23.4%23.4%23.4%23.4%23.5%23.5%23.5%23.5%23.5%23.6%23.6%23.6%23.6%23.6%23.7%23.7%23.7%23.7%23.7%23.8%23.8%23.8%23.8%23.8%23.9%23.9%23.9%23.9%23.9%23.9%24.0%24.0%24.0%24.0%24.0%24.1%24.1%24.1%24.1%24.1%24.2%24.2%24.2%24.2%24.2%24.3%24.3%24.3%24.3%24.3%24.4%24.4%24.4%24.4%24.4%24.4%24.5%24.5%24.5%24.5%24.5%24.6%24.6%24.6%24.6%24.6%24.7%24.7%24.7%24.7%24.7%24.8%24.8%24.8%24.8%24.8%24.9%24.9%24.9%24.9%24.9%24.9%25.0%25.0%25.0%25.0%25.0%25.1%25.1%25.1%25.1%25.1%25.2%25.2%25.2%25.2%25.2%25.3%25.3%25.3%25.3%25.3%25.3%25.4%25.4%25.4%25.4%25.4%25.5%25.5%25.5%25.5%25.5%25.6%25.6%25.6%25.6%25.6%25.7%25.7%25.7%25.7%25.7%25.8%25.8%25.8%25.8%25.8%25.8%25.9%25.9%25.9%25.9%25.9%26.0%26.0%26.0%26.0%26.0%26.1%26.1%26.1%26.1%26.1%26.2%26.2%26.2%26.2%26.2%26.3%26.3%26.3%26.3%26.3%26.3%26.4%26.4%26.4%26.4%26.4%26.5%26.5%26.5%26.5%26.5%26.6%26.6%26.6%26.6%26.6%26.7%26.7%26.7%26.7%26.7%26.8%26.8%26.8%26.8%26.8%26.8%26.9%26.9%26.9%26.9%26.9%27.0%27.0%27.0%27.0%27.0%27.1%27.1%27.1%27.1%27.1%27.2%27.2%27.2%27.2%27.2%27.3%27.3%27.3%27.3%27.3%27.3%27.4%27.4%27.4%27.4%27.4%27.5%27.5%27.5%27.5%27.5%27.6%27.6%27.6%27.6%27.6%27.7%27.7%27.7%27.7%27.7%27.8%27.8%27.8%27.8%27.8%27.8%27.9%27.9%27.9%27.9%27.9%28.0%28.0%28.0%28.0%28.0%28.1%28.1%28.1%28.1%28.1%28.2%28.2%28.2%28.2%28.2%28.3%28.3%28.3%28.3%28.3%28.3%28.4%28.4%28.4%28.4%28.4%28.5%28.5%28.5%28.5%28.5%28.6%28.6%28.6%28.6%28.6%28.7%28.7%28.7%28.7%28.7%28.8%28.8%28.8%28.8%28.8%28.8%28.9%28.9%28.9%28.9%28.9%29.0%29.0%29.0%29.0%29.0%29.1%29.1%29.1%29.1%29.1%29.2%29.2%29.2%29.2%29.2%29.3%29.3%29.3%29.3%29.3%29.3%29.4%29.4%29.4%29.4%29.4%29.5%29.5%29.5%29.5%29.5%29.6%29.6%29.6%29.6%29.6%29.7%29.7%29.7%29.7%29.7%29.8%29.8%29.8%29.8%29.8%29.8%29.9%29.9%29.9%29.9%29.9%30.0%30.0%30.0%30.0%30.0%30.1%30.1%30.1%30.1%30.1%30.2%30.2%30.2%30.2%30.2%30.3%30.3%30.3%30.3%30.3%30.3%30.4%30.4%30.4%30.4%30.4%30.5%30.5%30.5%30.5%30.5%30.6%30.6%30.6%30.6%30.6%30.7%30.7%30.7%30.7%30.7%30.8%30.8%30.8%30.8%30.8%30.8%30.9%30.9%30.9%30.9%30.9%31.0%31.0%31.0%31.0%31.0%31.1%31.1%31.1%31.1%31.1%31.2%31.2%31.2%31.2%31.2%31.3%31.3%31.3%31.3%31.3%31.3%31.4%31.4%31.4%31.4%31.4%31.5%31.5%31.5%31.5%31.5%31.6%31.6%31.6%31.6%31.6%31.7%31.7%31.7%31.7%31.7%31.7%31.8%31.8%31.8%31.8%31.8%31.9%31.9%31.9%31.9%31.9%32.0%32.0%32.0%32.0%32.0%32.1%32.1%32.1%32.1%32.1%32.2%32.2%32.2%32.2%32.2%32.2%32.3%32.3%32.3%32.3%32.3%32.4%32.4%32.4%32.4%32.4%32.5%32.5%32.5%32.5%32.5%32.6%32.6%32.6%32.6%32.6%32.7%32.7%32.7%32.7%32.7%32.7%32.8%32.8%32.8%32.8%32.8%32.9%32.9%32.9%32.9%32.9%33.0%33.0%33.0%33.0%33.0%33.1%33.1%33.1%33.1%33.1%33.2%33.2%33.2%33.2%33.2%33.2%33.3%33.3%33.3%33.3%33.3%33.4%33.4%33.4%33.4%33.4%33.5%33.5%33.5%33.5%33.5%33.6%33.6%33.6%33.6%33.6%33.7%33.7%33.7%33.7%33.7%33.7%33.8%33.8%33.8%33.8%33.8%33.9%33.9%33.9%33.9%33.9%34.0%34.0%34.0%34.0%34.0%34.1%34.1%34.1%34.1%34.1%34.2%34.2%34.2%34.2%34.2%34.2%34.3%34.3%34.3%34.3%34.3%34.4%34.4%34.4%34.4%34.4%34.5%34.5%34.5%34.5%34.5%34.6%34.6%34.6%34.6%34.6%34.7%34.7%34.7%34.7%34.7%34.7%34.8%34.8%34.8%34.8%34.8%34.9%34.9%34.9%34.9%34.9%35.0%35.0%35.0%35.0%35.0%35.1%35.1%35.1%35.1%35.1%35.2%35.2%35.2%35.2%35.2%35.2%35.3%35.3%35.3%35.3%35.3%35.4%35.4%35.4%35.4%35.4%35.5%35.5%35.5%35.5%35.5%35.6%35.6%35.6%35.6%35.6%35.7%35.7%35.7%35.7%35.7%35.7%35.8%35.8%35.8%35.8%35.8%35.9%35.9%35.9%35.9%35.9%36.0%36.0%36.0%36.0%36.0%36.1%36.1%36.1%36.1%36.1%36.2%36.2%36.2%36.2%36.2%36.2%36.3%36.3%36.3%36.3%36.3%36.4%36.4%36.4%36.4%36.4%36.5%36.5%36.5%36.5%36.5%36.6%36.6%36.6%36.6%36.6%36.7%36.7%36.7%36.7%36.7%36.7%36.8%36.8%36.8%36.8%36.8%36.9%36.9%36.9%36.9%36.9%37.0%37.0%37.0%37.0%37.0%37.1%37.1%37.1%37.1%37.1%37.2%37.2%37.2%37.2%37.2%37.2%37.3%37.3%37.3%37.3%37.3%37.4%37.4%37.4%37.4%37.4%37.5%37.5%37.5%37.5%37.5%37.6%37.6%37.6%37.6%37.6%37.6%37.7%37.7%37.7%37.7%37.7%37.8%37.8%37.8%37.8%37.8%37.9%37.9%37.9%37.9%37.9%38.0%38.0%38.0%38.0%38.0%38.1%38.1%38.1%38.1%38.1%38.1%38.2%38.2%38.2%38.2%38.2%38.3%38.3%38.3%38.3%38.3%38.4%38.4%38.4%38.4%38.4%38.5%38.5%38.5%38.5%38.5%38.6%38.6%38.6%38.6%38.6%38.6%38.7%38.7%38.7%38.7%38.7%38.8%38.8%38.8%38.8%38.8%38.9%38.9%38.9%38.9%38.9%39.0%39.0%39.0%39.0%39.0%39.1%39.1%39.1%39.1%39.1%39.1%39.2%39.2%39.2%39.2%39.2%39.3%39.3%39.3%39.3%39.3%39.4%39.4%39.4%39.4%39.4%39.5%39.5%39.5%39.5%39.5%39.6%39.6%39.6%39.6%39.6%39.6%39.7%39.7%39.7%39.7%39.7%39.8%39.8%39.8%39.8%39.8%39.9%39.9%39.9%39.9%39.9%40.0%40.0%40.0%40.0%40.0%40.1%40.1%40.1%40.1%40.1%40.1%40.2%40.2%40.2%40.2%40.2%40.3%40.3%40.3%40.3%40.3%40.4%40.4%40.4%40.4%40.4%40.5%40.5%40.5%40.5%40.5%40.6%40.6%40.6%40.6%40.6%40.6%40.7%40.7%40.7%40.7%40.7%40.8%40.8%40.8%40.8%40.8%40.9%40.9%40.9%40.9%40.9%41.0%41.0%41.0%41.0%41.0%41.1%41.1%41.1%41.1%41.1%41.1%41.2%41.2%41.2%41.2%41.2%41.3%41.3%41.3%41.3%41.3%41.4%41.4%41.4%41.4%41.4%41.5%41.5%41.5%41.5%41.5%41.6%41.6%41.6%41.6%41.6%41.6%41.7%41.7%41.7%41.7%41.7%41.8%41.8%41.8%41.8%41.8%41.9%41.9%41.9%41.9%41.9%42.0%42.0%42.0%42.0%42.0%42.1%42.1%42.1%42.1%42.1%42.1%42.2%42.2%42.2%42.2%42.2%42.3%42.3%42.3%42.3%42.3%42.4%42.4%42.4%42.4%42.4%42.5%42.5%42.5%42.5%42.5%42.6%42.6%42.6%42.6%42.6%42.6%42.7%42.7%42.7%42.7%42.7%42.8%42.8%42.8%42.8%42.8%42.9%42.9%42.9%42.9%42.9%43.0%43.0%43.0%43.0%43.0%43.1%43.1%43.1%43.1%43.1%43.1%43.2%43.2%43.2%43.2%43.2%43.3%43.3%43.3%43.3%43.3%43.4%43.4%43.4%43.4%43.4%43.5%43.5%43.5%43.5%43.5%43.6%43.6%43.6%43.6%43.6%43.6%43.7%43.7%43.7%43.7%43.7%43.8%43.8%43.8%43.8%43.8%43.9%43.9%43.9%43.9%43.9%44.0%44.0%44.0%44.0%44.0%44.0%44.1%44.1%44.1%44.1%44.1%44.2%44.2%44.2%44.2%44.2%44.3%44.3%44.3%44.3%44.3%44.4%44.4%44.4%44.4%44.4%44.5%44.5%44.5%44.5%44.5%44.5%44.6%44.6%44.6%44.6%44.6%44.7%44.7%44.7%44.7%44.7%44.8%44.8%44.8%44.8%44.8%44.9%44.9%44.9%44.9%44.9%45.0%45.0%45.0%45.0%45.0%45.0%45.1%45.1%45.1%45.1%45.1%45.2%45.2%45.2%45.2%45.2%45.3%45.3%45.3%45.3%45.3%45.4%45.4%45.4%45.4%45.4%45.5%45.5%45.5%45.5%45.5%45.5%45.6%45.6%45.6%45.6%45.6%45.7%45.7%45.7%45.7%45.7%45.8%45.8%45.8%45.8%45.8%45.9%45.9%45.9%45.9%45.9%46.0%46.0%46.0%46.0%46.0%46.0%46.1%46.1%46.1%46.1%46.1%46.2%46.2%46.2%46.2%46.2%46.3%46.3%46.3%46.3%46.3%46.4%46.4%46.4%46.4%46.4%46.5%46.5%46.5%46.5%46.5%46.5%46.6%46.6%46.6%46.6%46.6%46.7%46.7%46.7%46.7%46.7%46.8%46.8%46.8%46.8%46.8%46.9%46.9%46.9%46.9%46.9%47.0%47.0%47.0%47.0%47.0%47.0%47.1%47.1%47.1%47.1%47.1%47.2%47.2%47.2%47.2%47.2%47.3%47.3%47.3%47.3%47.3%47.4%47.4%47.4%47.4%47.4%47.5%47.5%47.5%47.5%47.5%47.5%47.6%47.6%47.6%47.6%47.6%47.7%47.7%47.7%47.7%47.7%47.8%47.8%47.8%47.8%47.8%47.9%47.9%47.9%47.9%47.9%48.0%48.0%48.0%48.0%48.0%48.0%48.1%48.1%48.1%48.1%48.1%48.2%48.2%48.2%48.2%48.2%48.3%48.3%48.3%48.3%48.3%48.4%48.4%48.4%48.4%48.4%48.5%48.5%48.5%48.5%48.5%48.5%48.6%48.6%48.6%48.6%48.6%48.7%48.7%48.7%48.7%48.7%48.8%48.8%48.8%48.8%48.8%48.9%48.9%48.9%48.9%48.9%49.0%49.0%49.0%49.0%49.0%49.0%49.1%49.1%49.1%49.1%49.1%49.2%49.2%49.2%49.2%49.2%49.3%49.3%49.3%49.3%49.3%49.4%49.4%49.4%49.4%49.4%49.5%49.5%49.5%49.5%49.5%49.5%49.6%49.6%49.6%49.6%49.6%49.7%49.7%49.7%49.7%49.7%49.8%49.8%49.8%49.8%49.8%49.9%49.9%49.9%49.9%49.9%50.0%50.0%50.0%50.0%50.0%50.0%50.1%50.1%50.1%50.1%50.1%50.2%50.2%50.2%50.2%50.2%50.3%50.3%50.3%50.3%50.3%50.4%50.4%50.4%50.4%50.4%50.4%50.5%50.5%50.5%50.5%50.5%50.6%50.6%50.6%50.6%50.6%50.7%50.7%50.7%50.7%50.7%50.8%50.8%50.8%50.8%50.8%50.9%50.9%50.9%50.9%50.9%50.9%51.0%51.0%51.0%51.0%51.0%51.1%51.1%51.1%51.1%51.1%51.2%51.2%51.2%51.2%51.2%51.3%51.3%51.3%51.3%51.3%51.4%51.4%51.4%51.4%51.4%51.4%51.5%51.5%51.5%51.5%51.5%51.6%51.6%51.6%51.6%51.6%51.7%51.7%51.7%51.7%51.7%51.8%51.8%51.8%51.8%51.8%51.9%51.9%51.9%51.9%51.9%51.9%52.0%52.0%52.0%52.0%52.0%52.1%52.1%52.1%52.1%52.1%52.2%52.2%52.2%52.2%52.2%52.3%52.3%52.3%52.3%52.3%52.4%52.4%52.4%52.4%52.4%52.4%52.5%52.5%52.5%52.5%52.5%52.6%52.6%52.6%52.6%52.6%52.7%52.7%52.7%52.7%52.7%52.8%52.8%52.8%52.8%52.8%52.9%52.9%52.9%52.9%52.9%52.9%53.0%53.0%53.0%53.0%53.0%53.1%53.1%53.1%53.1%53.1%53.2%53.2%53.2%53.2%53.2%53.3%53.3%53.3%53.3%53.3%53.4%53.4%53.4%53.4%53.4%53.4%53.5%53.5%53.5%53.5%53.5%53.6%53.6%53.6%53.6%53.6%53.7%53.7%53.7%53.7%53.7%53.8%53.8%53.8%53.8%53.8%53.9%53.9%53.9%53.9%53.9%53.9%54.0%54.0%54.0%54.0%54.0%54.1%54.1%54.1%54.1%54.1%54.2%54.2%54.2%54.2%54.2%54.3%54.3%54.3%54.3%54.3%54.4%54.4%54.4%54.4%54.4%54.4%54.5%54.5%54.5%54.5%54.5%54.6%54.6%54.6%54.6%54.6%54.7%54.7%54.7%54.7%54.7%54.8%54.8%54.8%54.8%54.8%54.9%54.9%54.9%54.9%54.9%54.9%55.0%55.0%55.0%55.0%55.0%55.1%55.1%55.1%55.1%55.1%55.2%55.2%55.2%55.2%55.2%55.3%55.3%55.3%55.3%55.3%55.4%55.4%55.4%55.4%55.4%55.4%55.5%55.5%55.5%55.5%55.5%55.6%55.6%55.6%55.6%55.6%55.7%55.7%55.7%55.7%55.7%55.8%55.8%55.8%55.8%55.8%55.9%55.9%55.9%55.9%55.9%55.9%56.0%56.0%56.0%56.0%56.0%56.1%56.1%56.1%56.1%56.1%56.2%56.2%56.2%56.2%56.2%56.3%56.3%56.3%56.3%56.3%56.4%56.4%56.4%56.4%56.4%56.4%56.5%56.5%56.5%56.5%56.5%56.6%56.6%56.6%56.6%56.6%56.7%56.7%56.7%56.7%56.7%56.8%56.8%56.8%56.8%56.8%56.8%56.9%56.9%56.9%56.9%56.9%57.0%57.0%57.0%57.0%57.0%57.1%57.1%57.1%57.1%57.1%57.2%57.2%57.2%57.2%57.2%57.3%57.3%57.3%57.3%57.3%57.3%57.4%57.4%57.4%57.4%57.4%57.5%57.5%57.5%57.5%57.5%57.6%57.6%57.6%57.6%57.6%57.7%57.7%57.7%57.7%57.7%57.8%57.8%57.8%57.8%57.8%57.8%57.9%57.9%57.9%57.9%57.9%58.0%58.0%58.0%58.0%58.0%58.1%58.1%58.1%58.1%58.1%58.2%58.2%58.2%58.2%58.2%58.3%58.3%58.3%58.3%58.3%58.3%58.4%58.4%58.4%58.4%58.4%58.5%58.5%58.5%58.5%58.5%58.6%58.6%58.6%58.6%58.6%58.7%58.7%58.7%58.7%58.7%58.8%58.8%58.8%58.8%58.8%58.8%58.9%58.9%58.9%58.9%58.9%59.0%59.0%59.0%59.0%59.0%59.1%59.1%59.1%59.1%59.1%59.2%59.2%59.2%59.2%59.2%59.3%59.3%59.3%59.3%59.3%59.3%59.4%59.4%59.4%59.4%59.4%59.5%59.5%59.5%59.5%59.5%59.6%59.6%59.6%59.6%59.6%59.7%59.7%59.7%59.7%59.7%59.8%59.8%59.8%59.8%59.8%59.8%59.9%59.9%59.9%59.9%59.9%60.0%60.0%60.0%60.0%60.0%60.1%60.1%60.1%60.1%60.1%60.2%60.2%60.2%60.2%60.2%60.3%60.3%60.3%60.3%60.3%60.3%60.4%60.4%60.4%60.4%60.4%60.5%60.5%60.5%60.5%60.5%60.6%60.6%60.6%60.6%60.6%60.7%60.7%60.7%60.7%60.7%60.8%60.8%60.8%60.8%60.8%60.8%60.9%60.9%60.9%60.9%60.9%61.0%61.0%61.0%61.0%61.0%61.1%61.1%61.1%61.1%61.1%61.2%61.2%61.2%61.2%61.2%61.3%61.3%61.3%61.3%61.3%61.3%61.4%61.4%61.4%61.4%61.4%61.5%61.5%61.5%61.5%61.5%61.6%61.6%61.6%61.6%61.6%61.7%61.7%61.7%61.7%61.7%61.8%61.8%61.8%61.8%61.8%61.8%61.9%61.9%61.9%61.9%61.9%62.0%62.0%62.0%62.0%62.0%62.1%62.1%62.1%62.1%62.1%62.2%62.2%62.2%62.2%62.2%62.3%62.3%62.3%62.3%62.3%62.3%62.4%62.4%62.4%62.4%62.4%62.5%62.5%62.5%62.5%62.5%62.6%62.6%62.6%62.6%62.6%62.7%62.7%62.7%62.7%62.7%62.7%62.8%62.8%62.8%62.8%62.8%62.9%62.9%62.9%62.9%62.9%63.0%63.0%63.0%63.0%63.0%63.1%63.1%63.1%63.1%63.1%63.2%63.2%63.2%63.2%63.2%63.2%63.3%63.3%63.3%63.3%63.3%63.4%63.4%63.4%63.4%63.4%63.5%63.5%63.5%63.5%63.5%63.6%63.6%63.6%63.6%63.6%63.7%63.7%63.7%63.7%63.7%63.7%63.8%63.8%63.8%63.8%63.8%63.9%63.9%63.9%63.9%63.9%64.0%64.0%64.0%64.0%64.0%64.1%64.1%64.1%64.1%64.1%64.2%64.2%64.2%64.2%64.2%64.2%64.3%64.3%64.3%64.3%64.3%64.4%64.4%64.4%64.4%64.4%64.5%64.5%64.5%64.5%64.5%64.6%64.6%64.6%64.6%64.6%64.7%64.7%64.7%64.7%64.7%64.7%64.8%64.8%64.8%64.8%64.8%64.9%64.9%64.9%64.9%64.9%65.0%65.0%65.0%65.0%65.0%65.1%65.1%65.1%65.1%65.1%65.2%65.2%65.2%65.2%65.2%65.2%65.3%65.3%65.3%65.3%65.3%65.4%65.4%65.4%65.4%65.4%65.5%65.5%65.5%65.5%65.5%65.6%65.6%65.6%65.6%65.6%65.7%65.7%65.7%65.7%65.7%65.7%65.8%65.8%65.8%65.8%65.8%65.9%65.9%65.9%65.9%65.9%66.0%66.0%66.0%66.0%66.0%66.1%66.1%66.1%66.1%66.1%66.2%66.2%66.2%66.2%66.2%66.2%66.3%66.3%66.3%66.3%66.3%66.4%66.4%66.4%66.4%66.4%66.5%66.5%66.5%66.5%66.5%66.6%66.6%66.6%66.6%66.6%66.7%66.7%66.7%66.7%66.7%66.7%66.8%66.8%66.8%66.8%66.8%66.9%66.9%66.9%66.9%66.9%67.0%67.0%67.0%67.0%67.0%67.1%67.1%67.1%67.1%67.1%67.2%67.2%67.2%67.2%67.2%67.2%67.3%67.3%67.3%67.3%67.3%67.4%67.4%67.4%67.4%67.4%67.5%67.5%67.5%67.5%67.5%67.6%67.6%67.6%67.6%67.6%67.7%67.7%67.7%67.7%67.7%67.7%67.8%67.8%67.8%67.8%67.8%67.9%67.9%67.9%67.9%67.9%68.0%68.0%68.0%68.0%68.0%68.1%68.1%68.1%68.1%68.1%68.2%68.2%68.2%68.2%68.2%68.2%68.3%68.3%68.3%68.3%68.3%68.4%68.4%68.4%68.4%68.4%68.5%68.5%68.5%68.5%68.5%68.6%68.6%68.6%68.6%68.6%68.7%68.7%68.7%68.7%68.7%68.7%68.8%68.8%68.8%68.8%68.8%68.9%68.9%68.9%68.9%68.9%69.0%69.0%69.0%69.0%69.0%69.1%69.1%69.1%69.1%69.1%69.1%69.2%69.2%69.2%69.2%69.2%69.3%69.3%69.3%69.3%69.3%69.4%69.4%69.4%69.4%69.4%69.5%69.5%69.5%69.5%69.5%69.6%69.6%69.6%69.6%69.6%69.6%69.7%69.7%69.7%69.7%69.7%69.8%69.8%69.8%69.8%69.8%69.9%69.9%69.9%69.9%69.9%70.0%70.0%70.0%70.0%70.0%70.1%70.1%70.1%70.1%70.1%70.1%70.2%70.2%70.2%70.2%70.2%70.3%70.3%70.3%70.3%70.3%70.4%70.4%70.4%70.4%70.4%70.5%70.5%70.5%70.5%70.5%70.6%70.6%70.6%70.6%70.6%70.6%70.7%70.7%70.7%70.7%70.7%70.8%70.8%70.8%70.8%70.8%70.9%70.9%70.9%70.9%70.9%71.0%71.0%71.0%71.0%71.0%71.1%71.1%71.1%71.1%71.1%71.1%71.2%71.2%71.2%71.2%71.2%71.3%71.3%71.3%71.3%71.3%71.4%71.4%71.4%71.4%71.4%71.5%71.5%71.5%71.5%71.5%71.6%71.6%71.6%71.6%71.6%71.6%71.7%71.7%71.7%71.7%71.7%71.8%71.8%71.8%71.8%71.8%71.9%71.9%71.9%71.9%71.9%72.0%72.0%72.0%72.0%72.0%72.1%72.1%72.1%72.1%72.1%72.1%72.2%72.2%72.2%72.2%72.2%72.3%72.3%72.3%72.3%72.3%72.4%72.4%72.4%72.4%72.4%72.5%72.5%72.5%72.5%72.5%72.6%72.6%72.6%72.6%72.6%72.6%72.7%72.7%72.7%72.7%72.7%72.8%72.8%72.8%72.8%72.8%72.9%72.9%72.9%72.9%72.9%73.0%73.0%73.0%73.0%73.0%73.1%73.1%73.1%73.1%73.1%73.1%73.2%73.2%73.2%73.2%73.2%73.3%73.3%73.3%73.3%73.3%73.4%73.4%73.4%73.4%73.4%73.5%73.5%73.5%73.5%73.5%73.6%73.6%73.6%73.6%73.6%73.6%73.7%73.7%73.7%73.7%73.7%73.8%73.8%73.8%73.8%73.8%73.9%73.9%73.9%73.9%73.9%74.0%74.0%74.0%74.0%74.0%74.1%74.1%74.1%74.1%74.1%74.1%74.2%74.2%74.2%74.2%74.2%74.3%74.3%74.3%74.3%74.3%74.4%74.4%74.4%74.4%74.4%74.5%74.5%74.5%74.5%74.5%74.6%74.6%74.6%74.6%74.6%74.6%74.7%74.7%74.7%74.7%74.7%74.8%74.8%74.8%74.8%74.8%74.9%74.9%74.9%74.9%74.9%75.0%75.0%75.0%75.0%75.0%75.1%75.1%75.1%75.1%75.1%75.1%75.2%75.2%75.2%75.2%75.2%75.3%75.3%75.3%75.3%75.3%75.4%75.4%75.4%75.4%75.4%75.5%75.5%75.5%75.5%75.5%75.5%75.6%75.6%75.6%75.6%75.6%75.7%75.7%75.7%75.7%75.7%75.8%75.8%75.8%75.8%75.8%75.9%75.9%75.9%75.9%75.9%76.0%76.0%76.0%76.0%76.0%76.0%76.1%76.1%76.1%76.1%76.1%76.2%76.2%76.2%76.2%76.2%76.3%76.3%76.3%76.3%76.3%76.4%76.4%76.4%76.4%76.4%76.5%76.5%76.5%76.5%76.5%76.5%76.6%76.6%76.6%76.6%76.6%76.7%76.7%76.7%76.7%76.7%76.8%76.8%76.8%76.8%76.8%76.9%76.9%76.9%76.9%76.9%77.0%77.0%77.0%77.0%77.0%77.0%77.1%77.1%77.1%77.1%77.1%77.2%77.2%77.2%77.2%77.2%77.3%77.3%77.3%77.3%77.3%77.4%77.4%77.4%77.4%77.4%77.5%77.5%77.5%77.5%77.5%77.5%77.6%77.6%77.6%77.6%77.6%77.7%77.7%77.7%77.7%77.7%77.8%77.8%77.8%77.8%77.8%77.9%77.9%77.9%77.9%77.9%78.0%78.0%78.0%78.0%78.0%78.0%78.1%78.1%78.1%78.1%78.1%78.2%78.2%78.2%78.2%78.2%78.3%78.3%78.3%78.3%78.3%78.4%78.4%78.4%78.4%78.4%78.5%78.5%78.5%78.5%78.5%78.5%78.6%78.6%78.6%78.6%78.6%78.7%78.7%78.7%78.7%78.7%78.8%78.8%78.8%78.8%78.8%78.9%78.9%78.9%78.9%78.9%79.0%79.0%79.0%79.0%79.0%79.0%79.1%79.1%79.1%79.1%79.1%79.2%79.2%79.2%79.2%79.2%79.3%79.3%79.3%79.3%79.3%79.4%79.4%79.4%79.4%79.4%79.5%79.5%79.5%79.5%79.5%79.5%79.6%79.6%79.6%79.6%79.6%79.7%79.7%79.7%79.7%79.7%79.8%79.8%79.8%79.8%79.8%79.9%79.9%79.9%79.9%79.9%80.0%80.0%80.0%80.0%80.0%80.0%80.1%80.1%80.1%80.1%80.1%80.2%80.2%80.2%80.2%80.2%80.3%80.3%80.3%80.3%80.3%80.4%80.4%80.4%80.4%80.4%80.5%80.5%80.5%80.5%80.5%80.5%80.6%80.6%80.6%80.6%80.6%80.7%80.7%80.7%80.7%80.7%80.8%80.8%80.8%80.8%80.8%80.9%80.9%80.9%80.9%80.9%81.0%81.0%81.0%81.0%81.0%81.0%81.1%81.1%81.1%81.1%81.1%81.2%81.2%81.2%81.2%81.2%81.3%81.3%81.3%81.3%81.3%81.4%81.4%81.4%81.4%81.4%81.5%81.5%81.5%81.5%81.5%81.5%81.6%81.6%81.6%81.6%81.6%81.7%81.7%81.7%81.7%81.7%81.8%81.8%81.8%81.8%81.8%81.9%81.9%81.9%81.9%81.9%81.9%82.0%82.0%82.0%82.0%82.0%82.1%82.1%82.1%82.1%82.1%82.2%82.2%82.2%82.2%82.2%82.3%82.3%82.3%82.3%82.3%82.4%82.4%82.4%82.4%82.4%82.4%82.5%82.5%82.5%82.5%82.5%82.6%82.6%82.6%82.6%82.6%82.7%82.7%82.7%82.7%82.7%82.8%82.8%82.8%82.8%82.8%82.9%82.9%82.9%82.9%82.9%82.9%83.0%83.0%83.0%83.0%83.0%83.1%83.1%83.1%83.1%83.1%83.2%83.2%83.2%83.2%83.2%83.3%83.3%83.3%83.3%83.3%83.4%83.4%83.4%83.4%83.4%83.4%83.5%83.5%83.5%83.5%83.5%83.6%83.6%83.6%83.6%83.6%83.7%83.7%83.7%83.7%83.7%83.8%83.8%83.8%83.8%83.8%83.9%83.9%83.9%83.9%83.9%83.9%84.0%84.0%84.0%84.0%84.0%84.1%84.1%84.1%84.1%84.1%84.2%84.2%84.2%84.2%84.2%84.3%84.3%84.3%84.3%84.3%84.4%84.4%84.4%84.4%84.4%84.4%84.5%84.5%84.5%84.5%84.5%84.6%84.6%84.6%84.6%84.6%84.7%84.7%84.7%84.7%84.7%84.8%84.8%84.8%84.8%84.8%84.9%84.9%84.9%84.9%84.9%84.9%85.0%85.0%85.0%85.0%85.0%85.1%85.1%85.1%85.1%85.1%85.2%85.2%85.2%85.2%85.2%85.3%85.3%85.3%85.3%85.3%85.4%85.4%85.4%85.4%85.4%85.4%85.5%85.5%85.5%85.5%85.5%85.6%85.6%85.6%85.6%85.6%85.7%85.7%85.7%85.7%85.7%85.8%85.8%85.8%85.8%85.8%85.9%85.9%85.9%85.9%85.9%85.9%86.0%86.0%86.0%86.0%86.0%86.1%86.1%86.1%86.1%86.1%86.2%86.2%86.2%86.2%86.2%86.3%86.3%86.3%86.3%86.3%86.4%86.4%86.4%86.4%86.4%86.4%86.5%86.5%86.5%86.5%86.5%86.6%86.6%86.6%86.6%86.6%86.7%86.7%86.7%86.7%86.7%86.8%86.8%86.8%86.8%86.8%86.9%86.9%86.9%86.9%86.9%86.9%87.0%87.0%87.0%87.0%87.0%87.1%87.1%87.1%87.1%87.1%87.2%87.2%87.2%87.2%87.2%87.3%87.3%87.3%87.3%87.3%87.4%87.4%87.4%87.4%87.4%87.4%87.5%87.5%87.5%87.5%87.5%87.6%87.6%87.6%87.6%87.6%87.7%87.7%87.7%87.7%87.7%87.8%87.8%87.8%87.8%87.8%87.8%87.9%87.9%87.9%87.9%87.9%88.0%88.0%88.0%88.0%88.0%88.1%88.1%88.1%88.1%88.1%88.2%88.2%88.2%88.2%88.2%88.3%88.3%88.3%88.3%88.3%88.3%88.4%88.4%88.4%88.4%88.4%88.5%88.5%88.5%88.5%88.5%88.6%88.6%88.6%88.6%88.6%88.7%88.7%88.7%88.7%88.7%88.8%88.8%88.8%88.8%88.8%88.8%88.9%88.9%88.9%88.9%88.9%89.0%89.0%89.0%89.0%89.0%89.1%89.1%89.1%89.1%89.1%89.2%89.2%89.2%89.2%89.2%89.3%89.3%89.3%89.3%89.3%89.3%89.4%89.4%89.4%89.4%89.4%89.5%89.5%89.5%89.5%89.5%89.6%89.6%89.6%89.6%89.6%89.7%89.7%89.7%89.7%89.7%89.8%89.8%89.8%89.8%89.8%89.8%89.9%89.9%89.9%89.9%89.9%90.0%90.0%90.0%90.0%90.0%90.1%90.1%90.1%90.1%90.1%90.2%90.2%90.2%90.2%90.2%90.3%90.3%90.3%90.3%90.3%90.3%90.4%90.4%90.4%90.4%90.4%90.5%90.5%90.5%90.5%90.5%90.6%90.6%90.6%90.6%90.6%90.7%90.7%90.7%90.7%90.7%90.8%90.8%90.8%90.8%90.8%90.8%90.9%90.9%90.9%90.9%90.9%91.0%91.0%91.0%91.0%91.0%91.1%91.1%91.1%91.1%91.1%91.2%91.2%91.2%91.2%91.2%91.3%91.3%91.3%91.3%91.3%91.3%91.4%91.4%91.4%91.4%91.4%91.5%91.5%91.5%91.5%91.5%91.6%91.6%91.6%91.6%91.6%91.7%91.7%91.7%91.7%91.7%91.8%91.8%91.8%91.8%91.8%91.8%91.9%91.9%91.9%91.9%91.9%92.0%92.0%92.0%92.0%92.0%92.1%92.1%92.1%92.1%92.1%92.2%92.2%92.2%92.2%92.2%92.3%92.3%92.3%92.3%92.3%92.3%92.4%92.4%92.4%92.4%92.4%92.5%92.5%92.5%92.5%92.5%92.6%92.6%92.6%92.6%92.6%92.7%92.7%92.7%92.7%92.7%92.8%92.8%92.8%92.8%92.8%92.8%92.9%92.9%92.9%92.9%92.9%93.0%93.0%93.0%93.0%93.0%93.1%93.1%93.1%93.1%93.1%93.2%93.2%93.2%93.2%93.2%93.3%93.3%93.3%93.3%93.3%93.3%93.4%93.4%93.4%93.4%93.4%93.5%93.5%93.5%93.5%93.5%93.6%93.6%93.6%93.6%93.6%93.7%93.7%93.7%93.7%93.7%93.8%93.8%93.8%93.8%93.8%93.8%93.9%93.9%93.9%93.9%93.9%94.0%94.0%94.0%94.0%94.0%94.1%94.1%94.1%94.1%94.1%94.2%94.2%94.2%94.2%94.2%94.2%94.3%94.3%94.3%94.3%94.3%94.4%94.4%94.4%94.4%94.4%94.5%94.5%94.5%94.5%94.5%94.6%94.6%94.6%94.6%94.6%94.7%94.7%94.7%94.7%94.7%94.7%94.8%94.8%94.8%94.8%94.8%94.9%94.9%94.9%94.9%94.9%95.0%95.0%95.0%95.0%95.0%95.1%95.1%95.1%95.1%95.1%95.2%95.2%95.2%95.2%95.2%95.2%95.3%95.3%95.3%95.3%95.3%95.4%95.4%95.4%95.4%95.4%95.5%95.5%95.5%95.5%95.5%95.6%95.6%95.6%95.6%95.6%95.7%95.7%95.7%95.7%95.7%95.7%95.8%95.8%95.8%95.8%95.8%95.9%95.9%95.9%95.9%95.9%96.0%96.0%96.0%96.0%96.0%96.1%96.1%96.1%96.1%96.1%96.2%96.2%96.2%96.2%96.2%96.2%96.3%96.3%96.3%96.3%96.3%96.4%96.4%96.4%96.4%96.4%96.5%96.5%96.5%96.5%96.5%96.6%96.6%96.6%96.6%96.6%96.7%96.7%96.7%96.7%96.7%96.7%96.8%96.8%96.8%96.8%96.8%96.9%96.9%96.9%96.9%96.9%97.0%97.0%97.0%97.0%97.0%97.1%97.1%97.1%97.1%97.1%97.2%97.2%97.2%97.2%97.2%97.2%97.3%97.3%97.3%97.3%97.3%97.4%97.4%97.4%97.4%97.4%97.5%97.5%97.5%97.5%97.5%97.6%97.6%97.6%97.6%97.6%97.7%97.7%97.7%97.7%97.7%97.7%97.8%97.8%97.8%97.8%97.8%97.9%97.9%97.9%97.9%97.9%98.0%98.0%98.0%98.0%98.0%98.1%98.1%98.1%98.1%98.1%98.2%98.2%98.2%98.2%98.2%98.2%98.3%98.3%98.3%98.3%98.3%98.4%98.4%98.4%98.4%98.4%98.5%98.5%98.5%98.5%98.5%98.6%98.6%98.6%98.6%98.6%98.7%98.7%98.7%98.7%98.7%98.7%98.8%98.8%98.8%98.8%98.8%98.9%98.9%98.9%98.9%98.9%99.0%99.0%99.0%99.0%99.0%99.1%99.1%99.1%99.1%99.1%99.2%99.2%99.2%99.2%99.2%99.2%99.3%99.3%99.3%99.3%99.3%99.4%99.4%99.4%99.4%99.4%99.5%99.5%99.5%99.5%99.5%99.6%99.6%99.6%99.6%99.6%99.7%99.7%99.7%99.7%99.7%99.7%99.8%99.8%99.8%99.8%99.8%99.9%99.9%99.9%99.9%99.9%100.0%100.0%100.0%100.0%\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# functions to show an image\n\n\ndef imshow(img):\n    img = img / 2 + 0.5     # unnormalize\n    npimg = img.numpy()\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.show()\n\n\n# get some random training images\ndataiter = iter(trainloader)\nimages, labels = next(dataiter)\n\n# show images\nimshow(torchvision.utils.make_grid(images))\n# print labels\nprint(' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))\n\n\n\n\n\n\n\n\nbird  truck truck plane"
  },
  {
    "objectID": "ai/pt/pt_data.html#creating-a-dataloader",
    "href": "ai/pt/pt_data.html#creating-a-dataloader",
    "title": "Loading data",
    "section": "Creating a DataLoader",
    "text": "Creating a DataLoader\nA DataLoader is an iterable feeding data to a model. When we train a model, we run it for each element of the DataLoader in a for loop:\nfor i in data_loader:\n    &lt;some model&gt;"
  },
  {
    "objectID": "ai/pt/pt_hpc.html",
    "href": "ai/pt/pt_hpc.html",
    "title": "Deep learning on production clusters",
    "section": "",
    "text": "This section is a summary of relevant information while using Python in an HPC context for deep learning.",
    "crumbs": [
      "AI",
      "<b><em>Intro DL with PyTorch</em></b>",
      "DL on production clusters"
    ]
  },
  {
    "objectID": "ai/pt/pt_hpc.html#run-code-in-a-job",
    "href": "ai/pt/pt_hpc.html#run-code-in-a-job",
    "title": "Deep learning on production clusters",
    "section": "Run code in a job",
    "text": "Run code in a job\nWhen you ssh into one of the Alliance clusters, you log into the login node.\nEverybody using a cluster uses that node to enter the cluster. Do not run anything computationally intensive on this node or you would make the entire cluster very slow for everyone. To run your code, you need to start an interactive job or submit a batch job to Slurm (the job scheduler used by the Alliance clusters).",
    "crumbs": [
      "AI",
      "<b><em>Intro DL with PyTorch</em></b>",
      "DL on production clusters"
    ]
  },
  {
    "objectID": "ai/pt/pt_hpc.html#plots",
    "href": "ai/pt/pt_hpc.html#plots",
    "title": "Deep learning on production clusters",
    "section": "Plots",
    "text": "Plots\nDo not run code that displays plots on screen. Instead, have them written to files.",
    "crumbs": [
      "AI",
      "<b><em>Intro DL with PyTorch</em></b>",
      "DL on production clusters"
    ]
  },
  {
    "objectID": "ai/pt/pt_hpc.html#data",
    "href": "ai/pt/pt_hpc.html#data",
    "title": "Deep learning on production clusters",
    "section": "Data",
    "text": "Data\n\nCopy files to/from the cluster\n\nFew files\nIf you need to copy files to or from the cluster, you can use scp from your local machine.\n\nCopy file from your computer to the cluster\n[local]$ scp &lt;/local/path/to/file&gt; &lt;user&gt;@&lt;hostname&gt;:&lt;path/in/cluster&gt;\n\nExpressions between the &lt; and &gt; signs need to be replaced by the relevant information (without those signs).\n\n\n\nCopy file from the cluster to your computer\n[local]$ scp &lt;user&gt;@&lt;hostname&gt;:&lt;cluster/path/to/file&gt; &lt;/local/path&gt;\n\n\n\nLarge amount of data\nUse Globus for large data transfers.\n\nThe Alliance is starting to store classic ML datasets on its clusters. So if your research uses a common dataset, it may be worth inquiring whether it might be available before downloading a copy.\n\n\n\n\nLarge collections of files\nThe Alliance clusters are optimized for very large files and are slowed by large collections of small files. Datasets with many small files need to be turned into single-file archives with tar. Failing to do so will affect performance not just for you, but for all users of the cluster.\n$ tar cf &lt;data&gt;.tar &lt;path/to/dataset/directory&gt;/*\n\n\nIf you want to also compress the files, replace tar cf with tar czf\nAs a modern alternative to tar, you can use Dar",
    "crumbs": [
      "AI",
      "<b><em>Intro DL with PyTorch</em></b>",
      "DL on production clusters"
    ]
  },
  {
    "objectID": "ai/pt/pt_hpc.html#interactive-jobs",
    "href": "ai/pt/pt_hpc.html#interactive-jobs",
    "title": "Deep learning on production clusters",
    "section": "Interactive jobs",
    "text": "Interactive jobs\nInteractive jobs are useful for code testing and development. They are not however the most efficient way to run code, so you should limit their use to testing and development.\nYou start an interactive job with:\n$ salloc --account=def-&lt;account&gt; --cpus-per-task=&lt;n&gt; --gres=gpu:&lt;n&gt; --mem=&lt;mem&gt; --time=&lt;time&gt;\nOur training cluster does not have GPUs, so for this workshop, do not use the --gres=gpu:&lt;n&gt; option.\nFor the workshop, you also don’t have to worry about the --account=def-&lt;account&gt; option (or, if you want, you can use --account=def-sponsor00).\nOur training cluster has a total of 60 CPUs on 5 compute nodes. Since there are many of you in this workshop, please be very mindful when running interactive jobs: if you request a lot of CPUs for a long time, the other workshop attendees won’t be able to use the cluster anymore until your interactive job requested time ends (even if you aren’t running any code).\nHere are my suggestions so that we don’t run into this problem:\n\nOnly start interactive jobs when you need to understand what Python is doing at every step, or to test, explore, and develop code (so where an interactive Python shell is really beneficial). Once you have a model, submit a batch job to Slurm instead\nWhen running interactive jobs on this training cluster, only request 1 CPU (so --cpus-per-task=1)\nOnly request the time that you will really use (e.g. for the lesson on Python tensors, maybe 30 min to 1 hour seems reasonable)\nIf you don’t need your job allocation anymore before it runs out, you can relinquish it with Ctrl+d\n\n\nBe aware that, on Cedar, you are not allowed to submit jobs from ~/home. Instead, you have to submit jobs from ~/scratch or ~/project.",
    "crumbs": [
      "AI",
      "<b><em>Intro DL with PyTorch</em></b>",
      "DL on production clusters"
    ]
  },
  {
    "objectID": "ai/pt/pt_hpc.html#batch-jobs",
    "href": "ai/pt/pt_hpc.html#batch-jobs",
    "title": "Deep learning on production clusters",
    "section": "Batch jobs",
    "text": "Batch jobs\nAs soon as you have a working Python script, you want to submit a batch job instead of running an interactive job. To do that, you need to write an sbatch script.\n\nJob script\n\nHere is an example script:\n\n#!/bin/bash\n#SBATCH --job-name=&lt;name&gt;*            # job name\n#SBATCH --account=def-&lt;account&gt;\n#SBATCH --time=&lt;time&gt;                 # max walltime in D-HH:MM or HH:MM:SS\n#SBATCH --cpus-per-task=&lt;number&gt;      # number of cores\n#SBATCH --gres=gpu:&lt;type&gt;:&lt;number&gt;    # type and number of GPU(s) per node\n#SBATCH --mem=&lt;mem&gt;                   # max memory (default unit is MB) per node\n#SBATCH --output=%x_%j.out*           # file name for the output\n#SBATCH --error=%x_%j.err*            # file name for errors\n#SBATCH --mail-user=&lt;email_address&gt;*\n#SBATCH --mail-type=ALL*\n\n# Load modules\n# (Do not use this in our workshop since we aren't using GPUs)\n# (Note: loading the Python module is not necessary\n# when you activate a Python virtual environment)\n# module load cudacore/.10.1.243 cuda/10 cudnn/7.6.5\n\n# Create a variable with the directory for your ML project\nSOURCEDIR=~/&lt;path/project/dir&gt;\n\n# Activate your Python virtual environment\nsource ~/env/bin/activate\n\n# Transfer and extract data to a compute node\nmkdir $SLURM_TMPDIR/data\ntar xf ~/projects/def-&lt;user&gt;/&lt;data&gt;.tar -C $SLURM_TMPDIR/data\n\n# Run your Python script on the data\npython $SOURCEDIR/&lt;script&gt;.py $SLURM_TMPDIR/data\n\n\n%x will get replaced by the script name and %j by the job number\nIf you compressed your data with tar czf, you need to extract it with tar xzf\nSBATCH options marked with a * are optional\nThere are various other options for email notifications\n\n\nYou may wonder why we transferred data to a compute node. This makes any I/O operation involving your data a lot faster, so it will speed up your code. Here is how this works:\nFirst, we create a temporary data directory in $SLURM_TMPDIR:\n$ mkdir $SLURM_TMPDIR/data\n\nThe variable $SLURM_TMPDIR is created by Slurm on the compute node where a job is running. Its path is /localscratch/&lt;user&gt;.&lt;jobid&gt;.0. Anything in it gets deleted when the job is done.\n\nThen we extract the data into it:\n$ tar xf ~/projects/def-&lt;user&gt;/&lt;data&gt;.tar -C $SLURM_TMPDIR/data\nIf your data is not in a tar file, you can simply copy it to the compute node running your job:\n$ cp -r ~/projects/def-&lt;user&gt;/&lt;data&gt; $SLURM_TMPDIR/data\n\n\nJob handling\n\nSubmit a job\n$ cd &lt;/dir/containing/job&gt;\n$ sbatch &lt;jobscript&gt;.sh\n\n\nCheck the status of your job(s)\n$ sq\n\nPD = pending\nR = running\nCG = completing (Slurm is doing the closing processes)\nNo information = your job has finished running\n\n\n\nCancel a job\n$ scancel &lt;jobid&gt;\n\n\nDisplay efficiency measures of a completed job\n$ seff &lt;jobid&gt;",
    "crumbs": [
      "AI",
      "<b><em>Intro DL with PyTorch</em></b>",
      "DL on production clusters"
    ]
  },
  {
    "objectID": "ai/pt/pt_hpc.html#gpus",
    "href": "ai/pt/pt_hpc.html#gpus",
    "title": "Deep learning on production clusters",
    "section": "GPU(s)",
    "text": "GPU(s)\n\nGPU types\nSeveral Alliance clusters have GPUs. Their numbers and types differ:\n From the Alliance Wiki\nThe default is 12G P100, but you can request another type with SBATCH --gres=gpu:&lt;type&gt;:&lt;number&gt; (example: --gres=gpu:p100l:1 to request a 16G P100 on Cedar). Please refer to the Alliance Wiki for more details.\n\n\nNumber of GPU(s)\nTry running your model on a single GPU first.\nIt is very likely that you do not need more than one GPU. Asking for more than you need will greatly increase your waiting time until your job is run. The lesson on distributed computing with PyTorch gives a few information as to when you might benefit from using several GPUs and provides some links to more resources. We will also offer workshops on distributed ML in the future. In any event, you should test your model before asking for several GPUs.\n\n\nCPU/GPU ratio\nHere are the Alliance recommendations:\nBéluga:\nNo more than 10 CPU per GPU.\nCedar:\nP100 GPU: no more than 6 CPU per GPU.\nV100 GPU: no more than 8 CPU per GPU.\nGraham:\nNo more than 16 CPU per GPU.",
    "crumbs": [
      "AI",
      "<b><em>Intro DL with PyTorch</em></b>",
      "DL on production clusters"
    ]
  },
  {
    "objectID": "ai/pt/pt_hpc.html#code-testing",
    "href": "ai/pt/pt_hpc.html#code-testing",
    "title": "Deep learning on production clusters",
    "section": "Code testing",
    "text": "Code testing\nIt might be wise to test your code in an interactive job before submitting a really big batch job to Slurm.\n\nActivate your Python virtual environment\n$ source ~/env/bin/activate\n\n\nStart an interactive job\n\nExample:\n\n$ salloc --account=def-&lt;account&gt; --gres=gpu:1 --cpus-per-task=6 --mem=32000 --time=0:30:0\n\n\nPrepare the data\nCreate a temporary data directory in $SLURM_TMPDIR:\n(env) $ mkdir $SLURM_TMPDIR/data\n\nThe variable $SLURM_TMPDIR is created by Slurm on the compute node where a job is running. Its path is /localscratch/&lt;user&gt;.&lt;jobid&gt;.0. Anything in it gets deleted when the job is done.\n\nExtract the data into it:\n(env) $ tar xf ~/projects/def-&lt;user&gt;/&lt;data&gt;.tar -C $SLURM_TMPDIR/data\n\n\nTry to run your code\nPlay in Python to test your code:\n(env) $ python\n&gt;&gt;&gt; import torch\n&gt;&gt;&gt; ...\nTo exit the virtual environment, run:\n(env) $ deactivate",
    "crumbs": [
      "AI",
      "<b><em>Intro DL with PyTorch</em></b>",
      "DL on production clusters"
    ]
  },
  {
    "objectID": "ai/pt/pt_hpc.html#checkpoints",
    "href": "ai/pt/pt_hpc.html#checkpoints",
    "title": "Deep learning on production clusters",
    "section": "Checkpoints",
    "text": "Checkpoints\nLong jobs should have a checkpoint at least every 24 hours. This ensures that an outage won’t lead to days of computation lost and it will help get the job started by the scheduler sooner.\nFor instance, you might want to have checkpoints every n epochs (choose n so that n epochs take less than 24 hours to run).\nIn PyTorch, you can create dictionaries with all the information necessary and save them as .tar files with torch.save(). You can then load them back with torch.load().\nThe information you want to save in each checkpoint includes the model’s state_dict, the optimizer’s state_dict, the epoch at which you stopped, the latest training loss, and anything else needed to restart training where you left off.\n\nFor example, saving a checkpoint during training could look something like this:\n\ntorch.save({\n    'epoch': &lt;last epoch run&gt;,\n    'model_state_dict': net.state_dict(),\n    'optimizer_state_dict': optimizer.state_dict(),\n    'loss': &lt;latest loss&gt;,\n}, &lt;path/to/checkpoint-file.tar&gt;)\n\nTo restart, initialize the model and optimizer, load the dictionary, and resume training:\n\n# Initialize the model and optimizer\nmodel = &lt;your model&gt;\noptimizer = &lt;your optimizer&gt;\n\n# Load the dictionary\ncheckpoint = torch.load(&lt;path/to/checkpoint-file.tar&gt;)\nmodel.load_state_dict(checkpoint['model_state_dict'])\noptimizer.load_state_dict(checkpoint['optimizer_state_dict'])\nepoch = checkpoint['epoch']\nloss = checkpoint['loss']\n\n# Resume training\nmodel.train()",
    "crumbs": [
      "AI",
      "<b><em>Intro DL with PyTorch</em></b>",
      "DL on production clusters"
    ]
  },
  {
    "objectID": "ai/pt/pt_hpc.html#tensorboard-on-the-cluster",
    "href": "ai/pt/pt_hpc.html#tensorboard-on-the-cluster",
    "title": "Deep learning on production clusters",
    "section": "TensorBoard on the cluster",
    "text": "TensorBoard on the cluster\nTensorBoard allows to visually track your model metrics (e.g. loss, accuracy, model graph, etc.). It requires a lot of processing power however, so if you want to use it on an Alliance cluster, do not run it from the login node. Instead, run it as part of your job. This section guides you through the whole workflow.\n\nLaunch TensorBoard\nFirst, you need to launch TensorBoard in the background (with a trailing &) before running your Python script. To do so, ad to your sbatch script:\ntensorboard --logdir=/tmp/&lt;your log dir&gt; --host 0.0.0.0 &\n\nExample:\n\n#!/bin/bash\n#SBATCH ...\n...\n\ntensorboard --logdir=/tmp/&lt;your log dir&gt; --host 0.0.0.0 &\npython $SOURCEDIR/&lt;script&gt;.py $SLURM_TMPDIR/data\n\n\nCreate a connection between the compute node and your computer\nOnce the job is running, you need to create a connection between the compute node running TensorBoard and your computer.\nFirst, you need to find the hostname of the compute node running the Tensorboard server. This is the value under NODELIST for your job when you run:\n$ sq\nThen, from your computer, enter this ssh command:\n[local]$ ssh -N -f -L localhost:6006:&lt;node hostname&gt;:6006 &lt;user&gt;@&lt;cluster&gt;.computecanada.ca\n\nReplace &lt;node hostname&gt; by the compute node hostname you just identified, &lt;user&gt; by your user name, and &lt;cluster&gt; by the name of the Alliance cluster hostname—e.g. beluga, cedar, graham.\n\n\n\nAccess TensorBoard\nYou can now open a browser (on your computer) and go to http://localhost:6006 to monitor your model running on a compute node in the cluster!",
    "crumbs": [
      "AI",
      "<b><em>Intro DL with PyTorch</em></b>",
      "DL on production clusters"
    ]
  },
  {
    "objectID": "ai/pt/pt_hpc.html#running-several-similar-jobs",
    "href": "ai/pt/pt_hpc.html#running-several-similar-jobs",
    "title": "Deep learning on production clusters",
    "section": "Running several similar jobs",
    "text": "Running several similar jobs\nA number of ML tasks (e.g. hyperparameter optimization) require running several instances of similar jobs. Grouping them into a single job with GLOST or GNU Parallel reduces the stress on the scheduler.",
    "crumbs": [
      "AI",
      "<b><em>Intro DL with PyTorch</em></b>",
      "DL on production clusters"
    ]
  },
  {
    "objectID": "ai/pt/pt_intro_slides.html#ml-allows-to-achieve-previously-impossible-tasks",
    "href": "ai/pt/pt_intro_slides.html#ml-allows-to-achieve-previously-impossible-tasks",
    "title": "Introduction to machine learning",
    "section": "ML allows to achieve previously impossible tasks",
    "text": "ML allows to achieve previously impossible tasks\n\nLet’s take the example of image recognition:\n\nIn typical computing, a programmer writes code that gives a computer detailed instructions of what to do\nCoding all the possible ways—pixel by pixel—that an image can represent, say, a dog is an impossibly large task: there are many breeds of dogs, the image can be a picture, a blurred picture, a drawing, a cartoon, the dog can be in all sorts of positions, wearing clothes, etc\nThere just aren’t enough resources to make the traditional programming approach able to create a computer program that can identify a dog in images\nBy feeding a very large number of dog images to a neural network however, we can train that network to recognize dogs in images that it has never seen (without explicitly programming how it does this!)"
  },
  {
    "objectID": "ai/pt/pt_intro_slides.html#old-concept-new-computing-power",
    "href": "ai/pt/pt_intro_slides.html#old-concept-new-computing-power",
    "title": "Introduction to machine learning",
    "section": "Old concept … new computing power",
    "text": "Old concept … new computing power\nThe concept is everything but new: Arthur Samuel came up with it in 1949 and built a self-learning Checkers-playing program in 1959\n\n\nMachine learning consists of feeding vast amounts of data to algorithms to strengthen pathways, so the excitement for the approach became somewhat dormant due to the lack of computing power and the lack of training data at the time\nThe advent of powerful computers, GPUs, and massive amounts of data have brought the old concept to the forefront\n\n\n\n\n\n\nFrom xkcd.com"
  },
  {
    "objectID": "ai/pt/pt_intro_slides.html#supervised-learning",
    "href": "ai/pt/pt_intro_slides.html#supervised-learning",
    "title": "Introduction to machine learning",
    "section": "Supervised learning",
    "text": "Supervised learning\n\nRegression is a form of supervised learning with continuous outputs\nClassification is supervised learning with discrete outputs\n\nSupervised learning uses training data in the form of example input/output pairs\nGoal\nFind the relationship between inputs and outputs"
  },
  {
    "objectID": "ai/pt/pt_intro_slides.html#unsupervised-learning",
    "href": "ai/pt/pt_intro_slides.html#unsupervised-learning",
    "title": "Introduction to machine learning",
    "section": "Unsupervised learning",
    "text": "Unsupervised learning\nClustering, social network analysis, market segmentation, PCA … are all forms of unsupervised learning\nUnsupervised learning uses unlabelled data\nGoal\nFind structure within the data"
  },
  {
    "objectID": "ai/pt/pt_intro_slides.html#reinforcement-learning",
    "href": "ai/pt/pt_intro_slides.html#reinforcement-learning",
    "title": "Introduction to machine learning",
    "section": "Reinforcement learning",
    "text": "Reinforcement learning\nThe algorithm explores by performing random actions and these actions are rewarded or punished (bonus points or penalties)\nThis is how algorithms learn to play games"
  },
  {
    "objectID": "ai/pt/pt_intro_slides.html#decide-on-an-architecture",
    "href": "ai/pt/pt_intro_slides.html#decide-on-an-architecture",
    "title": "Introduction to machine learning",
    "section": "Decide on an architecture",
    "text": "Decide on an architecture\n\nThe architecture won’t change during training\nThe type of architecture you choose (e.g. CNN, Transformer) depends on the type of data you have (e.g. vision, textual). The depth and breadth of your network depend on the amount of data and computing resource you have"
  },
  {
    "objectID": "ai/pt/pt_intro_slides.html#set-some-initial-parameters",
    "href": "ai/pt/pt_intro_slides.html#set-some-initial-parameters",
    "title": "Introduction to machine learning",
    "section": "Set some initial parameters",
    "text": "Set some initial parameters\n\nYou can initialize them randomly or get much better ones through transfer learning\nWhile the parameters are also part of the model, those will change during training"
  },
  {
    "objectID": "ai/pt/pt_intro_slides.html#get-some-labelled-data",
    "href": "ai/pt/pt_intro_slides.html#get-some-labelled-data",
    "title": "Introduction to machine learning",
    "section": "Get some labelled data",
    "text": "Get some labelled data\n\nWhen we say that we need a lot of data for machine learning, we mean “lots of labelled data” as this is what gets used for training models"
  },
  {
    "objectID": "ai/pt/pt_intro_slides.html#make-sure-to-keep-some-data-for-testing",
    "href": "ai/pt/pt_intro_slides.html#make-sure-to-keep-some-data-for-testing",
    "title": "Introduction to machine learning",
    "section": "Make sure to keep some data for testing",
    "text": "Make sure to keep some data for testing\n\nThose data won’t be used for training the model. Often people keep around 20% of their data for testing"
  },
  {
    "objectID": "ai/pt/pt_intro_slides.html#pass-data-and-parameters-through-the-architecture",
    "href": "ai/pt/pt_intro_slides.html#pass-data-and-parameters-through-the-architecture",
    "title": "Introduction to machine learning",
    "section": "Pass data and parameters through the architecture",
    "text": "Pass data and parameters through the architecture\n\nThe train data are the inputs and the process of calculating the outputs is the forward pass"
  },
  {
    "objectID": "ai/pt/pt_intro_slides.html#the-outputs-of-the-model-are-predictions",
    "href": "ai/pt/pt_intro_slides.html#the-outputs-of-the-model-are-predictions",
    "title": "Introduction to machine learning",
    "section": "The outputs of the model are predictions",
    "text": "The outputs of the model are predictions"
  },
  {
    "objectID": "ai/pt/pt_intro_slides.html#compare-those-predictions-to-the-train-labels",
    "href": "ai/pt/pt_intro_slides.html#compare-those-predictions-to-the-train-labels",
    "title": "Introduction to machine learning",
    "section": "Compare those predictions to the train labels",
    "text": "Compare those predictions to the train labels\n\nSince our data was labelled, we know what the true outputs are"
  },
  {
    "objectID": "ai/pt/pt_intro_slides.html#calculate-train-loss",
    "href": "ai/pt/pt_intro_slides.html#calculate-train-loss",
    "title": "Introduction to machine learning",
    "section": "Calculate train loss",
    "text": "Calculate train loss\n\nThe deviation of our predictions from the true outputs gives us a measure of training loss"
  },
  {
    "objectID": "ai/pt/pt_intro_slides.html#adjust-parameters",
    "href": "ai/pt/pt_intro_slides.html#adjust-parameters",
    "title": "Introduction to machine learning",
    "section": "Adjust parameters",
    "text": "Adjust parameters\n\nThe parameters get automatically adjusted to reduce the training loss through the mechanism of backpropagation. This is the actual training part\nThis process is repeated many times. Training models is pretty much a giant for loop"
  },
  {
    "objectID": "ai/pt/pt_intro_slides.html#from-model-to-program",
    "href": "ai/pt/pt_intro_slides.html#from-model-to-program",
    "title": "Introduction to machine learning",
    "section": "From model to program",
    "text": "From model to program\n\nRemember that the model architecture is fixed, but that the parameters change at each iteration of the training process"
  },
  {
    "objectID": "ai/pt/pt_intro_slides.html#evaluate-the-model",
    "href": "ai/pt/pt_intro_slides.html#evaluate-the-model",
    "title": "Introduction to machine learning",
    "section": "Evaluate the model",
    "text": "Evaluate the model\n\nWe can now use the testing set (which was never used to train the model) to evaluate our model: if we pass the test inputs through our program, we get some predictions that we can compare to the test labels (which are the true outputs)\nThis gives us the test loss: a measure of how well our model performs"
  },
  {
    "objectID": "ai/pt/pt_intro_slides.html#use-the-model",
    "href": "ai/pt/pt_intro_slides.html#use-the-model",
    "title": "Introduction to machine learning",
    "section": "Use the model",
    "text": "Use the model\n\nNow that we have a program, we can use it on unlabelled inputs to get what people ultimately want: unknown outputs\nThis is when we put our model to actual use to solve some problem"
  },
  {
    "objectID": "ai/pt/pt_model.html",
    "href": "ai/pt/pt_model.html",
    "title": "Building a model",
    "section": "",
    "text": "Key to creating neural networks in PyTorch is the torch.nn package which contains the nn.Module and a forward method which returns an output from some input.\nLet’s build a neural network to classify the MNIST.\n\nFirst, we need to define the architecture of the network. There are many types of architectures. For images, CNN are well suited.\nIn Python, you can define a subclass of an existing class with:\nclass YourSubclass(BaseClass):\n    &lt;definition of your subclass&gt;        \nThe subclass is derived from the base class and inherits its properties. PyTorch contains the class torch.nn.Module which is used as the base class when defining a neural network.\n\n# Load packages\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Net(nn.Module):\n    # Define the architecture of the network\n    def __init__(self):\n        super(Net, self).__init__()\n        # 1 input image channel, 6 output channels,\n        # 5x5 square convolution kernel\n        self.conv1 = nn.Conv2d(1, 6, 5)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        # an affine operation: y = Wx + b\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 5*5 from image dimension\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    # Set the flow of data through the network for the forward pass\n    # x represents the data\n    def forward(self, x):\n        # Max pooling over a (2, 2) window\n        # F.relu is the rectified-linear activation function\n        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n        # If the size is a square, you can specify with a single number\n        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n        # flatten all dimensions except the batch dimension\n        x = torch.flatten(x, 1)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\nLet’s create an instance of Net and print its structure:\n\nnet = Net()\nprint(net)\n\nNet(\n  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n  (fc1): Linear(in_features=400, out_features=120, bias=True)\n  (fc2): Linear(in_features=120, out_features=84, bias=True)\n  (fc3): Linear(in_features=84, out_features=10, bias=True)\n)\n\n\n\nparams = list(net.parameters())\nprint(len(params))\nprint(params[0].size())  # conv1's .weight\n\n10\ntorch.Size([6, 1, 5, 5])"
  },
  {
    "objectID": "ai/pt/pt_nn_slides.html#fully-connected-neural-networks",
    "href": "ai/pt/pt_nn_slides.html#fully-connected-neural-networks",
    "title": "NN vs biological neurons Types of NN",
    "section": "Fully connected neural networks",
    "text": "Fully connected neural networks\n\n\n\n\n\nFrom Glosser.ca, Wikipedia\n\n\n\nEach neuron receives inputs from every neuron of the previous layer and passes its output to every neuron of the next layer."
  },
  {
    "objectID": "ai/pt/pt_nn_slides.html#convolutional-neural-networks",
    "href": "ai/pt/pt_nn_slides.html#convolutional-neural-networks",
    "title": "NN vs biological neurons Types of NN",
    "section": "Convolutional neural networks",
    "text": "Convolutional neural networks\n\n\nFrom Programming Journeys by Rensu Theart\n\nConvolutional neural networks (CNN) are used for spatially structured data (e.g. images).\nImages have huge input sizes and would require a very large number of neurons in a fully connected neural net. In convolutional layers, neurons receive input from a subarea (called local receptive field) of the previous layer. This greatly reduces the number of parameters. Optionally, pooling (combining the outputs of neurons in a subarea) reduces the data dimensions."
  },
  {
    "objectID": "ai/pt/pt_nn_slides.html#recurrent-neural-networks",
    "href": "ai/pt/pt_nn_slides.html#recurrent-neural-networks",
    "title": "NN vs biological neurons Types of NN",
    "section": "Recurrent neural networks",
    "text": "Recurrent neural networks\n\n\nFrom fdeloche, Wikipedia\n\nRecurrent neural networks (RNN) such as Long Short-Term Memory (LSTM) are used for chain structured data (e.g. text).\nThey are not feedforward networks (i.e. networks for which the information moves only in the forward direction without any loop)."
  },
  {
    "objectID": "ai/pt/pt_nn_slides.html#transformers",
    "href": "ai/pt/pt_nn_slides.html#transformers",
    "title": "NN vs biological neurons Types of NN",
    "section": "Transformers",
    "text": "Transformers\nA combination of two RNNs (the encoder and the decoder) is used in sequence to sequence models for translation or picture captioning.\nIn 2014 the concept of attention (giving added weight to important words) was developed, greatly improving the ability of such models to process a lot of data.\nThe problem with recurrence is that it is not easily to parallelize (and thus to run fast on GPUs).\nIn 2017, a new model—the transformer—was proposed: by using only attention mechanisms and no recurrence, the transformer achieves better results in an easily parallelizable fashion.\nWith the addition of transfer learning, powerful transformers emerged in the field of NLP (e.g. Bidirectional Encoder Representations from Transformers (BERT) from Google and Generative Pre-trained Transformer-3 (GPT-3) from OpenAI)."
  },
  {
    "objectID": "ai/pt/pt_resources.html",
    "href": "ai/pt/pt_resources.html",
    "title": "Resources",
    "section": "",
    "text": "This section contains a list of general machine learning resources, resources specific to PyTorch, as well as resources for Python and fastai.\n\n\nMachine learning\nAlliance wiki ML page\n\nOpen-access preprints\nArxiv Sanity Preserver by Andrej Karpathy\nML papers in the computer science category on arXiv\nML papers in the stats category on arXiv\nDistill ML research online journal\n\n\nAdvice and sources\nAdvice and sources from ML research student\n\n\nGetting help\nStack Overflow [machine-learning] tag\nStack Overflow [deep-learning] tag\nStack Overflow [supervised-learning] tag\nStack Overflow [unsupervised-learning] tag\nStack Overflow [semisupervised-learning] tag\nStack Overflow [reinforcement-learning] tag\nStack Overflow [transfer-learning] tag\nStack Overflow [machine-learning-model] tag\nStack Overflow [learning-rate] tag\nStack Overflow [bayesian-deep-learning] tag\n\n\nFree introductory courses\ndeeplearning.ai\nfast.ai\nGoogle\n\n\nLists of open datasets\nbenchmarks.ai\nAIBench\nkaggle\nWikipedia\n\n\n\nPyTorch\nAlliance wiki PyTorch page\n\n\nDocumentation\nPyTorch website\nPyTorch documentation\nPyTorch tutorials\nPyTorch online courses\nPyTorch examples\n\n\nGetting help\nPyTorch Discourse forum\nStack Overflow [pytorch] tag\nStack Overflow [pytorch-dataloader] tag\nStack Overflow [pytorch-ignite] tag\n\n\nPre-trained models\nPyTorch Hub\n\n\n\nPython\nAlliance wiki Python page\n\nIDE\nProject Jupyter\nList of IDEs with description\nComparison of IDEs\nEmacs Python IDE\n\n\nShell\nIPython\nbpython\nptpython\n\n\nGetting help\nStack Overflow [python] tag\n\n\n\nfastai\n\nDocumentation\nManual\nTutorials\nPeer-reviewed paper\n\n\nBook\nPaperback version\nFree MOOC version of part 1 of the book\nJupyter notebooks version of the book\n\n\nGetting help\nDiscourse forum",
    "crumbs": [
      "AI",
      "<b><em>Intro DL with PyTorch</em></b>",
      "Resources"
    ]
  },
  {
    "objectID": "ai/pt/pt_tensors.html",
    "href": "ai/pt/pt_tensors.html",
    "title": "PyTorch tensors",
    "section": "",
    "text": "Before information can be processed by algorithms, it needs to be converted to floating point numbers. Indeed, you don’t pass a sentence or an image through a model; instead you input numbers representing a sequence of words or pixel values.\nAll these floating point numbers need to be stored in a data structure. The most suited structure is multidimensional (to hold several layers of information) and homogeneous—all data of the same type—for efficiency.\nPython already has several multidimensional array structures (e.g. NumPy’s ndarray) but the particularities of deep learning call for special characteristics such as the ability to run operations on GPUs and/or in a distributed fashion, the ability to keep track of computation graphs for automatic differentiation, and different defaults (lower precision for improved training performance).\nThe PyTorch tensor is a Python data structure with these characteristics that can easily be converted to/from NumPy’s ndarray and integrates well with other Python libraries such as Pandas.\nIn this section, we will explore the basics of PyTorch tensors.",
    "crumbs": [
      "AI",
      "<b><em>Intro DL with PyTorch</em></b>",
      "PyTorch tensors"
    ]
  },
  {
    "objectID": "ai/pt/pt_tensors.html#importing-pytorch",
    "href": "ai/pt/pt_tensors.html#importing-pytorch",
    "title": "PyTorch tensors",
    "section": "Importing PyTorch",
    "text": "Importing PyTorch\nFirst of all, we need to import the torch library:\n\nimport torch\n\nWe can check its version with:\n\ntorch.__version__\n\n'2.6.0+cu124'",
    "crumbs": [
      "AI",
      "<b><em>Intro DL with PyTorch</em></b>",
      "PyTorch tensors"
    ]
  },
  {
    "objectID": "ai/pt/pt_tensors.html#creating-tensors",
    "href": "ai/pt/pt_tensors.html#creating-tensors",
    "title": "PyTorch tensors",
    "section": "Creating tensors",
    "text": "Creating tensors\nThere are many ways to create tensors:\n\ntorch.tensor:   Input individual values\ntorch.arange:   1D tensor with a sequence of integers\ntorch.linspace:  1D linear scale tensor\ntorch.logspace:  1D log scale tensor\ntorch.rand:     Random numbers from a uniform distribution on [0, 1)\ntorch.randn:     Numbers from the standard normal distribution\ntorch.randperm:   Random permutation of integers\ntorch.empty:     Uninitialized tensor\ntorch.zeros:     Tensor filled with 0\ntorch.ones:     Tensor filled with 1\ntorch.eye:       Identity matrix\n\n\nFrom input values\n\nt = torch.tensor(3)\n\n\n\nYour turn:\n\nWithout using the shape descriptor, try to get the shape of the following tensors:\ntorch.tensor([0.9704, 0.1339, 0.4841])\n\ntorch.tensor([[0.9524, 0.0354],\n        [0.9833, 0.2562],\n        [0.0607, 0.6420]])\n\ntorch.tensor([[[0.4604, 0.2699],\n         [0.8360, 0.0317],\n         [0.3289, 0.1171]]])\n\ntorch.tensor([[[[0.0730, 0.8737],\n          [0.2305, 0.4719],\n          [0.0796, 0.2745]]],\n\n        [[[0.1534, 0.9442],\n          [0.3287, 0.9040],\n          [0.0948, 0.1480]]]])\n\nLet’s create a random tensor with a single element:\n\nt = torch.rand(1)\nt\n\ntensor([0.1952])\n\n\nWe can extract the value from a tensor with one element:\n\nt.item()\n\n0.19522696733474731\n\n\nAll these tensors have a single element, but an increasing number of dimensions:\n\ntorch.rand(1)\n\ntensor([0.2937])\n\n\n\ntorch.rand(1, 1)\n\ntensor([[0.9385]])\n\n\n\ntorch.rand(1, 1, 1)\n\ntensor([[[0.7764]]])\n\n\n\ntorch.rand(1, 1, 1, 1)\n\ntensor([[[[0.4473]]]])\n\n\n\nYou can tell the number of dimensions of a tensor easily by counting the number of opening square brackets.\n\n\ntorch.rand(1, 1, 1, 1).dim()\n\n4\n\n\nTensors can have multiple elements in one dimension:\n\ntorch.rand(6)\n\ntensor([0.6389, 0.1453, 0.8023, 0.3180, 0.4471, 0.4798])\n\n\n\ntorch.rand(6).dim()\n\n1\n\n\nAnd multiple elements in multiple dimensions:\n\ntorch.rand(2, 3, 4, 5)\n\ntensor([[[[0.9159, 0.0173, 0.2502, 0.2044, 0.5919],\n          [0.4154, 0.9943, 0.9573, 0.4240, 0.2554],\n          [0.1066, 0.2649, 0.7477, 0.1648, 0.8121],\n          [0.3351, 0.0105, 0.4873, 0.6219, 0.2475]],\n\n         [[0.7721, 0.4679, 0.4826, 0.5987, 0.9971],\n          [0.9861, 0.3395, 0.3855, 0.7349, 0.3245],\n          [0.3450, 0.9941, 0.4847, 0.6996, 0.4295],\n          [0.1400, 0.3646, 0.7771, 0.9690, 0.1722]],\n\n         [[0.6430, 0.0170, 0.4688, 0.2151, 0.6894],\n          [0.6014, 0.5355, 0.1968, 0.7655, 0.6725],\n          [0.4229, 0.4040, 0.6667, 0.7375, 0.7241],\n          [0.1703, 0.0980, 0.7046, 0.0855, 0.9696]]],\n\n\n        [[[0.2787, 0.1543, 0.1760, 0.7723, 0.3313],\n          [0.7928, 0.2783, 0.0737, 0.8541, 0.8127],\n          [0.4876, 0.3712, 0.5947, 0.7668, 0.1385],\n          [0.1390, 0.9622, 0.5546, 0.3023, 0.5994]],\n\n         [[0.6486, 0.1685, 0.9493, 0.6600, 0.5395],\n          [0.5364, 0.7494, 0.1369, 0.9789, 0.0888],\n          [0.0203, 0.2671, 0.4030, 0.3052, 0.0581],\n          [0.5803, 0.3366, 0.0981, 0.0196, 0.9879]],\n\n         [[0.8596, 0.5901, 0.6223, 0.6339, 0.2987],\n          [0.6060, 0.7458, 0.2324, 0.1630, 0.5129],\n          [0.5956, 0.1734, 0.5036, 0.7118, 0.9202],\n          [0.0886, 0.3873, 0.7526, 0.8082, 0.7475]]]])\n\n\n\ntorch.rand(2, 3, 4, 5).dim()\n\n4\n\n\n\ntorch.rand(2, 3, 4, 5).numel()\n\n120\n\n\n\ntorch.ones(2, 4)\n\ntensor([[1., 1., 1., 1.],\n        [1., 1., 1., 1.]])\n\n\n\nt = torch.rand(2, 3)\ntorch.zeros_like(t)             # Matches the size of t\n\ntensor([[0., 0., 0.],\n        [0., 0., 0.]])\n\n\n\ntorch.ones_like(t)\n\ntensor([[1., 1., 1.],\n        [1., 1., 1.]])\n\n\n\ntorch.randn_like(t)\n\ntensor([[-0.0760,  0.6491,  0.0362],\n        [ 0.5438, -1.2794,  0.6735]])\n\n\n\ntorch.arange(2, 10, 3)    # From 2 to 10 in increments of 3\n\ntensor([2, 5, 8])\n\n\n\ntorch.linspace(2, 10, 3)  # 3 elements from 2 to 10 on the linear scale\n\ntensor([ 2.,  6., 10.])\n\n\n\ntorch.logspace(2, 10, 3)  # Same on the log scale\n\ntensor([1.0000e+02, 1.0000e+06, 1.0000e+10])\n\n\n\ntorch.randperm(3)\n\ntensor([2, 0, 1])\n\n\n\ntorch.eye(3)\n\ntensor([[1., 0., 0.],\n        [0., 1., 0.],\n        [0., 0., 1.]])",
    "crumbs": [
      "AI",
      "<b><em>Intro DL with PyTorch</em></b>",
      "PyTorch tensors"
    ]
  },
  {
    "objectID": "ai/pt/pt_tensors.html#conversion-tofrom-numpy",
    "href": "ai/pt/pt_tensors.html#conversion-tofrom-numpy",
    "title": "PyTorch tensors",
    "section": "Conversion to/from NumPy",
    "text": "Conversion to/from NumPy\nPyTorch tensors can be converted to NumPy ndarrays and vice-versa in a very efficient manner as both objects share the same memory.\n\nFrom PyTorch tensor to NumPy ndarray\n\nt = torch.rand(2, 3)\nt\n\ntensor([[0.1417, 0.9127, 0.9021],\n        [0.9942, 0.5279, 0.4297]])\n\n\n\nt_np = t.numpy()\nt_np\n\narray([[0.14172977, 0.91267145, 0.9020647 ],\n       [0.9941583 , 0.5279334 , 0.42972422]], dtype=float32)\n\n\n\n\nFrom NumPy ndarray to PyTorch tensor\n\nimport numpy as np\na = np.random.rand(2, 3)\na\n\narray([[0.23670328, 0.64032567, 0.31378297],\n       [0.16256904, 0.61355664, 0.73836976]])\n\n\n\na_pt = torch.from_numpy(a)\na_pt\n\ntensor([[0.2367, 0.6403, 0.3138],\n        [0.1626, 0.6136, 0.7384]], dtype=torch.float64)\n\n\n\nNote the different default data types.",
    "crumbs": [
      "AI",
      "<b><em>Intro DL with PyTorch</em></b>",
      "PyTorch tensors"
    ]
  },
  {
    "objectID": "ai/pt/pt_tensors.html#indexing-tensors",
    "href": "ai/pt/pt_tensors.html#indexing-tensors",
    "title": "PyTorch tensors",
    "section": "Indexing tensors",
    "text": "Indexing tensors\n\nt = torch.rand(3, 4)\nt\n\ntensor([[0.2068, 0.1622, 0.5940, 0.4235],\n        [0.9162, 0.7183, 0.6857, 0.2356],\n        [0.6411, 0.8899, 0.0291, 0.3303]])\n\n\n\nt[:, 2]\n\ntensor([0.5940, 0.6857, 0.0291])\n\n\n\nt[1, :]\n\ntensor([0.9162, 0.7183, 0.6857, 0.2356])\n\n\n\nt[2, 3]\n\ntensor(0.3303)\n\n\n\nA word of caution about indexing\nWhile indexing elements of a tensor to extract some of the data as a final step of some computation is fine, you should not use indexing to run operations on tensor elements in a loop as this would be extremely inefficient.\nInstead, you want to use vectorized operations.",
    "crumbs": [
      "AI",
      "<b><em>Intro DL with PyTorch</em></b>",
      "PyTorch tensors"
    ]
  },
  {
    "objectID": "ai/pt/pt_tensors.html#vectorized-operations",
    "href": "ai/pt/pt_tensors.html#vectorized-operations",
    "title": "PyTorch tensors",
    "section": "Vectorized operations",
    "text": "Vectorized operations\nSince PyTorch tensors are homogeneous (i.e. made of a single data type), as with NumPy’s ndarrays, operations are vectorized and thus fast.\nNumPy is mostly written in C, PyTorch in C++. With either library, when you run vectorized operations on arrays/tensors, you don’t use raw Python (slow) but compiled C/C++ code (much faster).\nHere is an excellent post explaining Python vectorization & why it makes such a big difference.",
    "crumbs": [
      "AI",
      "<b><em>Intro DL with PyTorch</em></b>",
      "PyTorch tensors"
    ]
  },
  {
    "objectID": "ai/pt/pt_tensors.html#data-types",
    "href": "ai/pt/pt_tensors.html#data-types",
    "title": "PyTorch tensors",
    "section": "Data types",
    "text": "Data types\n\nDefault data type\nSince PyTorch tensors were built with efficiency in mind for neural networks, the default data type is 32-bit floating points.\nThis is sufficient for accuracy and much faster than 64-bit floating points.\n\nBy contrast, NumPy ndarrays use 64-bit as their default.\n\n\nt = torch.rand(2, 4)\nt.dtype\n\ntorch.float32\n\n\n\n\nSetting data type at creation\nThe type can be set with the dtype argument:\n\nt = torch.rand(2, 4, dtype=torch.float64)\nt\n\ntensor([[0.5765, 0.4829, 0.6444, 0.2722],\n        [0.6819, 0.0558, 0.6359, 0.0799]], dtype=torch.float64)\n\n\n\nPrinted tensors display attributes with values ≠ default values.\n\n\nt.dtype\n\ntorch.float64\n\n\n\n\nChanging data type\n\nt = torch.rand(2, 4)\nt.dtype\n\ntorch.float32\n\n\n\nt2 = t.type(torch.float64)\nt2.dtype\n\ntorch.float64\n\n\n\n\nList of data types\n\n\n\n\n\n\n\ndtype\nDescription\n\n\n\n\ntorch.float16 / torch.half\n16-bit / half-precision floating-point\n\n\ntorch.float32 / torch.float\n32-bit / single-precision floating-point\n\n\ntorch.float64 / torch.double\n64-bit / double-precision floating-point\n\n\ntorch.uint8\nunsigned 8-bit integers\n\n\ntorch.int8\nsigned 8-bit integers\n\n\ntorch.int16 / torch.short\nsigned 16-bit integers\n\n\ntorch.int32 / torch.int\nsigned 32-bit integers\n\n\ntorch.int64 / torch.long\nsigned 64-bit integers\n\n\ntorch.bool\nboolean",
    "crumbs": [
      "AI",
      "<b><em>Intro DL with PyTorch</em></b>",
      "PyTorch tensors"
    ]
  },
  {
    "objectID": "ai/pt/pt_tensors.html#simple-operations",
    "href": "ai/pt/pt_tensors.html#simple-operations",
    "title": "PyTorch tensors",
    "section": "Simple operations",
    "text": "Simple operations\n\nt1 = torch.tensor([[1, 2], [3, 4]])\nt1\n\ntensor([[1, 2],\n        [3, 4]])\n\n\n\nt2 = torch.tensor([[1, 1], [0, 0]])\nt2\n\ntensor([[1, 1],\n        [0, 0]])\n\n\nOperation performed between elements at corresponding locations:\n\nt1 + t2\n\ntensor([[2, 3],\n        [3, 4]])\n\n\nOperation applied to each element of the tensor:\n\nt1 + 1\n\ntensor([[2, 3],\n        [4, 5]])\n\n\n\nReduction\n\nt = torch.ones(2, 3, 4);\nt\n\ntensor([[[1., 1., 1., 1.],\n         [1., 1., 1., 1.],\n         [1., 1., 1., 1.]],\n\n        [[1., 1., 1., 1.],\n         [1., 1., 1., 1.],\n         [1., 1., 1., 1.]]])\n\n\n\nt.sum()   # Reduction over all entries\n\ntensor(24.)\n\n\n\nOther reduction functions (e.g. mean) behave the same way.\n\nReduction over a specific dimension:\n\nt.sum(0)\n\ntensor([[2., 2., 2., 2.],\n        [2., 2., 2., 2.],\n        [2., 2., 2., 2.]])\n\n\n\nt.sum(1)\n\ntensor([[3., 3., 3., 3.],\n        [3., 3., 3., 3.]])\n\n\n\nt.sum(2)\n\ntensor([[4., 4., 4.],\n        [4., 4., 4.]])\n\n\nReduction over multiple dimensions:\n\nt.sum((0, 1))\n\ntensor([6., 6., 6., 6.])\n\n\n\nt.sum((0, 2))\n\ntensor([8., 8., 8.])\n\n\n\nt.sum((1, 2))\n\ntensor([12., 12.])\n\n\n\n\nIn-place operations\nWith operators post-fixed with _:\n\nt1 = torch.tensor([1, 2])\nt1\n\ntensor([1, 2])\n\n\n\nt2 = torch.tensor([1, 1])\nt2\n\ntensor([1, 1])\n\n\n\nt1.add_(t2)\nt1\n\ntensor([2, 3])\n\n\n\nt1.zero_()\nt1\n\ntensor([0, 0])\n\n\n\nWhile reassignments will use new addresses in memory, in-place operations will use the same addresses.\n\n\n\nTensor views\nt = torch.tensor([[1, 2, 3], [4, 5, 6]]); print(t)\nt.size()\nt.view(6)\nt.view(3, 2)\nt.view(3, -1) # Same: with -1, the size is inferred from other dimensions\n\nNote the difference\n\nt1 = torch.tensor([[1, 2, 3], [4, 5, 6]])\nt1\n\ntensor([[1, 2, 3],\n        [4, 5, 6]])\n\n\n\nt2 = t1.t()\nt2\n\ntensor([[1, 4],\n        [2, 5],\n        [3, 6]])\n\n\n\nt3 = t1.view(3, 2)\nt3\n\ntensor([[1, 2],\n        [3, 4],\n        [5, 6]])\n\n\n\n\n\nLogical operations\n\nt1 = torch.randperm(5)\nt1\n\ntensor([0, 2, 3, 1, 4])\n\n\n\nt2 = torch.randperm(5)\nt2\n\ntensor([1, 4, 3, 0, 2])\n\n\nTest each element:\n\nt1 &gt; 3\n\ntensor([False, False, False, False,  True])\n\n\nTest corresponding pairs of elements:\n\nt1 &lt; t2\n\ntensor([ True,  True, False, False, False])",
    "crumbs": [
      "AI",
      "<b><em>Intro DL with PyTorch</em></b>",
      "PyTorch tensors"
    ]
  },
  {
    "objectID": "ai/pt/pt_tensors.html#device-attribute",
    "href": "ai/pt/pt_tensors.html#device-attribute",
    "title": "PyTorch tensors",
    "section": "Device attribute",
    "text": "Device attribute\nTensor data can be placed in the memory of various processor types:\n\nthe RAM of CPU,\nthe RAM of a GPU with CUDA support,\nthe RAM of a GPU with AMD’s ROCm support,\nthe RAM of an XLA device (e.g. Cloud TPU) with the torch_xla package.\n\nThe values for the device attributes are:\n\nCPU:  'cpu',\nGPU (CUDA & AMD’s ROCm):  'cuda',\nXLA:  xm.xla_device().\n\nThis last option requires to load the torch_xla package first:\nimport torch_xla\nimport torch_xla.core.xla_model as xm\n\nCreating a tensor on a specific device\nBy default, tensors are created on the CPU.\nYou can create a tensor on an accelerator by specifying the device attribute (our current training cluster does not have GPUs, so don’t run this on it):\nt_gpu = torch.rand(2, device='cuda')\n\n\nCopying a tensor to a specific device\nYou can also make copies of a tensor on other devices:\n# Make a copy of t on the GPU\nt_gpu = t.to(device='cuda')\nt_gpu = t.cuda()             # Alternative syntax\n\n# Make a copy of t_gpu on the CPU\nt = t_gpu.to(device='cpu')\nt = t_gpu.cpu()              # Alternative syntax\n\n\nMultiple GPUs\nIf you have multiple GPUs, you can optionally specify which one a tensor should be created on or copied to:\nt1 = torch.rand(2, device='cuda:0')  # Create a tensor on 1st GPU\nt2 = t1.to(device='cuda:0')          # Make a copy of t1 on 1st GPU\nt3 = t1.to(device='cuda:1')          # Make a copy of t1 on 2nd GPU\nOr the equivalent short forms:\nt2 = t1.cuda(0)\nt3 = t1.cuda(1)",
    "crumbs": [
      "AI",
      "<b><em>Intro DL with PyTorch</em></b>",
      "PyTorch tensors"
    ]
  },
  {
    "objectID": "ai/pt/pt_workflow.html",
    "href": "ai/pt/pt_workflow.html",
    "title": "Overall workflow",
    "section": "",
    "text": "This classic PyTorch tutorial goes over the entire workflow to create and train a simple image classifier.\nLet’s go over it step by step.",
    "crumbs": [
      "AI",
      "<b><em>Intro DL with PyTorch</em></b>",
      "Workflow"
    ]
  },
  {
    "objectID": "ai/pt/pt_workflow.html#the-data",
    "href": "ai/pt/pt_workflow.html#the-data",
    "title": "Overall workflow",
    "section": "The data",
    "text": "The data\nCIFAR-10 from the Canadian Institute for Advanced Research is a classic dataset of 60,000 color images falling into 10 classes (6,000 images in each class):\n\nairplane\nautomobile\nbird\ncat\ndeer\ndog\nfrog\nhorse\nship\ntruck\n\nThe images are of size 32x32 pixels (tiny!), which makes it very lightweight, quick to load and easy to play with.\n\nCreate a DataLoader\nA DataLoader is an iterable feeding data to a model at each iteration. The data loader transforms the data to the proper format, sets the batch size, whether the data is shuffled or not, and how the I/O is parallelized. You can create DataLoaders with the torch.utils.data.DataLoader class.\nLet’s create 2 DataLoaders: one for the train set and one for the test set.\n\nLoad packages\n\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\n\n\n\nCreate a transform object\nThe CIFAR-10 images in the TorchVision library are Image objects (from the PIL.Image module of the pillow package).\nWe need to normalize them and turn them into tensors:\n\ntransform = transforms.Compose(\n    [transforms.ToTensor(),\n     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n\n\n\nChoose a batch size\nRemember that the data move forward through the network (forward pass), outputting some estimates which are used to calculate some loss (or error) value. Then we get gradients through automatic differentiation and the model parameters are adjusted a little through gradient descent.\nYou do not have to have the entire training set go through this process each time: you can use batches.\nThe batch size is the number of items from the data that are processed before the model is updated. There is no hard rule to set good batch sizes and sizes tend to be picked through trial and error.\nHere are some rules to chose a batch size:\n\nmake sure that the batch fits in the CPU or GPU,\nsmall batches give faster results (each training iteration is very fast), but give less accuracy,\nlarge batches lead to slower training, but better accuracy.\n\nLet’s set the batch size to 4:\n\nbatch_size = 4\n\n\n\nPut it together into DataLoaders\n\ntrainset = torchvision.datasets.CIFAR10(root='./data',\n                                        train=True,\n                                        download=True,\n                                        transform=transform)\n\ntrainloader = torch.utils.data.DataLoader(trainset,\n                                          batch_size=batch_size,\n                                          shuffle=True,\n                                          num_workers=2)\n\ntestset = torchvision.datasets.CIFAR10(root='./data',\n                                       train=False,\n                                       download=True,\n                                       transform=transform)\n\ntestloader = torch.utils.data.DataLoader(testset,\n                                         batch_size=batch_size,\n                                         shuffle=False,\n                                         num_workers=2)\n\nWe will also need the classes:\n\nclasses = ('plane', 'car', 'bird', 'cat', 'deer',\n           'dog', 'frog', 'horse', 'ship', 'truck')\n\n\n\n\nVisualize a sample of the data\nThough not necessary, it can be useful to have a look at the data:\n\n# Load the packages for this\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Define a function to display an image\ndef imshow(img):\n    img = img / 2 + 0.5     # unnormalize\n    npimg = img.numpy()\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.show()\n\n# Get a batch of random training images\ndataiter = iter(trainloader)\nimages, labels = next(dataiter)\n\n# Display the images\nimshow(torchvision.utils.make_grid(images))\n\n# Print the labels\nprint(' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))\n\n\n\n\n\n\n\n\nship  truck deer  bird",
    "crumbs": [
      "AI",
      "<b><em>Intro DL with PyTorch</em></b>",
      "Workflow"
    ]
  },
  {
    "objectID": "ai/pt/pt_workflow.html#the-model",
    "href": "ai/pt/pt_workflow.html#the-model",
    "title": "Overall workflow",
    "section": "The model",
    "text": "The model\n\nArchitecture\nFirst, we need to define the architecture of the network. There are many types of architectures. For images, CNN are well suited.\nIn Python, you can define a subclass of an existing class with:\nclass YourSubclass(BaseClass):\n    &lt;definition of your subclass&gt;        \nThe subclass is derived from the base class and inherits its properties. PyTorch contains the class torch.nn.Module which is used as the base class when defining a neural network.\n\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Net(nn.Module):\n    # Define the architecture of the network\n    def __init__(self):\n        super().__init__()\n        # 3 input image channel (3 colour channels)\n        # 6 output channels,\n        # 5x5 square convolution kernel\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        # Max pooling over a (2, 2) window\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        # 5*5 from image dimension\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n        self.fc2 = nn.Linear(120, 84)\n        # 10 is the size of the output layer\n        # since there are 10 classes\n        self.fc3 = nn.Linear(84, 10)\n\n    # Set the flow of data through the network for the forward pass\n    # x represents the data\n    def forward(self, x):\n        # F.relu is the rectified-linear activation function\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        # flatten all dimensions except the batch dimension\n        x = torch.flatten(x, 1)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\nLet’s create an instance of Net and print its structure:\n\nnet = Net()\nprint(net)\n\nNet(\n  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n  (fc1): Linear(in_features=400, out_features=120, bias=True)\n  (fc2): Linear(in_features=120, out_features=84, bias=True)\n  (fc3): Linear(in_features=84, out_features=10, bias=True)\n)\n\n\n\n\nLoss function and optimizer\nWe need to chose a loss function that will be used to calculate the gradients through backpropagation as well as an optimizer to do the gradient descent.\nSGD with momentum has proved a very efficient optimizing technique and is widely used.\n\nimport torch.optim as optim\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)",
    "crumbs": [
      "AI",
      "<b><em>Intro DL with PyTorch</em></b>",
      "Workflow"
    ]
  },
  {
    "objectID": "ai/pt/pt_workflow.html#training",
    "href": "ai/pt/pt_workflow.html#training",
    "title": "Overall workflow",
    "section": "Training",
    "text": "Training\nWe can now train the model:\n\nfor epoch in range(2):  # loop over the dataset twice\n\n    running_loss = 0.0\n\n    for i, data in enumerate(trainloader, 0):\n        # get the inputs; data is a list of [inputs, labels]\n        inputs, labels = data\n\n        # zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward + backward + optimize\n        outputs = net(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        # print statistics\n        running_loss += loss.item()\n        if i % 2000 == 1999:    # print every 2000 mini-batches\n            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n            running_loss = 0.0\n\nprint('Finished Training')\n\n[1,  2000] loss: 2.181\n[1,  4000] loss: 1.825\n[1,  6000] loss: 1.653\n[1,  8000] loss: 1.577\n[1, 10000] loss: 1.527\n[1, 12000] loss: 1.467\n[2,  2000] loss: 1.398\n[2,  4000] loss: 1.369\n[2,  6000] loss: 1.320\n[2,  8000] loss: 1.317\n[2, 10000] loss: 1.289\n[2, 12000] loss: 1.272\nFinished Training",
    "crumbs": [
      "AI",
      "<b><em>Intro DL with PyTorch</em></b>",
      "Workflow"
    ]
  },
  {
    "objectID": "ai/pt/pt_workflow.html#testing",
    "href": "ai/pt/pt_workflow.html#testing",
    "title": "Overall workflow",
    "section": "Testing",
    "text": "Testing\n\nLittle test on one batch for fun\nLet’s now test our model on one batch of testing data.\nFirst, let’s get a batch of random testing data:\n\ndataiter = iter(testloader)\nimages, labels = next(dataiter)\n\nLet’s display them and print their true labels:\n\nimshow(torchvision.utils.make_grid(images))\nprint('Real: ', ' '.join(f'{classes[labels[j]]:5s}' for j in range(4)))\n\n\n\n\n\n\n\n\nReal:  cat   ship  ship  plane\n\n\nNow, let’s run the same batch of testing images through our model:\n\noutputs = net(images)\n\nLet’s get the best predictions for these:\n\n_, predicted = torch.max(outputs, 1)\n\nprint('Predicted: ', ' '.join(f'{classes[predicted[j]]:5s}'\n                              for j in range(4)))\n\nPredicted:  cat   ship  ship  ship \n\n\n\n\nMore serious testing\nThis was fun, but of course, with a sample of one, we can’t say anything about how good our model is. We need to test it on many more images from the test set.\nLet’s use the entire test set:\n\ncorrect = 0\ntotal = 0\n# since we're not training, we don't need to calculate the gradients for our outputs\nwith torch.no_grad():\n    for data in testloader:\n        images, labels = data\n        # calculate outputs by running images through the network\n        outputs = net(images)\n        # the class with the highest energy is what we choose as prediction\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\nprint(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n\nAccuracy of the network on the 10000 test images: 54 %\n\n\n\n\nPer class testing\nWe could see whether the model seem to perform better for some classes than others:\n\n# prepare to count predictions for each class\ncorrect_pred = {classname: 0 for classname in classes}\ntotal_pred = {classname: 0 for classname in classes}\n\n# again no gradients needed\nwith torch.no_grad():\n    for data in testloader:\n        images, labels = data\n        outputs = net(images)\n        _, predictions = torch.max(outputs, 1)\n        # collect the correct predictions for each class\n        for label, prediction in zip(labels, predictions):\n            if label == prediction:\n                correct_pred[classes[label]] += 1\n            total_pred[classes[label]] += 1\n\n\n# print accuracy for each class\nfor classname, correct_count in correct_pred.items():\n    accuracy = 100 * float(correct_count) / total_pred[classname]\n    print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')\n\nAccuracy for class: plane is 51.4 %\nAccuracy for class: car   is 62.2 %\nAccuracy for class: bird  is 59.9 %\nAccuracy for class: cat   is 34.1 %\nAccuracy for class: deer  is 39.0 %\nAccuracy for class: dog   is 47.9 %\nAccuracy for class: frog  is 54.0 %\nAccuracy for class: horse is 68.7 %\nAccuracy for class: ship  is 77.7 %\nAccuracy for class: truck is 53.4 %",
    "crumbs": [
      "AI",
      "<b><em>Intro DL with PyTorch</em></b>",
      "Workflow"
    ]
  },
  {
    "objectID": "ai/pt/wb_torchtensors.html",
    "href": "ai/pt/wb_torchtensors.html",
    "title": "Everything you wanted to know (and more) about PyTorch tensors",
    "section": "",
    "text": "Before information can be fed to artificial neural networks (ANNs), it needs to be converted to a form ANNs can process: floating point numbers. Indeed, you don’t pass a sentence or an image through an ANN; you input numbers representing a sequence of words or pixel values.\nAll these floating point numbers need to be stored in a data structure. The most suited structure is multidimensional (to hold several layers of information) and since all data is of the same type, it is an array.\nPython already has several multidimensional array structures—the most popular of which being NumPy’s ndarray—but the particularities of deep learning call for special characteristics: ability to run operations on GPUs and/or in a distributed fashion, as well as the ability to keep track of computation graphs for automatic differentiation.\nThe PyTorch tensor is a Python data structure with these characteristics that can also easily be converted to/from NumPy’s ndarray and integrates well with other Python libraries such as Pandas.\nIn this webinar, suitable for users of all levels, we will have a deep look at this data structure and go much beyond a basic introduction.\nIn particular, we will:\n\nsee how tensors are stored in memory,\nlook at the metadata which allows this efficient memory storage,\ncover the basics of working with tensors (indexing, vectorized operations…),\nmove tensors to/from GPUs,\nconvert tensors to/from NumPy ndarrays,\nsee how tensors work in distributed frameworks,\nsee how linear algebra can be done with PyTorch tensors.\n\n\nSlides (Click and wait: this reveal.js presentation may take a little time to load.)",
    "crumbs": [
      "AI",
      "<b><em>Webinars</em></b>",
      "PyTorch tensors in depth"
    ]
  },
  {
    "objectID": "ai/pt/wb_upscaling.html",
    "href": "ai/pt/wb_upscaling.html",
    "title": "Image upscaling",
    "section": "",
    "text": "Super-resolution—the process of (re)creating high resolution images from low resolution ones—is an old field, but deep neural networks have seen a sudden surge of new and very impressive methods over the past 10 years, from SRCCN to SRGAN to Transformers.\nIn this webinar, I will give a quick overview of these methods and show how the latest state-of-the-art model—SwinIR—performs on a few test images. We will use PyTorch as our framework.\n\nSlides (Click and wait: this reveal.js presentation is heavy and takes some time to load.)",
    "crumbs": [
      "AI",
      "<b><em>Webinars</em></b>",
      "Image upscaling"
    ]
  },
  {
    "objectID": "ai/pt/ws_audio_dataloader.html",
    "href": "ai/pt/ws_audio_dataloader.html",
    "title": "Creating an audio DataLoader",
    "section": "",
    "text": "import torch\nimport torchaudio\nimport matplotlib.pyplot as plt\n\n\n---------------------------------------------------------------------------\nModuleNotFoundError                       Traceback (most recent call last)\nCell In[1], line 2\n      1 import torch\n----&gt; 2 import torchaudio\n      3 import matplotlib.pyplot as plt\n\nModuleNotFoundError: No module named 'torchaudio'",
    "crumbs": [
      "AI",
      "<b><em>Workshops</em></b>",
      "Audio DataLoader with PyTorch"
    ]
  },
  {
    "objectID": "ai/pt/ws_audio_dataloader.html#load-packages",
    "href": "ai/pt/ws_audio_dataloader.html#load-packages",
    "title": "Creating an audio DataLoader",
    "section": "",
    "text": "import torch\nimport torchaudio\nimport matplotlib.pyplot as plt\n\n\n---------------------------------------------------------------------------\nModuleNotFoundError                       Traceback (most recent call last)\nCell In[1], line 2\n      1 import torch\n----&gt; 2 import torchaudio\n      3 import matplotlib.pyplot as plt\n\nModuleNotFoundError: No module named 'torchaudio'",
    "crumbs": [
      "AI",
      "<b><em>Workshops</em></b>",
      "Audio DataLoader with PyTorch"
    ]
  },
  {
    "objectID": "ai/pt/ws_audio_dataloader.html#download-and-unzip-data",
    "href": "ai/pt/ws_audio_dataloader.html#download-and-unzip-data",
    "title": "Creating an audio DataLoader",
    "section": "Download and unzip data",
    "text": "Download and unzip data\nPyTorch comes with many classic datasets.\n\nExamples:\n\nlist of available datasets for vision\nlist of audio datasets\nlist of texts datasets\n\n\nThis is convenient to develop and test your model, or to compare its performance with existing models using these datasets.\nHere, we will use the YESNO dataset which can be accessed through the torchaudio.datasets.YESNO class:\nhelp(torchaudio.datasets.YESNO)\nHelp on class YESNO in module torchaudio.datasets.yesno:\n\nclass YESNO(torch.utils.data.dataset.Dataset)\n\n |  YESNO(root: Union[str, pathlib.Path], url: str =\n |    'http://www.openslr.org/resources/1/waves_yesno.tar.gz', \n |    folder_in_archive: str = 'waves_yesno', \n |    download: bool = False) -&gt; None\n |  \n |  Args:\n |    root (str or Path): Path to the directory where the dataset is found \n |      or downloaded.\n |    url (str, optional): The URL to download the dataset from.\n |      (default: \"http://www.openslr.org/resources/1/waves_yesno.tar.gz\")\n |    folder_in_archive (str, optional):\n |      The top-level directory of the dataset. (default: \"waves_yesno\")\n |    download (bool, optional):\n |      Whether to download the dataset if it is not found at root path. \n |      (default: False).\nThe root argument sets the location of the downloaded data.\n\nWhere to store this data in the cluster\nWe will all use the same data. It would make little sense to all download it in our home directory.\n\nIn the Alliance clusters, a good place to store data shared amongst members of a project is in the /project file system.\nYou usually belong to /project/def-&lt;group&gt;, where &lt;group&gt; is the name of your PI. You can access it from your home directory through the symbolic link ~/projects/def-&lt;group&gt;.\n\nIn our training cluster, we are all part of the group def-sponsor00, accessible through /project/def-sponsor00 (or the hyperlink ~/projects/def-sponsor00).\nWe will thus use ~/projects/def-sponsor00/data as the root argument for torchaudio.datasets.yesno):\nyesno_data = torchaudio.datasets.YESNO(\n    '~/projects/def-sponsor00/data/',\n    download=True)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[2], line 1\n----&gt; 1 yesno_data = torchaudio.datasets.YESNO('/home/marie/projects/def-sponsor00/data/', download=True)\n\nNameError: name 'torchaudio' is not defined",
    "crumbs": [
      "AI",
      "<b><em>Workshops</em></b>",
      "Audio DataLoader with PyTorch"
    ]
  },
  {
    "objectID": "ai/pt/ws_audio_dataloader.html#explore-the-data",
    "href": "ai/pt/ws_audio_dataloader.html#explore-the-data",
    "title": "Creating an audio DataLoader",
    "section": "Explore the data",
    "text": "Explore the data\nA data point in YESNO is a tuple of waveform, sample_rate, and labels (the labels are 1 for “yes” and 0 for “no”).\nLet’s have a look at the first data point:\n\nyesno_data[0]\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[3], line 1\n----&gt; 1 yesno_data[0]\n\nNameError: name 'yesno_data' is not defined\n\n\n\nOr, more nicely:\n\nwaveform, sample_rate, labels = yesno_data[0]\nprint(\"Waveform: {}\\nSample rate: {}\\nLabels: {}\".format(waveform, sample_rate, labels))\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[4], line 1\n----&gt; 1 waveform, sample_rate, labels = yesno_data[0]\n      2 print(\"Waveform: {}\\nSample rate: {}\\nLabels: {}\".format(waveform, sample_rate, labels))\n\nNameError: name 'yesno_data' is not defined\n\n\n\nYou can also plot the data. For this, we will use pyplot from matplotlib.\nLet’s look at the waveform:\n\nplt.figure()\nplt.plot(waveform.t().numpy())\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[5], line 1\n----&gt; 1 plt.figure()\n      2 plt.plot(waveform.t().numpy())\n\nNameError: name 'plt' is not defined",
    "crumbs": [
      "AI",
      "<b><em>Workshops</em></b>",
      "Audio DataLoader with PyTorch"
    ]
  },
  {
    "objectID": "ai/pt/ws_audio_dataloader.html#split-the-data-into-a-training-set-and-a-testing-set",
    "href": "ai/pt/ws_audio_dataloader.html#split-the-data-into-a-training-set-and-a-testing-set",
    "title": "Creating an audio DataLoader",
    "section": "Split the data into a training set and a testing set",
    "text": "Split the data into a training set and a testing set\n\ntrain_size = int(0.8 * len(yesno_data))\ntest_size = len(yesno_data) - train_size\ntrain_dataset, test_dataset = torch.utils.data.random_split(yesno_data, [train_size, test_size])\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[6], line 1\n----&gt; 1 train_size = int(0.8 * len(yesno_data))\n      2 test_size = len(yesno_data) - train_size\n      3 train_dataset, test_dataset = torch.utils.data.random_split(yesno_data, [train_size, test_size])\n\nNameError: name 'yesno_data' is not defined",
    "crumbs": [
      "AI",
      "<b><em>Workshops</em></b>",
      "Audio DataLoader with PyTorch"
    ]
  },
  {
    "objectID": "ai/pt/ws_audio_dataloader.html#create-training-and-testing-dataloaders",
    "href": "ai/pt/ws_audio_dataloader.html#create-training-and-testing-dataloaders",
    "title": "Creating an audio DataLoader",
    "section": "Create training and testing DataLoaders",
    "text": "Create training and testing DataLoaders\nDataLoaders are Python iterables created by the torch.utils.data.DataLoader class from a dataset and a sampler.\nWe already have a dataset (yesno_data). Now we need a sampler (or sampling strategy) to draw samples from it. The sampling strategy contains the batch size, whether the data get shuffled prior to sampling, the number of workers used if the data is loaded in parallel, etc.\nTo create a training DataLoader with shuffled data and batch size of 1 (the default), we run:\n\ntrain_loader = torch.utils.data.DataLoader(train_dataset, shuffle=True)\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[7], line 1\n----&gt; 1 train_loader = torch.utils.data.DataLoader(train_dataset, shuffle=True)\n\nNameError: name 'train_dataset' is not defined\n\n\n\ndata_loader is an iterable of 0.8*60=48 elements (80% of the 60 samples in the YESNO dataset):\n\nlen(train_loader)\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[8], line 1\n----&gt; 1 len(train_loader)\n\nNameError: name 'train_loader' is not defined\n\n\n\nWe do the same to create the testing DataLoader:\n\ntest_loader = torch.utils.data.DataLoader(test_dataset, shuffle=True)\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[9], line 1\n----&gt; 1 test_loader = torch.utils.data.DataLoader(test_dataset, shuffle=True)\n\nNameError: name 'test_dataset' is not defined",
    "crumbs": [
      "AI",
      "<b><em>Workshops</em></b>",
      "Audio DataLoader with PyTorch"
    ]
  },
  {
    "objectID": "ai/sk_intro.html",
    "href": "ai/sk_intro.html",
    "title": "Introduction",
    "section": "",
    "text": "What is scikit-learn and how does it fit in the wide landscape of machine learning frameworks?",
    "crumbs": [
      "AI",
      "<b><em>ML with Scikit-learn</em></b>"
    ]
  },
  {
    "objectID": "ai/sk_intro.html#what-is-machine-learning",
    "href": "ai/sk_intro.html#what-is-machine-learning",
    "title": "Introduction",
    "section": "What is machine learning?",
    "text": "What is machine learning?\nMaybe we should start with some definitions:\n\nArtificial intelligence (AI) can be defined as any human-made system mimicking animal intelligence. This is a large and very diverse field.\nMachine learning (ML) is a subfield of AI that can be defined as computer programs whose performance at a task improves with experience. This learning can be split into three categories:\n\nSupervised learning\nRegression is a form of supervised learning with continuous outputs. Classification is supervised learning with discrete outputs.\nSupervised learning uses training data in the form of example input/output pairs.\nThe goal is to find the relationship between inputs and outputs.\nUnsupervised learning\nClustering, social network analysis, market segmentation, dimensionality reduction (e.g. PCA), anomaly detection … are all forms of unsupervised learning.\nUnsupervised learning uses unlabelled data and the goal is to find patterns within the data.\nReinforcement learning\nThe algorithm explores by performing random actions and these actions are rewarded or punished (bonus points or penalties).\nThis is how algorithms learn to play games.\n\nDeep learning (DL) is itself a subfield of machine learning using deep artificial neural networks.",
    "crumbs": [
      "AI",
      "<b><em>ML with Scikit-learn</em></b>"
    ]
  },
  {
    "objectID": "ai/sk_intro.html#what-is-scikit-learn",
    "href": "ai/sk_intro.html#what-is-scikit-learn",
    "title": "Introduction",
    "section": "What is scikit-learn?",
    "text": "What is scikit-learn?\nScikit-learn (also called sklearn) is a free and open source set of libraries for Python built on top of SciPy (thus using NumPy’s ndarray as the main data structure) used for machine learning.\nSklearn is a rich toolkit characterized by a clean, consistent, and very simple API.\nIt is easy to learn and very well documented. You can think of it as a vast collection of routines that are easy to apply and require little computing power.\n\n\nfrom scikit-learn.org",
    "crumbs": [
      "AI",
      "<b><em>ML with Scikit-learn</em></b>"
    ]
  },
  {
    "objectID": "ai/sk_intro.html#scikit-learn-in-the-ml-landscape",
    "href": "ai/sk_intro.html#scikit-learn-in-the-ml-landscape",
    "title": "Introduction",
    "section": "Scikit-learn in the ML landscape",
    "text": "Scikit-learn in the ML landscape\nThere are many machine learning frameworks. While sklearn has many advantages (ease of use, wide range of tools), it has clear limitations: there is no GPU support and deep learning and reinforcement learning are out of its scope (the only neural network implemented is a multilayer perceptron (an historical, very basic, and terribly inefficient neural network).\nIf you want to access the power and flexibility that deep neural networks provide, you should turn towards tools such as PyTorch (wonderful tool both in academia, research, and industry) or the higher-level Keras (easy API, easy to learn, but less flexible).",
    "crumbs": [
      "AI",
      "<b><em>ML with Scikit-learn</em></b>"
    ]
  },
  {
    "objectID": "ai/top_fl.html",
    "href": "ai/top_fl.html",
    "title": "Deep learning with JAX",
    "section": "",
    "text": "JAX is a fast open source Python library for function transformations (including differentiation) and array computations on accelerators (GPUs/TPUs). These attributes make it ideal for deep learning, but JAX is not, in itself, a deep learning library: it provides a structural framework on which libraries can be built without providing domain-specific tooling.\nTo make full use of JAX’s flexible autodiff and enhanced efficiency for deep learning while maintaining a syntax familiar to PyTorch users, a solid approach consists of using TorchData, TensorFlow Datasets, Grain, or Hugging Face Datasets to load the data, Flax to build neural networks, Optax for optimization, and Orbax for checkpointing.\nThis introductory course does not require any prior knowledge.\n\n Start course ➤",
    "crumbs": [
      "AI",
      "<b><em>Intro DL with JAX</em></b>"
    ]
  },
  {
    "objectID": "ai/top_pt.html",
    "href": "ai/top_pt.html",
    "title": "Getting started with PyTorch",
    "section": "",
    "text": "This introductory course to deep learning and neural networks with the PyTorch framework does not assume any prior knowledge in machine learning. Some basic Python is useful, but not strictly necessary.\n\n Start course ➤",
    "crumbs": [
      "AI",
      "<b><em>Intro DL with PyTorch</em></b>"
    ]
  },
  {
    "objectID": "ai/top_ws.html",
    "href": "ai/top_ws.html",
    "title": "AI workshops",
    "section": "",
    "text": "Audio DataLoader with  \n\n\n\n\nFinding pre-trained models\n\n\n\n\nIntro ML for the humanities\n\n\n\n\nQuick intro to DL, NLP, and LLMs",
    "crumbs": [
      "AI",
      "<b><em>Workshops</em></b>"
    ]
  },
  {
    "objectID": "ai/wb_copilot_slides.html#codex",
    "href": "ai/wb_copilot_slides.html#codex",
    "title": "AI-powered programming with",
    "section": "Codex",
    "text": "Codex\nOpenAI Codex—based on GPT-3—is the model behind GitHub Copilot\nAll the big corporate companies are rushing to launch a growing number of similar (and not free, not open source) productivity products (e.g. tabnine, Microsoft Visual Studio IntelliCode, Amazon CodeWhisperer)\nThese products generate code in a narrow context (auto-completion or transformation of natural language to code or vise-versa)"
  },
  {
    "objectID": "ai/wb_copilot_slides.html#alphacode-2",
    "href": "ai/wb_copilot_slides.html#alphacode-2",
    "title": "AI-powered programming with",
    "section": "AlphaCode 2",
    "text": "AlphaCode 2\nGoogle DeepMind AlphaCode 2—based on Gemini—stands out as a totally different (and for now totally unavailable) product generating code at the level of competitive programming (reaching the 85th percentile)\nThink of it as code evolution by “natural” selection:\n\na very large number of code samples are generated (think “mutations”)\na filtering and scoring system selects for the best candidates (that’s the selection part)\n\nAlphaCode 2 is able to solve much more open-ended problems"
  },
  {
    "objectID": "ai/wb_copilot_slides.html#what-about-foss",
    "href": "ai/wb_copilot_slides.html#what-about-foss",
    "title": "AI-powered programming with",
    "section": "What about FOSS?",
    "text": "What about FOSS?\nFree\nThese models are large and most convenient to run on servers\n → Price of cloud service\nSome self-hosted options exist. A very promising one is Tabby. Not practical for everyone\nOpen source\nWhile these models feed from open source code, they are themselves not open source 🙁\nThe open source community is trying to provide open source alternatives (e.g. Tabby). Despite the much more limited resources, the performance of some of these alternatives is very good"
  },
  {
    "objectID": "ai/wb_copilot_slides.html#what-is-github-copilot",
    "href": "ai/wb_copilot_slides.html#what-is-github-copilot",
    "title": "AI-powered programming with",
    "section": "What is GitHub Copilot?",
    "text": "What is GitHub Copilot?\n\n → Cloud-hosted AI programming assistant\n\n\nDeveloped by GitHub (Microsoft)\nRunning Codex, a model by OpenAI derived from the LLM GPT-3 and trained on open source code"
  },
  {
    "objectID": "ai/wb_copilot_slides.html#access",
    "href": "ai/wb_copilot_slides.html#access",
    "title": "AI-powered programming with",
    "section": "Access",
    "text": "Access\nIndividual or organization GitHub accounts\nRequires subscription\nStudents, teachers, and maintainers of popular open source projects can apply for free access"
  },
  {
    "objectID": "ai/wb_copilot_slides.html#safety",
    "href": "ai/wb_copilot_slides.html#safety",
    "title": "AI-powered programming with",
    "section": "Safety",
    "text": "Safety\nFilters are in place for offensive words, but…\nGenerated code comes with no guaranty of safety or quality\nA lawsuit is open against GitHub Copilot for licenses violation"
  },
  {
    "objectID": "ai/wb_copilot_slides.html#supported-languages",
    "href": "ai/wb_copilot_slides.html#supported-languages",
    "title": "AI-powered programming with",
    "section": "Supported languages",
    "text": "Supported languages\nAny language used in public repos\nQuality of suggestions is higher for languages with lots of data"
  },
  {
    "objectID": "ai/wb_copilot_slides.html#how-to-use-it",
    "href": "ai/wb_copilot_slides.html#how-to-use-it",
    "title": "AI-powered programming with",
    "section": "How to use it?",
    "text": "How to use it?\n\nStart typing code and get autocomplete suggestions\n\n\nWrite comments describing what the code should do and get code generation based on context\n\n\nIt is easy to:\n  → accept suggestions word by word\n  → line by line\n  → for entire functions\n  → cycle through different suggestions"
  },
  {
    "objectID": "ai/wb_copilot_slides.html#interface",
    "href": "ai/wb_copilot_slides.html#interface",
    "title": "AI-powered programming with",
    "section": "Interface",
    "text": "Interface\nExtensions to text editors:\n  → Visual Studio Code/Visual Studio\n  → Vim/Neovim/Emacs\n  → JetBrains IDEs\n  → Azure Data Studio"
  },
  {
    "objectID": "ai/wb_copilot_slides.html#get-a-subscription",
    "href": "ai/wb_copilot_slides.html#get-a-subscription",
    "title": "AI-powered programming with",
    "section": "Get a subscription",
    "text": "Get a subscription\nGo to your GitHub account page\n  → Settings\n  → Copilot\n  → Enable\nProvide free access or payment method\nSet settings"
  },
  {
    "objectID": "ai/wb_copilot_slides.html#vs-code",
    "href": "ai/wb_copilot_slides.html#vs-code",
    "title": "AI-powered programming with",
    "section": "VS Code",
    "text": "VS Code\n\nInstall the GitHub Copilot extension\n\n\nNext suggestion:       Alt+]\nPrevious suggestion:      Alt+[\nReject suggestion:       Esc\nAccept suggestion:      Tab\nAccept next suggested word: Ctrl+→  (Command+→  for macOS)\nSet your own key binding for editor.action.inlineSuggest.acceptNextLine to accept next suggested line\nOpen new tab with options:  Ctrl+Enter\nYou can also hover over suggestions"
  },
  {
    "objectID": "ai/wb_copilot_slides.html#vimneovim",
    "href": "ai/wb_copilot_slides.html#vimneovim",
    "title": "AI-powered programming with",
    "section": "Vim/Neovim",
    "text": "Vim/Neovim\nInstall Node.js\nClone https://github.com/github/copilot.vim\nConfigure:\n:Copilot setup\nEnable:\n:Copilot enable\nGet help:\n:help copilot"
  },
  {
    "objectID": "ai/wb_copilot_slides.html#emacs",
    "href": "ai/wb_copilot_slides.html#emacs",
    "title": "AI-powered programming with",
    "section": "Emacs",
    "text": "Emacs\nInstall Node.js\nAssuming straight is installed:\n(straight-use-package 'editorconfig)                   ; Copilot dependency\n\n(use-package copilot\n    :straight (:host github\n                     :repo \"copilot-emacs/copilot.el\"\n                     :files (\"dist\" \"*.el\"))\n    :hook (prog-mode . copilot-mode)                   ; Settings up to you\n    :bind ((\"C-8\" . copilot-complete)\n           :map copilot-completion-map\n           (\"C-j\" . copilot-accept-completion)\n           (\"C-f\" . copilot-accept-completion-by-word)\n           (\"C-t\" . copilot-accept-completion-by-line)\n           (\"C-n\" . copilot-next-completion)\n           (\"C-p\" . copilot-previous-completion)))\nLogin to your GitHub account (only needs to be done once): M-x copilot-login"
  },
  {
    "objectID": "ai/wb_copilot_slides.html#what-is-copilot-in-the-cli",
    "href": "ai/wb_copilot_slides.html#what-is-copilot-in-the-cli",
    "title": "AI-powered programming with",
    "section": "What is Copilot in the CLI?",
    "text": "What is Copilot in the CLI?\nIn beta\nAn extension to GitHub CLI (GitHub operations from the CLI)\n → Generate commands from natural language\n → Generate natural language explanations from commands\nTrained on data up to 2021\nLower performance for natural languages ≠ English\nBe very careful: the command line is powerful and you can delete your data or mess up your system if you don’t know what you are doing. Check commands carefully!"
  },
  {
    "objectID": "ai/wb_copilot_slides.html#setup-1",
    "href": "ai/wb_copilot_slides.html#setup-1",
    "title": "AI-powered programming with",
    "section": "Setup",
    "text": "Setup\n\nInstall GitHub CLI\nConnect to your GitHub account:\ngh auth login\n\n\n Install Copilot in the CLI:\ngh extension install github/gh-copilot\n\nUpdate with: gh extension upgrade gh-copilot"
  },
  {
    "objectID": "ai/wb_copilot_slides.html#usage",
    "href": "ai/wb_copilot_slides.html#usage",
    "title": "AI-powered programming with",
    "section": "Usage",
    "text": "Usage\nGet code explanations:\ngh copilot explain\n Get code from natural language:\ngh copilot suggest"
  },
  {
    "objectID": "ai/wb_copilot_slides.html#resources",
    "href": "ai/wb_copilot_slides.html#resources",
    "title": "AI-powered programming with",
    "section": "Resources",
    "text": "Resources\nGitHub support portal\nGitHub Copilot documentation\nStack Overflow [github-copilot] tag\ncopilot.el (unofficial Emacs plug-in)"
  },
  {
    "objectID": "ai/wb_dvc_slides.html#on-version-control",
    "href": "ai/wb_dvc_slides.html#on-version-control",
    "title": "Version control for data science & machine learning with DVC",
    "section": "On version control",
    "text": "On version control\nI won’t introduce here the benefits of using a good version control system such as Git\n\n\n\nOn the benefits of VCS"
  },
  {
    "objectID": "ai/wb_dvc_slides.html#extending-git-for-data",
    "href": "ai/wb_dvc_slides.html#extending-git-for-data",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Extending Git for data",
    "text": "Extending Git for data\nWhile Git is a wonderful tool for text files versioning (code, writings in markup formats), it isn’t a tool to manage changes to datasets\nSeveral open source tools—each with a different structure and functioning—extend Git capabilities to track data: Git LFS, git-annex, lakeFS, Dolt, DataLad"
  },
  {
    "objectID": "ai/wb_dvc_slides.html#extending-git-for-models-and-experiments",
    "href": "ai/wb_dvc_slides.html#extending-git-for-models-and-experiments",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Extending Git for models and experiments",
    "text": "Extending Git for models and experiments\nReproducible research and collaboration on data science and machine learning projects involve more than datasets management:\nExperiments and the models they produce also need to be tracked"
  },
  {
    "objectID": "ai/wb_dvc_slides.html#many-moving-parts",
    "href": "ai/wb_dvc_slides.html#many-moving-parts",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Many moving parts",
    "text": "Many moving parts\n\n*hp = hyperparameter\n\n\n\n\n\n\n\n\n\n\ndata1\n\ndata1\n\n\n\nmodel1\n\nmodel1\n\n\n\ndata1-&gt;model1\n\n\n\n\n\nmodel2\n\nmodel2\n\n\n\ndata1-&gt;model2\n\n\n\n\n\nmodel3\n\nmodel3\n\n\n\ndata1-&gt;model3\n\n\n\n\n\ndata2\n\ndata2\n\n\n\ndata2-&gt;model1\n\n\n\n\n\ndata2-&gt;model2\n\n\n\n\n\ndata2-&gt;model3\n\n\n\n\n\ndata3\n\ndata3\n\n\n\ndata3-&gt;model1\n\n\n\n\n\ndata3-&gt;model2\n\n\n\n\n\ndata3-&gt;model3\n\n\n\n\n\nhp1\n\nhp1\n\n\n\nhp1-&gt;model1\n\n\n\n\n\nhp1-&gt;model2\n\n\n\n\n\nhp1-&gt;model3\n\n\n\n\n\nhp2\n\nhp2\n\n\n\nhp2-&gt;model1\n\n\n\n\n\nhp2-&gt;model2\n\n\n\n\n\nhp2-&gt;model3\n\n\n\n\n\nhp3\n\nhp3\n\n\n\nhp3-&gt;model1\n\n\n\n\n\nhp3-&gt;model2\n\n\n\n\n\nhp3-&gt;model3\n\n\n\n\n\nperformance\n\nperformance1 ... performance27\n\n\n\nmodel1-&gt;performance\n\n\n\n\n\nmodel1-&gt;performance\n\n\n\n\n\nmodel1-&gt;performance\n\n\n\n\n\nmodel1-&gt;performance\n\n\n\n\n\nmodel1-&gt;performance\n\n\n\n\n\nmodel1-&gt;performance\n\n\n\n\n\nmodel1-&gt;performance\n\n\n\n\n\nmodel1-&gt;performance\n\n\n\n\n\nmodel1-&gt;performance\n\n\n\n\n\nmodel2-&gt;performance\n\n\n\n\n\nmodel2-&gt;performance\n\n\n\n\n\nmodel2-&gt;performance\n\n\n\n\n\nmodel2-&gt;performance\n\n\n\n\n\nmodel2-&gt;performance\n\n\n\n\n\nmodel2-&gt;performance\n\n\n\n\n\nmodel2-&gt;performance\n\n\n\n\n\nmodel2-&gt;performance\n\n\n\n\n\nmodel2-&gt;performance\n\n\n\n\n\nmodel3-&gt;performance\n\n\n\n\n\nmodel3-&gt;performance\n\n\n\n\n\nmodel3-&gt;performance\n\n\n\n\n\nmodel3-&gt;performance\n\n\n\n\n\nmodel3-&gt;performance\n\n\n\n\n\nmodel3-&gt;performance\n\n\n\n\n\nmodel3-&gt;performance\n\n\n\n\n\nmodel3-&gt;performance\n\n\n\n\n\nmodel3-&gt;performance\n\n\n\n\n\n\n\n\n\n\n\nHow did we get performance17 again? 🤯"
  },
  {
    "objectID": "ai/wb_dvc_slides.html#dvc-principles",
    "href": "ai/wb_dvc_slides.html#dvc-principles",
    "title": "Version control for data science & machine learning with DVC",
    "section": "DVC principles",
    "text": "DVC principles\nLarge files (datasets, models…) are kept outside Git\nEach large file or directory put under DVC tracking has an associated .dvc file\nGit only tracks the .dvc files (metadata)\n\nWorkflows can be tracked for collaboration and reproducibility\n\n\nDVC functions as a Makefile and allows to only rerun what is necessary"
  },
  {
    "objectID": "ai/wb_dvc_slides.html#installation",
    "href": "ai/wb_dvc_slides.html#installation",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Installation",
    "text": "Installation\nFor Linux (other OSes, refer to the doc):\n\npip:\npip install dvc\nconda\npipx (if you want dvc available everywhere without having to activate virtual envs):\npipx install dvc\n\n\nOptional dependencies [s3], [gdrive], etc. for remote storage"
  },
  {
    "objectID": "ai/wb_dvc_slides.html#how-to-run",
    "href": "ai/wb_dvc_slides.html#how-to-run",
    "title": "Version control for data science & machine learning with DVC",
    "section": "How to run",
    "text": "How to run\n\nTerminal\ndvc ...\nVS Code extension\nPython library if installed via pip or conda\nimport dvc.api\n\n\nIn this webinar, I will use DVC through the command line"
  },
  {
    "objectID": "ai/wb_dvc_slides.html#acknowledgements",
    "href": "ai/wb_dvc_slides.html#acknowledgements",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nCode and data for this webinar modified from:\n\nReal Python\nDataLad handbook\nDVC documentation"
  },
  {
    "objectID": "ai/wb_dvc_slides.html#the-project",
    "href": "ai/wb_dvc_slides.html#the-project",
    "title": "Version control for data science & machine learning with DVC",
    "section": "The project",
    "text": "The project\ntree -L 3\n├── LICENSE\n├── data\n│   ├── prepared\n│   └── raw\n│       ├── train\n│       └── val\n├── metrics\n├── model\n├── requirements.txt\n└── src\n    ├── evaluate.py\n    ├── prepare.py\n    └── train.py"
  },
  {
    "objectID": "ai/wb_dvc_slides.html#initialize-git-repo",
    "href": "ai/wb_dvc_slides.html#initialize-git-repo",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Initialize Git repo",
    "text": "Initialize Git repo\ngit init\nInitialized empty Git repository in dvc/.git/\n\nThis creates the .git directory\n\n\ngit status\nOn branch main\n\nNo commits yet\n\nUntracked files:\n    LICENSE\n    data/\n    requirements.txt\n    src/"
  },
  {
    "objectID": "ai/wb_dvc_slides.html#initialize-dvc-project",
    "href": "ai/wb_dvc_slides.html#initialize-dvc-project",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Initialize DVC project",
    "text": "Initialize DVC project\ndvc init\nInitialized DVC repository.\n\nYou can now commit the changes to git.\n\nYou will also see a note about usage analytics collection and info on how to opt out\n\n\nA .dvc directory and a .dvcignore file got created"
  },
  {
    "objectID": "ai/wb_dvc_slides.html#commit-dvc-system-files",
    "href": "ai/wb_dvc_slides.html#commit-dvc-system-files",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Commit DVC system files",
    "text": "Commit DVC system files\n\nDVC automatically staged its system file for us:\ngit status\nOn branch main\n\nNo commits yet\n\nChanges to be committed:\n    new file:   .dvc/.gitignore\n    new file:   .dvc/config\n    new file:   .dvcignore\n\nUntracked files:\n    LICENSE\n    data/\n    requirements.txt\n    src/\n\nSo we can directly commit:\ngit commit -m \"Initialize DVC\""
  },
  {
    "objectID": "ai/wb_dvc_slides.html#prepare-repo",
    "href": "ai/wb_dvc_slides.html#prepare-repo",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Prepare repo",
    "text": "Prepare repo\nLet’s work in a virtual environment:\n# Create venv and add to .gitignore\npython -m venv venv && echo venv &gt; .gitignore\n\n# Activate venv\nsource venv/bin/activate\n\n# Update pip\npython -m pip install --upgrade pip\n\n# Install packages needed\npython -m pip install -r requirements.txt"
  },
  {
    "objectID": "ai/wb_dvc_slides.html#clean-working-tree",
    "href": "ai/wb_dvc_slides.html#clean-working-tree",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Clean working tree",
    "text": "Clean working tree\ngit add .gitignore LICENSE requirements.txt\ngit commit -m \"Add general files\"\ngit add src\ngit commit -m \"Add scripts\"\ngit status\nOn branch main\nUntracked files:\n    data/\n\n\nNow, it is time to deal with the data"
  },
  {
    "objectID": "ai/wb_dvc_slides.html#put-data-under-dvc-tracking",
    "href": "ai/wb_dvc_slides.html#put-data-under-dvc-tracking",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Put data under DVC tracking",
    "text": "Put data under DVC tracking\nWe are still not tracking any data:\ndvc status\nThere are no data or pipelines tracked in this project yet.\nYou can choose what to track as a unit (i.e. each picture individually, the whole data directory as a unit)\nLet’s break it down by set:\ndvc add data/raw/train\ndvc add data/raw/val"
  },
  {
    "objectID": "ai/wb_dvc_slides.html#section",
    "href": "ai/wb_dvc_slides.html#section",
    "title": "Version control for data science & machine learning with DVC",
    "section": "",
    "text": "This adds data to .dvc/cache/files and created 3 files in data/raw:\n\n.gitignore\ntrain.dvc\nval.dvc\n\nThe .gitignore tells Git not to track the data:\ncat data/raw/.gitignore\n/train\n/val\nThe .dvc files contain the metadata for the cached directories"
  },
  {
    "objectID": "ai/wb_dvc_slides.html#tracked-data",
    "href": "ai/wb_dvc_slides.html#tracked-data",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Tracked data",
    "text": "Tracked data\nWe are all good:\ndvc status\nData and pipelines are up to date."
  },
  {
    "objectID": "ai/wb_dvc_slides.html#data-deduplication",
    "href": "ai/wb_dvc_slides.html#data-deduplication",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Data (de)duplication",
    "text": "Data (de)duplication\nLink between checked-out version of a file/directory and the cache:\n\nCache ⟷ working directory\n\n\n\n\n\n\n\n\nDuplication\nEditable\n\n\n\n\nReflinks*\nOnly when needed\nYes\n\n\nHardlinks/Symlinks\nNo\nNo\n\n\nCopies\nYes\nYes\n\n\n\n*Reflinks only available for a few file systems (Btrfs, XFS, OCFS2, or APFS)"
  },
  {
    "objectID": "ai/wb_dvc_slides.html#commit-the-metafiles",
    "href": "ai/wb_dvc_slides.html#commit-the-metafiles",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Commit the metafiles",
    "text": "Commit the metafiles\nThe metafiles should be put under Git version control\n\nYou can configure DVC to automatically stage its newly created system files:\ndvc config [--system] [--global] core.autostage true\n\nYou can then commit directly:\ngit commit -m \"Initial version of data\"\ngit status\nOn branch main\nnothing to commit, working tree clean"
  },
  {
    "objectID": "ai/wb_dvc_slides.html#track-changes-to-the-data",
    "href": "ai/wb_dvc_slides.html#track-changes-to-the-data",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Track changes to the data",
    "text": "Track changes to the data\nLet’s make some change to the data:\nrm data/raw/val/n03445777/ILSVRC2012_val*\n\nRemember that Git is not tracking the data:\ngit status\nOn branch main\nnothing to commit, working tree clean\n\n\nBut DVC is:\ndvc status\ndata/raw/val.dvc:\n    changed outs:\n            modified:           data/raw/val"
  },
  {
    "objectID": "ai/wb_dvc_slides.html#add-changes-to-dvc",
    "href": "ai/wb_dvc_slides.html#add-changes-to-dvc",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Add changes to DVC",
    "text": "Add changes to DVC\ndvc add data/raw/val\ndvc status\nData and pipelines are up to date.\n\nNow we need to commit the changes to the .dvc file to Git:\ngit status\nOn branch main\nChanges to be committed:\n    modified:   data/raw/val.dvc\n\nStaging happened automatically because I have set the autostage option to true on my system\n\ngit commit -m \"Delete data/raw/val/n03445777/ILSVRC2012_val*\""
  },
  {
    "objectID": "ai/wb_dvc_slides.html#check-out-older-versions",
    "href": "ai/wb_dvc_slides.html#check-out-older-versions",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Check out older versions",
    "text": "Check out older versions\nWhat if we want to go back to the 1st version of our data?\nFor this, we first use Git to checkout the proper commit, then run dvc checkout to have the data catch up to the .dvc file\nTo avoid forgetting to run the commands that will make DVC catch up to Git, we can automate this process by installing Git hooks:\ndvc install"
  },
  {
    "objectID": "ai/wb_dvc_slides.html#git-workflows",
    "href": "ai/wb_dvc_slides.html#git-workflows",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Git workflows",
    "text": "Git workflows\ngit checkout is ok to have a look, but a detached HEAD is not a good place to create new commits\nLet’s create a new branch and switch to it:\ngit switch -c alternative\nSwitched to a new branch 'alternative'\nGoing back and forth between both versions of our data is now as simple as switching branch:\ngit switch main\ngit switch alternative"
  },
  {
    "objectID": "ai/wb_dvc_slides.html#classic-workflow",
    "href": "ai/wb_dvc_slides.html#classic-workflow",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Classic workflow",
    "text": "Classic workflow\nThe Git project (including .dvc files) go to a Git remote (GitHub/GitLab/Bitbucket/server)\nThe data go to a DVC remote (AWS/Azure/Google Drive/server/etc.)"
  },
  {
    "objectID": "ai/wb_dvc_slides.html#dvc-remotes",
    "href": "ai/wb_dvc_slides.html#dvc-remotes",
    "title": "Version control for data science & machine learning with DVC",
    "section": "DVC remotes",
    "text": "DVC remotes\nDVC can use many cloud storage or remote machines/server via SSH, WebDAV, etc.\nLet’s create a local remote here:\n# Create a directory outside the project\nmkdir ../remote\n\n# Setup default (-d) remote\ndvc remote add -d local_remote ../remote\nSetting 'local_remote' as a default remote.\ncat .dvc/config\n[core]\n    remote = local_remote\n['remote \"local_remote\"']\n    url = ../../remote"
  },
  {
    "objectID": "ai/wb_dvc_slides.html#commit-remote-config",
    "href": "ai/wb_dvc_slides.html#commit-remote-config",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Commit remote config",
    "text": "Commit remote config\nThe new remote configuration should be committed:\ngit status\nOn branch alternative\n\nChanges not staged for commit:\n    modified:   .dvc/config\ngit add .\ngit commit -m \"Config remote\""
  },
  {
    "objectID": "ai/wb_dvc_slides.html#push-to-remotes",
    "href": "ai/wb_dvc_slides.html#push-to-remotes",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Push to remotes",
    "text": "Push to remotes\nLet’s push the data from the cache (.dvc/cache) to the remote:\ndvc push\n2702 files pushed\n\nWith Git hooks installed, dvc push is automatically run after git push\n(But the data is pushed to the DVC remote while the files tracked by Git get pushed to the Git remote)\n\nBy default, the entire data cache gets pushed to the remote, but there are many options\n\nExample: only push data corresponding to a certain .dvc files\ndvc push data/raw/val.dvc"
  },
  {
    "objectID": "ai/wb_dvc_slides.html#pull-from-remotes",
    "href": "ai/wb_dvc_slides.html#pull-from-remotes",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Pull from remotes",
    "text": "Pull from remotes\ndvc fetch downloads data from the remote into the cache. To have it update the working directory, follow by dvc checkout\nYou can do these 2 commands at the same time with dvc pull"
  },
  {
    "objectID": "ai/wb_dvc_slides.html#dvc-pipelines",
    "href": "ai/wb_dvc_slides.html#dvc-pipelines",
    "title": "Version control for data science & machine learning with DVC",
    "section": "DVC pipelines",
    "text": "DVC pipelines\nDVC pipelines create reproducible workflows and are functionally similar to Makefiles\nEach step in a pipeline is created with dvc stage add and add an entry to a dvc.yaml file\n\ndvc stage add options:\n-n: name of stage\n-d: dependency\n-o: output\n\n\nEach stage contains:\n\ncmd: the command executed\ndeps: the dependencies\nouts: the outputs\n\nThe file is then used to visualize the pipeline and run it"
  },
  {
    "objectID": "ai/wb_dvc_slides.html#example",
    "href": "ai/wb_dvc_slides.html#example",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Example",
    "text": "Example\nLet’s create a pipeline to run a classifier on our data\nThe pipeline contains 3 steps:\n\nprepare\ntrain\nevaluate"
  },
  {
    "objectID": "ai/wb_dvc_slides.html#create-a-pipeline",
    "href": "ai/wb_dvc_slides.html#create-a-pipeline",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Create a pipeline",
    "text": "Create a pipeline\n1st stage (data preparation):\ndvc stage add -n prepare -d src/prepare.py -d data/raw \\\n    -o data/prepared/train.csv -o data/prepared/test.csv \\\n    python src/prepare.py\nAdded stage 'prepare' in 'dvc.yaml'\n\n2nd stage (training)\ndvc stage add -n train -d src/train.py -d data/prepared/train.csv \\\n    -o model/model.joblib \\\n    python src/train.py\nAdded stage `train` in 'dvc.yaml'\n\n\n3rd stage (evaluation)\ndvc stage add -n evaluate -d src/evaluate.py -d model/model.joblib \\\n    -M metrics/accuracy.json \\\n    python src/evaluate.py\nAdded stage `evaluate` in 'dvc.yaml'"
  },
  {
    "objectID": "ai/wb_dvc_slides.html#commit-pipeline",
    "href": "ai/wb_dvc_slides.html#commit-pipeline",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Commit pipeline",
    "text": "Commit pipeline\ngit commit -m \"Define pipeline\"\nprepare:\n    changed deps:\n            modified:           data/raw\n            modified:           src/prepare.py\n    changed outs:\n            deleted:            data/prepared/test.csv\n            deleted:            data/prepared/train.csv\ntrain:\n    changed deps:\n            deleted:            data/prepared/train.csv\n            modified:           src/train.py\n    changed outs:\n            deleted:            model/model.joblib\nevaluate:\n    changed deps:\n            deleted:            model/model.joblib\n            modified:           src/evaluate.py\n    changed outs:\n            deleted:            metrics/accuracy.json\n[main 4aa331b] Define pipeline\n 3 files changed, 27 insertions(+)\n create mode 100644 data/prepared/.gitignore\n create mode 100644 dvc.yaml\n create mode 100644 model/.gitignore"
  },
  {
    "objectID": "ai/wb_dvc_slides.html#visualize-pipeline-in-a-dag",
    "href": "ai/wb_dvc_slides.html#visualize-pipeline-in-a-dag",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Visualize pipeline in a DAG",
    "text": "Visualize pipeline in a DAG\ndvc dag\n+--------------------+         +------------------+\n| data/raw/train.dvc |         | data/raw/val.dvc |\n+--------------------+         +------------------+\n                  ***           ***\n                     **       **\n                       **   **\n                    +---------+\n                    | prepare |\n                    +---------+\n                          *\n                          *\n                          *\n                      +-------+\n                      | train |\n                      +-------+\n                          *\n                          *\n                          *\n                    +----------+\n                    | evaluate |\n                    +----------+"
  },
  {
    "objectID": "ai/wb_dvc_slides.html#run-pipeline",
    "href": "ai/wb_dvc_slides.html#run-pipeline",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Run pipeline",
    "text": "Run pipeline\ndvc repro\n'data/raw/train.dvc' didn't change, skipping\n'data/raw/val.dvc' didn't change, skipping\nRunning stage 'prepare':\n&gt; python src/prepare.py\nGenerating lock file 'dvc.lock'\nUpdating lock file 'dvc.lock'\n\nRunning stage 'train':\n&gt; python src/train.py\nUpdating lock file 'dvc.lock'\n\nRunning stage 'evaluate':\n&gt; python src/evaluate.py\nUpdating lock file 'dvc.lock'\nUse `dvc push` to send your updates to remote storage."
  },
  {
    "objectID": "ai/wb_dvc_slides.html#dvc-repro-breakdown",
    "href": "ai/wb_dvc_slides.html#dvc-repro-breakdown",
    "title": "Version control for data science & machine learning with DVC",
    "section": "dvc repro breakdown",
    "text": "dvc repro breakdown\n\ndvc repro runs the dvc.yaml file in a Makefile fashion\nFirst, it looks at the dependencies: the data didn’t change\nThen it ran the commands to produce the outputs (since it is our first run, we had no outputs)\nWhen the 1st stage is run, a dvc.lock is created with information on that part of the run\nWhen the 2nd and 3rd stages are run, dvc.lock is updated\nAt the end of the run dvc.lock contains all the info about the run we just did (version of the data used, etc.)\nA new directory called runs is created in .dvc/cache with cached data for this run"
  },
  {
    "objectID": "ai/wb_dvc_slides.html#results-of-the-run",
    "href": "ai/wb_dvc_slides.html#results-of-the-run",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Results of the run",
    "text": "Results of the run\n\nThe prepared data was created in data/prepared (with a .gitignore to exclude it from Git—you don’t want to track results in Git, but the scripts that can reproduce them)\nA model was saved in model (with another .gitignore file)\nThe accuracy of this run was created in metrics"
  },
  {
    "objectID": "ai/wb_dvc_slides.html#clean-working-tree-1",
    "href": "ai/wb_dvc_slides.html#clean-working-tree-1",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Clean working tree",
    "text": "Clean working tree\nNow, we definitely want to create a commit with the dvc.lock\nWe could add the metrics resulting from this run in the same commit:\ngit add metrics\ngit commit -m \"First pipeline run and results\"\n\nOur working tree is now clean and our data/pipeline up to date:\ngit status\nOn branch alternative\nnothing to commit, working tree clean\ndvc status\nData and pipelines are up to date."
  },
  {
    "objectID": "ai/wb_dvc_slides.html#modify-pipeline",
    "href": "ai/wb_dvc_slides.html#modify-pipeline",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Modify pipeline",
    "text": "Modify pipeline\nFrom now on, if we edit one of the scripts, or one of the dependencies, dvc status will tell us what changed and dvc repro will only rerun the parts of the pipeline to update the result, pretty much as a Makefile would"
  },
  {
    "objectID": "ai/wb_dvc_slides.html#going-further-next-time",
    "href": "ai/wb_dvc_slides.html#going-further-next-time",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Going further … next time",
    "text": "Going further … next time\n DVC is a sophisticated tool with many additional features:\n\nCreation of data registries\nDVCLive\nA Python library to log experiment metrics\nVisualize the performance logs as plots\nContinuous integration\nWith the sister project CML (Continuous Machine Learning)"
  },
  {
    "objectID": "ai/wb_frameworks.html",
    "href": "ai/wb_frameworks.html",
    "title": "A map of current machine learning frameworks",
    "section": "",
    "text": "We are in a period of active development of new deep learning techniques, adding to the already mature area of traditional machine learning. This is leading to a vast and ever evolving field of implementations which can be disorienting.\nIn this webinar, I will guide you through a map of the current frameworks, organizing them based on their domain (machine learning vs deep learning) and the languages required to use them. I will also talk about the various automatic differentiation options available.\nTo narrow such a large topic, I am limiting the map to frameworks that can be used from Python, Julia, and R.\n\nSlides (Click and wait: this reveal.js presentation is heavy and takes some time to load.)",
    "crumbs": [
      "AI",
      "<b><em>Webinars</em></b>",
      "Map of current ML frameworks"
    ]
  },
  {
    "objectID": "ai/ws_dl_nlp_llm.html",
    "href": "ai/ws_dl_nlp_llm.html",
    "title": "A quick introduction to deep learning, NLP, and LLMs",
    "section": "",
    "text": "◦ How does deep learning really work?\n◦ What exactly are those large language models everybody talks about?\n◦ How can I build a neural network?\n◦ What is NLP?\nThis presentation will answer these questions in a non-technical manner to give you a high-level understanding of a discipline that has become crucial in all fields of research.\n\nSlides (Click and wait: the presentation might take a few instants to load)",
    "crumbs": [
      "AI",
      "<b><em>Workshops</em></b>",
      "Quick intro to DL, NLP, and LLMs"
    ]
  },
  {
    "objectID": "ai/ws_hss_intro.html",
    "href": "ai/ws_hss_intro.html",
    "title": "Introduction to machine learning for the humanities",
    "section": "",
    "text": "We hear about it all the time, but what really is machine learning? And what about deep learning? Neural networks?? How can any of this help me with my work? And how? Which tools do I need to make use of the transformative advances happening in that field??\nThis workshop will answer these questions in a non-technical manner to give you a high level overview of a discipline that has become crucial in all fields of research.\n\nSlides (Click and wait: the presentation might take a few instants to load)",
    "crumbs": [
      "AI",
      "<b><em>Workshops</em></b>",
      "Intro ML for the humanities"
    ]
  },
  {
    "objectID": "bash/index.html",
    "href": "bash/index.html",
    "title": "Unix shells",
    "section": "",
    "text": "Getting started with  and  \nA course in Bash and Zsh\n\n\n\n\nWorkshops\nVarious Unix shell topics\n\n\n\n\n\n\n60 min webinars\nVarious Unix shell topics",
    "crumbs": [
      "Bash/Zsh",
      "<br>&nbsp;<img src=\"img/logo_bash.png\" class=\"img-fluid\" style=\"width:1.6em\" alt=\"noshadow\"> &nbsp;/ &nbsp;<img src=\"img/logo_zsh.svg\" class=\"img-fluid\" style=\"width:1.3em\" alt=\"noshadow\"><br><br>"
    ]
  },
  {
    "objectID": "bash/intro_basics.html",
    "href": "bash/intro_basics.html",
    "title": "Shell basics",
    "section": "",
    "text": "What does it feel like to work in a shell?\nHere is a first basic orientation.",
    "crumbs": [
      "Bash/Zsh",
      "<b><em>Getting started</em></b>",
      "Shell basics"
    ]
  },
  {
    "objectID": "bash/intro_basics.html#the-prompt",
    "href": "bash/intro_basics.html#the-prompt",
    "title": "Shell basics",
    "section": "The prompt",
    "text": "The prompt\nIn command-line interfaces, a command prompt is a sequence of characters indicating that the interpreter is ready to accept input. It can also provide some information (e.g. time, error types, username, hostname, etc.)\nThe Bash prompt is customizable. By default, it gives the username and the hostname, and it ends with the dollar sign ($).",
    "crumbs": [
      "Bash/Zsh",
      "<b><em>Getting started</em></b>",
      "Shell basics"
    ]
  },
  {
    "objectID": "bash/intro_basics.html#commands",
    "href": "bash/intro_basics.html#commands",
    "title": "Shell basics",
    "section": "Commands",
    "text": "Commands\nBash comes with a number of commands: directives to the shell to perform particular tasks.\n\nExamples of commands:\n\n\nPrint working directory: pwd\nChange directory: cd\nPrint: echo\nPrint content of a file: cat\nList files and directories in working directory: ls\nCopy: cp\nMove or rename: mv\nCreate a new directory: mkdir\nCreate a new file: touch\n\nTo execute a command, you type it, then press the &lt;enter&gt;.\n\n\nYour turn:\n\nRun your first command:\nls\n\n\nCommand options\nCommands come with a number of flags (options).\nSome flags are short (a single hyphen followed by a single letter), others are long (two hyphens followed by a word or a series of words separated by hyphens). Some flags have both a long and short forms (in which case, both are totally equivalent).\n\nExamples of flags for the ls command:\n\n\nList all files and directories (not ignoring hidden files): ls -a or ls --all\nList files and directories in a long format: ls -l\nList files and directories in a human readable format (using units such as K, M, G): ls -h or ls --human-readable\n\nShort flags can be combined and the flag order doesn’t matter, so the followings are all equivalent:\n\nls -alh\nls -a -l -h\nls -ahl\nls -l -ha\nls --human-readable -al\nls --all --human-readable -l\n…\n\n\n\nCommands documentation\n\nMan pages\nThe manual page for a command can be accessed with the command man:\nman &lt;command&gt;\n\nThe &lt; and &gt; symbols are used to delineate a generic placeholder that you should replace by the value of your choice (here, for instance, man ls).\n\n\nMan pages open in a pager (usually less).\nUseful keybindings when you are in the pager:\nSPACE      scroll one screen down\nb          back one screen\nq          quit the pager\ng          go to the top of the document\n7g         go to line 7 from the top\nG          go to the bottom of the document\n/          search for a term\n           n will take you to the next result\n           N to the previous result\n\n\n\nYour turn:\n\n\nOpen the man page for the ls command.\nNavigate down a few pages, then navigate back up.\nSearch for the first 5 occurrences of the word “directory”.\nWhat does ls -r do?\nFinally, leave the pager.\n\n\n\n\nHelp on commands\nHelp for commands can be printed to the standard output (the terminal) with:\n&lt;command&gt; --help\n\n\nYour turn:\n\nPrint the help for the ls command in your terminal.\n\n\n\nType of commands\nTo know the nature of a command (e.g. shell built-in function, an alias that you created, or the path of an utility) run either of:\ncommand -V &lt;command&gt;\ntype &lt;command&gt;\n\n\nYour turn:\n\nWhat is the nature of the pwd command?",
    "crumbs": [
      "Bash/Zsh",
      "<b><em>Getting started</em></b>",
      "Shell basics"
    ]
  },
  {
    "objectID": "bash/intro_basics.html#shell-keybindings",
    "href": "bash/intro_basics.html#shell-keybindings",
    "title": "Shell basics",
    "section": "Shell keybindings",
    "text": "Shell keybindings\nHere are useful keybindings that you can use in the shell (they all come from the text editor Emacs):\ntab        auto-complete command\nC-l        clear the terminal\nC-p        navigate the command history backward\nC-n        navigate the command history forward\nC-a        go to the beginning of the line\nC-e        go to the end of the line\nC-k        delete to the end of the line\nC-u        delete to the beginning of the line\nC-f        go forward one character\nC-b        go backward one character\nM-f        go forward one word\nM-b        go backward one word\n\nC-l means: press the Ctrl (Windows) or Command key ⌘ (macOS) and l keys at the same time.\nM-f means: press the Alt (Windows) or Option (macOS) and f keys at the same time.",
    "crumbs": [
      "Bash/Zsh",
      "<b><em>Getting started</em></b>",
      "Shell basics"
    ]
  },
  {
    "objectID": "bash/intro_filesystem.html",
    "href": "bash/intro_filesystem.html",
    "title": "The Unix filesystem",
    "section": "",
    "text": "Unix shells allow to give instructions to a Unix operating system. The first thing you will need to know is how storage is organized on such a system.",
    "crumbs": [
      "Bash/Zsh",
      "<b><em>Getting started</em></b>",
      "The Unix filesystem"
    ]
  },
  {
    "objectID": "bash/intro_filesystem.html#structure",
    "href": "bash/intro_filesystem.html#structure",
    "title": "The Unix filesystem",
    "section": "Structure",
    "text": "Structure\nThe Unix filesystem is a rooted tree of directories. The root is denoted by /.\nSeveral directories exist under the root. Here are a few:\n\n/bin     This is where binaries are stored.\n/boot    There, you can find the files necessary for booting the system.\n/home    This directory contains all the users’ home directories.\n\nThese directories in turn can contain other directories. /home for instance contains the directories:\n\n/home/user01\n/home/user02\n/home/user03\n…\n\nThe home directory of each user in turn contains their files and directories.",
    "crumbs": [
      "Bash/Zsh",
      "<b><em>Getting started</em></b>",
      "The Unix filesystem"
    ]
  },
  {
    "objectID": "bash/intro_filesystem.html#navigation",
    "href": "bash/intro_filesystem.html#navigation",
    "title": "The Unix filesystem",
    "section": "Navigation",
    "text": "Navigation\n\nWorking directory\nThe current working directory can be obtained with:\npwd\n\nStands for print working directory.\n\n\n\nYour turn:\n\nWhat is your current working directory?\n\n\n\nChanging directory\nTo navigate to another directory, use cd (change directory) followed by the path of the directory.\n\nExample:\n\ncd /home\nBecause /home was the parent directory of our working directory (one level above in the rooted tree), we could have also gotten there with cd .. — the two dots represent one level up (a single dot represents the working directory).\n\n\nYour turn:\n\n\nWhat will happen if you run cd .. from /home?\nWhat will happen if you run cd . from /home?\n\n\nFrom any location, you can always go back to your home directory (e.g. /home/user09) by running cd without argument. Alternatively, you can use cd ~. This is because ~ gets expanded by the shell into the path of your home. Finally, you can use cd $HOME. $HOME is an environment variable representing the path of your home.\n\n\nYour turn:\n\nTry using cd - (that’s the minus sign) a few times. What does this command do?\n\n\n\nAbsolute and relative paths\n\nAbsolute paths give the full path from the root (e.g. /bin, /home/user09/file).\nRelative paths give the path relative to the working directory (e.g. ../dir/file, dir/subdir).\n\n\n\nYour turn:\n\nIs ~ an absolute or relative path?\n\n\nIn the filesystem below, the current working directory is /home/user01.\n\nWhat is the output of ls?\nWhat is the output of ls ../..?\nThe output of ls /thesis/src is:\n\n\nls: cannot access ‘/thesis/src’: No such file or directory\n\n  Why?\n\nWhat are 2 ways to navigate to the results directory?\nFrom the results directory, what are 2 ways to print the content of the src directory?\n\n\n\n\n\n\n\n\n\ncluster\n\n\n\n\nuser01--.bashrc\n\n\n\n\nuser01--.bash_profile\n\n\n\n\nuser01--thesis\n\n\n\n\n/--bin\n\n\n\n\n/--boot\n\n\n\n\n/--home\n\n\n\n\nhome--user01\n\n\n\n\nhome--user02\n\n\n\n\nhome--user03\n\n\n\n\nthesis--data\n\n\n\n\nthesis--ms\n\n\n\n\nthesis--results\n\n\n\n\nthesis--src\n\n\n\n\nresults--graph1\n\n\n\n\nresults--graph2\n\n\n\n\nsrc--script1\n\n\n\n\n.bashrc\n.bashrc\n\n\n\n.bash_profile\n.bash_profile\n\n\n\ngraph1\ngraph1\n\n\n\ngraph2\ngraph2\n\n\n\nscript1\nscript1\n\n\n\nuser01\n\nuser01\n\n\n\n/\n\n/\n\n\n\nbin\n\nbin\n\n\n\nboot\n\nboot\n\n\n\nhome\n\nhome\n\n\n\nuser02\n\nuser02\n\n\n\nuser03\n\nuser03\n\n\n\nthesis\n\nthesis\n\n\n\ndata\n\ndata\n\n\n\nms\n\nms\n\n\n\nresults\n\nresults\n\n\n\nsrc\n\nsrc",
    "crumbs": [
      "Bash/Zsh",
      "<b><em>Getting started</em></b>",
      "The Unix filesystem"
    ]
  },
  {
    "objectID": "bash/intro_filesystem.html#creating-files-and-directories",
    "href": "bash/intro_filesystem.html#creating-files-and-directories",
    "title": "The Unix filesystem",
    "section": "Creating files and directories",
    "text": "Creating files and directories\nFiles can be created with a text editor:\nnano newfile.txt\n\nThis opens the text editor “nano” with a blank file. The file actually gets created when you save it from within the text editor.\n\nor with the command touch:\ntouch newfile.txt\n\nThis creates an empty file.\n\ntouch can create multiple files at once:\ntouch file1 file2 file3\nNew directories can be created with mkdir. This command can also accept multiple arguments to create multiple directories at once:\nmkdir dir1 dir2",
    "crumbs": [
      "Bash/Zsh",
      "<b><em>Getting started</em></b>",
      "The Unix filesystem"
    ]
  },
  {
    "objectID": "bash/intro_filesystem.html#deleting",
    "href": "bash/intro_filesystem.html#deleting",
    "title": "The Unix filesystem",
    "section": "Deleting",
    "text": "Deleting\nFiles can be deleted with the command rm followed by their paths:\nrm file1 file2\nDirectories can be deleted with rm -r (“recursive”) followed by their paths or—if they are empty—with rmdir:\nrm -r dir1\nrmdir dir2   # only works if dir2 is empty\nBe careful that these commands are irreversible. By default, there is no trash in Linux systems.",
    "crumbs": [
      "Bash/Zsh",
      "<b><em>Getting started</em></b>",
      "The Unix filesystem"
    ]
  },
  {
    "objectID": "bash/intro_filesystem.html#copying-moving-and-renaming",
    "href": "bash/intro_filesystem.html#copying-moving-and-renaming",
    "title": "The Unix filesystem",
    "section": "Copying, moving, and renaming",
    "text": "Copying, moving, and renaming\nCopying is done with the cp command:\ncp thesis/src/script1 thesis/ms\nMoving and renaming are both done with the mv command:\n# rename script1 to script\nmv thesis/src/script1 thesis/src/script\n\n# move graph1 to the ms directory\nmv thesis/results/graph1 thesis/ms\n# this also works:\n# mv thesis/results/graph1 thesis/ms/graph1\n\n\nYour turn:\n\nWhy is there only one command to move and rename?",
    "crumbs": [
      "Bash/Zsh",
      "<b><em>Getting started</em></b>",
      "The Unix filesystem"
    ]
  },
  {
    "objectID": "bash/intro_functions.html",
    "href": "bash/intro_functions.html",
    "title": "Functions",
    "section": "",
    "text": "As in programming language, shell functions are blocks of code that can be accessed by their names.",
    "crumbs": [
      "Bash/Zsh",
      "<b><em>Getting started</em></b>",
      "Functions"
    ]
  },
  {
    "objectID": "bash/intro_functions.html#function-definition",
    "href": "bash/intro_functions.html#function-definition",
    "title": "Functions",
    "section": "Function definition",
    "text": "Function definition\n\nSyntax\nYou define a new function with the following syntax:\nname() {\n    command1\n    command2\n    ...\n}\n\n\nExample\ngreetings() {\n  echo hello\n}",
    "crumbs": [
      "Bash/Zsh",
      "<b><em>Getting started</em></b>",
      "Functions"
    ]
  },
  {
    "objectID": "bash/intro_functions.html#storing-functions",
    "href": "bash/intro_functions.html#storing-functions",
    "title": "Functions",
    "section": "Storing functions",
    "text": "Storing functions\nYou can define a new function directly in the terminal. Such function would however only be available during your current session. Since functions contain code that is intended to be run repeatedly, it makes sense to store function definitions in a file. Before functions become available, the file needs to be sourced (e.g. source file.sh).\nA convenient file is ~/.bashrc. The file is automatically sourced every time you start a shell so your functions will always be defined and ready for use.",
    "crumbs": [
      "Bash/Zsh",
      "<b><em>Getting started</em></b>",
      "Functions"
    ]
  },
  {
    "objectID": "bash/intro_functions.html#example-1",
    "href": "bash/intro_functions.html#example-1",
    "title": "Functions",
    "section": "Example",
    "text": "Example\nLet’s write a function called combine that takes all the files we pass to it, copies them into a randomly named directory, and prints that directory to the terminal:\ncombine() {\n  if [ $# -eq 0 ]; then\n    echo \"No arguments specified. Usage: combine file1 [file2 ...]\"\n    return 1                # Return a non-zero error code\n  fi\n  dir=$RANDOM$RANDOM\n  mkdir $dir\n  cp $@ $dir\n  echo look in the directory $dir\n}\n\n\nYour turn:\n\nWrite a function to swap two file names.\nAdd a check that both files exist before renaming them.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere is a video of a previous version of this workshop.",
    "crumbs": [
      "Bash/Zsh",
      "<b><em>Getting started</em></b>",
      "Functions"
    ]
  },
  {
    "objectID": "bash/intro_modern.html",
    "href": "bash/intro_modern.html",
    "title": "Modern utilities",
    "section": "",
    "text": "In recent years, a number of open-source utilities for the Unix shell have emerged. Some are meant as replacements for classic tools with improved performance, better defaults, or nicer-looking outputs; others add novel functionality. Several of them were recently installed on the Alliance clusters.\nIn this section we will cover a selection of tools that are very popular, well-maintained, and will improve your time on the command line.",
    "crumbs": [
      "Bash/Zsh",
      "<b><em>Getting started</em></b>",
      "Modern utilities"
    ]
  },
  {
    "objectID": "bash/intro_modern.html#how-to-choose-tools",
    "href": "bash/intro_modern.html#how-to-choose-tools",
    "title": "Modern utilities",
    "section": "How to choose tools?",
    "text": "How to choose tools?\nYou can install and experiment with any tool. A few things might help you decide whether or not a tool looks promising:\n\nHow popular is it? (GitHub stars)\nIs it maintained? (date of last commit)\nHow polished is the documentation?\nHow fast is it? (what language is it written in?)\n\nTools written in Shell or Python will be slower, while those written in compiled languages (Rust, C, Go) will be faster.",
    "crumbs": [
      "Bash/Zsh",
      "<b><em>Getting started</em></b>",
      "Modern utilities"
    ]
  },
  {
    "objectID": "bash/intro_modern.html#ls-in-colours-eza",
    "href": "bash/intro_modern.html#ls-in-colours-eza",
    "title": "Modern utilities",
    "section": "ls in colours: eza",
    "text": "ls in colours: eza\neza is a replacement for ls.\nFor the most part, it adds colours and uses the same options as ls, but it also adds a few features and has better default options.\n\nInstallation\n\nOn your own machine\nYou can find the installation instructions here.\n\n\nOn the Alliance clusters\neza is not installed on the Alliance clusters, so you will have to install it locally under your own user. This is easy to do because it is written in Rust and can be installed with the Rust package manager.\nFirst, you load a Rust module, then you install the package:\nmodule load rust/1.76.0\ncargo install eza\nFinally, make sure that ~/.cargo/bin is in your PATH as this is where the binary will get installed. So add to your .bashrc:\nexport PATH=$PATH:~/.cargo/bin\n\nYou only need to do this once in the production cluster you use. After eza has been installed, it will be accessible on subsequent sessions.\n\n\n\n\nUsage\neza\nprojects scratch\nYou now have different colours for different types of files. You can already see that the directory (projects) is in bold blue and the symlink (scratch) is in cyan (my website doesn’t render these colours but you will see them when you run the command in our training cluster).\nLet’s create some files quickly to see a bit more colours:\ntouch test.html test.md test.py test.R\neza\nprojects scratch test.html test.md test.py test.R\nNow you can see that the scripts are in bold yellow while the markup files are in gray.\nThe flags are similar to those of ls, but some defaults are slightly different:\neza -al\n.rw-------@ 2.8k user02 19 Dec 04:20 .bash_history\n.rw-------@   18 user02 13 Dec 00:55 .bash_logout\n.rw-r-----@  141 user02 19 Dec 04:20 .bash_profile\n.rw-r-----@ 7.3k user02 19 Dec 03:57 .bashrc\ndrwxr-x---@    - user02 19 Dec 03:57 .cargo\ndrwx------@    - user02 13 Dec 18:10 .ssh\ndrwxr-xr-x@    - root   13 Dec 00:57 projects\nlrwxrwxrwx@    - user02 13 Dec 00:55 scratch -&gt; /scratch/user02\n.rw-r-----@    0 user02 19 Dec 04:25 test.html\n.rw-r-----@    0 user02 19 Dec 04:25 test.md\n.rw-r-----@    0 user02 19 Dec 04:25 test.py\n.rw-r-----@    0 user02 19 Dec 04:29 test.R\n\nCompare with ls -al:\ntotal 60\ndrwx------.  12 user02 user02 4096 Dec 19 04:29 .\ndrwxr-xr-x. 101 root   root   4096 Dec 13 00:57 ..\n-rw-------.   1 user02 user02 2763 Dec 19 04:20 .bash_history\n-rw-------.   1 user02 user02   18 Dec 13 00:55 .bash_logout\n-rw-r-----.   1 user02 user02  141 Dec 19 04:20 .bash_profile\n-rw-r-----.   1 user02 user02 7320 Dec 19 03:57 .bashrc\ndrwxr-x---.   4 user02 user02  125 Dec 19 03:57 .cargo\ndrwx------.   2 user02 user02   48 Dec 13 18:10 .ssh\ndrwxr-xr-x.   2 root   user02   27 Dec 13 00:57 projects\nlrwxrwxrwx.   1 user02 user02   15 Dec 13 00:55 scratch -&gt; /scratch/user02\n-rw-r-----.   1 user02 user02    0 Dec 19 04:29 test.R\n-rw-r-----.   1 user02 user02    0 Dec 19 04:25 test.html\n-rw-r-----.   1 user02 user02    0 Dec 19 04:25 test.md\n-rw-r-----.   1 user02 user02    0 Dec 19 04:25 test.py\neza by default shows the output in a human readable format (-h flag with ls) and without the group (-G with ls).\n\nThere is also the additional -T flag which is equivalent to running the tree utility:\neza -T\n.\n├── projects\n│   └── def-sponsor00 -&gt; /project/def-sponsor00\n├── scratch -&gt; /scratch/user02\n├── test.html\n├── test.md\n├── test.py\n└── test.R\n\n\nAlias\nIf you want, you can alias it to ls by adding to your .bashrc file:\nalias ls=eza\nIf you ever want to use the true ls utility, you can do so with \\ls.\n\n\nAlternative\nIf you want a simpler and more lightweight way to add colours to your ls outputs, you can look at LS_COLORS. I did this for years until I found eza.\nTo install it locally in the Alliance clusters, you have to download and uncompress a script, and copy it to a proper location:\nmkdir ./LS_COLORS &&\n    curl -L https://api.github.com/repos/trapd00r/LS_COLORS/tarball/master | tar xzf - --directory=./LS_COLORS --strip=1 &&\n    mkdir -p ~/.local/share &&\n    cp ~/LS_COLORS/lscolors.sh ~/.local/share &&\n    rm -r ~/LS_COLORS\nThen you need to add the following to your .bashrc file to source the script and add a proper alias to ls:\nsource ~/.local/share/lscolors.sh\nalias ls='ls --color'",
    "crumbs": [
      "Bash/Zsh",
      "<b><em>Getting started</em></b>",
      "Modern utilities"
    ]
  },
  {
    "objectID": "bash/intro_modern.html#a-cat-with-wings-bat",
    "href": "bash/intro_modern.html#a-cat-with-wings-bat",
    "title": "Modern utilities",
    "section": "A cat with wings: bat",
    "text": "A cat with wings: bat\nbat is a replacement for cat with syntax highlighting for most programming languages, line numbers, and pager-like search and navigation.\n\nInstallation\nbat is installed on the Alliance clusters. To install it on your machine, you can follow the instructions here.\n\n\nUsage\nUse bat as you would use cat:\nbat .bash_profile\n───────┬───────────────────────────────────────────────────────────────────────────────────────────────────────\n       │ File: .bash_profile\n───────┼───────────────────────────────────────────────────────────────────────────────────────────────────────\n   1   │ # .bash_profile\n   2   │\n   3   │ # Get the aliases and functions\n   4   │ if [ -f ~/.bashrc ]; then\n   5   │     . ~/.bashrc\n   6   │ fi\n   7   │\n   8   │ # User specific environment and startup programs\n───────┴───────────────────────────────────────────────────────────────────────────────────────────────────────\nThen you are in your default pager.\nAmong other options, you can disable the frame with -nand also remove the line numbers with -p.",
    "crumbs": [
      "Bash/Zsh",
      "<b><em>Getting started</em></b>",
      "Modern utilities"
    ]
  },
  {
    "objectID": "bash/intro_modern.html#faster-find-fd",
    "href": "bash/intro_modern.html#faster-find-fd",
    "title": "Modern utilities",
    "section": "Faster find: fd",
    "text": "Faster find: fd\nfd is a replacement for find with the following assets:\n\nwritten in Rust, automatic parallelism ➔ with vastly improved performance,\nmore friendly syntax,\nby default excludes binaries as well as hidden files and directories,\nby default excludes patterns from .gitignore or other .ignore files.\n\n\nInstallation\nfg is installed on the Alliance clusters. To install it on your machine, you can follow the instructions here.\n\n\nBasic usage\nSearch file names for a pattern recursively in current directory:\nfd te\ntest.R\ntest.html\ntest.md\ntest.py\n\nfd uses regexp by default, so you can use pattern symbols:\nfd jx.*txt\n\n Search file names recursively in another directory:\nfd top bash/\n\n\nPrint all files in some directory to stdout\nCurrent directory:\nfd\nAnother directory:\nfd . bash/\n\n\nOptions\nSearch for files with a particular file extension:\nfd -e txt\nUse a globbing pattern instead of regexp:\nfd -g wb* bash/\nMatch full path instead of simply file name:\nfd -p bash/wb\nExecute command for each result of fd in parallel:\nfd top bash/ -x rg layout\nExecute command once with all results of fd as arguments:\nfd top bash/ -X rg layout\n\n\nExcluded files and directories\nBy default, fd excludes hidden files/directories and patterns in .gitignore (you can disable this with -H and -I respectively).\nThis makes fd combined with tree sometimes more useful than tree alone:\nfd . bash/ | tree --fromfile\n\nYou can make this a function:\nft () { fd $@ | tree --fromfile }\n\nExclude additional directories or patterns:\nfd -E *.txt -E img/ . bash/\nI personally prefer to disable the default settings and exclude patterns based on a file I created:\nalias fd='fd -u --ignore-file /home/marie/.fdignore'\nThis file (which can have any name you want) uses the same syntax as .gitignore.",
    "crumbs": [
      "Bash/Zsh",
      "<b><em>Getting started</em></b>",
      "Modern utilities"
    ]
  },
  {
    "objectID": "bash/intro_modern.html#rip-grep-ripgrep",
    "href": "bash/intro_modern.html#rip-grep-ripgrep",
    "title": "Modern utilities",
    "section": "RIP grep: ripgrep",
    "text": "RIP grep: ripgrep\nripgrep provides the rg utility—a replacement for grep with the following assets:\n\nwritten in Rust, automatic parallelism ➔ with vastly improved performance,\nby default excludes patterns from .gitignore or other .ignore files,\nby default excludes binaries as well as hidden files and directories,\nby default doesn’t follow symlinks.\n\n\nInstallation\nrg is installed on the Alliance clusters. To install it on your machine, you can follow the instructions here.\n\n\nUsage\nSearch lines in a file matching a pattern:\nrg .bash .bash_profile\n1:# .bash_profile\n4:if [ -f ~/.bashrc ]; then\n5:  . ~/.bashrc\nSearch lines matching pattern in all files in current directory (recursively):\nrg .bash\nrg and fd follow the same principles:\n\nuse of regexp by default,\nuse of globbing pattern with -g,\nsearch recursively by default,\nsame excluded files.\n\nYou can find full details on the syntax here.",
    "crumbs": [
      "Bash/Zsh",
      "<b><em>Getting started</em></b>",
      "Modern utilities"
    ]
  },
  {
    "objectID": "bash/intro_modern.html#smart-cd-zoxide",
    "href": "bash/intro_modern.html#smart-cd-zoxide",
    "title": "Modern utilities",
    "section": "Smart cd: zoxide",
    "text": "Smart cd: zoxide\nzoxide allows to easily jump to any directory.\n\nInstallation\nInstructions to install zoxide on your machine can be found here. Note that fzf (see below) adds cool functionality to it, so you might want to install it as well.\nInstalling zoxide on the Alliance clusters is extremely easy.\n\nFirst, you have to run the command:\n\ncurl -sSfL https://raw.githubusercontent.com/ajeetdsouza/zoxide/main/install.sh | sh\nThis will install the binary in ~/.local/bin.\n\nThen, you have to make sure that ~/.local/bin is in your PATH.\n\nAdd to your .bashrc:\nexport PATH=$PATH:~/.local/bin\n\nFinally, you need to add to your .bashrc file (if you want to use it in Bash):\n\neval \"$(zoxide init bash)\"\nOr, if you want to use it in Zsh, add to your .zshrc file:\neval \"$(zoxide init zsh)\"\n\nYou can change the z and zi commands (a bit hard to type) by whatever command you want. I personally changed them to j and ji (“j” for “jump”). To do this, replace the eval expression(s) above by:\neval \"$(zoxide init --cmd j bash)\"\nand/or\neval \"$(zoxide init --cmd j zsh)\"\n\n\n\nUsage\nNow, you can type z (or whatever command you chose—see above) instead of a regular cd command.\nIn addition, you can simplify the path you want to get to to just a few characters and zoxide will take you there.\nIf there are multiple locations matching your entry, the matching algorithm will chose the highest ranking path based on your frequency of use and how recently you visited a path.\nThis means that you can visit your usual places extremely easily. For less frequent places, add a bit more info.\nFinally, if you want to choose amongst all possible options in a completion framework, use zi instead and zoxide will open fzf (see below).\n\n\nAlternative\nA tool that served me well until someone pointed the faster and better zoxide to me is autojump.\n\nInstallation\nautojump is installed on the Alliance clusters, but you need to source a script before you can use it. Since you need to do this at every session, you should add to your .bashrc file:\n[[ -s $EPREFIX/etc/profile.d/autojump.sh ]] && source $EPREFIX/etc/profile.d/autojump.sh\nTo install autojump on your machine, you can follow the instructions here.\n\n\nUsage\nSimilar to zoxide but you need to visit directories so that they get entered into autojump’s database before you can jump to them.\nj is a wrapper for autojump, jc jumps to subdirectories of the current directory.\nYou can find the full usage documentation here.",
    "crumbs": [
      "Bash/Zsh",
      "<b><em>Getting started</em></b>",
      "Modern utilities"
    ]
  },
  {
    "objectID": "bash/intro_modern.html#fuzzy-finder-fzf",
    "href": "bash/intro_modern.html#fuzzy-finder-fzf",
    "title": "Modern utilities",
    "section": "Fuzzy finder: fzf",
    "text": "Fuzzy finder: fzf\nfzf allows to find elements of any list through incremental completion and fuzzy matching. It can be paired with any number of commands.\n\nInstallation\nfzf is installed on the Alliance clusters.\nTo get fzf kbds and fuzzy completion in your shell, add to your .bashrc:\neval \"$(fzf --bash)\"\nand/or your .zshrc:\nsource &lt;(fzf --zsh)\nTo install it on your machine, you can follow the instructions here.\n\n\nDirect usage\nIf you run fzf directly, it will search the current directory recursively, do a narrowing selection, and print the result:\nfzf\nYou can make use of fd (or whatever command you want) to filter out unnecessary entries by setting the FZF_DEFAULT_COMMAND environment variable (in your .bashrc or .zshrc):\nexport FZF_DEFAULT_COMMAND='fd -u --ignore-file /home/marie/.fdignore'\nRunning fzf by itself will now be equivalent to running your command of choice that you put in the FZF_DEFAULT_COMMAND environment variable piped to fzf. For all other ways to use fzf (see below), this has no effect so you still have the full flexibility of the tool.\n\n\nfzf key bindings\nfzf ships with three default kbds:\n\nCtl+t ➔ paste the selection into the command\nCtl+r ➔ paste the selection from history into the command\nAlt+c ➔ cd into the selection\n\nThe one that I find particularly useful is the first one. It is extremely useful to start typing a command (e.g. cp), then use Ctl+t to add the path to a file or directory you want to use as argument of the command before finishing to type your command. Try it out!\n\n\nPipe to fzf\nYou can pipe the output of any command that returns a list of elements into fzf to find any particular element easily through incremental completion.\nFor instance, you can pipe the output of ls into fzf:\nls | fzf\nTo look for a running process:\nalias proc='ps -ef | fzf --cycle -i -e +s --tac --reverse'\n\nproc\nTo kill a running process:\nproc_kill() {\n    local pid\n    pid=$(ps -ef |\n          sed 1d |\n          fzf --cycle --reverse -i -e -m --bind \"ctrl-o:toggle-all\" \\\n          --header \"Tab: toggle, C-o: toggle-all\" |\n          awk '{print $2}')\n    echo $pid | xargs kill -${1:-15}\n}\n\nproc_kill\nTo search your command history:\nhis() {\n    fc -ln 1 |\n    rg -v '^q$|^x$|^vs$|^ek .*$|^zoom$|^c$|^cca$|^rs ...$|^hobu$|^cd$|^kzo$|^ih.?$|^zre$|^j m$|^y$|^g$|^-$|^auradd$' |\n    fzf --cycle -i -e +s --tac --reverse |\n    sed 's/ *[0-9]* *//'\n}\n\nhis\nTo search your command history and run the selected entry:\nhis_run() {\n    $(fc -ln 1 |\n      rg -v '^q$|^x$|^vs$|^ek .*$|^zoom$|^c$|^cca$|^rs ...$|^hobu$|^cd$|^kzo$|^ih.?$|^zre$|^j m$|^y$|^g$|^-$|^auradd$' |\n      fzf --cycle -i -e +s --tac --reverse |\n      sed 's/ *[0-9]* *//')\n}\n\nhis_run\nfzf has many options to select the order of entries, the type of completion, whether or not to display a preview, etc. You can find the information in the documentation.",
    "crumbs": [
      "Bash/Zsh",
      "<b><em>Getting started</em></b>",
      "Modern utilities"
    ]
  },
  {
    "objectID": "bash/intro_modern.html#file-system-tuis",
    "href": "bash/intro_modern.html#file-system-tuis",
    "title": "Modern utilities",
    "section": "File system TUIs",
    "text": "File system TUIs\n\nWhat is a TUI?\nTerminal user interfaces (TUIs) are the predecessors to graphical user interfaces (GUIs) which are entirely text based and run in terminals.\nThey have remained very popular among command line aficionados because they are fast, efficient, powerful, and keyboard-driven, while being friendly and visual.\nFantastic modern ones keep being built for tasks as diverse as interfaces to Git, music players, games, emails, dashboards, and, for our purpose here, file system management.\n\n\nFormerly most popular file system TUIs\nThere are many file system TUIs and all of them are actually really good. The two most notable ones used to be:\n\nranger\n\nExtremely sophisticated, easy to customize, tons of features.\n\nBuilt in Python, it can be slow for operations in directories with thousands of files.\n\n\nnnn\n\nMinimalist and very fast (written in C).\n\nNot easy to customize (many customizations require compiling from source). Most functionalities rely on plugins that need to be installed. Not easy to get started with.\n\n\n\nThe new kid: yazi\nyazi is a brand new file system TUI that has quickly become the most popular out there.\nIt is extremely modern, very fast (written in Rust), very well documented, intuitive, easy to customize, and integrates with modern utilities such as fd, rg, zoxide, and fzf out of the box.\nOnly at version 0.4, it is not fully mature yet, but it has already more stars on GitHub than ranger and nnn because it combines ease of customization and sophistication with speed.\n\n\nAlternatives\nHere are other really good—just not as fast or full-featured—file system TUIs, in decreasing number of stars on GitHub:\n\nbroot\nsuperfile\nlf\nxplr\nfff (now archived)\nvifm\nmc",
    "crumbs": [
      "Bash/Zsh",
      "<b><em>Getting started</em></b>",
      "Modern utilities"
    ]
  },
  {
    "objectID": "bash/intro_modern.html#webinar",
    "href": "bash/intro_modern.html#webinar",
    "title": "Modern utilities",
    "section": "Webinar",
    "text": "Webinar\nI gave a webinar on this topic that you can find here.",
    "crumbs": [
      "Bash/Zsh",
      "<b><em>Getting started</em></b>",
      "Modern utilities"
    ]
  },
  {
    "objectID": "bash/intro_redirections.html",
    "href": "bash/intro_redirections.html",
    "title": "Redirections & pipes",
    "section": "",
    "text": "By default, commands that produce an output print it to the terminal. This output can however be redirected to be printed elsewhere (e.g. to a file) or to be passed as the argument of another command."
  },
  {
    "objectID": "bash/intro_redirections.html#redirections",
    "href": "bash/intro_redirections.html#redirections",
    "title": "Redirections & pipes",
    "section": "Redirections",
    "text": "Redirections\nBy default, commands that produce an output print it to standard output—that is, the terminal. This is what we have been doing so far.\nThe output can however be redirected with the &gt; sign. For instance, it can be redirected to a file, which is very handy if you want to save the result.\n\nExample:\n\nLet’s print the number of lines in each .pdb file in the molecules directory:\n\nwc -l *.pdb\n\nwc: '*.pdb': No such file or directory\n\n\n\n\nYour turn:\n\n\nWhat does the wc command do?\nWhat does the -l flag for this command do?\nHow did you find out?\n\n\nTo save this result into a file called lengths.txt, we run:\n\nwc -l *.pdb &gt; lengths.txt\n\nwc: '*.pdb': No such file or directory\n\n\n\nNote that &gt; always creates a new file. If a file called lengths.txt already exists, it will be overwritten. Be careful not to lose data this way!\nIf you don’t want to lose the content of the old file, you can append the output to the existing file with &gt;&gt; (&gt;&gt; will create a file lengths.txt if it doesn’t exist yet, but if it exists, it will append the new content below the old one).\n\n\n\nYour turn:\n\nHow can you make sure that you did create a file called lengths.txt?\n\nLet’s print its content to the terminal:\n\ncat lengths.txt\n\nAs you can see, it contains the output of the command wc -l *.pdb.\nOf course, we can print the content of the file with modification. For instance, we can sort it:\n\nsort -n lengths.txt\n\nAnd we can redirect this new output to a new file:\n\nsort -n lengths.txt &gt; sorted.txt\n\nInstead of printing an entire file to the terminal, you can print only part of it.\nLet’s print the first line of the new file sorted.txt:\n\nhead -1 sorted.txt"
  },
  {
    "objectID": "bash/intro_redirections.html#pipes",
    "href": "bash/intro_redirections.html#pipes",
    "title": "Redirections & pipes",
    "section": "Pipes",
    "text": "Pipes\nAnother form of redirection is the Bash pipe. Instead of redirecting the output to a different stream for printing, the output is passed as an argument to another command. This is very convenient because it allows to chain multiple commands without having to create files or variables to save intermediate results.\nFor instance, we could run the three commands we ran previously at once, without the creation of the two intermediate files:\n\nwc -l *.pdb | sort -n | head -1\n\nwc: '*.pdb': No such file or directory\n\n\nIn each case, the output of the command on the left-hand side (LHS) is passed as the input of the command on the right-hand side (RHS).\n\n\nYour turn:\n\nIn a directory we want to find the 3 files that have the least number of lines. Which command would work for this?\n\nwc -l * &gt; sort -n &gt; head -3\nwc -l * | sort -n | head 1-3\nwc -l * | sort -n | head -3\nwc -l * | head -3 | sort -n\n\n\nHere is a video of a previous version of this workshop."
  },
  {
    "objectID": "bash/intro_resources.html",
    "href": "bash/intro_resources.html",
    "title": "Resources",
    "section": "",
    "text": "This section lists a few useful Bash resources.\n\nOne very useful (although very dense) resource is the Bash manual. This is the absolute reference.\nThere are countless sites with Bash courses and guides. Here are a few:\n\nBash Guide for Beginners\nLearn X in Y minutes for Bash\nLinux Bash Shell Scripting Tutorial\n\nYou can also get information on Bash from within Bash with:\ninfo bash\nand:\nman bash\nThere are also countless resources online and don’t forget to Google anything you don’t know how to do: you will almost certainly find the answer on StackOverflow or some Stack Exchange site.",
    "crumbs": [
      "Bash/Zsh",
      "<b><em>Getting started</em></b>",
      "Resources"
    ]
  },
  {
    "objectID": "bash/intro_special_parameters.html",
    "href": "bash/intro_special_parameters.html",
    "title": "Special parameters",
    "section": "",
    "text": "A number of special parameters, all starting with $, get expanded by Bash.\n\n\n$1, $2, $3, … are positional special characters,\n$@ is an array-like construct referring of all positional parameters,\n$# expands to the number of arguments,\n$$ pid of the current shell,\n$! expands to the PID of the most recent background command,\n$0 expands to the name of the current shell or script.\n\n\n\n\nExample:\n\nfunction arguments {\n    echo First argument: $1\n    echo Second argument: $2\n    echo Third argument: $3\n    echo Number of arguments: $#\n    echo All arguments: $@\n}\n\narguments one two three four five\n\nFirst argument: one\nSecond argument: two\nThird argument: three\nNumber of arguments: 5\nAll arguments: one two three four five\n\n\nAdditionally, !! is replaced by the previous command.",
    "crumbs": [
      "Bash/Zsh",
      "<b><em>Getting started</em></b>",
      "Special parameters"
    ]
  },
  {
    "objectID": "bash/intro_variables.html",
    "href": "bash/intro_variables.html",
    "title": "Variables",
    "section": "",
    "text": "You can assign values to names. These names and the values they hold are called “variables”.\nVariables are a convenient way to reuse values.",
    "crumbs": [
      "Bash/Zsh",
      "<b><em>Getting started</em></b>",
      "Variables"
    ]
  },
  {
    "objectID": "bash/intro_variables.html#declaring-variables",
    "href": "bash/intro_variables.html#declaring-variables",
    "title": "Variables",
    "section": "Declaring variables",
    "text": "Declaring variables\nYou declare a variable (i.e. a name that holds a value) with the = sign:\nvar=value\n\nMake sure not to put spaces around the equal sign.\n\n\nExample:\n\n\nvar=5\n\nYou can delete a variable with the unset command:\n\nunset var",
    "crumbs": [
      "Bash/Zsh",
      "<b><em>Getting started</em></b>",
      "Variables"
    ]
  },
  {
    "objectID": "bash/intro_variables.html#expanding-variables",
    "href": "bash/intro_variables.html#expanding-variables",
    "title": "Variables",
    "section": "Expanding variables",
    "text": "Expanding variables\nTo expand a variable (to access its value), you need to prepend its name with $.\n\nThis is not what we want:\n\n\nvar=value\necho var\n\nvar\n\n\n\nThis however works:\n\n\nvar=value\necho $var\n\nvalue",
    "crumbs": [
      "Bash/Zsh",
      "<b><em>Getting started</em></b>",
      "Variables"
    ]
  },
  {
    "objectID": "bash/intro_variables.html#quotes",
    "href": "bash/intro_variables.html#quotes",
    "title": "Variables",
    "section": "Quotes",
    "text": "Quotes\n\nWhen declaring\nQuotes are necessary for values containing special characters such as spaces.\n\nThis doesn’t work:\n\n\nvar=string with spaces\necho $var\n\nbash: line 1: with: command not found\n\n\n\nThis works:\n\n\nvar=\"string with spaces\"\necho $var\n\nstring with spaces\n\n\n\nThis also works:\n\n\nvar='string with spaces'\necho $var\n\nstring with spaces\n\n\nWhen declaring variables, single and double quotes are equivalent. Which one should you use then? Use the one that is most convenient.\n\nThis is not good:\n\n\nvar='that's a string with spaces'\necho $var\n\nbash: -c: line 1: unexpected EOF while looking for matching `''\n\n\n\nThis works well:\n\n\nvar=\"that's a string with spaces\"\necho $var\n\nthat's a string with spaces\n\n\n\nAlternatively, single quotes can be escaped, but it is a little crazy: the first ' ends the first string, then the apostrophe needs to be escaped (\\'), finally, the third ' starts the second string.\n\nvar='that'\\''s a string with spaces'\necho $var\n\nthat's a string with spaces\n\n\n\n\nConversely, this is not good:\n\n\nvar=\"he said: \"string with spaces\"\"\necho $var\n\nbash: line 1: with: command not found\n\n\n\nWhile this works:\n\n\nvar='he said: \"string with spaces\"'\necho $var\n\nhe said: \"string with spaces\"\n\n\n\nDouble quotes as well can be escaped (simply by prepending them with \\):\n\nvar=\"he said: \\\"string with spaces\\\"\"\necho $var\n\nhe said: \"string with spaces\"\n\n\n\n\n\nWhen expanding\nWhile not necessary in many situations, it is safer to expand variables in double quotes, in case the expansion leads to problematic special characters. In the example above, this was not problematic and using $var or \"$var\" both work.\nIn the following example however, it is problematic:\nvar=\"string with spaces\"\ntouch $var\nThis creates 3 files called string, with, and spaces. Probably not what you wanted…\nThe following creates a single file called string with spaces:\nvar=\"string with spaces\"\ntouch \"$var\"\n\nTo be safe, it is thus a good habit to quote expanded variables.\n\nIt is important to note however that single quotes don’t expand variables (only double quotes do).\nThe following would thus create a file called $var:\nvar=\"string with spaces\"\ntouch '$var'",
    "crumbs": [
      "Bash/Zsh",
      "<b><em>Getting started</em></b>",
      "Variables"
    ]
  },
  {
    "objectID": "bash/intro_variables.html#exporting-variables",
    "href": "bash/intro_variables.html#exporting-variables",
    "title": "Variables",
    "section": "Exporting variables",
    "text": "Exporting variables\nUsing export ensures that all inherited processes of the current shell also have access to this variable:\n\nExample:\n\nvar=3\nzsh           # Launch Zsh (another shell)\necho $var\nThis returns nothing: var is not defined in the Zsh process.\nexport var=3\nzsh\necho $var\nThis returns 3: var got exported into the Zsh process.",
    "crumbs": [
      "Bash/Zsh",
      "<b><em>Getting started</em></b>",
      "Variables"
    ]
  },
  {
    "objectID": "bash/intro_variables.html#string-manipulation",
    "href": "bash/intro_variables.html#string-manipulation",
    "title": "Variables",
    "section": "String manipulation",
    "text": "String manipulation\n\nGetting a subset\n\nvar=\"hello\"\necho ${var:2}      # Print from character 2\necho ${var:2:1}    # Print 1 character from character 2\n\nllo\nl\n\n\n\nBash indexes from 0.\n\n\n\nSearch and replace\n\nvar=\"hello\"\necho ${var/l/L}    # Replace the first match of l by L\necho ${var//l/L}   # Replace all matches of l by L\n\nheLlo\nheLLo\n\n\n\n\nString concatenation\nIf you want to concatenate the expanded variable with another string, you need to use curly braces or quotes.\n\nThis does not return anything because there is no variable called varshine:\n\n\nvar=sun\necho $varshine\n\n\nThese two syntaxes do work:\n\n\nvar=sun\necho ${var}shine\necho \"$var\"shine\n\nsunshine\nsunshine",
    "crumbs": [
      "Bash/Zsh",
      "<b><em>Getting started</em></b>",
      "Variables"
    ]
  },
  {
    "objectID": "bash/intro_variables.html#environment-variables",
    "href": "bash/intro_variables.html#environment-variables",
    "title": "Variables",
    "section": "Environment variables",
    "text": "Environment variables\nEnvironment variables help control the behaviour of processes on a machine. You can think of them as customizations of your system.\nMany are set automatically.\n\nExample:\n\necho $HOME\n/home/user09\nThere are many other environment variables (e.g. PATH, PWD, PS1). To see the list, you can run printenv or env.\nIf you want to add new environment variables, you can add them to your ~/.bashrc file which is sourced each time you start a new shell.\nHere is a video of a previous version of this workshop.",
    "crumbs": [
      "Bash/Zsh",
      "<b><em>Getting started</em></b>",
      "Variables"
    ]
  },
  {
    "objectID": "bash/intro_zsh.html",
    "href": "bash/intro_zsh.html",
    "title": "Z shell",
    "section": "",
    "text": "Z shell (or Zsh) is a more modern Unix shell, very similar to Bash but with more functionality and ease of use.",
    "crumbs": [
      "Bash/Zsh",
      "<b><em>Getting started</em></b>",
      "Z shell"
    ]
  },
  {
    "objectID": "bash/intro_zsh.html#installation",
    "href": "bash/intro_zsh.html#installation",
    "title": "Z shell",
    "section": "Installation",
    "text": "Installation\nzsh is installed on the Alliance clusters. It is now also the default shell on macOS. For Windows users, you will probably have to use WSL. Here is a suggested setup (Unix shells such as Bash and Zsh are built for Unix systems and Windows is not a Unix system).",
    "crumbs": [
      "Bash/Zsh",
      "<b><em>Getting started</em></b>",
      "Z shell"
    ]
  },
  {
    "objectID": "bash/intro_zsh.html#first-launch",
    "href": "bash/intro_zsh.html#first-launch",
    "title": "Z shell",
    "section": "First launch",
    "text": "First launch\nWhen you launch zsh for the first time, a little program runs to help you create the Zsh config file .zshrc and set some options.\nYou will probably want to add to this file the aliases, functions, and sourcing that you normally have in your .bashrc file.\nOne thing that you do not have to copy over are the definitions of environment variables that you have defined in .bashrc with export because export ensures that the variables get exported into any shell launched from Bash (and that is what we are doing when we launch Zsh). If you have defined environment variables in .bashrc without exporting them and you want to use them in Zsh as well, you either need to export them in your .bashrc file or copy the variable definitions over to your .zshrc file.",
    "crumbs": [
      "Bash/Zsh",
      "<b><em>Getting started</em></b>",
      "Z shell"
    ]
  },
  {
    "objectID": "bash/intro_zsh.html#useful-plugins",
    "href": "bash/intro_zsh.html#useful-plugins",
    "title": "Z shell",
    "section": "Useful plugins",
    "text": "Useful plugins\nOut of the box, Zsh is nicer than Bash, but in addition, there are many additional niceties that have been built for it.\noh my zsh is a popular project and it contains many useful configurations and add-ons, but it also contain a lot of unnecessary things and is quite bloated.\nI personally prefer to keep things simpler and install 3 great plugins:\n\nsyntax highlighting,\nautosuggestions, and\nhistory substring search.\n\nThe syntax highlighting plug-in is installed on the Alliance clusters. To install the other two, you only have to clone the repos or scp the scripts themselves.\nClone the repos (you only need to do this once):\n# create a directory to store the scripts\nmkdir ~/.zsh_plugins\n\n# autosuggestions\ngit clone https://github.com/zsh-users/zsh-autosuggestions.git ~/.zsh_plugins/zsh-autosuggestions\n\n# history substring search\ngit clone https://github.com/zsh-users/zsh-history-substring-search.git ~/.zsh_plugins/zsh-history-substring-search\nThe you have to source the scripts (the one already installed on the Alliance clusters and the ones you just cloned under your own user).\nYou need to do this at each session, so you should add it to your .zshrc file:\nsource $EPREFIX/usr/share/zsh/site-functions/zsh-syntax-highlighting.zsh\nsource ~/.zsh_plugins/zsh-history-substring-search/zsh-history-substring-search.zsh\nsource ~/.zsh_plugins/zsh-autosuggestions/zsh-autosuggestions.zsh\nFor the scripts to become active in your current Zsh session, you also need to run these lines in your terminal or exit Zsh (with Ctl + d) and launch it back in (this sources the .zshrc file again and will thus source the scripts).\nNow paste the function we saw previously in zsh:\nproc_kill() {\n    local pid\n    pid=$(ps -ef |\n          sed 1d |\n          fzf --cycle --reverse -i -e -m --bind \"ctrl-o:toggle-all\" \\\n          --header \"Tab: toggle, C-o: toggle-all\" |\n          awk '{print $2}')\n    echo $pid | xargs kill -${1:-15}\n}\nand you will see the wonders of syntax highlighting in your shell inputs.\nTo use the history substring search, start typing some command then press Alt + p or Alt + n to cycle through all entries in your history that start with what you already typed.\nFinally, the autosuggestion will suggest commands based on your history and/or classic suggestions. You can accept the whole command with Ctl + e or accept a single word with Alt + f.",
    "crumbs": [
      "Bash/Zsh",
      "<b><em>Getting started</em></b>",
      "Z shell"
    ]
  },
  {
    "objectID": "bash/molecules/intro_find.html",
    "href": "bash/molecules/intro_find.html",
    "title": "Finding files",
    "section": "",
    "text": "For this section, we will play with files created by The Carpentries.\nYou can download them into a zip file called bash.zip with:\nwget http://bit.ly/bashfile -O bash.zip\nYou can then unzip that file with:\nunzip bash.zip\nFinally, you can delete the zip file:\nrm bash.zip\nYou should now have a data-shell directory with a molecules subdirectory.\ncd into it:\ncd data-shell/molecules"
  },
  {
    "objectID": "bash/molecules/intro_find.html#data-for-this-section",
    "href": "bash/molecules/intro_find.html#data-for-this-section",
    "title": "Finding files",
    "section": "",
    "text": "For this section, we will play with files created by The Carpentries.\nYou can download them into a zip file called bash.zip with:\nwget http://bit.ly/bashfile -O bash.zip\nYou can then unzip that file with:\nunzip bash.zip\nFinally, you can delete the zip file:\nrm bash.zip\nYou should now have a data-shell directory with a molecules subdirectory.\ncd into it:\ncd data-shell/molecules"
  },
  {
    "objectID": "bash/molecules/intro_find.html#command-find",
    "href": "bash/molecules/intro_find.html#command-find",
    "title": "Finding files",
    "section": "Command find",
    "text": "Command find\nSearch for files inside the current working directory:\nfind . -type f\n./methane.pdb\n./pentane.pdb\n./sorted.txt\n./propane.pdb\n./lengths.txt\n./cubane.pdb\n./ethane.pdb\n./octane.pdb\nfind . -type d will instead search for directories inside the current working directory.\nHere are other examples:\nfind . -maxdepth 1 -type f     # depth 1 is the current directory\nfind . -mindepth 2 -type f     # current directory and one level down\nfind . -name haiku.txt      # finds specific file\nls data       # shows one.txt two.txt\nfind . -name *.txt      # still finds one file -- why? answer: expands *.txt to haiku.txt\nfind . -name '*.txt'    # finds all three files -- good!\nLet’s wrap the last command into $()—called command substitution—as if it were a variable:\necho $(find . -name '*.txt')   # will print ./data/one.txt ./data/two.txt ./haiku.txt\nls -l $(find . -name '*.txt')   # will expand to ls -l ./data/one.txt ./data/two.txt ./haiku.txt\nwc -l $(find . -name '*.txt')   # will expand to wc -l ./data/one.txt ./data/two.txt ./haiku.txt\ngrep elegant $(find . -name '*.txt')   # will look for 'elegant' inside all *.txt files\n\n\nYour turn:\n\ngrep’s -v flag inverts pattern matching, so that only lines that do not match the pattern are printed.\nGiven that, which of the following commands will find all files in /data whose names end in ose.dat (e.g. sucrose.dat or maltose.dat), but do not contain the word temp?\n\nfind /data -name '*.dat' | grep ose | grep -v temp\nfind /data -name ose.dat | grep -v temp\ngrep -v temp $(find /data -name '*ose.dat')\nNone of the above\n\n\nHere is a video of a previous version of this workshop."
  },
  {
    "objectID": "bash/molecules/intro_find.html#running-a-command-on-the-results-of-find",
    "href": "bash/molecules/intro_find.html#running-a-command-on-the-results-of-find",
    "title": "Finding files",
    "section": "Running a command on the results of find",
    "text": "Running a command on the results of find\nLet’s say that you want to run a command on each of the files in the output of find. You can always do something using command substitution like this:\nfor f in $(find . -name \"*.txt\")\ndo\n    command on $f\ndone\nAlternatively, you can make it a one-liner:\nfind . -name \"*.txt\" -exec command {} \\;\nAnother—perhaps more elegant—one-line alternative is to use xargs. In its simplest usage, xargs command lets you construct a list of arguments:\nfind . -name \"*.txt\"                   # returns multiple lines\nfind . -name \"*.txt\" | xargs           # use those lines to construct a list\nfind . -name \"*.txt\" | xargs command   # pass this list as arguments to `command`\ncommand $(find . -name \"*.txt\")        # command substitution, achieving the same result (this is riskier!)\ncommand `(find . -name \"*.txt\")`       # alternative syntax for command substitution\nIn these examples, xargs achieves the same result as command substitution, but it is safer in terms of memory usage and the length of lists you can pass.\nWhen would you need to use this? A good example is with the command grep. grep takes a search stream (and not a list of files) as its standard input:\ncat filename | grep pattern\nTo pass a list of files to grep, you can use xargs that takes that list from its standard input and converts it into a list of arguments that is then passed to grep:\nfind . -name \"*.txt\" | xargs grep pattern   # search for `pattern` inside all those files (`grep` does not take a list of files as standard input)\n\n\nHere is a video of a previous version of this workshop."
  },
  {
    "objectID": "bash/molecules/intro_script.html",
    "href": "bash/molecules/intro_script.html",
    "title": "Writing scripts",
    "section": "",
    "text": "There are series of commands that you need to run regularly. Instead of having to type them each time, you can write them in a text file (called a script) with a .sh extension and execute that file whenever you want to run that set of commands. This is a great way to automate work.\nThis section covers scripts syntax and execution.",
    "crumbs": [
      "Bash/Zsh",
      "<b><em>Getting started</em></b>",
      "Writing scripts"
    ]
  },
  {
    "objectID": "bash/molecules/intro_script.html#writing-and-executing-scripts",
    "href": "bash/molecules/intro_script.html#writing-and-executing-scripts",
    "title": "Writing scripts",
    "section": "Writing and executing scripts",
    "text": "Writing and executing scripts\n\nScripts as arguments to bash\nA shell script is simply a text file. You can create it with a text editor such as nano which is installed on most systems.\nLet’s try to create one that we will call test.sh:\nnano test.sh\nIn the file, write the command: echo This is my first script.\nThis is the content of our test.sh file:\n\n\ntest.sh\n\necho This is my first script\n\nNow, how do we run this?\nWe simply pass it as an argument to the bash command:\nbash test.sh\nThis is my first script\nAnd it worked!\n\n\nShebang\nThere is another way to write and execute scripts: we can use a shebang.\nA shebang consists of the characters #! followed by the path of an executable. Here, the executable we want is bash and its path is /bin/bash.\nSo our script becomes:\n\n\ntest.sh\n\n#!/bin/bash\n\necho This is my first script.\n\nNow, the cool thing about this is that we don’t need to pass the script as an argument of the bash command anymore since the information that this should be executed by Bash is already written in the shebang. Instead, we can execute it with ./test.sh.\nBut there is a little twist:\n./test.sh\nbash: ./test.sh: Permission denied\nWe first need to make the file executable by changing its permissions.\n\n\nUnix permissions\nUnix systems such as Linux use POSIX permissions.\nTo add an executable permission to a file, you need to run:\nchmod u+x test.sh\nNow that our script is executable, we can run:\n./test.sh\nThis is my first script\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere and here are two videos of a previous version of this workshop.",
    "crumbs": [
      "Bash/Zsh",
      "<b><em>Getting started</em></b>",
      "Writing scripts"
    ]
  },
  {
    "objectID": "bash/molecules/intro_script.html#comments",
    "href": "bash/molecules/intro_script.html#comments",
    "title": "Writing scripts",
    "section": "Comments",
    "text": "Comments\nAnything to the right of the symbol # is ignored by the interpreter and is for human consumption only.\n# You can write full-line comments\n\npwd       # You can also write comments after a command\nComments are used to document scripts. DO USE THEM: future you will thank you.",
    "crumbs": [
      "Bash/Zsh",
      "<b><em>Getting started</em></b>",
      "Writing scripts"
    ]
  },
  {
    "objectID": "bash/top_intro.html",
    "href": "bash/top_intro.html",
    "title": "Getting started with Unix shells",
    "section": "",
    "text": "Unix shells such as Bash or Zsh are command line interpreters for Unix-like operating systems: the user enters commands as text—interactively in a terminal or in scripts—and the shell passes them to the operating system.\nGiving instructions to the machine via text instead of using a graphical user interface (GUI) is very powerful: while automating GUI operations is really difficult, it is easy to apply commands to many files, combine commands, and rerun scripts. Unix shells thus allow the creation of reproducible workflows and the automation of repetitive tasks. They also allow to securely access remote machines and HPC clusters.\nThis course is a hands-on introduction to Bash and Zsh. It will teach you to login to the Alliance supercomputers and covers common commands, loops, redirections, functions, wildcards, aliases, and scripting.\n\n Start course ➤",
    "crumbs": [
      "Bash/Zsh",
      "<b><em>Getting started</em></b>"
    ]
  },
  {
    "objectID": "bash/top_ws.html",
    "href": "bash/top_ws.html",
    "title": "Bash workshops",
    "section": "",
    "text": "Scripting for beginners\n\n\n\n\nSearching & manipulating text",
    "crumbs": [
      "Bash/Zsh",
      "<b><em>Workshops</em></b>"
    ]
  },
  {
    "objectID": "bash/wb_tools2.html",
    "href": "bash/wb_tools2.html",
    "title": "A few more of our favourite tools",
    "section": "",
    "text": "In a previous webinar, we presented three of our favourite command line tools. Today, we will introduce other tools we find really useful in our daily workflow:\n\nlazygit: a wonderful terminal UI for Git,\nbat: a great syntax highlighter,\nripgrep: a fast alternative to grep,\nfd: a /really/ fast alternative to find,\npass: a command line password manager.\n\nAlong the way, I will use a few other neat command line tools such as hyperfine—for sophisticated benchmarking—and diff-so-fancy—which makes your diffs a lot more readable.\nFor the Emacs users among you, we will finish the workshop with two Emacs utilities:\n\nTRAMP: a remote file access system,\nHelm: a “framework for incremental completions and narrowing selections”.",
    "crumbs": [
      "Bash/Zsh",
      "<b><em>Webinars</em></b>",
      "More fun tools for the CLI"
    ]
  },
  {
    "objectID": "bash/wb_tools3_slides.html#how-to-choose-tools",
    "href": "bash/wb_tools3_slides.html#how-to-choose-tools",
    "title": "Modern shell utilities",
    "section": "How to choose tools?",
    "text": "How to choose tools?\n\nPopularity (GitHub stars)\nIs it maintained? (date of last commit)\nHow polished is the documentation?\nHow fast is it? (what language is it written in?)\n\nShell/Python will be slower\nCompiled languages (Rust, C, Go) will be faster"
  },
  {
    "objectID": "bash/wb_tools3_slides.html#what-is-eza",
    "href": "bash/wb_tools3_slides.html#what-is-eza",
    "title": "Modern shell utilities",
    "section": "What is eza?",
    "text": "What is eza?\neza is a replacement for ls\n\nAdds colours\nBetter default options\nAdd tree feature"
  },
  {
    "objectID": "bash/wb_tools3_slides.html#installation",
    "href": "bash/wb_tools3_slides.html#installation",
    "title": "Modern shell utilities",
    "section": "Installation",
    "text": "Installation\nOn your machine\nInstructions here\nOn the Alliance clusters\neza is not installed on the Alliance clusters, so you have to install it locally under your own user. This is easy to do because it is written in Rust and can be installed with the Rust package manager\nLoad a Rust module, install eza, and make sure ~/.cargo/bin is in your path:\nmodule load rust/1.76.0\ncargo install eza\n\nYou only need to do this once. Once installed, eza will be accessible on subsequent sessions"
  },
  {
    "objectID": "bash/wb_tools3_slides.html#usage",
    "href": "bash/wb_tools3_slides.html#usage",
    "title": "Modern shell utilities",
    "section": "Usage",
    "text": "Usage\neza\n➔ Different colours for directories, symlinks, and different types of files and better defaults (compare ls -al with eza -al)\neza by default shows the output in a human readable format and without the group\nThe flags are similar to those of ls with the additional -T, equivalent to running the tree utility:\neza -T python/"
  },
  {
    "objectID": "bash/wb_tools3_slides.html#alias",
    "href": "bash/wb_tools3_slides.html#alias",
    "title": "Modern shell utilities",
    "section": "Alias",
    "text": "Alias\nYou can alias it to ls by adding to your .bashrc or .zshrc file:\nalias ls=eza\nIf you ever want to use the true ls utility, you can do so with \\ls"
  },
  {
    "objectID": "bash/wb_tools3_slides.html#alternative",
    "href": "bash/wb_tools3_slides.html#alternative",
    "title": "Modern shell utilities",
    "section": "Alternative",
    "text": "Alternative\nIf you want a simpler and more lightweight way to add colours to your ls outputs, you can look at LS_COLORS (I did this for years until I found eza)\nTo install it locally in the Alliance clusters, you download and uncompress a script, and copy it to a proper location:\nmkdir ./LS_COLORS &&\n    curl -L https://api.github.com/repos/trapd00r/LS_COLORS/tarball/master |\n        tar xzf - --directory=./LS_COLORS --strip=1 &&\n    mkdir -p ~/.local/share &&\n    cp ~/LS_COLORS/lscolors.sh ~/.local/share &&\n    rm -r ~/LS_COLORS\nThen you add to your .bashrc/.zshrc file the sourcing of the script and an alias to ls:\nsource ~/.local/share/lscolors.sh\nalias ls='ls --color'"
  },
  {
    "objectID": "bash/wb_tools3_slides.html#what-is-bat",
    "href": "bash/wb_tools3_slides.html#what-is-bat",
    "title": "Modern shell utilities",
    "section": "What is bat?",
    "text": "What is bat?\nbat is a replacement for cat\n\nAdds syntax highlighting for most programming languages\nAdds line numbers\nAdds pager-like search\nAdds pager-like navigation"
  },
  {
    "objectID": "bash/wb_tools3_slides.html#installation-1",
    "href": "bash/wb_tools3_slides.html#installation-1",
    "title": "Modern shell utilities",
    "section": "Installation",
    "text": "Installation\nOn your machine\nInstructions here\nOn the Alliance clusters\nbat is already installed on the Alliance clusters"
  },
  {
    "objectID": "bash/wb_tools3_slides.html#usage-1",
    "href": "bash/wb_tools3_slides.html#usage-1",
    "title": "Modern shell utilities",
    "section": "Usage",
    "text": "Usage\nUse bat as you would use cat:\nbat /home/marie/parvus/prog/progpy/pydoc/basics.py\nthen you are in your default pager\nAmong other options, you can disable the frame with -n\nand also remove the line numbers with -p"
  },
  {
    "objectID": "bash/wb_tools3_slides.html#what-is-fd",
    "href": "bash/wb_tools3_slides.html#what-is-fd",
    "title": "Modern shell utilities",
    "section": "What is fd?",
    "text": "What is fd?\nfd is a replacement for find\n\nWritten in Rust, automatic parallelism ➔ with vastly improved performance\nMore friendly syntax\nBy default excludes binaries as well as hidden files and directories\nBy default excludes patterns from .gitignore or other .ignore files"
  },
  {
    "objectID": "bash/wb_tools3_slides.html#installation-2",
    "href": "bash/wb_tools3_slides.html#installation-2",
    "title": "Modern shell utilities",
    "section": "Installation",
    "text": "Installation\nOn your machine\nInstructions here\nOn the Alliance clusters\nfd is already installed on the Alliance clusters"
  },
  {
    "objectID": "bash/wb_tools3_slides.html#basic-usage",
    "href": "bash/wb_tools3_slides.html#basic-usage",
    "title": "Modern shell utilities",
    "section": "Basic usage",
    "text": "Basic usage\nSearch file names for a pattern recursively in current directory:\nfd jx\n\nfd uses regexp by default, so you can use pattern symbols:\nfd jx.*txt\n\n Search file names recursively in another directory:\nfd top bash/"
  },
  {
    "objectID": "bash/wb_tools3_slides.html#print-all-files-in-some-directory-to-stdout",
    "href": "bash/wb_tools3_slides.html#print-all-files-in-some-directory-to-stdout",
    "title": "Modern shell utilities",
    "section": "Print all files in some directory to stdout",
    "text": "Print all files in some directory to stdout\nCurrent directory:\nfd\nAnother directory:\nfd . bash/"
  },
  {
    "objectID": "bash/wb_tools3_slides.html#options",
    "href": "bash/wb_tools3_slides.html#options",
    "title": "Modern shell utilities",
    "section": "Options",
    "text": "Options\nSearch for files with a particular file extension:\nfd -e txt\nUse a globbing pattern instead of regexp:\nfd -g wb* bash/\nExecute command for each result of fd in parallel:\nfd top bash/ -x rg layout\nExecute command once with all results of fd as arguments:\nfd top bash/ -X rg layout"
  },
  {
    "objectID": "bash/wb_tools3_slides.html#excluded-files-and-directories",
    "href": "bash/wb_tools3_slides.html#excluded-files-and-directories",
    "title": "Modern shell utilities",
    "section": "Excluded files and directories",
    "text": "Excluded files and directories\nBy default, fd excludes hidden files/directories and patterns in .gitignore (you can disable this with -H and -I respectively)\nThis makes fd combined with tree sometimes more useful than tree alone\nCompare tree bash/ with:\nfd . bash/ | tree --fromfile\n\nYou can make this a function:\nft () { fd $@ | tree --fromfile }\n\nExclude additional directories or patterns:\nfd -E *.txt -E img/ . bash/"
  },
  {
    "objectID": "bash/wb_tools3_slides.html#my-personal-alias",
    "href": "bash/wb_tools3_slides.html#my-personal-alias",
    "title": "Modern shell utilities",
    "section": "My personal alias",
    "text": "My personal alias\nI prefer to disable the default settings and exclude patterns based on a file I created:\nalias fd='fd -u --ignore-file /home/marie/.fdignore'"
  },
  {
    "objectID": "bash/wb_tools3_slides.html#what-is-ripgrep",
    "href": "bash/wb_tools3_slides.html#what-is-ripgrep",
    "title": "Modern shell utilities",
    "section": "What is ripgrep?",
    "text": "What is ripgrep?\nripgrep provides the rg utility—a replacement for grep\n\nWritten in Rust, automatic parallelism ➔ with vastly improved performance\nBy default excludes patterns from .gitignore or other .ignore files\nBy default excludes binaries as well as hidden files and directories\nBy default doesn’t follow symlinks"
  },
  {
    "objectID": "bash/wb_tools3_slides.html#installation-3",
    "href": "bash/wb_tools3_slides.html#installation-3",
    "title": "Modern shell utilities",
    "section": "Installation",
    "text": "Installation\nOn your machine\nInstructions here\nOn the Alliance clusters\nrg is already installed on the Alliance clusters"
  },
  {
    "objectID": "bash/wb_tools3_slides.html#usage-2",
    "href": "bash/wb_tools3_slides.html#usage-2",
    "title": "Modern shell utilities",
    "section": "Usage",
    "text": "Usage\nSearch lines in a file matching a pattern:\nrg colour /home/marie/parvus/prog/mint/bash/wb_tools3_slides.qmd\nSearch lines matching pattern in all files in current directory (recursively):\nrg colour\nrg and fd follow the same principles:\n\nUse regexp by default\nUse globbing pattern instead with -g\nSearch recursively by default\nSame excluded files"
  },
  {
    "objectID": "bash/wb_tools3_slides.html#what-is-zoxide",
    "href": "bash/wb_tools3_slides.html#what-is-zoxide",
    "title": "Modern shell utilities",
    "section": "What is zoxide?",
    "text": "What is zoxide?\nzoxide allows to easily jump to any directory"
  },
  {
    "objectID": "bash/wb_tools3_slides.html#installation-4",
    "href": "bash/wb_tools3_slides.html#installation-4",
    "title": "Modern shell utilities",
    "section": "Installation",
    "text": "Installation\nOn your machine\nInstructions here\n\nfzf (see below) adds cool functionality to it, so you might want to install it as well"
  },
  {
    "objectID": "bash/wb_tools3_slides.html#choose-a-different-command-name",
    "href": "bash/wb_tools3_slides.html#choose-a-different-command-name",
    "title": "Modern shell utilities",
    "section": "Choose a different command name",
    "text": "Choose a different command name\nUse this instead to use the command of your choice (e.g. j and ji)\ninstead of the default z and zi:\neval \"$(zoxide init --cmd j bash)\""
  },
  {
    "objectID": "bash/wb_tools3_slides.html#usage-3",
    "href": "bash/wb_tools3_slides.html#usage-3",
    "title": "Modern shell utilities",
    "section": "Usage",
    "text": "Usage\nType z (or whatever command you chose) instead of cd\nYou can simplify the path to just a few characters\nIf there are multiple locations matching your entry, the algorithm will chose the highest ranking one based on your visit frequency and how recently you visited a path\nThis means that you can visit your usual places with a few key strokes. For less frequent places, add more info\nFinally, if you want to choose amongst all possible options in a completion framework, use zi instead and zoxide will open fzf"
  },
  {
    "objectID": "bash/wb_tools3_slides.html#alternative-1",
    "href": "bash/wb_tools3_slides.html#alternative-1",
    "title": "Modern shell utilities",
    "section": "Alternative",
    "text": "Alternative\nA tool that served me well until someone pointed zoxide to me is autojump\nInstallation\nInstructions here for your machine\nautojump is installed on the Alliance clusters, but you need add to your .bashrc or .zshrc:\n[[ -s $EPREFIX/etc/profile.d/autojump.sh ]] && source $EPREFIX/etc/profile.d/autojump.sh\nUsage\nSimilar to zoxide but you first need to visit directories so that they get entered in a database\nj is a wrapper for autojump, jc jumps to subdirectories of current directory"
  },
  {
    "objectID": "bash/wb_tools3_slides.html#what-is-fzf",
    "href": "bash/wb_tools3_slides.html#what-is-fzf",
    "title": "Modern shell utilities",
    "section": "What is fzf?",
    "text": "What is fzf?\nfzf allows to find elements of any list through incremental completion and fuzzy matching. It can be paired with any number of commands"
  },
  {
    "objectID": "bash/wb_tools3_slides.html#installation-6",
    "href": "bash/wb_tools3_slides.html#installation-6",
    "title": "Modern shell utilities",
    "section": "Installation",
    "text": "Installation\nOn your machine\nInstructions here\nOn the Alliance clusters\nfzf is already installed on the Alliance clusters\nTo get fzf kbds and fuzzy completion in your shell, add to your .bashrc:\neval \"$(fzf --bash)\"\nand/or your .zshrc:\nsource &lt;(fzf --zsh)"
  },
  {
    "objectID": "bash/wb_tools3_slides.html#direct-usage",
    "href": "bash/wb_tools3_slides.html#direct-usage",
    "title": "Modern shell utilities",
    "section": "Direct usage",
    "text": "Direct usage\nIf you run fzf directly, it will search the current directory recursively, do a narrowing selection, and print the result:\nfzf\nYou can make use of fd to remove unnecessary entries:\nexport FZF_DEFAULT_COMMAND='fd -u --ignore-file /home/marie/.fdignore'"
  },
  {
    "objectID": "bash/wb_tools3_slides.html#fzf-kbds",
    "href": "bash/wb_tools3_slides.html#fzf-kbds",
    "title": "Modern shell utilities",
    "section": "fzf kbds",
    "text": "fzf kbds\nThere are 3 default kbds:\n\nCtl+t ➔ paste selected file/dir into the command\nCtl+r ➔ paste selected command from history into the command\nAlt+c ➔ cd into selected dir"
  },
  {
    "objectID": "bash/wb_tools3_slides.html#pipe-to-fzf",
    "href": "bash/wb_tools3_slides.html#pipe-to-fzf",
    "title": "Modern shell utilities",
    "section": "Pipe to fzf",
    "text": "Pipe to fzf\nYou can also pipe the output of any command that returns a list of elements into fzf\nLook for a file/directory:\nls | fzf\n Many flags to select order of entries, type of completion, preview, case-sensitivity, and more\nLook for a running process:\nps -ef | fzf --cycle -i -e +s --tac --reverse"
  },
  {
    "objectID": "bash/wb_tools3_slides.html#what-is-a-tui",
    "href": "bash/wb_tools3_slides.html#what-is-a-tui",
    "title": "Modern shell utilities",
    "section": "What is a TUI?",
    "text": "What is a TUI?\nTerminal user interfaces (TUIs) are the predecessors to graphical user interfaces (GUIs) which are entirely text based and run in terminals\nThey have remained very popular among command line aficionados because they are fast, efficient, powerful, and keyboard-driven, while being friendly and visual\nFantastic modern ones keep being built for tasks as diverse as interfaces to Git, music players, games, emails, dashboards, and, for our purpose here, file system management"
  },
  {
    "objectID": "bash/wb_tools3_slides.html#formerly-most-popular-file-system-tuis",
    "href": "bash/wb_tools3_slides.html#formerly-most-popular-file-system-tuis",
    "title": "Modern shell utilities",
    "section": "Formerly most popular file system TUIs",
    "text": "Formerly most popular file system TUIs\nThere are many file system TUIs and all of them are actually really good. The two most notable ones used to be:\n\nranger\n\nExtremely sophisticated, easy to customize, tons of features\n\nBuilt in Python, it can be slow for operations in directories with thousands of files\n\n\nnnn\n\nMinimalist and very fast (written in C)\n\nNot easy to customize (many customizations require compiling from source). Most functionalities rely on plugins that need to be installed. Not easy to get started with"
  },
  {
    "objectID": "bash/wb_tools3_slides.html#the-new-kid-yazi",
    "href": "bash/wb_tools3_slides.html#the-new-kid-yazi",
    "title": "Modern shell utilities",
    "section": "The new kid: yazi",
    "text": "The new kid: yazi\nyazi is a brand new fs TUI that has quickly become the most popular\nIt is extremely modern, very fast (written in Rust), very well documented, intuitive, easy to customize, and integrates with modern utilities such as fd, rg, zoxide, and fzf out of the box\nOnly at version 0.4, it is not fully mature yet, but it has already more stars on GitHub than ranger and nnn because it combines ease of customization and sophistication with speed"
  },
  {
    "objectID": "bash/wb_tools3_slides.html#alternatives",
    "href": "bash/wb_tools3_slides.html#alternatives",
    "title": "Modern shell utilities",
    "section": "Alternatives",
    "text": "Alternatives\nIn decreasing number of stars on GitHub:\n\nbroot\nsuperfile\nlf\nxplr\nfff (now archived)\nvifm\nmc"
  },
  {
    "objectID": "bash/wb_tools3_slides.html#my-3-favourite-plugins",
    "href": "bash/wb_tools3_slides.html#my-3-favourite-plugins",
    "title": "Modern shell utilities",
    "section": "My 3 favourite plugins",
    "text": "My 3 favourite plugins\nThere are many plugins for Z shell and the (very bloated) Oh My Zsh, but I am sticking to 3 great plugins inspired or directly coming from the Fish shell:\n\nSyntax highlighting\nAutosuggestions\nHistory substring search"
  },
  {
    "objectID": "bash/wb_tools3_slides.html#installation-7",
    "href": "bash/wb_tools3_slides.html#installation-7",
    "title": "Modern shell utilities",
    "section": "Installation",
    "text": "Installation\nAll plugins can be installed (info in their README) or simply Git cloned. zsh-syntax-highlighting is already installed on the Alliance clusters, so you only need to clone the other two:\n# create a directory to store the scripts\nmkdir ~/.zsh_plugins\n# autosuggestions\ngit clone https://github.com/zsh-users/zsh-autosuggestions.git ~/.zsh_plugins/zsh-autosuggestions\n# history substring search\ngit clone https://github.com/zsh-users/zsh-history-substring-search.git ~/.zsh_plugins/zsh-history-substring-search\nThen you need to source them (including zsh-syntax-highlighting), so add to your .zshrc file:\nsource $EPREFIX/usr/share/zsh/site-functions/zsh-syntax-highlighting.zsh\nsource ~/.zsh_plugins/zsh-history-substring-search/zsh-history-substring-search.zsh\nsource ~/.zsh_plugins/zsh-autosuggestions/zsh-autosuggestions.zsh"
  },
  {
    "objectID": "bash/wb_tools3_slides.html#usage-5",
    "href": "bash/wb_tools3_slides.html#usage-5",
    "title": "Modern shell utilities",
    "section": "Usage",
    "text": "Usage\nYou now have syntax highlighting in your shell inputs\nTo use the history substring search, start typing some command\nthen press Alt+p or Alt+n\nIt will cycle through all entries in your history that start that way\nFinally, the autosuggestion will suggest commands based on your history and/or classic suggestions\nAccept the whole command with Ctl+e or a single word with Alt+f"
  },
  {
    "objectID": "bash/ws_text.html",
    "href": "bash/ws_text.html",
    "title": "Searching & manipulating text",
    "section": "",
    "text": "cd ~/Desktop/data-shell/writing\nmore haiku.txt\nFirst let’s search for text in files:\ngrep not haiku.txt     # let's find all lines that contain the word 'not'\ngrep day haiku.txt     # now search for word 'day'\ngrep -w day haiku.txt     # search for a separate word 'day' (not 'today', etc.)\ngrep -w today haiku.txt   # search for 'today'\ngrep -w Today haiku.txt   # search for 'Today'\ngrep -i -w today haiku.txt       # both upper and lower case 'today'\ngrep -n -i -w today haiku.txt    # -n prints out numbers the matching lines\ngrep -n -i -w -v the haiku.txt   # -v searches for lines that do not contain 'the'\nman grep\nMore than two arguments to grep:\ngrep pattern file1 file2 file3   # all argument after the first one are assumed to be filenames\ngrep pattern *.txt   # the last argument will expand to the list of *.txt files\n\nThe Tao that is seen\nIs not the true Tao, until\nYou bring fresh toner.\nWith searching comes loss\nand the presence of absence:\n\"My Thesis\" not found.\nYesterday it worked.\nToday it is not working.\nSoftware is like that.\nFrom the above text, contained in the file haiku.txt, which command would result in the following output:\nand the presence of absence:\n\ngrep of haiku.txt\ngrep -E of haiku.txt\ngrep -w of haiku.txt \n\nHere is a video on this topic.",
    "crumbs": [
      "Bash/Zsh",
      "<b><em>Workshops</em></b>",
      "Searching & manipulating text"
    ]
  },
  {
    "objectID": "bash/ws_text.html#searching-inside-files-with-grep",
    "href": "bash/ws_text.html#searching-inside-files-with-grep",
    "title": "Searching & manipulating text",
    "section": "",
    "text": "cd ~/Desktop/data-shell/writing\nmore haiku.txt\nFirst let’s search for text in files:\ngrep not haiku.txt     # let's find all lines that contain the word 'not'\ngrep day haiku.txt     # now search for word 'day'\ngrep -w day haiku.txt     # search for a separate word 'day' (not 'today', etc.)\ngrep -w today haiku.txt   # search for 'today'\ngrep -w Today haiku.txt   # search for 'Today'\ngrep -i -w today haiku.txt       # both upper and lower case 'today'\ngrep -n -i -w today haiku.txt    # -n prints out numbers the matching lines\ngrep -n -i -w -v the haiku.txt   # -v searches for lines that do not contain 'the'\nman grep\nMore than two arguments to grep:\ngrep pattern file1 file2 file3   # all argument after the first one are assumed to be filenames\ngrep pattern *.txt   # the last argument will expand to the list of *.txt files\n\nThe Tao that is seen\nIs not the true Tao, until\nYou bring fresh toner.\nWith searching comes loss\nand the presence of absence:\n\"My Thesis\" not found.\nYesterday it worked.\nToday it is not working.\nSoftware is like that.\nFrom the above text, contained in the file haiku.txt, which command would result in the following output:\nand the presence of absence:\n\ngrep of haiku.txt\ngrep -E of haiku.txt\ngrep -w of haiku.txt \n\nHere is a video on this topic.",
    "crumbs": [
      "Bash/Zsh",
      "<b><em>Workshops</em></b>",
      "Searching & manipulating text"
    ]
  },
  {
    "objectID": "bash/ws_text.html#text-manipulation",
    "href": "bash/ws_text.html#text-manipulation",
    "title": "Searching & manipulating text",
    "section": "Text manipulation",
    "text": "Text manipulation\n(This example was kindly provided by John Simpson.)\nIn this section we’ll use two tools for text manipulation: sed and tr. Our goal is to calculate the frequency of all dictionary words in the novel “The Invisible Man” by Herbert Wells (public domain). First, let’s apply our knowledge of grep to this text:\n$ cd ~/Desktop/data-shell\n$ ls   # shows wellsInvisibleMan.txt\n$ wc wellsInvisibleMan.txt                          # number of lines, words, characters\n$ grep invisible wellsInvisibleMan.txt              # see the invisible man\n$ grep invisible wellsInvisibleMan.txt | wc -l      # returns 60; adding -w gives the same count\n$ grep -i invisible wellsInvisibleMan.txt | wc -l   # returns 176 (includes: invisible Invisible INVISIBLE)\nLet’s sidetrack for a second and see how we can use the “stream editor” sed:\n$ sed 's/[iI]nvisible/supervisible/g' wellsInvisibleMan.txt &gt; visibleMan.txt   # make him visible\n$ cat wellsInvisibleMan.txt | sed 's/[iI]nvisible/supervisible/g' &gt; visibleMan.txt   # this also works (standard input)\n$ grep supervisible visibleMan.txt   # see what happened to the now visible man\n$ grep -i invisible visibleMan.txt   # see what was not converted\n$ man sed\nNow let’s remove punctuation from the original file using “tr” (translate) command:\n$ cat wellsInvisibleMan.txt | tr -d \"[:punct:]\" &gt; invisibleNoPunct.txt    # tr only takes standard input\n$ tail wellsInvisibleMan.txt\n$ tail invisibleNoPunct.txt\nNext convert all upper case to lower case:\n$ cat invisibleNoPunct.txt | tr '[:upper:]' '[:lower:]' &gt; invisibleClean.txt\n$ tail invisibleClean.txt\nNext replace spaces with new lines:\n$ cat invisibleClean.txt | sed 's/ /\\'$'\\n/g' &gt; invisibleList.txt   # \\'$'\\n is a shortcut for a new line\n$ more invisibleList.txt\nNext remove empty lines:\n$ sed '/^$/d' invisibleList.txt  &gt; invisibleCompact.txt\nNext sort the list alphabetically, count each word’s occurrence, and remove duplicate words:\n$ cat invisibleCompact.txt | sort | uniq -c &gt; invisibleWords.txt\n$ more invisibleWords.txt\nNext sort the list into most frequent words:\n$ cat invisibleWords.txt | sort -gr &gt; invisibleFrequencyList.txt   # use 'man sort'\n$ more invisibleFrequencyList.txt\n\n\n\n\n\n\nYou can watch a video for this topic after the workshop.\nQuick reference:\nsed 's/pattern1/pattern2/' filename    # replace pattern1 with pattern2, one per line\nsed 's/pattern1/pattern2/g' filename   # same but multiple per line\nsed 's|pattern1|pattern2|g' filename   # same\n\ncat wellsInvisibleMan.txt | tr -d \"[:punct:]\" &gt; invisibleNoPunct.txt       # remove punctuation; tr only takes standard input\ncat invisibleNoPunct.txt | tr '[:upper:]' '[:lower:]' &gt; invisibleClean.txt # convert all upper case to lower case:\ncat invisibleClean.txt | sed 's/ /\\'$'\\n/g' &gt; invisibleList.txt            # replace spaces with new lines;\n                                                                           # \\'$'\\n is a shortcut for a new line\nsed '/^$/d' invisibleList.txt  &gt; invisibleCompact.txt   # remove empty lines\ncat invisibleCompact.txt | sort | uniq -c &gt; invisibleWords.txt   # sort the list alphabetically, count each word's occurrence\ncat invisibleWords.txt | sort -gr &gt; invisibleFrequencyList.txt   # sort the list into most frequent words\n\nWrite a script that takes an English-language file and print the list of its 100 most common words, along with the word count. Hint: use the workflow from the text manipulation video. Finally, convert this script into a bash function. (no need to type any answer)",
    "crumbs": [
      "Bash/Zsh",
      "<b><em>Workshops</em></b>",
      "Searching & manipulating text"
    ]
  },
  {
    "objectID": "bash/ws_text.html#column-based-text-processing-with-awk-scripting-language",
    "href": "bash/ws_text.html#column-based-text-processing-with-awk-scripting-language",
    "title": "Searching & manipulating text",
    "section": "Column-based text processing with awk scripting language",
    "text": "Column-based text processing with awk scripting language\ncd .../data-shell/writing\ncat haiku.txt   # 11 lines\nYou can define inline awk scripts with braces surrounded by single quotation:\nawk '{print $1}' haiku.txt       # $1 is the first field (word) in each line =&gt; processing columns\nawk '{print $0}' haiku.txt       # $0 is the whole line\nawk '{print}' haiku.txt          # the whole line is the default action\nawk -Fa '{print $1}' haiku.txt   # can specify another separator with -F (\"a\" in this case)\nYou can use multiple commands inside your awk script:\necho Hello Tom &gt; hello.txt\necho Hello John &gt;&gt; hello.txt\nawk '{$2=\"Adam\"; print $0}' hello.txt   # we replaced the second word in each line with \"Adam\"\nMost common awk usage is to postprocess output of other commands:\n/bin/ps aux    # display all running processes as multi-column output\n/bin/ps aux | awk '{print $2 \" \" $11}'   # print only the process number and the command\nAwk also takes patterns in addition to scripts:\nawk '/Yesterday|Today/' haiku.txt   # print the lines that contain the words Yesterday or Today\nAnd then you act on these patterns: if the pattern evaluates to True, then run the script:\nawk '/Yesterday|Today/{print $3}' haiku.txt\nawk '/Yesterday|Today/' haiku.txt | awk '{print $3}'   # same as previous line\nAwk has a number of built-in variables; the most commonly used is NR:\nawk 'NR&gt;1' haiku.txt    # if NumberRecord &gt;1 then print it (default action), i.e. skip the first line\nawk 'NR&gt;1{print $0}' haiku.txt   # last command expanded\nawk 'NR&gt;1 && NR &lt; 5' haiku.txt   # print lines 2-4\n\nExercise: write a awk script to process cities.csv to print only town/city names and their population and store it in a separate file populations.csv. Try to do everything in a single-line command.\n\nQuick reference:\nls -l | awk 'NR&gt;3 {print $5 \"  \" $9}'   # print 5th and 9th columns starting with line 4\nawk 'NR&gt;1 && NR &lt; 5' haiku.txt          # print lines 2-4\nawk '/Yesterday|Today/' haiku.txt       # print lines that contain Yesterday or Today\n\nWrite a one-line command that finds 5 largest files in the current directory and prints only their names and file sizes in the human-readable format (indicating bytes, kB, MB, GB, …) in the decreasing file-size order. Hint: use find, xargs, and awk. \n\nLet’s study together these commands:\n$ source ~/projects/def-sponsor00/shared/fzf/.fzf.bash\n$ kill -9 `/bin/ps aux | fzf | awk '{print $2}'`\n\nHere is a video on this topic.",
    "crumbs": [
      "Bash/Zsh",
      "<b><em>Workshops</em></b>",
      "Searching & manipulating text"
    ]
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact",
    "section": "",
    "text": "Please email us at: training at westdri dot ca."
  },
  {
    "objectID": "emacs/index.html",
    "href": "emacs/index.html",
    "title": "Emacs",
    "section": "",
    "text": "Getting started with  \nAn intro course to Emacs\n\n\n\n\n60 min webinars\nVarious Emacs topics",
    "crumbs": [
      "Emacs",
      "<br>&nbsp;<img src=\"img/logo_emacs.png\" class=\"img-fluid\" style=\"width:1.75em\" alt=\"noshadow\"><br><br>"
    ]
  },
  {
    "objectID": "emacs/intro_backup.html",
    "href": "emacs/intro_backup.html",
    "title": "Backups and auto-saving",
    "section": "",
    "text": "By default, Emacs has two mechanisms helping to prevent data loss: backups and auto-saving.",
    "crumbs": [
      "Emacs",
      "<em><b>Getting started</b></em>",
      "Backups and auto-saving"
    ]
  },
  {
    "objectID": "emacs/intro_backup.html#backups",
    "href": "emacs/intro_backup.html#backups",
    "title": "Backups and auto-saving",
    "section": "Backups",
    "text": "Backups\nBy default, Emacs creates a single backup file for each file (although, of course, this can be changed). Each time you re-open a file and make changes, the version prior to this current editing session gets saved as the backup file.\nBackup files have names ending with ~.\n\n\nYour turn:\n\nCreate a file called file.txt and add the following content in it:\nThis is my file.\nSave it (C-x C-s).\nSend Emacs to the background (C-z) and run ls in the terminal. You should see a file called file.txt.\ncat file.txt shows you that it contains: This is my file.\nNow, close, then re-open the file and make the following changes:\nThis is my file, but I have re-edited it.\nSave it again.\nUsing ls and cat again (remember that you can also run Bash commands within Emacs with M-!), you should see that, in addition to your file file.txt with the content: This is my file, but I have re-edited it., there is now a backup file called file.txt~ with the content: This is my file.\nAs long as you don’t close the file, nothing happens to the backup file. But if you close it, re-open it, make new changes to it, and save it, the backup file.txt~ will now contain: This is my file, but I have re-edited it. (the version of the last editing session).\n\nBackup files preserve files prior to the current editing session. This is useful if you make terrible mistakes during an editing session.\n\nIf you don’t like having backup files next to your files, you can hide them out of the way thanks to the variable backup-directory-alist which allows you to store backup files in a directory of your choice.\nAlso, remember that in Dired, you can remove all backup files by typing ~.",
    "crumbs": [
      "Emacs",
      "<em><b>Getting started</b></em>",
      "Backups and auto-saving"
    ]
  },
  {
    "objectID": "emacs/intro_backup.html#auto-saving",
    "href": "emacs/intro_backup.html#auto-saving",
    "title": "Backups and auto-saving",
    "section": "Auto-saving",
    "text": "Auto-saving\nAuto-saving is a much more familiar concept as many software do this. This ensures that you don’t lose too much work if your computer crashes. By default, this happens every 300 keystrokes or after 30 seconds of idle time. Now, what is unusual is that Emacs saves the file in a separate file so as not to touch the file you are working on (this can be very useful if saving a file triggers some costly process such as re-rendering a website or if you don’t want to save temporary sensitive information).\nAuto-saved files are marked with # at the start and end of their names.\n\n\nYour turn:\n\nEdit file.txt and don’t save it for 300 keystrokes or idle for 30 seconds after some edits without saving. Now run ls and you will see that an auto-save file #file.txt# got created.\n\n\nIf you don’t like having auto-save files next to your files, you can hide them out of the way thanks to the variable auto-save-file-name-transforms which allows you to store backup files in a directory of your choice.\nAlso, remember that in Dired, you can remove all auto-save files by typing #.\n\n\nRecovering data\nIf you run M-x recover-this-file from within file.txt, its content will be replaced by the content of #file.txt#. You can also do this from outside the file with M-x recover-file and then entering the file name.\nIf your system crashes while you have unsaved changes in a file, Emacs will offer you to recover the content of your file from its auto-saved version next time you open it.",
    "crumbs": [
      "Emacs",
      "<em><b>Getting started</b></em>",
      "Backups and auto-saving"
    ]
  },
  {
    "objectID": "emacs/intro_customize.html",
    "href": "emacs/intro_customize.html",
    "title": "Customizing Emacs",
    "section": "",
    "text": "Customizing Emacs is obviously a very complex topic. This section only aims at getting you started using the easy customization interface.",
    "crumbs": [
      "Emacs",
      "<em><b>Getting started</b></em>",
      "Customizing Emacs"
    ]
  },
  {
    "objectID": "emacs/intro_customize.html#customizing-emacs",
    "href": "emacs/intro_customize.html#customizing-emacs",
    "title": "Customizing Emacs",
    "section": "Customizing Emacs",
    "text": "Customizing Emacs\nIn Emacs, everything is customizable.\nThere are two ways to customize Emacs (and you use both):\n\nyou can write Emacs Lisp code in your init file (the configuration file that gets loaded when Emacs launches),\nyou can use the easy customization interface which automatically adds code in your init file.",
    "crumbs": [
      "Emacs",
      "<em><b>Getting started</b></em>",
      "Customizing Emacs"
    ]
  },
  {
    "objectID": "emacs/intro_customize.html#the-init-file",
    "href": "emacs/intro_customize.html#the-init-file",
    "title": "Customizing Emacs",
    "section": "The init file",
    "text": "The init file\nIt is standard to call the init file .emacs and create it in your home (~/). Of course, like everything else, this too can be customized 🙂\nThis file is read at startup and loads your Emacs configurations.",
    "crumbs": [
      "Emacs",
      "<em><b>Getting started</b></em>",
      "Customizing Emacs"
    ]
  },
  {
    "objectID": "emacs/intro_customize.html#easy-customization-interface",
    "href": "emacs/intro_customize.html#easy-customization-interface",
    "title": "Customizing Emacs",
    "section": "Easy customization interface",
    "text": "Easy customization interface\nThe easy customization interface can be accessed by running one of the customize commands, the most important of which are:\n\ncustomize-group for groups of variables,\ncustomize-variable for individual variable,\ncustomize-face for the appearance of one type of text.\n\n\n\nYour turn:\n\nType M-x customize-group python to enter the Python group and have a look at the various variables in that group.\nTry to change the default of the variable python-indent-offset. (You can select it from the group page, or you can access it directly with M-x customize-variable python-indent-offset).\nNow, try to customize the font-lock-keyword-face face. You can run M-x customize-face font-lock-keyword-face, but an easier way is to place the cursor on an element of that face and run M-x customize-face. The name of the face your cursor was on will appear as the default, so you can then simply press Return.\n\nThese will add customization at the top of your init file. If this file doesn’t exist, it will automatically create it.\nThe customization will be in the form:\n(custom-set-variables\n ;; custom-set-variables was added by Custom.\n ;; If you edit it by hand, you could mess it up, so be careful.\n ;; Your init file should contain only one such instance.\n ;; If there is more than one, they won't work right.\n(python-indent-offset 2))\n(custom-set-faces\n ;; custom-set-faces was added by Custom.\n ;; If you edit it by hand, you could mess it up, so be careful.\n ;; Your init file should contain only one such instance.\n ;; If there is more than one, they won't work right.\n (font-lock-keyword-face ((t (:foreground \"green\")))))",
    "crumbs": [
      "Emacs",
      "<em><b>Getting started</b></em>",
      "Customizing Emacs"
    ]
  },
  {
    "objectID": "emacs/intro_install.html",
    "href": "emacs/intro_install.html",
    "title": "Installation and access",
    "section": "",
    "text": "In this section, we will make sure that you can access Emacs on our remote cluster.",
    "crumbs": [
      "Emacs",
      "<em><b>Getting started</b></em>",
      "Installation and access"
    ]
  },
  {
    "objectID": "emacs/intro_install.html#installing-emacs-on-your-machine",
    "href": "emacs/intro_install.html#installing-emacs-on-your-machine",
    "title": "Installation and access",
    "section": "Installing Emacs on your machine",
    "text": "Installing Emacs on your machine\nTo download and install Emacs on your computer, simply follow the instructions in the official documentation.\n\nIf you are on Linux, make sure to install the version of Emacs with native compilation and jansson support.\nNative compilation was added as an option to Emacs 28 and greatly speeds up Emacs, in particular startup time. You will need libgccjit installed on your system.\nJansson support was added as an option to Emacs 27 and speeds up anything that involves JSON files and makes lsp-mode and eglot in particular much faster.\nIf you don’t use Linux, you will have to install libgccjit and the version of jansson for your OS (could be called libjansson), then compile Emacs from source with the --with-native-compilation and --with-json flags.",
    "crumbs": [
      "Emacs",
      "<em><b>Getting started</b></em>",
      "Installation and access"
    ]
  },
  {
    "objectID": "emacs/intro_install.html#accessing-emacs-on-the-alliance-clusters",
    "href": "emacs/intro_install.html#accessing-emacs-on-the-alliance-clusters",
    "title": "Installation and access",
    "section": "Accessing Emacs on the Alliance clusters",
    "text": "Accessing Emacs on the Alliance clusters\nTo ensure that we are all working in the same environment, for this course, we will use Emacs in a training cluster. This will also prepare you for using it on the Alliance clusters.\nAll you need to do is to log in to our cluster through SSH. Emacs is then available without having to load any module.\n\nWindows users\nLaunch PowerShell and type ssh to see whether OpenSSH is installed and enabled on your system. If it is, follow the instructions for macOS and Linux users below.\nIf it is not, install the free version of MobaXTerm and launch it, then follow the first 18% of this demo.\nFor “Remote host”, use the hostname we gave you.\nSelect the box “Specify username” and provide your username.\n\nNote that the password is entered through blind typing, meaning that you will not see anything happening as you type it. This is a Linux feature. While it is a little disturbing at first, do know that it is working. Make sure to type it slowly to avoid typos, then press the “enter” key on your keyboard.\n\n\n\nmacOS and Linux users\nOpen a terminal emulator:\n\nmacOS users:   Launch Terminal.\nLinux users:     Open the terminal emulator of your choice.\n\n(For Windows users with ssh available in PowerShell, use PowerShell as the terminal emulator).\nIn it, run:\nssh &lt;username&gt;@&lt;hostname&gt;\n\nReplace the username and hostname by their values. For instance:\nssh user021@somecluster.c3.ca\n\nYou will be asked a question, answer “Yes”.\nWhen prompted, type the password.\n\nNote that the password is entered through blind typing, meaning that you will not see anything happening as you type it. This is a Linux feature. While it is a little disturbing at first, do know that it is working. Make sure to type it slowly to avoid typos, then press the “enter” key on your keyboard.\n\n\n\nTroubleshooting\nProblems logging in are almost always due to typos. If you cannot log in, retry slowly, entering your password carefully.\nNow that you are logged in, in the next section, we will see how to launch Emacs.",
    "crumbs": [
      "Emacs",
      "<em><b>Getting started</b></em>",
      "Installation and access"
    ]
  },
  {
    "objectID": "emacs/intro_modes.html",
    "href": "emacs/intro_modes.html",
    "title": "Emacs modes",
    "section": "",
    "text": "At the core of Emacs functioning are modes. This section will explain what Emacs major and minor modes are.",
    "crumbs": [
      "Emacs",
      "<em><b>Getting started</b></em>",
      "Emacs modes"
    ]
  },
  {
    "objectID": "emacs/intro_modes.html#major-modes",
    "href": "emacs/intro_modes.html#major-modes",
    "title": "Emacs modes",
    "section": "Major modes",
    "text": "Major modes\nDifferent types of text require different behaviours, syntax highlighting, formatting, functions, variables, etc. Consequently, each type of buffer (e.g. Python script, Markdown document, Julia REPL, Bash shell, directory editor, pdf) is associated with a different major mode.\nFile extensions, particular markers in the file, or other elements tell Emacs to automatically switch to the appropriate major mode.\nOnly one major mode is active at a time.\nSwitching to a different major mode is possible by running the corresponding major mode command (e.g. M-x python-mode will switch to Python mode).\n\nFundamental mode\nfundamental-mode is the most basic major mode, with no particular feature. This is the mode enabled by default if Emacs cannot detect what specific major mode to enable.",
    "crumbs": [
      "Emacs",
      "<em><b>Getting started</b></em>",
      "Emacs modes"
    ]
  },
  {
    "objectID": "emacs/intro_modes.html#minor-modes",
    "href": "emacs/intro_modes.html#minor-modes",
    "title": "Emacs modes",
    "section": "Minor modes",
    "text": "Minor modes\nMinor modes provide additional and optional features that can be turned on or off (e.g. spell checking, auto-completion, auto-indentation, fancy undo behaviour, fancy parenthesis matching highlighting).\nMinor modes can be turned on/off by running the corresponding minor mode commands (e.g. M-x flyspell-mode will turn spell checking on/off).\nThe command consult-minor-mode-menu from the package consult makes this particularly easy (we will see in a later section how to install packages).\nEach mode comes with a set of commands. consult’s command consult-mode-command makes it easy to search for commands within each mode.\nAny number of minor modes can be active at the same time.",
    "crumbs": [
      "Emacs",
      "<em><b>Getting started</b></em>",
      "Emacs modes"
    ]
  },
  {
    "objectID": "emacs/intro_modes.html#list-of-enabled-modes",
    "href": "emacs/intro_modes.html#list-of-enabled-modes",
    "title": "Emacs modes",
    "section": "List of enabled modes",
    "text": "List of enabled modes\nBy default, C-h m or M-x describe-mode will open a list and description of the active modes.\nThe major mode can also be determined with C-h v major-mode (C-h v runs the command describe-variable).\nFinally, a list of minor modes can be viewed with C-h v minor-mode-list.\nHere too, the package consult makes this much nicer, thanks to the command consult-minor-mode-menu.",
    "crumbs": [
      "Emacs",
      "<em><b>Getting started</b></em>",
      "Emacs modes"
    ]
  },
  {
    "objectID": "emacs/intro_modes.html#the-mode-line",
    "href": "emacs/intro_modes.html#the-mode-line",
    "title": "Emacs modes",
    "section": "The mode line",
    "text": "The mode line\nAnother way to get information about enabled modes is the mode line.\nRemember that the mode line is that line near the bottom of the window with a series of information:",
    "crumbs": [
      "Emacs",
      "<em><b>Getting started</b></em>",
      "Emacs modes"
    ]
  },
  {
    "objectID": "emacs/intro_modes.html#hooks",
    "href": "emacs/intro_modes.html#hooks",
    "title": "Emacs modes",
    "section": "Hooks",
    "text": "Hooks\nMinor modes can be automatically enabled when other modes (major or minor) are enabled thanks to hooks.\n\nFor example, to enable the aggressive indent minor mode whenever the ESS R major mode is enabled, you can add to your init file:\n\n(add-hook 'ess-r-mode-hook 'aggressive-indent-mode)\n\nOr, using use-package, now part of base Emacs:\n\n(use-package aggressive-indent\n    :hook (ess-r-mode . aggressive-indent-mode))\nWe will learn how to customize Emacs in a later section, so don’t worry if this doesn’t make much sense yet.",
    "crumbs": [
      "Emacs",
      "<em><b>Getting started</b></em>",
      "Emacs modes"
    ]
  },
  {
    "objectID": "emacs/intro_modes.html#modes-source-code",
    "href": "emacs/intro_modes.html#modes-source-code",
    "title": "Emacs modes",
    "section": "Modes source code",
    "text": "Modes source code\nTo see the source code of a mode, run C-h v (or M-x describe-variable) followed by the name of the mode map. This will open a help buffer with a link to the source code file.\n\nFor example C-h v text-mode-map will open a help buffer with a link to text-mode.el.\n\n\nThe help buffer opened by C-h m or M-x describe-mode also gives a link to the source code of the major mode.\n\nLooking at the source code of a mode is very useful to customize it.",
    "crumbs": [
      "Emacs",
      "<em><b>Getting started</b></em>",
      "Emacs modes"
    ]
  },
  {
    "objectID": "emacs/intro_modes.html#polymode",
    "href": "emacs/intro_modes.html#polymode",
    "title": "Emacs modes",
    "section": "Polymode",
    "text": "Polymode\nWhile it is normally impossible to associate multiple major modes with a single buffer, Polymode allows to insert sections of a major mode within another major mode.\nThis is extremely convenient for instance to embed sections of code within human text, or even to have code executed within human text (e.g. R Markdown or its successor Quarto, Org Babel).\n\nFor example, here is the section of a markdown-mode buffer with snippets of julia-mode:\n\nJulia has \"assignment by operation\" operators:\n\n```{julia}\na = 2;\na += 7    # this is the same as a = a + 7\n```\n\nThere is a *left* division operator:\n\n```{julia}\n2\\8 == 8/2\n```\n\nIt can be rendered by Quarto into the following webpage:",
    "crumbs": [
      "Emacs",
      "<em><b>Getting started</b></em>",
      "Emacs modes"
    ]
  },
  {
    "objectID": "emacs/intro_remote.html",
    "href": "emacs/intro_remote.html",
    "title": "Editing remote files",
    "section": "",
    "text": "Once you have installed a bunch of packages and configured Emacs to your liking, you will find it annoying to work without all these extra niceties.\nIf you use the Alliance clusters, it would be a pain to try to replicate your Emacs packages and configs there: first, the Emacs versions may differ, then it is difficult to keep all customizations in sync on your machine and on your cluster account, finally, it is simply a loss of time because there is a much better way to go about it: TRAMP.",
    "crumbs": [
      "Emacs",
      "<em><b>Getting started</b></em>",
      "Editing remote files"
    ]
  },
  {
    "objectID": "emacs/intro_remote.html#what-is-tramp",
    "href": "emacs/intro_remote.html#what-is-tramp",
    "title": "Editing remote files",
    "section": "What is TRAMP?",
    "text": "What is TRAMP?\nTRAMP (Transparent Remote file Access, Multiple Protocol) allows to edit remote files over SSH or files belonging to different users from your local Emacs instance. This is very convenient because it allows you to keep your usual Emacs environment with your local settings and packages.\nFor exhaustive information, you can look at the TRAMP manual.",
    "crumbs": [
      "Emacs",
      "<em><b>Getting started</b></em>",
      "Editing remote files"
    ]
  },
  {
    "objectID": "emacs/intro_remote.html#how-to-use-tramp",
    "href": "emacs/intro_remote.html#how-to-use-tramp",
    "title": "Editing remote files",
    "section": "How to use TRAMP?",
    "text": "How to use TRAMP?\nTRAMP is very easy to use, all you have to do is use the following syntax for the file name:\n/method:user@hostname:/path/to/file\n\nExample:\n\nThe remote name of the .bashrc file in your home directory on a remote machine would look something like this:\n/ssh:user026@somename.ca:.bashrc\n\nTo use TRAMP to access files as root (if you have root access), use /sudo::/path/to/file.",
    "crumbs": [
      "Emacs",
      "<em><b>Getting started</b></em>",
      "Editing remote files"
    ]
  },
  {
    "objectID": "emacs/intro_remote.html#using-tramp-with-mfa",
    "href": "emacs/intro_remote.html#using-tramp-with-mfa",
    "title": "Editing remote files",
    "section": "Using TRAMP with MFA",
    "text": "Using TRAMP with MFA\nRecently, the Alliance clusters started using MFA. Consequently, in order to use TRAMP to access files on a cluster, you will first have to enter your Alliance password, then the method you used for MFA.\nThe prompt for the password is self-explanatory (“Password:”). That for the MFA method is not: Emacs will prompt you with: “Passcode:”, after which you should do one of the following:\n\nType 1 and you will be prompted to approve the request on the Duo Mobile App on your phone (this is a bit confusing at first because the prompt is not as clear as when you join through SSH. Here, you have to know that you need to type 1).\nEnter one of your backup codes.\nPress the button on your YubiKey if you have one.",
    "crumbs": [
      "Emacs",
      "<em><b>Getting started</b></em>",
      "Editing remote files"
    ]
  },
  {
    "objectID": "emacs/intro_remote.html#safely-storing-passwords",
    "href": "emacs/intro_remote.html#safely-storing-passwords",
    "title": "Editing remote files",
    "section": "Safely storing passwords",
    "text": "Safely storing passwords\nThe auth-source library (part of Emacs) enables Emacs to automatically access passwords saved in a ~/.authoinfo or ~/.authoinfo.gpg file. For safety reasons, you definitely should encrypt it with gpg and make it a ~/.authoinfo.gpg file.\nOnce your Alliance account password is safely stored in this file, you won’t have to type it to access your files remotely anymore.\nYou will however still have to use Duo App or YubiKey. But here too, you can make things easier for yourself if you create an SSH config file and add for the Alliance cluster host you use some configuration to make your SSH logins persist for some amount of time (e.g. 60 min):\nHost HOSTNAME\n    ControlPath ~/.ssh/cm-%r@%h:%p\n    ControlMaster auto\n    ControlPersist 60m",
    "crumbs": [
      "Emacs",
      "<em><b>Getting started</b></em>",
      "Editing remote files"
    ]
  },
  {
    "objectID": "emacs/intro_run.html",
    "href": "emacs/intro_run.html",
    "title": "Running Emacs",
    "section": "",
    "text": "There are several ways to run Emacs (as a GUI or in the terminal, with or without config file(s), as a server…). In this section, we will have a look at some very useful options.",
    "crumbs": [
      "Emacs",
      "<em><b>Getting started</b></em>",
      "Running Emacs"
    ]
  },
  {
    "objectID": "emacs/intro_run.html#list-of-options",
    "href": "emacs/intro_run.html#list-of-options",
    "title": "Running Emacs",
    "section": "List of options",
    "text": "List of options\nOn your computer, you can launch Emacs the way you launch any application (double-clicking on the icon, looking for it in your usual software launcher, etc.). On the cluster, you launch Emacs with the command emacs.\nLet’s try it:\nemacs\nTo close it, type C-x C-c. That brings you back to the terminal.\nEmacs, like most Linux commands, comes with a number of flags. To learn about them, you can open the manual page for Emacs:\nman emacs\n\nMan pages open in a pager (usually less).\nUseful keybindings when you are in the pager:\nSPACE      scroll one screen down\nb          backa one screen\nq          quit the pager\ng          go to the top of the document\n7g         go to line 7 from the top\nG          go to the bottom of the document\n/          search for a term\n           n will take you to the next result\n           N to the previous result",
    "crumbs": [
      "Emacs",
      "<em><b>Getting started</b></em>",
      "Running Emacs"
    ]
  },
  {
    "objectID": "emacs/intro_run.html#windowterminal",
    "href": "emacs/intro_run.html#windowterminal",
    "title": "Running Emacs",
    "section": "Window/terminal",
    "text": "Window/terminal\nWhen we launched Emacs earlier, it launched directly in the terminal. This is because we are accessing the remote machine without X11 forwarding, so we cannot use graphical applications there.\nIf we had enabled X11 forwarding (by having an X11 server running on our machine and using ssh -Y), or if we were launching Emacs from a terminal on our machine, the command emacs would have launched the GUI version of Emacs with clickable menu, etc. In those cases, if we still wanted to launch Emacs in the terminal, we would have to use the -nw flag (for no window).",
    "crumbs": [
      "Emacs",
      "<em><b>Getting started</b></em>",
      "Running Emacs"
    ]
  },
  {
    "objectID": "emacs/intro_run.html#initialization-file",
    "href": "emacs/intro_run.html#initialization-file",
    "title": "Running Emacs",
    "section": "Initialization file",
    "text": "Initialization file\nRight now, we do not have any initialization file (the user-written configuration file for Emacs). Later on however, we will create one and such file gets loaded automatically when Emacs starts. There are flags to prevent loading such file or to load a different configuration file.\nWe can also prevent the Emacs startup page with the --no-splash flag:\nemacs --no-splash",
    "crumbs": [
      "Emacs",
      "<em><b>Getting started</b></em>",
      "Running Emacs"
    ]
  },
  {
    "objectID": "emacs/intro_run.html#emacs-server",
    "href": "emacs/intro_run.html#emacs-server",
    "title": "Running Emacs",
    "section": "Emacs server",
    "text": "Emacs server\nYou might have noticed that it takes a little time for Emacs to launch. This wouldn’t happen on a local machine, particularly with the latest version of Emacs (Emacs has gotten much faster in recent years, but our cluster still has an older version of Emacs installed). Still, Emacs being so powerful, it can be a bit slow to launch once you customize it with countless packages.\nIt is annoying to have to wait each time you need to edit a file! But there is a nice solution: you can launch an Emacs server with emacs --daemon, then, each time you need to use Emacs, you connect to the running server with the emacsclient command. That way, you wait for Emacs to start only once (when you launch the server, but not afterwards.\nYou will get this warning when you launch the server, but you can ignore it:\n\nWarning: due to a long standing Gtk+ bug https://gitlab.gnome.org/GNOME/gtk/issues/221 Emacs might crash when run in daemon mode and the X11 connection is unexpectedly lost. Using an Emacs configured with –with-x-toolkit=lucid does not have this problem.\n\nIf the warning bothers you, you can redirect to /dev/null by running `emacs –daemon 2&gt; /dev/null’ instead.\nYou can also create an alias in your .bashrc file:\nalias es='emacs --daemon 2&gt; /dev/null'  # start Emacs server\nAnd you could even create an alias for emacsclient to make it easier to type since you will use it so often:\nalias ec='emacsclient'\nNow, to open a file called test.py you only have to run ec test.py.\nThe server keeps running even after you log out of your session. This can be convenient, but you probably shouldn’t let it run if you don’t plan on logging back in soon.\nTo kill the server, you can run the command kill-emacs from within Emacs (we will see how to run commands in a following section), or by running from the command line:\nemacsclient -e \"(kill-emacs)\"\nYou can create an alias for this too by adding something like this in your .bashrc:\nalias ek='emacsclient -e \"(kill-emacs)\"'  # kill Emacs server",
    "crumbs": [
      "Emacs",
      "<em><b>Getting started</b></em>",
      "Running Emacs"
    ]
  },
  {
    "objectID": "emacs/intro_ui.html",
    "href": "emacs/intro_ui.html",
    "title": "User interface",
    "section": "",
    "text": "To understand the documentation, it is important to learn a little bit of Emacs terminology. Here, we will see what are Emacs windows, buffers, and other parts of the user interface.",
    "crumbs": [
      "Emacs",
      "<em><b>Getting started</b></em>",
      "User interface"
    ]
  },
  {
    "objectID": "emacs/intro_ui.html#emacs-frames",
    "href": "emacs/intro_ui.html#emacs-frames",
    "title": "User interface",
    "section": "Emacs frames",
    "text": "Emacs frames\nWhen Emacs is run in a GUI fashion, what the OS usually calls a window is actually called in Emacs terminology a frame. It is possible to launch several Emacs frames. Right now, because we are running Emacs directly in the terminal, we can only have one Emacs frame.",
    "crumbs": [
      "Emacs",
      "<em><b>Getting started</b></em>",
      "User interface"
    ]
  },
  {
    "objectID": "emacs/intro_ui.html#emacs-windows",
    "href": "emacs/intro_ui.html#emacs-windows",
    "title": "User interface",
    "section": "Emacs windows",
    "text": "Emacs windows\nA frame can contain one or several windows.",
    "crumbs": [
      "Emacs",
      "<em><b>Getting started</b></em>",
      "User interface"
    ]
  },
  {
    "objectID": "emacs/intro_ui.html#buffers",
    "href": "emacs/intro_ui.html#buffers",
    "title": "User interface",
    "section": "Buffers",
    "text": "Buffers\nThe part of the window that contains the text to edit is called a buffer.\n\nBuffers can hold the content of a file, a running process (e.g. REPL, shell), an image, a pdf…",
    "crumbs": [
      "Emacs",
      "<em><b>Getting started</b></em>",
      "User interface"
    ]
  },
  {
    "objectID": "emacs/intro_ui.html#echo-area",
    "href": "emacs/intro_ui.html#echo-area",
    "title": "User interface",
    "section": "Echo area",
    "text": "Echo area\nAt the bottom of each window is an echo area. This is where Emacs prints outputs.",
    "crumbs": [
      "Emacs",
      "<em><b>Getting started</b></em>",
      "User interface"
    ]
  },
  {
    "objectID": "emacs/intro_ui.html#minibuffer",
    "href": "emacs/intro_ui.html#minibuffer",
    "title": "User interface",
    "section": "Minibuffer",
    "text": "Minibuffer\nThe minibuffer is a small buffer that appears in the echo area with a prompt and a cursor whenever Emacs expects some input from you.",
    "crumbs": [
      "Emacs",
      "<em><b>Getting started</b></em>",
      "User interface"
    ]
  },
  {
    "objectID": "emacs/intro_ui.html#mode-line",
    "href": "emacs/intro_ui.html#mode-line",
    "title": "User interface",
    "section": "Mode line",
    "text": "Mode line\nBetween the buffer and the echo area is the mode line, an area that gives information on running modes, file name and path, place of the cursor in the buffer, whether the document has been modified, etc.",
    "crumbs": [
      "Emacs",
      "<em><b>Getting started</b></em>",
      "User interface"
    ]
  },
  {
    "objectID": "emacs/intro_why.html",
    "href": "emacs/intro_why.html",
    "title": "Why Emacs?",
    "section": "",
    "text": "A good text editor is one of the most important tools a scientific programmer needs to choose and master.\nEmacs is a free, open source, and incredibly powerful text editor that can be turned into a complete IDE for writing and running code, and so much more.",
    "crumbs": [
      "Emacs",
      "<em><b>Getting started</b></em>",
      "Why Emacs?"
    ]
  },
  {
    "objectID": "emacs/intro_why.html#a-brief-history-of-emacs",
    "href": "emacs/intro_why.html#a-brief-history-of-emacs",
    "title": "Why Emacs?",
    "section": "A brief history of Emacs",
    "text": "A brief history of Emacs\nGNU Emacs is the most popular version of the Emacs text-editor family.\nThe first EMACS was written in 1976 by Davd A. Moon and Guy L. Steele Jr. at the MIT AI Lab. The development of GNU Emacs was started in 1984 by Richard Stallman—a free software movement activist and father of the copyleft concept. It is amongst the oldest free and open source software still under development. Despite its age, development actually remains very active with modern and powerful optimizations implemented constantly.\nGNU Emacs remains a symbol of the hacker culture of the 70s and 80s, particularly in its rivalry with vi as part of the editor war.\nIts editing functionalities are written in Emacs Lisp—a dialect of the Lisp programming language (the rest and the interpreter are written in C).\nThe official GNU Emacs website can be found here.",
    "crumbs": [
      "Emacs",
      "<em><b>Getting started</b></em>",
      "Why Emacs?"
    ]
  },
  {
    "objectID": "emacs/intro_why.html#why-use-emacs",
    "href": "emacs/intro_why.html#why-use-emacs",
    "title": "Why Emacs?",
    "section": "Why use Emacs?",
    "text": "Why use Emacs?\nTo brag. Obviously.\n\nBut there are other reasons:\n\nFree and open source\nEndlessly customizable\nAmazing diff\nEasy macros and automation\nText and file searching\nBookmarks\nGreat programming IDE\nLossless and endless undo/redo\nPowerful integration with Git\nEmail, Slack, Telegram, calendar, IRC…\nFile manager\nUnified set of shortcuts, search functionality, and environment for any tasks (easier than learning new IDEs and GUIs all the time!)\nTetris! and other games\nCalculator, calendar…\nX windows manager\nFun!\n…\n\nEmacs can do just about anything (except the laundry).\n\nNow … getting started can be daunting …\n\n… and it doesn’t necessarily get easier.\n\nBut it is worth it!",
    "crumbs": [
      "Emacs",
      "<em><b>Getting started</b></em>",
      "Why Emacs?"
    ]
  },
  {
    "objectID": "emacs/top_wb.html",
    "href": "emacs/top_wb.html",
    "title": "Emacs webinars",
    "section": "",
    "text": "Emacs as a programming IDE\n\n\n\n\nModern, faster & better Emacs\n\n\n\n\nUnderstanding Emacs modes\n\n\n\n\nUsing LLMs in Emacs\n\n\n\n\n\n\nFull Python IDE in Emacs",
    "crumbs": [
      "Emacs",
      "<b><em>Webinars</em></b>"
    ]
  },
  {
    "objectID": "emacs/wb_ide_slides.html#helm",
    "href": "emacs/wb_ide_slides.html#helm",
    "title": "Emacs as a programming IDE",
    "section": "Helm",
    "text": "Helm\nSearching in buffer\n\nNavigating open buffers and recent files\n\n\nNavigating file sections\n\n\nSelecting from kill ring\n\n\nMoving in mark ring\n\n\nLooking at active modes"
  },
  {
    "objectID": "emacs/wb_ide_slides.html#completion",
    "href": "emacs/wb_ide_slides.html#completion",
    "title": "Emacs as a programming IDE",
    "section": "Completion",
    "text": "Completion\ncompany-mode\n\nyasnippet\n\n\nDynamic abbrev expansion"
  },
  {
    "objectID": "emacs/wb_ide_slides.html#undoingredoing-with-undo-tree",
    "href": "emacs/wb_ide_slides.html#undoingredoing-with-undo-tree",
    "title": "Emacs as a programming IDE",
    "section": "Undoing/redoing with undo-tree",
    "text": "Undoing/redoing with undo-tree"
  },
  {
    "objectID": "emacs/wb_modes_slides.html#why-use-emacs",
    "href": "emacs/wb_modes_slides.html#why-use-emacs",
    "title": "Understanding Emacs modes",
    "section": "Why use Emacs?",
    "text": "Why use Emacs?\n\n\n\n To brag."
  },
  {
    "objectID": "emacs/wb_modes_slides.html#why-use-emacs-1",
    "href": "emacs/wb_modes_slides.html#why-use-emacs-1",
    "title": "Understanding Emacs modes",
    "section": "Why use Emacs?",
    "text": "Why use Emacs?\n\n\n To brag. Obviously."
  },
  {
    "objectID": "emacs/wb_modes_slides.html#why-use-emacs-2",
    "href": "emacs/wb_modes_slides.html#why-use-emacs-2",
    "title": "Understanding Emacs modes",
    "section": "Why use Emacs?",
    "text": "Why use Emacs?\n But there are other reasons:\n\nFree and open source\nEndlessly customizable\nAmazing diff\nMacros\nText and file searching\nGreat programming IDE\nLossless and endless undo/redo\nFun!\n…"
  },
  {
    "objectID": "emacs/wb_modes_slides.html#now-getting-started-can-be-daunting",
    "href": "emacs/wb_modes_slides.html#now-getting-started-can-be-daunting",
    "title": "Understanding Emacs modes",
    "section": "Now … getting started can be daunting",
    "text": "Now … getting started can be daunting"
  },
  {
    "objectID": "emacs/wb_modes_slides.html#and-it-doesnt-necessarily-get-easier",
    "href": "emacs/wb_modes_slides.html#and-it-doesnt-necessarily-get-easier",
    "title": "Understanding Emacs modes",
    "section": "… and it doesn’t necessarily get easier",
    "text": "… and it doesn’t necessarily get easier\n\n\n\n\n\n\n\nBut it’s all worth it!"
  },
  {
    "objectID": "emacs/wb_modes_slides.html#emacs-lisp",
    "href": "emacs/wb_modes_slides.html#emacs-lisp",
    "title": "Understanding Emacs modes",
    "section": "Emacs Lisp",
    "text": "Emacs Lisp\nEmacs Lisp is a dialect of the Lisp programming language developed especially to write the editing functionality of the Emacs text editor (the rest of Emacs and its interpreter are written in C)\nEmacs is endlessly customizable to anyone with a basic knowledge of Emacs Lisp. In particular, variables and functions setting the behaviour and appearance of the text editor can be created or modified\nThe language is well documented"
  },
  {
    "objectID": "emacs/wb_modes_slides.html#graphical-display",
    "href": "emacs/wb_modes_slides.html#graphical-display",
    "title": "Understanding Emacs modes",
    "section": "Graphical display",
    "text": "Graphical display"
  },
  {
    "objectID": "emacs/wb_modes_slides.html#graphical-display-1",
    "href": "emacs/wb_modes_slides.html#graphical-display-1",
    "title": "Understanding Emacs modes",
    "section": "Graphical display",
    "text": "Graphical display"
  },
  {
    "objectID": "emacs/wb_modes_slides.html#graphical-display-2",
    "href": "emacs/wb_modes_slides.html#graphical-display-2",
    "title": "Understanding Emacs modes",
    "section": "Graphical display",
    "text": "Graphical display"
  },
  {
    "objectID": "emacs/wb_modes_slides.html#graphical-display-3",
    "href": "emacs/wb_modes_slides.html#graphical-display-3",
    "title": "Understanding Emacs modes",
    "section": "Graphical display",
    "text": "Graphical display"
  },
  {
    "objectID": "emacs/wb_modes_slides.html#graphical-display-4",
    "href": "emacs/wb_modes_slides.html#graphical-display-4",
    "title": "Understanding Emacs modes",
    "section": "Graphical display",
    "text": "Graphical display"
  },
  {
    "objectID": "emacs/wb_modes_slides.html#graphical-display-5",
    "href": "emacs/wb_modes_slides.html#graphical-display-5",
    "title": "Understanding Emacs modes",
    "section": "Graphical display",
    "text": "Graphical display"
  },
  {
    "objectID": "emacs/wb_modes_slides.html#graphical-display-6",
    "href": "emacs/wb_modes_slides.html#graphical-display-6",
    "title": "Understanding Emacs modes",
    "section": "Graphical display",
    "text": "Graphical display"
  },
  {
    "objectID": "emacs/wb_modes_slides.html#graphical-display-7",
    "href": "emacs/wb_modes_slides.html#graphical-display-7",
    "title": "Understanding Emacs modes",
    "section": "Graphical display",
    "text": "Graphical display"
  },
  {
    "objectID": "emacs/wb_modes_slides.html#graphical-display-8",
    "href": "emacs/wb_modes_slides.html#graphical-display-8",
    "title": "Understanding Emacs modes",
    "section": "Graphical display",
    "text": "Graphical display"
  },
  {
    "objectID": "emacs/wb_modes_slides.html#graphical-display-9",
    "href": "emacs/wb_modes_slides.html#graphical-display-9",
    "title": "Understanding Emacs modes",
    "section": "Graphical display",
    "text": "Graphical display"
  },
  {
    "objectID": "emacs/wb_modes_slides.html#keybindings-kbd",
    "href": "emacs/wb_modes_slides.html#keybindings-kbd",
    "title": "Understanding Emacs modes",
    "section": "Keybindings (kbd)",
    "text": "Keybindings (kbd)\n\n\n\nFrom Ecol LG #134 by Javier Malonda"
  },
  {
    "objectID": "emacs/wb_modes_slides.html#kbd-notations",
    "href": "emacs/wb_modes_slides.html#kbd-notations",
    "title": "Understanding Emacs modes",
    "section": "Kbd notations",
    "text": "Kbd notations\nC-c means press the Control key and the C key together\nM-x means press the Alt (Windows) or Option (macOS) key and the X key together\nC-c m means press the Control key and the C key together, then press the M key\nC-c C-x m means press Ctl+C, then Ctl+X, then M\nC-x C-c M-w C-m M-v M-t M-u means that you probably should choose another kbd"
  },
  {
    "objectID": "emacs/wb_modes_slides.html#command-execution",
    "href": "emacs/wb_modes_slides.html#command-execution",
    "title": "Understanding Emacs modes",
    "section": "Command execution",
    "text": "Command execution\nA useful way to execute a command interactively, when it is not bound to a kbd, is to type M-x (this brings up the minibuffer, a place in which to type inputs) followed by the command name\n\nFor example, M-x count-words will output the number of lines, sentences, words, and characters of the current buffer in the echo area"
  },
  {
    "objectID": "emacs/wb_modes_slides.html#major-modes",
    "href": "emacs/wb_modes_slides.html#major-modes",
    "title": "Understanding Emacs modes",
    "section": "Major modes",
    "text": "Major modes\nDifferent types of text require different behaviours, syntax highlighting, formatting, functions, variables, etc.\nEach type of buffer (e.g. Python script, Markdown document, Julia REPL, Bash shell, directory editor, pdf) is associated with a different major mode\nFile extensions, particular markers in the file, or other elements tell Emacs to automatically switch to the appropriate major mode\nOnly one major mode is active at a time\nSwitching to a different major mode is possible by running the corresponding major mode command (e.g. M-x python-mode will switch to Python mode)"
  },
  {
    "objectID": "emacs/wb_modes_slides.html#fundamental-mode",
    "href": "emacs/wb_modes_slides.html#fundamental-mode",
    "title": "Understanding Emacs modes",
    "section": "Fundamental mode",
    "text": "Fundamental mode\nfundamental-mode is the most basic major mode, with no particular feature\nThis is the mode enabled by default if Emacs cannot detect what specific major mode to enable"
  },
  {
    "objectID": "emacs/wb_modes_slides.html#minor-modes",
    "href": "emacs/wb_modes_slides.html#minor-modes",
    "title": "Understanding Emacs modes",
    "section": "Minor modes",
    "text": "Minor modes\nMinor modes provide additional and optional features that can be turned on or off (e.g. spell checking, auto-completion, auto-indentation, fancy undo behaviour, fancy parenthesis matching highlighting)\nMinor modes can be turned on/off by running the corresponding minor mode commands (e.g. M-x flyspell-mode will turn spell checking on/off). The command consult-minor-mode-menu from the package consult makes this particularly easy\nEach mode comes with a set of commands. consult’s command consult-mode-command makes it easy to search for commands within each mode\nAny number of minor modes can be active at the same time"
  },
  {
    "objectID": "emacs/wb_modes_slides.html#list-of-enabled-modes",
    "href": "emacs/wb_modes_slides.html#list-of-enabled-modes",
    "title": "Understanding Emacs modes",
    "section": "List of enabled modes",
    "text": "List of enabled modes\nBy default, &lt;f1&gt; m or M-x describe-mode will open a list and description of the active modes\nThe major mode can also be determined with &lt;f1&gt; v major-mode (&lt;f1&gt; v runs the command describe-variable)\nA list of minor modes can also be viewed with &lt;f1&gt; v minor-mode-list\nAgain, consult’s consult-minor-mode-menu makes all this much nicer"
  },
  {
    "objectID": "emacs/wb_modes_slides.html#the-mode-line",
    "href": "emacs/wb_modes_slides.html#the-mode-line",
    "title": "Understanding Emacs modes",
    "section": "The mode line",
    "text": "The mode line\nAnother way to get information about enabled modes is the mode line"
  },
  {
    "objectID": "emacs/wb_modes_slides.html#hooks",
    "href": "emacs/wb_modes_slides.html#hooks",
    "title": "Understanding Emacs modes",
    "section": "Hooks",
    "text": "Hooks\nMinor modes can be automatically enabled when other modes (major or minor) are enabled thanks to hooks\n\nFor example, to enable the aggressive indent minor mode whenever the ESS R major mode is enabled, you can add to your init file:\n\n(add-hook 'ess-r-mode-hook 'aggressive-indent-mode)\n\nOr, using use-package, now part of base Emacs:\n\n(use-package aggressive-indent\n:hook (ess-r-mode . aggressive-indent-mode))"
  },
  {
    "objectID": "emacs/wb_modes_slides.html#modes-source-code",
    "href": "emacs/wb_modes_slides.html#modes-source-code",
    "title": "Understanding Emacs modes",
    "section": "Modes source code",
    "text": "Modes source code\nTo see the source code of a mode, run &lt;f1&gt; v (or M-x describe-variable) followed by the name of the mode map\nThis will open a help buffer with a link to the source code file\n\nFor example &lt;f1&gt; v text-mode-map will open a help buffer with a link to text-mode.el\n\n\nThe help buffer opened by &lt;f1&gt; m or M-x describe-mode also gives a link to the source code of the major mode\n\nLooking at the source code of a mode is very useful to customize it"
  },
  {
    "objectID": "emacs/wb_modes_slides.html#customizing-modes",
    "href": "emacs/wb_modes_slides.html#customizing-modes",
    "title": "Understanding Emacs modes",
    "section": "Customizing modes",
    "text": "Customizing modes\nIn Emacs, everything is customizable\nTo customize modes, you can write Emacs Lisp code in your init file (the configuration file that gets loaded when Emacs launches) or you can use the easy customization interface\n\nFor example, to customize the Markdown major mode, you would run M-x customize-group markdown"
  },
  {
    "objectID": "emacs/wb_modes_slides.html#evaluation-order",
    "href": "emacs/wb_modes_slides.html#evaluation-order",
    "title": "Understanding Emacs modes",
    "section": "Evaluation order",
    "text": "Evaluation order\nIf you write your own Emacs code, be careful that functions and variables take the value of their last loaded version. The order in which Emacs code is evaluated thus matters\nYou want to evaluate as little as possible when you launch Emacs to speed up start-up time (lazy evaluation): you don’t want to load every single package that you have installed\nThis means that if you overwrite a function or variable of a mode in your init file, the init file is read at start-up, but when that mode is launched, the default function/variable will overwrite the custom one you wrote in your init file"
  },
  {
    "objectID": "emacs/wb_modes_slides.html#evaluation-order-1",
    "href": "emacs/wb_modes_slides.html#evaluation-order-1",
    "title": "Understanding Emacs modes",
    "section": "Evaluation order",
    "text": "Evaluation order\nTo by-pass this problem, you can use eval-after-load\n\nExample:\n(eval-after-load\n \"markdown\"\n '(defun markdown-demote ()\n    ...))\n\n\nuse-package has the :init and :config keyword symbols that ensure that the following expressions are evaluated respectively before or after the loading of a package"
  },
  {
    "objectID": "emacs/wb_modes_slides.html#customizing-kbd",
    "href": "emacs/wb_modes_slides.html#customizing-kbd",
    "title": "Understanding Emacs modes",
    "section": "Customizing kbd",
    "text": "Customizing kbd\nMost modes come with specific keymaps: sets of kbd only active when the mode is enabled. These kbd of course can be customized\n\nFor example, to modify the kbd for the function markdown-outline-previous in the markdown-mode-map:\n\n(define-key markdown-mode-map (kbd \"M-p\") 'markdown-outline-previous)\n\nOr, using use-package:\n\n(use-package markdown-mode\n    :bind (:map markdown-mode-map\n                (\"M-p\" . markdown-outline-previous)))"
  },
  {
    "objectID": "emacs/wb_modes_slides.html#polymode",
    "href": "emacs/wb_modes_slides.html#polymode",
    "title": "Understanding Emacs modes",
    "section": "Polymode",
    "text": "Polymode\nWhile it is normally impossible to associate multiple major modes with a single buffer, Polymode allows to insert sections of a major mode within another major mode\nThis is extremely convenient for instance to embed sections of code within human text, or even to have code executed within human text (e.g. R Markdown or its successor Quarto, Org Babel)"
  },
  {
    "objectID": "emacs/wb_modes_slides.html#polymode-1",
    "href": "emacs/wb_modes_slides.html#polymode-1",
    "title": "Understanding Emacs modes",
    "section": "Polymode",
    "text": "Polymode\nmarkdown-mode with snippets of julia-mode:\nJulia has \"assignment by operation\" operators:\n\n```{julia}\na = 2;\na += 7    # this is the same as a = a + 7\n```\n\nThere is a *left* division operator:\n\n```{julia}\n2\\8 == 8/2\n```"
  },
  {
    "objectID": "emacs/wb_modes_slides.html#polymode-2",
    "href": "emacs/wb_modes_slides.html#polymode-2",
    "title": "Understanding Emacs modes",
    "section": "Polymode",
    "text": "Polymode\nRendered by Quarto into:"
  },
  {
    "objectID": "emacs/wb_new_tools_slides.html#lexical-binding",
    "href": "emacs/wb_new_tools_slides.html#lexical-binding",
    "title": "Modern Emacs",
    "section": "Lexical binding",
    "text": "Lexical binding\nIntroduced in version 24\nLexical binding can be used instead of dynamic binding for Emacs Lisp code\nSet as a file local variable\n\nDynamic binding\nName resolution depends on program state (runtime context), determined at run time\nGlobal environment for all variables\nMakes modifying behaviour easy\n\n\nLexical binding\nName resolution depends on lexical context (static context), determined at compile time\nLocal environments of functions and let, defconst, defvar, etc. expressions\nMakes compiler optimization much easier → faster Elisp code = faster Emacs"
  },
  {
    "objectID": "emacs/wb_new_tools_slides.html#jit-native-compilation",
    "href": "emacs/wb_new_tools_slides.html#jit-native-compilation",
    "title": "Modern Emacs",
    "section": "JIT native compilation",
    "text": "JIT native compilation\nIntroduced in version 28\nRequires libgccjit\nBuild Emacs --with-native-compilation\nPackages can also be compiled natively (automatic with straight)\n\nFaster startup\nSpeedup of 2.5 to 5 compared to corresponding byte-compiled code"
  },
  {
    "objectID": "emacs/wb_new_tools_slides.html#lazy-loading",
    "href": "emacs/wb_new_tools_slides.html#lazy-loading",
    "title": "Modern Emacs",
    "section": "Lazy loading",
    "text": "Lazy loading\nBuilt-in since version 29\nFine-tuned loading of packages with use-package\nIntegrates nicely with straight\n\nFaster startup time\nMore organized init file\nEasier to reload configurations for single package"
  },
  {
    "objectID": "emacs/wb_new_tools_slides.html#accurate-syntax-tree",
    "href": "emacs/wb_new_tools_slides.html#accurate-syntax-tree",
    "title": "Modern Emacs",
    "section": "Accurate syntax tree",
    "text": "Accurate syntax tree\nBuilt-in since version 29\nTree-sitter for Emacs\nCode is parsed accurately instead of using regexp\n\nPerfect syntax highlighting, indentation, and navigation\nFaster\n\nSimplest setup with treesit-auto:\n(use-package treesit-auto\n  :config\n  (treesit-auto-add-to-auto-mode-alist 'all))"
  },
  {
    "objectID": "emacs/wb_new_tools_slides.html#history-of-code-completion-in-emacs-ido",
    "href": "emacs/wb_new_tools_slides.html#history-of-code-completion-in-emacs-ido",
    "title": "Modern Emacs",
    "section": "History of code completion in Emacs: IDO",
    "text": "History of code completion in Emacs: IDO\n\n\n\nFrom Xah Emacs Blog"
  },
  {
    "objectID": "emacs/wb_new_tools_slides.html#history-of-code-completion-in-emacs-ido-vertical",
    "href": "emacs/wb_new_tools_slides.html#history-of-code-completion-in-emacs-ido-vertical",
    "title": "Modern Emacs",
    "section": "History of code completion in Emacs: IDO vertical",
    "text": "History of code completion in Emacs: IDO vertical\n\n\n\nFrom oremacs"
  },
  {
    "objectID": "emacs/wb_new_tools_slides.html#history-of-code-completion-in-emacs-helm",
    "href": "emacs/wb_new_tools_slides.html#history-of-code-completion-in-emacs-helm",
    "title": "Modern Emacs",
    "section": "History of code completion in Emacs: HELM",
    "text": "History of code completion in Emacs: HELM\n\n\n\nFrom oracleyue"
  },
  {
    "objectID": "emacs/wb_new_tools_slides.html#history-of-code-completion-in-emacs-ivy",
    "href": "emacs/wb_new_tools_slides.html#history-of-code-completion-in-emacs-ivy",
    "title": "Modern Emacs",
    "section": "History of code completion in Emacs: Ivy",
    "text": "History of code completion in Emacs: Ivy\nWith optional Counsel & Swiper\n\n\n\nFrom abo-abo/swiper"
  },
  {
    "objectID": "emacs/wb_new_tools_slides.html#new-framework",
    "href": "emacs/wb_new_tools_slides.html#new-framework",
    "title": "Modern Emacs",
    "section": "New framework",
    "text": "New framework\nExternal packages\nUse default Emacs functions (less code)\nFaster, flexible, customizable with discrete units"
  },
  {
    "objectID": "emacs/wb_new_tools_slides.html#packages",
    "href": "emacs/wb_new_tools_slides.html#packages",
    "title": "Modern Emacs",
    "section": "Packages",
    "text": "Packages\n\n\nMinibuffer\n\nvertico    frontend completion UI\norderless  backend completion style\nconsult   backend completion functions\nmarginalia  annotations\nembark     actions on completion buffer\n\n\nIn buffer\n\ncorfu    frontend completion UI\norderless  backend completion style\ncape    backend completion functions\neglot     backend LSP client"
  },
  {
    "objectID": "emacs/wb_new_tools_slides.html#compared-to-previous-frameworks",
    "href": "emacs/wb_new_tools_slides.html#compared-to-previous-frameworks",
    "title": "Modern Emacs",
    "section": "Compared to previous frameworks",
    "text": "Compared to previous frameworks\n\nIntegrates beautifully with internal Emacs functions\nEasy jump back & forth between buffer and completion buffer\nMuch faster than HELM\nLightning fast previews with auto-closing buffers\nEasy customization"
  },
  {
    "objectID": "emacs/wb_new_tools_slides.html#example-configuration",
    "href": "emacs/wb_new_tools_slides.html#example-configuration",
    "title": "Modern Emacs",
    "section": "Example configuration",
    "text": "Example configuration\nVertico (frontend for completion in minibuffer)\n(use-package vertico\n  :init\n  (vertico-mode 1)\n  (vertico-multiform-mode 1)\n  :config\n  (setq vertico-multiform-commands\n    '((consult-line buffer)\n      (consult-line-thing-at-point buffer)\n      (consult-recent-file buffer)\n      (consult-mode-command buffer)\n      (consult-complex-command buffer)\n      (embark-bindings buffer)\n      (consult-locate buffer)\n      (consult-project-buffer buffer)\n      (consult-ripgrep buffer)\n      (consult-fd buffer)))\n  :bind (:map vertico-map\n          (\"C-k\" . kill-whole-line)\n          (\"C-u\" . kill-whole-line)\n          (\"C-o\" . vertico-next-group)\n          (\"&lt;tab&gt;\" . minibuffer-complete)\n          (\"M-&lt;return&gt;\" . minibuffer-force-complete-and-exit)))\n\n;; save search history\n(use-package savehist\n  :init\n  (savehist-mode 1))"
  },
  {
    "objectID": "emacs/wb_new_tools_slides.html#language-server-protocol-client",
    "href": "emacs/wb_new_tools_slides.html#language-server-protocol-client",
    "title": "Modern Emacs",
    "section": "Language Server Protocol client",
    "text": "Language Server Protocol client\nBuilt-in since version 29\nEglot (Emacs Polyglot) allows to connect to a programming language server\nExample: Julia\nNeed to install an LSP for Julia:\n(straight-use-package 'eglot-jl)\nThen run eglot-jl-init\nNow eglot in a Julia buffer connects to the server\n\nSimilarly, you can install an LSP for R or Python or any language and use Eglot with R, Python, or whatever language"
  },
  {
    "objectID": "emacs/wb_new_tools_slides.html#to-all-emacs-developers-and-maintainers",
    "href": "emacs/wb_new_tools_slides.html#to-all-emacs-developers-and-maintainers",
    "title": "Modern Emacs",
    "section": "❤ to all Emacs developers and maintainers",
    "text": "❤ to all Emacs developers and maintainers\nIn particular,\ndevelopers, maintainers, and contributors to Emacs core,\ndevelopers and maintainers to some of the mentioned packages:\nDaniel Mendler\nOmar Antolín Camarena\nJoão Távora\nRobb Enzmann\nJohn Wiegley\nAdam B\nand all their contributors"
  },
  {
    "objectID": "git/intro_aliases.html",
    "href": "git/intro_aliases.html",
    "title": "Aliases",
    "section": "",
    "text": "You might find it convenient to replace some long Git commands with shorter versions.\nFor example, wouldn’t it be nice if instead of typing git status you could type git st? It turns out this is easy to do via Git aliases.\n\nHere is the syntax to create a Git alias for all your Git repos:\ngit config --global alias.st 'status'\nThis creates a new Git command git st that expands to git status.\nOf course, this is particularly useful with longer commands.\n\nHere is an example to list all files in a repository:\n\ngit config --global alias.list 'ls-tree --full-tree -r HEAD'\nTo get the list of files, you can now simply run:\ngit list\nGlobal aliases are stored in your ~/.gitconfig file. You can print them with:\ngit config --list\n\nYou can set aliases for specific repos without the --global flag (although, it is unclear to me why you would ever want to do this).\n\n\nHere is another example:\n\ngit config --global alias.graph \\\n \"log --graph \\\n      --date-order \\\n      --date=short \\\n      --pretty=format:'%C(cyan)%h %C(yellow)%ar %C(auto)%s%+b %C(green)%ae'\"\nNow, git graph will give you the log in the form of a compact graph with nice colour-coding.\n\nHere is a more complex example involving an argument and a Unix command:\n\nImagine that you want to create an alias that allows you to easily run commands such as: git grep \"test\" $(git rev-list --all).\nThis searches for the string “test” in all previous commits. This command takes an argument (the string “test”) and it uses the output from another Unix command (git rev-list --all) as its input.\nTo turn this into an alias, you need to create an executable bash script and place it in your $PATH environment variable:\n# Assuming that `$HOME/bin` is listed in your `$PATH`\n\n# Create a Bash script in `$HOME/bin`\ncat &lt;&lt; EOF &gt; $HOME/bin/git-search\n#!/bin/bash\ngit grep \\${1} \\$(git rev-list --all)\nEOF\n\n# Make it executable\nchmod u+x $HOME/bin/git-search\nYou can now run git search test to search the entire Git project history for the string “test”.\nOf course, it works with any other string: git search Methods will search for the string “Methods”.",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Aliases"
    ]
  },
  {
    "objectID": "git/intro_changes.html",
    "href": "git/intro_changes.html",
    "title": "Inspecting changes",
    "section": "",
    "text": "While git status gives you information on the files that were changed since the last commit, it doesn’t provide any information on what those changes are.\nIn this section, we will see how we can get information on the changes to the files contents.",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Inspecting changes"
    ]
  },
  {
    "objectID": "git/intro_changes.html#the-three-trees-of-git",
    "href": "git/intro_changes.html#the-three-trees-of-git",
    "title": "Inspecting changes",
    "section": "The three trees of Git",
    "text": "The three trees of Git\nBefore we can jump into this section, we need to understand a bit more how Git works.\nOne useful mental representation is to imagine three file trees:\n\n\nThe working tree\nLet’s imagine that you are starting to work on a project.\nFirst, you create a directory.\nIn it, you create several sub-directories.\nIn those, you create a number of files.\nYou can open these files, read them, edit them, etc. This is something you are very familiar with.\nIn the Git world, this is the working directory or working tree of the project.\nThat is: an uncompressed version of your files that you can access and edit.\nYou can think of it as a sandbox because this is where you can experiment with the project. This is where the project gets developed.\nNow, Git has two other important pieces in its architecture.\n\n\nThe index\nIf you want the project history to be useful to future you, it has to be nice and tidy. You don’t want to record snapshots haphazardly or you will never be able to find anything back.\nBefore you record a snapshot, you carefully select the elements of the project as it is now that would be useful to write to the project history together. The index or staging area is what allows to do that: it contains the suggested future commit.\n\n\nHEAD\nFinally, the last tree in Git architecture is one snapshot in the project history that serves as a reference version of the project: if you want to see what you have been experimenting on in your “sandbox”, you need to compare the state of the working directory with some snapshot.\nRemember that HEAD is a pointer pointing at a branch, that a branch is itself a pointer pointing at a commit, and finally that a commit is a Git object pointing at compressed blobs containing data about your project at a certain commit. When the HEAD pointer moves around, whatever commit it points to populates the HEAD tree with the corresponding data.\nAs we saw earlier, when you create a commit, HEAD automatically points to the new commit. So the HEAD tree is often filled with the last snapshot you created. But—as we will see later—we can move the HEAD pointer around through other ways. So the HEAD tree can be populated by any snapshot in your project history.",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Inspecting changes"
    ]
  },
  {
    "objectID": "git/intro_changes.html#git-diff",
    "href": "git/intro_changes.html#git-diff",
    "title": "Inspecting changes",
    "section": "git diff",
    "text": "git diff\nThe command git diff prints the differences between any two Git objects. In particular, it allows to compare any two of the three trees with each other.\n\nDiff between working directory & index\n\ngit diff displays the differences between the working directory (the uncompressed files) and the index (the staging area):\ngit diff\nRight now, git diff does not return anything because our working tree and staging area are at the same point.\nLet’s make some changes in our proposal:\nnano ms/proposal.md\nNow, if we run git diff again, we get:\ndiff --git a/ms/proposal.md b/ms/proposal.md\nindex 0fb5cc8..4fdc1b0 100644\n--- a/ms/proposal.md\n+++ b/ms/proposal.md\n@@ -16,4 +16,4 @@ We hope to achieve a lot.\n\n # Conclusion\n\n-This is truly a great proposal.\n+This is truly a great proposal, but it needs a little more work.\n\n\nDiff between index & last commit\n\nTo see what would be committed if you ran git commit (that is the differences between the index and the last commit), you need to run instead:\ngit diff --cached\nWe aren’t getting any output because we haven’t staged anything that isn’t in our last commit. Let’s stage our changes and try again:\ngit add .\ngit diff --cached\ndiff --git a/ms/proposal.md b/ms/proposal.md\nindex 0fb5cc8..4fdc1b0 100644\n--- a/ms/proposal.md\n+++ b/ms/proposal.md\n@@ -16,4 +16,4 @@ We hope to achieve a lot.\n\n # Conclusion\n\n-This is truly a great proposal.\n+This is truly a great proposal, but it needs a little more work.\nThe changes are now between the staging area and HEAD.\n\nIf we run git diff now, we aren’t getting any output because the staging area has caught up with the working tree: they are now the same, so no differences.\n\ngit diff --cached is very convenient to check what will enter in the next commit.\n\n\nDiff between working directory & last commit\n\nFinally, to see the differences between your working directory and HEAD, you run:\ngit diff HEAD\nThis will be the sum of the previous two.\nRight now, git diff --cached and git diff HEAD print the same result, but if we make new changes, they will become different since git diff HEAD will reflect all the changes between the working tree and the last commit, while git diff --cached will only contain the differences between the staging area and the last commit.\n\n\nYour turn:\n\nModify one of the tracked files to visualize this.",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Inspecting changes"
    ]
  },
  {
    "objectID": "git/intro_first_steps.html",
    "href": "git/intro_first_steps.html",
    "title": "First steps",
    "section": "",
    "text": "In this section, we will initialize our first Git repository, learn to explore it, and create a few commits.",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "First steps"
    ]
  },
  {
    "objectID": "git/intro_first_steps.html#get-a-mock-project",
    "href": "git/intro_first_steps.html#get-a-mock-project",
    "title": "First steps",
    "section": "Get a mock project",
    "text": "Get a mock project\nLet’s download a mock project with a couple of files to practice with.\n\n1. Download the zip file\nNavigate to a suitable location (e.g. cd ~), then download the file with:\nwget --no-check-certificate \\\n     'https://docs.google.com/uc?export=download&id=1SJV5mRGexf91lNyFwdS_JmuAXX0xS4pE' \\\n     -O project.zip\n\nIf you are working on your own machine (and not on our training cluster), you can alternatively download the file with this button:  Download the data \nNote that if you do this, you will have to know how to navigate to your Downloads directory from the command line.\n\n\n\n2. Unzip the file\nUnzip the file with:\nunzip project.zip\nYou should now have a project directory with a number of subdirectories and files. This is the project we will use today.\n\n\n3. Enter in the project root\ncd in the root of the project:\ncd project",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "First steps"
    ]
  },
  {
    "objectID": "git/intro_first_steps.html#inspect-the-project",
    "href": "git/intro_first_steps.html#inspect-the-project",
    "title": "First steps",
    "section": "Inspect the project",
    "text": "Inspect the project\nFirst, let’s have a look at our (very small) mock project.\nLet’s list the content of project:\nls -F\ndata/\nms/\nresults/\nsrc/\nThere are 4 subdirectories.\nls -a     # Show hidden files\n.\n..\ndata\nms\nresults\nsrc\nAs you can see, there are no hidden files.\nls -R\n.:\ndata\nms\nresults\nsrc\n\n./data:\ndataset.csv\n\n./ms:\nproposal.md\n\n./results:\n\n./src:\nscript.py\nNow we can see the content of each subdirectory.\nYou probably don’t have the tree command on your machine, so this won’t work for you, but don’t worry about it: this is only to show you the same result in a more readable format:\ntree\n.\n├── data\n│   └── dataset.csv\n├── ms\n│   └── proposal.md\n├── results\n└── src\n    └── script.py\n\n5 directories, 3 files\nLet’s look at the content of the files.\nThis is our very exciting data set:\ncat data/dataset.csv\nvar1,var2,var3,\n1,2,1,\n0,1,0,\n3,3,3\nAnd a no less exciting manuscript (the proposal for our project):\ncat ms/proposal.md\n# Summary\n\nThis is the summary for our proposal.\n\n# Funding\n\nInformation on funding for the project.\n\n# Methods\n\nHere we have some methods using our Python scripts.\n\n# Expected results\n\nWe hope to achieve a lot.\n\n# Conclusion\n\nThis is truly a great proposal.\nAnd finally, a Python script:\ncat src/script.py\nimport pandas as pd\nfrom matplotlib import pyplot as plt\n\ndf = pd.read_csv('../data/dataset.csv')\n\ndf.plot()\nplt.savefig('../results/plot.png', dpi=300)",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "First steps"
    ]
  },
  {
    "objectID": "git/intro_first_steps.html#initializing-a-git-repository",
    "href": "git/intro_first_steps.html#initializing-a-git-repository",
    "title": "First steps",
    "section": "Initializing a Git repository",
    "text": "Initializing a Git repository\nOur project is just a bunch of files and subdirectories. To turn it into a Git repository, we run:\ngit init\nInitialized empty Git repository in project/.git/\n\nMake sure to be at the root of the project (here, inside the project directory) before initializing the repository.\n\n\nGit is verbose: you will often get useful feed-back after running commands. Read them!\n\nWhen you run this command, Git creates a .git repository. This is where it will store all its files (all those blob objects, pointers, and other files).\nYou can see that this repository was created by running:\nls -a\n.\n..\n.git\ndata\nms\nresults\nsrc\n\nIf you run git init in the wrong location, you can easily fix this by deleting the .git directory that you created (e.g. rm -r .git).\n\n\nGit commands\nAs you might have already noticed, Git commands start with git.\nA typical command is of the form:\ngit &lt;command&gt; [flags] [arguments]\n\nExample of a command we used to configure Git:\n\ngit config --global \"Your Name\"\n\nExample of a much simpler command with no flag nor argument:\n\ngit init",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "First steps"
    ]
  },
  {
    "objectID": "git/intro_first_steps.html#status-of-the-repository",
    "href": "git/intro_first_steps.html#status-of-the-repository",
    "title": "First steps",
    "section": "Status of the repository",
    "text": "Status of the repository\nOne command you will run often when working with Git is git status. It gives you information on new changes to your project:\ngit status\nOn branch main\n\nNo commits yet\n\nUntracked files:\n  (use \"git add &lt;file&gt;...\" to include in what will be committed)\n        data/\n        ms/\n        src/\n\nnothing added to commit but untracked files present (use \"git add\" to track)\nThere is a lot of information here:\n\nWe are on branch main. That is the default name for the branch that gets created automatically as soon as we initialize a Git repository.\nThere are no commits yet (normal: we just initialized a new repository).\nThere are untracked files in our repo.\n\nIt is time to create a first commit…",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "First steps"
    ]
  },
  {
    "objectID": "git/intro_first_steps.html#creating-commits",
    "href": "git/intro_first_steps.html#creating-commits",
    "title": "First steps",
    "section": "Creating commits",
    "text": "Creating commits\nRemember that commits are those “snapshots” of your project at certain moments in time. You should create a new commit whenever you think that your project is at a point to which you might want to go back to later. There is no rule about when or how often to create commits: it is really up to you.\nBefore we create our first commit, we need to decide what file(s) we want to add to this commit.\nWe could add everything.\nWe can also be more selective and add files one by one.\nWe can even add only sections of files.\nHow do we tell Git what to add to the next commit?\n\nThe staging area\nGit has a staging area: a way to select what to add to the next commit. Files (or sections of files) get added to the staging area with git add.\nTo add all the files at once, we run, from the root of the project:\ngit add .\nThe . represents the current directory. Because Git adds files recursively, this will add all new files.\nTo add a particular file, we add its path as an argument to git add.\n\nExample:\n\ngit add ms/proposal.md\nTo add all the files in a directory, we add the path of that directory as an argument to git add.\n\nExample:\n\ngit add ms\n\nSince there is a single file in the ms directory, both commands will in our case lead to the same result.\n\nLet’s run that last command:\ngit add ms\nIt looks like nothing happened. Did it work? How can I know?\nAnswer: by running git status again. That’s the command to go to whenever you need to get some update on the status of the repo:\ngit status\nOn branch main\n\nNo commits yet\n\nChanges to be committed:\n  (use \"git rm --cached &lt;file&gt;...\" to unstage)\n        new file:   ms/proposal.md\n\nUntracked files:\n  (use \"git add &lt;file&gt;...\" to include in what will be committed)\n        data/\n        src/\nIt worked! We can see that the content of the ms subdirectory is ready to be committed. It is in the staging area.\n\n\nYour turn:\n\nWhat if we change our mind as to the composition of our first commit and we want to create our first commit with the Python script instead? To do that, we need to unstage proposal.md first.\nHow can we do that? Give it a try.\nThen, how can we stage the Python script?\n\n\n\nCommitting\nOnce we are happy with the content of the future commit, it is time to create it. The command for this is git commit.\nRemember that each commit contains the following metadata:\n\nauthor,\ndate and time,\nthe hash of parent commit(s),\na message.\n\nThe first three can be set by Git automatically (you have configured Git so it knows who the author is).\nGit has no way to come up with the forth one however. You need to create it yourself.\nIf you run git commit, Git will open your text editor so that you can type the message. If you want to enter the message directly from the command line, you use the -m flag followed by the quoted message:\ngit commit -m \"Initial commit\"\n[main (root-commit) 61abf96] Initial commit\n 1 file changed, 19 insertions(+)\n create mode 100644 ms/proposal.md\nWe now have a first commit. Its hash starts (in my case—yours will be different of course) with 61abf96.\n\nGood commit messages\n\n\nFrom xkcd.com\n\n\nUse the present tense.\nThe first line is a summary of the commit and is less than 50 characters long.\nLeave a blank line below.\nThen add the body of your commit message with more details.\n\n\nExample of a good commit message:\n\ngit commit -m \"Reduce boundary conditions by a factor of 0.3\n\nUpdate boundaries\nRerun model and update table\nRephrase method section in ms\"\nFuture you will thank you! (And so will your collaborators).\n\nIf we run git status once more, we now get:\nOn branch main\nUntracked files:\n  (use \"git add &lt;file&gt;...\" to include in what will be committed)\n        data/\n        src/\n\nnothing added to commit but untracked files present (use \"git add\" to track)\nThe directory ms has disappeared from the list of untracked files: its content has now been committed to history.\n\n\nUnderstanding the staging area\nNew Git users are often confused about the two-step commit process (first, you stage with git add, then you commit with git commit). This intermediate step seems, at first, totally unnecessary. In fact, it is very useful: without it, commits would always include all new changes made to a project and they would thus be very messy. The staging area allows to prepare (“stage”) the next commit. This way, you only commit what you want when you want.\n\nLet’s go over a simple example:\n\nWe don’t always work linearly. Maybe you are working on a section of your manuscript when you realize by chance that there is a mistake in your script. You fix that mistake. On your next commit, it might make little sense to commit together that fix and your manuscript changes since they are not related. If your commits are random bag of changes, it will be very hard for future you to navigate your project history.\nIt is a lot better to only stage your script fix, commit it, then only stage your manuscript update, and commit this in a different commit.\nThe staging area allows you to pick and chose the changes from one or various files that constitute some coherent change to the project and that make sense to commit together.",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "First steps"
    ]
  },
  {
    "objectID": "git/intro_install.html",
    "href": "git/intro_install.html",
    "title": "Installation and setup",
    "section": "",
    "text": "In this section, we will learn how to install and configure Git.",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Installation and setup"
    ]
  },
  {
    "objectID": "git/intro_install.html#installing-git-on-your-machine",
    "href": "git/intro_install.html#installing-git-on-your-machine",
    "title": "Installation and setup",
    "section": "Installing Git on your machine",
    "text": "Installing Git on your machine\n\nYou don’t have to install Git locally for this course if you plan on using our training cluster.\n\n\nmacOS/Linux users\nInstall Git from the official website.\n\n\nWindows users\nInstall Git for Windows. This will also install Git Bash, a Bash emulator.\n\nGit is built for Unix-like systems (Linux and macOS). In order to use Git from the command line on Windows, you need a Unix shell such as Bash. To make this very easy, Git for Windows comes with its Bash emulator.",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Installation and setup"
    ]
  },
  {
    "objectID": "git/intro_install.html#git-in-the-alliance-clusters",
    "href": "git/intro_install.html#git-in-the-alliance-clusters",
    "title": "Installation and setup",
    "section": "Git in the Alliance clusters",
    "text": "Git in the Alliance clusters\nOn the Alliance clusters, Git is not only already installed, but it gets automatically loaded at each session, so you don’t have to load a module for it (as you do for most software). This is because Git is an integral part of any good workflow.\n\nLogging in our training cluster through SSH\n\nA username, hostname, and password will be given to you during the workshop.\n\n\nNote that this temporary cluster will only be available for the duration of this course.\n\n\nOpen a terminal emulator\nWindows users:  Install the free version of MobaXTerm and launch it.\nmacOS users:   Launch Terminal.\nLinux users:     Open the terminal emulator of your choice.\n\n\nAccess the cluster through secure shell\n\nWindows users\nFollow the first 18% of this demo.\nFor “Remote host”, use the hostname we gave you.\nSelect the box “Specify username” and provide your username.\n\nNote that the password is entered through blind typing, meaning that you will not see anything happening as you type it. This is a Linux feature. While it is a little disturbing at first, do know that it is working. Make sure to type it slowly to avoid typos, then press the “enter” key on your keyboard.\n\n\n\nmacOS and Linux users\nIn the terminal, run:\nssh &lt;username&gt;@&lt;hostname&gt;\n\nReplace the username and hostname by their values. For instance:\nssh user21@somecluster.c3.ca\n\nYou will be asked a question, answer “Yes”.\nWhen prompted, type the password.\n\nNote that the password is entered through blind typing, meaning that you will not see anything happening as you type it. This is a Linux feature. While it is a little disturbing at first, do know that it is working. Make sure to type it slowly to avoid typos, then press the “enter” key on your keyboard.\n\n\n\n\nTroubleshooting\nProblems logging in are almost always due to typos. If you cannot log in, retry slowly, entering your password carefully.",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Installation and setup"
    ]
  },
  {
    "objectID": "git/intro_install.html#using-git",
    "href": "git/intro_install.html#using-git",
    "title": "Installation and setup",
    "section": "Using Git",
    "text": "Using Git\nWe will use Git from the command line throughout this workshop. Why? There are very friendly GUIs (graphical user interfaces) software for Git, but there are many good reasons to learn how to use Git from the command line:\n\nGUI software tend to be buggy,\nthey are limited to simple operations and you can’t use Git to its full potential with them,\nwhen you work on a remote machine (e.g. on the Alliance clusters), using Git from the command line is so much more convenient,\nthere will be situations in your life when you will not have access to your favourite Git GUI software and you will have to use the command line, so you need to know how to use it, even if this is not what you will do in your every day workflow.\n\n\nIf you work on your machine\nmacOS users:      Open Terminal.\nWindows users:    Open Git Bash.\nLinux users:    Open the terminal emulator of your choice.\n\n\nIf you work on our training cluster\nYou are already set: as mentioned above, Git is available at every session without loading any module.",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Installation and setup"
    ]
  },
  {
    "objectID": "git/intro_install.html#configuring-git",
    "href": "git/intro_install.html#configuring-git",
    "title": "Installation and setup",
    "section": "Configuring Git",
    "text": "Configuring Git\nBefore you can use Git, you need to set some basic configuration. You will do this in the terminal you just opened.\n\nList settings\ngit config --list\n\n\nUser identity\ngit config --global user.name \"&lt;Your Name&gt;\"\ngit config --global user.email \"&lt;your@email&gt;\"\n\nExample:\n\ngit config --global user.name \"John Doe\"\ngit config --global user.email \"john.doe@gmail.com\"\n\nIt is recommended to use your real name and real email address: when you will collaborate on projects, you will probably want this information to be attached to your commits rather than a weird pseudo.\n\n\n\nText editor\ngit config --global core.editor \"&lt;text-editor&gt;\"\n\nExample for nano:\n\ngit config --global core.editor \"nano\"\n\n\nLine ending\n\nmacOS, Linux, or WSL\ngit config --global core.autocrlf input\n\n\nWindows\ngit config --global core.autocrlf true\n\n\n\nProject-specific configuration\nYou can also set project-specific configurations (e.g. maybe you want to use a different email address for a certain project).\nIn that case, navigate to your project and run the command without the --global flag.\n\nExample:\n\ncd /path/to/project\ngit config user.email \"your_other@email\"",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Installation and setup"
    ]
  },
  {
    "objectID": "git/intro_remotes.html",
    "href": "git/intro_remotes.html",
    "title": "Remotes",
    "section": "",
    "text": "Remotes are copies of a project and its history.\nThey can be located anywhere, including on external drive or on the same machine as the project, although they are often on a different machine to serve as backup, or on a network (e.g. internet) to serve as a syncing hub for collaborations.\nPopular online Git repository managers & hosting services:\n\nGitHub\nGitLab\nBitbucket\n\nLet’s see how to create and manage remotes.",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Remotes"
    ]
  },
  {
    "objectID": "git/intro_remotes.html#creating-a-remote-on-github",
    "href": "git/intro_remotes.html#creating-a-remote-on-github",
    "title": "Remotes",
    "section": "Creating a remote on GitHub",
    "text": "Creating a remote on GitHub\n\nCreate a free GitHub account\nIf you don’t already have one, sign up for a free GitHub account.\n\nTo avoid having to type your password all the time, you should set up SSH for your account.\n\n\n\nCreate an empty repository on GitHub\n\nGo to the GitHub website, login, and go to your home page.\nLook for the Repositories tab & click the green New button.\nEnter the name you want for your repo, without spaces.\nMake the repository public or private.\n\n\n\nLink empty repository to your repo\nClick on the Code green drop-down button, select SSH if you have set SSH for your GitHub account or HTTPS and copy the address.\nIn the command line, cd inside your project, and add the remote:\ngit remote add &lt;remote-name&gt; &lt;remote-address&gt;\nremote-name is a convenience name to identify that remote. You can choose any name, but since Git automatically call the remote origin when you clone a repo, it is common practice to use origin as the name for the first remote.\n\nExample (using an SSH address):\n\ngit remote add origin git@github.com:&lt;user&gt;/&lt;repo&gt;.git\n\nExample (using an HTTPS address):\n\ngit remote add origin https://github.com/&lt;user&gt;/&lt;repo&gt;.git",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Remotes"
    ]
  },
  {
    "objectID": "git/intro_remotes.html#getting-information-on-remotes",
    "href": "git/intro_remotes.html#getting-information-on-remotes",
    "title": "Remotes",
    "section": "Getting information on remotes",
    "text": "Getting information on remotes\nList remotes:\ngit remote\nList remotes with their addresses:\ngit remote -v\nGet more information on a remote:\ngit remote show &lt;remote-name&gt;\n\nExample:\n\ngit remote show origin",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Remotes"
    ]
  },
  {
    "objectID": "git/intro_remotes.html#managing-remotes",
    "href": "git/intro_remotes.html#managing-remotes",
    "title": "Remotes",
    "section": "Managing remotes",
    "text": "Managing remotes\nRename a remote:\ngit remote rename &lt;old-remote-name&gt; &lt;new-remote-name&gt;\nDelete a remote:\ngit remote remove &lt;remote-name&gt;\nChange the address of a remote:\ngit remote set-url &lt;remote-name&gt; &lt;new-url&gt; [&lt;old-url&gt;]",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Remotes"
    ]
  },
  {
    "objectID": "git/intro_remotes.html#getting-data-from-a-remote",
    "href": "git/intro_remotes.html#getting-data-from-a-remote",
    "title": "Remotes",
    "section": "Getting data from a remote",
    "text": "Getting data from a remote\nIf you collaborate on a project, you have to get the data added by your teammates to keep your local project up to date.\nTo download new data from a remote, you have 2 options:\n\ngit fetch\ngit pull\n\n\nFetching changes\nFetching downloads the data from a remote that you don’t already have in your local version of the project:\ngit fetch &lt;remote-name&gt;\nThe branches on the remote are now accessible locally as &lt;remote-name&gt;/&lt;branch&gt;. You can inspect them or you can merge them into your local branches.\n\nExample:\n\ngit fetch origin\n\n\nPulling changes\nPulling fetches the changes & merges them onto your local branches:\ngit pull &lt;remote-name&gt; &lt;branch&gt;\n\nExample:\n\ngit pull origin main\nIf your branch is already tracking a remote branch, you can omit the arguments:\ngit pull",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Remotes"
    ]
  },
  {
    "objectID": "git/intro_remotes.html#pushing-to-a-remote",
    "href": "git/intro_remotes.html#pushing-to-a-remote",
    "title": "Remotes",
    "section": "Pushing to a remote",
    "text": "Pushing to a remote\nUploading data to the remote is called pushing:\ngit push &lt;remote-name&gt; &lt;branch-name&gt;\n\nExample:\n\ngit push origin main\nYou can set an upstream branch to track a local branch with the -u flag:\ngit push -u &lt;remote-name&gt; &lt;branch-name&gt;\n\nExample:\n\ngit push -u origin main\nFrom now on, all you have to run when you are on main is:\ngit push\n \n\nby jscript",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Remotes"
    ]
  },
  {
    "objectID": "git/intro_stash.html",
    "href": "git/intro_stash.html",
    "title": "Stashing",
    "section": "",
    "text": "Stashing is a way to put changes aside for some time. Changes can be reapplied later (and/or applied on other branches).\nWhy would you want to do that? And how?",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Stashing"
    ]
  },
  {
    "objectID": "git/intro_stash.html#what-are-git-stashes",
    "href": "git/intro_stash.html#what-are-git-stashes",
    "title": "Stashing",
    "section": "What are Git stashes?",
    "text": "What are Git stashes?\nHaving a clean working tree is necessary for many Git operations and recommended for others. If you have changes from an unfinished piece of work getting in the way, you could do an ugly commit to get rid of them. But if you care about having an organized history with meaningful commits (or if you don’t want others to see your messy drafts), stashing is a much better alternative.",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Stashing"
    ]
  },
  {
    "objectID": "git/intro_stash.html#creating-a-stash",
    "href": "git/intro_stash.html#creating-a-stash",
    "title": "Stashing",
    "section": "Creating a stash",
    "text": "Creating a stash\nYou can stash the changes in your modified files with:\ngit stash\nTo also include new (untracked) files, you have to use the -u flag:\ngit stash -u",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Stashing"
    ]
  },
  {
    "objectID": "git/intro_stash.html#listing-stashes",
    "href": "git/intro_stash.html#listing-stashes",
    "title": "Stashing",
    "section": "Listing stashes",
    "text": "Listing stashes\nOf course, you don’t want to lose or forget about your stashes.\nYou can list them with:\ngit stash list\nStashes are also shown when you run git log (with any of its variations) with the --all flag.\n\nExample:\n\ngit log --graph --oneline --all",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Stashing"
    ]
  },
  {
    "objectID": "git/intro_stash.html#re-applying-changes-from-a-stash",
    "href": "git/intro_stash.html#re-applying-changes-from-a-stash",
    "title": "Stashing",
    "section": "Re-applying changes from a stash",
    "text": "Re-applying changes from a stash\nTo re-apply the changes (or apply them on another branch), you run:\ngit stash apply\nIf you had staged changes, you can also restore the state of the index by running instead:\ngit stash apply --index\nIf you created multiple stashes, this will apply the last one you created. If this is not what you want, you have to specify which stash you want to use with the reflog syntax:\n\nstash@{0} is the last stash (so you can omit it)\n\nstash@{1} is the one before it\n\nstash@{2} the one before that\n\netc.\n\n\nTo apply the stash before last:\n\ngit stash apply stash@{1}",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Stashing"
    ]
  },
  {
    "objectID": "git/intro_stash.html#deleting-a-stash",
    "href": "git/intro_stash.html#deleting-a-stash",
    "title": "Stashing",
    "section": "Deleting a stash",
    "text": "Deleting a stash\nYou delete the last (or the only) stash with:\ngit stash drop\nHere again, if you want to delete another stash, specify it with its reflog index.\n\nTo delete the antepenultimate stash:\n\ngit stash drop stash@{2}\nYou can apply and delete a stash at the same time with:\ngit stash pop\nThis is convenient, but less flexible.",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Stashing"
    ]
  },
  {
    "objectID": "git/intro_three_trees.html",
    "href": "git/intro_three_trees.html",
    "title": "The three trees of Git",
    "section": "",
    "text": "One useful mental representation of the functioning of Git is to imagine three file trees."
  },
  {
    "objectID": "git/intro_three_trees.html#the-three-trees-of-git",
    "href": "git/intro_three_trees.html#the-three-trees-of-git",
    "title": "The three trees of Git",
    "section": "The three trees of Git",
    "text": "The three trees of Git\n\nWorking directory\nLet’s imagine that you are starting to work on a project.\nFirst, you create a directory.\nIn it, you create several sub-directories.\nIn those, you create a number of files.\nYou can open these files, read them, edit them, etc. This is something you are very familiar with.\nIn the Git world, this is the working directory or working tree of the project.\nThat is: an uncompressed version of your files that you can access and edit.\nYou can think of it as a sandbox because this is where you can experiment with the project. This is where the project gets developed.\nNow, Git has two other important pieces in its architecture.\n\n\nIndex\nIf you want the project history to be useful to future you, it has to be nice and tidy. You don’t want to record snapshots haphazardly or you will never be able to find anything back.\nBefore you record a snapshot, you carefully select the elements of the project as it is now that would be useful to write to the project history together. The index or staging area is what allows to do that: it contains the suggested future snapshot.\n\n\nHEAD\nFinally, the last tree in Git architecture is one snapshot in the project history that serves as a reference version of the project: if you want to see what you have been experimenting on in your “sandbox”, you need to compare the state of the working directory with some snapshot.\nRemember that HEAD is a pointer pointing at a branch, that a branch is itself a pointer pointing at a commit, and finally that a commit is a Git object pointing at a snapshot. When the HEAD pointer moves around, whatever snapshot it points to populates the HEAD tree.\nAs we saw earlier, when you create a commit, HEAD automatically points to the new commit. So the HEAD tree is often filled with the last snapshot you created. But—as we will see later—we can move the HEAD pointer around through other ways. So the HEAD tree can be populated by any snapshot in your project history."
  },
  {
    "objectID": "git/intro_three_trees.html#status-of-the-three-trees",
    "href": "git/intro_three_trees.html#status-of-the-three-trees",
    "title": "The three trees of Git",
    "section": "Status of the three trees",
    "text": "Status of the three trees\nTo display the status of these trees, you run:\ngit status"
  },
  {
    "objectID": "git/intro_three_trees.html#three-trees-in-action",
    "href": "git/intro_three_trees.html#three-trees-in-action",
    "title": "The three trees of Git",
    "section": "Three trees in action",
    "text": "Three trees in action\n\nClean working tree\nWe say that the working tree is “clean” when all changes tracked by Git were staged and committed:\n\nHere is an example for a project with a single file called File at version v1.\n\n\n\n\nMaking changes to the working tree\nWhen you edit files in your project, you make changes in the working directory or working tree.\n\nFor instance, you make changes to File. Let’s say that it is now at version v2:\n\n\nThe other two trees remain at version v1.\nIf you run git status, this is what you get:\nOn branch main\nChanges not staged for commit:\n  (use \"git add &lt;file&gt;...\" to update what will be committed)\n  (use \"git restore &lt;file&gt;...\" to discard changes in working directory)\n        modified:   File\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\n\n\nStaging changes\nYou stage that file (meaning that you will include the changes of that file in the next commit) with:\ngit add File\nAfter which, your Git trees look like this:\n\nNow, the index also has File at version v2 and git status returns:\nOn branch main\nChanges to be committed:\n  (use \"git restore --staged &lt;file&gt;...\" to unstage)\n        modified:   File\n\n\nCommitting changes\nFinally, you create a snapshot and the commit pointing to it—recording the staged changes to history—with:\ngit commit -m \"Added File\"\n-m is a flag that allows to provide the commit message directly in the command line. If you don’t use it, Git will open a text editor so that you can type the message. Without a message, there can be no commit.\nNow your trees look like this:\n\nOur working tree is clean again and git status returns:\nOn branch main\nnothing to commit, working tree clean\nThis means that there are no uncommitted changes in the working tree or the staging area: all the changes have been written to history.\n\nYou don’t have to stage all the changes in the working directory before making a commit; that is actually the whole point of the staging area.\nThis means that the working directory is not necessarily clean after you have created a new commit."
  },
  {
    "objectID": "git/intro_tools.html",
    "href": "git/intro_tools.html",
    "title": "Tools for a friendlier Git",
    "section": "",
    "text": "Two great open-source tools to work with Git in a nice visual manner while remaining in the command line.",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Tools for a friendlier Git"
    ]
  },
  {
    "objectID": "git/intro_tools.html#fzf",
    "href": "git/intro_tools.html#fzf",
    "title": "Tools for a friendlier Git",
    "section": "fzf",
    "text": "fzf\nfzf is a fantastic multi-platform command line fuzzy finder with a huge versatility.\n\nIn this video, I demo quickly how it can be used with Git:\n\n\nfzf is easy to install on your machine and can also be installed on the Alliance clusters with:\ngit clone --depth 1 https://github.com/junegunn/fzf.git ~/.fzf &&\n    ~/.fzf/install\n\n# then answer y, y, n to the 3 questions",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Tools for a friendlier Git"
    ]
  },
  {
    "objectID": "git/intro_tools.html#lazygit",
    "href": "git/intro_tools.html#lazygit",
    "title": "Tools for a friendlier Git",
    "section": "lazygit",
    "text": "lazygit\nlazygit is an excellent multi-platform terminal user interface for Git. It is my favourite Git interface and I use it all the time. You can find the installation instructions here.\nI recently gave a webinar on it that you can find here. Below is the video from the webinar:",
    "crumbs": [
      "Git",
      "<b><em>Getting started</em></b>",
      "Tools for a friendlier Git"
    ]
  },
  {
    "objectID": "git/practice_repo/ws_search.html",
    "href": "git/practice_repo/ws_search.html",
    "title": "Searching a version-controlled project",
    "section": "",
    "text": "What is the point of creating all these commits if you are unable to make use of them because you can’t find the information you need in them?\nIn this workshop, we will learn how to search:\n\nyour files (at any of their versions) and\nyour commit logs.\n\nBy the end of the workshop, you should be able to retrieve anything you need from your versioned project.",
    "crumbs": [
      "Git",
      "<b><em>Workshops</em></b>",
      "Searching a Git project"
    ]
  },
  {
    "objectID": "git/practice_repo/ws_search.html#installation",
    "href": "git/practice_repo/ws_search.html#installation",
    "title": "Searching a version-controlled project",
    "section": "Installation",
    "text": "Installation\nmacOS & Linux users:\nInstall Git from the official website.\nWindows users:\nInstall Git for Windows. This will also install “Git Bash”, a Bash emulator.",
    "crumbs": [
      "Git",
      "<b><em>Workshops</em></b>",
      "Searching a Git project"
    ]
  },
  {
    "objectID": "git/practice_repo/ws_search.html#using-git",
    "href": "git/practice_repo/ws_search.html#using-git",
    "title": "Searching a version-controlled project",
    "section": "Using Git",
    "text": "Using Git\nWe will use Git from the command line throughout this workshop.\nmacOS users:    open “Terminal”.\nWindows users:   open “Git Bash”.\nLinux users:    open the terminal emulator of your choice.",
    "crumbs": [
      "Git",
      "<b><em>Workshops</em></b>",
      "Searching a Git project"
    ]
  },
  {
    "objectID": "git/practice_repo/ws_search.html#practice-repo",
    "href": "git/practice_repo/ws_search.html#practice-repo",
    "title": "Searching a version-controlled project",
    "section": "Practice repo",
    "text": "Practice repo\n\nGet a repo\nYou are welcome to use a repository of yours to follow this workshop. Alternatively, you can clone a practice repo I have on GitHub:\n\nNavigate to an appropriate location:\n\ncd /path/to/appropriate/location\n\nClone the repo:\n\n# If you have set SSH for your GitHub account\ngit clone git@github.com:prosoitos/practice_repo.git\n# If you haven't set SSH\ngit clone https://github.com/prosoitos/practice_repo.git\n\nEnter the repo:\n\ncd practice_repo",
    "crumbs": [
      "Git",
      "<b><em>Workshops</em></b>",
      "Searching a Git project"
    ]
  },
  {
    "objectID": "git/practice_repo/ws_search.html#searching-files",
    "href": "git/practice_repo/ws_search.html#searching-files",
    "title": "Searching a version-controlled project",
    "section": "Searching files",
    "text": "Searching files\nThe first thing that can happen is that you are looking for a certain pattern somewhere in your project (for instance a certain function or a certain word).\n\ngit grep\nThe main command to look through versioned files is git grep.\nYou might be familiar with the command-line utility grep which allows to search for lines matching a certain pattern in files. git grep does a similar job with these differences:\n\nit is much faster since all files under version control are already indexed by Git,\nyou can easily search any commit without having to check it out,\nit has features lacking in grep such as, for instance, pattern arithmetic or tree search using globs.\n\n\n\nLet’s try it\nBy default, git grep searches recursively through the tracked files in the working directory (that is, the current version of the tracked files).\nFirst, let’s look for the word test in the current version of the tracked files in the test repo:\n\ngit grep test\n\nadrian.txt:Adrian's test text file.\nformerlyadrian.txt:Adrian's test text file.\nms/protocol.md:This is my test.\nms/smabraha.txt:This is a test file that I wanted to make, then push it somehow\nredone17.txt:this is a test file from redone17\nsrc/test_manuel.py:def test(model, device, test_loader):\nsrc/test_manuel.py:    test_loss = 0\nsrc/test_manuel.py:        for data, target in test_loader:\nsrc/test_manuel.py:            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\nsrc/test_manuel.py:    test_loss /= len(test_loader.dataset)\nsrc/test_manuel.py:        test_loss, correct, len(test_loader.dataset),\nsrc/test_manuel.py:        100. * correct / len(test_loader.dataset)))\nsrc/test_manuel.py:    test_data = datasets.MNIST(\nsrc/test_manuel.py:    test_loader = torch.utils.data.DataLoader(test_data, batch_size=100)\nsrc/test_manuel.py:        test(model, device, test_loader)\ntestAV1.txt:This is a test\ntext-collab.txt:This is the collaboration testing\n\n\nLet’s add blank lines between the results of each file for better readability:\n\ngit grep --break test\n\nadrian.txt:Adrian's test text file.\n\nformerlyadrian.txt:Adrian's test text file.\n\nms/protocol.md:This is my test.\n\nms/smabraha.txt:This is a test file that I wanted to make, then push it somehow\n\nredone17.txt:this is a test file from redone17\n\nsrc/test_manuel.py:def test(model, device, test_loader):\nsrc/test_manuel.py:    test_loss = 0\nsrc/test_manuel.py:        for data, target in test_loader:\nsrc/test_manuel.py:            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\nsrc/test_manuel.py:    test_loss /= len(test_loader.dataset)\nsrc/test_manuel.py:        test_loss, correct, len(test_loader.dataset),\nsrc/test_manuel.py:        100. * correct / len(test_loader.dataset)))\nsrc/test_manuel.py:    test_data = datasets.MNIST(\nsrc/test_manuel.py:    test_loader = torch.utils.data.DataLoader(test_data, batch_size=100)\nsrc/test_manuel.py:        test(model, device, test_loader)\n\ntestAV1.txt:This is a test\n\ntext-collab.txt:This is the collaboration testing\n\n\nLet’s also put the file names on separate lines:\n\ngit grep --break --heading test\n\nadrian.txt\nAdrian's test text file.\n\nformerlyadrian.txt\nAdrian's test text file.\n\nms/protocol.md\nThis is my test.\n\nms/smabraha.txt\nThis is a test file that I wanted to make, then push it somehow\n\nredone17.txt\nthis is a test file from redone17\n\nsrc/test_manuel.py\ndef test(model, device, test_loader):\n    test_loss = 0\n        for data, target in test_loader:\n            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n    test_loss /= len(test_loader.dataset)\n        test_loss, correct, len(test_loader.dataset),\n        100. * correct / len(test_loader.dataset)))\n    test_data = datasets.MNIST(\n    test_loader = torch.utils.data.DataLoader(test_data, batch_size=100)\n        test(model, device, test_loader)\n\ntestAV1.txt\nThis is a test\n\ntext-collab.txt\nThis is the collaboration testing\n\n\nWe can display the line numbers for the results with the -n flag:\n\ngit grep --break --heading -n test\n\nadrian.txt\n1:Adrian's test text file.\n\nformerlyadrian.txt\n1:Adrian's test text file.\n\nms/protocol.md\n9:This is my test.\n\nms/smabraha.txt\n1:This is a test file that I wanted to make, then push it somehow\n\nredone17.txt\n1:this is a test file from redone17\n\nsrc/test_manuel.py\n50:def test(model, device, test_loader):\n52:    test_loss = 0\n55:        for data, target in test_loader:\n58:            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n62:    test_loss /= len(test_loader.dataset)\n65:        test_loss, correct, len(test_loader.dataset),\n66:        100. * correct / len(test_loader.dataset)))\n84:    test_data = datasets.MNIST(\n90:    test_loader = torch.utils.data.DataLoader(test_data, batch_size=100)\n97:        test(model, device, test_loader)\n\ntestAV1.txt\n1:This is a test\n\ntext-collab.txt\n1:This is the collaboration testing\n\n\nNotice how the results for the file src/test_manuel.py involve functions. It would be very convenient to have the names of the functions in which test appears.\nWe can do this with the -p flag:\n\ngit grep --break --heading -p test src/test_manuel.py\n\nsrc/test_manuel.py\ndef train(model, device, train_loader, optimizer, epoch):\ndef test(model, device, test_loader):\n    test_loss = 0\n        for data, target in test_loader:\n            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n    test_loss /= len(test_loader.dataset)\n        test_loss, correct, len(test_loader.dataset),\n        100. * correct / len(test_loader.dataset)))\ndef main():\n    test_data = datasets.MNIST(\n    test_loader = torch.utils.data.DataLoader(test_data, batch_size=100)\n        test(model, device, test_loader)\n\n\n\nWe added the argument src/test_manuel.py to limit the search to that file.\n\nWe can now see that the word test appears in the functions test and main.\nNow, instead of printing all the matching lines, let’s print the number of matches per file:\n\ngit grep -c test\n\nadrian.txt:1\nformerlyadrian.txt:1\nms/protocol.md:1\nms/smabraha.txt:1\nredone17.txt:1\nsrc/test_manuel.py:10\ntestAV1.txt:1\ntext-collab.txt:1\n\n\n\n\nMore complex patterns\ngit grep in fact searches for regular expressions. test is a regular expression matching test, but we can look for more complex patterns.\nLet’s look for image:\n\ngit grep image\n\n\nNo output means that the search is not returning any result.\n\nLet’s make this search case insensitive:\n\ngit grep -i image\n\nsrc/new_file.py:from PIL import Image\nsrc/new_file.py:berlin1_lr = Image.open(\"/home/marie/parvus/pwg/wtm/slides/static/img/upscaling/lr/berlin_1945_1.jpg\")\nsrc/new_file.py:berlin1_hr = Image.open(\"/home/marie/parvus/pwg/wtm/slides/static/img/upscaling/hr/berlin_1945_1.png\")\n\n\nWe are now getting some results as Image was present in three lines of the file src/new_file.py.\nLet’s now search for data:\n\ngit grep data\n\n.gitignore:data/\nms/protocol.md:Collected and analyzed amazing data\nsrc/new_file.py:from datasets import load_dataset\nsrc/new_file.py:set5 = load_dataset('eugenesiow/Set5', 'bicubic_x4', split='validation')\nsrc/test_manuel.py:from torchvision import datasets, transforms\nsrc/test_manuel.py:    for batch_idx, (data, target) in enumerate(train_loader):\nsrc/test_manuel.py:        data, target = data.to(device), target.to(device)\nsrc/test_manuel.py:        output = model(data)\nsrc/test_manuel.py:                epoch, batch_idx * len(data), len(train_loader.dataset),\nsrc/test_manuel.py:        for data, target in test_loader:\nsrc/test_manuel.py:            data, target = data.to(device), target.to(device)\nsrc/test_manuel.py:            output = model(data)\nsrc/test_manuel.py:    test_loss /= len(test_loader.dataset)\nsrc/test_manuel.py:        test_loss, correct, len(test_loader.dataset),\nsrc/test_manuel.py:        100. * correct / len(test_loader.dataset)))\nsrc/test_manuel.py:    train_data = datasets.MNIST(\nsrc/test_manuel.py:        '~/parvus/pwg/wtm/tml/data',\nsrc/test_manuel.py:        # '~/projects/def-sponsor00/data',\nsrc/test_manuel.py:    test_data = datasets.MNIST(\nsrc/test_manuel.py:        '~/parvus/pwg/wtm/tml/data',\nsrc/test_manuel.py:        # '~/projects/def-sponsor00/data',\nsrc/test_manuel.py:    train_loader = torch.utils.data.DataLoader(train_data, batch_size=50)\nsrc/test_manuel.py:    test_loader = torch.utils.data.DataLoader(test_data, batch_size=100)\n\n\nWe are getting results for the word data, but also for the pattern data in longer expressions such as train_data or dataset. If we only want results for the word data, we can use the -w flag:\n\ngit grep -w data\n\n.gitignore:data/\nms/protocol.md:Collected and analyzed amazing data\nsrc/test_manuel.py:    for batch_idx, (data, target) in enumerate(train_loader):\nsrc/test_manuel.py:        data, target = data.to(device), target.to(device)\nsrc/test_manuel.py:        output = model(data)\nsrc/test_manuel.py:                epoch, batch_idx * len(data), len(train_loader.dataset),\nsrc/test_manuel.py:        for data, target in test_loader:\nsrc/test_manuel.py:            data, target = data.to(device), target.to(device)\nsrc/test_manuel.py:            output = model(data)\nsrc/test_manuel.py:        '~/parvus/pwg/wtm/tml/data',\nsrc/test_manuel.py:        # '~/projects/def-sponsor00/data',\nsrc/test_manuel.py:        '~/parvus/pwg/wtm/tml/data',\nsrc/test_manuel.py:        # '~/projects/def-sponsor00/data',\nsrc/test_manuel.py:    train_loader = torch.utils.data.DataLoader(train_data, batch_size=50)\nsrc/test_manuel.py:    test_loader = torch.utils.data.DataLoader(test_data, batch_size=100)\n\n\nNow, let’s use a more complex regular expression. We want the counts for the pattern \".*_.*\" (i.e. any name with a snail case such as train_loader):\n\ngit grep -c \".*_.*\"\n\n.gitignore:4\nsrc/new_file.py:22\nsrc/test_manuel.py:29\n\n\nLet’s print the first 3 results per file:\n\ngit grep -m 3 \".*_.*\"\n\n.gitignore:hidden_file\n.gitignore:search_cache/\n.gitignore:ws_search_cache/html\nsrc/new_file.py:from datasets import load_dataset\nsrc/new_file.py:set5 = load_dataset('eugenesiow/Set5', 'bicubic_x4', split='validation')\nsrc/new_file.py:set5.column_names\nsrc/test_manuel.py:from torch.optim.lr_scheduler import StepLR\nsrc/test_manuel.py:    def __init__(self):\nsrc/test_manuel.py:        super(Net, self).__init__()\n\n\nAs you can see, our results also include __init__ which is not what we were looking for. So let’s exclude __:\n\ngit grep -m 3 -e \".*_.*\" --and --not -e \"__\"\n\n.gitignore:hidden_file\n.gitignore:search_cache/\n.gitignore:ws_search_cache/html\nsrc/new_file.py:from datasets import load_dataset\nsrc/new_file.py:set5 = load_dataset('eugenesiow/Set5', 'bicubic_x4', split='validation')\nsrc/new_file.py:set5.column_names\nsrc/test_manuel.py:from torch.optim.lr_scheduler import StepLR\nsrc/test_manuel.py:        x = F.max_pool2d(x, 2)\nsrc/test_manuel.py:        output = F.log_softmax(x, dim=1)\n\n\n\nFor simple searches, you don’t have to use the -e flag before the pattern you are searching for. Here however, our command has gotten complex enough that we have to use it before each pattern.\n\nLet’s make sure this worked as expected:\n\ngit grep -c \".*_.*\"\necho \"---\"\ngit grep -c \"__\"\necho \"---\"\ngit grep -ce \".*_.*\" --and --not -e \"__\"\n\n.gitignore:4\nsrc/new_file.py:22\nsrc/test_manuel.py:29\n---\nsrc/test_manuel.py:2\n---\n.gitignore:4\nsrc/new_file.py:22\nsrc/test_manuel.py:27\n\n\nThere were 2 lines matching __ in src/test_manuel.py and we have indeed excluded them from our search.\nExtended regular expressions are also covered with the flag -E.\n\n\nSearching other trees\nSo far, we have searched the current version of tracked files, but we can just as easily search files at any commit.\nLet’s search for test in the tracked files 20 commits ago:\n\ngit grep test HEAD~20\n\nHEAD~20:adrian.txt:Adrian's test text file.\nHEAD~20:formerlyadrian.txt:Adrian's test text file.\nHEAD~20:ms/protocol.md:This is my test.\nHEAD~20:ms/smabraha.txt:This is a test file that I wanted to make, then push it somehow\nHEAD~20:redone17.txt:this is a test file from redone17\nHEAD~20:testAV1.txt:This is a test\nHEAD~20:text-collab.txt:This is the collaboration testing\n\n\n\nAs you can see, the file src/test_manuel.py is not in the results. Either it didn’t exist or it didn’t have the word test at that commit.\n\nIf you want to search tracked files AND untracked files, you need to use the --untracked flag.\nLet’s create a new (thus untracked) file with some content including the word test:\n\necho \"This is a test\" &gt; newfile\n\nNow compare the following:\n\ngit grep -c test\n\nadrian.txt:1\nformerlyadrian.txt:1\nms/protocol.md:1\nms/smabraha.txt:1\nredone17.txt:1\nsrc/test_manuel.py:10\ntestAV1.txt:1\ntext-collab.txt:1\n\n\nwith:\n\ngit grep -c --untracked test\n\nadrian.txt:1\nformerlyadrian.txt:1\nms/protocol.md:1\nms/smabraha.txt:1\nredone17.txt:1\nsrc/test_manuel.py:10\ntestAV1.txt:1\ntext-collab.txt:1\nws_search.rmarkdown:41\n\n\n\nThis last result also returned our untracked file newfile.\n\nIf you want to search untracked and ignored files (meaning all your files), use the flags --untracked --no-exclude-standard.\nLet’s see what the .gitignore file contains:\n\ncat .gitignore\n\ndata/\noutput/\nhidden_file\nsearch_cache/\nsearch.qmd\nnewfile\nimg\nws_search_cache/html\nws_search.qmd\n\n\nThe directory data is in .gitignore. This means that it is not under version control and it thus doesn’t exist in our repo (since we cloned our repo, we only have the version-controlled files). Let’s create it:\nmkdir data\nNow, let’s create a file in it that contains test:\n\necho \"And another test\" &gt; data/file\n\nWe can rerun our previous two searches to verify that files excluded from version control are not searched:\n\ngit grep -c test\n\nadrian.txt:1\nformerlyadrian.txt:1\nms/protocol.md:1\nms/smabraha.txt:1\nredone17.txt:1\nsrc/test_manuel.py:10\ntestAV1.txt:1\ntext-collab.txt:1\n\n\n\ngit grep -c --untracked test\n\nadrian.txt:1\nformerlyadrian.txt:1\nms/protocol.md:1\nms/smabraha.txt:1\nredone17.txt:1\nsrc/test_manuel.py:10\ntestAV1.txt:1\ntext-collab.txt:1\nws_search.rmarkdown:41\n\n\nAnd now, let’s try:\n\ngit grep -c --untracked --no-exclude-standard test\n\nadrian.txt:1\ndata/file:1\nformerlyadrian.txt:1\nms/protocol.md:1\nms/smabraha.txt:1\nnewfile:1\nredone17.txt:1\nsrc/test_manuel.py:10\ntestAV1.txt:1\ntext-collab.txt:1\nws_search.qmd:41\nws_search.rmarkdown:41\n\n\n\ndata/file, despite being excluded from version control, is also searched.\n\n\n\nSearching all commits\nWe saw that git grep &lt;pattern&gt; &lt;commit&gt; can search a pattern in any commit. Now, what if we all to search all commits for a pattern?\nFor this, we pass the expression $(git rev-list --all) in lieu of &lt;commit&gt;.\ngit rev-list --all creates a list of all the commits in a way that can be used as an argument to other functions. The $() allows to run the expression inside it and pass the result as and argument.\nTo search for test in all the commits, we thus run:\ngit grep \"test\" $(git rev-list --all)\nI am not running this command has it has a huge output. Instead, I will limit the search to the last two commits:\n\ngit grep \"test\" $(git rev-list --all -2)\n\n388fdc13de66537cac2169253cb385dfd409e710:adrian.txt:Adrian's test text file.\n388fdc13de66537cac2169253cb385dfd409e710:formerlyadrian.txt:Adrian's test text file.\n388fdc13de66537cac2169253cb385dfd409e710:ms/protocol.md:This is my test.\n388fdc13de66537cac2169253cb385dfd409e710:ms/smabraha.txt:This is a test file that I wanted to make, then push it somehow\n388fdc13de66537cac2169253cb385dfd409e710:redone17.txt:this is a test file from redone17\n388fdc13de66537cac2169253cb385dfd409e710:src/test_manuel.py:def test(model, device, test_loader):\n388fdc13de66537cac2169253cb385dfd409e710:src/test_manuel.py:    test_loss = 0\n388fdc13de66537cac2169253cb385dfd409e710:src/test_manuel.py:        for data, target in test_loader:\n388fdc13de66537cac2169253cb385dfd409e710:src/test_manuel.py:            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n388fdc13de66537cac2169253cb385dfd409e710:src/test_manuel.py:    test_loss /= len(test_loader.dataset)\n388fdc13de66537cac2169253cb385dfd409e710:src/test_manuel.py:        test_loss, correct, len(test_loader.dataset),\n388fdc13de66537cac2169253cb385dfd409e710:src/test_manuel.py:        100. * correct / len(test_loader.dataset)))\n388fdc13de66537cac2169253cb385dfd409e710:src/test_manuel.py:    test_data = datasets.MNIST(\n388fdc13de66537cac2169253cb385dfd409e710:src/test_manuel.py:    test_loader = torch.utils.data.DataLoader(test_data, batch_size=100)\n388fdc13de66537cac2169253cb385dfd409e710:src/test_manuel.py:        test(model, device, test_loader)\n388fdc13de66537cac2169253cb385dfd409e710:testAV1.txt:This is a test\n388fdc13de66537cac2169253cb385dfd409e710:text-collab.txt:This is the collaboration testing\n423f454765d45e21e0ae401da0b3dec2d84113ce:adrian.txt:Adrian's test text file.\n423f454765d45e21e0ae401da0b3dec2d84113ce:formerlyadrian.txt:Adrian's test text file.\n423f454765d45e21e0ae401da0b3dec2d84113ce:ms/protocol.md:This is my test.\n423f454765d45e21e0ae401da0b3dec2d84113ce:ms/smabraha.txt:This is a test file that I wanted to make, then push it somehow\n423f454765d45e21e0ae401da0b3dec2d84113ce:redone17.txt:this is a test file from redone17\n423f454765d45e21e0ae401da0b3dec2d84113ce:src/test_manuel.py:def test(model, device, test_loader):\n423f454765d45e21e0ae401da0b3dec2d84113ce:src/test_manuel.py:    test_loss = 0\n423f454765d45e21e0ae401da0b3dec2d84113ce:src/test_manuel.py:        for data, target in test_loader:\n423f454765d45e21e0ae401da0b3dec2d84113ce:src/test_manuel.py:            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n423f454765d45e21e0ae401da0b3dec2d84113ce:src/test_manuel.py:    test_loss /= len(test_loader.dataset)\n423f454765d45e21e0ae401da0b3dec2d84113ce:src/test_manuel.py:        test_loss, correct, len(test_loader.dataset),\n423f454765d45e21e0ae401da0b3dec2d84113ce:src/test_manuel.py:        100. * correct / len(test_loader.dataset)))\n423f454765d45e21e0ae401da0b3dec2d84113ce:src/test_manuel.py:    test_data = datasets.MNIST(\n423f454765d45e21e0ae401da0b3dec2d84113ce:src/test_manuel.py:    test_loader = torch.utils.data.DataLoader(test_data, batch_size=100)\n423f454765d45e21e0ae401da0b3dec2d84113ce:src/test_manuel.py:        test(model, device, test_loader)\n423f454765d45e21e0ae401da0b3dec2d84113ce:testAV1.txt:This is a test\n423f454765d45e21e0ae401da0b3dec2d84113ce:text-collab.txt:This is the collaboration testing\n\n\n\nIn combination with the fuzzy finder tool fzf, this can make finding a particular commit extremely easy.\nFor instance, the code below allows you to dynamically search in the result through incremental completion:\ngit grep \"test\" $(git rev-list --all) | fzf --cycle -i -e\nOr even better, you can automatically copy the short form of the hash of the selected commit to clipboard so that you can use it with git show, git checkout, etc.:\ngit grep \"test\" $(git rev-list --all) |\n    fzf --cycle -i -e |\n    cut -c 1-7 |\n    xclip -r -selection clipboard\n\nHere, I am using xclip to copy to the clipboard as I am on Linux. Depending on your OS you might need to use a different tool.\n\nOf course, you can create a function in your .bashrc file with such code so that you wouldn’t have to type it each time:\ngrep_all_commits () {\n    git grep \"$1\" $(git rev-list --all) |\n        fzf --cycle -i -e |\n        cut -c 1-7 |\n        xclip -r -selection clipboard\n}\nAlternatively, you can pass the result directly into whatever git command you want to use that commit for.\nHere is an example with git show:\ngit grep \"test\" $(git rev-list --all) |\n    fzf --cycle -i -e |\n    cut -c 1-7 |\n    git show\nAnd if you wanted to get really fancy, you could go with:\ngit grep \"test\" $(git rev-list --all) |\n    fzf --cycle -i -e --no-multi \\\n        --ansi --preview=\"$_viewGitLogLine\" \\\n        --header \"enter: view, C-c: copy hash\" \\\n        --bind \"enter:execute:$_viewGitLogLine | less -R\" \\\n        --bind \"ctrl-c:execute:$_gitLogLineToHash |\n        xclip -r -selection clipboard\"\nWrapped in a function:\ngrep_all_commits_preview () {\n    git grep \"$1\" $(git rev-list --all) |\n        fzf --cycle -i -e --no-multi \\\n            --ansi --preview=\"$_viewGitLogLine\" \\\n            --header \"enter: view, C-c: copy hash\" \\\n            --bind \"enter:execute:$_viewGitLogLine |\n              less -R\" \\\n            --bind \"ctrl-c:execute:$_gitLogLineToHash |\n        xclip -r -selection clipboard\"\n}\nThis last function allows you to search through all the results in an incremental fashion while displaying a preview of the selected diff (the changes made at that particular commit). If you want to see more of the diff than the preview displays, press &lt;enter&gt; (then q to quit the pager), if you want to copy the hash of a commit, press C-c (Control + c).\nWith this function, you can now instantly get a preview of the changes made to any line containing an expression for any file, at any commit, and copy the hash of the selected commit. This is really powerful.\n\n\n\nAliases\nIf you don’t want to type a series of flags all the time, you can configure aliases for Git. For instance, Alex Razoumov uses the alias git search for git grep --break --heading -n -i.\nLet’s add to it the -p flag. Here is how you would set this alias:\ngit config --global alias.search 'grep --break --heading -n -i -p'\n\nThis setting gets added to your main Git configuration file (on Linux, by default, at ~/.gitconfig).\n\nFrom there on, you can use your alias with:\n\ngit search test\n\ngit: 'search' is not a git command. See 'git --help'.",
    "crumbs": [
      "Git",
      "<b><em>Workshops</em></b>",
      "Searching a Git project"
    ]
  },
  {
    "objectID": "git/practice_repo/ws_search.html#searching-logs",
    "href": "git/practice_repo/ws_search.html#searching-logs",
    "title": "Searching a version-controlled project",
    "section": "Searching logs",
    "text": "Searching logs\nThe second thing that can happen is that you are looking for some pattern in your version control logs.\n\ngit log\ngit log allows to get information on commit logs.\nBy default, it outputs all the commits of the current branch.\nLet’s show the logs of the last 3 commits:\n\ngit log -3\n\ncommit 388fdc13de66537cac2169253cb385dfd409e710\nAuthor: Marie-Helene Burle &lt;marie.burle@westdri.ca&gt;\nDate:   Thu Dec 14 20:55:30 2023 -0800\n\n    update gitignore\n\ncommit 423f454765d45e21e0ae401da0b3dec2d84113ce\nMerge: 7342af5 818c32a\nAuthor: Marie-Helene Burle &lt;marie.burle@westdri.ca&gt;\nDate:   Tue Dec 12 17:30:07 2023 -0800\n\n    Merge branch 'main' of github.com:prosoitos/practice_repo\n\ncommit 7342af5dfff53dc51dfaf99da1e29448fd253e03\nAuthor: Marie-Helene Burle &lt;marie.burle@westdri.ca&gt;\nDate:   Tue Dec 12 17:29:59 2023 -0800\n\n    update gitignore\n\n\nThe output can be customized thanks to a plethora of options.\nFor instance, here are the logs of the last 15 commits, in a graph, with one line per commit:\n\ngit log --graph --oneline -n 15\n\n* 388fdc1 update gitignore\n*   423f454 Merge branch 'main' of github.com:prosoitos/practice_repo\n|\\  \n| * 818c32a Delete ml_models directory\n| * b3c2414 Created using Colaboratory\n* | 7342af5 update gitignore\n|/  \n* e3cfb2e Update gitignore with Quarto files\n* 15fdec6 Update README.org\n* 15d4ee9 change values training\n* 06efa34 add lots of code\n* 1457143 remove stupid line\n* 711e1dc add real py content to test_manual.py\n* 90016aa adding new python file\n*   2c0f612 Merge branch 'main' of github.com:prosoitos/git_workshop_collab\n|\\  \n| *   6f7d03d Merge branch 'main' of https://github.com/prosoitos/git_workshop_collab into main\n| |\\  \n| * \\   3c53269 Merge branch 'main' of https://github.com/prosoitos/git_workshop_collab into main\n| |\\ \\  \n\n\nBut git log has also flags that allow to search for patterns.\n\n\nSearching commit messages\nOne of the reasons it is so important to write informative commit messages is that they are key to finding commits later on.\nTo look for a pattern in all your commit messages, use git log --grep=&lt;pattern&gt;.\nLet’s look for test in the commit messages and limit the output to 3 commits:\n\ngit log --grep=test -3\n\ncommit 711e1dc53011e5071b17dc7c35b516f6e066f396\nAuthor: Marie-Helene Burle &lt;marie.burle@westgrid.ca&gt;\nDate:   Tue Mar 15 11:52:48 2022 -0700\n\n    add real py content to test_manual.py\n\ncommit a55ca0d60d82578c94bd49fc4ca987727b851216\nAuthor: Manuelhrokr &lt;zl.manuel@protonmail.com&gt;\nDate:   Thu Feb 17 15:19:42 2022 -0700\n\n    new comment add just as test\n\ncommit ea74e46f487fba09c31524a110fdf060796e3cf8\nAuthor: mpkin &lt;mikin@physics.ubc.ca&gt;\nDate:   Thu Sep 23 14:51:24 2021 -0700\n\n    Add test_mk.txt\n\n\nFor a more compact output:\n\ngit log --grep=\"test\" -3 --oneline\n\n711e1dc add real py content to test_manual.py\na55ca0d new comment add just as test\nea74e46 Add test_mk.txt\n\n\n\nHere too you can use this in combination to fzf with for instance:\ngit log --grep=\"test\" | fzf --cycle -i -e\nOr:\ngit log --grep=\"test\" --oneline |\n    fzf --cycle -i -e --no-multi \\\n        --ansi --preview=\"$_viewGitLogLine\" \\\n        --header \"enter: view, C-c: copy hash\" \\\n        --bind \"enter:execute:$_viewGitLogLine | less -R\" \\\n        --bind \"ctrl-c:execute:$_gitLogLineToHash |\n        xclip -r -selection clipboard\"\n\n\n\nChanges made to a pattern\nRemember that test was present in the file src/test_manuel.py. If we want to see when the pattern was first created and then each time it was modified, we use the -L flag in this fashion:\ngit log -L :&lt;pattern&gt;:file\nIn our case:\n\ngit log -L :test:src/test_manuel.py\n\ncommit 711e1dc53011e5071b17dc7c35b516f6e066f396\nAuthor: Marie-Helene Burle &lt;marie.burle@westgrid.ca&gt;\nDate:   Tue Mar 15 11:52:48 2022 -0700\n\n    add real py content to test_manual.py\n\ndiff --git a/src/test_manuel.py b/src/test_manuel.py\n--- a/src/test_manuel.py\n+++ b/src/test_manuel.py\n@@ -1,1 +50,19 @@\n-test\n+def test(model, device, test_loader):\n+    model.eval()\n+    test_loss = 0\n+    correct = 0\n+    with torch.no_grad():\n+        for data, target in test_loader:\n+            data, target = data.to(device), target.to(device)\n+            output = model(data)\n+            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n+            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n+            correct += pred.eq(target.view_as(pred)).sum().item()\n+\n+    test_loss /= len(test_loader.dataset)\n+\n+    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n+        test_loss, correct, len(test_loader.dataset),\n+        100. * correct / len(test_loader.dataset)))\n+\n+\n\ncommit 90016aa3ed3a6cf71e206392bbf10adfe1a14c17\nAuthor: Manuelhrokr &lt;zl.manuel@protonmail.com&gt;\nDate:   Thu Feb 17 15:33:03 2022 -0700\n\n    adding new python file\n\ndiff --git a/src/test_manuel.py b/src/test_manuel.py\n--- /dev/null\n+++ b/src/test_manuel.py\n@@ -0,0 +1,1 @@\n+test\n\n\nThis is very useful if you want to see, for instance, changes made to a function in a script.\n\n\nChanges in number of occurrences of a pattern\nNow, if we want to list all commits that created a change in the number of occurrences of test in our project, we run:\n\ngit log -S test --oneline\n\n818c32a Delete ml_models directory\nb3c2414 Created using Colaboratory\n711e1dc add real py content to test_manual.py\n90016aa adding new python file\n652faa5 delete my file\nb684eac Deleted file\n6717236 For collab\nca1845d delete alex.txt\n6b56198 editing adrians text file\n01a7358 test dtrad\ne44a454 Create testAV1.txt\n5ee88e6 For collab\ncf3d4ea Collab-test\n13faa1e test, test\n0366115 Adrian's test file\n9ebd3ce This is my test\n6dfefa8 create redone17.txt\ne43163c added alex.txt\n\n\nThis can be useful to identify the commit you need.",
    "crumbs": [
      "Git",
      "<b><em>Workshops</em></b>",
      "Searching a Git project"
    ]
  },
  {
    "objectID": "git/practice_repo/ws_search.html#tldr",
    "href": "git/practice_repo/ws_search.html#tldr",
    "title": "Searching a version-controlled project",
    "section": "TL;DR",
    "text": "TL;DR\nHere are the search functions you are the most likely to use:\n\nSearch for a pattern in the current version of your tracked files:\n\ngit grep &lt;pattern&gt;\n\nSearch for a pattern in your files at a certain commit:\n\ngit grep &lt;pattern&gt; &lt;commit&gt;\n\nSearch for a pattern in your files in all the commits:\n\ngit grep &lt;pattern&gt; $(git rev-list --all)\n\nSearch for a pattern in your commit messages:\n\ngit log --grep=&lt;pattern&gt;\nNow you should be able to find pretty much anything in your projects and their histories.",
    "crumbs": [
      "Git",
      "<b><em>Workshops</em></b>",
      "Searching a Git project"
    ]
  },
  {
    "objectID": "git/top_wb.html",
    "href": "git/top_wb.html",
    "title": "Git webinars",
    "section": "",
    "text": "Data version control with\n\n\n\n\nA great Git UI: Lazygit",
    "crumbs": [
      "Git",
      "<b><em>Webinars</em></b>"
    ]
  },
  {
    "objectID": "git/wb_dvc.html",
    "href": "git/wb_dvc.html",
    "title": "Version control for data science and machine learning with DVC",
    "section": "",
    "text": "Data version control (DVC) is an open source tool that brings all the versioning and collaboration capabilities you use on your code with Git to your data and machine learning workflow.\nIf you use datasets in your work, it makes it easy to track their evolution.\nIf you are in the field of machine learning, it additionally allows you to track your models, manage your pipelines from parameters to metrics, collaborate on your experiments, and integrate with the continuous integration tool for machine learning projects CML.\nThis webinar will show you how to get started with DVC, first in the simple case where you just want to put your data under version control, then in the more complex situation where you want to manage your machine learning workflow in a more organized and reproducible fashion.\n\nSlides (Click and wait: this reveal.js presentation may take a little time to load.)",
    "crumbs": [
      "Git",
      "<b><em>Webinars</em></b>",
      "Data version control with DVC"
    ]
  },
  {
    "objectID": "git/wb_lazygit.html",
    "href": "git/wb_lazygit.html",
    "title": "A great Git UI: lazygit",
    "section": "",
    "text": "While it is important to know how to use Git from the command line, this makes for an austere experience: a series of commands are required to gather information on the state of the working tree, see changes to files, or get a schematic of the commit history. Committing sections of files interactively and other operations are just awkward affairs. On the other hand, the many graphic interfaces for Git are often buggy, slow, and limiting.\nOne option is to write exciting functions with tools such as fzf to make things more friendly and visual. A simpler and more polished option is to use an already built user interface for Git that runs directly in the command line. lazygit is one such open source tool. After years of development, it is a mature, beautiful tool that allows to perform any Git operation in the command line in a convenient, fast, and visual fashion.\nIn this webinar, I will demo how I use lazygit in my daily workflow to run routine as well as more complex Git commands.\n\nSlides (Click and wait: this reveal.js presentation may take a little time to load.)",
    "crumbs": [
      "Git",
      "<b><em>Webinars</em></b>",
      "A great Git TUI: lazygit"
    ]
  },
  {
    "objectID": "git/ws_collab.html",
    "href": "git/ws_collab.html",
    "title": "Collaborating through Git & GitHub",
    "section": "",
    "text": "Using Internet hosting services such as GitHub, Git is a powerful collaboration tool.\nIn this workshop, we will cover the three classic collaboration situations and see how a collaborative workflow works.",
    "crumbs": [
      "Git",
      "<b><em>Workshops</em></b>",
      "Collaborating through Git"
    ]
  },
  {
    "objectID": "git/ws_collab.html#three-situations",
    "href": "git/ws_collab.html#three-situations",
    "title": "Collaborating through Git & GitHub",
    "section": "Three situations",
    "text": "Three situations\nWhen you collaborate on a project through Git and a remote such as GitHub, there are three situations:\n\nyou create a project on your machine and want others to contribute to it (1),\nyou want to contribute to a project started by others and\n\nyou have write access to it (2),\nyou do not have write access to it (3).\n\n\n\n(1) You start the project\nIn this first situation, you are the author of a project (you have a project under version control on your own machine) and you want to initiate a collaboration with others on it using GitHub as a remote.\n\nCreate a remote on GitHub\nYou need to create a remote on GitHub.\n\nCreate a free GitHub account\nIf you don’t already have one, sign up for a free GitHub account.\n\nTo avoid having to type your password all the time, you should set up SSH for your account.\n\n\n\nCreate an empty repository on GitHub\n\nGo to the GitHub website, login, and go to your home page.\nLook for the Repositories tab & click the green New button.\nEnter the name you want for your repo, without spaces.\nMake the repository public or private.\n\n\n\nLink empty repository to your repo\nClick on the Code green drop-down button, select SSH if you have set SSH for your GitHub account or HTTPS and copy the address.\nIn the command line, cd inside your project, and add the remote:\ngit remote add &lt;remote-name&gt; &lt;remote-address&gt;\nremote-name is a convenience name to identify that remote. You can choose any name, but since Git automatically call the remote origin when you clone a repo, it is common practice to use origin as the name for the first remote.\n\nExample (using an SSH address):\n\ngit remote add origin git@github.com:&lt;user&gt;/&lt;repo&gt;.git\n\nExample (using an HTTPS address):\n\ngit remote add origin https://github.com/&lt;user&gt;/&lt;repo&gt;.git\nIf you don’t want to grant others write access to the project, and you only accept contributions through pull requests, you are set.\nIf you want to grant your collaborators write access to the project however, you need to add them to it.\n\n\n\nInvite collaborators\n\nGo to your GitHub project page.\nClick on the Settings tab.\nClick on the Manage access section on the left-hand side (you will be prompted for your GitHub password).\nClick on the Invite a collaborator green button.\nInvite your collaborators with one of their GitHub user name, their email address, or their full name.\n\n\n\n\n(2) Write access to project\nIn this second situation, someone else started a project and they are inviting you to collaborate to it, giving you write access to the project.\nIn this case, you need to clone the project: cd to the location where you want your local copy, then:\ngit clone &lt;remote-address&gt; &lt;local-name&gt;\nThis sets the project as a remote to your new local copy and that remote is automatically called origin.\nWithout &lt;local-name&gt;, the repo will have the name of the last part of the remote address.\n\n\n(3) No write access to project\nIn this third situation, someone else started a project and you want to collaborate to it, but you do not have write access to it.\nIn this case, you will have to submit pull requests.\nHere is the workflow for a pull request (PR):\n\nFork the project on GitHub.\nClone your fork on your machine.\nAdd the initial project as a second remote & call it upstream.\nPull from upstream to update your local project.\nCreate & checkout a new branch.\nMake & commit your changes on that branch.\nPush that branch to your fork (i.e. origin — remember that you do not have write access to upstream).\nGo to the original project GitHub’s page & open a pull request.",
    "crumbs": [
      "Git",
      "<b><em>Workshops</em></b>",
      "Collaborating through Git"
    ]
  },
  {
    "objectID": "git/ws_collab.html#collaborative-workflow",
    "href": "git/ws_collab.html#collaborative-workflow",
    "title": "Collaborating through Git & GitHub",
    "section": "Collaborative workflow",
    "text": "Collaborative workflow\n\nPulling and pushing\nWhen you collaborate with others using GitHub (or other remote), you and others will work simultaneously on some project. How does this work?\nTo upload your changes to the remote on GitHub you push to it with git push.\nIf one of your collaborators has made changes to the remote (pushing from their own local version of the project), you won’t be able to push. Instead, you will get the following message:\nTo xxx.git\n ! [rejected]        main -&gt; main (fetch first)\nerror: failed to push some refs to 'xxx.git'\nhint: Updates were rejected because the remote contains work that you do\nhint: not have locally. This is usually caused by another repository pushing\nhint: to the same ref. You may want to first integrate the remote changes\nhint: (e.g., 'git pull ...') before pushing again.\nhint: See the 'Note about fast-forwards' in 'git push --help' for details.\nThe solution?\nYou first have to download (git pull) their work onto your machine, merge it with yours (which will happen automatically if there are no conflicts), before you can push your work to GitHub.\nNow… what if there are conflicts?\n\n\nResolving conflicts\n\n\nFrom crystallize.com\n\nGit works line by line. As long as your collaborators and you aren’t working on the same line(s) of the same file(s) at the same time, there will not be any problem. If however you modified one or more of the same line(s) of the same file(s), Git will not be able to decide which version should be kept. When you git pull their work on your machine, the automatic merging will get interrupted and Git will ask you to resolve the conflict(s) before the merge can resume. It will conveniently tell you which file(s) contain the conflict(s).\nThere are fancy tools to resolve conflicts, but you can do it in any text editor: simply open the file(s) listed by Git as having conflicts and look for the following markers:\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\nThis is your version.\n=======\nThis is the alternative version of the same section of the file.\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; alternative version\nThese markers are added by Git to signal the areas of conflict. It is up to you to choose between the two versions (or create a third one) and remove the conflict markers. After that, you can stage the file(s) which contained the conflicts to finish the merge (and then you can commit).",
    "crumbs": [
      "Git",
      "<b><em>Workshops</em></b>",
      "Collaborating through Git"
    ]
  },
  {
    "objectID": "git/ws_search.html",
    "href": "git/ws_search.html",
    "title": "Searching a version-controlled project",
    "section": "",
    "text": "What is the point of creating all these commits if you are unable to make use of them because you can’t find the information you need in them?\nIn this workshop, we will learn how to search:\n\nyour files (at any of their versions) and\nyour commit logs.\n\nBy the end of the workshop, you should be able to retrieve anything you need from your versioned project."
  },
  {
    "objectID": "git/ws_search.html#installation",
    "href": "git/ws_search.html#installation",
    "title": "Searching a version-controlled project",
    "section": "Installation",
    "text": "Installation\nmacOS & Linux users:\nInstall Git from the official website.\nWindows users:\nInstall Git for Windows. This will also install “Git Bash”, a Bash emulator."
  },
  {
    "objectID": "git/ws_search.html#using-git",
    "href": "git/ws_search.html#using-git",
    "title": "Searching a version-controlled project",
    "section": "Using Git",
    "text": "Using Git\nWe will use Git from the command line throughout this workshop.\nmacOS users:    open “Terminal”.\nWindows users:   open “Git Bash”.\nLinux users:    open the terminal emulator of your choice."
  },
  {
    "objectID": "git/ws_search.html#practice-repo",
    "href": "git/ws_search.html#practice-repo",
    "title": "Searching a version-controlled project",
    "section": "Practice repo",
    "text": "Practice repo\n\nGet a repo\nYou are welcome to use a repository of yours to follow this workshop. Alternatively, you can clone a practice repo I have on GitHub:\n\nNavigate to an appropriate location:\n\ncd /path/to/appropriate/location\n\nClone the repo:\n\n# If you have set SSH for your GitHub account\ngit clone git@github.com:prosoitos/practice_repo.git\n# If you haven't set SSH\ngit clone https://github.com/prosoitos/practice_repo.git\n\nEnter the repo:\n\ncd practice_repo"
  },
  {
    "objectID": "git/ws_search.html#searching-files",
    "href": "git/ws_search.html#searching-files",
    "title": "Searching a version-controlled project",
    "section": "Searching files",
    "text": "Searching files\nThe first thing that can happen is that you are looking for a certain pattern somewhere in your project (for instance a certain function or a certain word).\n\ngit grep\nThe main command to look through versioned files is git grep.\nYou might be familiar with the command-line utility grep which allows to search for lines matching a certain pattern in files. git grep does a similar job with these differences:\n\nit is much faster since all files under version control are already indexed by Git,\nyou can easily search any commit without having to check it out,\nit has features lacking in grep such as, for instance, pattern arithmetic or tree search using globs.\n\n\n\nLet’s try it\nBy default, git grep searches recursively through the tracked files in the working directory (that is, the current version of the tracked files).\nFirst, let’s look for the word test in the current version of the tracked files in the test repo:\n\ngit grep test\n\nintro_aliases.qmd:Now, let's build an alias for a more complex command: `git grep \"test\" $(git rev-list --all)`. This example\nintro_aliases.qmd:from the *\"Searching a Git project\"* section below will search for the string \"test\" in all previous\nintro_aliases.qmd:commits. There are two problems with this command: (1) it takes an argument (the string \"test\"), and (2) it\nintro_aliases.qmd:git search test\nintro_aliases.qmd:should search the entire current Git project history for \"test\".\nintro_branches.qmd:git branch test\nintro_branches.qmd:git switch test\nintro_branches.qmd:* test\nintro_branches.qmd:The `*` shows the branch you are currently on (i.e. the branch to which `HEAD` points to). In our example, the project has two branches and we are on the branch `test`.\nintro_branches.qmd:git diff main test\nintro_branches.qmd:When you are happy with the changes you made on your test branch, you can merge it into `main`.\nintro_branches.qmd:If you have only created new commits on the branch `test`, the merge is called a \"fast-forward merge\" because `main` and `test` have not diverged: it is simply a question of having `main` catch up to `test`.\nintro_branches.qmd:git merge test\nintro_branches.qmd:Then, usually, you delete the branch `test` as it has served its purpose:\nintro_branches.qmd:git branch -d test\nintro_branches.qmd:Alternatively, you can switch back to `test` and do the next bit of experimental work on it. This allows to keep `main` free of mishaps and bad developments.\nintro_branches.qmd:Let's go back to our situation before we created the branch `test`:\nintro_branches.qmd:This time, you create a branch called `test2`:\nintro_branches.qmd:To merge your branch `test2` into `main`, a new commit is now required. Git will create this new commit automatically. As long as there is no conflict, it is just as easy as a fast-forward merge:\nintro_branches.qmd:git merge test2\nintro_branches.qmd:After which, you can delete the (now useless) test branch (with `git branch -d test2`):\nintro_branches.qmd:&gt;&gt;&gt;&gt;&gt;&gt;&gt; test2\nintro_intro_old.qmd:The pointer `HEAD`, which normally points to the branch `main` which itself points to latest commit, can be moved around. By moving `HEAD` to any commit, you can revisit the state of your project at that particular version.\nintro_intro_old.qmd:Instead of working on your branch `main`, you create a test branch and work on it (so `HEAD` is on the branch `test` and both move along as you create commits):\nintro_intro_old.qmd:When you are happy with the changes you made on your test branch, you decide to merge `main` onto it.\nintro_intro_old.qmd:Then you do the fast-forward merge from `main` onto `test` (so `main` catches up to `test`):\nintro_intro_old.qmd:Then, usually, you delete the branch `test` as it has served its purpose (with `git branch -d test`). Alternatively, you can switch back to it and do the next bit of experimental work in it.\nintro_intro_old.qmd:This allows to keep `main` free of possible mishaps and bad developments (if you aren't happy with the work you did on your test branch, you can simply delete it and Git will clean the commits that are on it but not on `main` during the next garbage collection.\nintro_intro_old.qmd:You create a test branch and switch to it:\nintro_intro_old.qmd:To merge your main branch and your test branch, a new commit is now required (note that the command is the same as in the case of a fast-forward merge: `git merge`. Git will create the new commit automatically. As long as there is no conflict, it is just as easy as a fast-forward merge. We will talk later about resolving conflicts).\nintro_intro_old.qmd:After which, you can delete the (now useless) test branch (with `git branch -d test2`):\nintro_remotes.qmd:Click on the `Code` green drop-down button, select SSH [if you have set SSH for your GitHub account](https://docs.github.com/en/free-pro-team@latest/github/authenticating-to-github/connecting-to-github-with-ssh) or HTTPS and copy the address.\nintro_revisiting_old_commits_alternate.qmd:The pointer `HEAD`, which normally points to the branch `main` which itself points to latest commit, can be moved around. By moving `HEAD` to any commit, you can revisit the state of your project at that particular version.\nintro_undo.qmd:Here is a common scenario: you make a commit, then realize that you forgot to include some changes in that commit; or you aren't happy with the commit message; or both. You can edit your latest commit with the `--amend` flag:\nws_collab.qmd:Click on the `Code` green drop-down button, select SSH [if you have set SSH for your GitHub account](https://docs.github.com/en/free-pro-team@latest/github/authenticating-to-github/connecting-to-github-with-ssh) or HTTPS and copy the address.\n\n\nLet’s add blank lines between the results of each file for better readability:\n\ngit grep --break test\n\nintro_aliases.qmd:Now, let's build an alias for a more complex command: `git grep \"test\" $(git rev-list --all)`. This example\nintro_aliases.qmd:from the *\"Searching a Git project\"* section below will search for the string \"test\" in all previous\nintro_aliases.qmd:commits. There are two problems with this command: (1) it takes an argument (the string \"test\"), and (2) it\nintro_aliases.qmd:git search test\nintro_aliases.qmd:should search the entire current Git project history for \"test\".\n\nintro_branches.qmd:git branch test\nintro_branches.qmd:git switch test\nintro_branches.qmd:* test\nintro_branches.qmd:The `*` shows the branch you are currently on (i.e. the branch to which `HEAD` points to). In our example, the project has two branches and we are on the branch `test`.\nintro_branches.qmd:git diff main test\nintro_branches.qmd:When you are happy with the changes you made on your test branch, you can merge it into `main`.\nintro_branches.qmd:If you have only created new commits on the branch `test`, the merge is called a \"fast-forward merge\" because `main` and `test` have not diverged: it is simply a question of having `main` catch up to `test`.\nintro_branches.qmd:git merge test\nintro_branches.qmd:Then, usually, you delete the branch `test` as it has served its purpose:\nintro_branches.qmd:git branch -d test\nintro_branches.qmd:Alternatively, you can switch back to `test` and do the next bit of experimental work on it. This allows to keep `main` free of mishaps and bad developments.\nintro_branches.qmd:Let's go back to our situation before we created the branch `test`:\nintro_branches.qmd:This time, you create a branch called `test2`:\nintro_branches.qmd:To merge your branch `test2` into `main`, a new commit is now required. Git will create this new commit automatically. As long as there is no conflict, it is just as easy as a fast-forward merge:\nintro_branches.qmd:git merge test2\nintro_branches.qmd:After which, you can delete the (now useless) test branch (with `git branch -d test2`):\nintro_branches.qmd:&gt;&gt;&gt;&gt;&gt;&gt;&gt; test2\n\nintro_intro_old.qmd:The pointer `HEAD`, which normally points to the branch `main` which itself points to latest commit, can be moved around. By moving `HEAD` to any commit, you can revisit the state of your project at that particular version.\nintro_intro_old.qmd:Instead of working on your branch `main`, you create a test branch and work on it (so `HEAD` is on the branch `test` and both move along as you create commits):\nintro_intro_old.qmd:When you are happy with the changes you made on your test branch, you decide to merge `main` onto it.\nintro_intro_old.qmd:Then you do the fast-forward merge from `main` onto `test` (so `main` catches up to `test`):\nintro_intro_old.qmd:Then, usually, you delete the branch `test` as it has served its purpose (with `git branch -d test`). Alternatively, you can switch back to it and do the next bit of experimental work in it.\nintro_intro_old.qmd:This allows to keep `main` free of possible mishaps and bad developments (if you aren't happy with the work you did on your test branch, you can simply delete it and Git will clean the commits that are on it but not on `main` during the next garbage collection.\nintro_intro_old.qmd:You create a test branch and switch to it:\nintro_intro_old.qmd:To merge your main branch and your test branch, a new commit is now required (note that the command is the same as in the case of a fast-forward merge: `git merge`. Git will create the new commit automatically. As long as there is no conflict, it is just as easy as a fast-forward merge. We will talk later about resolving conflicts).\nintro_intro_old.qmd:After which, you can delete the (now useless) test branch (with `git branch -d test2`):\n\nintro_remotes.qmd:Click on the `Code` green drop-down button, select SSH [if you have set SSH for your GitHub account](https://docs.github.com/en/free-pro-team@latest/github/authenticating-to-github/connecting-to-github-with-ssh) or HTTPS and copy the address.\n\nintro_revisiting_old_commits_alternate.qmd:The pointer `HEAD`, which normally points to the branch `main` which itself points to latest commit, can be moved around. By moving `HEAD` to any commit, you can revisit the state of your project at that particular version.\n\nintro_undo.qmd:Here is a common scenario: you make a commit, then realize that you forgot to include some changes in that commit; or you aren't happy with the commit message; or both. You can edit your latest commit with the `--amend` flag:\n\nws_collab.qmd:Click on the `Code` green drop-down button, select SSH [if you have set SSH for your GitHub account](https://docs.github.com/en/free-pro-team@latest/github/authenticating-to-github/connecting-to-github-with-ssh) or HTTPS and copy the address.\n\n\nLet’s also put the file names on separate lines:\n\ngit grep --break --heading test\n\nintro_aliases.qmd\nNow, let's build an alias for a more complex command: `git grep \"test\" $(git rev-list --all)`. This example\nfrom the *\"Searching a Git project\"* section below will search for the string \"test\" in all previous\ncommits. There are two problems with this command: (1) it takes an argument (the string \"test\"), and (2) it\ngit search test\nshould search the entire current Git project history for \"test\".\n\nintro_branches.qmd\ngit branch test\ngit switch test\n* test\nThe `*` shows the branch you are currently on (i.e. the branch to which `HEAD` points to). In our example, the project has two branches and we are on the branch `test`.\ngit diff main test\nWhen you are happy with the changes you made on your test branch, you can merge it into `main`.\nIf you have only created new commits on the branch `test`, the merge is called a \"fast-forward merge\" because `main` and `test` have not diverged: it is simply a question of having `main` catch up to `test`.\ngit merge test\nThen, usually, you delete the branch `test` as it has served its purpose:\ngit branch -d test\nAlternatively, you can switch back to `test` and do the next bit of experimental work on it. This allows to keep `main` free of mishaps and bad developments.\nLet's go back to our situation before we created the branch `test`:\nThis time, you create a branch called `test2`:\nTo merge your branch `test2` into `main`, a new commit is now required. Git will create this new commit automatically. As long as there is no conflict, it is just as easy as a fast-forward merge:\ngit merge test2\nAfter which, you can delete the (now useless) test branch (with `git branch -d test2`):\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; test2\n\nintro_intro_old.qmd\nThe pointer `HEAD`, which normally points to the branch `main` which itself points to latest commit, can be moved around. By moving `HEAD` to any commit, you can revisit the state of your project at that particular version.\nInstead of working on your branch `main`, you create a test branch and work on it (so `HEAD` is on the branch `test` and both move along as you create commits):\nWhen you are happy with the changes you made on your test branch, you decide to merge `main` onto it.\nThen you do the fast-forward merge from `main` onto `test` (so `main` catches up to `test`):\nThen, usually, you delete the branch `test` as it has served its purpose (with `git branch -d test`). Alternatively, you can switch back to it and do the next bit of experimental work in it.\nThis allows to keep `main` free of possible mishaps and bad developments (if you aren't happy with the work you did on your test branch, you can simply delete it and Git will clean the commits that are on it but not on `main` during the next garbage collection.\nYou create a test branch and switch to it:\nTo merge your main branch and your test branch, a new commit is now required (note that the command is the same as in the case of a fast-forward merge: `git merge`. Git will create the new commit automatically. As long as there is no conflict, it is just as easy as a fast-forward merge. We will talk later about resolving conflicts).\nAfter which, you can delete the (now useless) test branch (with `git branch -d test2`):\n\nintro_remotes.qmd\nClick on the `Code` green drop-down button, select SSH [if you have set SSH for your GitHub account](https://docs.github.com/en/free-pro-team@latest/github/authenticating-to-github/connecting-to-github-with-ssh) or HTTPS and copy the address.\n\nintro_revisiting_old_commits_alternate.qmd\nThe pointer `HEAD`, which normally points to the branch `main` which itself points to latest commit, can be moved around. By moving `HEAD` to any commit, you can revisit the state of your project at that particular version.\n\nintro_undo.qmd\nHere is a common scenario: you make a commit, then realize that you forgot to include some changes in that commit; or you aren't happy with the commit message; or both. You can edit your latest commit with the `--amend` flag:\n\nws_collab.qmd\nClick on the `Code` green drop-down button, select SSH [if you have set SSH for your GitHub account](https://docs.github.com/en/free-pro-team@latest/github/authenticating-to-github/connecting-to-github-with-ssh) or HTTPS and copy the address.\n\n\nWe can display the line numbers for the results with the -n flag:\n\ngit grep --break --heading -n test\n\nintro_aliases.qmd\n48:Now, let's build an alias for a more complex command: `git grep \"test\" $(git rev-list --all)`. This example\n49:from the *\"Searching a Git project\"* section below will search for the string \"test\" in all previous\n50:commits. There are two problems with this command: (1) it takes an argument (the string \"test\"), and (2) it\n68:git search test\n71:should search the entire current Git project history for \"test\".\n\nintro_branches.qmd\n54:git branch test\n72:git switch test\n99:* test\n102:The `*` shows the branch you are currently on (i.e. the branch to which `HEAD` points to). In our example, the project has two branches and we are on the branch `test`.\n109:git diff main test\n116:When you are happy with the changes you made on your test branch, you can merge it into `main`.\n120:If you have only created new commits on the branch `test`, the merge is called a \"fast-forward merge\" because `main` and `test` have not diverged: it is simply a question of having `main` catch up to `test`.\n135:git merge test\n140:Then, usually, you delete the branch `test` as it has served its purpose:\n143:git branch -d test\n148:Alternatively, you can switch back to `test` and do the next bit of experimental work on it. This allows to keep `main` free of mishaps and bad developments.\n154:Let's go back to our situation before we created the branch `test`:\n158:This time, you create a branch called `test2`:\n182:To merge your branch `test2` into `main`, a new commit is now required. Git will create this new commit automatically. As long as there is no conflict, it is just as easy as a fast-forward merge:\n185:git merge test2\n190:After which, you can delete the (now useless) test branch (with `git branch -d test2`):\n215:&gt;&gt;&gt;&gt;&gt;&gt;&gt; test2\n\nintro_intro_old.qmd\n904:The pointer `HEAD`, which normally points to the branch `main` which itself points to latest commit, can be moved around. By moving `HEAD` to any commit, you can revisit the state of your project at that particular version.\n1219:Instead of working on your branch `main`, you create a test branch and work on it (so `HEAD` is on the branch `test` and both move along as you create commits):\n1227:When you are happy with the changes you made on your test branch, you decide to merge `main` onto it.\n1233:Then you do the fast-forward merge from `main` onto `test` (so `main` catches up to `test`):\n1237:Then, usually, you delete the branch `test` as it has served its purpose (with `git branch -d test`). Alternatively, you can switch back to it and do the next bit of experimental work in it.\n1238:This allows to keep `main` free of possible mishaps and bad developments (if you aren't happy with the work you did on your test branch, you can simply delete it and Git will clean the commits that are on it but not on `main` during the next garbage collection.\n1250:You create a test branch and switch to it:\n1270:To merge your main branch and your test branch, a new commit is now required (note that the command is the same as in the case of a fast-forward merge: `git merge`. Git will create the new commit automatically. As long as there is no conflict, it is just as easy as a fast-forward merge. We will talk later about resolving conflicts).\n1274:After which, you can delete the (now useless) test branch (with `git branch -d test2`):\n\nintro_remotes.qmd\n45:Click on the `Code` green drop-down button, select SSH [if you have set SSH for your GitHub account](https://docs.github.com/en/free-pro-team@latest/github/authenticating-to-github/connecting-to-github-with-ssh) or HTTPS and copy the address.\n\nintro_revisiting_old_commits_alternate.qmd\n3:The pointer `HEAD`, which normally points to the branch `main` which itself points to latest commit, can be moved around. By moving `HEAD` to any commit, you can revisit the state of your project at that particular version.\n\nintro_undo.qmd\n16:Here is a common scenario: you make a commit, then realize that you forgot to include some changes in that commit; or you aren't happy with the commit message; or both. You can edit your latest commit with the `--amend` flag:\n\nws_collab.qmd\n52:Click on the `Code` green drop-down button, select SSH [if you have set SSH for your GitHub account](https://docs.github.com/en/free-pro-team@latest/github/authenticating-to-github/connecting-to-github-with-ssh) or HTTPS and copy the address.\n\n\nNotice how the results for the file src/test_manuel.py involve functions. It would be very convenient to have the names of the functions in which test appears.\nWe can do this with the -p flag:\n\ngit grep --break --heading -p test src/test_manuel.py\n\nfatal: ambiguous argument 'src/test_manuel.py': unknown revision or path not in the working tree.\nUse '--' to separate paths from revisions, like this:\n'git &lt;command&gt; [&lt;revision&gt;...] -- [&lt;file&gt;...]'\n\n\n\nWe added the argument src/test_manuel.py to limit the search to that file.\n\nWe can now see that the word test appears in the functions test and main.\nNow, instead of printing all the matching lines, let’s print the number of matches per file:\n\ngit grep -c test\n\nintro_aliases.qmd:5\nintro_branches.qmd:17\nintro_intro_old.qmd:9\nintro_remotes.qmd:1\nintro_revisiting_old_commits_alternate.qmd:1\nintro_undo.qmd:1\nws_collab.qmd:1\n\n\n\n\nMore complex patterns\ngit grep in fact searches for regular expressions. test is a regular expression matching test, but we can look for more complex patterns.\nLet’s look for image:\n\ngit grep image\n\nintro_ignore.qmd:- Non-text files (e.g. images, office documents)\n\n\n\nNo output means that the search is not returning any result.\n\nLet’s make this search case insensitive:\n\ngit grep -i image\n\nintro_ignore.qmd:- Non-text files (e.g. images, office documents)\n\n\nWe are now getting some results as Image was present in three lines of the file src/new_file.py.\nLet’s now search for data:\n\ngit grep data\n\nintro_changes.qmd:Remember that HEAD is a pointer pointing at a branch, that a branch is itself a pointer pointing at a commit, and finally that a commit is a Git object pointing at compressed blobs containing data about your project at a certain commit. When the HEAD pointer moves around, whatever commit it points to populates the [HEAD]{.emph} tree with the corresponding data.\nintro_first_steps.qmd:Alternatively, you can download the file with this button: \nintro_first_steps.qmd:data/\nintro_first_steps.qmd:data\nintro_first_steps.qmd:data\nintro_first_steps.qmd:./data:\nintro_first_steps.qmd:dataset.csv\nintro_first_steps.qmd:├── data\nintro_first_steps.qmd:│   └── dataset.csv\nintro_first_steps.qmd:This is our very exciting data set:\nintro_first_steps.qmd:cat data/dataset.csv\nintro_first_steps.qmd:df = pd.read_csv('../data/dataset.csv')\nintro_first_steps.qmd:data\nintro_first_steps.qmd:        data/\nintro_first_steps.qmd:        data/\nintro_first_steps.qmd:Remember that each commit contains the following metadata:\nintro_first_steps.qmd:        data/\nintro_ignore.qmd:- Your initial data\nintro_ignore.qmd:Notice how `data/` is not listed in the untracked files anymore.\nintro_ignore.qmd:git commit -m \"Add .gitignore file with data and results\"\nintro_ignore.qmd:[main a1df8e5] Add .gitignore file with data and results\nintro_intro_old.qmd:mkdir chapter3/src chapter3/ms chapter3/data chapter3/results\nintro_intro_old.qmd:df &lt;- data.frame(\nintro_intro_old.qmd:data\nintro_intro_old.qmd:Each commit is identified by a unique *hash* and contains these metadata:\nintro_intro_old.qmd:@@ -7,3 +7,5 @@ df &lt;- data.frame(\nintro_intro_old.qmd:@@ -7,3 +7,5 @@ df &lt;- data.frame(\nintro_intro_old.qmd:Not everything should be under version control. For instance, you don't want to put under version control non-text files or your initial data. You also shouldn't put under version control documents that can be easily recreated such as graphs and script outputs.\nintro_intro_old.qmd:echo \"/data/\nintro_intro_old.qmd:This creates a `.gitignore` file with two entries (`/data/` and `/results/`) and from now on, any file in either of these directories will be ignored by Git.\nintro_intro_old.qmd:git commit -m \"Add .gitignore file with data and results\"\nintro_intro_old.qmd:[main a1df8e5] Add .gitignore file with data and results\nintro_intro_old.qmd:    Add .gitignore file with data and results\nintro_intro_old.qmd:a1df8e5 (HEAD -&gt; main) Add .gitignore file with data and results\nintro_intro_old.qmd:|     Add .gitignore file with data and results\nintro_intro_old.qmd:* a1df8e5 88 seconds ago  (HEAD -&gt; main)Add .gitignore file with data and results xxx@xxx\nintro_intro_old.qmd:In addition to displaying the commit metadata, this also displays the difference with the previous commit.\nintro_intro_old.qmd:    Add .gitignore file with data and results\nintro_intro_old.qmd:+/data/\nintro_intro_old.qmd:@@ -7,3 +7,5 @@ df &lt;- data.frame(\nintro_intro_old.qmd:    Add .gitignore file with data and results\nintro_intro_slides.qmd:The data is stored as blobs, doesn't create unnecessary copies (unchanged files are referenced from old blobs), and uses excellent compression\nintro_intro_slides.qmd:Each commit has a unique *hash* and contains the following metadata:\nintro_logs.qmd:    Add .gitignore file with data and results\nintro_logs.qmd:c4ab5e7 Add .gitignore file with data and results\nintro_logs.qmd:|     Add .gitignore file with data and results\nintro_logs.qmd:* c4ab5e7 34 minutes ago Add .gitignore file with data and results xxx@xxx\nintro_logs.qmd:+df = pd.read_csv('../data/dataset.csv')\nintro_logs.qmd:    Add .gitignore file with data and results\nintro_logs.qmd:+/data/\nintro_logs.qmd:In addition to displaying the commit metadata, `git show` also displays the diff of that commit with its parent commit.\nintro_remotes.qmd:## Getting data from a remote\nintro_remotes.qmd:If you collaborate on a project, you have to get the data added by your teammates to keep your local project up to date.\nintro_remotes.qmd:To download new data from a remote, you have 2 options:\nintro_remotes.qmd:*Fetching* downloads the data from a remote that you don't already have in your local version of the project:\nintro_remotes.qmd:Uploading data to the remote is called *pushing*:\nintro_undo.qmd:As you just experienced, this command leads to data loss. \\\nBinary file project.zip matches\nwb_dvc.qmd:title: Version control for data science and machine learning with DVC\n\n\nWe are getting results for the word data, but also for the pattern data in longer expressions such as train_data or dataset. If we only want results for the word data, we can use the -w flag:\n\ngit grep -w data\n\nintro_changes.qmd:Remember that HEAD is a pointer pointing at a branch, that a branch is itself a pointer pointing at a commit, and finally that a commit is a Git object pointing at compressed blobs containing data about your project at a certain commit. When the HEAD pointer moves around, whatever commit it points to populates the [HEAD]{.emph} tree with the corresponding data.\nintro_first_steps.qmd:Alternatively, you can download the file with this button: \nintro_first_steps.qmd:data/\nintro_first_steps.qmd:data\nintro_first_steps.qmd:data\nintro_first_steps.qmd:./data:\nintro_first_steps.qmd:├── data\nintro_first_steps.qmd:This is our very exciting data set:\nintro_first_steps.qmd:cat data/dataset.csv\nintro_first_steps.qmd:df = pd.read_csv('../data/dataset.csv')\nintro_first_steps.qmd:data\nintro_first_steps.qmd:        data/\nintro_first_steps.qmd:        data/\nintro_first_steps.qmd:        data/\nintro_ignore.qmd:- Your initial data\nintro_ignore.qmd:Notice how `data/` is not listed in the untracked files anymore.\nintro_ignore.qmd:git commit -m \"Add .gitignore file with data and results\"\nintro_ignore.qmd:[main a1df8e5] Add .gitignore file with data and results\nintro_intro_old.qmd:mkdir chapter3/src chapter3/ms chapter3/data chapter3/results\nintro_intro_old.qmd:df &lt;- data.frame(\nintro_intro_old.qmd:data\nintro_intro_old.qmd:@@ -7,3 +7,5 @@ df &lt;- data.frame(\nintro_intro_old.qmd:@@ -7,3 +7,5 @@ df &lt;- data.frame(\nintro_intro_old.qmd:Not everything should be under version control. For instance, you don't want to put under version control non-text files or your initial data. You also shouldn't put under version control documents that can be easily recreated such as graphs and script outputs.\nintro_intro_old.qmd:echo \"/data/\nintro_intro_old.qmd:This creates a `.gitignore` file with two entries (`/data/` and `/results/`) and from now on, any file in either of these directories will be ignored by Git.\nintro_intro_old.qmd:git commit -m \"Add .gitignore file with data and results\"\nintro_intro_old.qmd:[main a1df8e5] Add .gitignore file with data and results\nintro_intro_old.qmd:    Add .gitignore file with data and results\nintro_intro_old.qmd:a1df8e5 (HEAD -&gt; main) Add .gitignore file with data and results\nintro_intro_old.qmd:|     Add .gitignore file with data and results\nintro_intro_old.qmd:* a1df8e5 88 seconds ago  (HEAD -&gt; main)Add .gitignore file with data and results xxx@xxx\nintro_intro_old.qmd:    Add .gitignore file with data and results\nintro_intro_old.qmd:+/data/\nintro_intro_old.qmd:@@ -7,3 +7,5 @@ df &lt;- data.frame(\nintro_intro_old.qmd:    Add .gitignore file with data and results\nintro_intro_slides.qmd:The data is stored as blobs, doesn't create unnecessary copies (unchanged files are referenced from old blobs), and uses excellent compression\nintro_logs.qmd:    Add .gitignore file with data and results\nintro_logs.qmd:c4ab5e7 Add .gitignore file with data and results\nintro_logs.qmd:|     Add .gitignore file with data and results\nintro_logs.qmd:* c4ab5e7 34 minutes ago Add .gitignore file with data and results xxx@xxx\nintro_logs.qmd:+df = pd.read_csv('../data/dataset.csv')\nintro_logs.qmd:    Add .gitignore file with data and results\nintro_logs.qmd:+/data/\nintro_remotes.qmd:## Getting data from a remote\nintro_remotes.qmd:If you collaborate on a project, you have to get the data added by your teammates to keep your local project up to date.\nintro_remotes.qmd:To download new data from a remote, you have 2 options:\nintro_remotes.qmd:*Fetching* downloads the data from a remote that you don't already have in your local version of the project:\nintro_remotes.qmd:Uploading data to the remote is called *pushing*:\nintro_undo.qmd:As you just experienced, this command leads to data loss. \\\nBinary file project.zip matches\nwb_dvc.qmd:title: Version control for data science and machine learning with DVC\n\n\nNow, let’s use a more complex regular expression. We want the counts for the pattern \".*_.*\" (i.e. any name with a snail case such as train_loader):\n\ngit grep -c \".*_.*\"\n\nimg/01.png:16\nimg/02.png:32\nimg/03.png:31\nimg/04.png:26\nimg/05.png:31\nimg/06.png:32\nimg/07.png:30\nimg/08.png:34\nimg/09.png:35\nimg/10.png:41\nimg/11.png:47\nimg/12.png:40\nimg/13.png:39\nimg/14.png:32\nimg/15.png:38\nimg/16.png:43\nimg/17.png:34\nimg/18.png:35\nimg/19.png:30\nimg/20.png:33\nimg/21.png:40\nimg/22.png:41\nimg/23.png:47\nimg/24.png:64\nimg/25.png:66\nimg/26.png:50\nimg/27.png:60\nimg/28.png:57\nimg/29.png:33\nimg/30.png:39\nimg/31.png:14\nimg/32.png:16\nimg/33.png:18\nimg/34.png:16\nimg/35.png:20\nimg/36.png:18\nimg/37.png:18\nimg/51.png:55\nimg/52.png:46\nimg/53.png:55\nimg/collab.jpg:178\nimg/git_graph.png:121\nimg/gitout.png:42\nimg/logo_git.png:4\nimg/vc.jpg:259\nindex.qmd:4\nintro_documentation.qmd:1\nintro_first_steps.qmd:4\nintro_install.qmd:2\nintro_intro.qmd:1\nintro_intro_old.qmd:8\nintro_intro_slides.qmd:5\nintro_logs.qmd:1\nintro_tags.qmd:5\nintro_time_travel.qmd:1\ntop_intro.qmd:2\ntop_ws.qmd:3\nwb_dvc.qmd:1\n\n\nLet’s print the first 3 results per file:\n\ngit grep -m 3 \".*_.*\"\n\nBinary file img/01.png matches\nBinary file img/02.png matches\nBinary file img/03.png matches\nBinary file img/04.png matches\nBinary file img/05.png matches\nBinary file img/06.png matches\nBinary file img/07.png matches\nBinary file img/08.png matches\nBinary file img/09.png matches\nBinary file img/10.png matches\nBinary file img/11.png matches\nBinary file img/12.png matches\nBinary file img/13.png matches\nBinary file img/14.png matches\nBinary file img/15.png matches\nBinary file img/16.png matches\nBinary file img/17.png matches\nBinary file img/18.png matches\nBinary file img/19.png matches\nBinary file img/20.png matches\nBinary file img/21.png matches\nBinary file img/22.png matches\nBinary file img/23.png matches\nBinary file img/24.png matches\nBinary file img/25.png matches\nBinary file img/26.png matches\nBinary file img/27.png matches\nBinary file img/28.png matches\nBinary file img/29.png matches\nBinary file img/30.png matches\nBinary file img/31.png matches\nBinary file img/32.png matches\nBinary file img/33.png matches\nBinary file img/34.png matches\nBinary file img/35.png matches\nBinary file img/36.png matches\nBinary file img/37.png matches\nBinary file img/51.png matches\nBinary file img/52.png matches\nBinary file img/53.png matches\nBinary file img/collab.jpg matches\nBinary file img/git_graph.png matches\nBinary file img/gitout.png matches\nBinary file img/logo_git.png matches\nBinary file img/vc.jpg matches\nindex.qmd:  Version control & collaboration with &nbsp;[![](img/logo_git.png){width=\"1.3em\" fig-alt=\"noshadow\"}](https://git-scm.com/)\nindex.qmd:[Getting started with &nbsp;![](img/logo_git.png){width=\"1.2em\" fig-alt=\"noshadow\"}](top_intro.qmd){.card-title2 .stretched-link}\nindex.qmd:[Workshops](top_ws.qmd){.card-title2 .stretched-link}\nintro_documentation.qmd:All these methods lead to the same thing: the manual page corresponding to the command will open in a pager (usually [less](https://en.wikipedia.org/wiki/Less_(Unix))). A pager is a program which makes it easier to read documents in the command line.\nintro_first_steps.qmd:  - first_steps.html\nintro_first_steps.qmd:wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1SJV5mRGexf91lNyFwdS_JmuAXX0xS4pE' -O project.zip\nintro_first_steps.qmd:df = pd.read_csv('../data/dataset.csv')\nintro_install.qmd:Git is built for Unix-like systems (Linux and MacOS). In order to use Git from the command line on Windows, you need a Unix shell such as [Bash](https://en.wikipedia.org/wiki/Bash_(Unix_shell)). To make this very easy, Git for Windows comes with its Bash emulator.\nintro_install.qmd:git config user.email \"your_other@email\"\nintro_intro.qmd:[Slides](intro_intro_slides.html){.btn .btn-outline-primary} [(Click and wait: the presentation might take a few instants to load)]{.inlinenote}\nintro_intro_old.qmd:  - intro_old.html\nintro_intro_old.qmd:&lt;script type=\"text/javascript\" src=\"https://ssl.gstatic.com/trends_nrtr/3045_RC01/embed_loader.js\"&gt;&lt;/script&gt; &lt;script type=\"text/javascript\"&gt; trends.embed.renderExploreWidget(\"TIMESERIES\", {\"comparisonItem\":[{\"keyword\":\"/m/05vqwg\",\"geo\":\"\",\"time\":\"2004-01-01 2022-10-03\"},{\"keyword\":\"/m/08441_\",\"geo\":\"\",\"time\":\"2004-01-01 2022-10-03\"},{\"keyword\":\"/m/012ct9\",\"geo\":\"\",\"time\":\"2004-01-01 2022-10-03\"},{\"keyword\":\"/m/09d6g\",\"geo\":\"\",\"time\":\"2004-01-01 2022-10-03\"}],\"category\":0,\"property\":\"\"}, {\"exploreQuery\":\"date=all&q=%2Fm%2F05vqwg,%2Fm%2F08441_,%2Fm%2F012ct9,%2Fm%2F09d6g\",\"guestPath\":\"https://trends.google.com:443/trends/embed/\"}); &lt;/script&gt;\nintro_intro_old.qmd:git config user.email \"your_other@email\"\nintro_intro_slides.qmd:  - intro_slides.html\nintro_intro_slides.qmd:frontpic: \"img/git_graph.png\"\nintro_intro_slides.qmd:    logo: /img/logo_sfudrac.png\nintro_logs.qmd:+df = pd.read_csv('../data/dataset.csv')\nintro_tags.qmd:git tag J_Climate_2009\nintro_tags.qmd:git show J_Climate_2009\nintro_tags.qmd:git checkout J_Climate_2009\nintro_time_travel.qmd:  - time_travel.html\ntop_intro.qmd:description: An introductory course to version control with &nbsp;[![](img/logo_git.png){width=\"1.3em\" fig-alt=\"noshadow\"}](https://git-scm.com/)\ntop_intro.qmd:[[Start course ➤](intro_intro.qmd)]{.topinline}\ntop_ws.qmd:[Searching a Git project](practice_repo/ws_search.qmd){.card-title-ws .stretched-link}\ntop_ws.qmd:[Collaborating through Git](ws_collab.qmd){.card-title-ws .stretched-link}\ntop_ws.qmd:[Contributing to projects](ws_contrib.qmd){.card-title-ws .stretched-link}\nwb_dvc.qmd:[As DVC is a popular tool in machine learning, **please find this webinar [in the AI section](/ai/wb_dvc.html){.stretched-link}**.]{.btn-redirect}\n\n\nAs you can see, our results also include __init__ which is not what we were looking for. So let’s exclude __:\n\ngit grep -m 3 -e \".*_.*\" --and --not -e \"__\"\n\nBinary file img/01.png matches\nBinary file img/02.png matches\nBinary file img/03.png matches\nBinary file img/04.png matches\nBinary file img/05.png matches\nBinary file img/06.png matches\nBinary file img/07.png matches\nBinary file img/08.png matches\nBinary file img/09.png matches\nBinary file img/10.png matches\nBinary file img/11.png matches\nBinary file img/12.png matches\nBinary file img/13.png matches\nBinary file img/14.png matches\nBinary file img/15.png matches\nBinary file img/16.png matches\nBinary file img/17.png matches\nBinary file img/18.png matches\nBinary file img/19.png matches\nBinary file img/20.png matches\nBinary file img/21.png matches\nBinary file img/22.png matches\nBinary file img/23.png matches\nBinary file img/24.png matches\nBinary file img/25.png matches\nBinary file img/26.png matches\nBinary file img/27.png matches\nBinary file img/28.png matches\nBinary file img/29.png matches\nBinary file img/30.png matches\nBinary file img/31.png matches\nBinary file img/32.png matches\nBinary file img/33.png matches\nBinary file img/34.png matches\nBinary file img/35.png matches\nBinary file img/36.png matches\nBinary file img/37.png matches\nBinary file img/51.png matches\nBinary file img/52.png matches\nBinary file img/53.png matches\nBinary file img/collab.jpg matches\nBinary file img/git_graph.png matches\nBinary file img/gitout.png matches\nBinary file img/logo_git.png matches\nBinary file img/vc.jpg matches\nindex.qmd:  Version control & collaboration with &nbsp;[![](img/logo_git.png){width=\"1.3em\" fig-alt=\"noshadow\"}](https://git-scm.com/)\nindex.qmd:[Getting started with &nbsp;![](img/logo_git.png){width=\"1.2em\" fig-alt=\"noshadow\"}](top_intro.qmd){.card-title2 .stretched-link}\nindex.qmd:[Workshops](top_ws.qmd){.card-title2 .stretched-link}\nintro_documentation.qmd:All these methods lead to the same thing: the manual page corresponding to the command will open in a pager (usually [less](https://en.wikipedia.org/wiki/Less_(Unix))). A pager is a program which makes it easier to read documents in the command line.\nintro_first_steps.qmd:  - first_steps.html\nintro_first_steps.qmd:wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1SJV5mRGexf91lNyFwdS_JmuAXX0xS4pE' -O project.zip\nintro_first_steps.qmd:df = pd.read_csv('../data/dataset.csv')\nintro_install.qmd:Git is built for Unix-like systems (Linux and MacOS). In order to use Git from the command line on Windows, you need a Unix shell such as [Bash](https://en.wikipedia.org/wiki/Bash_(Unix_shell)). To make this very easy, Git for Windows comes with its Bash emulator.\nintro_install.qmd:git config user.email \"your_other@email\"\nintro_intro.qmd:[Slides](intro_intro_slides.html){.btn .btn-outline-primary} [(Click and wait: the presentation might take a few instants to load)]{.inlinenote}\nintro_intro_old.qmd:  - intro_old.html\nintro_intro_old.qmd:&lt;script type=\"text/javascript\" src=\"https://ssl.gstatic.com/trends_nrtr/3045_RC01/embed_loader.js\"&gt;&lt;/script&gt; &lt;script type=\"text/javascript\"&gt; trends.embed.renderExploreWidget(\"TIMESERIES\", {\"comparisonItem\":[{\"keyword\":\"/m/05vqwg\",\"geo\":\"\",\"time\":\"2004-01-01 2022-10-03\"},{\"keyword\":\"/m/08441_\",\"geo\":\"\",\"time\":\"2004-01-01 2022-10-03\"},{\"keyword\":\"/m/012ct9\",\"geo\":\"\",\"time\":\"2004-01-01 2022-10-03\"},{\"keyword\":\"/m/09d6g\",\"geo\":\"\",\"time\":\"2004-01-01 2022-10-03\"}],\"category\":0,\"property\":\"\"}, {\"exploreQuery\":\"date=all&q=%2Fm%2F05vqwg,%2Fm%2F08441_,%2Fm%2F012ct9,%2Fm%2F09d6g\",\"guestPath\":\"https://trends.google.com:443/trends/embed/\"}); &lt;/script&gt;\nintro_intro_old.qmd:git config user.email \"your_other@email\"\nintro_intro_slides.qmd:  - intro_slides.html\nintro_intro_slides.qmd:frontpic: \"img/git_graph.png\"\nintro_intro_slides.qmd:    logo: /img/logo_sfudrac.png\nintro_logs.qmd:+df = pd.read_csv('../data/dataset.csv')\nintro_tags.qmd:git tag J_Climate_2009\nintro_tags.qmd:git show J_Climate_2009\nintro_tags.qmd:git checkout J_Climate_2009\nintro_time_travel.qmd:  - time_travel.html\ntop_intro.qmd:description: An introductory course to version control with &nbsp;[![](img/logo_git.png){width=\"1.3em\" fig-alt=\"noshadow\"}](https://git-scm.com/)\ntop_intro.qmd:[[Start course ➤](intro_intro.qmd)]{.topinline}\ntop_ws.qmd:[Searching a Git project](practice_repo/ws_search.qmd){.card-title-ws .stretched-link}\ntop_ws.qmd:[Collaborating through Git](ws_collab.qmd){.card-title-ws .stretched-link}\ntop_ws.qmd:[Contributing to projects](ws_contrib.qmd){.card-title-ws .stretched-link}\nwb_dvc.qmd:[As DVC is a popular tool in machine learning, **please find this webinar [in the AI section](/ai/wb_dvc.html){.stretched-link}**.]{.btn-redirect}\n\n\n\nFor simple searches, you don’t have to use the -e flag before the pattern you are searching for. Here however, our command has gotten complex enough that we have to use it before each pattern.\n\nLet’s make sure this worked as expected:\n\ngit grep -c \".*_.*\"\necho \"---\"\ngit grep -c \"__\"\necho \"---\"\ngit grep -ce \".*_.*\" --and --not -e \"__\"\n\nimg/01.png:16\nimg/02.png:32\nimg/03.png:31\nimg/04.png:26\nimg/05.png:31\nimg/06.png:32\nimg/07.png:30\nimg/08.png:34\nimg/09.png:35\nimg/10.png:41\nimg/11.png:47\nimg/12.png:40\nimg/13.png:39\nimg/14.png:32\nimg/15.png:38\nimg/16.png:43\nimg/17.png:34\nimg/18.png:35\nimg/19.png:30\nimg/20.png:33\nimg/21.png:40\nimg/22.png:41\nimg/23.png:47\nimg/24.png:64\nimg/25.png:66\nimg/26.png:50\nimg/27.png:60\nimg/28.png:57\nimg/29.png:33\nimg/30.png:39\nimg/31.png:14\nimg/32.png:16\nimg/33.png:18\nimg/34.png:16\nimg/35.png:20\nimg/36.png:18\nimg/37.png:18\nimg/51.png:55\nimg/52.png:46\nimg/53.png:55\nimg/collab.jpg:178\nimg/git_graph.png:121\nimg/gitout.png:42\nimg/logo_git.png:4\nimg/vc.jpg:259\nindex.qmd:4\nintro_documentation.qmd:1\nintro_first_steps.qmd:4\nintro_install.qmd:2\nintro_intro.qmd:1\nintro_intro_old.qmd:8\nintro_intro_slides.qmd:5\nintro_logs.qmd:1\nintro_tags.qmd:5\nintro_time_travel.qmd:1\ntop_intro.qmd:2\ntop_ws.qmd:3\nwb_dvc.qmd:1\n---\nimg/01.png:1\nimg/02.png:2\nimg/03.png:2\nimg/04.png:1\nimg/05.png:3\nimg/06.png:3\nimg/07.png:1\nimg/08.png:1\nimg/09.png:1\nimg/10.png:1\nimg/11.png:1\nimg/12.png:1\nimg/13.png:2\nimg/14.png:2\nimg/15.png:3\nimg/16.png:1\nimg/17.png:1\nimg/18.png:2\nimg/19.png:1\nimg/20.png:1\nimg/21.png:1\nimg/22.png:2\nimg/23.png:4\nimg/24.png:2\nimg/25.png:1\nimg/26.png:1\nimg/27.png:2\nimg/28.png:3\nimg/29.png:2\nimg/30.png:1\nimg/31.png:1\nimg/51.png:1\nimg/52.png:2\nimg/53.png:1\nimg/collab.jpg:1\nimg/git_graph.png:2\nimg/gitout.png:1\n---\nimg/01.png:15\nimg/02.png:30\nimg/03.png:29\nimg/04.png:25\nimg/05.png:28\nimg/06.png:29\nimg/07.png:29\nimg/08.png:33\nimg/09.png:34\nimg/10.png:40\nimg/11.png:46\nimg/12.png:39\nimg/13.png:37\nimg/14.png:30\nimg/15.png:35\nimg/16.png:42\nimg/17.png:33\nimg/18.png:33\nimg/19.png:29\nimg/20.png:32\nimg/21.png:39\nimg/22.png:39\nimg/23.png:43\nimg/24.png:62\nimg/25.png:65\nimg/26.png:49\nimg/27.png:58\nimg/28.png:54\nimg/29.png:31\nimg/30.png:38\nimg/31.png:13\nimg/32.png:16\nimg/33.png:18\nimg/34.png:16\nimg/35.png:20\nimg/36.png:18\nimg/37.png:18\nimg/51.png:54\nimg/52.png:44\nimg/53.png:54\nimg/collab.jpg:177\nimg/git_graph.png:119\nimg/gitout.png:41\nimg/logo_git.png:4\nimg/vc.jpg:259\nindex.qmd:4\nintro_documentation.qmd:1\nintro_first_steps.qmd:4\nintro_install.qmd:2\nintro_intro.qmd:1\nintro_intro_old.qmd:8\nintro_intro_slides.qmd:5\nintro_logs.qmd:1\nintro_tags.qmd:5\nintro_time_travel.qmd:1\ntop_intro.qmd:2\ntop_ws.qmd:3\nwb_dvc.qmd:1\n\n\nThere were 2 lines matching __ in src/test_manuel.py and we have indeed excluded them from our search.\nExtended regular expressions are also covered with the flag -E.\n\n\nSearching other trees\nSo far, we have searched the current version of tracked files, but we can just as easily search files at any commit.\nLet’s search for test in the tracked files 20 commits ago:\n\ngit grep test HEAD~20\n\nHEAD~20:intro_aliases.qmd:Now, let's build an alias for a more complex command: `git grep \"test\" $(git rev-list --all)`. This example\nHEAD~20:intro_aliases.qmd:from the *\"Searching a Git project\"* section below will search for the string \"test\" in all previous\nHEAD~20:intro_aliases.qmd:commits. There are two problems with this command: (1) it takes an argument (the string \"test\"), and (2) it\nHEAD~20:intro_aliases.qmd:git search test\nHEAD~20:intro_aliases.qmd:should search the entire current Git project history for \"test\".\nHEAD~20:intro_branches.qmd:git branch test\nHEAD~20:intro_branches.qmd:git switch test\nHEAD~20:intro_branches.qmd:* test\nHEAD~20:intro_branches.qmd:The `*` shows the branch you are currently on (i.e. the branch to which `HEAD` points to). In our example, the project has two branches and we are on the branch `test`.\nHEAD~20:intro_branches.qmd:git diff main test\nHEAD~20:intro_branches.qmd:When you are happy with the changes you made on your test branch, you can merge it into `main`.\nHEAD~20:intro_branches.qmd:If you have only created new commits on the branch `test`, the merge is called a \"fast-forward merge\" because `main` and `test` have not diverged: it is simply a question of having `main` catch up to `test`.\nHEAD~20:intro_branches.qmd:git merge test\nHEAD~20:intro_branches.qmd:Then, usually, you delete the branch `test` as it has served its purpose:\nHEAD~20:intro_branches.qmd:git branch -d test\nHEAD~20:intro_branches.qmd:Alternatively, you can switch back to `test` and do the next bit of experimental work on it. This allows to keep `main` free of mishaps and bad developments.\nHEAD~20:intro_branches.qmd:Let's go back to our situation before we created the branch `test`:\nHEAD~20:intro_branches.qmd:This time, you create a branch called `test2`:\nHEAD~20:intro_branches.qmd:To merge your branch `test2` into `main`, a new commit is now required. Git will create this new commit automatically. As long as there is no conflict, it is just as easy as a fast-forward merge:\nHEAD~20:intro_branches.qmd:git merge test2\nHEAD~20:intro_branches.qmd:After which, you can delete the (now useless) test branch (with `git branch -d test2`):\nHEAD~20:intro_branches.qmd:&gt;&gt;&gt;&gt;&gt;&gt;&gt; test2\nHEAD~20:intro_intro_old.qmd:The pointer `HEAD`, which normally points to the branch `main` which itself points to latest commit, can be moved around. By moving `HEAD` to any commit, you can revisit the state of your project at that particular version.\nHEAD~20:intro_intro_old.qmd:Instead of working on your branch `main`, you create a test branch and work on it (so `HEAD` is on the branch `test` and both move along as you create commits):\nHEAD~20:intro_intro_old.qmd:When you are happy with the changes you made on your test branch, you decide to merge `main` onto it.\nHEAD~20:intro_intro_old.qmd:Then you do the fast-forward merge from `main` onto `test` (so `main` catches up to `test`):\nHEAD~20:intro_intro_old.qmd:Then, usually, you delete the branch `test` as it has served its purpose (with `git branch -d test`). Alternatively, you can switch back to it and do the next bit of experimental work in it.\nHEAD~20:intro_intro_old.qmd:This allows to keep `main` free of possible mishaps and bad developments (if you aren't happy with the work you did on your test branch, you can simply delete it and Git will clean the commits that are on it but not on `main` during the next garbage collection.\nHEAD~20:intro_intro_old.qmd:You create a test branch and switch to it:\nHEAD~20:intro_intro_old.qmd:To merge your main branch and your test branch, a new commit is now required (note that the command is the same as in the case of a fast-forward merge: `git merge`. Git will create the new commit automatically. As long as there is no conflict, it is just as easy as a fast-forward merge. We will talk later about resolving conflicts).\nHEAD~20:intro_intro_old.qmd:After which, you can delete the (now useless) test branch (with `git branch -d test2`):\nHEAD~20:intro_remotes.qmd:Click on the `Code` green drop-down button, select SSH [if you have set SSH for your GitHub account](https://docs.github.com/en/free-pro-team@latest/github/authenticating-to-github/connecting-to-github-with-ssh) or HTTPS and copy the address.\nHEAD~20:intro_revisiting_old_commits_alternate.qmd:The pointer `HEAD`, which normally points to the branch `main` which itself points to latest commit, can be moved around. By moving `HEAD` to any commit, you can revisit the state of your project at that particular version.\nHEAD~20:intro_undo.qmd:Here is a common scenario: you make a commit, then realize that you forgot to include some changes in that commit; or you aren't happy with the commit message; or both. You can edit your latest commit with the `--amend` flag:\nHEAD~20:ws_collab.qmd:Click on the `Code` green drop-down button, select SSH [if you have set SSH for your GitHub account](https://docs.github.com/en/free-pro-team@latest/github/authenticating-to-github/connecting-to-github-with-ssh) or HTTPS and copy the address.\n\n\n\nAs you can see, the file src/test_manuel.py is not in the results. Either it didn’t exist or it didn’t have the word test at that commit.\n\nIf you want to search tracked files AND untracked files, you need to use the --untracked flag.\nLet’s create a new (thus untracked) file with some content including the word test:\n\necho \"This is a test\" &gt; newfile\n\nNow compare the following:\n\ngit grep -c test\n\nintro_aliases.qmd:5\nintro_branches.qmd:17\nintro_intro_old.qmd:9\nintro_remotes.qmd:1\nintro_revisiting_old_commits_alternate.qmd:1\nintro_undo.qmd:1\nws_collab.qmd:1\n\n\nwith:\n\ngit grep -c --untracked test\n\nindex.html:1\nintro_aliases.html:4\nintro_aliases.qmd:5\nintro_branches.html:18\nintro_branches.qmd:17\nintro_changes.html:1\nintro_documentation.html:1\nintro_first_steps.html:1\nintro_ignore.html:1\nintro_install.html:1\nintro_intro.html:1\nintro_intro_old.qmd:9\nintro_intro_slides.html:18\nintro_logs.html:1\nintro_remotes.html:2\nintro_remotes.qmd:1\nintro_reset.html:1\nintro_resources.html:1\nintro_revisiting_old_commits_alternate.html:2\nintro_revisiting_old_commits_alternate.qmd:1\nintro_stash.html:1\nintro_tags.html:1\nintro_three_trees.html:1\nintro_time_travel.html:1\nintro_tools.html:1\nintro_undo.html:2\nintro_undo.qmd:1\nnewfile:1\ntop_intro.html:1\ntop_ws.html:1\nwb_dvc.html:1\nws_collab.html:2\nws_collab.qmd:1\nws_contrib.html:1\nws_search.rmarkdown:41\n\n\n\nThis last result also returned our untracked file newfile.\n\nIf you want to search untracked and ignored files (meaning all your files), use the flags --untracked --no-exclude-standard.\nLet’s see what the .gitignore file contains:\n\ncat .gitignore\n\ncat: .gitignore: No such file or directory\n\n\nThe directory data is in .gitignore. This means that it is not under version control and it thus doesn’t exist in our repo (since we cloned our repo, we only have the version-controlled files). Let’s create it:\nmkdir data\nNow, let’s create a file in it that contains test:\n\necho \"And another test\" &gt; data/file\n\nbash: line 1: data/file: No such file or directory\n\n\nWe can rerun our previous two searches to verify that files excluded from version control are not searched:\n\ngit grep -c test\n\nintro_aliases.qmd:5\nintro_branches.qmd:17\nintro_intro_old.qmd:9\nintro_remotes.qmd:1\nintro_revisiting_old_commits_alternate.qmd:1\nintro_undo.qmd:1\nws_collab.qmd:1\n\n\n\ngit grep -c --untracked test\n\nindex.html:1\nintro_aliases.html:4\nintro_aliases.qmd:5\nintro_branches.html:18\nintro_branches.qmd:17\nintro_changes.html:1\nintro_documentation.html:1\nintro_first_steps.html:1\nintro_ignore.html:1\nintro_install.html:1\nintro_intro.html:1\nintro_intro_old.qmd:9\nintro_intro_slides.html:18\nintro_logs.html:1\nintro_remotes.html:2\nintro_remotes.qmd:1\nintro_reset.html:1\nintro_resources.html:1\nintro_revisiting_old_commits_alternate.html:2\nintro_revisiting_old_commits_alternate.qmd:1\nintro_stash.html:1\nintro_tags.html:1\nintro_three_trees.html:1\nintro_time_travel.html:1\nintro_tools.html:1\nintro_undo.html:2\nintro_undo.qmd:1\nnewfile:1\ntop_intro.html:1\ntop_ws.html:1\nwb_dvc.html:1\nws_collab.html:2\nws_collab.qmd:1\nws_contrib.html:1\nws_search.rmarkdown:41\n\n\nAnd now, let’s try:\n\ngit grep -c --untracked --no-exclude-standard test\n\nindex.html:1\nintro_aliases.html:4\nintro_aliases.qmd:5\nintro_branches.html:18\nintro_branches.qmd:17\nintro_changes.html:1\nintro_documentation.html:1\nintro_first_steps.html:1\nintro_ignore.html:1\nintro_install.html:1\nintro_intro.html:1\nintro_intro_old.qmd:9\nintro_intro_slides.html:18\nintro_logs.html:1\nintro_remotes.html:2\nintro_remotes.qmd:1\nintro_reset.html:1\nintro_resources.html:1\nintro_revisiting_old_commits_alternate.html:2\nintro_revisiting_old_commits_alternate.qmd:1\nintro_stash.html:1\nintro_tags.html:1\nintro_three_trees.html:1\nintro_time_travel.html:1\nintro_tools.html:1\nintro_undo.html:2\nintro_undo.qmd:1\nnewfile:1\ntop_intro.html:1\ntop_ws.html:1\nwb_dvc.html:1\nws_collab.html:2\nws_collab.qmd:1\nws_contrib.html:1\nws_search.rmarkdown:41\n\n\n\ndata/file, despite being excluded from version control, is also searched.\n\n\n\nSearching all commits\nWe saw that git grep &lt;pattern&gt; &lt;commit&gt; can search a pattern in any commit. Now, what if we all to search all commits for a pattern?\nFor this, we pass the expression $(git rev-list --all) in lieu of &lt;commit&gt;.\ngit rev-list --all creates a list of all the commits in a way that can be used as an argument to other functions. The $() allows to run the expression inside it and pass the result as and argument.\nTo search for test in all the commits, we thus run:\ngit grep \"test\" $(git rev-list --all)\nI am not running this command has it has a huge output. Instead, I will limit the search to the last two commits:\n\ngit grep \"test\" $(git rev-list --all -2)\n\nf1802fb9273fdbaad5fa0f1381ff8b18a84a15ce:intro_aliases.qmd:Now, let's build an alias for a more complex command: `git grep \"test\" $(git rev-list --all)`. This example\nf1802fb9273fdbaad5fa0f1381ff8b18a84a15ce:intro_aliases.qmd:from the *\"Searching a Git project\"* section below will search for the string \"test\" in all previous\nf1802fb9273fdbaad5fa0f1381ff8b18a84a15ce:intro_aliases.qmd:commits. There are two problems with this command: (1) it takes an argument (the string \"test\"), and (2) it\nf1802fb9273fdbaad5fa0f1381ff8b18a84a15ce:intro_aliases.qmd:git search test\nf1802fb9273fdbaad5fa0f1381ff8b18a84a15ce:intro_aliases.qmd:should search the entire current Git project history for \"test\".\nf1802fb9273fdbaad5fa0f1381ff8b18a84a15ce:intro_branches.qmd:git branch test\nf1802fb9273fdbaad5fa0f1381ff8b18a84a15ce:intro_branches.qmd:git switch test\nf1802fb9273fdbaad5fa0f1381ff8b18a84a15ce:intro_branches.qmd:* test\nf1802fb9273fdbaad5fa0f1381ff8b18a84a15ce:intro_branches.qmd:The `*` shows the branch you are currently on (i.e. the branch to which `HEAD` points to). In our example, the project has two branches and we are on the branch `test`.\nf1802fb9273fdbaad5fa0f1381ff8b18a84a15ce:intro_branches.qmd:git diff main test\nf1802fb9273fdbaad5fa0f1381ff8b18a84a15ce:intro_branches.qmd:When you are happy with the changes you made on your test branch, you can merge it into `main`.\nf1802fb9273fdbaad5fa0f1381ff8b18a84a15ce:intro_branches.qmd:If you have only created new commits on the branch `test`, the merge is called a \"fast-forward merge\" because `main` and `test` have not diverged: it is simply a question of having `main` catch up to `test`.\nf1802fb9273fdbaad5fa0f1381ff8b18a84a15ce:intro_branches.qmd:git merge test\nf1802fb9273fdbaad5fa0f1381ff8b18a84a15ce:intro_branches.qmd:Then, usually, you delete the branch `test` as it has served its purpose:\nf1802fb9273fdbaad5fa0f1381ff8b18a84a15ce:intro_branches.qmd:git branch -d test\nf1802fb9273fdbaad5fa0f1381ff8b18a84a15ce:intro_branches.qmd:Alternatively, you can switch back to `test` and do the next bit of experimental work on it. This allows to keep `main` free of mishaps and bad developments.\nf1802fb9273fdbaad5fa0f1381ff8b18a84a15ce:intro_branches.qmd:Let's go back to our situation before we created the branch `test`:\nf1802fb9273fdbaad5fa0f1381ff8b18a84a15ce:intro_branches.qmd:This time, you create a branch called `test2`:\nf1802fb9273fdbaad5fa0f1381ff8b18a84a15ce:intro_branches.qmd:To merge your branch `test2` into `main`, a new commit is now required. Git will create this new commit automatically. As long as there is no conflict, it is just as easy as a fast-forward merge:\nf1802fb9273fdbaad5fa0f1381ff8b18a84a15ce:intro_branches.qmd:git merge test2\nf1802fb9273fdbaad5fa0f1381ff8b18a84a15ce:intro_branches.qmd:After which, you can delete the (now useless) test branch (with `git branch -d test2`):\nf1802fb9273fdbaad5fa0f1381ff8b18a84a15ce:intro_branches.qmd:&gt;&gt;&gt;&gt;&gt;&gt;&gt; test2\nf1802fb9273fdbaad5fa0f1381ff8b18a84a15ce:intro_intro_old.qmd:The pointer `HEAD`, which normally points to the branch `main` which itself points to latest commit, can be moved around. By moving `HEAD` to any commit, you can revisit the state of your project at that particular version.\nf1802fb9273fdbaad5fa0f1381ff8b18a84a15ce:intro_intro_old.qmd:Instead of working on your branch `main`, you create a test branch and work on it (so `HEAD` is on the branch `test` and both move along as you create commits):\nf1802fb9273fdbaad5fa0f1381ff8b18a84a15ce:intro_intro_old.qmd:When you are happy with the changes you made on your test branch, you decide to merge `main` onto it.\nf1802fb9273fdbaad5fa0f1381ff8b18a84a15ce:intro_intro_old.qmd:Then you do the fast-forward merge from `main` onto `test` (so `main` catches up to `test`):\nf1802fb9273fdbaad5fa0f1381ff8b18a84a15ce:intro_intro_old.qmd:Then, usually, you delete the branch `test` as it has served its purpose (with `git branch -d test`). Alternatively, you can switch back to it and do the next bit of experimental work in it.\nf1802fb9273fdbaad5fa0f1381ff8b18a84a15ce:intro_intro_old.qmd:This allows to keep `main` free of possible mishaps and bad developments (if you aren't happy with the work you did on your test branch, you can simply delete it and Git will clean the commits that are on it but not on `main` during the next garbage collection.\nf1802fb9273fdbaad5fa0f1381ff8b18a84a15ce:intro_intro_old.qmd:You create a test branch and switch to it:\nf1802fb9273fdbaad5fa0f1381ff8b18a84a15ce:intro_intro_old.qmd:To merge your main branch and your test branch, a new commit is now required (note that the command is the same as in the case of a fast-forward merge: `git merge`. Git will create the new commit automatically. As long as there is no conflict, it is just as easy as a fast-forward merge. We will talk later about resolving conflicts).\nf1802fb9273fdbaad5fa0f1381ff8b18a84a15ce:intro_intro_old.qmd:After which, you can delete the (now useless) test branch (with `git branch -d test2`):\nf1802fb9273fdbaad5fa0f1381ff8b18a84a15ce:intro_remotes.qmd:Click on the `Code` green drop-down button, select SSH [if you have set SSH for your GitHub account](https://docs.github.com/en/free-pro-team@latest/github/authenticating-to-github/connecting-to-github-with-ssh) or HTTPS and copy the address.\nf1802fb9273fdbaad5fa0f1381ff8b18a84a15ce:intro_revisiting_old_commits_alternate.qmd:The pointer `HEAD`, which normally points to the branch `main` which itself points to latest commit, can be moved around. By moving `HEAD` to any commit, you can revisit the state of your project at that particular version.\nf1802fb9273fdbaad5fa0f1381ff8b18a84a15ce:intro_undo.qmd:Here is a common scenario: you make a commit, then realize that you forgot to include some changes in that commit; or you aren't happy with the commit message; or both. You can edit your latest commit with the `--amend` flag:\nf1802fb9273fdbaad5fa0f1381ff8b18a84a15ce:ws_collab.qmd:Click on the `Code` green drop-down button, select SSH [if you have set SSH for your GitHub account](https://docs.github.com/en/free-pro-team@latest/github/authenticating-to-github/connecting-to-github-with-ssh) or HTTPS and copy the address.\n397ef976e18724c06713ffbf7ebe205b7016a35f:intro_aliases.qmd:Now, let's build an alias for a more complex command: `git grep \"test\" $(git rev-list --all)`. This example\n397ef976e18724c06713ffbf7ebe205b7016a35f:intro_aliases.qmd:from the *\"Searching a Git project\"* section below will search for the string \"test\" in all previous\n397ef976e18724c06713ffbf7ebe205b7016a35f:intro_aliases.qmd:commits. There are two problems with this command: (1) it takes an argument (the string \"test\"), and (2) it\n397ef976e18724c06713ffbf7ebe205b7016a35f:intro_aliases.qmd:git search test\n397ef976e18724c06713ffbf7ebe205b7016a35f:intro_aliases.qmd:should search the entire current Git project history for \"test\".\n397ef976e18724c06713ffbf7ebe205b7016a35f:intro_branches.qmd:git branch test\n397ef976e18724c06713ffbf7ebe205b7016a35f:intro_branches.qmd:git switch test\n397ef976e18724c06713ffbf7ebe205b7016a35f:intro_branches.qmd:* test\n397ef976e18724c06713ffbf7ebe205b7016a35f:intro_branches.qmd:The `*` shows the branch you are currently on (i.e. the branch to which `HEAD` points to). In our example, the project has two branches and we are on the branch `test`.\n397ef976e18724c06713ffbf7ebe205b7016a35f:intro_branches.qmd:git diff main test\n397ef976e18724c06713ffbf7ebe205b7016a35f:intro_branches.qmd:When you are happy with the changes you made on your test branch, you can merge it into `main`.\n397ef976e18724c06713ffbf7ebe205b7016a35f:intro_branches.qmd:If you have only created new commits on the branch `test`, the merge is called a \"fast-forward merge\" because `main` and `test` have not diverged: it is simply a question of having `main` catch up to `test`.\n397ef976e18724c06713ffbf7ebe205b7016a35f:intro_branches.qmd:git merge test\n397ef976e18724c06713ffbf7ebe205b7016a35f:intro_branches.qmd:Then, usually, you delete the branch `test` as it has served its purpose:\n397ef976e18724c06713ffbf7ebe205b7016a35f:intro_branches.qmd:git branch -d test\n397ef976e18724c06713ffbf7ebe205b7016a35f:intro_branches.qmd:Alternatively, you can switch back to `test` and do the next bit of experimental work on it. This allows to keep `main` free of mishaps and bad developments.\n397ef976e18724c06713ffbf7ebe205b7016a35f:intro_branches.qmd:Let's go back to our situation before we created the branch `test`:\n397ef976e18724c06713ffbf7ebe205b7016a35f:intro_branches.qmd:This time, you create a branch called `test2`:\n397ef976e18724c06713ffbf7ebe205b7016a35f:intro_branches.qmd:To merge your branch `test2` into `main`, a new commit is now required. Git will create this new commit automatically. As long as there is no conflict, it is just as easy as a fast-forward merge:\n397ef976e18724c06713ffbf7ebe205b7016a35f:intro_branches.qmd:git merge test2\n397ef976e18724c06713ffbf7ebe205b7016a35f:intro_branches.qmd:After which, you can delete the (now useless) test branch (with `git branch -d test2`):\n397ef976e18724c06713ffbf7ebe205b7016a35f:intro_branches.qmd:&gt;&gt;&gt;&gt;&gt;&gt;&gt; test2\n397ef976e18724c06713ffbf7ebe205b7016a35f:intro_intro_old.qmd:The pointer `HEAD`, which normally points to the branch `main` which itself points to latest commit, can be moved around. By moving `HEAD` to any commit, you can revisit the state of your project at that particular version.\n397ef976e18724c06713ffbf7ebe205b7016a35f:intro_intro_old.qmd:Instead of working on your branch `main`, you create a test branch and work on it (so `HEAD` is on the branch `test` and both move along as you create commits):\n397ef976e18724c06713ffbf7ebe205b7016a35f:intro_intro_old.qmd:When you are happy with the changes you made on your test branch, you decide to merge `main` onto it.\n397ef976e18724c06713ffbf7ebe205b7016a35f:intro_intro_old.qmd:Then you do the fast-forward merge from `main` onto `test` (so `main` catches up to `test`):\n397ef976e18724c06713ffbf7ebe205b7016a35f:intro_intro_old.qmd:Then, usually, you delete the branch `test` as it has served its purpose (with `git branch -d test`). Alternatively, you can switch back to it and do the next bit of experimental work in it.\n397ef976e18724c06713ffbf7ebe205b7016a35f:intro_intro_old.qmd:This allows to keep `main` free of possible mishaps and bad developments (if you aren't happy with the work you did on your test branch, you can simply delete it and Git will clean the commits that are on it but not on `main` during the next garbage collection.\n397ef976e18724c06713ffbf7ebe205b7016a35f:intro_intro_old.qmd:You create a test branch and switch to it:\n397ef976e18724c06713ffbf7ebe205b7016a35f:intro_intro_old.qmd:To merge your main branch and your test branch, a new commit is now required (note that the command is the same as in the case of a fast-forward merge: `git merge`. Git will create the new commit automatically. As long as there is no conflict, it is just as easy as a fast-forward merge. We will talk later about resolving conflicts).\n397ef976e18724c06713ffbf7ebe205b7016a35f:intro_intro_old.qmd:After which, you can delete the (now useless) test branch (with `git branch -d test2`):\n397ef976e18724c06713ffbf7ebe205b7016a35f:intro_remotes.qmd:Click on the `Code` green drop-down button, select SSH [if you have set SSH for your GitHub account](https://docs.github.com/en/free-pro-team@latest/github/authenticating-to-github/connecting-to-github-with-ssh) or HTTPS and copy the address.\n397ef976e18724c06713ffbf7ebe205b7016a35f:intro_revisiting_old_commits_alternate.qmd:The pointer `HEAD`, which normally points to the branch `main` which itself points to latest commit, can be moved around. By moving `HEAD` to any commit, you can revisit the state of your project at that particular version.\n397ef976e18724c06713ffbf7ebe205b7016a35f:intro_undo.qmd:Here is a common scenario: you make a commit, then realize that you forgot to include some changes in that commit; or you aren't happy with the commit message; or both. You can edit your latest commit with the `--amend` flag:\n397ef976e18724c06713ffbf7ebe205b7016a35f:ws_collab.qmd:Click on the `Code` green drop-down button, select SSH [if you have set SSH for your GitHub account](https://docs.github.com/en/free-pro-team@latest/github/authenticating-to-github/connecting-to-github-with-ssh) or HTTPS and copy the address.\n\n\n\nIn combination with the fuzzy finder tool fzf, this can make finding a particular commit extremely easy.\nFor instance, the code below allows you to dynamically search in the result through incremental completion:\ngit grep \"test\" $(git rev-list --all) | fzf --cycle -i -e\nOr even better, you can automatically copy the short form of the hash of the selected commit to clipboard so that you can use it with git show, git checkout, etc.:\ngit grep \"test\" $(git rev-list --all) |\n    fzf --cycle -i -e |\n    cut -c 1-7 |\n    xclip -r -selection clipboard\n\nHere, I am using xclip to copy to the clipboard as I am on Linux. Depending on your OS you might need to use a different tool.\n\nOf course, you can create a function in your .bashrc file with such code so that you wouldn’t have to type it each time:\ngrep_all_commits () {\n    git grep \"$1\" $(git rev-list --all) |\n        fzf --cycle -i -e |\n        cut -c 1-7 |\n        xclip -r -selection clipboard\n}\nAlternatively, you can pass the result directly into whatever git command you want to use that commit for.\nHere is an example with git show:\ngit grep \"test\" $(git rev-list --all) |\n    fzf --cycle -i -e |\n    cut -c 1-7 |\n    git show\nAnd if you wanted to get really fancy, you could go with:\ngit grep \"test\" $(git rev-list --all) |\n    fzf --cycle -i -e --no-multi \\\n        --ansi --preview=\"$_viewGitLogLine\" \\\n        --header \"enter: view, C-c: copy hash\" \\\n        --bind \"enter:execute:$_viewGitLogLine | less -R\" \\\n        --bind \"ctrl-c:execute:$_gitLogLineToHash |\n        xclip -r -selection clipboard\"\nWrapped in a function:\ngrep_all_commits_preview () {\n    git grep \"$1\" $(git rev-list --all) |\n        fzf --cycle -i -e --no-multi \\\n            --ansi --preview=\"$_viewGitLogLine\" \\\n            --header \"enter: view, C-c: copy hash\" \\\n            --bind \"enter:execute:$_viewGitLogLine |\n              less -R\" \\\n            --bind \"ctrl-c:execute:$_gitLogLineToHash |\n        xclip -r -selection clipboard\"\n}\nThis last function allows you to search through all the results in an incremental fashion while displaying a preview of the selected diff (the changes made at that particular commit). If you want to see more of the diff than the preview displays, press &lt;enter&gt; (then q to quit the pager), if you want to copy the hash of a commit, press C-c (Control + c).\nWith this function, you can now instantly get a preview of the changes made to any line containing an expression for any file, at any commit, and copy the hash of the selected commit. This is really powerful.\n\n\n\nAliases\nIf you don’t want to type a series of flags all the time, you can configure aliases for Git. For instance, Alex Razoumov uses the alias git search for git grep --break --heading -n -i.\nLet’s add to it the -p flag. Here is how you would set this alias:\ngit config --global alias.search 'grep --break --heading -n -i -p'\n\nThis setting gets added to your main Git configuration file (on Linux, by default, at ~/.gitconfig).\n\nFrom there on, you can use your alias with:\n\ngit search test\n\ngit: 'search' is not a git command. See 'git --help'."
  },
  {
    "objectID": "git/ws_search.html#searching-logs",
    "href": "git/ws_search.html#searching-logs",
    "title": "Searching a version-controlled project",
    "section": "Searching logs",
    "text": "Searching logs\nThe second thing that can happen is that you are looking for some pattern in your version control logs.\n\ngit log\ngit log allows to get information on commit logs.\nBy default, it outputs all the commits of the current branch.\nLet’s show the logs of the last 3 commits:\n\ngit log -3\n\ncommit f1802fb9273fdbaad5fa0f1381ff8b18a84a15ce\nAuthor: Marie-Helene Burle &lt;marie.burle@westdri.ca&gt;\nDate:   Wed Apr 24 12:26:52 2024 -0700\n\n    update site\n\ncommit 397ef976e18724c06713ffbf7ebe205b7016a35f\nAuthor: Marie-Helene Burle &lt;marie.burle@westdri.ca&gt;\nDate:   Wed Apr 24 12:26:43 2024 -0700\n\n    styles: improve callouts\n\ncommit e24aebab9ca82555effde05942503ba677df36e3\nAuthor: Marie-Helene Burle &lt;marie.burle@westdri.ca&gt;\nDate:   Wed Apr 24 12:25:09 2024 -0700\n\n    minor edits numpy\n\n\nThe output can be customized thanks to a plethora of options.\nFor instance, here are the logs of the last 15 commits, in a graph, with one line per commit:\n\ngit log --graph --oneline -n 15\n\n* f1802fb9 update site\n* 397ef976 styles: improve callouts\n* e24aebab minor edits numpy\n* 212577bc minor edit benchmark\n* 84337761 update site\n* b44b9816 big improvements benchmark\n* 45cd8e29 update site\n* 4d9e0e27 correct static function explanation + make graph bigger\n* 949277e2 jx numpy: change order headers\n* e0ccfb29 update site\n* e79d100d edit jax parallel\n* 5e7e0a0b update site\n* 7b202e6f small edits to jax parallel\n* 66f50cb4 add jax parallel to navbar\n* c9d7da6e update site\n\n\nBut git log has also flags that allow to search for patterns.\n\n\nSearching commit messages\nOne of the reasons it is so important to write informative commit messages is that they are key to finding commits later on.\nTo look for a pattern in all your commit messages, use git log --grep=&lt;pattern&gt;.\nLet’s look for test in the commit messages and limit the output to 3 commits:\n\ngit log --grep=test -3\n\ncommit 6f07fd90be8045378b482d5ca0175446b42797c8\nAuthor: Marie-Helene Burle &lt;marie.burle@westdri.ca&gt;\nDate:   Tue Feb 27 20:32:43 2024 -0800\n\n    add test csv data file into the site\n\ncommit 7167606e3188e9497768761963af0c4bdc7aad90\nAuthor: Marie-Helene Burle &lt;marie.burle@westdri.ca&gt;\nDate:   Tue Feb 27 20:16:20 2024 -0800\n\n    add test csv data file\n\ncommit 87f11c6715a5da31888dc6b92645156e6738d207\nAuthor: Marie-Helene Burle &lt;marie.burle@westdri.ca&gt;\nDate:   Mon Dec 18 14:06:51 2023 -0800\n\n    test blockquote media for phones\n\n\nFor a more compact output:\n\ngit log --grep=\"test\" -3 --oneline\n\n6f07fd90 add test csv data file into the site\n7167606e add test csv data file\n87f11c67 test blockquote media for phones\n\n\n\nHere too you can use this in combination to fzf with for instance:\ngit log --grep=\"test\" | fzf --cycle -i -e\nOr:\ngit log --grep=\"test\" --oneline |\n    fzf --cycle -i -e --no-multi \\\n        --ansi --preview=\"$_viewGitLogLine\" \\\n        --header \"enter: view, C-c: copy hash\" \\\n        --bind \"enter:execute:$_viewGitLogLine | less -R\" \\\n        --bind \"ctrl-c:execute:$_gitLogLineToHash |\n        xclip -r -selection clipboard\"\n\n\n\nChanges made to a pattern\nRemember that test was present in the file src/test_manuel.py. If we want to see when the pattern was first created and then each time it was modified, we use the -L flag in this fashion:\ngit log -L :&lt;pattern&gt;:file\nIn our case:\n\ngit log -L :test:src/test_manuel.py\n\nfatal: There is no path git/src/test_manuel.py in the commit\n\n\nThis is very useful if you want to see, for instance, changes made to a function in a script.\n\n\nChanges in number of occurrences of a pattern\nNow, if we want to list all commits that created a change in the number of occurrences of test in our project, we run:\n\ngit log -S test --oneline\n\n84337761 update site\n45cd8e29 update site\n93bb9017 add jax benchmark section\n112a5403 update site\n1c85d9fe edits to install\n5a3afe3a jax parallel: remove asynchronous dispatch moved to benchmarking section\n7da77a9a update site\nf0f936cc finish jax webinar slides presentation and embed resources\na847af69 update site\ncd5ac347 update site after embedding resources JAX webinar slides\n1639488f update site\ne5b6373a update site\nfc1e5925 improve jax jit\nc8da0b3d update site\nd395fe8b add intro to ipython + better formatting with tabset (rather than columns)\nb1e330ba add JAX section on jobs\n67a51585 add JAX section on installation\n361c27bf update title and abstract jax webinar\neba13650 update title and abstract jax course\nf84c38bc update site\n8877228e update site\ncd4d8869 update site\n4a6b96cb update site\nb3e68d9b update site\nf9abd0f1 update site\nf162c6a9 update site\nb0f28e7b update site\ncc696002 update site\n7f4d8801 update site\n373d2f49 add content in intro ml nlp slides\n93503a04 update site after full render\n0fa44f29 update site\ned3df92b update site\n8976cb6a update site\n7b1d418e update site\nc813fca7 update site\n4c2cca59 update site\n4ec83107 update\n289d7eee update\n2e9865d5 update\ndb5eebf7 update\nc59f0926 update\n84016b9c update\n62af7b93 update\n88bcbaba update\n428ee1db update\n97cf5731 update\n27bb78e3 add qmd files from molecules\n8e70f8b6 update site\ne395a204 update\nc0a93fbd add prefix (intro_, ws_, wb_) to bash, git, and tools sections\n63ba55fc update\ndbded239 update\n1ca5c158 update\n79ebb58b minor fixes dvc slides\nac8afe70 update\n525ca03c dvc slides before another big change\nd287f33c update\na6dc00eb update\na4e14d34 update\n91d6403f update\na38386a2 update\n413a323d update\ne126a321 update after render\nfee353f5 update\ne934a832 update site\n7e6fe93b update site after render\n10277778 update\ne2640dc6 update\n69ab00ba update site\n0cd6525f update site\n484054e8 update site\n7d4a19bb finish draft stateless\nf4bae193 update site\nc2aadc75 minor edits\ne9f983a8 update site\n8aa9f4e5 finish quick draft of parallel section\n719274ed update site\nb648de49 combine datasets loading from 3 options in one section\nba698653 update site\n169845ab version with: loading datasets with Hugging Face\n5376cca0 update site\n7aaf321f add state draft\n4d98a5b0 update site after full render\n4fbc9736 update site\nfbad50d0 update site\n7c19d98d update site\n92dad162 update site\n4a5ef1ea update site\nf763bc1a update site\n3b955ea5 update site\n27e1a097 update site\n3158ff17 update site\n7c132d5e update site\n141204ed add flax abstract\n52152ef3 update JAX abstract after removing dl part with flax\n8f250d57 update site\n255b3205 update freeze\nb61b6df9 update site\n99a77db3 finish jit draft\n57d56299 update site\nd7e0bcb9 finish jx numpy section\nd344ad8a update site\n858e5422 JAX: big revamp course structure\n62c400b2 update site\n1b2e846f jx principles: add async dispatch\n8c8116e0 update site\n09080851 update site\n8f32ea61 update site\ncd78f159 udpate site\n4a003c52 update site\n4b3bec75 update site\ne8e73865 update site\nc545c684 update site\n0b628db2 finish hpc data partition chapter\n1a5c59f7 save a version of hpc_partition before modifying it\n08552b77 update site\nc7fccd5b update site\nd85a0541 edit running htop on local machine\nb5fa9d35 update site\nacb1a4ff rename and heavily modify the foreach chapter\n4536fc63 add pics\n7c318e74 f4 and f5: do testing on the cluster and remove quarto comment\n2aa72b9b update site\n22192dbf add example releasing memory\n08d8f896 add jx profiling page\na841844f improve hpc optimization\nbde35ec8 improve intro indexing\nf1780432 jx why: replace gtrend embedded by img (keeps breaking), improve graph color, add abstract, add a bit of content\n78c3b7b9 add colors to jax intro diagrams\nfb5662b6 add a number of jx early drafts\nc54f6ed7 add new jx sections and update site using a virtual env for Python\ndbcfca2a add prefix for various sections in julia and r, prevent old and bk files from being executed/rendered, prevent webscraping files from being created\n43b79abd update site\n64ddfd7d small edits to hpc r before course\n9204ff32 add jax top intro\n09f64aee add info on profiling\n0cce7374 move jax below PyTorch as it is a more advanced course\nf33135e1 update gcc and r module versions\n96c23b77 update site\n279f6937 update site after full render\n084d5221 embed resources\nae9b67be update site\n4e9c611e more improvements and little tweaks frameworks slides. Add more info\nd58ae469 update site\n445bb296 improvements frameworks slides\n21882057 update site\nfd17244c update formatting framework slides\n185e051a update site\nbead0b06 many tweaks of formatting\nc68c4611 first very rough draft of frameworks slides\n09f1fe51 replace mermaid diagram in file system exercise with a graphviz one\nc6d4a350 filesystem: add exercise with a diagram\n2798c6a1 minor fix\n7a08552d update skl workflow\nac76eea3 update site\n8a504230 add sklearn workflow\nea800fde add an sklearn serie\nc7ac7301 update site\nccf1a85c added content to aliases.qmd\nf17c8c95 added aliases.qmd\ndc7aae4e update site\n74a4e08b finish logs\n0ffe3e1a add logs draft\n23827256 add project.zip\nc8afe95a add downloadthis extension\nfe3f956d add abstract to documentation section\nbb145734 edits intro slides\n40571641 embed resources intro slides\nffa2c749 total revamp of git intro with simply link to slides\n28032012 update ml course\n043c6cf4 minor edits r course\n4888db47 edit sections on how to run r\ne16c9467 update site\n44ceba8e update site\n0ece9c2e update site\n51b8534e replace old webscraping (Python) workshop to new version from DHSI\nfecc161b add webscraping old to gitignore\n4bb8faf0 little improvements web scraping R\nb784f17d update site\n91fa0247 update site\n3bbe8ac5 add (bad) intro blurb to Python course\n3a3361af update site\nf589a660 rename the ext section into talks\n73dfeda0 improvements collections section\n6617a5bc update site\n622631fc fix and improve pandas section\n3eaacd19 update site\nf8c47d0a add index for new big section (talks)\ne29ae75f update site\n9e6a1b8a edit scripts\n0f60d0db finish redirections\n583f27c1 add filesystem section\na7c08558 gis slides: fix typo\nfec32a19 makie: add content in html below video\n954eb10d makie slides: minor improvements\n90adee86 more info in workflow section\n2250d59a minor edits workflow\n39f737d4 add workflow section\n2cd8c6f3 update navbar by moving data, model, and training in a single section\n5f33a182 make backup of autograd in autograd_old and start to make new version of autograd (not complete)\nfc853e13 some edits to training, but still not complete\n01f0f732 finish tensor section\n24b059f9 finalize parallel loops\ne5401892 again many changes to parallel loops before changing yet again\n1428b5fa many changes to parallel loops before making yet many new alternative changes\n6abe68e0 move copy on modify from basics to indexing and make it better\nfd1d1408 update site\n0649af93 move concepts to reading and create a new intro section with slides\nba7938d1 update site\n2525e9e8 finish function section\nb71c7614 finish control flow section\n7f226584 finalize plotting section\n302627a3 add plotting section\na477bd4d add publishing page with links to quarto workshop and webinar\na30a0759 add data structures section\nc5a92ed6 add blurb basics\n68489a39 basics: change title + move a lot of content into section specific pages\n58e586ec packages: add blurb\n848f4362 update navbar\nb7284048 minor edits bash intro\n075dd527 update site\na519d857 create wildcards section\n42f51d4a rename file from search to find\n7df872bf add videos: 4 workshops for HSS series + staff to staff webinar + regular webinar\nb2e565c7 update site\n590bb505 quarto: add installation links\neca80cdb make slides less wordy based on the s2s webinar given on quarto\n43d1ea7f update site\nf424bba7 add slides for quarto staff to staff webinar\n1f33a777 update site\n99c7e56f add new minor optimization\ndd71a56a turn the parallel loop lesson from the webinar version to the workshop version using batch jobs\n07871c1f add 2nd optimization by louis\n55e7c61a update site\nabb1ced7 update site\nf758b8c1 update site\n61f3b8ec add function suggested by workshop participant\n649f8258 update site\nbafb7696 remove profiler from performance section\n176e5ba4 finish optimization section\n3d42df1c add section on memory\n1ba9cc4a important commit: remove \"avoid type conversions\" in R hpc optimizations section as this doesn't change the timing consistently\nb64856dd first draft optimizations\na563fa97 re-render site\n95086e88 update site\n473d722b many edits r resource page\ne0ab5c78 tweak all heading levels\n934f9a86 update site\n37cef298 remove front page for workshops and webinars + add logo image for front page for intro and hpc courses + re-shuffle a few sections + move most ml topics into a course + minor edits (abstract, etc.)\nfc82cf5d update site\n3bcf2df8 rename first git section of git course to match structure of other topics\n628525d5 fix how to download bash data\n952d027c create front pages for Bash and cards on main topic page\nc6ea3e02 add buttons to r main page\nf3e70692 several improvements to web scraping\nc5258d5f split parallel r section into 3 section and add improvements and edits\n046b607d minor fixes hss slides\ncda4fc9d add missing image and very minor edits ml hss slides\nb9f8b5f5 embed resources intro hss ml slides\n898cdd93 update site\n4c07296e add intro ml for hss slides\n38a825da many formatting edits all reveal.js presentations\n8688a717 replace workshop by webinar in all webinars\n20224c1e update site\n50fcdc72 finish script section (shortened. Need to add more content)\n5ef06d8d finish function section\nbf2653fa move control flow, script, and search to molecules folder\nbad3da30 transfer: add globus and abstract\ne7412c33 finish redirections and move it to the molecules repo to run code\n23bed49d add html_children\n9ff42fbe add html section\n9c2f06ef add delay at each iteration to reduce risk of being blocked\n246c64d1 web scraping: minor edits and improvements\n0d08979d add explanations and comments web scraping\n4599ed35 disable cache for webscraping as it conflicts with rvest\n2703353f minor edit nav titles\n5cd69306 change rstudio server time to 1.5h and remove jh option image as it is not the right one\n8159f9b2 minor edits: add some explanations, improve code a bit\nb78b4b6d add first decent draft webscraping with more or less all code and some explanations\n1c251034 add alliance wiki page for r in intro hss resources\ncf851391 first draft bash redirections and pipes\n03b9e332 first draft bash script\n20a392b6 add a little content to intro r\n5c3cfb03 add 2 new sections (not covered by alex)\nfb69b01d add draft content to intro hss r\n6c6d0cf3 add bash empty chapters for online course\nb900e688 intro hpc slides by re-embeding resources\n66bca7e2 fix typo git search\nd72b1706 finish hpc r slides\n1f14f6be open link to hpc slides\n95f20745 finish control flow chapter\n12d248d8 add many little things in list and make a correction for strings\nc9d2e1dd update site\na65bd054 remove out of the package section everything that can live elsewhere\n173d128f move content jh instructions to a new chapter on running Python\nb259e525 remove alex acknowledgement\nb4b87e69 minor edit git front page\nbb2c6fe2 add acknowledgement of alex content\ndd132ab7 supress redundancy between basic and functions\n6f2f0ae4 finish list section (Python)\ndb33cd1f finish basic chapter (Python)\n7ac57447 first draft collections\n04db2fcc edit basics\nfd677ff1 add first draft Python intro hpc copied from Julia. Still needs lots of work\n7464d141 add first draft Python functions (not far from ready)\n1ccb1569 add first draft web scraping in Python\n663e401f yml: uncomment Python tab and add first two Python workshops\n4450be7c add alias\n479f1aa6 many small edits search + add fzf example for searching the logs\n2556ab4e update site\nc53e0f51 add a big info block with more fancy searches using fzf\n1752db62 first final version of search workshop\nfab10006 minor updates hpc r slides\n534a0da6 first draft hpc in r slides\n2f647bd8 remove unnecessary jupyter: python3 in 2 ml revealjs\n919d2061 git lessons: adjust img new names + fixes, corrections, improvements\n7a3d5b13 rename all git diagram img in some sensible order\n16ca4f6a finish branches and add it to the nav\n2725f415 improve front page image\n115aa01f edits many julia files: remove unnecessary jupyter: julia-1.8, small additions, small fixes, small improvements\n9db0e744 add preliminary draft of branches (git) and commented out nav entry\n4ca06a6a add distributed (julia)\na181d66f add symlink for search.qmd which is in the nested git repo\nbdb30cb3 edit control flow\nda7332f8 add remotes\n774a9d45 rename git main workshop\n8f8eb031 add tags and its img\nc1d8385f add multithreading\na31c5aad edits, additions, fixes\n2ba97085 remove from basics elements moved to other files\n8472020c remove from intro hpc everything that is intro julia (move it in various other sections)\na8f09800 turn arrays to collections and add content\n0cc0c59a add julia functions\n1d65319d add julia control flow\n5ec2bccf add julia basics\n22494671 add julia types\n4b09f00e add julia performance\n55813908 add non interactive julia\nf9dd47ca add julia arrays\nea059d51 edit paths and remove shadow img three trees\n38e370fe add undo\n29eb1ac1 add tools\n81fa8648 add stashing\n73ee1b14 add ignore\nd8970df8 add three trees of git\n8148a866 move all top levels to h2 instead of h1 following chat with quarto developpers\n4637b686 julia intro: change header levels + reformat all code blocks\nc072c6c9 add packages\n1601205a add r resources\n3e81b34a add contrib workshop\n5cc2efa0 move section about collaboration through git to new workshop\n96108ed4 uncomment grid section with wider body width\ncddb972c add ml hpc.qmd\n6fa885a4 add quarto link to about\n08e488af make all page start with h1 instead of h2 + add author where missing + move intro to def block\nf09e60bf update site\n9f97ec82 add 5 new ml workshops\n50a465a0 add note about revealjs presentations slow to load in all links\n2a6a3faf improvement to mnist: small additions + run code\ne2056339 add choosing frameworks\n2e281131 add concept workshop in ml\n68183ee7 add autograd to workshops\n8e6c61c4 add mnist to workshops\ne8b21668 add intro scripting workshop\n6e5faa1f add julia intro hpc workshop\n2ab3deb6 add julia covid plotting\n7e19653b add torchtensor slides\n515a4ddb fix R logo (not good on light bg) and add logos for all other sections\n6a781ea0 rename ml intro hss\n06496189 finish formatting upscaling slides\n44bc8530 reduce high res pics upscaling because GitHub's limit is reach with revealjs with self embedded option\n7989e1a5 add gis mapping slides\n0454b612 add upscaling slides\nd9e4e106 turn link to slides into buttons\n82356532 add _site to vc to solve publish issue on GitHub\neb7f3b17 delete publish.yml for GitHub actions\n82781458 update freeze\nf7f08ca0 update freeze\n74174941 update freeze\nccc30a73 update freeze\n9ef79da4 update freeze\nf23a4123 remove in code lengthy comment and add note instead\nb75bf0d5 add custom title-slide.html with partial template for revealjs title slide\n16ebe722 re-add makie slides *after* having rebuilt the site with freeze true\n348b7acb remove makie slides\nf131770c makie webinar: re-add slides\n36539180 remove makie slides for now for gh actions to build\necda9e1d update freeze with julia makie slides\nff2ec56b add makie slides\n6d406585 front page: switch buttons to cards and readjust content accordingly\nfc203393 update freeze\n4ceac573 add outputs of quarto demos so as not to have to run them all the time (annoying with latex). works with blocking rendering of that dir in yml\n0bbc3117 update freeze with computations from r the basics\n71c4ab68 add r the basics from autumn school 22 to r workshops\nb2e73700 add all quarto example files\n358bfb88 add quarto webinar\n818cb8c7 first commit with _freeze (for the quarto examples)\n8b21abf0 add all ml webinars\nfeafdc57 front page: finalize title and add aside about main site\n13dad000 big changes to about page\n3dd050c0 add publish.yml file for GitHub actions\nb6fc959e add 2022_git_sfu.qmd\n\n\nThis can be useful to identify the commit you need."
  },
  {
    "objectID": "git/ws_search.html#tldr",
    "href": "git/ws_search.html#tldr",
    "title": "Searching a version-controlled project",
    "section": "TL;DR",
    "text": "TL;DR\nHere are the search functions you are the most likely to use:\n\nSearch for a pattern in the current version of your tracked files:\n\ngit grep &lt;pattern&gt;\n\nSearch for a pattern in your files at a certain commit:\n\ngit grep &lt;pattern&gt; &lt;commit&gt;\n\nSearch for a pattern in your files in all the commits:\n\ngit grep &lt;pattern&gt; $(git rev-list --all)\n\nSearch for a pattern in your commit messages:\n\ngit log --grep=&lt;pattern&gt;\nNow you should be able to find pretty much anything in your projects and their histories."
  },
  {
    "objectID": "julia/hpc_distributed.html",
    "href": "julia/hpc_distributed.html",
    "title": "Distributed computing",
    "section": "",
    "text": "Julia supports distributed computing thanks to the module Distributed from its standard library.\nThere are two ways to launch several Julia processes (called “workers”):\n\n\nJulia can be started with the -p flag followed by the number of workers by running (in a terminal):\njulia -p n\nThis launches n workers, available for parallel computations, in addition to the process running the interactive prompt (so there are n + 1 Julia processes in total).\nThe module Distributed is needed whenever you want to use several workers, but the -p flag loads it automatically.\n\nExample:\n\njulia -p 4\nWithin Julia, you can see how many workers are running with:\nnworkers()\nThe total number of processes can be seen with:\nnprocs()\n\n\n\nAlternatively, workers can be started from within a Julia session. In this case, you need to load the module Distributed explicitly:\nusing Distributed\nTo launch n workers:\naddprocs(n)\n\nExample:\n\naddprocs(4)",
    "crumbs": [
      "Julia",
      "<b><em>High-performance Julia</em></b>",
      "Distributed computing"
    ]
  },
  {
    "objectID": "julia/hpc_distributed.html#launching-several-julia-processes",
    "href": "julia/hpc_distributed.html#launching-several-julia-processes",
    "title": "Distributed computing",
    "section": "",
    "text": "Julia supports distributed computing thanks to the module Distributed from its standard library.\nThere are two ways to launch several Julia processes (called “workers”):\n\n\nJulia can be started with the -p flag followed by the number of workers by running (in a terminal):\njulia -p n\nThis launches n workers, available for parallel computations, in addition to the process running the interactive prompt (so there are n + 1 Julia processes in total).\nThe module Distributed is needed whenever you want to use several workers, but the -p flag loads it automatically.\n\nExample:\n\njulia -p 4\nWithin Julia, you can see how many workers are running with:\nnworkers()\nThe total number of processes can be seen with:\nnprocs()\n\n\n\nAlternatively, workers can be started from within a Julia session. In this case, you need to load the module Distributed explicitly:\nusing Distributed\nTo launch n workers:\naddprocs(n)\n\nExample:\n\naddprocs(4)",
    "crumbs": [
      "Julia",
      "<b><em>High-performance Julia</em></b>",
      "Distributed computing"
    ]
  },
  {
    "objectID": "julia/hpc_distributed.html#managing-workers",
    "href": "julia/hpc_distributed.html#managing-workers",
    "title": "Distributed computing",
    "section": "Managing workers",
    "text": "Managing workers\nTo list all the worker process identifiers:\nworkers()\n\nThe process running the Julia prompt has id 1.\n\nTo kill a worker:\nrmprocs(&lt;pid&gt;)\nwhere &lt;pid&gt; is the process identifier of the worker you want to kill (you can kill several workers by providing a list of pids).",
    "crumbs": [
      "Julia",
      "<b><em>High-performance Julia</em></b>",
      "Distributed computing"
    ]
  },
  {
    "objectID": "julia/hpc_distributed.html#using-workers",
    "href": "julia/hpc_distributed.html#using-workers",
    "title": "Distributed computing",
    "section": "Using workers",
    "text": "Using workers\nThere are a number of macros that are very convenient here:\n\nTo execute an expression on all processes, there is @everywhere\n\nFor instance, if your parallel code requires a module or an external package to run, you need to load that module or package with @everywhere:\n@everywhere using DataFrames\nIf the parallel code requires a script to run:\n@everywhere include(\"script.jl\")\nIf it requires a function that you are defining, you need to define it on all the workers:\n@everywhere function &lt;name&gt;(&lt;arguments&gt;)\n    &lt;body&gt;\nend\n\nTo assign a task to a particular worker, you use @spawnat\n\nThe first argument indicates the process id, the second argument is the expression that should be evaluated:\n@spawnat &lt;pid&gt; &lt;expression&gt;\n@spawnat returns of Future: the placeholder for a computation of unknown status and time. The function fetch waits for a Future to complete and returns the result of the computation.\n\nExample:\n\nThe function myid gives the id of the current process. As I mentioned earlier, the process running the interactive Julia prompt has the pid 1. So myid() normally returns 1.\nBut we can “spawn” myid on one of the worker, for instance the first worker (so pid 2):\n@spawnat 2 myid()\nAs you can see, we get a Future as a result. But if we pass it through fetch, we get the result of myid ran on the worker with pid 2:\nfetch(@spawnat 2 myid())\nIf you want tasks to be assigned to any worker automatically, you can pass the symbol :any to @spawnat instead of the worker id:\n@spawnat :any myid()\nTo get the result:\nfetch(@spawnat :any myid())\nIf you run this multiple times, you will see that myid is run on any of your available workers. This will however never return 1, except when you only have one running Julia process (in that case, the process running the prompt is considered a worker).",
    "crumbs": [
      "Julia",
      "<b><em>High-performance Julia</em></b>",
      "Distributed computing"
    ]
  },
  {
    "objectID": "julia/hpc_multithreading.html",
    "href": "julia/hpc_multithreading.html",
    "title": "Multi-threading",
    "section": "",
    "text": "Julia, which was built with efficiency in mind, aimed from the start to have parallel programming abilities. These however came gradually: first, there were coroutines, which is not parallel programming, but allows independent executions of elements of code; then there was a macro allowing for loops to run on several cores, but this would not work on nested loops and it did not integrate with the coroutines or I/O. With version 1.3 however multi-threading capabilities were born.\nWhat is great about Julia’s new task parallelism is that it is incredibly easy to use: no need to write low-level code as with MPI to set where tasks are run. Everything is automatic.",
    "crumbs": [
      "Julia",
      "<b><em>High-performance Julia</em></b>",
      "Multi-threading"
    ]
  },
  {
    "objectID": "julia/hpc_multithreading.html#launching-julia-on-multiple-threads",
    "href": "julia/hpc_multithreading.html#launching-julia-on-multiple-threads",
    "title": "Multi-threading",
    "section": "Launching Julia on multiple threads",
    "text": "Launching Julia on multiple threads\nTo use Julia with multiple threads, we need to launch julia with the JULIA_NUM_THREADS environment variable or with the flag --threads/-t:\n$ JULIA_NUM_THREADS=n julia\nor\n$ julia -t n\nFirst, we need to know how many threads we actually have on our machine.\nThere are many Linux tools for this, but here are two particularly convenient options:\n# To get the total number of available processes\n$ nproc\n\n# For more information (# of sockets, cores per socket, threads per core)\n$ lscpu | grep -E '(S|s)ocket|Thread|^CPU\\(s\\)'\nSince I have 4 available processes (2 cores with 2 threads each), I can launch Julia on 4 threads:\n$ JULIA_NUM_THREADS=4 julia\nThis can also be done from within the Juno IDE.\nTo see how many threads we are using, as well as the ID of the current thread, you can run:\nThreads.nthreads()\nThreads.threadid()",
    "crumbs": [
      "Julia",
      "<b><em>High-performance Julia</em></b>",
      "Multi-threading"
    ]
  },
  {
    "objectID": "julia/hpc_multithreading.html#for-loops-on-multiple-threads",
    "href": "julia/hpc_multithreading.html#for-loops-on-multiple-threads",
    "title": "Multi-threading",
    "section": "For loops on multiple threads",
    "text": "For loops on multiple threads\n\n\nYour turn:\n\nLaunch Julia on 1 thread and run the function below. Then run Julia on the maximum number of threads you have on your machine and run the same function.\n\nThreads.@threads for i = 1:10\n    println(\"i = $i on thread $(Threads.threadid())\")\nend\nUtilities such as htop allow you to visualize the working threads.",
    "crumbs": [
      "Julia",
      "<b><em>High-performance Julia</em></b>",
      "Multi-threading"
    ]
  },
  {
    "objectID": "julia/hpc_multithreading.html#generalization-of-multi-threading",
    "href": "julia/hpc_multithreading.html#generalization-of-multi-threading",
    "title": "Multi-threading",
    "section": "Generalization of multi-threading",
    "text": "Generalization of multi-threading\nLet’s consider the example presented in a Julia blog post in July 2019.\nBoth scripts sort a one dimensional array of 20,000,000 floats between 0 and 1, one with parallelism and one without.\nScript 1, without parallelism: sort.jl.\n# Create one dimensional array of 20,000,000 floats between 0 and 1\na = rand(20000000);\n\n# Use the MergeSort algorithm of the sort function\n# (in the standard Julia Base library)\nb = copy(a); @time sort!(b, alg = MergeSort);\n\n# Let's run the function a second time to remove the effect\n# of the initial compilation\nb = copy(a); @time sort!(b, alg = MergeSort);\nScript 2, with parallelism: psort.jl.\nimport Base.Threads.@spawn\n\n# The psort function is the same as the MergeSort algorithm\n# of the Base sort function with the addition of\n# the @spawn macro on one of the recursive calls\n\n# Sort the elements of `v` in place, from indices `lo` to `hi` inclusive\n\nfunction psort!(v, lo::Int=1, hi::Int = length(v))\n    \n    # 1 or 0 elements: nothing to do\n    if lo &gt;= hi\n        return v\n    end\n    \n    # Below some cutoff: run in serial\n    if hi - lo &lt; 100000\n        sort!(view(v, lo:hi), alg = MergeSort)\n        return v\n    end\n    \n    # Find the midpoint\n    mid = (lo + hi) &gt;&gt;&gt; 1\n    \n    # Task to sort the lower half\n    # will run in parallel with the current call sorting the upper half\n    half = @spawn psort!(v, lo, mid)\n    psort!(v, mid + 1, hi)\n    # Wait for the lower half to finish\n    wait(half)\n\n    # Workspace for merging\n    temp = v[lo:mid]\n    \n    # Merge the two sorted sub-arrays\n    i, k, j = 1, lo, mid + 1\n    @inbounds while k &lt; j &lt;= hi\n        if v[j] &lt; temp[i]\n            v[k] = v[j]\n            j += 1\n        else\n            v[k] = temp[i]\n            i += 1\n        end\n        k += 1\n    end\n    @inbounds while k &lt; j\n        v[k] = temp[i]\n        k += 1\n        i += 1\n    end\n    \n    return v\nend\n\na = rand(20000000);\n\n# Now, let's use our function\nb = copy(a); @time psort!(b);\n\n# And running it a second time to remove\n# the effect of the initial compilation\nb = copy(a); @time psort!(b);\nNow, we can test both scripts with one or multiple threads.\n\nSingle thread, non-parallel script:\n\n$ julia /path/to/sort.jl\n2.234024 seconds (111.88 k allocations: 82.489 MiB, 0.21% gc time)\n2.158333 seconds (11 allocations: 76.294 MiB, 0.51% gc time)\n\nNote the lower time for the 2nd run due to pre-compilation.\n\n\nSingle thread, parallel script:\n\n$ julia /path/to/psort.jl\n2.748138 seconds (336.77 k allocations: 703.200 MiB, 2.24% gc time)\n2.438032 seconds (3.58 k allocations: 686.932 MiB, 0.27% gc time)\n\nEven longer time: normal, there was more to run (import package, read function).\n\n\n2 threads, non-parallel script:\n\n$ JULIA_NUM_THREADS=2 julia /path/to/sort.jl\n2.233720 seconds (111.87 k allocations: 82.145 MiB, 0.21% gc time)\n2.155232 seconds (11 allocations: 76.294 MiB, 0.54% gc time)\n\nRemarkably similar to the single thread: the addition of a thread did not change anything.\n\n\n2 threads, parallel script:\n\n$ JULIA_NUM_THREADS=2 julia /path/to/psort.jl\n1.773643 seconds (336.99 k allocations: 703.171 MiB, 4.08% gc time)\n1.460539 seconds (3.79 k allocations: 686.935 MiB, 0.47% gc time)\n\n33% faster.\nNot twice as fast as one could have hoped since processes have to wait for each other. But that’s a good improvement.\n\n\n4 threads, non-parallel script:\n\n$ JULIA_NUM_THREADS=4 julia /path/to/sort.jl\n2.231717 seconds (111.87 k allocations: 82.145 MiB, 0.21% gc time)\n2.153509 seconds (11 allocations: 76.294 MiB, 0.53% gc time)\n\nAgain: same result as the single thread.\n\n\n4 threads, parallel script:\n\n$ JULIA_NUM_THREADS=4 julia /path/to/psort.jl\n1.291714 seconds (336.98 k allocations: 703.171 MiB, 3.48% gc time)\n1.194282 seconds (3.78 k allocations: 686.935 MiB, 5.19% gc time)\n\nEven though we only split our code in 2 tasks, there is still an improvement over the 2 thread run.",
    "crumbs": [
      "Julia",
      "<b><em>High-performance Julia</em></b>",
      "Multi-threading"
    ]
  },
  {
    "objectID": "julia/index.html",
    "href": "julia/index.html",
    "title": "Julia",
    "section": "",
    "text": "Getting started with  \nAn intro course to Julia\n\n\n\n\nHigh-performance  \nAn HPC course in Julia\n\n\n\n\n\n\n60 min webinars\nVarious Julia topics",
    "crumbs": [
      "Julia",
      "<br>&nbsp;<img src=\"img/logo_julia.png\" class=\"img-fluid\" style=\"width:1.65em\" alt=\"noshadow\"><br><br>"
    ]
  },
  {
    "objectID": "julia/intro_collections.html",
    "href": "julia/intro_collections.html",
    "title": "Collections",
    "section": "",
    "text": "Values can be stored in collections. This workshop introduces tuples, dictionaries, sets, and arrays in Julia.",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Collections"
    ]
  },
  {
    "objectID": "julia/intro_collections.html#tuples",
    "href": "julia/intro_collections.html#tuples",
    "title": "Collections",
    "section": "Tuples",
    "text": "Tuples\nTuples are immutable, indexable, and possibly heterogeneous collections of elements. The order of elements matters.\n\n# Possibly heterogeneous (values can be of different types)\ntypeof((2, 'a', 1.0, \"test\"))\n\nTuple{Int64, Char, Float64, String}\n\n\n\n# Indexable (note that indexing in Julia starts with 1)\nx = (2, 'a', 1.0, \"test\");\nx[3]\n\n1.0\n\n\n\n# Immutable (they cannot be modified)\n# So this returns an error\nx[3] = 8\n\nLoadError: MethodError: no method matching setindex!(::Tuple{Int64, Char, Float64, String}, ::Int64, ::Int64)\nMethodError: no method matching setindex!(::Tuple{Int64, Char, Float64, String}, ::Int64, ::Int64)\n\nStacktrace:\n [1] top-level scope\n   @ In[4]:3\n\n\n\nNamed tuples\nTuples can have named components:\n\ntypeof((a=2, b='a', c=1.0, d=\"test\"))\n\n@NamedTuple{a::Int64, b::Char, c::Float64, d::String}\n\n\n\nx = (a=2, b='a', c=1.0, d=\"test\");\nx.c\n\n1.0",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Collections"
    ]
  },
  {
    "objectID": "julia/intro_collections.html#dictionaries",
    "href": "julia/intro_collections.html#dictionaries",
    "title": "Collections",
    "section": "Dictionaries",
    "text": "Dictionaries\nJulia also has dictionaries: associative collections of key/value pairs:\n\nx = Dict(\"Name\"=&gt;\"Roger\", \"Age\"=&gt;52, \"Index\"=&gt;0.3)\n\nDict{String, Any} with 3 entries:\n  \"Index\" =&gt; 0.3\n  \"Age\"   =&gt; 52\n  \"Name\"  =&gt; \"Roger\"\n\n\n\"Name\", \"Age\", and \"Index\" are the keys; \"Roger\", 52, and 0.3 are the values.\nThe =&gt; operator is the same as the Pair function:\n\np = \"foo\" =&gt; 7\n\n\"foo\" =&gt; 7\n\n\n\nq = Pair(\"bar\", 8)\n\n\"bar\" =&gt; 8\n\n\nDictionaries can be heterogeneous (as in this example) and the order doesn’t matter. They are also indexable:\n\nx[\"Name\"]\n\n\"Roger\"\n\n\nAnd mutable (they can be modified):\n\nx[\"Name\"] = \"Alex\";\nx\n\nDict{String, Any} with 3 entries:\n  \"Index\" =&gt; 0.3\n  \"Age\"   =&gt; 52\n  \"Name\"  =&gt; \"Alex\"",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Collections"
    ]
  },
  {
    "objectID": "julia/intro_collections.html#sets",
    "href": "julia/intro_collections.html#sets",
    "title": "Collections",
    "section": "Sets",
    "text": "Sets\nSets are collections without duplicates. The order of elements doesn’t matter.\n\nset1 = Set([9, 4, 8, 2, 7, 8])\n\nSet{Int64} with 5 elements:\n  4\n  7\n  2\n  9\n  8\n\n\n\nNotice how this is a set of 5 (and not 6) elements: the duplicated 8 didn’t matter.\n\n\nset2 = Set([10, 2, 3])\n\nSet{Int64} with 3 elements:\n  2\n  10\n  3\n\n\nYou can compare sets:\n\n# The union is the set of elements that are in one OR the other set\nunion(set1, set2)\n\nSet{Int64} with 7 elements:\n  4\n  7\n  2\n  10\n  9\n  8\n  3\n\n\n\n# The intersect is the set of elements that are in one AND the other set\nintersect(set1, set2)\n\nSet{Int64} with 1 element:\n  2\n\n\n\n# The setdiff is the set of elements that are in the first set but not in the second\n# Note that the order matters here\nsetdiff(set1, set2)\n\nSet{Int64} with 4 elements:\n  4\n  7\n  9\n  8\n\n\nSets can be heterogeneous:\n\nSet([\"test\", 9, :a])\n\nSet{Any} with 3 elements:\n  :a\n  \"test\"\n  9",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Collections"
    ]
  },
  {
    "objectID": "julia/intro_collections.html#arrays",
    "href": "julia/intro_collections.html#arrays",
    "title": "Collections",
    "section": "Arrays",
    "text": "Arrays\n\nVectors\nUnidimensional arrays in Julia are called vectors.\n\nVectors of one element\n\n[3]\n\n1-element Vector{Int64}:\n 3\n\n\n\n[3.4]\n\n1-element Vector{Float64}:\n 3.4\n\n\n\n[\"Hello, World!\"]\n\n1-element Vector{String}:\n \"Hello, World!\"\n\n\n\n\nVectors of multiple elements\n\n[3, 4]\n\n2-element Vector{Int64}:\n 3\n 4\n\n\n\n\n\nTwo dimensional arrays\n\n[3 4]\n\n1×2 Matrix{Int64}:\n 3  4\n\n\n\n[[1, 3] [1, 2]]\n\n2×2 Matrix{Int64}:\n 1  1\n 3  2\n\n\n\n\nSyntax subtleties\nThese 3 syntaxes are equivalent:\n\n[2 4 8]\n\n1×3 Matrix{Int64}:\n 2  4  8\n\n\n\nhcat(2, 4, 8)\n\n1×3 Matrix{Int64}:\n 2  4  8\n\n\n\ncat(2, 4, 8, dims=2)\n\n1×3 Matrix{Int64}:\n 2  4  8\n\n\nThese 4 syntaxes are equivalent:\n\n[2\n 4\n 8]\n\n3-element Vector{Int64}:\n 2\n 4\n 8\n\n\n\n[2; 4; 8]\n\n3-element Vector{Int64}:\n 2\n 4\n 8\n\n\n\nvcat(2, 4, 8)\n\n3-element Vector{Int64}:\n 2\n 4\n 8\n\n\n\ncat(2, 4, 8, dims=1)\n\n3-element Vector{Int64}:\n 2\n 4\n 8\n\n\nElements separated by semi-colons or end of lines get expanded vertically.\nThose separated by commas do not get expanded.\nElements separated by spaces or tabs get expanded horizontally.\n\n\nYour turn:\n\nCompare the outputs of the following:\n\n\n[1:2; 3:4]\n\n4-element Vector{Int64}:\n 1\n 2\n 3\n 4\n\n\n\n[1:2\n 3:4]\n\n4-element Vector{Int64}:\n 1\n 2\n 3\n 4\n\n\n\n[1:2, 3:4]\n\n2-element Vector{UnitRange{Int64}}:\n 1:2\n 3:4\n\n\n\n[1:2 3:4]\n\n2×2 Matrix{Int64}:\n 1  3\n 2  4\n\n\n\n\nArrays and types\nIn Julia, arrays can be heterogeneous:\n\n[3, \"hello\"]\n\n2-element Vector{Any}:\n 3\n  \"hello\"\n\n\nThis is possible because all elements of an array, no matter of what types, will always sit below the Any type in the type hierarchy.\n\n\nInitializing arrays\nBelow are examples of some of the functions initializing arrays:\n\nrand(2, 3, 4)\n\n2×3×4 Array{Float64, 3}:\n[:, :, 1] =\n 0.70497   0.452224  0.210217\n 0.152121  0.808499  0.748643\n\n[:, :, 2] =\n 0.964218  0.533504  0.295138\n 0.530122  0.705078  0.448783\n\n[:, :, 3] =\n 0.101024  0.702216  0.351094\n 0.451474  0.643441  0.193529\n\n[:, :, 4] =\n 0.365804  0.593161  0.213761\n 0.908817  0.669264  0.160509\n\n\n\nrand(Int64, 2, 3, 4)\n\n2×3×4 Array{Int64, 3}:\n[:, :, 1] =\n  -808940715765468093  -1584927078315600374  -5301199987516324173\n -6596392331988765638  -6192885842242193678   1889096344742778536\n\n[:, :, 2] =\n -1263311441971715837   -398863679696473412    425946792632171343\n -8887634749817674030  -6532441838130849674  -5790650878322099032\n\n[:, :, 3] =\n -3504949663976209361  -7056126120819890696   9014204101180695865\n  5444915959299197671   7453311557699154449  -7332672815187269775\n\n[:, :, 4] =\n -1027239353605623832  8546329529560148599  5006263260814316361\n  3614836023227257818  -380255779183739001  9031894209972587885\n\n\n\nzeros(Int64, 2, 5)\n\n2×5 Matrix{Int64}:\n 0  0  0  0  0\n 0  0  0  0  0\n\n\n\nones(2, 5)\n\n2×5 Matrix{Float64}:\n 1.0  1.0  1.0  1.0  1.0\n 1.0  1.0  1.0  1.0  1.0\n\n\n\nreshape([1, 2, 4, 2], (2, 2))\n\n2×2 Matrix{Int64}:\n 1  4\n 2  2\n\n\n\nfill(\"test\", (2, 2))\n\n2×2 Matrix{String}:\n \"test\"  \"test\"\n \"test\"  \"test\"\n\n\n\n\nBroadcasting\nTo apply a function to each element of a collection rather than to the collection as a whole, Julia uses broadcasting.\n\na = [-3, 2, -5]\n\n3-element Vector{Int64}:\n -3\n  2\n -5\n\n\nabs(a)\nLoadError: MethodError: no method matching abs(::Vector{Int64})\nThis doesn’t work because the function abs only applies to single elements.\nBy broadcasting abs, you apply it to each element of a:\n\nbroadcast(abs, a)\n\n3-element Vector{Int64}:\n 3\n 2\n 5\n\n\nThe dot notation is equivalent:\n\nabs.(a)\n\n3-element Vector{Int64}:\n 3\n 2\n 5\n\n\nIt can also be applied to the pipe, to unary and binary operators, etc.\n\na .|&gt; abs\n\n3-element Vector{Int64}:\n 3\n 2\n 5\n\n\n\n\nYour turn:\n\nTry to understand the difference between the following 2 expressions:\n\n\nabs.(a) == a .|&gt; abs\n\ntrue\n\n\n\nabs.(a) .== a .|&gt; abs\n\n3-element BitVector:\n 1\n 1\n 1\n\n\n\nHint: 0/1 are a short-form notations for false/true in arrays of Booleans.\n\n\n\nComprehensions\nJulia has an array comprehension syntax similar to Python’s:\n\n[ 3i + j for i=1:10, j=3 ]\n\n10-element Vector{Int64}:\n  6\n  9\n 12\n 15\n 18\n 21\n 24\n 27\n 30\n 33",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Collections"
    ]
  },
  {
    "objectID": "julia/intro_collections.html#indexing",
    "href": "julia/intro_collections.html#indexing",
    "title": "Collections",
    "section": "Indexing",
    "text": "Indexing\nAs in other mathematically oriented languages such as R, Julia starts indexing at 1.\nIndexing is done with square brackets:\n\na = [1 2; 3 4]\n\n2×2 Matrix{Int64}:\n 1  2\n 3  4\n\n\n\na[1, 1]\n\n1\n\n\n\na[1, :]\n\n2-element Vector{Int64}:\n 1\n 2\n\n\n\na[:, 1]\n\n2-element Vector{Int64}:\n 1\n 3\n\n\n\n# Here, we are indexing a tuple\n(2, 4, 1.0, \"test\")[2]\n\n4\n\n\n\n\nYour turn:\n\nIndex the element on the 3rd row and 2nd column of b:\n\nb = [\"wrong\" \"wrong\" \"wrong\"; \"wrong\" \"wrong\" \"wrong\"; \"wrong\" \"you got it\" \"wrong\"]\n\n3×3 Matrix{String}:\n \"wrong\"  \"wrong\"       \"wrong\"\n \"wrong\"  \"wrong\"       \"wrong\"\n \"wrong\"  \"you got it\"  \"wrong\"\n\n\n\n\n\nYour turn:\n\na = [1 2; 3 4]\na[1, 1]\na[1, :]\nHow can I get the second column?\nHow can I get the tuple (2, 4)? (a tuple is a list of elements)\n\nAs in Python, by default, arrays are passed by sharing:\n\na = [1, 2, 3];\na[1] = 0;\na\n\n3-element Vector{Int64}:\n 0\n 2\n 3\n\n\nThis prevents the unwanted copying of arrays.",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Collections"
    ]
  },
  {
    "objectID": "julia/intro_functions.html",
    "href": "julia/intro_functions.html",
    "title": "Functions",
    "section": "",
    "text": "Functions are objects containing a set of instructions.\nWhen you pass a tuple of argument(s) (possibly an empty tuple) to them, you get one or more values as output.",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Functions"
    ]
  },
  {
    "objectID": "julia/intro_functions.html#operators",
    "href": "julia/intro_functions.html#operators",
    "title": "Functions",
    "section": "Operators",
    "text": "Operators\nOperators are functions and can be written in a way that shows the tuple of arguments more explicitly.\n\nFor instance, you can use the addition operator (+) in 2 ways:\n\n\n3 + 2\n+(3, 2)\n\n5\n\n\nThe multiplication operator can be omitted when this does not create any ambiguity:\n\na = 3;\n2a\n\n6\n\n\nJulia has “assignment by operation” operators:\n\na = 2;\na += 7    # this is the same as a = a + 7\n\n9\n\n\nThere is a left division operator:\n\n2\\8 == 8/2\n\ntrue\n\n\nJulia supports fraction operations:\n\n4//8\n\n1//2\n\n\n\n1//2 + 3//4\n\n5//4",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Functions"
    ]
  },
  {
    "objectID": "julia/intro_functions.html#function-definition",
    "href": "julia/intro_functions.html#function-definition",
    "title": "Functions",
    "section": "Function definition",
    "text": "Function definition\nThere are 2 ways to define a new function:\n\nLong form\nfunction &lt;name&gt;(&lt;arguments&gt;)\n    &lt;body&gt;\nend\n\nExample:\n\n\nfunction hello1()\n    println(\"Hello\")\nend\n\nhello1 (generic function with 1 method)\n\n\n\n\nAssignment form\n&lt;name&gt;(&lt;arguments&gt;) = &lt;body&gt;\n\nExample:\n\n\nhello1() = println(\"Hello\")\n\nhello1 (generic function with 1 method)\n\n\nThe function hello1 defined with this terse syntax is exactly the same as the one we defined above.\n\n\nStylistic convention\nJulia suggests to use lower case without underscores as function names when the name is readable enough.",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Functions"
    ]
  },
  {
    "objectID": "julia/intro_functions.html#calling-functions",
    "href": "julia/intro_functions.html#calling-functions",
    "title": "Functions",
    "section": "Calling functions",
    "text": "Calling functions\nSince you pass a tuple to a function when you run it, you call a function by appending parentheses to its name:\n\nhello1()\n\nHello\n\n\n\nHere, our function does not take any argument, so the tuple is empty.",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Functions"
    ]
  },
  {
    "objectID": "julia/intro_functions.html#arguments",
    "href": "julia/intro_functions.html#arguments",
    "title": "Functions",
    "section": "Arguments",
    "text": "Arguments\n\nNo argument\nOur function hello1 does not accept any argument. If we pass an argument, we get an error message:\nhello1(\"Bob\")\nLoadError: MethodError: no method matching hello1(::String)\n\n\nOne argument\nTo define a function which accepts an argument, we need to add a placeholder for it in the function definition.\n\nSo let’s try this:\n\n\nfunction hello2(name)\n    println(\"Hello name\")\nend\n\nhello2 (generic function with 1 method)\n\n\n\nhello2(\"Bob\")\n\nHello name\n\n\nMmm … not quite … this function works but does not give the result we wanted.\nHere, we need to use string interpolation:\n\nfunction hello3(name)\n    println(\"Hello $name\")\nend\n\nhello3 (generic function with 1 method)\n\n\n$name in the body of the function points to name in the tuple of argument.\nWhen we run the function, $name is replaced by the value we used in lieu of name in the function definition:\n\nhello3(\"Bob\")\n\nHello Bob\n\n\nHere is the corresponding assignment form for hello3:\n\nhello3(name) = println(\"Hello $name\")\n\nhello3 (generic function with 1 method)\n\n\n\nNote that this dollar sign is only required with strings. Here is an example with integers:\n\n\nfunction addTwo(a)\n    a + 2\nend\n\naddTwo (generic function with 1 method)\n\n\nAnd the corresponding assignment form:\n\naddTwo(a) = a + 2\n\naddTwo (generic function with 1 method)\n\n\n\naddTwo(4)\n\n6\n\n\n\n\nMultiple arguments\nNow, let’s write a function which accepts 2 arguments. For this, we put 2 placeholders in the tuple passed to the function in the function definition:\n\nfunction hello4(name1, name2)\n    println(\"Hello $name1 and $name2\")\nend\n\nhello4 (generic function with 1 method)\n\n\nThis means that this function expects a tuple of 2 values:\n\nhello4(\"Bob\", \"Pete\")\n\nHello Bob and Pete\n\n\n\n\nYour turn:\n\nSee what happens when you pass no argument, a single argument, or three arguments to this function.\n\n\n\nDefault arguments\nYou can set a default value for some or all arguments. In this case, the function will run with or without a value passed for those arguments. If no value is given, the default is used. If a value is given, it will replace the default.\n\nExample:\n\n\nfunction hello5(name=\"\")\n    println(\"Hello $name\")\nend\n\nhello5 (generic function with 2 methods)\n\n\n\nhello5()\n\nHello \n\n\n\nhello5(\"Bob\")\n\nHello Bob\n\n\n\nAnother example:\n\n\nfunction addSomethingOrTwo(a, b=2)\n    a + b\nend\n\naddSomethingOrTwo (generic function with 2 methods)\n\n\n\naddSomethingOrTwo(3)\n\n5\n\n\n\naddSomethingOrTwo(3, 4)\n\n7",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Functions"
    ]
  },
  {
    "objectID": "julia/intro_functions.html#returning-the-result",
    "href": "julia/intro_functions.html#returning-the-result",
    "title": "Functions",
    "section": "Returning the result",
    "text": "Returning the result\nIn Julia, functions return the value(s) of the last expression automatically. If you want to return something else instead, you need to use the return statement. This causes the function to exit early.\n\nLook at these 5 functions:\n\nfunction test1(x, y)\n    x + y\nend\n\nfunction test2(x, y)\n    return x + y\nend\n\nfunction test3(x, y)\n    x * y\n    x + y\nend\n\nfunction test4(x, y)\n    return x * y\n    x + y\nend\n\nfunction test5(x, y)\n    return x * y\n    return x + y\nend\n\nfunction test6(x, y)\n    x * y, x + y\nend\n\n\nYour turn:\n\nWithout running the code, try to guess the outputs of:\n\ntest1(1, 2)\ntest2(1, 2)\ntest3(1, 2)\ntest4(1, 2)\ntest5(1, 2)\ntest6(1, 2)\n\n\nYour turn:\n\nNow, run the code and draw some conclusions on the behaviour of the return statement.",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Functions"
    ]
  },
  {
    "objectID": "julia/intro_functions.html#anonymous-functions",
    "href": "julia/intro_functions.html#anonymous-functions",
    "title": "Functions",
    "section": "Anonymous functions",
    "text": "Anonymous functions\nAnonymous functions are functions which aren’t given a name:\nfunction (&lt;arguments&gt;)\n    &lt;body&gt;\nend\nIn compact form:\n&lt;arguments&gt; -&gt; &lt;body&gt;\n\nExample:\n\n\nfunction (name)\n    println(\"Hello $name\")\nend\n\n#13 (generic function with 1 method)\n\n\nCompact form:\n\nname -&gt; println(\"Hello $name\")\n\n#15 (generic function with 1 method)\n\n\n\nWhen would you want to use anonymous functions?\nThis is very useful for functional programming (when you apply a function—for instance map—to other functions to apply them in a vectorized manner which avoids repetitions).\n\nExample:\n\n\nmap(name -&gt; println(\"Hello $name\"), [\"Bob\", \"Lucie\", \"Sophie\"]);\n\nHello Bob\nHello Lucie\nHello Sophie",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Functions"
    ]
  },
  {
    "objectID": "julia/intro_functions.html#pipes",
    "href": "julia/intro_functions.html#pipes",
    "title": "Functions",
    "section": "Pipes",
    "text": "Pipes\n|&gt; is the pipe in Julia. It redirects the output of the expression on the left as the input of the expression on the right.\n\nThe following 2 expressions are equivalent:\n\nprintln(\"Hello\")\n\"Hello\" |&gt; println\n\nHere is another example:\n\n\nsqrt(2) == 2 |&gt; sqrt\n\ntrue",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Functions"
    ]
  },
  {
    "objectID": "julia/intro_functions.html#function-composition",
    "href": "julia/intro_functions.html#function-composition",
    "title": "Functions",
    "section": "Function composition",
    "text": "Function composition\nYou can pass a function inside another function:\n&lt;function2&gt;(&lt;function1&gt;(&lt;arguments&gt;))\n&lt;arguments&gt; will be passed to &lt;function1&gt; and the result will then be passed to &lt;function2&gt;.\nAn equivalent syntax is to use the composition operator ∘ (in the REPL, type \\circ then press tab):\n(&lt;function2&gt; ∘ &lt;function1&gt;)(&lt;arguments&gt;)\n\nExample:\n\n\n# sum is our first function\nsum(1:3)\n\n6\n\n\n\n# sqrt is the second function\nsqrt(sum(1:3))\n\n2.449489742783178\n\n\n\n# This is equivalent\n(sqrt ∘ sum)(1:3)\n\n2.449489742783178\n\n\n\n\nYour turn:\n\nWrite three other equivalent expressions using the pipe.\n\n\nAnother example:\n\n\nexp(+(-3, 1))\n\n(exp ∘ +)(-3, 1)\n\n0.1353352832366127\n\n\n\n\nYour turn:\n\nTry to write the same expression in another 2 different ways.",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Functions"
    ]
  },
  {
    "objectID": "julia/intro_functions.html#mutating-functions",
    "href": "julia/intro_functions.html#mutating-functions",
    "title": "Functions",
    "section": "Mutating functions",
    "text": "Mutating functions\nFunctions usually do not modify their argument(s):\n\na = [-2, 3, -5]\n\n3-element Vector{Int64}:\n -2\n  3\n -5\n\n\n\nsort(a)\n\n3-element Vector{Int64}:\n -5\n -2\n  3\n\n\n\na\n\n3-element Vector{Int64}:\n -2\n  3\n -5\n\n\nJulia has a set of functions which modify their argument(s). By convention, their names end with !\n\nThe function sort has a mutating equivalent sort!:\n\n\nsort!(a);\na\n\n3-element Vector{Int64}:\n -5\n -2\n  3\n\n\n\nIf you write functions which modify their arguments, make sure to follow this convention too.",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Functions"
    ]
  },
  {
    "objectID": "julia/intro_functions.html#broadcasting",
    "href": "julia/intro_functions.html#broadcasting",
    "title": "Functions",
    "section": "Broadcasting",
    "text": "Broadcasting\nTo apply a function to each element of a collection rather than to the collection as a whole, Julia uses broadcasting.\n\nLet’s create a collection (here a tuple):\n\n\na = (2, 3)\n\n(2, 3)\n\n\n\nIf we pass a to the string function, that function applies to the whole collection:\n\n\nstring(a)\n\n\"(2, 3)\"\n\n\n\nIn contrast, we can broadcast the function string to all elements of a:\n\n\nbroadcast(string, a)\n\n(\"2\", \"3\")\n\n\n\nAn alternative syntax is to add a period after the function name:\n\n\nstring.(a)\n\n(\"2\", \"3\")\n\n\n\nHere is another example:\n\na = [-3, 2, -5]\nabs(a)\nERROR: MethodError: no method matching abs(::Array{Int64,1})\nThis doesn’t work because the function abs only applies to single elements.\nBy broadcasting abs, you apply it to each element of a:\n\nbroadcast(abs, a)\n\n(2, 3)\n\n\nThe dot notation is equivalent:\n\nabs.(a)\n\n(2, 3)\n\n\nIt can also be applied to the pipe, to unary and binary operators, etc.\n\nExample:\n\n\na .|&gt; abs\n\n(2, 3)\n\n\n\n\nYour turn:\n\nTry to understand the difference between the following 2 expressions:\n\n\nabs.(a) == a .|&gt; abs\nabs.(a) .== a .|&gt; abs\n\n(true, true)",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Functions"
    ]
  },
  {
    "objectID": "julia/intro_functions.html#multiple-dispatch",
    "href": "julia/intro_functions.html#multiple-dispatch",
    "title": "Functions",
    "section": "Multiple dispatch",
    "text": "Multiple dispatch\nIn some programming languages, functions can be polymorphic (multiple versions exist under the same function name). The process of selecting which version to use is called dispatch.\nThere are multiple types of dispatch depending on the language:\n\nDynamic dispatch: the process of selecting one version of a function at run time.\nSingle dispatch: the choice of version is based on a single object.\n\n\nThis is typical of object-oriented languages such as Python, C++, Java, Smalltalk, etc.\n\n\nMultiple dispatch: the choice of version is based on the combination of all operands and their types.\n\n\nThis the case of Lisp and Julia. In Julia, these versions are called methods.",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Functions"
    ]
  },
  {
    "objectID": "julia/intro_functions.html#methods",
    "href": "julia/intro_functions.html#methods",
    "title": "Functions",
    "section": "Methods",
    "text": "Methods\nRunning methods(+) let’s you see that the function + has 206 methods!\nMethods can be added to existing functions.\n\n\nYour turn:\n\nRun the following and try to understand the outputs:\nabssum(x::Int64, y::Int64) = abs(x + y)\nabssum(x::Float64, y::Float64) = abs(x + y)\n\nabssum(2, 4)\nabssum(2.0, 4.0)\nabssum(2, 4.0)\nWhat could you do if you wanted the last expression to work?",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Functions"
    ]
  },
  {
    "objectID": "julia/intro_macros.html",
    "href": "julia/intro_macros.html",
    "title": "Macros",
    "section": "",
    "text": "Julia code is itself data and can be manipulated by the language while it is running.",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Macros"
    ]
  },
  {
    "objectID": "julia/intro_macros.html#metaprogramming",
    "href": "julia/intro_macros.html#metaprogramming",
    "title": "Macros",
    "section": "Metaprogramming",
    "text": "Metaprogramming\n\nLarge influence from Lisp.\nSince Julia is entirely written in Julia, it is particularly well suited for metaprogramming.",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Macros"
    ]
  },
  {
    "objectID": "julia/intro_macros.html#parsing-and-evaluating",
    "href": "julia/intro_macros.html#parsing-and-evaluating",
    "title": "Macros",
    "section": "Parsing and evaluating",
    "text": "Parsing and evaluating\nLet’s start with something simple:\n\n2 + 3\n\n5\n\n\nHow is this run internally?\nThe string \"2 + 3\" gets parsed into an expression:\n\nMeta.parse(\"2 + 3\")\n\n:(2 + 3)\n\n\nThen that expression gets evaluated:\n\neval(Meta.parse(\"2 + 3\"))\n\n5",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Macros"
    ]
  },
  {
    "objectID": "julia/intro_macros.html#macros",
    "href": "julia/intro_macros.html#macros",
    "title": "Macros",
    "section": "Macros",
    "text": "Macros\nThey resemble functions and just like functions, they accept as input a tuple of arguments.\nBUT macros return an expression which is compiled directly rather than requiring a runtime eval call.\nSo they execute before the rest of the code is run.\nMacro’s names are preceded by @ (e.g. @time).\nJulia comes with many macros and you can create your own with:\nmacro &lt;name&gt;()\n    &lt;body&gt;\nend",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Macros"
    ]
  },
  {
    "objectID": "julia/intro_macros.html#stylistic-conventions",
    "href": "julia/intro_macros.html#stylistic-conventions",
    "title": "Macros",
    "section": "Stylistic conventions",
    "text": "Stylistic conventions\nAs with functions, Julia suggests to use lower case, without underscores, as macro names.",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Macros"
    ]
  },
  {
    "objectID": "julia/intro_packages.html",
    "href": "julia/intro_packages.html",
    "title": "Packages",
    "section": "",
    "text": "Julia comes with a collection of packages. In Linux, they are in /usr/share/julia/stdlib/vx.x.\nHere is the list:\nBase64\nCRC32c\nDates\nDelimitedFiles\nDistributed\nFileWatching\nFuture\nInteractiveUtils\nLibdl\nLibGit2\nLinearAlgebra\nLogging\nMarkdown\nMmap\nPkg\nPrintf\nProfile\nRandom\nREPL\nSerialization\nSHA\nSharedArrays\nSockets\nSparseArrays\nStatistics\nSuiteSparse\nTest\nUnicode\nUUIDs",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Packages"
    ]
  },
  {
    "objectID": "julia/intro_packages.html#standard-library",
    "href": "julia/intro_packages.html#standard-library",
    "title": "Packages",
    "section": "",
    "text": "Julia comes with a collection of packages. In Linux, they are in /usr/share/julia/stdlib/vx.x.\nHere is the list:\nBase64\nCRC32c\nDates\nDelimitedFiles\nDistributed\nFileWatching\nFuture\nInteractiveUtils\nLibdl\nLibGit2\nLinearAlgebra\nLogging\nMarkdown\nMmap\nPkg\nPrintf\nProfile\nRandom\nREPL\nSerialization\nSHA\nSharedArrays\nSockets\nSparseArrays\nStatistics\nSuiteSparse\nTest\nUnicode\nUUIDs",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Packages"
    ]
  },
  {
    "objectID": "julia/intro_packages.html#installing-additional-packages",
    "href": "julia/intro_packages.html#installing-additional-packages",
    "title": "Packages",
    "section": "Installing additional packages",
    "text": "Installing additional packages\nYou can install additional packages.\nThese go to your personal library in ~/.julia (this is also where your REPL history is saved).\nAll registered packages are on GitHub and can easily be searched here.\nThe GitHub star system allows you to easily judge the popularity of a package and to see whether it is under current development.\nIn addition to these, there are unregistered packages and you can build your own.\n\n\nYour turn:\n\nTry to find a list of popular plotting packages.\n\nYou can manage your personal library easily in package mode with the commands:\n(env) pkg&gt; add &lt;package&gt;   # install &lt;package&gt;\n(env) pkg&gt; rm &lt;package&gt;    # uninstall &lt;package&gt;\n(env) pkg&gt; up &lt;package&gt;    # upgrade &lt;package&gt;\n(env) pkg&gt; st              # st or status: list installed packages\n(env) pkg&gt; up              # up or upgrade: upgrade all packages\n\nReplace &lt;package&gt; by the name of the package (e.g. Plots ).\n\nYou can install, uninstall, or update several packages at once by listing them with a space:\n(env) pkg&gt; add &lt;package1&gt; &lt;package2&gt; &lt;package3&gt;\nAn alternative to this convenience mode is to load the package manager (package Pkg, part of stdlib) and use it as you would any other package:\nusing Pkg\n\nPkg.add(\"&lt;package&gt;\")        # install &lt;package&gt;\nPkg.rm(\"&lt;package&gt;\")         # uninstall &lt;package&gt;\nPkg.status(\"&lt;package&gt;\")     # status of &lt;package&gt;\nPkg.update(\"&lt;package&gt;\")     # update &lt;package&gt;\nPkg.update()                # status of all installed packages\nPkg.status()                # update all packages\n\nThe short forms up and st do not work in this context.\n\nTo install, uninstall, or update several packages at once in this context, you need to create an array:\nPkg.add([\"&lt;package1&gt;\", \"&lt;package2&gt;\", \"&lt;package3&gt;\"])\n\n\nYour turn:\n\nCheck your list of packages; install the packages Plots, GR, Distributions, StatsPlots, and UnicodePlot; then check that list again.\n\n\n\nYour turn:\n\nNow go explore your ~/.julia. If you don’t find it, make sure that your file explorer allows you to see hidden files.",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Packages"
    ]
  },
  {
    "objectID": "julia/intro_packages.html#loading-packages",
    "href": "julia/intro_packages.html#loading-packages",
    "title": "Packages",
    "section": "Loading packages",
    "text": "Loading packages\nWhether a package from the standard library or one you installed, before you can use a package you need to load it. This has to be done at each new Julia session so the code to load packages should be part of your scripts.\nThis is done with the using command (e.g. using Plots).",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Packages"
    ]
  },
  {
    "objectID": "julia/intro_resources.html",
    "href": "julia/intro_resources.html",
    "title": "Resources",
    "section": "",
    "text": "Here are a couple of Julia resources.\n\n\nDocumentation\n\nOfficial Julia website\nOfficial Julia manual\nAlliance wiki Julia page\nOnline training material\nThe Julia YouTube channel\nThe Julia Wikibook\nA blog aggregator for Julia\n\n\n\nGetting help\n\nDiscourse forum\n[julia] tag on Stack Overflow\nSlack team (you need to agree to the community code of conduct at slackinvite.julialang.org to receive an invitation)\n#julialang hashtag on Twitter\nSubreddit\nGitter channel\n#julia IRC channel on Freenode",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Resources"
    ]
  },
  {
    "objectID": "julia/intro_types.html",
    "href": "julia/intro_types.html",
    "title": "Types",
    "section": "",
    "text": "Type safety (catching errors of inadequate type) performed at compilation time.\n\nExamples: C, C++, Java, Fortran, Haskell.\n\n\n\n\nType safety performed at runtime.\n\nExamples: Python, JavaScript, PHP, Ruby, Lisp.\n\n\n\n\n\nJulia type system is dynamic (types are unknown until runtime), but types can be declared, optionally bringing the advantages of static type systems.\nThis gives users the freedom to choose between an easy and convenient language, or a clearer, faster, and more robust one (or a combination of the two).",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Types"
    ]
  },
  {
    "objectID": "julia/intro_types.html#types-systems",
    "href": "julia/intro_types.html#types-systems",
    "title": "Types",
    "section": "",
    "text": "Type safety (catching errors of inadequate type) performed at compilation time.\n\nExamples: C, C++, Java, Fortran, Haskell.\n\n\n\n\nType safety performed at runtime.\n\nExamples: Python, JavaScript, PHP, Ruby, Lisp.\n\n\n\n\n\nJulia type system is dynamic (types are unknown until runtime), but types can be declared, optionally bringing the advantages of static type systems.\nThis gives users the freedom to choose between an easy and convenient language, or a clearer, faster, and more robust one (or a combination of the two).",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Types"
    ]
  },
  {
    "objectID": "julia/intro_types.html#julia-types-a-hierarchical-tree",
    "href": "julia/intro_types.html#julia-types-a-hierarchical-tree",
    "title": "Types",
    "section": "Julia types: a hierarchical tree",
    "text": "Julia types: a hierarchical tree\nAt the bottom:  concrete types.\nAbove:     abstract types (concepts for collections of concrete types).\nAt the top:     the Any type, encompassing all types.\n\n\nFrom O’Reilly\n\nOne common type missing in this diagram is the boolean type.\nIt is a subtype of the integer type, as can be tested with the subtype operator &lt;:\n\nBool &lt;: Integer\n\ntrue\n\n\nIt can also be made obvious by the following:\n\nfalse == 0\n\ntrue\n\n\n\ntrue == 1\n\ntrue\n\n\n\na = true;\nb = false;\n3a + 2b\n\n3",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Types"
    ]
  },
  {
    "objectID": "julia/intro_types.html#optional-type-declaration",
    "href": "julia/intro_types.html#optional-type-declaration",
    "title": "Types",
    "section": "Optional type declaration",
    "text": "Optional type declaration\nDone with ::\n&lt;value&gt;::&lt;type&gt;\n\nExample:\n\n\n2::Int\n\n2",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Types"
    ]
  },
  {
    "objectID": "julia/intro_types.html#illustration-of-type-safety",
    "href": "julia/intro_types.html#illustration-of-type-safety",
    "title": "Types",
    "section": "Illustration of type safety",
    "text": "Illustration of type safety\nThis works:\n\n2::Int\n\n2\n\n\nThis doesn’t work:\n\n2.0::Int\n\nLoadError: TypeError: in typeassert, expected Int64, got a value of type Float64\nTypeError: in typeassert, expected Int64, got a value of type Float64\n\nStacktrace:\n [1] top-level scope\n   @ In[8]:1\n\n\nType declaration is not yet supported on global variables; this is used in local contexts such as inside a function.\n\nExample:\n\n\nfunction floatsum(a, b)\n    (a + b)::Float64\nend\n\nfloatsum (generic function with 1 method)\n\n\nThis works:\n\nfloatsum(2.3, 1.0)\n\n3.3\n\n\nThis doesn’t work:\n\nfloatsum(2, 4)\n\nLoadError: TypeError: in typeassert, expected Float64, got a value of type Int64\nTypeError: in typeassert, expected Float64, got a value of type Int64\n\nStacktrace:\n [1] floatsum(a::Int64, b::Int64)\n   @ Main ./In[9]:2\n [2] top-level scope\n   @ In[11]:1",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Types"
    ]
  },
  {
    "objectID": "julia/intro_types.html#information-and-conversion",
    "href": "julia/intro_types.html#information-and-conversion",
    "title": "Types",
    "section": "Information and conversion",
    "text": "Information and conversion\nThe typeof function gives the type of an object:\n\ntypeof(2)\n\nInt64\n\n\n\ntypeof(2.0)\n\nFloat64\n\n\n\ntypeof(\"Hello, World!\")\n\nString\n\n\n\ntypeof(true)\n\nBool\n\n\n\ntypeof((2, 4, 1.0, \"test\"))\n\nTuple{Int64, Int64, Float64, String}\n\n\nConversion between types is possible in some cases:\n\nInt(2.0)\n\n2\n\n\n\ntypeof(Int(2.0))\n\nInt64\n\n\n\nChar(2.0)\n\n'\\x02': ASCII/Unicode U+0002 (category Cc: Other, control)\n\n\n\ntypeof(Char(2.0))\n\nChar",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Types"
    ]
  },
  {
    "objectID": "julia/intro_types.html#stylistic-convention",
    "href": "julia/intro_types.html#stylistic-convention",
    "title": "Types",
    "section": "Stylistic convention",
    "text": "Stylistic convention\nThe names of types start with a capital letter and camel case is used in multiple-word names.",
    "crumbs": [
      "Julia",
      "<b><em>Getting started</em></b>",
      "Types"
    ]
  },
  {
    "objectID": "julia/wb_firstdab.html",
    "href": "julia/wb_firstdab.html",
    "title": "First dab at Julia",
    "section": "",
    "text": "Julia is fast: just-in-time (JIT) compilation and multiple dispatch bring efficiency to interactivity. People often say that using Julia feels like running R or python with a speed almost comparable to that of C.\nBut Julia also comes with parallel computing and multi-threading capabilities.\nIn this webinar, after a quickly presentation of some of the key features of Julia’s beautifully concise syntax, I will dive into using Julia for HPC.",
    "crumbs": [
      "Julia",
      "<b><em>Webinars</em></b>",
      "First gentle dab at Julia"
    ]
  },
  {
    "objectID": "julia/wb_makie.html",
    "href": "julia/wb_makie.html",
    "title": "Makie",
    "section": "",
    "text": "There are several popular data visualization libraries for the Julia programming language (e.g. Plots, Gadfly, VegaLite, Makie). They vary in their precompilation time, time to first plot, layout capabilities, ability to handle 3D data, ease of use, and syntax style. In this landscape, Makie focuses on high performance, fancy layouts, and extensibility.\nMakie comes with multiple backends. In this webinar, we will cover:\n\nGLMakie (ideal for interactive 2D and 3D plotting)\nWGLMakie (an equivalent that runs within browsers)\nCairoMakie (best for high-quality vector graphics)\n\nWe will also see how to run Makie in the Alliance clusters.\nSlides (Click and wait: this reveal.js presentation is heavy and takes some time to load.)",
    "crumbs": [
      "Julia",
      "<b><em>Webinars</em></b>",
      "Data visualization with Makie"
    ]
  },
  {
    "objectID": "julia/wb_makie.html#plotting-in-julia",
    "href": "julia/wb_makie.html#plotting-in-julia",
    "title": "Makie",
    "section": "Plotting in Julia",
    "text": "Plotting in Julia\nThere are many options to create plots in Julia. Some of the most popular ones are:\n\nPlots.jl: high-level API for working with different back-ends (GR, Pyplot, Plotly…),\nPyPlot.jl: Julia interface to Matplotlib’s matplotlib.pyplot,\nPlotlyJS.jl: Julia interface to plotly.js,\nPlotlyLight.jl: the fastest plotting option in Julia by far, but limited features,\nGadfly.jl: following the grammar of graphics popularized by Hadley Wickham in R,\nVegaLite.jl: grammar of interactive graphics,\nPGFPlotsX.jl: Julia interface to the PGFPlots LaTeX package,\nUnicodePlots.jl: plots in the terminal 🙂,\nMakie.jl: powerful plotting ecosystem: animation, 3D, GPU optimization.\n\nThis webinar focuses on Makie.jl.",
    "crumbs": [
      "Julia",
      "<b><em>Webinars</em></b>",
      "Data visualization with Makie"
    ]
  },
  {
    "objectID": "julia/wb_makie.html#the-makie-ecosystem",
    "href": "julia/wb_makie.html#the-makie-ecosystem",
    "title": "Makie",
    "section": "The Makie ecosystem",
    "text": "The Makie ecosystem\nMakie consists of a core package (Makie), with the plots functionalities.\nIn addition to this, a backend is needed to render plots into images or vector graphics. Three backends are available:\n\nCairoMakie: vector graphics or high-quality 2D plots. Creates, but does not display plots (you need an IDE that does or you can use ElectronDisplay.jl),\nGLMakie: based on OpenGL; 3D rendering and interactivity in GLFW window (no vector graphics),\nWGLMakie: web version of GLMakie (plots rendered in a browser instead of a window).",
    "crumbs": [
      "Julia",
      "<b><em>Webinars</em></b>",
      "Data visualization with Makie"
    ]
  },
  {
    "objectID": "julia/wb_makie.html#resources",
    "href": "julia/wb_makie.html#resources",
    "title": "Makie",
    "section": "Resources",
    "text": "Resources\nHere are some links and resources useful to get started with the Makie ecosystem:\n\nthe official Makie documentation,\nJulia Data Science book, chapter 5,\nthe project Beautiful Makie contains many great plot examples,\ncheatsheets:\n\nfor 2D plotting:\n\n\nFrom: Storopoli, Huijzer and Alonso (2021). Julia Data Science. https://juliadatascience.io. ISBN: 97984898\n\nfor 3D plotting:\n\n\nFrom: Storopoli, Huijzer and Alonso (2021). Julia Data Science. https://juliadatascience.io. ISBN: 97984898",
    "crumbs": [
      "Julia",
      "<b><em>Webinars</em></b>",
      "Data visualization with Makie"
    ]
  },
  {
    "objectID": "julia/wb_makie.html#troubleshooting",
    "href": "julia/wb_makie.html#troubleshooting",
    "title": "Makie",
    "section": "Troubleshooting",
    "text": "Troubleshooting\nCairoMakie and WGLMakie should install without issues. Installing GLMakie however can be challenging. This page may lead you towards a solution.",
    "crumbs": [
      "Julia",
      "<b><em>Webinars</em></b>",
      "Data visualization with Makie"
    ]
  },
  {
    "objectID": "julia/wb_makie.html#extensions",
    "href": "julia/wb_makie.html#extensions",
    "title": "Makie",
    "section": "Extensions",
    "text": "Extensions\nA number of extensions have been built on top of Makie:\n\nGeoMakie.jl add geographical plotting utilities to Makie,\nAlgebraOfGraphics.jl turns plotting into a simple algebra of building blocks,\nGraphMakie.jl to create network graphs.",
    "crumbs": [
      "Julia",
      "<b><em>Webinars</em></b>",
      "Data visualization with Makie"
    ]
  },
  {
    "objectID": "julia/wb_makie.html#fundamental-functioning",
    "href": "julia/wb_makie.html#fundamental-functioning",
    "title": "Makie",
    "section": "Fundamental functioning",
    "text": "Fundamental functioning\n\nFigure\nLoad the package (here, we are using CairoMakie):\n\nusing CairoMakie                        # no need to import Makie itself\n\nCreate a Figure (container object):\n\nfig = Figure()\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie ~/.julia/packages/Makie/QGPt0/src/scenes.jl:220\n\n\n\n\n\n\n\n\n\n\ntypeof(fig)\n\nFigure\n\n\nYou can customize a Figure:\n\nfig2 = Figure(backgroundcolor=:grey22, resolution=(300, 300))\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie ~/.julia/packages/Makie/QGPt0/src/scenes.jl:220\n\n\n\n\n\n\n\n\n\nMakie uses the Colors.jl package as a dependency. You can find a list of all named colours here.\nTo use CSS specification (e.g. hex), you need to install Colors explicitly and use its color parsing capabilities:\n\nusing Colors\nfig3 = Figure(backgroundcolor=colorant\"#adc2eb\")\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie ~/.julia/packages/Makie/QGPt0/src/scenes.jl:220\n\n\n\n\n\n\n\n\n\n\n\nAxis\nThen, you can create an Axis:\n\nax = Axis(Figure()[1, 1])\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie ~/.julia/packages/Makie/QGPt0/src/scenes.jl:220\n\n\nAxis with 0 plots:\n\n\n\ntypeof(ax)\n\nAxis\n\n\n\nAxis(fig3[1, 1])  # fig3[1, 1] sets the subplot layout: fig[row, col]\nfig3\n\n\n\n\n\n\n\n\n\nAxis(fig[2, 3])  # This is what happens if we change the layout\nfig\n\n\n\n\n\n\n\n\n\nAxis(fig3[2, 3])  # We can add another axis on fig3\nfig3\n\n\n\n\n\n\n\n\nAxis are customizable:\n\nfig4 = Figure()\nAxis(fig4[1, 1],\n     xlabel=\"x label\",\n     ylabel=\"y label\",\n     title=\"Title of the plot\")\nfig4\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie ~/.julia/packages/Makie/QGPt0/src/scenes.jl:220\n\n\n\n\n\n\n\n\n\n\n\nPlot\nFinally, you can add a plot:\n\nfig = Figure()\nax = Axis(fig[1, 1])\nx = LinRange(-10, 10, 20)\ny = x\nscatter!(ax, x, y)  # Functions with ! transform their arguments\nfig\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie ~/.julia/packages/Makie/QGPt0/src/scenes.jl:220\n\n\n\n\n\n\n\n\n\nOf course, there are many plotting functions, e.g. scatterlines!:\n\nfig = Figure()\nax = Axis(fig[1, 1])\nx = LinRange(-10, 10, 20)\ny = x\nscatterlines!(ax, x, y)  # Functions with ! transform their arguments\nfig\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie ~/.julia/packages/Makie/QGPt0/src/scenes.jl:220\n\n\n\n\n\n\n\n\n\nWe can also use lines!:\n\nfig = Figure()\nax = Axis(fig[1, 1])\nx = LinRange(-10, 10, 20)\ny = sin.(x)  # The . means that the function is broadcast to each element of x\nlines!(ax, x, y)\nfig\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie ~/.julia/packages/Makie/QGPt0/src/scenes.jl:220\n\n\n\n\n\n\n\n\n\nLet’s add points to get a smoother line:\n\nfig = Figure()\nax = Axis(fig[1, 1])\nx = LinRange(-10, 10, 1000)\ny = sin.(x)  # The . means that the function is broadcast to each element of x\nlines!(ax, x, y)\nfig\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie ~/.julia/packages/Makie/QGPt0/src/scenes.jl:220\n\n\n\n\n\n\n\n\n\nNow, you don’t have to create the Figure, Axis, and plot one at a time. You can create them at the same time with, for instance lines:\n\nx = LinRange(-10, 10, 1000)\ny = sin.(x)\nlines(x, y)  # Note the use of lines instead of lines!\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie ~/.julia/packages/Makie/QGPt0/src/scenes.jl:220\n\n\n\n\n\n\n\n\n\nOr even more simply:\n\nx = LinRange(-10, 10, 1000)\nlines(x, sin)\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie ~/.julia/packages/Makie/QGPt0/src/scenes.jl:220\n\n\n\n\n\n\n\n\n\nThis is a lot simpler, but it is important to understand the concepts of the Figure and Axis objects as you will need it to customize them:\n\nx = LinRange(-10, 10, 1000)\ny = cos.(x)\nlines(x, y;\n      figure=(; backgroundcolor=:green),\n      axis=(; title=\"Cosinus function\", xlabel=\"x label\", ylabel=\"y label\"))\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie ~/.julia/packages/Makie/QGPt0/src/scenes.jl:220\n\n\n\n\n\n\n\n\n\nWhen you create the Figure, Axis, and plot at the same time, you create a FigureAxisPlot object:\n\nx = LinRange(-10, 10, 1000)\ny = cos.(x)\nobj = lines(x, y;\n            figure=(; backgroundcolor=:green),\n            axis=(; title=\"Cosinus function\",\n                  xlabel=\"x label\",\n                  ylabel=\"y label\"));\ntypeof(obj)\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie ~/.julia/packages/Makie/QGPt0/src/scenes.jl:220\n\n\nMakie.FigureAxisPlot\n\n\n\nNote the ; in the figure and axis value. This is because these are one-element NamedTuples.\n\nThe mutating functions (with !) can be used to add plots to an existing figure, but first, you need to decompose the FigureAxisPlot object:\n\nfig, ax, plot = lines(x, sin)\nlines!(ax, x, cos)  # Remember that we are transforming the Axis object\nfig                 # Now we can plot the transformed Figure\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie ~/.julia/packages/Makie/QGPt0/src/scenes.jl:220\n\n\n\n\n\n\n\n\n\nOr we can add several plots on different Axis in the same Figure:\n\nfig, ax1, plot = lines(x, sin)\nax2 = Axis(fig[1, 2])\nlines!(ax2, x, cos)\nfig\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie ~/.julia/packages/Makie/QGPt0/src/scenes.jl:220",
    "crumbs": [
      "Julia",
      "<b><em>Webinars</em></b>",
      "Data visualization with Makie"
    ]
  },
  {
    "objectID": "julia/wb_makie.html#examples",
    "href": "julia/wb_makie.html#examples",
    "title": "Makie",
    "section": "Examples",
    "text": "Examples\n\n2D\n\nusing CairoMakie\nusing StatsBase, LinearAlgebra\nusing Interpolations, OnlineStats\nusing Distributions\nCairoMakie.activate!(type = \"png\")\n\nfunction eq_hist(matrix; nbins = 256 * 256)\n    h_eq = fit(Histogram, vec(matrix), nbins = nbins)\n    h_eq = normalize(h_eq, mode = :density)\n    cdf = cumsum(h_eq.weights)\n    cdf = cdf / cdf[end]\n    edg = h_eq.edges[1]\n    interp_linear = LinearInterpolation(edg, [cdf..., cdf[end]])\n    out = reshape(interp_linear(vec(matrix)), size(matrix))\n    return out\nend\n\nfunction getcounts!(h, fn; n = 100)\n    for _ in 1:n\n        vals = eigvals(fn())\n        x0 = real.(vals)\n        y0 = imag.(vals)\n        fit!(h, zip(x0,y0))\n    end\nend\n\nm(;a=10rand()-5, b=10rand()-5) = [0 0 0 a; -1 -1 1 0; b 0 0 0; -1 -1 -1 -1]\n\nh = HeatMap(range(-3.5,3.5,length=1200), range(-3.5,3.5, length=1200))\ngetcounts!(h, m; n=2_000_000)\n\nwith_theme(theme_black()) do\n    fig = Figure(figure_padding=0,resolution=(600,600))\n    ax = Axis(fig[1,1]; aspect = DataAspect())\n    heatmap!(ax,-3.5..3.5, -3.5..3.5, eq_hist(h.counts); colormap = :bone_1)\n    hidedecorations!(ax)\n    hidespines!(ax)\n    fig\nend\n\nPrecompiling OnlineStats\n  ✓ OnlineStatsBase\n  ✓ OnlineStats\n  2 dependencies successfully precompiled in 3 seconds. 48 already precompiled.\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie ~/.julia/packages/Makie/QGPt0/src/scenes.jl:220\n\n\n\n\n\n\n\n3D\nusing GLMakie, Random\nGLMakie.activate!()\n\nRandom.seed!(13)\nx = -6:0.5:6\ny = -6:0.5:6\nz = 6exp.( -(x.^2 .+ y' .^ 2)./4)\n\nbox = Rect3(Point3f(-0.5), Vec3f(1))\nn = 100\ng(x) = x^(1/10)\nalphas = [g(x) for x in range(0,1,length=n)]\ncmap_alpha = resample_cmap(:linear_worb_100_25_c53_n256, n, alpha = alphas)\n\nwith_theme(theme_dark()) do\n    fig, ax, = meshscatter(x, y, z;\n                           marker=box,\n                           markersize = 0.5,\n                           color = vec(z),\n                           colormap = cmap_alpha,\n                           colorrange = (0,6),\n                           axis = (;\n                                   type = Axis3,\n                                   aspect = :data,\n                                   azimuth = 7.3,\n                                   elevation = 0.189,\n            perspectiveness = 0.5),\n        figure = (;\n            resolution =(1200,800)))\n    meshscatter!(ax, x .+ 7, y, z./2;\n        markersize = 0.25,\n        color = vec(z./2),\n        colormap = cmap_alpha,\n        colorrange = (0, 6),\n        ambient = Vec3f(0.85, 0.85, 0.85),\n        backlight = 1.5f0)\n    xlims!(-5.5,10)\n    ylims!(-5.5,5.5)\n    hidedecorations!(ax; grid = false)\n    hidespines!(ax)\n    fig\nend\n\n\nFor more examples, have a look at Beautiful Makie.",
    "crumbs": [
      "Julia",
      "<b><em>Webinars</em></b>",
      "Data visualization with Makie"
    ]
  },
  {
    "objectID": "julia/wb_makie.html#compiling-sysimages",
    "href": "julia/wb_makie.html#compiling-sysimages",
    "title": "Makie",
    "section": "Compiling sysimages",
    "text": "Compiling sysimages\nWhile Makie is extremely powerful, its compilation time and its time to first plot are extremely long. For this reason, it might save you a lot of time to create a sysimage (a file containing information from a Julia session such as loaded packages, global variables, compiled code, etc.) with PackageCompiler.jl.\n\nThe upcoming Julia 1.9 will do this automatically.",
    "crumbs": [
      "Julia",
      "<b><em>Webinars</em></b>",
      "Data visualization with Makie"
    ]
  },
  {
    "objectID": "julia/wb_makie.html#using-the-alliance-clusters",
    "href": "julia/wb_makie.html#using-the-alliance-clusters",
    "title": "Makie",
    "section": "Using the Alliance clusters",
    "text": "Using the Alliance clusters\n\nCairoMakie\nCairoMakie will run without problem on the Alliance clusters. It is not designed for interactivity, so saving to file is what makes the most sense.\n\nExample:\n\nsave(\"graph.png\", fig)\n\nRemember however that CairoMakie is 2D only (for now).\n\n\n\nGLMakie\nGLMakie relies on GLFW to create windows with OpenGL. GLFW doesn’t support creating contexts without an associated window. The dependency GLFW.jl will thus not install in the clusters—even with X11 forwarding—unless you use VDI nodes, VNC, or Virtual GL.\n\n\nWGLMakie\nYou can setup a server with JSServe.jl as per the documentation. However, this method is intended for the creation of interactive widgets, e.g. for a website. While this is really cool, it isn’t optimized for performance. There might also be a way to create an SSH tunnel to your local browser, although there is no documentation on this.\nBest probably is to save to file.\n\nConclusion about the Makie ecosystem on production clusters:\n\n2D plots: use CairoMakie and save to file,\n3D plots: use WGLMakie and save to file.",
    "crumbs": [
      "Julia",
      "<b><em>Webinars</em></b>",
      "Data visualization with Makie"
    ]
  },
  {
    "objectID": "newsletter.html",
    "href": "newsletter.html",
    "title": "Training events mailing list",
    "section": "",
    "text": "If you want to get informed about upcoming training events, please subscribe to our mailing list:  \n(We will only email you about training events.)"
  },
  {
    "objectID": "python/hpc_polars.html",
    "href": "python/hpc_polars.html",
    "title": "Polars data frames",
    "section": "",
    "text": "Polars is a modern open source and very fast data frame framework for Python, Rust, JS, R, and Ruby. In this course, we will cover Polars for Python.\nMulti-threaded queries, SIMD vectorization, automatic parallelization, columnar Apache Arrow memory format, and lazy evaluation make Polars much faster than pandas. Polars is superior to pandas in other ways as well such as proper support for missing data and an ability to work with more data than can fit in memory. The API is intuitive and Polars integrates well with other Python libraries from the scientific programming toolbox.\n\n Start course ➤",
    "crumbs": [
      "Python",
      "<b><em>Faster Python</em></b>",
      "Polars: faster data frames"
    ]
  },
  {
    "objectID": "python/intro_basics.html",
    "href": "python/intro_basics.html",
    "title": "Python: the basics",
    "section": "",
    "text": "Python is a hugely popular interpreted language with a simple, easily readable syntax, and a large collection of external packages.\nIt was created by Dutch programmer Guido van Rossum in the 80s, with a launch in 1989.\nSince the start of the PYPL PopularitY of Programming Language index (based on the number of tutorial searches in Google) in 2004, its popularity has grown steadily, reaching the number one position in 2018. As of June 2024, its advantage over other programming languages keeps increasing.",
    "crumbs": [
      "Python",
      "<b><em>Getting started</em></b>",
      "Python: the basics"
    ]
  },
  {
    "objectID": "python/intro_basics.html#about-python",
    "href": "python/intro_basics.html#about-python",
    "title": "Python: the basics",
    "section": "",
    "text": "Python is a hugely popular interpreted language with a simple, easily readable syntax, and a large collection of external packages.\nIt was created by Dutch programmer Guido van Rossum in the 80s, with a launch in 1989.\nSince the start of the PYPL PopularitY of Programming Language index (based on the number of tutorial searches in Google) in 2004, its popularity has grown steadily, reaching the number one position in 2018. As of June 2024, its advantage over other programming languages keeps increasing.",
    "crumbs": [
      "Python",
      "<b><em>Getting started</em></b>",
      "Python: the basics"
    ]
  },
  {
    "objectID": "python/intro_basics.html#the-standard-library",
    "href": "python/intro_basics.html#the-standard-library",
    "title": "Python: the basics",
    "section": "The standard library",
    "text": "The standard library\nPython comes with a standard library. As soon as you launch the program, you can access part of the standard library such as the built-in functions and built-in constants:\n\nExample:\n\n\ntype(3)    # type is a built-in function\n\nint\n\n\nMost of the standard library however is held in several thematic modules. Each module contains additional functions, constants, and facilities. Before you can use them, you need to load them into your session.\n\nExample: the os module\nThe os module contains the function getcwd returning the path of the current working directory as a string.\nThis function cannot be used directly:\n\ngetcwd()\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[2], line 1\n----&gt; 1 getcwd()\n\nNameError: name 'getcwd' is not defined\n\n\n\nIn order to access it, you have several options:\n\nLoad the module, then access the function as a method of the module:\n\n\nimport os\nos.getcwd()\n\n'/home/marie/parvus/prog/mint/python'\n\n\n\nYou can create an alias for the module:\n\nimport os as o\no.getcwd()\n\n'/home/marie/parvus/prog/mint/python'\n\n\nWhile it is a little silly for a module with such a short name, it is very convenient with modules of longer names.\n\n\nImport the function directly:\n\n\nfrom os import getcwd\ngetcwd()\n\n'/home/marie/parvus/prog/mint/python'",
    "crumbs": [
      "Python",
      "<b><em>Getting started</em></b>",
      "Python: the basics"
    ]
  },
  {
    "objectID": "python/intro_basics.html#help-and-documentation",
    "href": "python/intro_basics.html#help-and-documentation",
    "title": "Python: the basics",
    "section": "Help and documentation",
    "text": "Help and documentation\n\nModule\nYou can get help on a module thanks to the help function, but only after you have loaded that module into your session:\nimport os\nhelp(os)\nHelp on module os:\n\nNAME\n    os - OS routines for NT or Posix depending on what system we're on.\n\nMODULE REFERENCE\n    https://docs.python.org/3.10/library/os.html\n\n    The following documentation is automatically generated from the Python\n    source files.  It may be incomplete, incorrect or include features that\n    are considered implementation detail and may vary between Python\n    implementations.  When in doubt, consult the module reference at the\n    location listed above.\n    \n... \n\n\nFunctions\nYou can also access the internal Python documentation on a function with help:\n\nhelp(max)\n\nHelp on built-in function max in module builtins:\n\nmax(...)\n    max(iterable, *[, default=obj, key=func]) -&gt; value\n    max(arg1, arg2, *args, *[, key=func]) -&gt; value\n\n    With a single iterable argument, return its biggest item. The\n    default keyword-only argument specifies an object to return if\n    the provided iterable is empty.\n    With two or more positional arguments, return the largest argument.\n\n\n\n\nIn Jupyter, you can also use ?max or max?.\n\nAlternatively, you can print the __doc__ method of the function:\n\nprint(max.__doc__)\n\nmax(iterable, *[, default=obj, key=func]) -&gt; value\nmax(arg1, arg2, *args, *[, key=func]) -&gt; value\n\nWith a single iterable argument, return its biggest item. The\ndefault keyword-only argument specifies an object to return if\nthe provided iterable is empty.\nWith two or more positional arguments, return the largest argument.\n\n\n\n\nMethods of object types\nSome methods belong to specific objects types (e.g. lists have a method called append).\nIn those cases, help(&lt;method&gt;) won’t work.\n\nExample:\n\n\nhelp(append)\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[8], line 1\n----&gt; 1 help(append)\n\nNameError: name 'append' is not defined\n\n\n\nWhat you need to run instead is help(&lt;object&gt;.&lt;method&gt;).\n\nExample:\n\n\nhelp(list.append)\n\nHelp on method_descriptor:\n\nappend(self, object, /) unbound builtins.list method\n    Append object to the end of the list.",
    "crumbs": [
      "Python",
      "<b><em>Getting started</em></b>",
      "Python: the basics"
    ]
  },
  {
    "objectID": "python/intro_basics.html#syntax",
    "href": "python/intro_basics.html#syntax",
    "title": "Python: the basics",
    "section": "Syntax",
    "text": "Syntax\nCommands are usually written one per line, but you can write multiple commands on the same line with the separator ;:\n\na = 2.0; a\n\n2.0\n\n\nTabs or 4 spaces (the number of spaces can be customized in many IDEs) have a syntactic meaning in Python and are not just for human readability:\n\n# Incorrect code\nfor i in [1, 2]:\nprint(i)\n\n\n  Cell In[11], line 3\n    print(i)\n    ^\nIndentationError: expected an indented block after 'for' statement on line 2\n\n\n\n\n\n# Correct code\nfor i in [1, 2]:\n    print(i)\n\n1\n2\n\n\n\nIDEs and good text editors can indent the code automatically.\n\nComments (snippets of text for human consumption and ignored by Python) are marked by #:\n\n# This is a full-line comment\n\na         # This is an inline comment\n\n2.0\n\n\nPEP 8—the style guide for Python code—suggests a maximum of 72 characters per line for comments. Try to keep comments to the point and spread them over multiple lines if they are too long.",
    "crumbs": [
      "Python",
      "<b><em>Getting started</em></b>",
      "Python: the basics"
    ]
  },
  {
    "objectID": "python/intro_basics.html#creating-and-deleting-objects",
    "href": "python/intro_basics.html#creating-and-deleting-objects",
    "title": "Python: the basics",
    "section": "Creating and deleting objects",
    "text": "Creating and deleting objects\n\nAssignment\nThe assignment statement = binds a name (a reference) and a value to create an object (variable, data structure, function, or method).\n\nFor instance, we can bind the name a and the value 1 to create the variable a:\n\n\na = 1\n\nYou can define multiple objects at once (here variables), assigning them the same value:\n\na = b = 10\nprint(a, b)\n\n10 10\n\n\n… or different values:\n\na, b = 1, 2\nprint(a, b)\n\n1 2\n\n\n\n\nYour turn:\n\n\na = 1\nb = a\na = 2\n\nWhat do you think the value of b is now?\n\n\n\nChoosing names\nWhile I am using a and b a lot in this workshop (since the code has no other purpose than to demo the language itself), in your scripts you should use meaningful names (e.g. survival, age, year, species, temperature). It will make reading the code this much easier.\nMake sure not to use the names of built-in functions or built-in constants.\n\n\nDeleting objects\nDeletion of the names can be done with the del statement:\n\nvar = 3\nvar\n\n3\n\n\n\ndel var\nvar\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[19], line 2\n      1 del var\n----&gt; 2 var\n\nNameError: name 'var' is not defined\n\n\n\nThe Python garbage collector automatically removes values with no names bound to them from memory.",
    "crumbs": [
      "Python",
      "<b><em>Getting started</em></b>",
      "Python: the basics"
    ]
  },
  {
    "objectID": "python/intro_basics.html#data-types",
    "href": "python/intro_basics.html#data-types",
    "title": "Python: the basics",
    "section": "Data types",
    "text": "Data types\nPython comes with multiple built-in types.\n\nExamples (non exhaustive):\n\n\ntype(1), type(1.0), type('1'), type(3+2j), type(True)\n\n(int, float, str, complex, bool)\n\n\n\nint = integer\nfloat = floating point number\ncomplex = complex number\nstr = string\nbool = Boolean\n\nPython is dynamically-typed: names do not have types, but they are bound to typed values and they can be bound over time to values of different types.\n\nvar = 2.3\ntype1 = type(var)\nvar = \"A string.\"\ntype2 = type(var)\n\ntype1, type2\n\n(float, str)\n\n\nYou can also convert the type of some values:\n\n'4', type('4'), int('4'), type(int('4'))\n\n('4', str, 4, int)\n\n\n\nfloat(3)\n\n3.0\n\n\n\nstr(3.4)\n\n'3.4'\n\n\n\nbool(0)\n\nFalse\n\n\n\nbool(1)\n\nTrue\n\n\n\nint(True)\n\n1\n\n\n\nfloat(False)\n\n0.0\n\n\nOf course, not all conversions are possible:\n\nint('red')\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[29], line 1\n----&gt; 1 int('red')\n\nValueError: invalid literal for int() with base 10: 'red'\n\n\n\nYou might be surprised by some of the conversions:\n\nint(3.9)\n\n3\n\n\n\nbool(3.4)\n\nTrue",
    "crumbs": [
      "Python",
      "<b><em>Getting started</em></b>",
      "Python: the basics"
    ]
  },
  {
    "objectID": "python/intro_basics.html#quotes",
    "href": "python/intro_basics.html#quotes",
    "title": "Python: the basics",
    "section": "Quotes",
    "text": "Quotes\nPairs of single and double quotes are used to create strings. PEP 8 does not recommend one style over the other. It does suggest however that once you have chosen a style, you stick to it to make scripts consistent.\n\n\"This is a string.\"\n\n'This is a string.'\n\n\n\ntype(\"This is a string.\")\n\nstr\n\n\n\n'This is also a string.'\n\n'This is also a string.'\n\n\n\ntype('This is also a string.')\n\nstr\n\n\nApostrophes and textual quotes interfere with Python quotes. In these cases, use the opposite style to avoid any problem:\n\n# This doesn't work\n'This string isn't easy'\n\n\n  Cell In[36], line 2\n    'This string isn't easy'\n                           ^\nSyntaxError: unterminated string literal (detected at line 2)\n\n\n\n\n\n# This is good\n\"This string isn't easy\"\n\n\"This string isn't easy\"\n\n\n\n# This doesn't work\n\"He said: \"this is a problem.\"\"\n\n\n  Cell In[38], line 2\n    \"He said: \"this is a problem.\"\"\n               ^\nSyntaxError: invalid syntax\n\n\n\n\n\n# This is good\n'He said: \"this is a problem.\"'\n\n'He said: \"this is a problem.\"'\n\n\nSometimes, neither option works and you have to escape some of the quotes with \\:\n\n# This doesn't work\n\"He said: \"this string isn't easy\"\"\n\n\n  Cell In[40], line 2\n    \"He said: \"this string isn't easy\"\"\n                              ^\nSyntaxError: unterminated string literal (detected at line 2)\n\n\n\n\n\n# This doesn't work either\n'He said: \"this string isn't easy\"'\n\n\n  Cell In[41], line 2\n    'He said: \"this string isn't easy\"'\n                                     ^\nSyntaxError: unterminated string literal (detected at line 2)\n\n\n\n\n\n# You can use double quotes and escape double quotes in the string\n\"He said: \\\"this string isn't easy\\\"\"\n\n'He said: \"this string isn\\'t easy\"'\n\n\n\n# Or you can use single quotes and escape single quotes in the string\n'He said: \"this string isn\\'t easy\"'\n\n'He said: \"this string isn\\'t easy\"'",
    "crumbs": [
      "Python",
      "<b><em>Getting started</em></b>",
      "Python: the basics"
    ]
  },
  {
    "objectID": "python/intro_basics.html#basic-operations",
    "href": "python/intro_basics.html#basic-operations",
    "title": "Python: the basics",
    "section": "Basic operations",
    "text": "Basic operations\n\n3 + 2\n\n5\n\n\n\n3.0 - 2.0\n\n1.0\n\n\n\n10 / 2\n\n5.0\n\n\n\nNotice how the result can be of a different type\n\nVariables can be used in operations:\n\na = 3\na + 2\n\n5\n\n\na = a + 10 can be replaced by the more elegant:\n\na += 10\na\n\n13",
    "crumbs": [
      "Python",
      "<b><em>Getting started</em></b>",
      "Python: the basics"
    ]
  },
  {
    "objectID": "python/intro_control_flow.html",
    "href": "python/intro_control_flow.html",
    "title": "Control flow",
    "section": "",
    "text": "Control flow statements alter the linear execution of code, allowing for one or another section of code to be executed, or for one section of code to be executed multiple times.",
    "crumbs": [
      "Python",
      "<b><em>Getting started</em></b>",
      "Control flow"
    ]
  },
  {
    "objectID": "python/intro_control_flow.html#conditionals",
    "href": "python/intro_control_flow.html#conditionals",
    "title": "Control flow",
    "section": "Conditionals",
    "text": "Conditionals\nConditionals dictate the flow of information based on predicates (statements that return True or False).\n\nExample predicates:\n\n4 &lt; 3\n2 == 4\n2 != 4\n2 in range(5)\n2 not in range(5)\n3 &lt;= 4 and 4 &gt; 5\n3 &lt;= 4 and 4 &gt; 5 and 3 != 2\n3 &lt;= 4 or 4 &gt; 5\n\nIf statements\nIn the simplest case, we have:\nif &lt;predicate&gt;:\n    &lt;some action&gt;\nThis translates to:\n\nIf &lt;predicate&gt; evaluates to True, the body of the if statement gets evaluated (&lt;some action&gt; is run),\nIf &lt;predicate&gt; evaluates to False, nothing happens.\n\n\nExamples:\n\n\nx = 3\nif x &gt;= 0:\n    print(x, 'is positive')\n\n3 is positive\n\n\n\nx = -3\nif x &gt;= 0:\n    print(x, 'is positive')\n\n\nNothing gets returned since the predicate returned False.\n\n\n\nIf else statements\nLet’s add an else statement so that our code also returns something when the predicate evaluates to False:\nif &lt;predicate&gt;:\n    &lt;some action&gt;\nelse:\n    &lt;some other action&gt;\n\nExample:\n\n\nx = -3\nif x &gt;= 0:\n    print(x, 'is positive')\nelse:\n    print(x, 'is negative')\n\n-3 is negative\n\n\n\n\nIf elif else\nWe can make this even more complex with:\nif &lt;predicate1&gt;:\n    &lt;some action&gt;\nelif &lt;predicate2&gt;:\n    &lt;some other action&gt;    \nelse:\n    &lt;yet some other action&gt;\n\nExample:\n\n\nx = -3\nif x &gt; 0:\n    print(x, 'is positive')\nelif x &lt; 0:\n    print(x, 'is negative')\nelse:\n    print(x, 'is zero')\n\n-3 is negative",
    "crumbs": [
      "Python",
      "<b><em>Getting started</em></b>",
      "Control flow"
    ]
  },
  {
    "objectID": "python/intro_control_flow.html#loops",
    "href": "python/intro_control_flow.html#loops",
    "title": "Control flow",
    "section": "Loops",
    "text": "Loops\n\nFor loops\nFor loops run a set of instructions for each element of an iterable.\nAn iterable is any Python object cable of returning the items it contains one at a time.\n\nExamples of iterables:\n\nrange(5)\n'a string is an iterable'\n[2, 'word', 4.0]\nFor loops follow the syntax:\nfor &lt;iterable&gt;:\n    &lt;some action&gt;\n\nExample:\n\n\nfor i in range(5):\n    print(i)\n\n0\n1\n2\n3\n4\n\n\n\n\nYour turn:\n\nRemember that the indentation matters in Python.\nWhat do you think that this will print?\nfor i in range(5):\n    print(i)\nprint(i)\n\nStrings are iterables too, so this works:\n\nfor i in 'a string is an iterable':\n    print(i)\n\na\n \ns\nt\nr\ni\nn\ng\n \ni\ns\n \na\nn\n \ni\nt\ne\nr\na\nb\nl\ne\n\n\nTo iterate over multiple iterables at the same time, a convenient option is to use the function zip which creates an iterator of tuples:\n\nfor i, j in zip([1, 2, 3, 4], [3, 4, 5, 6]):\n    print(i + j)\n\n4\n6\n8\n10\n\n\n\n\nWhile loops\nWhile loops run as long as a predicate remains true. They follow the syntax:\nwhile &lt;predicate&gt;:\n    &lt;some action&gt;\n\nExample:\n\n\ni = 0\nwhile i &lt;= 10:\n    print(i)\n    i += 1\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10",
    "crumbs": [
      "Python",
      "<b><em>Getting started</em></b>",
      "Control flow"
    ]
  },
  {
    "objectID": "python/intro_hpc.html",
    "href": "python/intro_hpc.html",
    "title": "High-performance computing in Python",
    "section": "",
    "text": "This section gives a brief introduction on how to use Python on the Digital Research Alliance of Canada supercomputers.",
    "crumbs": [
      "Python",
      "<b><em>Getting started</em></b>",
      "Intro HPC in Python"
    ]
  },
  {
    "objectID": "python/intro_hpc.html#interactive-sessions-for-hpc",
    "href": "python/intro_hpc.html#interactive-sessions-for-hpc",
    "title": "High-performance computing in Python",
    "section": "Interactive sessions for HPC",
    "text": "Interactive sessions for HPC\nWhen you launch a Jupyter session from a JupyterHub, you are running a Slurm job on a compute node. If you want to play for 8 hours in Jupyter, you are requesting an 8 hour job. Now, most of the time you spend on Jupyter is spent typing, running bits and pieces of code, or doing nothing at all. If you ask for GPUs, many CPUs, and lots of RAM, all of it will remain idle most of the time. This is a suboptimal use of resources.\nIn addition, if you ask for lots of resources for a long time, you will have to wait for a while before they get allocated to you.\nLastly, you will go through your allocations quickly.\nAll of this applies equally for interactive sessions launched from an SSH session with salloc.",
    "crumbs": [
      "Python",
      "<b><em>Getting started</em></b>",
      "Intro HPC in Python"
    ]
  },
  {
    "objectID": "python/intro_hpc.html#a-better-approach",
    "href": "python/intro_hpc.html#a-better-approach",
    "title": "High-performance computing in Python",
    "section": "A better approach",
    "text": "A better approach\nA more efficient strategy is to develop and test your code with small samples, few iterations, etc. in an interactive job (from an SSH session in the cluster with salloc), on your own computer, or in Jupyter. Once you are confident that your code works, launch an sbatch job from an SSH session in the cluster to run the code as a script on all your data. This ensures that heavy duty resources that you requested are actually put to use to run your heavy calculations and not seating idle while you are thinking, typing, etc.",
    "crumbs": [
      "Python",
      "<b><em>Getting started</em></b>",
      "Intro HPC in Python"
    ]
  },
  {
    "objectID": "python/intro_hpc.html#logging-in-the-cluster",
    "href": "python/intro_hpc.html#logging-in-the-cluster",
    "title": "High-performance computing in Python",
    "section": "Logging in the cluster",
    "text": "Logging in the cluster\nOpen a terminal emulator:\nWindows users:  launch MobaXTerm.\nmacOS users:   launch Terminal.\nLinux users:     launch xterm or the terminal emulator of your choice.\nThen access the cluster through secure shell:\n$ ssh &lt;username&gt;@&lt;hostname&gt;    # enter password",
    "crumbs": [
      "Python",
      "<b><em>Getting started</em></b>",
      "Intro HPC in Python"
    ]
  },
  {
    "objectID": "python/intro_hpc.html#accessing-python",
    "href": "python/intro_hpc.html#accessing-python",
    "title": "High-performance computing in Python",
    "section": "Accessing Python",
    "text": "Accessing Python\nThis is done with the Lmod tool through the module command. You can find the full documentation here and below are the subcommands you will need:\n# get help on the module command\n$ module help\n$ module --help\n$ module -h\n\n# list modules that are already loaded\n$ module list\n\n# see which modules are available for Python\n$ module spider python\n\n# see how to load Python 3.10.2\n$ module spider python/3.10.2\n\n# load Python 3.10.2 with the required gcc module first\n# (the order is important)\n$ module load gcc/7.3.0 python/3.10.2\n\n# you can see that we now have Python 3.10.2 loaded\n$ module list",
    "crumbs": [
      "Python",
      "<b><em>Getting started</em></b>",
      "Intro HPC in Python"
    ]
  },
  {
    "objectID": "python/intro_hpc.html#copying-files-to-the-cluster",
    "href": "python/intro_hpc.html#copying-files-to-the-cluster",
    "title": "High-performance computing in Python",
    "section": "Copying files to the cluster",
    "text": "Copying files to the cluster\nWe will create a python_workshop directory in ~/scratch, then copy our Python script in it.\n$ mkdir ~/scratch/python_job\nOpen a new terminal window and from your local terminal (make sure that you are not on the remote terminal by looking at the bash prompt) run:\n$ scp /local/path/to/sort.jl &lt;username&gt;@&lt;hostname&gt;:scratch/python_job\n$ scp /local/path/to/psort.jl &lt;username&gt;@&lt;hostname&gt;:scratch/python_job\n\n# enter password",
    "crumbs": [
      "Python",
      "<b><em>Getting started</em></b>",
      "Intro HPC in Python"
    ]
  },
  {
    "objectID": "python/intro_hpc.html#job-scripts",
    "href": "python/intro_hpc.html#job-scripts",
    "title": "High-performance computing in Python",
    "section": "Job scripts",
    "text": "Job scripts\nWe will not run an interactive session with Python on the cluster: we already have Python scripts ready to run. All we need to do is to write job scripts to submit to Slurm, the job scheduler used by the Alliance clusters.\nWe will create 2 scripts: one to run Python on one core and one on as many cores as are available.\n\n\nYour turn:\n\nHow many processors are there on our training cluster?\n\nSave your job scripts in the files ~/scratch/python_job/job_python1c.sh and job_python2c.sh for one and two cores respectively.\nHere is what our single core Slurm script looks like:\n#!/bin/bash\n#SBATCH --job-name=python1c         # job name\n#SBATCH --time=00:01:00             # max walltime 1 min\n#SBATCH --cpus-per-task=1           # number of cores\n#SBATCH --mem=1000                  # max memory (default unit is megabytes)\n#SBATCH --output=python1c%j.out     # file name for the output\n#SBATCH --error=python1c%j.err      # file name for errors\n# %j gets replaced with the job number\n\npython sort.py\n\n\nYour turn:\n\nWrite the script for 2 cores.\n\nNow, we can submit our jobs to the cluster:\n$ cd ~/scratch/python_job\n$ sbatch job_python1c.sh\n$ sbatch job_python2c.sh\nAnd we can check their status with:\n$ sq      # This is an Alliance alias for `squeue -u $USER $@`\n\nPD stands for pending\nR stands for running",
    "crumbs": [
      "Python",
      "<b><em>Getting started</em></b>",
      "Intro HPC in Python"
    ]
  },
  {
    "objectID": "python/intro_hpc.html#run-python-on-our-training-cluster",
    "href": "python/intro_hpc.html#run-python-on-our-training-cluster",
    "title": "High-performance computing in Python",
    "section": "Run Python on our training cluster",
    "text": "Run Python on our training cluster\nThis is not the method I recommend for this workshop, but I am adding it as this is something you might want to use if you need to run heavy computations.\nFirst, you need to load the Python module.\nSee which Python modules are available:\nmodule spider python\nSee how to install one module:\n\nExample:\n\nmodule spider python/3.11.5\nLoad the required dependencies (first) and the module:\nmodule load StdEnv/2023 python/3.11.5\nYou can check that the modules were loaded with:\nmodule list\nAnd verify the Python version with:\npython --version",
    "crumbs": [
      "Python",
      "<b><em>Getting started</em></b>",
      "Intro HPC in Python"
    ]
  },
  {
    "objectID": "python/intro_resources.html",
    "href": "python/intro_resources.html",
    "title": "Resources",
    "section": "",
    "text": "This section contains a list of useful Python resources.\n\n\nPython website\nOfficial documentation\nBeginners’ guide\nOfficial Python tutorial\n\n\nAlliance wiki\nAlliance wiki Python page\n\n\nBooks\nTutorial for non-programmers\nAutomate the boring stuff with Python\n\n\nWorkshops\nSoftware Carpentry Python workshop\nData Carpentry Python workshop\n\n\nIDE\nProject Jupyter\nList of IDEs with description\nComparison of IDEs\nEmacs Python IDE\n\n\nShell\nIPython\nbpython\nptpython\n\n\nGetting help\nStack Overflow [python] tag\n\n\nPackages\nPython package index (PyPI)",
    "crumbs": [
      "Python",
      "<b><em>Getting started</em></b>",
      "Resources"
    ]
  },
  {
    "objectID": "python/intro_text.html",
    "href": "python/intro_text.html",
    "title": "Playing with text",
    "section": "",
    "text": "There are fancy tools to scrape the web and play with text. In preparation for those, in this section, we will download a text file from the internet and play with it using simple commands.",
    "crumbs": [
      "Python",
      "<b><em>Getting started</em></b>",
      "Playing with text"
    ]
  },
  {
    "objectID": "python/intro_text.html#downloading-a-text-file-from-a-url",
    "href": "python/intro_text.html#downloading-a-text-file-from-a-url",
    "title": "Playing with text",
    "section": "Downloading a text file from a URL",
    "text": "Downloading a text file from a URL\nFirst, we need to load the urllib.request module from the Python standard library. It contains functions to deal with URLs:\n\nimport urllib.request\n\nThe snippet of text we will play with is in a text file containing the very beginning of the novel Going Postal by Terry Pratchett and located at the URL https://mint.westdri.ca/python/data/pratchett.txt. We can create a variable that we call url (we can call it whatever we want) and that contains the string of the URL:\n\nurl = \"https://mint.westdri.ca/python/data/pratchett.txt\"\n\n\nprint(url)\n\nhttps://mint.westdri.ca/python/data/pratchett.txt\n\n\n\ntype(url)\n\nstr\n\n\nTo download a text file from a URL, we use the urllib.request.urlopen function:\n\nurllib.request.urlopen(url)\n\n&lt;http.client.HTTPResponse at 0x7a046b59db70&gt;\n\n\nThis return an HTTPResponse object. It is not very useful in this form, but we can get the text out of it by applying the read method:\n\nurllib.request.urlopen(url).read()\n\nb'They say that the prospect of being hanged in the morning concentrates a man\\'s mind wonderfully; unfortunately, what the mind inevitably concentrates on is that, in the morning, it will be in a body that is going to be hanged.\\nThe man going to be hanged had been named Moist von Lipwig by doting if unwise parents, but he was not going to embarrass the name, insofar as that was still possible, by being hung under it. To the world in general, and particularly on that bit of it known as the death warrant, he was Alfred Spangler.\\nAnd he took a more positive approach to the situation and had concentrated his mind on the prospect of not being hanged in the morning, and, most particularly, on the prospect of removing all the crumbling mortar from around a stone in his cell wall with a spoon. So far the work had taken him five weeks and reduced the spoon to something like a nail file. Fortunately, no one ever came to change the bedding here, or else they would have discovered the world\\'s heaviest mattress.\\nIt was a large and heavy stone that was currently the object of his attentions, and, at some point, a huge staple had been hammered into it as an anchor for manacles.\\nMoist sat down facing the wall, gripped the iron ring in both hands, braced his legs against the stones on either side, and heaved.\\nHis shoulders caught fire, and a red mist filled his vision, but the block slid out with a faint and inappropriate tinkling noise. Moist managed to ease it away from the hole and peered inside.\\nAt the far end was another block, and the mortar around it looked suspiciously strong and fresh.\\nJust in front of it was a new spoon. It was shiny.\\nAs he studied it, he heard the clapping behind him. He turned his head, tendons twanging a little riff of agony, and saw several of the wardens watching him through the bars.\\n\"Well done, Mr. Spangler!\" said one of them. \"Ron here owes me five dollars! I told him you were a sticker!! \\'He\\'s a sticker,\\' I said!\"\\n\"You set this up, did you, Mr. Wilkinson?\" said Moist weakly, watching the glint of light on the spoon.\\n\"Oh, not us, sir. Lord Vetinari\\'s orders. He insists that all condemned prisoners should be offered the prospect of freedom.\"\\n\"Freedom? But there\\'s a damn great stone through there!\"\\n\"Yes, there is that, sir, yes, there is that,\" said the warden. \"It\\'s only the prospect, you see. Not actual free freedom as such. Hah, that\\'d be a bit daft, eh?\"\\n\"I suppose so, yes,\" said Moist. He didn\\'t say \"you bastards.\" The wardens had treated him quite civilly these past six weeks, and he made a point of getting on with people. He was very, very good at it. People skills were part of his stock-in-trade; they were nearly the whole of it.\\nBesides, these people had big sticks. So, speaking carefully, he added: \"Some people might consider this cruel, Mr. Wilkinson.\"\\n\"Yes, sir, we asked him about that, sir, but he said no, it wasn\\'t. He said it provided\"--his forehead wrinkled \"--occ-you-pay-shun-all ther-rap-py, healthy exercise, prevented moping, and offered that greatest of all treasures, which is Hope, sir.\"\\n\"Hope,\" muttered Moist glumly.\\n\"Not upset, are you, sir?\"\\n\"Upset? Why should I be upset, Mr. Wilkinson?\"\\n\"Only the last bloke we had in this cell, he managed to get down that drain, sir. Very small man. Very agile.\"\\n'\n\n\nWe can save our text in a new variable:\n\nencoded_text = urllib.request.urlopen(url).read()\n\nNow, encoded_text is not of a very convenient type:\n\ntype(encoded_text)\n\nbytes\n\n\nBefore we can really start playing with it, we want to convert it to a string by decoding it:\n\ntext = encoded_text.decode(\"utf-8\")\ntype(text)\n\nstr\n\n\nWe know have a string, which is great to work on. Let’s print our text:\n\nprint(text)\n\nThey say that the prospect of being hanged in the morning concentrates a man's mind wonderfully; unfortunately, what the mind inevitably concentrates on is that, in the morning, it will be in a body that is going to be hanged.\nThe man going to be hanged had been named Moist von Lipwig by doting if unwise parents, but he was not going to embarrass the name, insofar as that was still possible, by being hung under it. To the world in general, and particularly on that bit of it known as the death warrant, he was Alfred Spangler.\nAnd he took a more positive approach to the situation and had concentrated his mind on the prospect of not being hanged in the morning, and, most particularly, on the prospect of removing all the crumbling mortar from around a stone in his cell wall with a spoon. So far the work had taken him five weeks and reduced the spoon to something like a nail file. Fortunately, no one ever came to change the bedding here, or else they would have discovered the world's heaviest mattress.\nIt was a large and heavy stone that was currently the object of his attentions, and, at some point, a huge staple had been hammered into it as an anchor for manacles.\nMoist sat down facing the wall, gripped the iron ring in both hands, braced his legs against the stones on either side, and heaved.\nHis shoulders caught fire, and a red mist filled his vision, but the block slid out with a faint and inappropriate tinkling noise. Moist managed to ease it away from the hole and peered inside.\nAt the far end was another block, and the mortar around it looked suspiciously strong and fresh.\nJust in front of it was a new spoon. It was shiny.\nAs he studied it, he heard the clapping behind him. He turned his head, tendons twanging a little riff of agony, and saw several of the wardens watching him through the bars.\n\"Well done, Mr. Spangler!\" said one of them. \"Ron here owes me five dollars! I told him you were a sticker!! 'He's a sticker,' I said!\"\n\"You set this up, did you, Mr. Wilkinson?\" said Moist weakly, watching the glint of light on the spoon.\n\"Oh, not us, sir. Lord Vetinari's orders. He insists that all condemned prisoners should be offered the prospect of freedom.\"\n\"Freedom? But there's a damn great stone through there!\"\n\"Yes, there is that, sir, yes, there is that,\" said the warden. \"It's only the prospect, you see. Not actual free freedom as such. Hah, that'd be a bit daft, eh?\"\n\"I suppose so, yes,\" said Moist. He didn't say \"you bastards.\" The wardens had treated him quite civilly these past six weeks, and he made a point of getting on with people. He was very, very good at it. People skills were part of his stock-in-trade; they were nearly the whole of it.\nBesides, these people had big sticks. So, speaking carefully, he added: \"Some people might consider this cruel, Mr. Wilkinson.\"\n\"Yes, sir, we asked him about that, sir, but he said no, it wasn't. He said it provided\"--his forehead wrinkled \"--occ-you-pay-shun-all ther-rap-py, healthy exercise, prevented moping, and offered that greatest of all treasures, which is Hope, sir.\"\n\"Hope,\" muttered Moist glumly.\n\"Not upset, are you, sir?\"\n\"Upset? Why should I be upset, Mr. Wilkinson?\"\n\"Only the last bloke we had in this cell, he managed to get down that drain, sir. Very small man. Very agile.\"\n\n\n\nAnd now we can start playing with the data 🙂",
    "crumbs": [
      "Python",
      "<b><em>Getting started</em></b>",
      "Playing with text"
    ]
  },
  {
    "objectID": "python/intro_text.html#counting-things",
    "href": "python/intro_text.html#counting-things",
    "title": "Playing with text",
    "section": "Counting things",
    "text": "Counting things\nOne of the things we can do with our text is counting things.\n\nCounting characters\nFor instance, we can count the number of characters thanks to the len function:\n\nlen(text)\n\n3294\n\n\nWe can count the number of occurrences of any sequence of character with the method count.\nFor instance, the see how many times the letter “e” appears in the text, we would run:\n\ntext.count(\"e\")\n\n301\n\n\nFor the name of the main character “Moist”, we would do:\n\ntext.count(\"Moist\")\n\n6\n\n\nNow, it is not very likely that a sentence\n\n\nCounting words\nOr we could try to see how many words there are in this text.\n\n\nYour turn:\n\nHow would you go about this?\n\nAnother method to count the number of words is to use the split method:\n\nwords = text.split()\nprint(words)\n\n['They', 'say', 'that', 'the', 'prospect', 'of', 'being', 'hanged', 'in', 'the', 'morning', 'concentrates', 'a', \"man's\", 'mind', 'wonderfully;', 'unfortunately,', 'what', 'the', 'mind', 'inevitably', 'concentrates', 'on', 'is', 'that,', 'in', 'the', 'morning,', 'it', 'will', 'be', 'in', 'a', 'body', 'that', 'is', 'going', 'to', 'be', 'hanged.', 'The', 'man', 'going', 'to', 'be', 'hanged', 'had', 'been', 'named', 'Moist', 'von', 'Lipwig', 'by', 'doting', 'if', 'unwise', 'parents,', 'but', 'he', 'was', 'not', 'going', 'to', 'embarrass', 'the', 'name,', 'insofar', 'as', 'that', 'was', 'still', 'possible,', 'by', 'being', 'hung', 'under', 'it.', 'To', 'the', 'world', 'in', 'general,', 'and', 'particularly', 'on', 'that', 'bit', 'of', 'it', 'known', 'as', 'the', 'death', 'warrant,', 'he', 'was', 'Alfred', 'Spangler.', 'And', 'he', 'took', 'a', 'more', 'positive', 'approach', 'to', 'the', 'situation', 'and', 'had', 'concentrated', 'his', 'mind', 'on', 'the', 'prospect', 'of', 'not', 'being', 'hanged', 'in', 'the', 'morning,', 'and,', 'most', 'particularly,', 'on', 'the', 'prospect', 'of', 'removing', 'all', 'the', 'crumbling', 'mortar', 'from', 'around', 'a', 'stone', 'in', 'his', 'cell', 'wall', 'with', 'a', 'spoon.', 'So', 'far', 'the', 'work', 'had', 'taken', 'him', 'five', 'weeks', 'and', 'reduced', 'the', 'spoon', 'to', 'something', 'like', 'a', 'nail', 'file.', 'Fortunately,', 'no', 'one', 'ever', 'came', 'to', 'change', 'the', 'bedding', 'here,', 'or', 'else', 'they', 'would', 'have', 'discovered', 'the', \"world's\", 'heaviest', 'mattress.', 'It', 'was', 'a', 'large', 'and', 'heavy', 'stone', 'that', 'was', 'currently', 'the', 'object', 'of', 'his', 'attentions,', 'and,', 'at', 'some', 'point,', 'a', 'huge', 'staple', 'had', 'been', 'hammered', 'into', 'it', 'as', 'an', 'anchor', 'for', 'manacles.', 'Moist', 'sat', 'down', 'facing', 'the', 'wall,', 'gripped', 'the', 'iron', 'ring', 'in', 'both', 'hands,', 'braced', 'his', 'legs', 'against', 'the', 'stones', 'on', 'either', 'side,', 'and', 'heaved.', 'His', 'shoulders', 'caught', 'fire,', 'and', 'a', 'red', 'mist', 'filled', 'his', 'vision,', 'but', 'the', 'block', 'slid', 'out', 'with', 'a', 'faint', 'and', 'inappropriate', 'tinkling', 'noise.', 'Moist', 'managed', 'to', 'ease', 'it', 'away', 'from', 'the', 'hole', 'and', 'peered', 'inside.', 'At', 'the', 'far', 'end', 'was', 'another', 'block,', 'and', 'the', 'mortar', 'around', 'it', 'looked', 'suspiciously', 'strong', 'and', 'fresh.', 'Just', 'in', 'front', 'of', 'it', 'was', 'a', 'new', 'spoon.', 'It', 'was', 'shiny.', 'As', 'he', 'studied', 'it,', 'he', 'heard', 'the', 'clapping', 'behind', 'him.', 'He', 'turned', 'his', 'head,', 'tendons', 'twanging', 'a', 'little', 'riff', 'of', 'agony,', 'and', 'saw', 'several', 'of', 'the', 'wardens', 'watching', 'him', 'through', 'the', 'bars.', '\"Well', 'done,', 'Mr.', 'Spangler!\"', 'said', 'one', 'of', 'them.', '\"Ron', 'here', 'owes', 'me', 'five', 'dollars!', 'I', 'told', 'him', 'you', 'were', 'a', 'sticker!!', \"'He's\", 'a', \"sticker,'\", 'I', 'said!\"', '\"You', 'set', 'this', 'up,', 'did', 'you,', 'Mr.', 'Wilkinson?\"', 'said', 'Moist', 'weakly,', 'watching', 'the', 'glint', 'of', 'light', 'on', 'the', 'spoon.', '\"Oh,', 'not', 'us,', 'sir.', 'Lord', \"Vetinari's\", 'orders.', 'He', 'insists', 'that', 'all', 'condemned', 'prisoners', 'should', 'be', 'offered', 'the', 'prospect', 'of', 'freedom.\"', '\"Freedom?', 'But', \"there's\", 'a', 'damn', 'great', 'stone', 'through', 'there!\"', '\"Yes,', 'there', 'is', 'that,', 'sir,', 'yes,', 'there', 'is', 'that,\"', 'said', 'the', 'warden.', '\"It\\'s', 'only', 'the', 'prospect,', 'you', 'see.', 'Not', 'actual', 'free', 'freedom', 'as', 'such.', 'Hah,', \"that'd\", 'be', 'a', 'bit', 'daft,', 'eh?\"', '\"I', 'suppose', 'so,', 'yes,\"', 'said', 'Moist.', 'He', \"didn't\", 'say', '\"you', 'bastards.\"', 'The', 'wardens', 'had', 'treated', 'him', 'quite', 'civilly', 'these', 'past', 'six', 'weeks,', 'and', 'he', 'made', 'a', 'point', 'of', 'getting', 'on', 'with', 'people.', 'He', 'was', 'very,', 'very', 'good', 'at', 'it.', 'People', 'skills', 'were', 'part', 'of', 'his', 'stock-in-trade;', 'they', 'were', 'nearly', 'the', 'whole', 'of', 'it.', 'Besides,', 'these', 'people', 'had', 'big', 'sticks.', 'So,', 'speaking', 'carefully,', 'he', 'added:', '\"Some', 'people', 'might', 'consider', 'this', 'cruel,', 'Mr.', 'Wilkinson.\"', '\"Yes,', 'sir,', 'we', 'asked', 'him', 'about', 'that,', 'sir,', 'but', 'he', 'said', 'no,', 'it', \"wasn't.\", 'He', 'said', 'it', 'provided\"--his', 'forehead', 'wrinkled', '\"--occ-you-pay-shun-all', 'ther-rap-py,', 'healthy', 'exercise,', 'prevented', 'moping,', 'and', 'offered', 'that', 'greatest', 'of', 'all', 'treasures,', 'which', 'is', 'Hope,', 'sir.\"', '\"Hope,\"', 'muttered', 'Moist', 'glumly.', '\"Not', 'upset,', 'are', 'you,', 'sir?\"', '\"Upset?', 'Why', 'should', 'I', 'be', 'upset,', 'Mr.', 'Wilkinson?\"', '\"Only', 'the', 'last', 'bloke', 'we', 'had', 'in', 'this', 'cell,', 'he', 'managed', 'to', 'get', 'down', 'that', 'drain,', 'sir.', 'Very', 'small', 'man.', 'Very', 'agile.\"']\n\n\n\n\nYour turn:\n\nWhat is the type of the variable words?\n\nTo get its length, we can use the len function:\n\nlen(words)\n\n590\n\n\nNow, let’s try to count how many times the word the is in the text.\n\n\nYour turn:\n\nWe could use:\n\ntext.count(\"the\") + text.count(\"The\")\n\n49\n\n\nbut it won’t answer our question. Why?\n\nInstead, we should use the list of words that we called words and count how many of them are equal to the. We do this with a loop:\n\n# We set our counter (the number of occurrences) to zero:\noccurrences = 0\n\n# And now we can use a loop to test the words one by one and add 1 to our counter each time the equality returns true\nfor word in words:\n    if word == \"the\" or word == \"The\":\n        occurrences += 1\n\nprint(occurrences)\n\n36\n\n\n\nAn alternative syntax that looks a lot more elegant is the following:\n\nsum(word == \"the\" or word == \"The\" for word in words)\n\n36\n\n\nHowever, elegance and short syntax don’t necessarily mean fast code.\nWe can benchmark Python code very easy when we use Jupyter or IPython by using the magic %%timeit at the top of a code cell.\nLet’s try it:\n%%timeit\n\n# We set our counter (the number of occurrences) to zero:\noccurrences = 0\n\n# And now we can use a loop to test the words one by one and add 1 to our counter each time the equality returns true\nfor word in words:\n    if word == \"the\" or word == \"The\":\n        occurrences += 1\n9.52 μs ± 510 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n\nI removed the print function so that we don’t end up printing the result a bunch of times: timeit does a lot of tests and takes the average. At each run, we would have a printed result!\n\nAnd for the other method\n%%timeit\n\noccurrences = sum(word == \"the\" or word == \"The\" for word in words)\n24.2 μs ± 243 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n\nTo make a fair comparison with the previous expression, I am not printing the result here either, but assigning it to a variable.\n\nAs you can see, the short neat-looking expression takes more than twice the time of the not so nice-looking one. Without benchmarking, it is very hard to predict what code is efficient.\n\n\nRemoving punctuation\nNow, let’s count the number of times the word “sir” occurs in the text:\n\noccurrences = 0\n\nfor word in words:\n    if word == \"sir\" or word == \"Sir\":\n        occurrences += 1\n\nprint(occurrences)\n\n0\n\n\nMmm… that is strange because, if we read the text, we can see that the word “sir” actually occurs in the text…\nLooking carefully at our list words, we can see what the problem is: the word “sir” appears as sir., sir,, sir.\", sir?\".\nThis shows that in order to do a cleaner job and get our method to work for any word, we need to remove the punctuation.\nStep one, we remove the punctuation from our text string:\n\nimport string\n\nclean_text = text.translate(str.maketrans('', '', string.punctuation))\nprint(clean_text)\n\nThey say that the prospect of being hanged in the morning concentrates a mans mind wonderfully unfortunately what the mind inevitably concentrates on is that in the morning it will be in a body that is going to be hanged\nThe man going to be hanged had been named Moist von Lipwig by doting if unwise parents but he was not going to embarrass the name insofar as that was still possible by being hung under it To the world in general and particularly on that bit of it known as the death warrant he was Alfred Spangler\nAnd he took a more positive approach to the situation and had concentrated his mind on the prospect of not being hanged in the morning and most particularly on the prospect of removing all the crumbling mortar from around a stone in his cell wall with a spoon So far the work had taken him five weeks and reduced the spoon to something like a nail file Fortunately no one ever came to change the bedding here or else they would have discovered the worlds heaviest mattress\nIt was a large and heavy stone that was currently the object of his attentions and at some point a huge staple had been hammered into it as an anchor for manacles\nMoist sat down facing the wall gripped the iron ring in both hands braced his legs against the stones on either side and heaved\nHis shoulders caught fire and a red mist filled his vision but the block slid out with a faint and inappropriate tinkling noise Moist managed to ease it away from the hole and peered inside\nAt the far end was another block and the mortar around it looked suspiciously strong and fresh\nJust in front of it was a new spoon It was shiny\nAs he studied it he heard the clapping behind him He turned his head tendons twanging a little riff of agony and saw several of the wardens watching him through the bars\nWell done Mr Spangler said one of them Ron here owes me five dollars I told him you were a sticker Hes a sticker I said\nYou set this up did you Mr Wilkinson said Moist weakly watching the glint of light on the spoon\nOh not us sir Lord Vetinaris orders He insists that all condemned prisoners should be offered the prospect of freedom\nFreedom But theres a damn great stone through there\nYes there is that sir yes there is that said the warden Its only the prospect you see Not actual free freedom as such Hah thatd be a bit daft eh\nI suppose so yes said Moist He didnt say you bastards The wardens had treated him quite civilly these past six weeks and he made a point of getting on with people He was very very good at it People skills were part of his stockintrade they were nearly the whole of it\nBesides these people had big sticks So speaking carefully he added Some people might consider this cruel Mr Wilkinson\nYes sir we asked him about that sir but he said no it wasnt He said it providedhis forehead wrinkled occyoupayshunall therrappy healthy exercise prevented moping and offered that greatest of all treasures which is Hope sir\nHope muttered Moist glumly\nNot upset are you sir\nUpset Why should I be upset Mr Wilkinson\nOnly the last bloke we had in this cell he managed to get down that drain sir Very small man Very agile\n\n\n\nAnd now we split it into words:\n\nclean_words = clean_text.split()\nprint(clean_words)\n\n['They', 'say', 'that', 'the', 'prospect', 'of', 'being', 'hanged', 'in', 'the', 'morning', 'concentrates', 'a', 'mans', 'mind', 'wonderfully', 'unfortunately', 'what', 'the', 'mind', 'inevitably', 'concentrates', 'on', 'is', 'that', 'in', 'the', 'morning', 'it', 'will', 'be', 'in', 'a', 'body', 'that', 'is', 'going', 'to', 'be', 'hanged', 'The', 'man', 'going', 'to', 'be', 'hanged', 'had', 'been', 'named', 'Moist', 'von', 'Lipwig', 'by', 'doting', 'if', 'unwise', 'parents', 'but', 'he', 'was', 'not', 'going', 'to', 'embarrass', 'the', 'name', 'insofar', 'as', 'that', 'was', 'still', 'possible', 'by', 'being', 'hung', 'under', 'it', 'To', 'the', 'world', 'in', 'general', 'and', 'particularly', 'on', 'that', 'bit', 'of', 'it', 'known', 'as', 'the', 'death', 'warrant', 'he', 'was', 'Alfred', 'Spangler', 'And', 'he', 'took', 'a', 'more', 'positive', 'approach', 'to', 'the', 'situation', 'and', 'had', 'concentrated', 'his', 'mind', 'on', 'the', 'prospect', 'of', 'not', 'being', 'hanged', 'in', 'the', 'morning', 'and', 'most', 'particularly', 'on', 'the', 'prospect', 'of', 'removing', 'all', 'the', 'crumbling', 'mortar', 'from', 'around', 'a', 'stone', 'in', 'his', 'cell', 'wall', 'with', 'a', 'spoon', 'So', 'far', 'the', 'work', 'had', 'taken', 'him', 'five', 'weeks', 'and', 'reduced', 'the', 'spoon', 'to', 'something', 'like', 'a', 'nail', 'file', 'Fortunately', 'no', 'one', 'ever', 'came', 'to', 'change', 'the', 'bedding', 'here', 'or', 'else', 'they', 'would', 'have', 'discovered', 'the', 'worlds', 'heaviest', 'mattress', 'It', 'was', 'a', 'large', 'and', 'heavy', 'stone', 'that', 'was', 'currently', 'the', 'object', 'of', 'his', 'attentions', 'and', 'at', 'some', 'point', 'a', 'huge', 'staple', 'had', 'been', 'hammered', 'into', 'it', 'as', 'an', 'anchor', 'for', 'manacles', 'Moist', 'sat', 'down', 'facing', 'the', 'wall', 'gripped', 'the', 'iron', 'ring', 'in', 'both', 'hands', 'braced', 'his', 'legs', 'against', 'the', 'stones', 'on', 'either', 'side', 'and', 'heaved', 'His', 'shoulders', 'caught', 'fire', 'and', 'a', 'red', 'mist', 'filled', 'his', 'vision', 'but', 'the', 'block', 'slid', 'out', 'with', 'a', 'faint', 'and', 'inappropriate', 'tinkling', 'noise', 'Moist', 'managed', 'to', 'ease', 'it', 'away', 'from', 'the', 'hole', 'and', 'peered', 'inside', 'At', 'the', 'far', 'end', 'was', 'another', 'block', 'and', 'the', 'mortar', 'around', 'it', 'looked', 'suspiciously', 'strong', 'and', 'fresh', 'Just', 'in', 'front', 'of', 'it', 'was', 'a', 'new', 'spoon', 'It', 'was', 'shiny', 'As', 'he', 'studied', 'it', 'he', 'heard', 'the', 'clapping', 'behind', 'him', 'He', 'turned', 'his', 'head', 'tendons', 'twanging', 'a', 'little', 'riff', 'of', 'agony', 'and', 'saw', 'several', 'of', 'the', 'wardens', 'watching', 'him', 'through', 'the', 'bars', 'Well', 'done', 'Mr', 'Spangler', 'said', 'one', 'of', 'them', 'Ron', 'here', 'owes', 'me', 'five', 'dollars', 'I', 'told', 'him', 'you', 'were', 'a', 'sticker', 'Hes', 'a', 'sticker', 'I', 'said', 'You', 'set', 'this', 'up', 'did', 'you', 'Mr', 'Wilkinson', 'said', 'Moist', 'weakly', 'watching', 'the', 'glint', 'of', 'light', 'on', 'the', 'spoon', 'Oh', 'not', 'us', 'sir', 'Lord', 'Vetinaris', 'orders', 'He', 'insists', 'that', 'all', 'condemned', 'prisoners', 'should', 'be', 'offered', 'the', 'prospect', 'of', 'freedom', 'Freedom', 'But', 'theres', 'a', 'damn', 'great', 'stone', 'through', 'there', 'Yes', 'there', 'is', 'that', 'sir', 'yes', 'there', 'is', 'that', 'said', 'the', 'warden', 'Its', 'only', 'the', 'prospect', 'you', 'see', 'Not', 'actual', 'free', 'freedom', 'as', 'such', 'Hah', 'thatd', 'be', 'a', 'bit', 'daft', 'eh', 'I', 'suppose', 'so', 'yes', 'said', 'Moist', 'He', 'didnt', 'say', 'you', 'bastards', 'The', 'wardens', 'had', 'treated', 'him', 'quite', 'civilly', 'these', 'past', 'six', 'weeks', 'and', 'he', 'made', 'a', 'point', 'of', 'getting', 'on', 'with', 'people', 'He', 'was', 'very', 'very', 'good', 'at', 'it', 'People', 'skills', 'were', 'part', 'of', 'his', 'stockintrade', 'they', 'were', 'nearly', 'the', 'whole', 'of', 'it', 'Besides', 'these', 'people', 'had', 'big', 'sticks', 'So', 'speaking', 'carefully', 'he', 'added', 'Some', 'people', 'might', 'consider', 'this', 'cruel', 'Mr', 'Wilkinson', 'Yes', 'sir', 'we', 'asked', 'him', 'about', 'that', 'sir', 'but', 'he', 'said', 'no', 'it', 'wasnt', 'He', 'said', 'it', 'providedhis', 'forehead', 'wrinkled', 'occyoupayshunall', 'therrappy', 'healthy', 'exercise', 'prevented', 'moping', 'and', 'offered', 'that', 'greatest', 'of', 'all', 'treasures', 'which', 'is', 'Hope', 'sir', 'Hope', 'muttered', 'Moist', 'glumly', 'Not', 'upset', 'are', 'you', 'sir', 'Upset', 'Why', 'should', 'I', 'be', 'upset', 'Mr', 'Wilkinson', 'Only', 'the', 'last', 'bloke', 'we', 'had', 'in', 'this', 'cell', 'he', 'managed', 'to', 'get', 'down', 'that', 'drain', 'sir', 'Very', 'small', 'man', 'Very', 'agile']\n\n\nThis is a much better list to work from and this one will work for any word. For the word “sir” for instance, we would do:\n\noccurrences = 0\n\nfor word in clean_words:\n    if word == \"sir\" or word == \"Sir\":\n        occurrences += 1\n\nprint(occurrences)\n\n7\n\n\n\n\nRemoving case\nNow, having to look for the word of interest with and without capital letter as we have been doing so far is not the most robust method: what if the text had “SIR” in all caps? After all, Death in Pratchett novels speaks in all caps! Of course, we could add this as a third option (if word == \"sir\" or word == \"Sir\" or word == \"SIR\"), but that is becoming a little tedious.\nA better solution is to turn the whole text into lower case before splitting it into words. That way we don’t have to worry about case.\nLet’s remove all capital letters:\n\nfinal_text = clean_text.lower()\n\nNow we split it into words:\n\nfinal_words = final_text.split()\nprint(final_words)\n\n['they', 'say', 'that', 'the', 'prospect', 'of', 'being', 'hanged', 'in', 'the', 'morning', 'concentrates', 'a', 'mans', 'mind', 'wonderfully', 'unfortunately', 'what', 'the', 'mind', 'inevitably', 'concentrates', 'on', 'is', 'that', 'in', 'the', 'morning', 'it', 'will', 'be', 'in', 'a', 'body', 'that', 'is', 'going', 'to', 'be', 'hanged', 'the', 'man', 'going', 'to', 'be', 'hanged', 'had', 'been', 'named', 'moist', 'von', 'lipwig', 'by', 'doting', 'if', 'unwise', 'parents', 'but', 'he', 'was', 'not', 'going', 'to', 'embarrass', 'the', 'name', 'insofar', 'as', 'that', 'was', 'still', 'possible', 'by', 'being', 'hung', 'under', 'it', 'to', 'the', 'world', 'in', 'general', 'and', 'particularly', 'on', 'that', 'bit', 'of', 'it', 'known', 'as', 'the', 'death', 'warrant', 'he', 'was', 'alfred', 'spangler', 'and', 'he', 'took', 'a', 'more', 'positive', 'approach', 'to', 'the', 'situation', 'and', 'had', 'concentrated', 'his', 'mind', 'on', 'the', 'prospect', 'of', 'not', 'being', 'hanged', 'in', 'the', 'morning', 'and', 'most', 'particularly', 'on', 'the', 'prospect', 'of', 'removing', 'all', 'the', 'crumbling', 'mortar', 'from', 'around', 'a', 'stone', 'in', 'his', 'cell', 'wall', 'with', 'a', 'spoon', 'so', 'far', 'the', 'work', 'had', 'taken', 'him', 'five', 'weeks', 'and', 'reduced', 'the', 'spoon', 'to', 'something', 'like', 'a', 'nail', 'file', 'fortunately', 'no', 'one', 'ever', 'came', 'to', 'change', 'the', 'bedding', 'here', 'or', 'else', 'they', 'would', 'have', 'discovered', 'the', 'worlds', 'heaviest', 'mattress', 'it', 'was', 'a', 'large', 'and', 'heavy', 'stone', 'that', 'was', 'currently', 'the', 'object', 'of', 'his', 'attentions', 'and', 'at', 'some', 'point', 'a', 'huge', 'staple', 'had', 'been', 'hammered', 'into', 'it', 'as', 'an', 'anchor', 'for', 'manacles', 'moist', 'sat', 'down', 'facing', 'the', 'wall', 'gripped', 'the', 'iron', 'ring', 'in', 'both', 'hands', 'braced', 'his', 'legs', 'against', 'the', 'stones', 'on', 'either', 'side', 'and', 'heaved', 'his', 'shoulders', 'caught', 'fire', 'and', 'a', 'red', 'mist', 'filled', 'his', 'vision', 'but', 'the', 'block', 'slid', 'out', 'with', 'a', 'faint', 'and', 'inappropriate', 'tinkling', 'noise', 'moist', 'managed', 'to', 'ease', 'it', 'away', 'from', 'the', 'hole', 'and', 'peered', 'inside', 'at', 'the', 'far', 'end', 'was', 'another', 'block', 'and', 'the', 'mortar', 'around', 'it', 'looked', 'suspiciously', 'strong', 'and', 'fresh', 'just', 'in', 'front', 'of', 'it', 'was', 'a', 'new', 'spoon', 'it', 'was', 'shiny', 'as', 'he', 'studied', 'it', 'he', 'heard', 'the', 'clapping', 'behind', 'him', 'he', 'turned', 'his', 'head', 'tendons', 'twanging', 'a', 'little', 'riff', 'of', 'agony', 'and', 'saw', 'several', 'of', 'the', 'wardens', 'watching', 'him', 'through', 'the', 'bars', 'well', 'done', 'mr', 'spangler', 'said', 'one', 'of', 'them', 'ron', 'here', 'owes', 'me', 'five', 'dollars', 'i', 'told', 'him', 'you', 'were', 'a', 'sticker', 'hes', 'a', 'sticker', 'i', 'said', 'you', 'set', 'this', 'up', 'did', 'you', 'mr', 'wilkinson', 'said', 'moist', 'weakly', 'watching', 'the', 'glint', 'of', 'light', 'on', 'the', 'spoon', 'oh', 'not', 'us', 'sir', 'lord', 'vetinaris', 'orders', 'he', 'insists', 'that', 'all', 'condemned', 'prisoners', 'should', 'be', 'offered', 'the', 'prospect', 'of', 'freedom', 'freedom', 'but', 'theres', 'a', 'damn', 'great', 'stone', 'through', 'there', 'yes', 'there', 'is', 'that', 'sir', 'yes', 'there', 'is', 'that', 'said', 'the', 'warden', 'its', 'only', 'the', 'prospect', 'you', 'see', 'not', 'actual', 'free', 'freedom', 'as', 'such', 'hah', 'thatd', 'be', 'a', 'bit', 'daft', 'eh', 'i', 'suppose', 'so', 'yes', 'said', 'moist', 'he', 'didnt', 'say', 'you', 'bastards', 'the', 'wardens', 'had', 'treated', 'him', 'quite', 'civilly', 'these', 'past', 'six', 'weeks', 'and', 'he', 'made', 'a', 'point', 'of', 'getting', 'on', 'with', 'people', 'he', 'was', 'very', 'very', 'good', 'at', 'it', 'people', 'skills', 'were', 'part', 'of', 'his', 'stockintrade', 'they', 'were', 'nearly', 'the', 'whole', 'of', 'it', 'besides', 'these', 'people', 'had', 'big', 'sticks', 'so', 'speaking', 'carefully', 'he', 'added', 'some', 'people', 'might', 'consider', 'this', 'cruel', 'mr', 'wilkinson', 'yes', 'sir', 'we', 'asked', 'him', 'about', 'that', 'sir', 'but', 'he', 'said', 'no', 'it', 'wasnt', 'he', 'said', 'it', 'providedhis', 'forehead', 'wrinkled', 'occyoupayshunall', 'therrappy', 'healthy', 'exercise', 'prevented', 'moping', 'and', 'offered', 'that', 'greatest', 'of', 'all', 'treasures', 'which', 'is', 'hope', 'sir', 'hope', 'muttered', 'moist', 'glumly', 'not', 'upset', 'are', 'you', 'sir', 'upset', 'why', 'should', 'i', 'be', 'upset', 'mr', 'wilkinson', 'only', 'the', 'last', 'bloke', 'we', 'had', 'in', 'this', 'cell', 'he', 'managed', 'to', 'get', 'down', 'that', 'drain', 'sir', 'very', 'small', 'man', 'very', 'agile']\n\n\n\n\nYour turn:\n\nWhat would the code look like now to count the number of times the word “sir” appears?\n\n\n\n\nCounting unique words\nYet something else we can count is the number of unique words in the text. The simplest way to do this is to turn our list of words into a set and see how many elements this set contains:\n\nlen(set(final_words))\n\n292",
    "crumbs": [
      "Python",
      "<b><em>Getting started</em></b>",
      "Playing with text"
    ]
  },
  {
    "objectID": "python/intro_text.html#extracting-characters-from-strings",
    "href": "python/intro_text.html#extracting-characters-from-strings",
    "title": "Playing with text",
    "section": "Extracting characters from strings",
    "text": "Extracting characters from strings\n\nIndexing\nLet’s go back to our text. Remember that we have this object text which is a list.\n\ntype(text)\n\nstr\n\n\nYou can extract characters from strings by indexing.\nIndexing in Python is done with square brackets and starts at 0 (the first element has index 0). This means that we can extract the first character with:\n\nprint(text[0])\n\nT\n\n\n\n\nYour turn:\n\nHow would you index the 4th element? Try it out. It should return “y”.\n\nYou can extract the last element with a minus sign (and this time, the indexing starts at 1):\n\nprint(text[-1])\n\n\n\n\n\nWe aren’t getting any output here because the last character is the special character \\n which encodes for a line break. You can see it when you don’t use the print function (print makes things look nicer and transforms those characters into what they represent):\n\ntext[-1]\n\n'\\n'\n\n\n\n\nYour turn:\n\nQuestion 1:\nHow would you get the last letter of the text?\nQuestion 2:\nHow would you index the 11th element from the end? Give it a try. You should get “V”.\n\n\n\nSlicing\nYou can also extract multiple contiguous elements with a slice. A slice is also defined with square brackets, but this time you add a colon in it. Left of the colon is the start of the slice and right of the colon is the end of the slice.\nIn Python, the left element of a slice is included, but the right element is excluded.\nFirst, let’s omit both indices on either side of the colon:\n\nprint(text[:])\n\nThey say that the prospect of being hanged in the morning concentrates a man's mind wonderfully; unfortunately, what the mind inevitably concentrates on is that, in the morning, it will be in a body that is going to be hanged.\nThe man going to be hanged had been named Moist von Lipwig by doting if unwise parents, but he was not going to embarrass the name, insofar as that was still possible, by being hung under it. To the world in general, and particularly on that bit of it known as the death warrant, he was Alfred Spangler.\nAnd he took a more positive approach to the situation and had concentrated his mind on the prospect of not being hanged in the morning, and, most particularly, on the prospect of removing all the crumbling mortar from around a stone in his cell wall with a spoon. So far the work had taken him five weeks and reduced the spoon to something like a nail file. Fortunately, no one ever came to change the bedding here, or else they would have discovered the world's heaviest mattress.\nIt was a large and heavy stone that was currently the object of his attentions, and, at some point, a huge staple had been hammered into it as an anchor for manacles.\nMoist sat down facing the wall, gripped the iron ring in both hands, braced his legs against the stones on either side, and heaved.\nHis shoulders caught fire, and a red mist filled his vision, but the block slid out with a faint and inappropriate tinkling noise. Moist managed to ease it away from the hole and peered inside.\nAt the far end was another block, and the mortar around it looked suspiciously strong and fresh.\nJust in front of it was a new spoon. It was shiny.\nAs he studied it, he heard the clapping behind him. He turned his head, tendons twanging a little riff of agony, and saw several of the wardens watching him through the bars.\n\"Well done, Mr. Spangler!\" said one of them. \"Ron here owes me five dollars! I told him you were a sticker!! 'He's a sticker,' I said!\"\n\"You set this up, did you, Mr. Wilkinson?\" said Moist weakly, watching the glint of light on the spoon.\n\"Oh, not us, sir. Lord Vetinari's orders. He insists that all condemned prisoners should be offered the prospect of freedom.\"\n\"Freedom? But there's a damn great stone through there!\"\n\"Yes, there is that, sir, yes, there is that,\" said the warden. \"It's only the prospect, you see. Not actual free freedom as such. Hah, that'd be a bit daft, eh?\"\n\"I suppose so, yes,\" said Moist. He didn't say \"you bastards.\" The wardens had treated him quite civilly these past six weeks, and he made a point of getting on with people. He was very, very good at it. People skills were part of his stock-in-trade; they were nearly the whole of it.\nBesides, these people had big sticks. So, speaking carefully, he added: \"Some people might consider this cruel, Mr. Wilkinson.\"\n\"Yes, sir, we asked him about that, sir, but he said no, it wasn't. He said it provided\"--his forehead wrinkled \"--occ-you-pay-shun-all ther-rap-py, healthy exercise, prevented moping, and offered that greatest of all treasures, which is Hope, sir.\"\n\"Hope,\" muttered Moist glumly.\n\"Not upset, are you, sir?\"\n\"Upset? Why should I be upset, Mr. Wilkinson?\"\n\"Only the last bloke we had in this cell, he managed to get down that drain, sir. Very small man. Very agile.\"\n\n\n\nThis returns the full text. This is because when a slice boundary is omitted, by default it starts at the very beginning of the object you are slicing.\nWe can test that we indeed get the full text by comparing it to the non-sliced version of text:\n\ntext[:] == text\n\nTrue\n\n\nNow, let’s slice the first 10 elements of text:\n\nprint(text[:10])\n\nThey say t\n\n\n\nLet’s explain this code a bit:\nWe want our slice to start at the beginning of the text, so we are omitting that boundary (we could also use 0 left of the colon).\nBecause indexing starts at 0, the 10th element is actually not “t”, but the following “h”. The reason we get “t” rather than “h” is because the right boundary of a slice is excluded.\n\n\n\nYour turn:\n\nQuestion 1:\nTry to write some code that will return “prospect”.\n\n\n\nQuestion 2:\nNow, remember how we created the words object earlier? Try to use it to get the same result.\n\n\n\n\n\n\nStriding\nA last way to extract characters out of a string is to use strides. A stride is defined with square brackets and 3 values separated by colons. The first value is the left boundary (included), the second value is the right boundary (excluded), and the third value is the step. By default (if omitted), the step is 1.\n\n\nYour turn:\n\nQuestion 1:\nWhat do you think that text[::] would return?\nQuestion 2:\nHow would you test it?\nQuestion 3:\nHow would you get every 3rd character of the whole text?\n\nNow, a fun one: the step can also take a negative value. With -1, we get the text backward! This is because - indicates that we want to step from the end and 1 means that we want every character:\n\nprint(text[::-1])\n\n\n\".eliga yreV .nam llams yreV .ris ,niard taht nwod teg ot deganam eh ,llec siht ni dah ew ekolb tsal eht ylnO\"\n\"?nosnikliW .rM ,tespu eb I dluohs yhW ?tespU\"\n\"?ris ,uoy era ,tespu toN\"\n.ylmulg tsioM derettum \",epoH\"\n\".ris ,epoH si hcihw ,serusaert lla fo tsetaerg taht dereffo dna ,gnipom detneverp ,esicrexe yhtlaeh ,yp-par-reht lla-nuhs-yap-uoy-cco--\" delknirw daeherof sih--\"dedivorp ti dias eH .t'nsaw ti ,on dias eh tub ,ris ,taht tuoba mih deksa ew ,ris ,seY\"\n\".nosnikliW .rM ,leurc siht redisnoc thgim elpoep emoS\" :dedda eh ,ylluferac gnikaeps ,oS .skcits gib dah elpoep eseht ,sediseB\n.ti fo elohw eht ylraen erew yeht ;edart-ni-kcots sih fo trap erew slliks elpoeP .ti ta doog yrev ,yrev saw eH .elpoep htiw no gnitteg fo tniop a edam eh dna ,skeew xis tsap eseht yllivic etiuq mih detaert dah snedraw ehT \".sdratsab uoy\" yas t'ndid eH .tsioM dias \",sey ,os esoppus I\"\n\"?he ,tfad tib a eb d'taht ,haH .hcus sa modeerf eerf lautca toN .ees uoy ,tcepsorp eht ylno s'tI\" .nedraw eht dias \",taht si ereht ,sey ,ris ,taht si ereht ,seY\"\n\"!ereht hguorht enots taerg nmad a s'ereht tuB ?modeerF\"\n\".modeerf fo tcepsorp eht dereffo eb dluohs srenosirp denmednoc lla taht stsisni eH .sredro s'iraniteV droL .ris ,su ton ,hO\"\n.noops eht no thgil fo tnilg eht gnihctaw ,ylkaew tsioM dias \"?nosnikliW .rM ,uoy did ,pu siht tes uoY\"\n\"!dias I ',rekcits a s'eH' !!rekcits a erew uoy mih dlot I !srallod evif em sewo ereh noR\" .meht fo eno dias \"!relgnapS .rM ,enod lleW\"\n.srab eht hguorht mih gnihctaw snedraw eht fo lareves was dna ,ynoga fo ffir elttil a gnignawt snodnet ,daeh sih denrut eH .mih dniheb gnippalc eht draeh eh ,ti deiduts eh sA\n.ynihs saw tI .noops wen a saw ti fo tnorf ni tsuJ\n.hserf dna gnorts ylsuoicipsus dekool ti dnuora ratrom eht dna ,kcolb rehtona saw dne raf eht tA\n.edisni dereep dna eloh eht morf yawa ti esae ot deganam tsioM .esion gnilknit etairporppani dna tniaf a htiw tuo dils kcolb eht tub ,noisiv sih dellif tsim der a dna ,erif thguac sredluohs siH\n.devaeh dna ,edis rehtie no senots eht tsniaga sgel sih decarb ,sdnah htob ni gnir nori eht deppirg ,llaw eht gnicaf nwod tas tsioM\n.selcanam rof rohcna na sa ti otni deremmah neeb dah elpats eguh a ,tniop emos ta ,dna ,snoitnetta sih fo tcejbo eht yltnerruc saw taht enots yvaeh dna egral a saw tI\n.sserttam tseivaeh s'dlrow eht derevocsid evah dluow yeht esle ro ,ereh gniddeb eht egnahc ot emac reve eno on ,yletanutroF .elif lian a ekil gnihtemos ot noops eht decuder dna skeew evif mih nekat dah krow eht raf oS .noops a htiw llaw llec sih ni enots a dnuora morf ratrom gnilbmurc eht lla gnivomer fo tcepsorp eht no ,ylralucitrap tsom ,dna ,gninrom eht ni degnah gnieb ton fo tcepsorp eht no dnim sih detartnecnoc dah dna noitautis eht ot hcaorppa evitisop erom a koot eh dnA\n.relgnapS derflA saw eh ,tnarraw htaed eht sa nwonk ti fo tib taht no ylralucitrap dna ,lareneg ni dlrow eht oT .ti rednu gnuh gnieb yb ,elbissop llits saw taht sa rafosni ,eman eht ssarrabme ot gniog ton saw eh tub ,stnerap esiwnu fi gnitod yb giwpiL nov tsioM deman neeb dah degnah eb ot gniog nam ehT\n.degnah eb ot gniog si taht ydob a ni eb lliw ti ,gninrom eht ni ,taht si no setartnecnoc ylbativeni dnim eht tahw ,yletanutrofnu ;yllufrednow dnim s'nam a setartnecnoc gninrom eht ni degnah gnieb fo tcepsorp eht taht yas yehT",
    "crumbs": [
      "Python",
      "<b><em>Getting started</em></b>",
      "Playing with text"
    ]
  },
  {
    "objectID": "python/intro_text.html#string-concatenation",
    "href": "python/intro_text.html#string-concatenation",
    "title": "Playing with text",
    "section": "String concatenation",
    "text": "String concatenation\nStrings are fun because they can be concatenated with the operator +:\n\nprint(\"This is the beginning of Going Postal:\" + \"\\n\\n\" + text)\n\nThis is the beginning of Going Postal:\n\nThey say that the prospect of being hanged in the morning concentrates a man's mind wonderfully; unfortunately, what the mind inevitably concentrates on is that, in the morning, it will be in a body that is going to be hanged.\nThe man going to be hanged had been named Moist von Lipwig by doting if unwise parents, but he was not going to embarrass the name, insofar as that was still possible, by being hung under it. To the world in general, and particularly on that bit of it known as the death warrant, he was Alfred Spangler.\nAnd he took a more positive approach to the situation and had concentrated his mind on the prospect of not being hanged in the morning, and, most particularly, on the prospect of removing all the crumbling mortar from around a stone in his cell wall with a spoon. So far the work had taken him five weeks and reduced the spoon to something like a nail file. Fortunately, no one ever came to change the bedding here, or else they would have discovered the world's heaviest mattress.\nIt was a large and heavy stone that was currently the object of his attentions, and, at some point, a huge staple had been hammered into it as an anchor for manacles.\nMoist sat down facing the wall, gripped the iron ring in both hands, braced his legs against the stones on either side, and heaved.\nHis shoulders caught fire, and a red mist filled his vision, but the block slid out with a faint and inappropriate tinkling noise. Moist managed to ease it away from the hole and peered inside.\nAt the far end was another block, and the mortar around it looked suspiciously strong and fresh.\nJust in front of it was a new spoon. It was shiny.\nAs he studied it, he heard the clapping behind him. He turned his head, tendons twanging a little riff of agony, and saw several of the wardens watching him through the bars.\n\"Well done, Mr. Spangler!\" said one of them. \"Ron here owes me five dollars! I told him you were a sticker!! 'He's a sticker,' I said!\"\n\"You set this up, did you, Mr. Wilkinson?\" said Moist weakly, watching the glint of light on the spoon.\n\"Oh, not us, sir. Lord Vetinari's orders. He insists that all condemned prisoners should be offered the prospect of freedom.\"\n\"Freedom? But there's a damn great stone through there!\"\n\"Yes, there is that, sir, yes, there is that,\" said the warden. \"It's only the prospect, you see. Not actual free freedom as such. Hah, that'd be a bit daft, eh?\"\n\"I suppose so, yes,\" said Moist. He didn't say \"you bastards.\" The wardens had treated him quite civilly these past six weeks, and he made a point of getting on with people. He was very, very good at it. People skills were part of his stock-in-trade; they were nearly the whole of it.\nBesides, these people had big sticks. So, speaking carefully, he added: \"Some people might consider this cruel, Mr. Wilkinson.\"\n\"Yes, sir, we asked him about that, sir, but he said no, it wasn't. He said it provided\"--his forehead wrinkled \"--occ-you-pay-shun-all ther-rap-py, healthy exercise, prevented moping, and offered that greatest of all treasures, which is Hope, sir.\"\n\"Hope,\" muttered Moist glumly.\n\"Not upset, are you, sir?\"\n\"Upset? Why should I be upset, Mr. Wilkinson?\"\n\"Only the last bloke we had in this cell, he managed to get down that drain, sir. Very small man. Very agile.\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf you want to go much beyond this (e.g. sentences tokenization, natural language processing (NLP), etc.), you probably want to install a library for this such as NLTK or spaCy.",
    "crumbs": [
      "Python",
      "<b><em>Getting started</em></b>",
      "Playing with text"
    ]
  },
  {
    "objectID": "python/nlp_data.html",
    "href": "python/nlp_data.html",
    "title": "Getting the data",
    "section": "",
    "text": "In this section, we will import the pdf of a book from an online URL into Python.",
    "crumbs": [
      "Python",
      "<b><em>Text analysis</em></b>",
      "Getting the data"
    ]
  },
  {
    "objectID": "python/nlp_data.html#the-text",
    "href": "python/nlp_data.html#the-text",
    "title": "Getting the data",
    "section": "The text",
    "text": "The text\n\n\n\n\n\n\nWyrd Sisters, the sixth Discworld novel by Terry Pratchett published in 1988, has countless references to Macbeth (including, obviously, the title), other Shakespeare’s plays, the Marx Brothers, Charlie Chaplin, and Laurel and Hardy.  The book is available as a pdf at this URL and this is the text we will use for this course.\n\n\n \n\n\n\n\n\nArt by Josh Kirby used for the cover of Wyrd Sisters",
    "crumbs": [
      "Python",
      "<b><em>Text analysis</em></b>",
      "Getting the data"
    ]
  },
  {
    "objectID": "python/nlp_data.html#packages-needed",
    "href": "python/nlp_data.html#packages-needed",
    "title": "Getting the data",
    "section": "Packages needed",
    "text": "Packages needed\nFirst off, we need to load two of the packages that you installed in the previous section:\n\nRequests: this package sends requests to websites to download information. We will use it to download the pdf.\nPyMuPDF: this package will allow us to extract the content from the pdf.\n\nLet’s load the packages into our session to make them available:\n\nimport requests\nimport pymupdf\n\n\n---------------------------------------------------------------------------\nModuleNotFoundError                       Traceback (most recent call last)\nCell In[1], line 2\n      1 import requests\n----&gt; 2 import pymupdf\n\nModuleNotFoundError: No module named 'pymupdf'",
    "crumbs": [
      "Python",
      "<b><em>Text analysis</em></b>",
      "Getting the data"
    ]
  },
  {
    "objectID": "python/nlp_data.html#download-the-data",
    "href": "python/nlp_data.html#download-the-data",
    "title": "Getting the data",
    "section": "Download the data",
    "text": "Download the data\nFirst, let’s create a string with the URL of the online pdf:\n\nurl = \"https://funnyengwish.wordpress.com/wp-content/uploads/2017/05/pratchett_terry_wyrd_sisters_-_royallib_ru.pdf\"\n\nNow we can send a request to that URL to download the data and create a response object:\n\nresponse = requests.get(url)\n\nLet’s print the value of our response to ensure that it was successful:\n\nprint(response)\n\n&lt;Response [200]&gt;\n\n\n\nOn the list of HTTP status codes, you can see that 200 means OK. So our request was successful.\n\nThen we extract the text from the pdf:\n\ndata = response.content\ndoc = pymupdf.Document(stream=data)\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[5], line 2\n      1 data = response.content\n----&gt; 2 doc = pymupdf.Document(stream=data)\n\nNameError: name 'pymupdf' is not defined\n\n\n\nLet’s explore this doc object that we created.\nIt is a Document object from the pymupdf package:\n\ntype(doc)\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[6], line 1\n----&gt; 1 type(doc)\n\nNameError: name 'doc' is not defined\n\n\n\nThe first element corresponds to the first page of the pdf:\n\ndoc[0]\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[7], line 1\n----&gt; 1 doc[0]\n\nNameError: name 'doc' is not defined\n\n\n\n\nRemember that indexing in Python starts at 0.\n\n\ntype(doc[0])\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[8], line 1\n----&gt; 1 type(doc[0])\n\nNameError: name 'doc' is not defined\n\n\n\nThe pdf had 139 pages:\n\nlen(doc)\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[9], line 1\n----&gt; 1 len(doc)\n\nNameError: name 'doc' is not defined\n\n\n\nWe can get the text of the first page with the get_text method. Let’s create an string that we call page1 with this text:\n\npage1 = doc[0].get_text()\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[10], line 1\n----&gt; 1 page1 = doc[0].get_text()\n\nNameError: name 'doc' is not defined\n\n\n\nWe can now print the text of the first page of the pdf:\n\nprint(page1)\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[11], line 1\n----&gt; 1 print(page1)\n\nNameError: name 'page1' is not defined",
    "crumbs": [
      "Python",
      "<b><em>Text analysis</em></b>",
      "Getting the data"
    ]
  },
  {
    "objectID": "python/nlp_normalization.html",
    "href": "python/nlp_normalization.html",
    "title": "Text normalization",
    "section": "",
    "text": "TextBlob allows to transform text—something very useful in preparation for text analysis.",
    "crumbs": [
      "Python",
      "<b><em>Text analysis</em></b>",
      "Text normalization"
    ]
  },
  {
    "objectID": "python/nlp_normalization.html#case",
    "href": "python/nlp_normalization.html#case",
    "title": "Text normalization",
    "section": "Case",
    "text": "Case\nThere are methods to change the case of TextBlob objects.\nFor example, capitalization (let’s only print the first 1000 characters)\n\nprint(text.title()[:1000])\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[2], line 1\n----&gt; 1 print(text.title()[:1000])\n\nNameError: name 'text' is not defined\n\n\n\nOr transformation to upper case:\n\nprint(text.upper()[:1000])\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[3], line 1\n----&gt; 1 print(text.upper()[:1000])\n\nNameError: name 'text' is not defined",
    "crumbs": [
      "Python",
      "<b><em>Text analysis</em></b>",
      "Text normalization"
    ]
  },
  {
    "objectID": "python/nlp_normalization.html#number",
    "href": "python/nlp_normalization.html#number",
    "title": "Text normalization",
    "section": "Number",
    "text": "Number\nThe number (singular/plural) of particular words can also be changed:\n\nprint(text.words[6])\nprint(text.words[6].singularize())\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[4], line 1\n----&gt; 1 print(text.words[6])\n      2 print(text.words[6].singularize())\n\nNameError: name 'text' is not defined\n\n\n\n\nprint(text.words[42])\nprint(text.words[42].pluralize())\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[5], line 1\n----&gt; 1 print(text.words[42])\n      2 print(text.words[42].pluralize())\n\nNameError: name 'text' is not defined",
    "crumbs": [
      "Python",
      "<b><em>Text analysis</em></b>",
      "Text normalization"
    ]
  },
  {
    "objectID": "python/nlp_normalization.html#lemmatization",
    "href": "python/nlp_normalization.html#lemmatization",
    "title": "Text normalization",
    "section": "Lemmatization",
    "text": "Lemmatization\nLemmatization reduces all words to their lemma (dictionary or canonical form) so that inflected words such as “dog” and “dogs” aren’t counted in separate categories in analyses.\n\nNouns\nThe lemmatize method uses as its default argument \"n\" (for noun):\n\nprint(TextBlob(\"heirs\").words[0].lemmatize())\nprint(TextBlob(\"daggers\").words[0].lemmatize())\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[6], line 1\n----&gt; 1 print(TextBlob(\"heirs\").words[0].lemmatize())\n      2 print(TextBlob(\"daggers\").words[0].lemmatize())\n\nNameError: name 'TextBlob' is not defined\n\n\n\n\nBe careful: you can’t always trust that TextBlob will work properly. It is a library very easy to use, but it has its limitations.\nFor instance, I am not sure why this one doesn’t work:\n\nprint(TextBlob(\"men\").words[0].lemmatize())\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[7], line 1\n----&gt; 1 print(TextBlob(\"men\").words[0].lemmatize())\n\nNameError: name 'TextBlob' is not defined\n\n\n\nWhile this totally works:\n\nprint(TextBlob(\"policemen\").words[0].lemmatize())\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[8], line 1\n----&gt; 1 print(TextBlob(\"policemen\").words[0].lemmatize())\n\nNameError: name 'TextBlob' is not defined\n\n\n\nUsing the more complex and more powerful NLTK Python library, you can implement the solution suggested here.\n\n\n\nVerbs\nTo lemmatize verbs, you need to pass \"v\" (for verbs) to the lemmatize method:\n\nprint(TextBlob(\"seen\").words[0].lemmatize(\"v\"))\nprint(TextBlob(\"seeing\").words[0].lemmatize(\"v\"))\nprint(TextBlob(\"sees\").words[0].lemmatize(\"v\"))\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[9], line 1\n----&gt; 1 print(TextBlob(\"seen\").words[0].lemmatize(\"v\"))\n      2 print(TextBlob(\"seeing\").words[0].lemmatize(\"v\"))\n      3 print(TextBlob(\"sees\").words[0].lemmatize(\"v\"))\n\nNameError: name 'TextBlob' is not defined\n\n\n\n\n\nYour turn:\n\nWhy is this one not working?\n\nprint(TextBlob(\"saw\").words[0].lemmatize(\"v\"))\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[10], line 1\n----&gt; 1 print(TextBlob(\"saw\").words[0].lemmatize(\"v\"))\n\nNameError: name 'TextBlob' is not defined\n\n\n\n\nExamples from the text:\n\nprint(TextBlob(\"starring\").words[0].lemmatize(\"v\"))\nprint(TextBlob(\"stabbed\").words[0].lemmatize(\"v\"))\nprint(TextBlob(\"howled\").words[0].lemmatize(\"v\"))\nprint(TextBlob(\"rejoicing\").words[0].lemmatize(\"v\"))\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[11], line 1\n----&gt; 1 print(TextBlob(\"starring\").words[0].lemmatize(\"v\"))\n      2 print(TextBlob(\"stabbed\").words[0].lemmatize(\"v\"))\n      3 print(TextBlob(\"howled\").words[0].lemmatize(\"v\"))\n\nNameError: name 'TextBlob' is not defined\n\n\n\n\n\nAdjectives\nTo lemmatize adjectives, you need to pass \"a\" (for adjectives) to the lemmatize method:\n\nprint(TextBlob(\"youngest\").words[0].lemmatize(\"a\"))\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[12], line 1\n----&gt; 1 print(TextBlob(\"youngest\").words[0].lemmatize(\"a\"))\n\nNameError: name 'TextBlob' is not defined",
    "crumbs": [
      "Python",
      "<b><em>Text analysis</em></b>",
      "Text normalization"
    ]
  },
  {
    "objectID": "python/nlp_normalization.html#correction",
    "href": "python/nlp_normalization.html#correction",
    "title": "Text normalization",
    "section": "Correction",
    "text": "Correction\nThe correct method attempts to correct spelling mistakes:\n\nprint(TextBlob(\"Somethingg with speling mystakes\").correct())\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[13], line 1\n----&gt; 1 print(TextBlob(\"Somethingg with speling mystakes\").correct())\n\nNameError: name 'TextBlob' is not defined\n\n\n\n\nThere are however limitations since the method is based on a lexicon and isn’t aware of the relationship between words (and thus cannot correct grammatical errors):\n\nprint(TextBlob(\"Some thingg with speling mystake\").correct())\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[14], line 1\n----&gt; 1 print(TextBlob(\"Some thingg with speling mystake\").correct())\n\nNameError: name 'TextBlob' is not defined\n\n\n\nAn example even more obvious:\n\nprint(TextBlob(\"He drink\").correct())\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[15], line 1\n----&gt; 1 print(TextBlob(\"He drink\").correct())\n\nNameError: name 'TextBlob' is not defined",
    "crumbs": [
      "Python",
      "<b><em>Text analysis</em></b>",
      "Text normalization"
    ]
  },
  {
    "objectID": "python/polars_dataframes.html",
    "href": "python/polars_dataframes.html",
    "title": "The world of data frames",
    "section": "",
    "text": "Let’s talk about data frames, how they came to the world of programming, how pandas had the monopoly for many years in Python, and how things are changing very quickly at the moment.",
    "crumbs": [
      "Python",
      "<b><em>Faster Python</em></b>",
      "Polars: faster data frames",
      "The world of data frames"
    ]
  },
  {
    "objectID": "python/polars_dataframes.html#tabular-data",
    "href": "python/polars_dataframes.html#tabular-data",
    "title": "The world of data frames",
    "section": "Tabular data",
    "text": "Tabular data\nMany fields of machine learning, data science, and humanities rely on tabular data where:\n\ncolumns hold variables and are homogeneous (same data type)—you can think of them as vectors,\nrows contain observations and can be heterogeneous.\n\nEarly computer options to manipulate such data were limited to spreadsheets (e.g. Microsoft Excel).\nDataframes (data frames or DataFrames) are two dimensional objects that brought tabular data to programming.",
    "crumbs": [
      "Python",
      "<b><em>Faster Python</em></b>",
      "Polars: faster data frames",
      "The world of data frames"
    ]
  },
  {
    "objectID": "python/polars_dataframes.html#early-history-of-data-frames",
    "href": "python/polars_dataframes.html#early-history-of-data-frames",
    "title": "The world of data frames",
    "section": "Early history of data frames",
    "text": "Early history of data frames\nAfter data frames emerged in S, then R, they were added to Python with the library pandas in 2008:\n\n\n\n\n\n\n\n\n\ny1\n1990\n\n\n\ny2\n2000\n\n\n\ny1--y2\n\n\n\n\ny3\n2008\n\n\n\ny2--y3\n\n\n\n\nl1\n\nS programming language\n\n\n\n\n\nl2\n\nR\n\n\n\n\n\n\nl3\n\npandas (Python)\n\n\n\n\n\n\n\n\n\n\n\nAfter which, pandas remained the Python data frame library for a long time.",
    "crumbs": [
      "Python",
      "<b><em>Faster Python</em></b>",
      "Polars: faster data frames",
      "The world of data frames"
    ]
  },
  {
    "objectID": "python/polars_dataframes.html#issues-with-pandas",
    "href": "python/polars_dataframes.html#issues-with-pandas",
    "title": "The world of data frames",
    "section": "Issues with pandas",
    "text": "Issues with pandas\nWes McKinney—the author of pandas—himself has complaints about pandas:\n\ninternals too far from the hardware,\nno support for memory-mapped datasets,\npoor performance in database and file ingest / export,\nlack of proper support for missing data,\nlack of memory use and RAM management transparency,\nweak support for categorical data,\ncomplex groupby operations awkward and slow,\nappending data to a DataFrame tedious and costly,\nlimited and non-extensible type metadata,\neager evaluation model with no query planning,\nslow and limited multicore algorithms for large datasets.",
    "crumbs": [
      "Python",
      "<b><em>Faster Python</em></b>",
      "Polars: faster data frames",
      "The world of data frames"
    ]
  },
  {
    "objectID": "python/polars_dataframes.html#a-rich-new-field",
    "href": "python/polars_dataframes.html#a-rich-new-field",
    "title": "The world of data frames",
    "section": "A rich new field",
    "text": "A rich new field\nOver the past few years, there has been an explosion of faster alternatives.\n\nParallel computing\nThe Python global interpreter lock (GIL) gets in the way of multi-threading, but several libraries allow the use of Python on multiple cores:\n\nRay\nDask\nApache Spark\n\nFugue provides a unified interface for distributed computing that works with all three libraries.\nTo use data frames on multiple cores, Dask and Spark have APIs for pandas and Modin provides a drop-in replacement for pandas in all three libraries.\n\n\nAccelerators\nRAPIDS brings data frames on the GPUs with the cuDF library and integration with pandas is easy.\n\n\nLazy out-of-core\nVaex exists as an alternative to pandas.\n\n\nSQL\nStructured query language (SQL) handles relational databases, but the distinction between SQL and data frame software is getting increasingly blurry with most libraries now able to handle both.\nDuckDB is a very fast and popular option with good integration with pandas.\nMany additional options such as dbt and the snowflake snowpark Python API exist, although integration with pandas is not always as good.\n\n\nPolars\nPolars uses Apache Arrow columnar memory format—the new standard for efficiency.\nMost libraries are developing an integration with Polars, lodging it nicely in the Python ecosystem.",
    "crumbs": [
      "Python",
      "<b><em>Faster Python</em></b>",
      "Polars: faster data frames",
      "The world of data frames"
    ]
  },
  {
    "objectID": "python/polars_dataframes.html#best-data-frame-strategy",
    "href": "python/polars_dataframes.html#best-data-frame-strategy",
    "title": "The world of data frames",
    "section": "Best data frame strategy",
    "text": "Best data frame strategy\nFor maximum efficiency, the best strategy currently seems to be:\n\nSingle machine   ➔  use Polars.\nCluster       ➔  use Polars + fugue (example benchmark, documentation of Polars integration).\nGPUs available    ➔  use Polars + RAPIDS library cuDF (Polars integration coming soon).\nCluster with GPUs  ➔  use Polars + fugue + RAPIDS.\n\nNo matter the scenario, Polars is better than pandas and you should use it instead.",
    "crumbs": [
      "Python",
      "<b><em>Faster Python</em></b>",
      "Polars: faster data frames",
      "The world of data frames"
    ]
  },
  {
    "objectID": "python/polars_install.html",
    "href": "python/polars_install.html",
    "title": "Installation",
    "section": "",
    "text": "The best way to install Polars is to do so inside a Python virtual environment.\nFor this course, we will use a JupyterHub running on a training cluster.",
    "crumbs": [
      "Python",
      "<b><em>Faster Python</em></b>",
      "Polars: faster data frames",
      "Installation"
    ]
  },
  {
    "objectID": "python/polars_install.html#installing-polars",
    "href": "python/polars_install.html#installing-polars",
    "title": "Installation",
    "section": "Installing Polars",
    "text": "Installing Polars\n\nPersonal computer\npython -m venv ~/env                  # Create virtual env\nsource ~/env/bin/activate             # Activate virtual env\npip install --upgrade pip             # Update pip\npip install polars                    # Install Polars\n\n\nAlliance clusters\nPolars wheels are available for Polars (always prefer wheels when possible):\npython -m venv ~/env                  # Create virtual env\nsource ~/env/bin/activate             # Activate virtual env\npip install --upgrade pip --no-index  # Update pip from wheel\npip install polars --no-index         # Install Polars from wheel",
    "crumbs": [
      "Python",
      "<b><em>Faster Python</em></b>",
      "Polars: faster data frames",
      "Installation"
    ]
  },
  {
    "objectID": "python/polars_install.html#running-polars-for-this-course",
    "href": "python/polars_install.html#running-polars-for-this-course",
    "title": "Installation",
    "section": "Running Polars for this course",
    "text": "Running Polars for this course\nFor this course, we will use JupyterLab on a training cluster via JupyterHub—a set of tools that spawn and manage multiple instances of JupyterLab servers.\n\nLog in to JupyterHub\n\ngo to the URL we will give you in class,\nsign in with the username and password we will give you,\nleave OTP blank,\nyou don’t need to edit anything in the server options,\npress start.\n\n\nNote that, unlike other JupyterHubs you might have used (e.g. Syzygy), this JupyterHub is not permanent and will be destroyed at the end of this course.\n\nIf you don’t need all the time you asked for after all, it is a great thing to log out (the resources you are using on this cluster are shared amongst many people and when resources are allocated to you, they aren’t available to other people. So it is a good thing not to ask for unnecessary resources and have them sit idle when others could be using them).\nTo log out, click on “File” in the top menu and select “Log out” at the very bottom.\nIf you would like to make a change to the information you entered on the server option page after you have pressed “start”, log out in the same way, log back in, edit the server options, and press start again.\n\n\nStart a Python notebook\nTo start a Jupyter notebook with the Python kernel, click on the button “Python 3” in the “Notebook” section (top row of buttons).",
    "crumbs": [
      "Python",
      "<b><em>Faster Python</em></b>",
      "Polars: faster data frames",
      "Installation"
    ]
  },
  {
    "objectID": "python/polars_pandas.html",
    "href": "python/polars_pandas.html",
    "title": "Comparison with pandas",
    "section": "",
    "text": "As pandas was the only data frame library for Python for a long time, many Python users are familiar with it and a comparison with Polars might be useful.",
    "crumbs": [
      "Python",
      "<b><em>Faster Python</em></b>",
      "Polars: faster data frames",
      "Comparison with pandas"
    ]
  },
  {
    "objectID": "python/polars_pandas.html#overview",
    "href": "python/polars_pandas.html#overview",
    "title": "Comparison with pandas",
    "section": "Overview",
    "text": "Overview\n\n\n\n\npandas\nPolars\n\n\n\n\nAvailable for\nPython\nRust, Python, R, NodeJS\n\n\nWritten in\nCython\nRust\n\n\nMultithreading\nSome operations\nYes (GIL released)\n\n\nIndex\nRows are indexed\nInteger positions are used\n\n\nEvaluation\nEager\nEager and lazy\n\n\nQuery optimizer\nNo\nYes\n\n\nOut-of-core\nNo\nYes\n\n\nSIMD vectorization\nYes\nYes\n\n\nData in memory\nWith NumPy arrays\nWith Apache Arrow arrays\n\n\nMemory efficiency\nPoor\nExcellent\n\n\nHandling of missing data\nInconsistent\nConsistent, promotes type stability",
    "crumbs": [
      "Python",
      "<b><em>Faster Python</em></b>",
      "Polars: faster data frames",
      "Comparison with pandas"
    ]
  },
  {
    "objectID": "python/polars_pandas.html#performance",
    "href": "python/polars_pandas.html#performance",
    "title": "Comparison with pandas",
    "section": "Performance",
    "text": "Performance\n\nExample 1\nLet’s use the FizzBuzz problem.\nIn his pandas course, Alex compares multiple methods and shows that the best method uses masks. Let’s see how Polars fares in comparison to pandas’ best method.\nFirst, let’s load the packages we will need:\n\nimport pandas as pd\nimport numpy as np\nimport polars as pl\n\nAnd let’s make sure that the code works.\nWith pandas:\n\ndf_pd = pd.DataFrame()\nsize = 10_000\ndf_pd[\"number\"] = np.arange(1, size+1)\ndf_pd[\"response\"] = df_pd[\"number\"].astype(str)\ndf_pd.loc[df_pd[\"number\"] % 3 == 0, \"response\"] = \"Fizz\"\ndf_pd.loc[df_pd[\"number\"] % 5 == 0, \"response\"] = \"Buzz\"\ndf_pd.loc[df_pd[\"number\"] % 15 == 0, \"response\"] = \"FizzBuzz\"\n\nprint(df_pd)\n\n      number response\n0          1        1\n1          2        2\n2          3     Fizz\n3          4        4\n4          5     Buzz\n...      ...      ...\n9995    9996     Fizz\n9996    9997     9997\n9997    9998     9998\n9998    9999     Fizz\n9999   10000     Buzz\n\n[10000 rows x 2 columns]\n\n\nWith Polars:\n\nsize = 10_000\ndf_pl = pl.DataFrame({\"number\": np.arange(1, size+1)})\ndf_pl.with_columns(pl.col(\"number\").cast(pl.String).alias(\"response\"))\ndf_pl = df_pl.with_columns(\n    pl.when(pl.col(\"number\") % 3 == 0)\n    .then(pl.lit(\"Fizz\"))\n    .when(pl.col(\"number\") % 5 == 0)\n    .then(pl.lit(\"Buzz\"))\n    .when(pl.col(\"number\") % 15 == 0)\n    .then(pl.lit(\"FizzBuzz\"))\n    .otherwise(pl.col(\"number\"))\n    .alias(\"response\")\n)\n\nprint(df_pl)\n\nshape: (10_000, 2)\n┌────────┬──────────┐\n│ number ┆ response │\n│ ---    ┆ ---      │\n│ i64    ┆ str      │\n╞════════╪══════════╡\n│ 1      ┆ 1        │\n│ 2      ┆ 2        │\n│ 3      ┆ Fizz     │\n│ 4      ┆ 4        │\n│ 5      ┆ Buzz     │\n│ …      ┆ …        │\n│ 9996   ┆ Fizz     │\n│ 9997   ┆ 9997     │\n│ 9998   ┆ 9998     │\n│ 9999   ┆ Fizz     │\n│ 10000  ┆ Buzz     │\n└────────┴──────────┘\n\n\nNow, let’s time them.\npandas:\n\n%%timeit\n\ndf_pd = pd.DataFrame()\nsize = 10_000\ndf_pd[\"number\"] = np.arange(1, size+1)\ndf_pd[\"response\"] = df_pd[\"number\"].astype(str)\ndf_pd.loc[df_pd[\"number\"] % 3 == 0, \"response\"] = \"Fizz\"\ndf_pd.loc[df_pd[\"number\"] % 5 == 0, \"response\"] = \"Buzz\"\ndf_pd.loc[df_pd[\"number\"] % 15 == 0, \"response\"] = \"FizzBuzz\"\n\n3.7 ms ± 26.4 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\nPolars:\n\n%%timeit\n\nsize = 10_000\ndf_pl = pl.DataFrame({\"number\": np.arange(1, size+1)})\ndf_pl.with_columns(pl.col(\"number\").cast(pl.String).alias(\"response\"))\ndf_pl.with_columns(\n    pl.when(pl.col(\"number\") % 3 == 0)\n    .then(pl.lit(\"Fizz\"))\n    .when(pl.col(\"number\") % 5 == 0)\n    .then(pl.lit(\"Buzz\"))\n    .when(pl.col(\"number\") % 15 == 0)\n    .then(pl.lit(\"FizzBuzz\"))\n    .otherwise(pl.col(\"number\"))\n    .alias(\"response\")\n)\n\n583 μs ± 2.78 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n\n\nThat’s a speedup of 9 (the longer the series, the larger this speedup will be).\n\n\nExample 2\nFor a second example, let’s go back to the jeopardy example with a large file and compare the timing of pandas and Polar.\nFirst, let’s make sure that the code works.\npandas:\n\ndf_pd = pd.read_csv(\"https://raw.githubusercontent.com/razoumov/publish/master/jeopardy.csv\")\ndf_pd.loc[df_pd[\"Category\"] == \"HISTORY\"].shape\n\n(349, 7)\n\n\nPolars:\n\ndf_pl = pl.read_csv(\"https://raw.githubusercontent.com/razoumov/publish/master/jeopardy.csv\")\ndf_pl.filter(pl.col(\"Category\") == \"HISTORY\").shape\n\n(349, 7)\n\n\nAnd now for timings.\npandas:\n\n%%timeit\n\ndf_pd = pd.read_csv(\"https://raw.githubusercontent.com/razoumov/publish/master/jeopardy.csv\")\ndf_pd.loc[df_pd[\"Category\"] == \"HISTORY\"].shape\n\n1.15 s ± 14.8 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\n\nPolars:\n\n%%timeit\n\ndf_pl = pl.read_csv(\"https://raw.githubusercontent.com/razoumov/publish/master/jeopardy.csv\")\ndf_pl.filter(pl.col(\"Category\") == \"HISTORY\").shape\n\n825 ms ± 26.9 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\n\nThat’s a speedup of 2.\nBut it gets much better with lazy evaluation. First, we create a LazyFrame instead of a DataFrame by using scan_csv instead of read_csv. The query is not evaluated but a graph is created. This allows the query optimizer to combine operations and perform optimizations where possible, very much the way compilers work. To evaluate the query and get a result, we use the collect method.\nLet’s make sure that the lazy Polars code gives us the same result:\n\ndf_pl = pl.scan_csv(\"https://raw.githubusercontent.com/razoumov/publish/master/jeopardy.csv\")\ndf_pl.filter(pl.col(\"Category\") == \"HISTORY\").collect().shape\n\n(349, 7)\n\n\nLazy timing:\n\n%%timeit\n\ndf_pl = pl.scan_csv(\"https://raw.githubusercontent.com/razoumov/publish/master/jeopardy.csv\")\ndf_pl.filter(pl.col(\"Category\") == \"HISTORY\").collect().shape\n\n72.9 ms ± 12.2 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\nThat’s a speedup of 20 (the larger the file, the larger this speedup will be).\n\nPandas is trying to fight back: v 2.0 came with optional Arrow support instead of NumPy, then it became the default engine, but performance remains way below that of Polars (e.g. in DataCamp benchmarks, official benchmarks, many blog posts for whole scripts or individual tasks).\n\n\nComparison with other frameworks\nComparisons between Polars and distributed (Dask, Ray, Spark) or GPU (RAPIDS) libraries aren’t the most pertinent since they can be used in combination with Polars and the benefits can thus be combined.\nIt only makes sense to compare Polars with other libraries occupying the same “niche” such as pandas or Vaex.\nFor Vaex, some benchmark found it twice slower, but this could have changed with recent developments.\nOne framework performing better than Polars in some benchmarks is datatable (derived from the R package data.table), but it hasn’t been developed for a year—a sharp contrast with the fast development of Polars.",
    "crumbs": [
      "Python",
      "<b><em>Faster Python</em></b>",
      "Polars: faster data frames",
      "Comparison with pandas"
    ]
  },
  {
    "objectID": "python/polars_pandas.html#migrating-from-pandas",
    "href": "python/polars_pandas.html#migrating-from-pandas",
    "title": "Comparison with pandas",
    "section": "Migrating from Pandas",
    "text": "Migrating from Pandas\nRead the migration guide: it will help you write Polars code rather than “literally translated” Pandas code that runs, but doesn’t make use of Polars’ strengths. The differences in style mostly come from the fact that Polars runs in parallel.",
    "crumbs": [
      "Python",
      "<b><em>Faster Python</em></b>",
      "Polars: faster data frames",
      "Comparison with pandas"
    ]
  },
  {
    "objectID": "python/polars_structures.html",
    "href": "python/polars_structures.html",
    "title": "Data structures",
    "section": "",
    "text": "Polars provides two fundamental data structures: series and data frames.",
    "crumbs": [
      "Python",
      "<b><em>Faster Python</em></b>",
      "Polars: faster data frames",
      "Data structures"
    ]
  },
  {
    "objectID": "python/polars_structures.html#series",
    "href": "python/polars_structures.html#series",
    "title": "Data structures",
    "section": "Series",
    "text": "Series\nIn Polars, series are one-dimensional and homogeneous (all elements have the same data type).\n\nIn other frameworks or languages (e.g. pandas, R), such data structure would be called a vector.\n\n\nimport polars as pl\n\ns1 = pl.Series(range(5))\nprint(s1)\n\nshape: (5,)\nSeries: '' [i64]\n[\n    0\n    1\n    2\n    3\n    4\n]\n\n\n\nData types\nPolars infers data types from the data. Defaults are Int64 and Float64. For other options, you can create typed series by specifying the type:\n\ns2 = pl.Series(range(5), dtype=pl.Int32)\nprint(s2)\n\nshape: (5,)\nSeries: '' [i32]\n[\n    0\n    1\n    2\n    3\n    4\n]\n\n\n\n\nNamed series\nSeries can be named:\n\ns3 = pl.Series(\"Name\", [\"Bob\", \"Luc\", \"Lucy\"])\nprint(s3)\n\nshape: (3,)\nSeries: 'Name' [str]\n[\n    \"Bob\"\n    \"Luc\"\n    \"Lucy\"\n]",
    "crumbs": [
      "Python",
      "<b><em>Faster Python</em></b>",
      "Polars: faster data frames",
      "Data structures"
    ]
  },
  {
    "objectID": "python/polars_structures.html#data-frames",
    "href": "python/polars_structures.html#data-frames",
    "title": "Data structures",
    "section": "Data frames",
    "text": "Data frames\nData frames are two-dimensional and composed of named series of equal lengths. This means that data frames are heterogeneous, but that columns contain homogeneous data.\nThey can be created from:\n\nlists of series:\n\n\ndf1 = pl.DataFrame([s3, pl.Series(\"Colour\", [\"Red\", \"Green\", \"Blue\"])])\nprint(df1)\n\nshape: (3, 2)\n┌──────┬────────┐\n│ Name ┆ Colour │\n│ ---  ┆ ---    │\n│ str  ┆ str    │\n╞══════╪════════╡\n│ Bob  ┆ Red    │\n│ Luc  ┆ Green  │\n│ Lucy ┆ Blue   │\n└──────┴────────┘\n\n\n\ndictionaries:\n\n\nfrom datetime import date\n\ndf2 = pl.DataFrame(\n    {\n        \"Date\": [\n            date(2024, 10, 1),\n            date(2024, 10, 2),\n            date(2024, 10, 3),\n            date(2024, 10, 6)\n        ],\n        \"Rain\": [2.1, 0.5, 0.0, 1.8],\n        \"Cloud cover\": [1, 1, 0, 2]\n        }\n    )\nprint(df2)\n\nshape: (4, 3)\n┌────────────┬──────┬─────────────┐\n│ Date       ┆ Rain ┆ Cloud cover │\n│ ---        ┆ ---  ┆ ---         │\n│ date       ┆ f64  ┆ i64         │\n╞════════════╪══════╪═════════════╡\n│ 2024-10-01 ┆ 2.1  ┆ 1           │\n│ 2024-10-02 ┆ 0.5  ┆ 1           │\n│ 2024-10-03 ┆ 0.0  ┆ 0           │\n│ 2024-10-06 ┆ 1.8  ┆ 2           │\n└────────────┴──────┴─────────────┘\n\n\n\nNumPy ndarrays:\n\n\nimport numpy as np\n\ndf3 = pl.DataFrame(np.array([(1, 2), (3, 4)]))\nprint(df3)\n\nshape: (2, 2)\n┌──────────┬──────────┐\n│ column_0 ┆ column_1 │\n│ ---      ┆ ---      │\n│ i64      ┆ i64      │\n╞══════════╪══════════╡\n│ 1        ┆ 2        │\n│ 3        ┆ 4        │\n└──────────┴──────────┘\n\n\nBecause NumPy ndarrays are stored in memory by rows, the values in the first dimension of the array fill in the first row. If you want to fill in the data frame by column, you use the orient parameter:\n\ndf4 = pl.DataFrame(np.array([(1, 2), (3, 4)]), orient=\"col\")\nprint(df4)\n\nshape: (2, 2)\n┌──────────┬──────────┐\n│ column_0 ┆ column_1 │\n│ ---      ┆ ---      │\n│ i64      ┆ i64      │\n╞══════════╪══════════╡\n│ 1        ┆ 3        │\n│ 2        ┆ 4        │\n└──────────┴──────────┘\n\n\nTo specify column names, you can use the schema parameter:\n\ndf5 = pl.DataFrame(np.array([(1, 2), (3, 4)]), schema=[\"Var1\", \"Var2\"])\nprint(df5)\n\nshape: (2, 2)\n┌──────┬──────┐\n│ Var1 ┆ Var2 │\n│ ---  ┆ ---  │\n│ i64  ┆ i64  │\n╞══════╪══════╡\n│ 1    ┆ 2    │\n│ 3    ┆ 4    │\n└──────┴──────┘",
    "crumbs": [
      "Python",
      "<b><em>Faster Python</em></b>",
      "Polars: faster data frames",
      "Data structures"
    ]
  },
  {
    "objectID": "python/polars_structures.html#data-types-1",
    "href": "python/polars_structures.html#data-types-1",
    "title": "Data structures",
    "section": "Data types",
    "text": "Data types\nTo specify data types different from the default, you also use the schema parameter:\n\ndf6 = pl.DataFrame(\n    {\n        \"Rain\": [2.1, 0.5, 0.0, 1.8],\n        \"Cloud cover\": [1, 1, 0, 2],\n    },\n    schema={\"Rain\": pl.Float32, \"Cloud cover\": pl.Int32}\n)\nprint(df6)\n\nshape: (4, 2)\n┌──────┬─────────────┐\n│ Rain ┆ Cloud cover │\n│ ---  ┆ ---         │\n│ f32  ┆ i32         │\n╞══════╪═════════════╡\n│ 2.1  ┆ 1           │\n│ 0.5  ┆ 1           │\n│ 0.0  ┆ 0           │\n│ 1.8  ┆ 2           │\n└──────┴─────────────┘",
    "crumbs": [
      "Python",
      "<b><em>Faster Python</em></b>",
      "Polars: faster data frames",
      "Data structures"
    ]
  },
  {
    "objectID": "python/polars_types.html",
    "href": "python/polars_types.html",
    "title": "Data types",
    "section": "",
    "text": "Data types supported by Polars are, for the most part, quite classic.",
    "crumbs": [
      "Python",
      "<b><em>Faster Python</em></b>",
      "Polars: faster data frames",
      "Data types"
    ]
  },
  {
    "objectID": "python/polars_types.html#list-of-data-types",
    "href": "python/polars_types.html#list-of-data-types",
    "title": "Data types",
    "section": "List of data types",
    "text": "List of data types\n\n\n\n\n\n\n\n\n\nnumeric\nnumeric\n\n\n\ntemporal\ntemporal\n\n\n\n\nsint\nsigned integer\n\n\n\nnumeric--sint\n\n\n\n\nuint\nunsigned integer\n\n\n\nnumeric--uint\n\n\n\n\nfloat\nfloat\n\n\n\nnumeric--float\n\n\n\n\nDecimal\n\nDecimal\n\n\n\nnumeric--Decimal\n\n\n\n\nnested\nnested\n\n\n\n\nDate\n\nDate\n\n\n\ntemporal--Date\n\n\n\n\nTime\n\nTime\n\n\n\ntemporal--Time\n\n\n\n\nDatetime\n\nDatetime\n\n\n\ntemporal--Datetime\n\n\n\n\nDuration\n\nDuration\n\n\n\ntemporal--Duration\n\n\n\n\nList\n\nList\n\n\n\nnested--List\n\n\n\n\nStruct\n\nStruct\n\n\n\nnested--Struct\n\n\n\n\nArray\n\nArray\n\n\n\nnested--Array\n\n\n\n\nInt8\n\nInt8\n\n\n\nsint--Int8\n\n\n\n\nInt16\n\nInt16\n\n\n\nsint--Int16\n\n\n\n\nInt32\n\nInt32\n\n\n\nsint--Int32\n\n\n\n\nInt64\n\nInt64\n\n\n\nsint--Int64\n\n\n\n\nUInt8\n\nUInt8\n\n\n\nuint--UInt8\n\n\n\n\nUInt16\n\nUInt16\n\n\n\nuint--UInt16\n\n\n\n\nUInt32\n\nUInt32\n\n\n\nuint--UInt32\n\n\n\n\nUInt64\n\nUInt64\n\n\n\nuint--UInt64\n\n\n\n\nFloat32\n\nFloat32\n\n\n\nfloat--Float32\n\n\n\n\nFloat64\n\nFloat64\n\n\n\nfloat--Float64\n\n\n\n\nBoolean\n\nBoolean\n\n\n\nBoolean--Int8\n\n\n\n\nDecimal--Date\n\n\n\n\nDate--Time\n\n\n\n\nTime--Datetime\n\n\n\n\nDatetime--Duration\n\n\n\n\nString\n\nString\n\n\n\nDuration--String\n\n\n\n\nList--Struct\n\n\n\n\nStruct--Array\n\n\n\n\nBinary\n\nBinary\n\n\n\nString--Binary\n\n\n\n\nObject\n\nObject\n\n\n\nBinary--Object\n\n\n\n\nCategorical\n\nCategorical\n\n\n\nObject--Categorical\n\n\n\n\nEnum\n\nEnum\n\n\n\nCategorical--Enum\n\n\n\n\nEnum--List\n\n\n\n\nNull\n\nNull\n\n\n\nNull--Boolean\n\n\n\n\nInt8--Int16\n\n\n\n\nInt16--Int32\n\n\n\n\nInt32--Int64\n\n\n\n\nInt64--UInt8\n\n\n\n\nUInt8--UInt16\n\n\n\n\nUInt16--UInt32\n\n\n\n\nUInt32--UInt64\n\n\n\n\nUInt64--Float32\n\n\n\n\nFloat32--Float64\n\n\n\n\nFloat64--Decimal",
    "crumbs": [
      "Python",
      "<b><em>Faster Python</em></b>",
      "Polars: faster data frames",
      "Data types"
    ]
  },
  {
    "objectID": "python/polars_types.html#exotic-polars-types",
    "href": "python/polars_types.html#exotic-polars-types",
    "title": "Data types",
    "section": "Exotic Polars types",
    "text": "Exotic Polars types\nStructs are series combining multiple columns.\nEnum is used for categorical variables. It is stricter, but also more efficient than the more flexible and slower Categorical type. When categories are all known before runtime, use Enum. If you need to infer the categories at runtime, use Categorical.\nObject allows to wrap any Python object.",
    "crumbs": [
      "Python",
      "<b><em>Faster Python</em></b>",
      "Polars: faster data frames",
      "Data types"
    ]
  },
  {
    "objectID": "python/top_intro.html",
    "href": "python/top_intro.html",
    "title": "Getting started in Python",
    "section": "",
    "text": "This introductory course in Python does not assume any prior programming experience.\n\n Start course ➤",
    "crumbs": [
      "Python",
      "<b><em>Getting started</em></b>"
    ]
  },
  {
    "objectID": "python/top_wb.html",
    "href": "python/top_wb.html",
    "title": "Python webinars",
    "section": "",
    "text": "Faster DataFrames with  \n\n\n\n\nAccelerated arrays & AD with \n\n\n\n\nIntro programming for HSS\n\n\n\n\nuv package manager",
    "crumbs": [
      "Python",
      "<em><b>Webinars</b></em>"
    ]
  },
  {
    "objectID": "python/wb_hss_prog.html",
    "href": "python/wb_hss_prog.html",
    "title": "Introduction to programming for the humanities",
    "section": "",
    "text": "An introduction to our programming course for the humanities at the Digital Humanities Summer Institute (DHSI).\n\nSlides (Click and wait: this reveal.js presentation may take a little time to load.)",
    "crumbs": [
      "Python",
      "<em><b>Webinars</b></em>",
      "Intro programming for HSS"
    ]
  },
  {
    "objectID": "python/wb_jax.html",
    "href": "python/wb_jax.html",
    "title": "Accelerated array computing and flexible differentiation with JAX",
    "section": "",
    "text": "JAX is an open source Python library for high-performance array computing and flexible automatic differentiation.\nHigh-performance computing is achieved by asynchronous dispatch, just-in-time compilation, the XLA compiler for linear algebra, and full compatibility with accelerators (GPUs and TPUs).\nAutomatic differentiation uses Autograd and works with complex control flows (conditions, recursions), second and third-order derivatives, forward and reverse modes. This makes JAX ideal for machine learning and neural network libraries such as Flax are built on it.\nThis webinar will give an overview of JAX’s principles and functioning.\n\nSlides (Click and wait: this reveal.js presentation may take a little time to load.)",
    "crumbs": [
      "Python",
      "<em><b>Webinars</b></em>",
      "Accelerated arrays & AD with JAX"
    ]
  },
  {
    "objectID": "python/wb_polars.html",
    "href": "python/wb_polars.html",
    "title": "DataFrames on steroids with Polars",
    "section": "",
    "text": "Polars is a modern open source and very fast DataFrame framework for Python, Rust, JS, R, and Ruby.\nIn this webinar, I will demo Polars for Python and show how much faster it is compared to pandas while remaining just as convenient.\n\nSlides (Click and wait: this reveal.js presentation may take a little time to load.)",
    "crumbs": [
      "Python",
      "<em><b>Webinars</b></em>",
      "Faster data frames with Polars"
    ]
  },
  {
    "objectID": "python/wb_uv.html",
    "href": "python/wb_uv.html",
    "title": "A tool to rule them all:",
    "section": "",
    "text": "Despite being the most popular programming language, Python has never had a good package and version manager. Some languages come with an internal manager (e.g. R, Julia) while others come with well-built command line managers (e.g. Cargo for Rust). Python on the other hand has seen the development of an ever-growing and never-satisfactory suite of tools to manage its packages, versions, projects, and virtual environments: pip, pipx, pipenv, poetry, pyenv, venv, virtualenv to name just a few.\nIn February 2024, Astral might have finally put an end to the jumble when they launched uv, a fast and well-documented tool written in Rust which elegantly handles the gamut of tasks associated with Python versions, packages, and projects.\nIn this webinar, I will show you how to use uv to manage Python projects, packages, virtual environments, versions, and more.\n\nComing up on May 6.",
    "crumbs": [
      "Python",
      "<em><b>Webinars</b></em>",
      "`uv` package manager"
    ]
  },
  {
    "objectID": "python/ws_pandas.html",
    "href": "python/ws_pandas.html",
    "title": "Data frames with pandas",
    "section": "",
    "text": "pandas is a Python library built to manipulate data frames and time series.\nFor this section, we will use the Covid-19 data from the Johns Hopkins University CSSE repository.\nYou can visualize this data in a dashboard created by the Johns Hopkins University Center for Systems Science and Engineering.",
    "crumbs": [
      "Python",
      "<em><b>Workshops</b></em>",
      "Data frames with pandas"
    ]
  },
  {
    "objectID": "python/ws_pandas.html#setup",
    "href": "python/ws_pandas.html#setup",
    "title": "Data frames with pandas",
    "section": "Setup",
    "text": "Setup\nFirst, we need to load the pandas library and read in the data from the web:\n\n# Load the pandas library and create a shorter name for it\nimport pandas as pd\n\n# The global confirmed cases are available in CSV format at the url:\nurl = \"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv\"\n\n# pandas allows to read in data from the web directly\ncases = pd.read_csv(url)",
    "crumbs": [
      "Python",
      "<em><b>Workshops</b></em>",
      "Data frames with pandas"
    ]
  },
  {
    "objectID": "python/ws_pandas.html#first-look-at-the-data",
    "href": "python/ws_pandas.html#first-look-at-the-data",
    "title": "Data frames with pandas",
    "section": "First look at the data",
    "text": "First look at the data\nWhat does our data look like?\n\ncases\n\n\n\n\n\n\n\n\nProvince/State\nCountry/Region\nLat\nLong\n1/22/20\n1/23/20\n1/24/20\n1/25/20\n1/26/20\n1/27/20\n...\n2/28/23\n3/1/23\n3/2/23\n3/3/23\n3/4/23\n3/5/23\n3/6/23\n3/7/23\n3/8/23\n3/9/23\n\n\n\n\n0\nNaN\nAfghanistan\n33.939110\n67.709953\n0\n0\n0\n0\n0\n0\n...\n209322\n209340\n209358\n209362\n209369\n209390\n209406\n209436\n209451\n209451\n\n\n1\nNaN\nAlbania\n41.153300\n20.168300\n0\n0\n0\n0\n0\n0\n...\n334391\n334408\n334408\n334427\n334427\n334427\n334427\n334427\n334443\n334457\n\n\n2\nNaN\nAlgeria\n28.033900\n1.659600\n0\n0\n0\n0\n0\n0\n...\n271441\n271448\n271463\n271469\n271469\n271477\n271477\n271490\n271494\n271496\n\n\n3\nNaN\nAndorra\n42.506300\n1.521800\n0\n0\n0\n0\n0\n0\n...\n47866\n47875\n47875\n47875\n47875\n47875\n47875\n47875\n47890\n47890\n\n\n4\nNaN\nAngola\n-11.202700\n17.873900\n0\n0\n0\n0\n0\n0\n...\n105255\n105277\n105277\n105277\n105277\n105277\n105277\n105277\n105288\n105288\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n284\nNaN\nWest Bank and Gaza\n31.952200\n35.233200\n0\n0\n0\n0\n0\n0\n...\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n\n\n285\nNaN\nWinter Olympics 2022\n39.904200\n116.407400\n0\n0\n0\n0\n0\n0\n...\n535\n535\n535\n535\n535\n535\n535\n535\n535\n535\n\n\n286\nNaN\nYemen\n15.552727\n48.516388\n0\n0\n0\n0\n0\n0\n...\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n\n\n287\nNaN\nZambia\n-13.133897\n27.849332\n0\n0\n0\n0\n0\n0\n...\n343012\n343012\n343079\n343079\n343079\n343135\n343135\n343135\n343135\n343135\n\n\n288\nNaN\nZimbabwe\n-19.015438\n29.154857\n0\n0\n0\n0\n0\n0\n...\n263921\n264127\n264127\n264127\n264127\n264127\n264127\n264127\n264276\n264276\n\n\n\n\n289 rows × 1147 columns\n\n\n\n\n# Quick summary of the data\ncases.describe()\n\n\n\n\n\n\n\n\nLat\nLong\n1/22/20\n1/23/20\n1/24/20\n1/25/20\n1/26/20\n1/27/20\n1/28/20\n1/29/20\n...\n2/28/23\n3/1/23\n3/2/23\n3/3/23\n3/4/23\n3/5/23\n3/6/23\n3/7/23\n3/8/23\n3/9/23\n\n\n\n\ncount\n287.000000\n287.000000\n289.000000\n289.000000\n289.000000\n289.000000\n289.000000\n289.000000\n289.000000\n289.000000\n...\n2.890000e+02\n2.890000e+02\n2.890000e+02\n2.890000e+02\n2.890000e+02\n2.890000e+02\n2.890000e+02\n2.890000e+02\n2.890000e+02\n2.890000e+02\n\n\nmean\n19.718719\n22.182084\n1.927336\n2.273356\n3.266436\n4.972318\n7.335640\n10.134948\n19.307958\n21.346021\n...\n2.336755e+06\n2.337519e+06\n2.338173e+06\n2.338805e+06\n2.338992e+06\n2.339187e+06\n2.339387e+06\n2.339839e+06\n2.340460e+06\n2.341073e+06\n\n\nstd\n25.956609\n77.870931\n26.173664\n26.270191\n32.707271\n45.523871\n63.623197\n85.724481\n210.329649\n211.628535\n...\n8.506608e+06\n8.511285e+06\n8.514488e+06\n8.518031e+06\n8.518408e+06\n8.518645e+06\n8.519346e+06\n8.521641e+06\n8.524968e+06\n8.527765e+06\n\n\nmin\n-71.949900\n-178.116500\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n0.000000e+00\n\n\n25%\n4.072192\n-32.823050\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n1.456700e+04\n1.456700e+04\n1.456700e+04\n1.456700e+04\n1.456700e+04\n1.456700e+04\n1.456700e+04\n1.456700e+04\n1.456700e+04\n1.456700e+04\n\n\n50%\n21.512583\n20.939400\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n1.032480e+05\n1.032480e+05\n1.032480e+05\n1.032480e+05\n1.032480e+05\n1.032480e+05\n1.032480e+05\n1.032480e+05\n1.032480e+05\n1.032480e+05\n\n\n75%\n40.401784\n89.224350\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n1.051998e+06\n1.052122e+06\n1.052247e+06\n1.052382e+06\n1.052519e+06\n1.052664e+06\n1.052664e+06\n1.052926e+06\n1.053068e+06\n1.053213e+06\n\n\nmax\n71.706900\n178.065000\n444.000000\n444.000000\n549.000000\n761.000000\n1058.000000\n1423.000000\n3554.000000\n3554.000000\n...\n1.034435e+08\n1.035339e+08\n1.035898e+08\n1.036487e+08\n1.036508e+08\n1.036470e+08\n1.036555e+08\n1.036909e+08\n1.037558e+08\n1.038027e+08\n\n\n\n\n8 rows × 1145 columns\n\n\n\n\nOf course, this value is meaningless for Lat and Long!\n\n\n# Data types of the various columns\ncases.dtypes\n\nProvince/State     object\nCountry/Region     object\nLat               float64\nLong              float64\n1/22/20             int64\n                   ...   \n3/5/23              int64\n3/6/23              int64\n3/7/23              int64\n3/8/23              int64\n3/9/23              int64\nLength: 1147, dtype: object\n\n\n\ncases.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 289 entries, 0 to 288\nColumns: 1147 entries, Province/State to 3/9/23\ndtypes: float64(2), int64(1143), object(2)\nmemory usage: 2.5+ MB\n\n\n\ncases.shape\n\n(289, 1147)",
    "crumbs": [
      "Python",
      "<em><b>Workshops</b></em>",
      "Data frames with pandas"
    ]
  },
  {
    "objectID": "python/ws_pandas.html#cases-per-country-by-date",
    "href": "python/ws_pandas.html#cases-per-country-by-date",
    "title": "Data frames with pandas",
    "section": "Cases per country by date",
    "text": "Cases per country by date\nThe dataset is a time series: this means that we have the cumulative numbers up to each date.\n\n# Let's get rid of the latitude and longitude to simplify our data\nsimple = cases.drop(columns=['Lat', 'Long'])\nsimple\n\n\n\n\n\n\n\n\nProvince/State\nCountry/Region\n1/22/20\n1/23/20\n1/24/20\n1/25/20\n1/26/20\n1/27/20\n1/28/20\n1/29/20\n...\n2/28/23\n3/1/23\n3/2/23\n3/3/23\n3/4/23\n3/5/23\n3/6/23\n3/7/23\n3/8/23\n3/9/23\n\n\n\n\n0\nNaN\nAfghanistan\n0\n0\n0\n0\n0\n0\n0\n0\n...\n209322\n209340\n209358\n209362\n209369\n209390\n209406\n209436\n209451\n209451\n\n\n1\nNaN\nAlbania\n0\n0\n0\n0\n0\n0\n0\n0\n...\n334391\n334408\n334408\n334427\n334427\n334427\n334427\n334427\n334443\n334457\n\n\n2\nNaN\nAlgeria\n0\n0\n0\n0\n0\n0\n0\n0\n...\n271441\n271448\n271463\n271469\n271469\n271477\n271477\n271490\n271494\n271496\n\n\n3\nNaN\nAndorra\n0\n0\n0\n0\n0\n0\n0\n0\n...\n47866\n47875\n47875\n47875\n47875\n47875\n47875\n47875\n47890\n47890\n\n\n4\nNaN\nAngola\n0\n0\n0\n0\n0\n0\n0\n0\n...\n105255\n105277\n105277\n105277\n105277\n105277\n105277\n105277\n105288\n105288\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n284\nNaN\nWest Bank and Gaza\n0\n0\n0\n0\n0\n0\n0\n0\n...\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n\n\n285\nNaN\nWinter Olympics 2022\n0\n0\n0\n0\n0\n0\n0\n0\n...\n535\n535\n535\n535\n535\n535\n535\n535\n535\n535\n\n\n286\nNaN\nYemen\n0\n0\n0\n0\n0\n0\n0\n0\n...\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n\n\n287\nNaN\nZambia\n0\n0\n0\n0\n0\n0\n0\n0\n...\n343012\n343012\n343079\n343079\n343079\n343135\n343135\n343135\n343135\n343135\n\n\n288\nNaN\nZimbabwe\n0\n0\n0\n0\n0\n0\n0\n0\n...\n263921\n264127\n264127\n264127\n264127\n264127\n264127\n264127\n264276\n264276\n\n\n\n\n289 rows × 1145 columns\n\n\n\n Some countries (e.g. Australia) are split between several provinces or states so we will have to add the values of all their provinces/states to get their totals.\nHere is how to make the sum for all Australian states:\nLet’s first select all the data for Australia: we want all the rows for which the Country/Region column is equal to Australia.\nFirst, we want to select the Country/Region column. There are several ways to index in pandas.\nWhen indexing columns, one can use square brackets directly after the DataFrame to index:\n\nsimple['Country/Region']\n\n0               Afghanistan\n1                   Albania\n2                   Algeria\n3                   Andorra\n4                    Angola\n               ...         \n284      West Bank and Gaza\n285    Winter Olympics 2022\n286                   Yemen\n287                  Zambia\n288                Zimbabwe\nName: Country/Region, Length: 289, dtype: object\n\n\nHowever, it is more efficient to use the .loc or .iloc methods.\n\nUse .loc when using labels or booleans:\n\n\nsimple.loc[:, 'Country/Region']\n\n0               Afghanistan\n1                   Albania\n2                   Algeria\n3                   Andorra\n4                    Angola\n               ...         \n284      West Bank and Gaza\n285    Winter Olympics 2022\n286                   Yemen\n287                  Zambia\n288                Zimbabwe\nName: Country/Region, Length: 289, dtype: object\n\n\n\nUse .iloc when using indices:\n\n\nsimple.iloc[:, 1]\n\n0               Afghanistan\n1                   Albania\n2                   Algeria\n3                   Andorra\n4                    Angola\n               ...         \n284      West Bank and Gaza\n285    Winter Olympics 2022\n286                   Yemen\n287                  Zambia\n288                Zimbabwe\nName: Country/Region, Length: 289, dtype: object\n\n\n\nCountry/Region is the 2nd column, but indexing starts at 0 in Python.\n\nThen we need a conditional to filter the rows for which the value is equal to Australia:\n\nsimple.loc[:, 'Country/Region'] == 'Australia'\n\n0      False\n1      False\n2      False\n3      False\n4      False\n       ...  \n284    False\n285    False\n286    False\n287    False\n288    False\nName: Country/Region, Length: 289, dtype: bool\n\n\nFinally, we index, out of our entire data frame, the rows for which that condition returns True:\n\nsimple.loc[simple.loc[:, 'Country/Region'] == 'Australia']\n\n\n\n\n\n\n\n\nProvince/State\nCountry/Region\n1/22/20\n1/23/20\n1/24/20\n1/25/20\n1/26/20\n1/27/20\n1/28/20\n1/29/20\n...\n2/28/23\n3/1/23\n3/2/23\n3/3/23\n3/4/23\n3/5/23\n3/6/23\n3/7/23\n3/8/23\n3/9/23\n\n\n\n\n9\nAustralian Capital Territory\nAustralia\n0\n0\n0\n0\n0\n0\n0\n0\n...\n232018\n232018\n232619\n232619\n232619\n232619\n232619\n232619\n232619\n232974\n\n\n10\nNew South Wales\nAustralia\n0\n0\n0\n0\n3\n4\n4\n4\n...\n3900969\n3900969\n3908129\n3908129\n3908129\n3908129\n3908129\n3908129\n3908129\n3915992\n\n\n11\nNorthern Territory\nAustralia\n0\n0\n0\n0\n0\n0\n0\n0\n...\n104931\n104931\n105021\n105021\n105021\n105021\n105021\n105021\n105021\n105111\n\n\n12\nQueensland\nAustralia\n0\n0\n0\n0\n0\n0\n0\n1\n...\n1796633\n1796633\n1800236\n1800236\n1800236\n1800236\n1800236\n1800236\n1800236\n1800236\n\n\n13\nSouth Australia\nAustralia\n0\n0\n0\n0\n0\n0\n0\n0\n...\n880207\n880207\n881911\n881911\n881911\n881911\n881911\n881911\n881911\n883620\n\n\n14\nTasmania\nAustralia\n0\n0\n0\n0\n0\n0\n0\n0\n...\n286264\n286264\n286264\n286897\n286897\n286897\n286897\n286897\n286897\n287507\n\n\n15\nVictoria\nAustralia\n0\n0\n0\n0\n1\n1\n1\n1\n...\n2874262\n2874262\n2877260\n2877260\n2877260\n2877260\n2877260\n2877260\n2877260\n2880559\n\n\n16\nWestern Australia\nAustralia\n0\n0\n0\n0\n0\n0\n0\n0\n...\n1291077\n1291077\n1293461\n1293461\n1293461\n1293461\n1293461\n1293461\n1293461\n1293461\n\n\n\n\n8 rows × 1145 columns\n\n\n\n\nHere we use .loc to index based on a boolean array.\n\nWe can now make the sum for all of Australia for each day:\n\ntotal_australia = simple.loc[simple.loc[:, 'Country/Region'] == 'Australia'].sum(numeric_only=True)\ntotal_australia\n\n1/22/20           0\n1/23/20           0\n1/24/20           0\n1/25/20           0\n1/26/20           4\n             ...   \n3/5/23     11385534\n3/6/23     11385534\n3/7/23     11385534\n3/8/23     11385534\n3/9/23     11399460\nLength: 1143, dtype: int64\n\n\nWe can do this for all countries by grouping them:\n\ntotals = simple.groupby('Country/Region').sum(numeric_only=True)\ntotals\n\n\n\n\n\n\n\n\n1/22/20\n1/23/20\n1/24/20\n1/25/20\n1/26/20\n1/27/20\n1/28/20\n1/29/20\n1/30/20\n1/31/20\n...\n2/28/23\n3/1/23\n3/2/23\n3/3/23\n3/4/23\n3/5/23\n3/6/23\n3/7/23\n3/8/23\n3/9/23\n\n\nCountry/Region\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAfghanistan\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n209322\n209340\n209358\n209362\n209369\n209390\n209406\n209436\n209451\n209451\n\n\nAlbania\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n334391\n334408\n334408\n334427\n334427\n334427\n334427\n334427\n334443\n334457\n\n\nAlgeria\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n271441\n271448\n271463\n271469\n271469\n271477\n271477\n271490\n271494\n271496\n\n\nAndorra\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n47866\n47875\n47875\n47875\n47875\n47875\n47875\n47875\n47890\n47890\n\n\nAngola\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n105255\n105277\n105277\n105277\n105277\n105277\n105277\n105277\n105288\n105288\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\nWest Bank and Gaza\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n703228\n\n\nWinter Olympics 2022\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n535\n535\n535\n535\n535\n535\n535\n535\n535\n535\n\n\nYemen\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n11945\n\n\nZambia\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n343012\n343012\n343079\n343079\n343079\n343135\n343135\n343135\n343135\n343135\n\n\nZimbabwe\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n263921\n264127\n264127\n264127\n264127\n264127\n264127\n264127\n264276\n264276\n\n\n\n\n201 rows × 1143 columns\n\n\n\n Now, we can look at the totals for any date:\n\ntotals.loc[:, '6/12/21']\n\nCountry/Region\nAfghanistan              88740\nAlbania                 132449\nAlgeria                 133070\nAndorra                  13813\nAngola                   36600\n                         ...  \nWest Bank and Gaza      311018\nWinter Olympics 2022         0\nYemen                     6857\nZambia                  110332\nZimbabwe                 39852\nName: 6/12/21, Length: 201, dtype: int64\n\n\nTo make it easier to read, let’s order those numbers by decreasing order:\n\ntotals.loc[:, '6/12/21'].sort_values(ascending=False)\n\nCountry/Region\nUS                      33573694\nIndia                   29439989\nBrazil                  17385952\nFrance                   5799565\nTurkey                   5325435\n                          ...   \nPalau                          0\nTonga                          0\nSummer Olympics 2020           0\nTuvalu                         0\nWinter Olympics 2022           0\nName: 6/12/21, Length: 201, dtype: int64\n\n\nWe can also index the data for a particular country by indexing a row instead of a column:\n\ntotals.loc['Albania', :]\n\n1/22/20         0\n1/23/20         0\n1/24/20         0\n1/25/20         0\n1/26/20         0\n            ...  \n3/5/23     334427\n3/6/23     334427\n3/7/23     334427\n3/8/23     334443\n3/9/23     334457\nName: Albania, Length: 1143, dtype: int64\n\n\nWhen indexing rows, this syntax can be simplified to:\n\ntotals.loc['Albania']\n\n1/22/20         0\n1/23/20         0\n1/24/20         0\n1/25/20         0\n1/26/20         0\n            ...  \n3/5/23     334427\n3/6/23     334427\n3/7/23     334427\n3/8/23     334443\n3/9/23     334457\nName: Albania, Length: 1143, dtype: int64",
    "crumbs": [
      "Python",
      "<em><b>Workshops</b></em>",
      "Data frames with pandas"
    ]
  },
  {
    "objectID": "python/ws_pandas.html#global-totals",
    "href": "python/ws_pandas.html#global-totals",
    "title": "Data frames with pandas",
    "section": "Global totals",
    "text": "Global totals\nNow, what if we want to have the world totals for each day? We calculate the columns totals (i.e. the sum across countries):\n\ntotals.sum()\n\n1/22/20          557\n1/23/20          657\n1/24/20          944\n1/25/20         1437\n1/26/20         2120\n             ...    \n3/5/23     676024901\n3/6/23     676082941\n3/7/23     676213378\n3/8/23     676392824\n3/9/23     676570149\nLength: 1143, dtype: int64\n\n\n\n\nYour turn:\n\nHow many confirmed cases were there in Venezuela by March 10, 2021?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFirst, we need to select the data for Venezuela:\n\nvenez = totals.loc['Venezuela']\nvenez\n\n1/22/20         0\n1/23/20         0\n1/24/20         0\n1/25/20         0\n1/26/20         0\n            ...  \n3/5/23     552051\n3/6/23     552125\n3/7/23     552157\n3/8/23     552157\n3/9/23     552162\nName: Venezuela, Length: 1143, dtype: int64\n\n\nThen, we need to select for the proper date:\n\nanswer = venez.loc['3/10/21']\nanswer\n\nnp.int64(143321)\n\n\nWe could have done it at once by indexing the row and column:\n\ntotals.loc['Venezuela', '3/10/21']\n\nnp.int64(143321)",
    "crumbs": [
      "Python",
      "<em><b>Workshops</b></em>",
      "Data frames with pandas"
    ]
  },
  {
    "objectID": "python/ws_pandas.html#pandas-documentation",
    "href": "python/ws_pandas.html#pandas-documentation",
    "title": "Data frames with pandas",
    "section": "pandas documentation",
    "text": "pandas documentation\n\nA user Guide to pandas\nFull documentation",
    "crumbs": [
      "Python",
      "<em><b>Workshops</b></em>",
      "Data frames with pandas"
    ]
  },
  {
    "objectID": "r/hpc_clusters.html",
    "href": "r/hpc_clusters.html",
    "title": "R on HPC clusters",
    "section": "",
    "text": "In this section, you will learn how to use R on an Alliance cluster: load modules, install packages, and run jobs.",
    "crumbs": [
      "R",
      "<b><em>High-performance R</em></b>",
      "R on HPC clusters"
    ]
  },
  {
    "objectID": "r/hpc_clusters.html#modules",
    "href": "r/hpc_clusters.html#modules",
    "title": "R on HPC clusters",
    "section": "Modules",
    "text": "Modules\nOn the Alliance clusters, a number of utilities are available right away (e.g. Bash utilities, git, tmux, various text editors). Before you can use more specialized software however, you have to load the module corresponding to the version of your choice as well as any potential dependencies.\n\nR\nFirst, of course, we need an R module.\nTo see which versions of R are available on a cluster, run:\nmodule spider r\nTo see the dependencies of a particular version (e.g. r/4.4.0), run:\nmodule spider r/4.4.0\nThis shows us that we need StdEnv/2023 to load r/4.4.0.\n\n\nC compiler\nIf you plan on installing any R package, you will also need a C compiler.\nIn theory, one could use the proprietary Intel compiler which is loaded by default on the Alliance clusters, but it is recommended to replace it with the GCC compiler (R packages can be compiled by any C compiler—also including Clang and LLVM—but the default GCC compiler is the best way to avoid headaches).\n\n\nYour turn:\n\n\nHow can you check which gcc versions are available on our training cluster?\nWhat are the dependencies required by gcc/13.3?\n\n\n\n\nLoading the modules\nOnce you know which modules you need, you can load them. The order is important: the dependencies (here StdEnv/2023) must be listed before the modules which depend on them.\nmodule load StdEnv/2023 gcc/13.3 r/4.4.0",
    "crumbs": [
      "R",
      "<b><em>High-performance R</em></b>",
      "R on HPC clusters"
    ]
  },
  {
    "objectID": "r/hpc_clusters.html#installing-r-packages",
    "href": "r/hpc_clusters.html#installing-r-packages",
    "title": "R on HPC clusters",
    "section": "Installing R packages",
    "text": "Installing R packages\n\nFor this course, all packages have already been installed in a communal library to save us time and avoiding putting stress on the login node by all installing packages at the same time. The section below is thus for reference only.\nHowever, for this course only, you will have to run the following to load the communal library (run this now):\necho export R_LIBS='/project/def-sponsor00/R/lib' &gt;&gt; .bashrc &&\nexport R_LIBS='/project/def-sponsor00/R/lib'\n\nTo install a package, launch the interactive R console with:\nR\nIn the R console, run:\ninstall.packages(\"&lt;package_name&gt;\", repos=\"&lt;url-cran-mirror&gt;\")\n\nrepos argument: chose a CRAN mirror close to the location of your cluster or use https://cloud.r-project.org/.\n\n\nThe first time you install a package, R will ask you whether you want to create a personal library in your home directory. Answer yes to both questions. Your packages will now install under ~/.\n\n\nSome packages require additional modules to be loaded before they can be installed. Other packages need additional R packages as dependencies. In either case, you will get explicit error messages. Adding the argument dependencies = T helps in the second case, but you will still have to add packages manually from time to time.\n\nTo leave the R console, press &lt;Ctrl+D&gt;.",
    "crumbs": [
      "R",
      "<b><em>High-performance R</em></b>",
      "R on HPC clusters"
    ]
  },
  {
    "objectID": "r/hpc_clusters.html#running-r-jobs",
    "href": "r/hpc_clusters.html#running-r-jobs",
    "title": "R on HPC clusters",
    "section": "Running R jobs",
    "text": "Running R jobs\nThere are two types of jobs that can be launched on an Alliance cluster: interactive jobs and batch jobs. We will practice both and discuss their respective merits and when to use which.\nFor this course, I purposefully built a rather small cluster (10 nodes with 4 CPUs and 15GB each) to give a tangible illustration of the constraints of resource sharing.\n\nInteractive jobs\n\nWhile it is fine to run R on the login node when you install packages, you must start a SLURM job before any heavy computation.\n\nTo run R interactively, you should launch an salloc session.\n\nExample to launch an interactive job on a single CPU with 3500MB of memory for 2h:\n\nsalloc --time=2:00:00 --mem-per-cpu=3500M\nThis takes you to a compute node where you can now launch R to run computations:\nR\n\nThis however leads to the same inefficient use of resources as happens when running an RStudio server: all the resources that you requested are blocked for you while your job is running, whether you are making use of them (running heavy computations) or not (thinking, typing code, running computations that use only a fraction of the requested resources).\nInteractive jobs are thus best kept to develop code.\n\n\n\nScripts\nTo run an R script called &lt;your_script&gt;.R, you first need to write a job script:\n\nExample to run a script on 4 CPUs with 3500MB per CPU for 15min:\n\n\n&lt;your_job&gt;.sh\n\n#!/bin/bash\n#SBATCH --account=def-&lt;your_account&gt;\n#SBATCH --time=15\n#SBATCH --mem-per-cpu=3500M\n#SBATCH --cpus-per-task=4\n#SBATCH --job-name=\"&lt;your_job&gt;\"\nmodule load StdEnv/2023 gcc/13.3 r/4.4.0\nRscript &lt;your_script&gt;.R\n\n\n\nNote that R scripts are run with the command Rscript (not R).\n\nThen launch your job with:\nsbatch &lt;your_job&gt;.sh\nYou can monitor your job with sq (an alias for squeue -u $USER $@).\n\nBatch jobs are the best approach to run parallel computations, particularly when they require a lot of hardware.\nIt will save you lots of waiting time (Alliance clusters) or money (commercial clusters).",
    "crumbs": [
      "R",
      "<b><em>High-performance R</em></b>",
      "R on HPC clusters"
    ]
  },
  {
    "objectID": "r/hpc_foreach.html",
    "href": "r/hpc_foreach.html",
    "title": "foreach and doFuture",
    "section": "",
    "text": "One of the options to parallelize code with the future package is to use foreach with doFuture. In this section, we will go over an example using the random forest algorithm.",
    "crumbs": [
      "R",
      "<b><em>High-performance R</em></b>",
      "foreach and doFuture"
    ]
  },
  {
    "objectID": "r/hpc_foreach.html#our-example-code-random-forest",
    "href": "r/hpc_foreach.html#our-example-code-random-forest",
    "title": "foreach and doFuture",
    "section": "Our example code: random forest",
    "text": "Our example code: random forest\n\nOn the iris dataset\nRandom forest is a commonly used ensemble learning technique for classification and regression. The idea is to combine the results from many decision trees on bootstrap samples of the dataset to improve the predictive accuracy and control over-fitting. The algorithm used was developed by Tin Kam Ho, then improved by Leo Breiman and Adele Cutler. An implementation in R is provided by the randomForest() function from the randomForest package. Let’s use it to classify the iris dataset that comes packaged with R.\nFirst, let’s have a look at the dataset:\n\n# Structure of the dataset\nstr(iris)\n\n'data.frame':   150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\n# Dimensions of the dataset\ndim(iris)\n\n[1] 150   5\n\n# First 6 data points\nhead(iris)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\n# The 3 species (3 levels of the factor)\nlevels(iris$Species)\n\n[1] \"setosa\"     \"versicolor\" \"virginica\" \n\n\nThe goal is to create a random forest model (let’s call it rf) that can classify an iris flower in one of the 3 species based on the 4 measurements of its sepals and petals.\n\nlibrary(randomForest)\n\nrandomForest 4.7-1.1\n\n\nType rfNews() to see new features/changes/bug fixes.\n\nset.seed(123)\nrf &lt;- randomForest(Species ~ ., data=iris)\n\n\nOur response variable (Species) is a factor, so classification is assumed.\nThe . on the right side of the formula represents all other variables (so we are using all variables, except for the response variable Species of course, as feature variables).\n\n\nrf\n\n\nCall:\n randomForest(formula = Species ~ ., data = iris) \n               Type of random forest: classification\n                     Number of trees: 500\nNo. of variables tried at each split: 2\n\n        OOB estimate of  error rate: 4.67%\nConfusion matrix:\n           setosa versicolor virginica class.error\nsetosa         50          0         0        0.00\nversicolor      0         47         3        0.06\nvirginica       0          4        46        0.08\n\n\nAs can be seen by the confusion matrix, our model performs well.\nWe can use it on new data to make predictions. Let’s try with some made-up data:\n\nnew_data &lt;- data.frame(\n  Sepal.Length = c(5.3, 4.6, 6.5),\n  Sepal.Width = c(3.1, 3.9, 2.5),\n  Petal.Length = c(1.5, 1.5, 5.0),\n  Petal.Width = c(0.2, 0.1, 2.1)\n)\n\nnew_data\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width\n1          5.3         3.1          1.5         0.2\n2          4.6         3.9          1.5         0.1\n3          6.5         2.5          5.0         2.1\n\npredict(rf, new_data)\n\n        1         2         3 \n   setosa    setosa virginica \nLevels: setosa versicolor virginica\n\n\n\n\nLet’s make it big\nNow, the iris dataset only has 150 observations and we used the default number of trees (500) of the randomForest() function, so things ran fast. Often, random forests are run on large datasets. Let’s artificially increase the iris dataset and use more trees to create a situation in which parallelization would make sense.\nOne easy way is to replicate each row 100 times (and we can then delete the row names that get created by this operation):\n\nbig_iris &lt;- iris[rep(seq_len(nrow(iris)), each = 1e2), ]\nrownames(big_iris) &lt;- NULL\n\n\ndim(big_iris)\n\n[1] 15000     5\n\n\nAnd then we can run randomForest() on this dataset and 2000 trees.",
    "crumbs": [
      "R",
      "<b><em>High-performance R</em></b>",
      "foreach and doFuture"
    ]
  },
  {
    "objectID": "r/hpc_foreach.html#hidden-parallelism-check",
    "href": "r/hpc_foreach.html#hidden-parallelism-check",
    "title": "foreach and doFuture",
    "section": "Hidden parallelism check",
    "text": "Hidden parallelism check\nBefore parallelizing your code, remember to check whether the package you are using is already doing any parallelization under the hood (after all, maybe the randomForest package runs things in parallel. We don’t know).\nOne way to do this is to test the package on your local machine and, while some sample code is running, to open htop and see how many cores are used.\nWhy do this on your local machine? because on the cluster, if you launch htop while your batch job is running, you will be looking at processes running on the login node while your code is running on compute node(s). So this will not help you. You could salloc on the/one of the compute node(s) running your job and run htop there, but in production clusters, compute nodes are large and you will see all the processes from all the other users using that compute node. So this test is just easier done locally.\nOn my machine I ran:\nlibrary(randomForest)\n\nbig_iris &lt;- iris[rep(seq_len(nrow(iris)), each = 1e2), ]\nrownames(big_iris) &lt;- NULL\n\nset.seed(123)\nrf &lt;- randomForest(Species ~ ., data=big_iris, ntree=2000)\nAnd I could confirm that the function does not run in parallel.\nSo let’s parallelize this code.",
    "crumbs": [
      "R",
      "<b><em>High-performance R</em></b>",
      "foreach and doFuture"
    ]
  },
  {
    "objectID": "r/hpc_foreach.html#the-foreach-package",
    "href": "r/hpc_foreach.html#the-foreach-package",
    "title": "foreach and doFuture",
    "section": "The foreach package",
    "text": "The foreach package\nThe foreach package provides a construct for repeated executions, i.e. it can replace for loops, while loops, repeat loops, and functional programming code written with the *apply functions or the purrr package. The foreach vignette gives many examples.",
    "crumbs": [
      "R",
      "<b><em>High-performance R</em></b>",
      "foreach and doFuture"
    ]
  },
  {
    "objectID": "r/hpc_foreach.html#the-dofuture-package",
    "href": "r/hpc_foreach.html#the-dofuture-package",
    "title": "foreach and doFuture",
    "section": "The doFuture package",
    "text": "The doFuture package\nThe most useful part of foreach is that it allows for easily parallelization with countless backends: doFuture, doMC, doMPI, doParallel, doRedis, doRNG, doSNOW, and doAzureParallel.\nThe doFuture package is the most modern of these backends. It allows to evaluate foreach expressions across the evaluation strategies of the future package very easily. All you have to do is to register it as a backend, declare the evaluation strategy of futures of your choice, make sure to generate parallel-safe random numbers for reproducibility (if your code uses randomness), and replace %do% with %dofuture%.",
    "crumbs": [
      "R",
      "<b><em>High-performance R</em></b>",
      "foreach and doFuture"
    ]
  },
  {
    "objectID": "r/hpc_foreach.html#benchmarks",
    "href": "r/hpc_foreach.html#benchmarks",
    "title": "foreach and doFuture",
    "section": "Benchmarks",
    "text": "Benchmarks\nWe will run and benchmark all versions of our code by submitting batch jobs to Slurm.\n\nInitial code\nFirst, let’s benchmark the initial (non parallel, not using foreach) code. We need to create an R script. Let’s call it reference.R (I will use Emacs, but you can use the nano text editor with the command nano to write the script if you want):\n\n\nreference.R\n\nlibrary(randomForest)\nlibrary(bench)\n\nbig_iris &lt;- iris[rep(seq_len(nrow(iris)), each = 1e2), ]\nrownames(big_iris) &lt;- NULL\n\ncat(\"\\nBenchmarking results:\\n\\n\")\n\nset.seed(123)\nmark(rf &lt;- randomForest(Species ~ ., data=big_iris, ntree=2000))\n\nThen we need to create a Bash script for Slurm. Let’s call it reference.sh:\n\n\nreference.sh\n\n#!/bin/bash\n#SBATCH --time=5\n#SBATCH --mem-per-cpu=7500M\n\nRscript reference.R\n\n\nYou can see the full list of sbatch options here.\n\nAnd now we submit the job with:\nsbatch reference.sh\nYou can monitor your job with sq. The result will be written to a file called slurm-xx.out with xx being the number of the job that just ran. To see the result, we can simply print the content of that file to screen with cat (you can run ls to see the list of files in the current directory). Make sure that your job has finished running before printing the result (otherwise you might get a partial output which can be confusing).\ncat slurm-xx.out    # Replace xx by the job number\nrandomForest 4.7-1.1\nType rfNews() to see new features/changes/bug fixes.\n\nBenchmarking results:\n\n# A tibble: 1 × 13\n  expression      min median `itr/sec` mem_alloc `gc/sec` n_itr  n_gc total_time\n  &lt;bch:expr&gt;    &lt;bch&gt; &lt;bch:&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;   &lt;bch:tm&gt;\n1 rf &lt;- random… 6.33s  6.33s     0.158        NA    0.474     1     3      6.33s\n# ℹ 4 more variables: result &lt;list&gt;, memory &lt;list&gt;, time &lt;list&gt;, gc &lt;list&gt;\nWarning message:\nSome expressions had a GC in every iteration; so filtering is disabled.\n\n\nforeach expression\nNow, let’s try the foreach version:\n\n\nforeach.R\n\nlibrary(foreach)\nlibrary(randomForest)\nlibrary(bench)\n\nbig_iris &lt;- iris[rep(seq_len(nrow(iris)), each = 1e2), ]\nrownames(big_iris) &lt;- NULL\n\ncat(\"\\nBenchmarking results:\\n\\n\")\n\nset.seed(123)\nmark(\n  rf &lt;- foreach(ntree = rep(250, 8), .combine = combine) %do%\n    randomForest(Species ~ ., data=big_iris, ntree=ntree)\n)\n\n\n\nforeach.sh\n\n#!/bin/bash\n#SBATCH --time=5\n#SBATCH --mem-per-cpu=7500M\n\nRscript foreach.R\n\nsbatch foreach.sh\nrandomForest 4.7-1.1\nType rfNews() to see new features/changes/bug fixes.\n\nBenchmarking results:\n\n# A tibble: 1 × 13\n  expression      min median `itr/sec` mem_alloc `gc/sec` n_itr  n_gc total_time\n  &lt;bch:expr&gt;    &lt;bch&gt; &lt;bch:&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;   &lt;bch:tm&gt;\n1 rf &lt;- foreac… 7.04s  7.04s     0.142        NA     4.55     1    32      7.04s\n# ℹ 4 more variables: result &lt;list&gt;, memory &lt;list&gt;, time &lt;list&gt;, gc &lt;list&gt;\nWarning message:\nSome expressions had a GC in every iteration; so filtering is disabled.\n\nThe foreach expression is slower than the standard expression (it is always the case: foreach slows things down before this overhead gets offset by parallelization).\n\n\n\nPlan sequential\nYou might wonder why the sequential evaluation strategy exists (i.e. why go through all the trouble of writing your code with foreach and doFuture to then run it without parallelism?).\nThere are many reasons:\n\nIt can be very useful for debugging.\nIt makes it easy to switch the futures execution strategy back and forth for different sections of the code (maybe you don’t want to run everything in parallel).\nIt allows other people to run the same code on their different hardware without changing it (if they don’t have the resources to run things in parallel, they only have to change the execution strategy).\n\nTo turn the code into a parallelizable version with doFuture, we replace %do% with %dofuture%.\nHere, we also need to use the option .options.future = list(seed = TRUE): whenever your parallel code rely on a random process, it isn’t enough to use set.seed() to ensure reproducibility, you also need to generate parallel-safe random numbers. In random forest, each tree is trained on a random subset of the data and random variables are selected for splitting at each node. The option .options.future = list(seed = TRUE) pregenerates the random seeds using L’Ecuyer-CMRG RNG streams1.\nThis is the parallelizable foreach code, but run sequentially:\n\n\nsequential.R\n\nlibrary(doFuture)    # Also loads foreach and future\nlibrary(randomForest)\nlibrary(bench)\n\nregisterDoFuture()   # Set the parallel backend\nplan(sequential)     # Set the evaluation strategy\n\nbig_iris &lt;- iris[rep(seq_len(nrow(iris)), each = 1e2), ]\nrownames(big_iris) &lt;- NULL\n\ncat(\"\\nBenchmarking results:\\n\\n\")\n\nset.seed(123)\nmark(\n  rf &lt;- foreach(\n    ntree = rep(250, 8),\n    .options.future = list(seed = TRUE),\n    .combine = combine\n  ) %dofuture%\n    randomForest(Species ~ ., data=big_iris, ntree=ntree)\n)\n\n\n\nsequential.sh\n\n#!/bin/bash\n#SBATCH --time=5\n#SBATCH --mem-per-cpu=7500M\n\nRscript sequential.R\n\nsbatch sequential.sh\nLoading required package: foreach\nLoading required package: future\nrandomForest 4.7-1.1\nType rfNews() to see new features/changes/bug fixes.\n\nBenchmarking results:\n\n# A tibble: 1 × 13\n  expression      min median `itr/sec` mem_alloc `gc/sec` n_itr  n_gc total_time\n  &lt;bch:expr&gt;    &lt;bch&gt; &lt;bch:&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;   &lt;bch:tm&gt;\n1 rf &lt;- foreac… 8.39s  8.39s     0.119        NA     3.81     1    32      8.39s\n# ℹ 4 more variables: result &lt;list&gt;, memory &lt;list&gt;, time &lt;list&gt;, gc &lt;list&gt;\nWarning message:\nSome expressions had a GC in every iteration; so filtering is disabled.\n\nEach time we add unnecessary complexity in the code, things run a little slower.\n\n\n\nMulti-processing in shared memory\nNow, it is time to parallelize. First, we will use multiple cores on a single node (shared-memory parallelism).\n\nNumber of cores\nThe future package provides the availableCores() function to detect the number of available cores. We will run it as part of our script as a check on our available hardware.\nThe cluster for this course is made of 20 nodes with 4 CPUs each. We want to test shared memory parallelism, so our job needs to stay within one node. We can thus ask for a maximum of 4 CPUs and we want to ensure that we aren’t getting them on different nodes. Let’s go with that maximum of 4 cores.\n\n\nMultisession\nShared memory multi-processing can be run with plan(multisession) that will spawn new R sessions in the background to evaluate futures.\n\n\nmultisession.R\n\nlibrary(doFuture)\nlibrary(randomForest)\nlibrary(bench)\n\n# Check number of cores:\ncat(\"\\nWe have\", availableCores(), \"cores.\\n\\n\")\n\nregisterDoFuture()   # Set the parallel backend\nplan(multisession)   # Set the evaluation strategy\n\nbig_iris &lt;- iris[rep(seq_len(nrow(iris)), each = 1e2), ]\nrownames(big_iris) &lt;- NULL\n\ncat(\"\\nBenchmarking results:\\n\\n\")\n\nset.seed(123)\nmark(\n  rf &lt;- foreach(\n    ntree = rep(250, 8),\n    .options.future = list(seed = TRUE),\n    .combine = combine\n  ) %dofuture%\n    randomForest(Species ~ ., data=big_iris, ntree=ntree)\n)\n\n\n\nmultisession.sh\n\n#!/bin/bash\n#SBATCH --time=5\n#SBATCH --mem-per-cpu=7500M\n#SBATCH --cpus-per-task=4\n\nRscript multisession.R\n\nsbatch multisession.sh\nLoading required package: foreach\nLoading required package: future\nrandomForest 4.7-1.1\nType rfNews() to see new features/changes/bug fixes.\n\nWe have 4 cores.\n\nBenchmarking results:\n\n# A tibble: 1 × 13\n  expression      min median `itr/sec` mem_alloc `gc/sec` n_itr  n_gc total_time\n  &lt;bch:expr&gt;    &lt;bch&gt; &lt;bch:&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;   &lt;bch:tm&gt;\n1 rf &lt;- foreac… 2.72s  2.72s     0.368        NA     2.21     1     6      2.72s\n# ℹ 4 more variables: result &lt;list&gt;, memory &lt;list&gt;, time &lt;list&gt;, gc &lt;list&gt;\nWarning message:\nSome expressions had a GC in every iteration; so filtering is disabled.\nSpeedup: 3.1.\n\nNot too bad, considering that the ideal speedup, without any overhead, would be 4.\n\n\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=4\ncould be used instead of:\n#SBATCH --cpus-per-task=4\nWhat matters is to have 4 cores running on the same node to be in a shared memory parallelism scenario.\n\n\n\nMulticore\nShared memory multi-processing can also be run with plan(multicore) (except on Windows) that will fork the current R process to evaluate futures.\n\n\nmulticore.R\n\nlibrary(doFuture)\nlibrary(randomForest)\nlibrary(bench)\n\n# Check number of cores:\ncat(\"\\nWe have\", availableCores(), \"cores.\\n\\n\")\n\nregisterDoFuture()   # Set the parallel backend\nplan(multicore)      # Set the evaluation strategy\n\nbig_iris &lt;- iris[rep(seq_len(nrow(iris)), each = 1e2), ]\nrownames(big_iris) &lt;- NULL\n\ncat(\"\\nBenchmarking results:\\n\\n\")\n\nset.seed(123)\nmark(\n  rf &lt;- foreach(\n    ntree = rep(250, 8),\n    .options.future = list(seed = TRUE),\n    .combine = combine\n  ) %dofuture%\n    randomForest(Species ~ ., data=big_iris, ntree=ntree)\n)\n\n\n\nmulticore.sh\n\n#!/bin/bash\n#SBATCH --time=5\n#SBATCH --mem-per-cpu=7500M\n#SBATCH --cpus-per-task=4\n\nRscript multicore.R\n\nsbatch multicore.sh\nLoading required package: foreach\nLoading required package: future\nrandomForest 4.7-1.1\nType rfNews() to see new features/changes/bug fixes.\n\nWe have 4 cores.\n\nBenchmarking results:\n\n# A tibble: 1 × 13\n  expression      min median `itr/sec` mem_alloc `gc/sec` n_itr  n_gc total_time\n  &lt;bch:expr&gt;    &lt;bch&gt; &lt;bch:&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;   &lt;bch:tm&gt;\n1 rf &lt;- foreac… 3.15s  3.15s     0.318        NA     13.7     1    43      3.15s\n# ℹ 4 more variables: result &lt;list&gt;, memory &lt;list&gt;, time &lt;list&gt;, gc &lt;list&gt;\nWarning message:\nSome expressions had a GC in every iteration; so filtering is disabled.\nSpeedup: 2.7.\n\nWhile in theory we should get a similar speedup, we are getting a lower one here.\n\n\n\n\nMulti-processing in distributed memory\nLet’s run our distributed parallel code using 8 cores across 2 nodes.\nWe need to create a cluster of workers. We do this by creating a character vector with the names of the nodes our tasks are running on and passing it to the makeCluster() function from the parallel package (included in R):\n# Create a character vector with the nodes names\nhosts &lt;- system(\"srun hostname -s\", intern = T)\n\n# Create the cluster of workers\ncl &lt;- parallel::makeCluster(hosts)\nWe can verify that we did get 8 tasks by accessing the SLURM_NTASKS environment variable from within R:\nas.numeric(Sys.getenv(\"SLURM_NTASKS\"))\nHere is the R script:\n\n\ndistributed.R\n\nlibrary(doFuture)\nlibrary(randomForest)\nlibrary(bench)\n\n# Check number of tasks:\ncat(\"\\nWe have\", as.numeric(Sys.getenv(\"SLURM_NTASKS\")), \"tasks.\\n\\n\")\n\n# Create a character vector with the nodes names\nhosts &lt;- system(\"srun hostname -s\", intern = T)\n\n# Look at the location of our tasks:\ncat(\"\\nOur tasks are running on the following nodes: \", hosts)\n\n# Create the cluster of workers\ncl &lt;- parallel::makeCluster(hosts)\n\nregisterDoFuture()           # Set the parallel backend\nplan(cluster, workers = cl)  # Set the evaluation strategy\n\nbig_iris &lt;- iris[rep(seq_len(nrow(iris)), each = 1e2), ]\nrownames(big_iris) &lt;- NULL\n\ncat(\"\\nBenchmarking results:\\n\\n\")\n\nset.seed(123)\nmark(\n  rf &lt;- foreach(\n    ntree = rep(250, 8),\n    .options.future = list(seed = TRUE),\n    .combine = combine\n  ) %dofuture%\n    randomForest(Species ~ ., data=big_iris, ntree=ntree)\n)\n\n\nThe cluster of workers can be stopped with:\nparallel::stopCluster(cl)\nHere, this is not necessary since our job stops running as soon as the execution is complete, but in other systems, this will prevent you from monopolizing hardware or paying unnecessarily.\n\nAnd now we need to ask Slurm for 8 tasks on 2 nodes:\n\n\ndistributed.sh\n\n#!/bin/bash\n#SBATCH --time=5\n#SBATCH --mem-per-cpu=7500M\n#SBATCH --ntasks-per-node=4\n#SBATCH --nodes=2\n\nRscript distributed.R\n\nsbatch distributed.sh\nLoading required package: foreach\nLoading required package: future\nrandomForest 4.7-1.1\nType rfNews() to see new features/changes/bug fixes.\n\nWe have 8 tasks.\n\nOur tasks are running on the following nodes: \"node1\" \"node1\" \"node1\" \"node1\" \"node2\" \"node2\" \"node2\" \"node2\"\n\nBenchmarking results:\n\n# A tibble: 1 × 13\n  expression      min median `itr/sec` mem_alloc `gc/sec` n_itr  n_gc total_time\n  &lt;bch:expr&gt;    &lt;bch&gt; &lt;bch:&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;   &lt;bch:tm&gt;\n1 rf &lt;- foreac…  1.6s   1.6s     0.624        NA     3.12     1     5       1.6s\n# ℹ 4 more variables: result &lt;list&gt;, memory &lt;list&gt;, time &lt;list&gt;, gc &lt;list&gt;\nWarning message:\nSome expressions had a GC in every iteration; so filtering is disabled.\nSpeedup: 5.2.\n\nThe overhead is larger in distributed parallelism due to message passing between nodes. We are further from the ideal speedup of 8, but we still got a speedup larger than what we could have obtained with shared-memory parallelism.\n\n\n#SBATCH --ntasks=8\ncould be used instead of:\n#SBATCH --ntasks-per-node=4\n#SBATCH --nodes=2\nHowever the latter is slightly better because it allows us to use 2 full nodes instead of having tasks running on any number of nodes. However, it also means that we might have to wait longer for our job to run as it is more restrictive.",
    "crumbs": [
      "R",
      "<b><em>High-performance R</em></b>",
      "foreach and doFuture"
    ]
  },
  {
    "objectID": "r/hpc_foreach.html#footnotes",
    "href": "r/hpc_foreach.html#footnotes",
    "title": "foreach and doFuture",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nL’Ecuyer, P. (1999). Good parameters and implementations for combined multiple recursive random number generators. Operations Research, 47, 159–164.↩︎",
    "crumbs": [
      "R",
      "<b><em>High-performance R</em></b>",
      "foreach and doFuture"
    ]
  },
  {
    "objectID": "r/hpc_memory.html",
    "href": "r/hpc_memory.html",
    "title": "Memory management",
    "section": "",
    "text": "Memory can be a limiting factor and releasing it when not needed can be critical to avoid out of memory states. On the other hand, memoisation is an optimization technique which consists of caching the results of heavy computations for re-use.\nMemory and speed are thus linked in a trade-off.",
    "crumbs": [
      "R",
      "<b><em>High-performance R</em></b>",
      "Memory management"
    ]
  },
  {
    "objectID": "r/hpc_memory.html#releasing-memory",
    "href": "r/hpc_memory.html#releasing-memory",
    "title": "Memory management",
    "section": "Releasing memory",
    "text": "Releasing memory\nIt is best to avoid creating very large intermediate objects that take space in memory unnecessarily.\n\nOne option is to use nested functions or functions chained with pipes.\nAnother option is to create the intermediate objects within the local environment of a function as they will automatically be deleted as soon as the function has finished running.\n\nLet’s go over a basic example: let’s extract the sepal width variable from the iris dataset (one of the datasets that come packaged with R), take the natural logarithm of the values, and round them to one decimal place.\nFirst, let’s delete all objects inside our environment to make our little test as clean as possible:\n\nrm(list=ls())\nls()\n\ncharacter(0)\n\n\nNow, we could perform our task this way:\n\nsepalwidth &lt;- iris$Sepal.Width\nsepalwidth_ln &lt;- log(sepalwidth)\nround(sepalwidth_ln, 1)\n\n  [1] 1.3 1.1 1.2 1.1 1.3 1.4 1.2 1.2 1.1 1.1 1.3 1.2 1.1 1.1 1.4 1.5 1.4 1.3\n [19] 1.3 1.3 1.2 1.3 1.3 1.2 1.2 1.1 1.2 1.3 1.2 1.2 1.1 1.2 1.4 1.4 1.1 1.2\n [37] 1.3 1.3 1.1 1.2 1.3 0.8 1.2 1.3 1.3 1.1 1.3 1.2 1.3 1.2 1.2 1.2 1.1 0.8\n [55] 1.0 1.0 1.2 0.9 1.1 1.0 0.7 1.1 0.8 1.1 1.1 1.1 1.1 1.0 0.8 0.9 1.2 1.0\n [73] 0.9 1.0 1.1 1.1 1.0 1.1 1.1 1.0 0.9 0.9 1.0 1.0 1.1 1.2 1.1 0.8 1.1 0.9\n [91] 1.0 1.1 1.0 0.8 1.0 1.1 1.1 1.1 0.9 1.0 1.2 1.0 1.1 1.1 1.1 1.1 0.9 1.1\n[109] 0.9 1.3 1.2 1.0 1.1 0.9 1.0 1.2 1.1 1.3 1.0 0.8 1.2 1.0 1.0 1.0 1.2 1.2\n[127] 1.0 1.1 1.0 1.1 1.0 1.3 1.0 1.0 1.0 1.1 1.2 1.1 1.1 1.1 1.1 1.1 1.0 1.2\n[145] 1.2 1.1 0.9 1.1 1.2 1.1\n\n\nBut this creates the unnecessary intermediate variables sepalwidth and sepalwidth_ln which get stored in memory:\n\nls()\n\n[1] \"sepalwidth\"    \"sepalwidth_ln\"\n\n\nFor very large objects, this is not ideal.\nLet’s clear objects in our environment again:\n\nrm(list=ls())\nls()\n\ncharacter(0)\n\n\nA better option is to use nested functions:\n\nround(log(iris$Sepal.Width), 1)\n\n  [1] 1.3 1.1 1.2 1.1 1.3 1.4 1.2 1.2 1.1 1.1 1.3 1.2 1.1 1.1 1.4 1.5 1.4 1.3\n [19] 1.3 1.3 1.2 1.3 1.3 1.2 1.2 1.1 1.2 1.3 1.2 1.2 1.1 1.2 1.4 1.4 1.1 1.2\n [37] 1.3 1.3 1.1 1.2 1.3 0.8 1.2 1.3 1.3 1.1 1.3 1.2 1.3 1.2 1.2 1.2 1.1 0.8\n [55] 1.0 1.0 1.2 0.9 1.1 1.0 0.7 1.1 0.8 1.1 1.1 1.1 1.1 1.0 0.8 0.9 1.2 1.0\n [73] 0.9 1.0 1.1 1.1 1.0 1.1 1.1 1.0 0.9 0.9 1.0 1.0 1.1 1.2 1.1 0.8 1.1 0.9\n [91] 1.0 1.1 1.0 0.8 1.0 1.1 1.1 1.1 0.9 1.0 1.2 1.0 1.1 1.1 1.1 1.1 0.9 1.1\n[109] 0.9 1.3 1.2 1.0 1.1 0.9 1.0 1.2 1.1 1.3 1.0 0.8 1.2 1.0 1.0 1.0 1.2 1.2\n[127] 1.0 1.1 1.0 1.1 1.0 1.3 1.0 1.0 1.0 1.1 1.2 1.1 1.1 1.1 1.1 1.1 1.0 1.2\n[145] 1.2 1.1 0.9 1.1 1.2 1.1\n\n\nAn equivalent option is to chain functions:\n\niris$Sepal.Width |&gt; log() |&gt; round(1)\n\n  [1] 1.3 1.1 1.2 1.1 1.3 1.4 1.2 1.2 1.1 1.1 1.3 1.2 1.1 1.1 1.4 1.5 1.4 1.3\n [19] 1.3 1.3 1.2 1.3 1.3 1.2 1.2 1.1 1.2 1.3 1.2 1.2 1.1 1.2 1.4 1.4 1.1 1.2\n [37] 1.3 1.3 1.1 1.2 1.3 0.8 1.2 1.3 1.3 1.1 1.3 1.2 1.3 1.2 1.2 1.2 1.1 0.8\n [55] 1.0 1.0 1.2 0.9 1.1 1.0 0.7 1.1 0.8 1.1 1.1 1.1 1.1 1.0 0.8 0.9 1.2 1.0\n [73] 0.9 1.0 1.1 1.1 1.0 1.1 1.1 1.0 0.9 0.9 1.0 1.0 1.1 1.2 1.1 0.8 1.1 0.9\n [91] 1.0 1.1 1.0 0.8 1.0 1.1 1.1 1.1 0.9 1.0 1.2 1.0 1.1 1.1 1.1 1.1 0.9 1.1\n[109] 0.9 1.3 1.2 1.0 1.1 0.9 1.0 1.2 1.1 1.3 1.0 0.8 1.2 1.0 1.0 1.0 1.2 1.2\n[127] 1.0 1.1 1.0 1.1 1.0 1.3 1.0 1.0 1.0 1.1 1.2 1.1 1.1 1.1 1.1 1.1 1.0 1.2\n[145] 1.2 1.1 0.9 1.1 1.2 1.1\n\n\nAnother option is to create the intermediate variables in the local environment of a function:\n\nget_sepalwidth &lt;- function(dataset) {\n  sepalwidth &lt;- dataset$Sepal.Width\n  sepalwidth_ln &lt;- log(sepalwidth)\n  round(sepalwidth_ln, 1)\n}\n\nget_sepalwidth(iris)\n\n  [1] 1.3 1.1 1.2 1.1 1.3 1.4 1.2 1.2 1.1 1.1 1.3 1.2 1.1 1.1 1.4 1.5 1.4 1.3\n [19] 1.3 1.3 1.2 1.3 1.3 1.2 1.2 1.1 1.2 1.3 1.2 1.2 1.1 1.2 1.4 1.4 1.1 1.2\n [37] 1.3 1.3 1.1 1.2 1.3 0.8 1.2 1.3 1.3 1.1 1.3 1.2 1.3 1.2 1.2 1.2 1.1 0.8\n [55] 1.0 1.0 1.2 0.9 1.1 1.0 0.7 1.1 0.8 1.1 1.1 1.1 1.1 1.0 0.8 0.9 1.2 1.0\n [73] 0.9 1.0 1.1 1.1 1.0 1.1 1.1 1.0 0.9 0.9 1.0 1.0 1.1 1.2 1.1 0.8 1.1 0.9\n [91] 1.0 1.1 1.0 0.8 1.0 1.1 1.1 1.1 0.9 1.0 1.2 1.0 1.1 1.1 1.1 1.1 0.9 1.1\n[109] 0.9 1.3 1.2 1.0 1.1 0.9 1.0 1.2 1.1 1.3 1.0 0.8 1.2 1.0 1.0 1.0 1.2 1.2\n[127] 1.0 1.1 1.0 1.1 1.0 1.3 1.0 1.0 1.0 1.1 1.2 1.1 1.1 1.1 1.1 1.1 1.0 1.2\n[145] 1.2 1.1 0.9 1.1 1.2 1.1\n\n\nNone of these options left intermediate variables in our environment:\n\nls()\n\n[1] \"get_sepalwidth\"\n\n\nNote that in the case of a very large function, it might still be beneficial to run rm() inside the function to clear the memory for other processes coming next within that function. But this is a pretty rare case.\nIf you really have to create large intermediate objects in the global environment, make sure to delete them as soon as you don’t need them anymore (e.g. rm(sepalwidth, sepalwidth_ln)).\n\nrm() deletes the names of variables (the pointers to objects in memory). But as soon as all the pointers to an object in memory are deleted, the garbage collector clears its value and releases the memory it used.",
    "crumbs": [
      "R",
      "<b><em>High-performance R</em></b>",
      "Memory management"
    ]
  },
  {
    "objectID": "r/hpc_memory.html#caching",
    "href": "r/hpc_memory.html#caching",
    "title": "Memory management",
    "section": "Caching",
    "text": "Caching\nMemoisation is a technique by which some results are cached to avoid re-calculating them. This is convenient in a variety of settings (e.g. to reduce calls to an API, to avoid repeating heavy computations). In particular, it improves the efficiency of recursive function calls dramatically.\nLet’s consider the calculation of the Fibonacci numbers as an example. Those numbers form a sequence starting with 0, 11, after which each number is the sum of the previous two (so the series starts with: 0, 1, 1, 2, 3, 5, 8, 13...).\nHere is a function that would return the nth Fibonacci number2:\nfib &lt;- function(n) {\n  if(n == 0) {\n    return(0)\n  } else if(n == 1) {\n    return(1)\n  } else {\n    Recall(n - 1) + Recall(n - 2)\n  }\n}\nIt can be written more tersely as:\n\nfib &lt;- function(n) {\n  if(n == 0) return(0)\n  if(n == 1) return(1)\n  Recall(n - 1) + Recall(n - 2)\n}\n\n\nRecall() is a placeholder for the name of the recursive function. We could have used fib() instead, but Recall() is more robust as it allows for function renaming.\n\nMemoisation is very useful here because, for each Fibonacci number, we need to calculate the two preceding Fibonacci numbers and to calculate each of those we need to calculate the two Fibonacci numbers preceding that one and to calculate… etc. That is a large number of calculations, but, thanks to caching, we don’t have to calculate any one of them more than once.\nThe packages R.cache and memoise both allow for memoisation with an incredibly simple syntax.\nApplying the latter to our function gives us:\n\nlibrary(memoise)\n\nfibmem &lt;- memoise(\n  function(n) {\n    if(n == 0) return(0)\n    if(n == 1) return(1)\n    Recall(n - 1) + Recall(n - 2)\n  }\n)\n\nWe can do some benchmarking to see the speedup for the 30th Fibonacci number:\n\nlibrary(bench)\n\nn &lt;- 30\nmark(fib(n), fibmem(n))\n\nWarning: Some expressions had a GC in every iteration; so filtering is\ndisabled.\n\n\n# A tibble: 2 × 6\n  expression      min   median `itr/sec` mem_alloc `gc/sec`\n  &lt;bch:expr&gt; &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt;\n1 fib(n)        1.62s    1.62s     0.616    32.9KB     18.5\n2 fibmem(n)   41.22µs  44.62µs 20807.       68.3KB     14.6\n\n\nThe speedup is over 35,000!",
    "crumbs": [
      "R",
      "<b><em>High-performance R</em></b>",
      "Memory management"
    ]
  },
  {
    "objectID": "r/hpc_memory.html#footnotes",
    "href": "r/hpc_memory.html#footnotes",
    "title": "Memory management",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAlternative versions have the sequence start with 1, 1 or with 1, 2.↩︎\nThere are more efficient ways to calculate the Fibonacci numbers, but this inefficient function is a great example to show the advantage of memoisation.↩︎",
    "crumbs": [
      "R",
      "<b><em>High-performance R</em></b>",
      "Memory management"
    ]
  },
  {
    "objectID": "r/hpc_parallel_r.html",
    "href": "r/hpc_parallel_r.html",
    "title": "Running R code in parallel",
    "section": "",
    "text": "The parallel package has been part of the base package group since R version 2.14.0.\nThis means that it is comes with R, however it needs to be loaded in a session before its content can be accessed:\nlibrary(parallel)\nMost parallel approaches in R build on this package.\nAll other packages mentioned in this lesson are external packages and need to be installed with install.packages()."
  },
  {
    "objectID": "r/hpc_parallel_r.html#base-r-parallel-package",
    "href": "r/hpc_parallel_r.html#base-r-parallel-package",
    "title": "Running R code in parallel",
    "section": "",
    "text": "The parallel package has been part of the base package group since R version 2.14.0.\nThis means that it is comes with R, however it needs to be loaded in a session before its content can be accessed:\nlibrary(parallel)\nMost parallel approaches in R build on this package.\nAll other packages mentioned in this lesson are external packages and need to be installed with install.packages()."
  },
  {
    "objectID": "r/hpc_parallel_r.html#parallelly-package",
    "href": "r/hpc_parallel_r.html#parallelly-package",
    "title": "Running R code in parallel",
    "section": "parallelly package",
    "text": "parallelly package\nThe parallelly package—part of the futureverse suite of packages developed by Henrik Bengtsson—adds functionality to the parallel package."
  },
  {
    "objectID": "r/hpc_partition.html",
    "href": "r/hpc_partition.html",
    "title": "Partitioning data with multidplyr",
    "section": "",
    "text": "The package multidplyr provides simple techniques to partition data across a set of workers on the same node.",
    "crumbs": [
      "R",
      "<b><em>High-performance R</em></b>",
      "Partitioning data"
    ]
  },
  {
    "objectID": "r/hpc_partition.html#data-partitioning-for-memory",
    "href": "r/hpc_partition.html#data-partitioning-for-memory",
    "title": "Partitioning data with multidplyr",
    "section": "Data partitioning for memory",
    "text": "Data partitioning for memory\n\nCase example\nWhat if we have an even bigger dataset?\nThe randomForest() function has limitations:\n\nIt is a memory hog.\nIt doesn’t run if your data frame has too many rows.\n\nIf you try to run:\n\n\nbigger.R\n\nlibrary(randomForest)\n\nbigger_iris &lt;- iris[rep(seq_len(nrow(iris)), each = 1e3), ]\nrownames(bigger_iris) &lt;- NULL\n\nset.seed(123)\nrf &lt;- randomForest(Species ~ ., data = bigger_iris)\n\nrf\n\non a single core, you will get:\nrandomForest 4.7-1.1\nType rfNews() to see new features/changes/bug fixes.\n/var/spool/slurmd/job00016/slurm_script: line 5: 74451 Killed                  Rscript data_partition.R\nslurmstepd: error: Detected 1 oom-kill event(s) in StepId=16.batch. Some of your processes may have been killed by the cgroup out-of-memory handler.\nYou have ran out of memory.\nReducing the number of trees won’t help as the problem comes from the size of the data frame.\nSimilarly, using foreach and doFuture as we did previously won’t help either because that spreads the number of trees on various cores, but again, the problem doesn’t come from the number of trees, but for the size of the dataset.\n\nWith plan(multisession), you would get:\nCluster with multisession\nError in unserialize(node$con) :\n  MultisessionFuture (doFuture2-3) failed to receive message results from cluster RichSOCKnode #3 (PID 445273 on localhost ‘localhost’). The reason reported was ‘error reading from connection’. Post-mortem diagnostic: No process exists with this PID, i.e. the localhost worker is no longer alive. The total size of the 3 globals exported is 5.15 MiB. There are three globals: ‘big_iris’ (5.15 MiB of class ‘list’), ‘...future.seeds_ii’ (160 bytes of class ‘list’) and ‘...future.x_ii’ (112 bytes of class ‘list’)\nAnd with plan(multicore):\nCluster with multicore\nError: Failed to retrieve the result of MulticoreFuture (doFuture2-2) from the forked worker (on localhost; PID 444769). Post-mortem diagnostic: No process exists with this PID, i.e. the forked localhost worker is no longer alive. The total size of the 3 globals exported is 5.15 MiB. There are three globals: ‘big_iris’ (5.15 MiB of class ‘list’), ‘...future.seeds_ii’ (160 bytes of class ‘list’) and ‘...future.x_ii’ (112 bytes of class ‘list’)\nIn addition: Warning message:\nIn mccollect(jobs = jobs, wait = TRUE) :\n  1 parallel job did not deliver a result\n\nYou can even try spreading the trees on multiple nodes, but things will fail as well, without any error message.\nOf course, you could always try on a different machine—one with more memory. I used my machine which has more memory than this training cluster and it worked.\nBut then, what if big_iris is even bigger? Say, if we have this for instance:\nbigger_iris &lt;- iris[rep(seq_len(nrow(iris)), each = 1e4), ]\nThen no amount of memory will save you and you will get errors similar to this:\nError in randomForest.default(m, y, ...) : \n  long vectors (argument 28) are not supported in .C\nThat’s because randomForest() does not accept datasets with too many rows.\n\nThe bottom line is that there are situation in which the data is just too big. In such cases, you want to look at data parallelism: instead of splitting your code into tasks that can run in parallel as we did previously, you split the data into chunks and run the code in parallel on those chunks.\n\n\nOf course, you could also simply run the code on a subset of your data. In many situation, reducing your data by sampling it properly will be good enough. But there are situations in which you want to use a huge dataset.\n\nYou could split the data manually and run the code on each chunk, but it would be tedious and very lengthy. And to run the code on all the chunks in parallel, you could implement that yourself. There is a much simpler option provided by the multidplyr package.\n\n\nUsing multidplyr\nTo see what happens as we use multidplyr, let’s first run the code in an interactive session on one node with 4 cores:\n# Launch the interactive job\nsalloc --time=50 --mem-per-cpu=7500M --cpus-per-task=4\n\n# Then launch R\nR\nFirst, we load the packages that are running in the main session:\n\nlibrary(multidplyr)\nlibrary(dplyr, warn.conflicts = FALSE)\n\n\nWe load dplyr for the do() function.\nNotice that we aren’t loading the randomForest package yet: that’s because we will use it on workers, not in the main session.\n\nThen we need to create a cluster of workers. Let’s use 4 workers that we will run on a full node:\n\ncl &lt;- new_cluster(4)\ncl\n\n4 session cluster [....]\n\n\nNow we can load the randomForest package on each worker:\ncluster_library(cl, \"randomForest\")\nrandomForest 4.7-1.1\nType rfNews() to see new features/changes/bug fixes.\n\nAttaching package: ‘randomForest’\n\nThe following object is masked from ‘package:dplyr’:\n\n    combine\nOf course, we need to generate our big dataset:\n\nbigger_iris &lt;- iris[rep(seq_len(nrow(iris)), each = 1e3), ]\nrownames(bigger_iris) &lt;- NULL\n\nThen we create a partitioned data frame on the workers with the partition() function. The function will try to split the data as heavenly as possible among workers.\nIf you group observations by some variable (with dplyr::group_by()) beforehand, multidplyr will ensure that all data points in a group end up on the same worker. This is very convenient in a lot of cases, but is not relevant here. Without grouping observations first, it is unclear how partition() chooses which observation goes to which worker. In our data, we have all the setosa observations first, then all the versicolor, and finally all the virginica. We want to make sure that the randomForest() function runs on a sample of all 3 species. We will thus randomly shuffle the data before partitioning it (when we were parallelizing by splitting the trees, we didn’t have to worry about that since each subset of trees was running on the entire dataset):\n\n# Shuffle the rows of the data frame randomly\nset.seed(11)\nbigger_iris_shuffled &lt;- bigger_iris[sample(nrow(bigger_iris)), ]\n\n# You can check that they are shuffled\nhead(bigger_iris_shuffled)\n\n       Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n65570           6.7         3.1          4.4         1.4 versicolor\n19004           5.1         3.8          1.5         0.3     setosa\n73612           6.1         2.8          4.7         1.2 versicolor\n28886           5.2         3.4          1.4         0.2     setosa\n121310          5.6         2.8          4.9         2.0  virginica\n21667           5.1         3.7          1.5         0.4     setosa\n\n\n# Create the partitioned data frame\nsplit_iris &lt;- partition(bigger_iris_shuffled, cl)\nsplit_iris\nSource: party_df [150,000 x 5]\nShards: 4 [37,500--37,500 rows]\n\n# A data frame: 150,000 × 5\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n         &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;\n1          6.7         3.1          4.4         1.4 versicolor\n2          5.6         2.8          4.9         2   virginica\n3          6.4         2.8          5.6         2.2 virginica\n4          5.6         2.5          3.9         1.1 versicolor\n5          4.7         3.2          1.6         0.2 setosa\n6          6.7         3            5           1.7 versicolor\n# ℹ 149,994 more rows\n# ℹ Use `print(n = ...)` to see more rows\nIf we want the code to be reproducible, we should set the seed on each worker:\ncluster_send(cl, set.seed(123))\n\nRun cluster_send() to send code to each worker when you aren’t interested in any result (as is the case here) and cluster_call() if you want a computation to be executed on each worker and a result to be returned.\n\nNow we can run the randomForest() function on each worker:\nsplit_rfs &lt;- split_iris %&gt;%\n  do(rf = randomForest(Species ~ ., data = .))\nsplit_rfs is a partitioned data frame containing the results from each worker (the intermediate randomForest models):\nsplit_rfs\nSource: party_df [4 x 1]\nShards: 4 [1--1 rows]\n\n# A data frame: 4 × 1\n  rf\n  &lt;list&gt;\n1 &lt;rndmFrs.&gt;\n2 &lt;rndmFrs.&gt;\n3 &lt;rndmFrs.&gt;\n4 &lt;rndmFrs.&gt;\nNow we need to bring the partitioned results in the main process:\nrfs &lt;- split_rfs %&gt;% collect()\nrfs is a data frame with a single column called rf:\nrfs\n# A tibble: 4 × 1\n  rf\n  &lt;list&gt;\n1 &lt;rndmFrs.&gt;\n2 &lt;rndmFrs.&gt;\n3 &lt;rndmFrs.&gt;\n4 &lt;rndmFrs.&gt;\nWhich means that rfs$rf is a list:\ntypeof(rfs$rf)\n[1] \"list\"\nEach element of this list is a randomForest object (the 4 intermediate models created by the 4 workers):\nrfs$rf\n[[1]]\n\nCall:\n randomForest(formula = Species ~ ., data = .)\n               Type of random forest: classification\n                     Number of trees: 500\nNo. of variables tried at each split: 2\n\n        OOB estimate of  error rate: 0%\nConfusion matrix:\n           setosa versicolor virginica class.error\nsetosa      12500          0         0           0\nversicolor      0      12500         0           0\nvirginica       0          0     12500           0\n\n[[2]]\n\nCall:\n randomForest(formula = Species ~ ., data = .)\n               Type of random forest: classification\n                     Number of trees: 500\nNo. of variables tried at each split: 2\n\n        OOB estimate of  error rate: 0%\nConfusion matrix:\n           setosa versicolor virginica class.error\nsetosa      12500          0         0           0\nversicolor      0      12500         0           0\nvirginica       0          0     12500           0\n\n[[3]]\n\nCall:\n randomForest(formula = Species ~ ., data = .)\n               Type of random forest: classification\n                     Number of trees: 500\nNo. of variables tried at each split: 2\n\n        OOB estimate of  error rate: 0%\nConfusion matrix:\n           setosa versicolor virginica class.error\nsetosa      12500          0         0           0\nversicolor      0      12500         0           0\nvirginica       0          0     12500           0\n\n[[4]]\n\nCall:\n randomForest(formula = Species ~ ., data = .)\n               Type of random forest: classification\n                     Number of trees: 500\nNo. of variables tried at each split: 2\n\n        OOB estimate of  error rate: 0%\nConfusion matrix:\n           setosa versicolor virginica class.error\nsetosa      12500          0         0           0\nversicolor      0      12500         0           0\nvirginica       0          0     12500           0\n\nIf you don’t need to explore the intermediate objects, you can combine the commands as:\nrfs &lt;- split_iris %&gt;%\n  do(rf = randomForest(Species ~ ., data = .)) %&gt;%\n  collect()\n\nFinally, we need to combine the 4 randomForest models into a single one. This can be done with the combine() function from the randomForest package (the same function we already used in our foreach expressions):\nrf_all &lt;- do.call(combine, rfs$rf)\n\nBe careful that randomForest and dplyr both have a combine() function. The one we want here is the one from the randomForest package. To avoid all conflict and confusion, you can use randomForest::combine(). combine() is ok if you make sure to load dplyr before randomForest since latest loaded functions overwrite earlier loaded ones.\n\nWhy are we using do.call()? If we use:\ncombine(rfs$rf)\nWe get the silly message:\nError in combine(rfs$rf) :\n  Argument must be a list of randomForest objects\nThat is because randomForest::combine() expects a list of randomForest objects, but cannot accept an object of type list.\nHere is our final randomForest model:\nrf_all\nCall:\n randomForest(formula = Species ~ ., data = .)\n               Type of random forest: classification\n                     Number of trees: 2000\nNo. of variables tried at each split: 2\nThis is it: by splitting our data frame on 4 cores, we could run the code and create a randomForest model using whole of the data.\nWe can test our model:\nnew_data &lt;- data.frame(\n  Sepal.Length = c(5.3, 4.6, 6.5),\n  Sepal.Width = c(3.1, 3.9, 2.5),\n  Petal.Length = c(1.5, 1.5, 5.0),\n  Petal.Width = c(0.2, 0.1, 2.1)\n)\n\npredict(rf_all, new_data)\n        1         2         3\n   setosa    setosa virginica\nLevels: setosa versicolor virginica\nRunning this in an interactive session was useful to see what happens, but the way you would actually do this is by writing a script (let’s call it partition.R):\n\n\npartition.R\n\nlibrary(multidplyr)\nlibrary(dplyr, warn.conflicts = FALSE)\n\n# Create cluster of workers\ncl &lt;- new_cluster(4)\n\n# Load randomForest on each worker\ncluster_library(cl, \"randomForest\")\n\n# Create our big data frame\nbigger_iris &lt;- iris[rep(seq_len(nrow(iris)), each = 1e3), ]\nrownames(bigger_iris) &lt;- NULL\n\n# Create a partitioned data frame on the workers\nsplit_iris &lt;- partition(bigger_iris, cl)\n\n# Set the seed on each worker\ncluster_send(cl, set.seed(123))\n\n# Run the randomForest() function on each worker\nrfs &lt;- split_iris %&gt;%\n  do(rf = randomForest(Species ~ ., data = .)) %&gt;%\n  collect()\n\n# Combine the randomForest models into one\nrf_all &lt;- do.call(combine, rfs$rf)\n\nAnd run it with a Bash partition.sh script:\n\n\npartition.sh\n\n#!/bin/bash\n#SBATCH --time=10\n#SBATCH --mem-per-cpu=7500M\n#SBATCH --cpus-per-task=4\n\nRscript partition.R\n\n\n\nConclusion\nmultidplyr allowed us to split our data frame across multiple workers on one node and this solved the memory issue we had with our large dataset.",
    "crumbs": [
      "R",
      "<b><em>High-performance R</em></b>",
      "Partitioning data"
    ]
  },
  {
    "objectID": "r/hpc_partition.html#data-partitioning-for-speed",
    "href": "r/hpc_partition.html#data-partitioning-for-speed",
    "title": "Partitioning data with multidplyr",
    "section": "Data partitioning for speed",
    "text": "Data partitioning for speed\nBeside the memory advantage, are we getting any speedup from data parallelization? i.e. how does this code compare with the parallelization we did as regard the number of trees with foreach and doFuture?\nWe want to make sure to compare the same things. So we go back to our smaller big_iris and we up the number of trees back to 2000.\nWe will compare it with the plans multisession and multicore that we performed earlier. The minimum and median times for these two options for shared memory parallelism were of 2.72s and 3.15s respectively.\n\n\npartition_bench.R\n\nlibrary(multidplyr)\nlibrary(dplyr, warn.conflicts = FALSE)\nlibrary(bench)\n\ncl &lt;- new_cluster(4)\ncluster_library(cl, \"randomForest\")\n\nbig_iris &lt;- iris[rep(seq_len(nrow(iris)), each = 1e2), ]\nrownames(big_iris) &lt;- NULL\n\ncluster_send(cl, set.seed(123))\n\npart_rf &lt;- function(data, cluster) {\n  split_data &lt;- partition(data, cluster)\n  rfs &lt;- split_data %&gt;%\n    do(rf = randomForest(Species ~ ., data = ., ntree = 2000)) %&gt;%\n    collect()\n  do.call(combine, rfs$rf)\n}\n\nmark(rf_all &lt;- part_rf(big_iris, cl))\n\n\n\npartition_bench.sh\n\n#!/bin/bash\n#SBATCH --time=10\n#SBATCH --mem-per-cpu=7500M\n#SBATCH --cpus-per-task=4\n\nRscript partition_bench.R\n\nsbatch partition_bench.sh\nrandomForest 4.7-1.1\nType rfNews() to see new features/changes/bug fixes.\n\nAttaching package: ‘randomForest’\n\nThe following object is masked from ‘package:dplyr’:\n\n    combine\n\n# A tibble: 1 × 13\n  expression      min median `itr/sec` mem_alloc `gc/sec` n_itr  n_gc total_time\n  &lt;bch:expr&gt;    &lt;bch&gt; &lt;bch:&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;   &lt;bch:tm&gt;\n1 rf_all &lt;- pa… 2.48s  2.48s     0.403        NA     2.02     1     5      2.48s\n# ℹ 4 more variables: result &lt;list&gt;, memory &lt;list&gt;, time &lt;list&gt;, gc &lt;list&gt;\nWarning message:\nSome expressions had a GC in every iteration; so filtering is disabled.\n\nWhat about distributed memory?\nCan multidplyr run in distributed memory? There is nothing on this in the documentation, so I tried it.\nI upped the number of workers to 8 and ran the code on 2 nodes with 4 cores per node and got no speedup. I also created a dataset 10 times bigger (with each = 1e4), which creates an OOM on 4 cores one a single node and tried it on 11 nodes with 4 cores (10 to match the 10 times size increase, plus one to play safe). This didn’t solve the OOM issue. I tried various other tests, all with no success.\nIn conclusion, it seems that multidply’s way of creating a cluster of workers doesn’t have a mechanism to spread them across nodes and that the package thus does not allow to split data across nodes.\nIn cases where your data is so big that it doesn’t fit in the memory of a single node, it doesn’t seem that any R package currently allow to split the data automatically for you.\n\n\nConclusion\nAs we could see, we got similar results: in this case, it is the same to spread the number of trees running on the full data on 4 cores (as we did with foreach and doFuture or to run all the trees on the data spread on 4 cores.\nThe difference being that foreach and doFuture allowed us to spread the trees across nodes while multidplyr does not allow this for the data.",
    "crumbs": [
      "R",
      "<b><em>High-performance R</em></b>",
      "Partitioning data"
    ]
  },
  {
    "objectID": "r/hpc_partition.html#direct-data-loading",
    "href": "r/hpc_partition.html#direct-data-loading",
    "title": "Partitioning data with multidplyr",
    "section": "Direct data loading",
    "text": "Direct data loading\nThe method we used is very convenient, but it involves copying the data to the workers. If you want to save some memory, you can load the split data directly to the workers.\nFor this, first, split your data into several files and have all those files (and only those files) in a directory.\nThen, you can run:\nlibrary(multidplyr)\nlibrary(dplyr)\nlibrary(vroom)\n\n# Create the cluster of workers\ncl &lt;- new_cluster(4)\n\n# Create a character vector with the list of data files\nfiles &lt;- dir(\"/path/to/data/directory\", full.names = TRUE)\n\n# Split up the vector amongst the workers\ncluster_assign_partition(cl, files = files)\n\n# Create a data frame called split_iris on each worker\ncluster_send(cl, split_iris &lt;- vroom(files))\n\n# Create the partitioned data frame from the workers' data frames\nsplit_iris &lt;- party_df(cl, \"split_iris\")\nFrom here on, you can work as we did earlier.",
    "crumbs": [
      "R",
      "<b><em>High-performance R</em></b>",
      "Partitioning data"
    ]
  },
  {
    "objectID": "r/hpc_rcpp.html",
    "href": "r/hpc_rcpp.html",
    "title": "Writing C++ in R with Rcpp",
    "section": "",
    "text": "Sometimes, parallelization is not an option, either because the code is hard to parallelize or because of lack of hardware. In such cases, one way to increase speed is to replace slow R code with C++ code. The package Rcpp makes this easier by creating mappings between both languages and allowing you to embed snippets of C++ code directly in R and removing the need for pre-compilation.",
    "crumbs": [
      "R",
      "<b><em>High-performance R</em></b>",
      "Writing C++ in R with Rcpp"
    ]
  },
  {
    "objectID": "r/hpc_rcpp.html#back-to-fibonacci",
    "href": "r/hpc_rcpp.html#back-to-fibonacci",
    "title": "Writing C++ in R with Rcpp",
    "section": "Back to Fibonacci",
    "text": "Back to Fibonacci\nDo you remember the Fibonacci numbers? Here was a naive implementation in R:\n\nfib &lt;- function(n) {\n  if(n == 0) return(0)\n  if(n == 1) return(1)\n  Recall(n - 1) + Recall(n - 2)\n}\n\nThis function gives the nth number in the sequence.\n\nExample:\n\n\nfib(30)\n\n[1] 832040",
    "crumbs": [
      "R",
      "<b><em>High-performance R</em></b>",
      "Writing C++ in R with Rcpp"
    ]
  },
  {
    "objectID": "r/hpc_rcpp.html#rcpp",
    "href": "r/hpc_rcpp.html#rcpp",
    "title": "Writing C++ in R with Rcpp",
    "section": "Rcpp",
    "text": "Rcpp\nLet’s translate this function in C++ within R!\nFirst we need to load the Rcpp package:\n\nlibrary(Rcpp)\n\nWe then use the function cppFunction() to assign to an R function a function written in C++:\n\nfibRcpp &lt;- cppFunction( '\nint fibonacci(const int x) {\n   if (x == 0) return(0);\n   if (x == 1) return(1);\n   return (fibonacci(x - 1)) + fibonacci(x - 2);\n}\n' )\n\nWe can call our function as any R function:\nfibRcpp(30)\n[1] 832040\nWe can compare both functions:\nlibrary(bench)\n\nn &lt;- 30\nmark(fib(n), fibRcpp(n))\n# A tibble: 2 × 13\n  expression      min   median `itr/sec` mem_alloc `gc/sec` n_itr  n_gc\n  &lt;bch:expr&gt; &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;\n1 fib(n)        1.66s    1.66s     0.601    44.7KB     22.8     1    38\n2 fibRcpp(n)   1.08ms   1.08ms   901.       2.49KB      0     451     0\n  total_time result    memory                 time            \n    &lt;bch:tm&gt; &lt;list&gt;    &lt;list&gt;                 &lt;list&gt;          \n1      1.66s &lt;dbl [1]&gt; &lt;Rprofmem [6,778 × 3]&gt; &lt;bench_tm [1]&gt;  \n2   500.37ms &lt;int [1]&gt; &lt;Rprofmem [1 × 3]&gt;     &lt;bench_tm [451]&gt;\n  gc                \n  &lt;list&gt;            \n1 &lt;tibble [1 × 3]&gt;  \n2 &lt;tibble [451 × 3]&gt;\nWarning message:\nSome expressions had a GC in every iteration; so filtering is disabled.\nThe speedup is 1,537, which is amazing.\nIn this particular example, we saw that memoisation gives an even more incredible speedup (35,000!), but while memoisation will only work in very specific situations (e.g. recursive function calls), using C++ code is a general method to provide speedup. It is particularly useful when:\n\nthere are large numbers of function calls (R is particularly slow with function calls),\nyou need data structures that are missing in R,\nyou want to create efficient packages (fast R packages are written in C++ and many use Rcpp).\n\n\nIn this example, we declared the C++ function directly in R. It is possible to use source files instead.",
    "crumbs": [
      "R",
      "<b><em>High-performance R</em></b>",
      "Writing C++ in R with Rcpp"
    ]
  },
  {
    "objectID": "r/hpc_run.html",
    "href": "r/hpc_run.html",
    "title": "SSH login",
    "section": "",
    "text": "This section will show you how to access our temporary remote cluster through SSH.",
    "crumbs": [
      "R",
      "<b><em>High-performance R</em></b>",
      "SSH login"
    ]
  },
  {
    "objectID": "r/hpc_run.html#why-not-use-an-rstudio-server",
    "href": "r/hpc_run.html#why-not-use-an-rstudio-server",
    "title": "SSH login",
    "section": "Why not use an RStudio server?",
    "text": "Why not use an RStudio server?\nIn our introduction to R, we used an RStudio server running on a remote cluster. In this course, we will log in a similar remote supercomputer using Secure Shell, then run R scripts from the command line.\nWhy are we not making use of the interactivity of R which is an interpreted language and why are we not using the added comfort of an IDE? The short answer is: resource efficiency.\nOnce you have developed your code in an interactive fashion in the IDE of your choice using small hardware resources on a sample of your data, running scripts allows you to only request large resources when you need them (i.e. when your code is running). This prevents heavy resources from sitting idle when not in use, as would happen in an interactive session while you type, think, etc. It will save you money on commercial clusters and waiting time on the Alliance clusters.\nThis course being about high-performance R, let’s learn to use it through scripts.",
    "crumbs": [
      "R",
      "<b><em>High-performance R</em></b>",
      "SSH login"
    ]
  },
  {
    "objectID": "r/hpc_run.html#remote-connection-to-the-cluster-via-ssh",
    "href": "r/hpc_run.html#remote-connection-to-the-cluster-via-ssh",
    "title": "SSH login",
    "section": "Remote connection to the cluster via SSH",
    "text": "Remote connection to the cluster via SSH\nYou do not need to install anything on your machine for this course since we will run everything on this remote cluster.\n\nStep one: get a username and the password\nWe will give you a link to an etherpad during the workshop. Add your name next to a free username to claim it.\nWe will also give you the password for our training cluster.\n\n\nStep two: run the ssh command\n\n •  Linux and macOS users\nLinux users:   open the terminal emulator of your choice.\nmacOS users:   open “Terminal”.\nThen type:\nssh userxx@hostname\n\n\nReplace userxx by your username (e.g. user09).\nReplace hostname by the hostname we will give you the day of the workshop.\n\n\n\n\n •  Windows users\nWe suggest using the free version of MobaXterm. MobaXterm comes with a terminal emulator and a GUI interface for SSH sessions.\nOpen MobaXterm, click on “Session”, then “SSH”, and fill in the Remote host name and your username.\n\nHere is a live demo.\n\n\n\n\nStep three: enter the password\nWhen prompted, enter the password we gave you.\nYou will not see any character as you type the password: this is called blind typing and is a Linux safety feature. It can be unsettling at first not to get any feed-back while typing as it really looks like it is not working. Type slowly and make sure not to make typos.\nYou will be asked whether you are sure that you want to continue connecting. Answer “yes”.\nYou are now logged in and your prompt should look like the following (with your actual username):\n[userxx@login1 ~]$\n\n\nTroubleshooting\nProblems logging in are almost always due to typos. If you cannot log in, retry slowly, entering your password carefully.",
    "crumbs": [
      "R",
      "<b><em>High-performance R</em></b>",
      "SSH login"
    ]
  },
  {
    "objectID": "r/hss_automation.html",
    "href": "r/hss_automation.html",
    "title": "Automation",
    "section": "",
    "text": "One of the strengths of programming is the ability to automate tasks.\nIn this section, we will see how a loop can automate the creation of file names.\n\nLet’s say that we now want to import data from 5 files arc1.csv, …, arc5.csv and create 5 data frames with their data.\nWe need a character vector with the file names.\nWe could create it this way:\n\nfiles &lt;- c(\n  \"https://mint.westdri.ca/r/hss_data/arc1.csv\",\n  \"https://mint.westdri.ca/r/hss_data/arc2.csv\",\n  \"https://mint.westdri.ca/r/hss_data/arc3.csv\",\n  \"https://mint.westdri.ca/r/hss_data/arc4.csv\",\n  \"https://mint.westdri.ca/r/hss_data/arc5.csv\"\n)\n\nIt works of course:\n\nfiles\n\n[1] \"https://mint.westdri.ca/r/hss_data/arc1.csv\"\n[2] \"https://mint.westdri.ca/r/hss_data/arc2.csv\"\n[3] \"https://mint.westdri.ca/r/hss_data/arc3.csv\"\n[4] \"https://mint.westdri.ca/r/hss_data/arc4.csv\"\n[5] \"https://mint.westdri.ca/r/hss_data/arc5.csv\"\n\n\nBut if we had 50 files instead of 5, it would be quite a tedium! And if we had 500 files, it would be unrealistic. A better approach is to write a loop.\nIn order to store the results of a loop, we need to create an empty object and assign to it the result of the loop at each iteration. It is very important to pre-allocate memory: by creating an empty object of the final size, the necessary memory to hold this object is requested once (then the object gets filled in while the loop runs). Without this, more memory would have to be allocated at each iteration of the loop and this is highly inefficient.\nSo let’s create an empty vector of length 5 and of type character:\n\nfiles &lt;- character(5)\n\nNow we can fill in our vector with the proper values with the loop:\n\nfor (i in 1:5) {\n  files[i] &lt;- paste0(\"https://mint.westdri.ca/r/hss_data/arc\", i, \".csv\")\n}\n\nThis gives us the same result, but the big difference is that it is scalable:\n\nfiles\n\n[1] \"https://mint.westdri.ca/r/hss_data/arc1.csv\"\n[2] \"https://mint.westdri.ca/r/hss_data/arc2.csv\"\n[3] \"https://mint.westdri.ca/r/hss_data/arc3.csv\"\n[4] \"https://mint.westdri.ca/r/hss_data/arc4.csv\"\n[5] \"https://mint.westdri.ca/r/hss_data/arc5.csv\"\n\n\nNow, if our files were not named following such a nice sequence, we would have to modify our loop a little. Below are two examples:\n\nfiles &lt;- character(5)\n\nfor (i in seq_along(c(3, 6, 9, 10, 14))) {\n  files[i] &lt;- paste0(\n    \"https://mint.westdri.ca/r/hss_data/arc\",\n    c(3, 6, 9, 10, 14)[i],\n    \".csv\"\n  )\n}\n\nfiles\n\n[1] \"https://mint.westdri.ca/r/hss_data/arc3.csv\" \n[2] \"https://mint.westdri.ca/r/hss_data/arc6.csv\" \n[3] \"https://mint.westdri.ca/r/hss_data/arc9.csv\" \n[4] \"https://mint.westdri.ca/r/hss_data/arc10.csv\"\n[5] \"https://mint.westdri.ca/r/hss_data/arc14.csv\"\n\n\n\nfiles &lt;- character(5)\n\nfor (i in seq_along(c(\"_a\", \"_b\", \"_c\", \"_d\", \"_e\"))) {\n  files[i] &lt;- paste0(\n    \"https://mint.westdri.ca/r/hss_data/arc\",\n    c(\"_a\", \"_b\", \"_c\", \"_d\", \"_e\")[i],\n    \".csv\"\n  )\n}\n\nfiles\n\n[1] \"https://mint.westdri.ca/r/hss_data/arc_a.csv\"\n[2] \"https://mint.westdri.ca/r/hss_data/arc_b.csv\"\n[3] \"https://mint.westdri.ca/r/hss_data/arc_c.csv\"\n[4] \"https://mint.westdri.ca/r/hss_data/arc_d.csv\"\n[5] \"https://mint.westdri.ca/r/hss_data/arc_e.csv\"\n\n\n\nIf you had all the files in one directory, an alternative approach would be to create a list of all the names matching a regular expression.\nIn our case, we would use:\nfiles &lt;- list.files(pattern=\"^arc\\\\d+\\\\.csv$\")",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Automation"
    ]
  },
  {
    "objectID": "r/hss_explore.html",
    "href": "r/hss_explore.html",
    "title": "Data exploration",
    "section": "",
    "text": "An important first step of data analysis is to have a look at the data. In this section, we will explore the us_contagious_diseases dataset from the dslabs package.",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Data exploration"
    ]
  },
  {
    "objectID": "r/hss_explore.html#load-the-dslabs-package",
    "href": "r/hss_explore.html#load-the-dslabs-package",
    "title": "Data exploration",
    "section": "Load the dslabs package",
    "text": "Load the dslabs package\nThis package contains a number of datasets. To access any of them, we first need to load the package:\n\nlibrary(dslabs)\n\n\nlibrary() is a function:\n\nclass(library)\n\n[1] \"function\"\n\n\nFunctions are the “verbs” of programming languages. They do things.\nlibrary() is a function that loads packages into the current session so that their content becomes available.\ndslabs is the argument that we pass to the function library(): it is this particular packages that we are loading in the session here.\nclass() is also a function: it tells what class an object belongs to. In class(library), library is the argument of the function class().",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Data exploration"
    ]
  },
  {
    "objectID": "r/hss_explore.html#printing-data-to-screen",
    "href": "r/hss_explore.html#printing-data-to-screen",
    "title": "Data exploration",
    "section": "Printing data to screen",
    "text": "Printing data to screen\nTo print all the data, we would simply run us_contagious_diseases. There are a lot of rows however, so we only want to print a subset to the screen.\nTo print the first six rows, we use the function head(), using our data as the argument:\n\nhead(us_contagious_diseases)\n\n      disease   state year weeks_reporting count population\n1 Hepatitis A Alabama 1966              50   321    3345787\n2 Hepatitis A Alabama 1967              49   291    3364130\n3 Hepatitis A Alabama 1968              52   314    3386068\n4 Hepatitis A Alabama 1969              49   380    3412450\n5 Hepatitis A Alabama 1970              51   413    3444165\n6 Hepatitis A Alabama 1971              51   378    3481798\n\n\nIf you look at the documentation of the head() function (by running ?head), you can see that it accepts another argument that allows us to set the number of rows to print.\nLet’s print the first 15 rows:\n\nhead(us_contagious_diseases, n = 15)\n\n       disease   state year weeks_reporting count population\n1  Hepatitis A Alabama 1966              50   321    3345787\n2  Hepatitis A Alabama 1967              49   291    3364130\n3  Hepatitis A Alabama 1968              52   314    3386068\n4  Hepatitis A Alabama 1969              49   380    3412450\n5  Hepatitis A Alabama 1970              51   413    3444165\n6  Hepatitis A Alabama 1971              51   378    3481798\n7  Hepatitis A Alabama 1972              45   342    3524543\n8  Hepatitis A Alabama 1973              45   467    3571209\n9  Hepatitis A Alabama 1974              45   244    3620548\n10 Hepatitis A Alabama 1975              46   286    3671246\n11 Hepatitis A Alabama 1976              50   220    3721914\n12 Hepatitis A Alabama 1977              43   206    3771085\n13 Hepatitis A Alabama 1978              41   203    3817217\n14 Hepatitis A Alabama 1979              47   257    3858703\n15 Hepatitis A Alabama 1980              37   200    3893888\n\n\n\nBy default, n = 6 which is why head() prints six rows unless we specify otherwise. The L in the documentation of the print() function (n = 6L) means that 6 is an integer. You can ignore this for now.\nArguments can be passed to functions as positional arguments (then they have to respect the position of the function definition) or as named arguments (in that case, you need to use the arguments names).\nThat means that iff we keep the arguments in the right order, we can omit the name of the argument (n here) and only write its value (15). :\n\nhead(us_contagious_diseases, 15)\n\n       disease   state year weeks_reporting count population\n1  Hepatitis A Alabama 1966              50   321    3345787\n2  Hepatitis A Alabama 1967              49   291    3364130\n3  Hepatitis A Alabama 1968              52   314    3386068\n4  Hepatitis A Alabama 1969              49   380    3412450\n5  Hepatitis A Alabama 1970              51   413    3444165\n6  Hepatitis A Alabama 1971              51   378    3481798\n7  Hepatitis A Alabama 1972              45   342    3524543\n8  Hepatitis A Alabama 1973              45   467    3571209\n9  Hepatitis A Alabama 1974              45   244    3620548\n10 Hepatitis A Alabama 1975              46   286    3671246\n11 Hepatitis A Alabama 1976              50   220    3721914\n12 Hepatitis A Alabama 1977              43   206    3771085\n13 Hepatitis A Alabama 1978              41   203    3817217\n14 Hepatitis A Alabama 1979              47   257    3858703\n15 Hepatitis A Alabama 1980              37   200    3893888\n\n\nIf the arguments are given to the function out of order however, we do need to use their names.\nThis won’t work because R needs an integer for n or for the 2nd argument:\n\nhead(15, us_contagious_diseases)\n\nError in head.default(15, us_contagious_diseases): invalid 'n' - must be numeric, possibly NA.\n\n\nThis however works:\n\nhead(n = 15, us_contagious_diseases)\n\n       disease   state year weeks_reporting count population\n1  Hepatitis A Alabama 1966              50   321    3345787\n2  Hepatitis A Alabama 1967              49   291    3364130\n3  Hepatitis A Alabama 1968              52   314    3386068\n4  Hepatitis A Alabama 1969              49   380    3412450\n5  Hepatitis A Alabama 1970              51   413    3444165\n6  Hepatitis A Alabama 1971              51   378    3481798\n7  Hepatitis A Alabama 1972              45   342    3524543\n8  Hepatitis A Alabama 1973              45   467    3571209\n9  Hepatitis A Alabama 1974              45   244    3620548\n10 Hepatitis A Alabama 1975              46   286    3671246\n11 Hepatitis A Alabama 1976              50   220    3721914\n12 Hepatitis A Alabama 1977              43   206    3771085\n13 Hepatitis A Alabama 1978              41   203    3817217\n14 Hepatitis A Alabama 1979              47   257    3858703\n15 Hepatitis A Alabama 1980              37   200    3893888\n\n\n\nWe can also print the last 6 rows of the data:\n\ntail(us_contagious_diseases)\n\n       disease   state year weeks_reporting count population\n16060 Smallpox Wyoming 1947              49     1     276297\n16061 Smallpox Wyoming 1948              24     1     280803\n16062 Smallpox Wyoming 1949               0     0     285544\n16063 Smallpox Wyoming 1950               1     2     290529\n16064 Smallpox Wyoming 1951               1     1     295744\n16065 Smallpox Wyoming 1952               1     1     301083\n\n\n\n\nYour turn:\n\nHow would you print the last 10 rows of the data?",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Data exploration"
    ]
  },
  {
    "objectID": "r/hss_explore.html#structure-of-the-data-object",
    "href": "r/hss_explore.html#structure-of-the-data-object",
    "title": "Data exploration",
    "section": "Structure of the data object",
    "text": "Structure of the data object\nus_contagious_diseases is an R object containing the dataset, but what kind of object is it?\n\nclass(us_contagious_diseases)\n\n[1] \"data.frame\"\n\n\nOur data is in a class of R object called a data frame.\nWe can get its full structure with:\n\nstr(us_contagious_diseases)\n\n'data.frame':   16065 obs. of  6 variables:\n $ disease        : Factor w/ 7 levels \"Hepatitis A\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ state          : Factor w/ 51 levels \"Alabama\",\"Alaska\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ year           : num  1966 1967 1968 1969 1970 ...\n $ weeks_reporting: num  50 49 52 49 51 51 45 45 45 46 ...\n $ count          : num  321 291 314 380 413 378 342 467 244 286 ...\n $ population     : num  3345787 3364130 3386068 3412450 3444165 ...\n\n\nThe names of the variables can be obtained with:\n\nnames(us_contagious_diseases)\n\n[1] \"disease\"         \"state\"           \"year\"            \"weeks_reporting\"\n[5] \"count\"           \"population\"     \n\n\nYou can display the data frame in a tabular fashion thanks to:\nView(us_contagious_diseases)",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Data exploration"
    ]
  },
  {
    "objectID": "r/hss_explore.html#dimensions-of-our-data-frame",
    "href": "r/hss_explore.html#dimensions-of-our-data-frame",
    "title": "Data exploration",
    "section": "Dimensions of our data frame",
    "text": "Dimensions of our data frame\n\ndim(us_contagious_diseases)\n\n[1] 16065     6\n\nncol(us_contagious_diseases)\n\n[1] 6\n\nnrow(us_contagious_diseases)\n\n[1] 16065\n\n\n\nlength(us_contagious_diseases)\n\n[1] 6\n\nlength(us_contagious_diseases$disease)\n\n[1] 16065",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Data exploration"
    ]
  },
  {
    "objectID": "r/hss_explore.html#summary-statistics",
    "href": "r/hss_explore.html#summary-statistics",
    "title": "Data exploration",
    "section": "Summary statistics",
    "text": "Summary statistics\n\nsummary(us_contagious_diseases)\n\n        disease            state            year      weeks_reporting\n Hepatitis A:2346   Alabama   :  315   Min.   :1928   Min.   : 0.00  \n Measles    :3825   Alaska    :  315   1st Qu.:1950   1st Qu.:31.00  \n Mumps      :1785   Arizona   :  315   Median :1975   Median :46.00  \n Pertussis  :2856   Arkansas  :  315   Mean   :1971   Mean   :37.38  \n Polio      :2091   California:  315   3rd Qu.:1990   3rd Qu.:50.00  \n Rubella    :1887   Colorado  :  315   Max.   :2011   Max.   :52.00  \n Smallpox   :1275   (Other)   :14175                                 \n     count          population      \n Min.   :     0   Min.   :   86853  \n 1st Qu.:     7   1st Qu.: 1018755  \n Median :    69   Median : 2749249  \n Mean   :  1492   Mean   : 4107584  \n 3rd Qu.:   525   3rd Qu.: 4996229  \n Max.   :132342   Max.   :37607525  \n                  NA's   :214",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Data exploration"
    ]
  },
  {
    "objectID": "r/hss_help.html",
    "href": "r/hss_help.html",
    "title": "Help and documentation",
    "section": "",
    "text": "One of the strengths of R is its great documentation. Here, we will learn how to make use of it.",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Help and documentation"
    ]
  },
  {
    "objectID": "r/hss_help.html#general-documentation",
    "href": "r/hss_help.html#general-documentation",
    "title": "Help and documentation",
    "section": "General documentation",
    "text": "General documentation\nTo get started with R, you can launch the general documentation with:\nhelp.start()",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Help and documentation"
    ]
  },
  {
    "objectID": "r/hss_help.html#help-on-functions",
    "href": "r/hss_help.html#help-on-functions",
    "title": "Help and documentation",
    "section": "Help on functions",
    "text": "Help on functions\nTo get help on specific objects (e.g. the function sum), you can run:\nhelp(sum)\nor:\n?sum\n\nThe documentation pages always follow the same format:\n\nName of the object and the package it comes from\nA short description of the object\nThe code to use it\nAn explanation of the arguments (in the case of functions)\nExplanation with greater details\nThe value returned (in the case of functions)\nExamples of code snippets that can be run",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Help and documentation"
    ]
  },
  {
    "objectID": "r/hss_manipulate.html",
    "href": "r/hss_manipulate.html",
    "title": "Data extraction",
    "section": "",
    "text": "It is often useful to focus on sections of the data to plot or analyse. In this section, we will see how to extract various elements of the us_contagious_diseases dataset from the dslabs package.",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Data extraction"
    ]
  },
  {
    "objectID": "r/hss_manipulate.html#load-packages",
    "href": "r/hss_manipulate.html#load-packages",
    "title": "Data extraction",
    "section": "Load packages",
    "text": "Load packages\nOne of the tidyverse packages is very useful for data manipulation: dplyr. Let’s load the dslabs package again as well as dplyr:\n\nlibrary(dslabs)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Data extraction"
    ]
  },
  {
    "objectID": "r/hss_manipulate.html#indexing",
    "href": "r/hss_manipulate.html#indexing",
    "title": "Data extraction",
    "section": "Indexing",
    "text": "Indexing\nYou can extract a subset of the data using their position by indexing. Indexing in R starts with 1 (in many languages, the first index is 0) and it is done with square brackets. Since a data frame has two dimensions, there are two possible indices in the square brackets:\n\nthe row index,\nthe column index.\n\nYou can index a single element:\n\nus_contagious_diseases[1, 1]\n\n[1] Hepatitis A\nLevels: Hepatitis A Measles Mumps Pertussis Polio Rubella Smallpox\n\nus_contagious_diseases[1, 2]\n\n[1] Alabama\n51 Levels: Alabama Alaska Arizona Arkansas California Colorado ... Wyoming\n\n\nOr a full row:\n\nus_contagious_diseases[1, ]\n\n      disease   state year weeks_reporting count population\n1 Hepatitis A Alabama 1966              50   321    3345787\n\nus_contagious_diseases[3000, ]\n\n     disease                state year weeks_reporting count population\n3000 Measles District Of Columbia 1981              27     2     631010\n\n\n\n\nYour turn:\n\nHow would you index the year column?",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Data extraction"
    ]
  },
  {
    "objectID": "r/hss_manipulate.html#filtering-rows",
    "href": "r/hss_manipulate.html#filtering-rows",
    "title": "Data extraction",
    "section": "Filtering rows",
    "text": "Filtering rows\nYou can also filter data points based on their values:\n\nus_contagious_diseases |&gt;\n  filter(state == \"California\") |&gt;\n  count()\n\n    n\n1 315\n\n\n\n\nYour turn:\n\nHow many data points are there for the state of Arizona?\n\n\nus_contagious_diseases |&gt;\n  filter(state == \"California\" & year &gt; 2000)\n\n       disease      state year weeks_reporting count population\n1  Hepatitis A California 2001              40  1599   34199784\n2  Hepatitis A California 2002              49  1364   34529758\n3  Hepatitis A California 2003              46  1045   34861711\n4  Hepatitis A California 2004              48   788   35195792\n5  Hepatitis A California 2005              49   905   35532154\n6  Hepatitis A California 2006              52   688   35870957\n7  Hepatitis A California 2007              51   312   36212364\n8  Hepatitis A California 2008              52   337   36556548\n9  Hepatitis A California 2009              52   239   36903684\n10 Hepatitis A California 2010              49   201   37253956\n11 Hepatitis A California 2011              49   176   37607525\n12     Measles California 2001              40    34   34199784\n13     Measles California 2002              33     0   34529758\n14       Mumps California 2001              49    37   34199784\n15       Mumps California 2002              49    66   34529758\n16   Pertussis California 2001              40   440   34199784\n17   Pertussis California 2002              43   698   34529758\n18   Pertussis California 2003              41   635   34861711\n19   Pertussis California 2004              36   498   35195792\n20   Pertussis California 2005              45  1609   35532154\n21   Pertussis California 2006              42   831   35870957\n22   Pertussis California 2007              29    95   36212364\n23   Pertussis California 2008              39   276   36556548\n24   Pertussis California 2009              40   415   36903684\n25   Pertussis California 2010              48  1265   37253956\n26   Pertussis California 2011              49  1145   37607525\n27     Rubella California 2001               1     0   34199784\n28     Rubella California 2002              29     2   34529758\n\n\n\nus_contagious_diseases |&gt;\n  filter(state == \"California\" & year &gt; 2000) |&gt;\n  arrange(year)\n\n       disease      state year weeks_reporting count population\n1  Hepatitis A California 2001              40  1599   34199784\n2      Measles California 2001              40    34   34199784\n3        Mumps California 2001              49    37   34199784\n4    Pertussis California 2001              40   440   34199784\n5      Rubella California 2001               1     0   34199784\n6  Hepatitis A California 2002              49  1364   34529758\n7      Measles California 2002              33     0   34529758\n8        Mumps California 2002              49    66   34529758\n9    Pertussis California 2002              43   698   34529758\n10     Rubella California 2002              29     2   34529758\n11 Hepatitis A California 2003              46  1045   34861711\n12   Pertussis California 2003              41   635   34861711\n13 Hepatitis A California 2004              48   788   35195792\n14   Pertussis California 2004              36   498   35195792\n15 Hepatitis A California 2005              49   905   35532154\n16   Pertussis California 2005              45  1609   35532154\n17 Hepatitis A California 2006              52   688   35870957\n18   Pertussis California 2006              42   831   35870957\n19 Hepatitis A California 2007              51   312   36212364\n20   Pertussis California 2007              29    95   36212364\n21 Hepatitis A California 2008              52   337   36556548\n22   Pertussis California 2008              39   276   36556548\n23 Hepatitis A California 2009              52   239   36903684\n24   Pertussis California 2009              40   415   36903684\n25 Hepatitis A California 2010              49   201   37253956\n26   Pertussis California 2010              48  1265   37253956\n27 Hepatitis A California 2011              49   176   37607525\n28   Pertussis California 2011              49  1145   37607525\n\n\n\nus_contagious_diseases |&gt;\n  filter(state == \"California\" & year &gt; 2000) |&gt;\n  arrange(count)\n\n       disease      state year weeks_reporting count population\n1      Measles California 2002              33     0   34529758\n2      Rubella California 2001               1     0   34199784\n3      Rubella California 2002              29     2   34529758\n4      Measles California 2001              40    34   34199784\n5        Mumps California 2001              49    37   34199784\n6        Mumps California 2002              49    66   34529758\n7    Pertussis California 2007              29    95   36212364\n8  Hepatitis A California 2011              49   176   37607525\n9  Hepatitis A California 2010              49   201   37253956\n10 Hepatitis A California 2009              52   239   36903684\n11   Pertussis California 2008              39   276   36556548\n12 Hepatitis A California 2007              51   312   36212364\n13 Hepatitis A California 2008              52   337   36556548\n14   Pertussis California 2009              40   415   36903684\n15   Pertussis California 2001              40   440   34199784\n16   Pertussis California 2004              36   498   35195792\n17   Pertussis California 2003              41   635   34861711\n18 Hepatitis A California 2006              52   688   35870957\n19   Pertussis California 2002              43   698   34529758\n20 Hepatitis A California 2004              48   788   35195792\n21   Pertussis California 2006              42   831   35870957\n22 Hepatitis A California 2005              49   905   35532154\n23 Hepatitis A California 2003              46  1045   34861711\n24   Pertussis California 2011              49  1145   37607525\n25   Pertussis California 2010              48  1265   37253956\n26 Hepatitis A California 2002              49  1364   34529758\n27 Hepatitis A California 2001              40  1599   34199784\n28   Pertussis California 2005              45  1609   35532154\n\n\n\nus_contagious_diseases |&gt;\n  filter(state == \"California\" & year &gt; 2000) |&gt;\n  arrange(desc(count))\n\n       disease      state year weeks_reporting count population\n1    Pertussis California 2005              45  1609   35532154\n2  Hepatitis A California 2001              40  1599   34199784\n3  Hepatitis A California 2002              49  1364   34529758\n4    Pertussis California 2010              48  1265   37253956\n5    Pertussis California 2011              49  1145   37607525\n6  Hepatitis A California 2003              46  1045   34861711\n7  Hepatitis A California 2005              49   905   35532154\n8    Pertussis California 2006              42   831   35870957\n9  Hepatitis A California 2004              48   788   35195792\n10   Pertussis California 2002              43   698   34529758\n11 Hepatitis A California 2006              52   688   35870957\n12   Pertussis California 2003              41   635   34861711\n13   Pertussis California 2004              36   498   35195792\n14   Pertussis California 2001              40   440   34199784\n15   Pertussis California 2009              40   415   36903684\n16 Hepatitis A California 2008              52   337   36556548\n17 Hepatitis A California 2007              51   312   36212364\n18   Pertussis California 2008              39   276   36556548\n19 Hepatitis A California 2009              52   239   36903684\n20 Hepatitis A California 2010              49   201   37253956\n21 Hepatitis A California 2011              49   176   37607525\n22   Pertussis California 2007              29    95   36212364\n23       Mumps California 2002              49    66   34529758\n24       Mumps California 2001              49    37   34199784\n25     Measles California 2001              40    34   34199784\n26     Rubella California 2002              29     2   34529758\n27     Measles California 2002              33     0   34529758\n28     Rubella California 2001               1     0   34199784",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Data extraction"
    ]
  },
  {
    "objectID": "r/hss_manipulate.html#selecting-columns",
    "href": "r/hss_manipulate.html#selecting-columns",
    "title": "Data extraction",
    "section": "Selecting columns",
    "text": "Selecting columns\nWe saw how to index columns from their position. It is also possible to select them based on their names:\n\nhead(us_contagious_diseases$year, 50)\n\n [1] 1966 1967 1968 1969 1970 1971 1972 1973 1974 1975 1976 1977 1978 1979 1980\n[16] 1981 1982 1983 1984 1985 1986 1987 1988 1989 1990 1991 1992 1993 1994 1995\n[31] 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010\n[46] 2011 1966 1967 1968 1969\n\n\nIf you want to select several columns, you can use the select() function from dplyr:\n\nus_contagious_diseases |&gt;\n  filter(state == \"California\" & year &gt; 2000 & disease == \"Hepatitis A\") |&gt;\n  select(year, count, population)\n\n   year count population\n1  2001  1599   34199784\n2  2002  1364   34529758\n3  2003  1045   34861711\n4  2004   788   35195792\n5  2005   905   35532154\n6  2006   688   35870957\n7  2007   312   36212364\n8  2008   337   36556548\n9  2009   239   36903684\n10 2010   201   37253956\n11 2011   176   37607525",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Data extraction"
    ]
  },
  {
    "objectID": "r/hss_manipulate.html#grouping-data",
    "href": "r/hss_manipulate.html#grouping-data",
    "title": "Data extraction",
    "section": "Grouping data",
    "text": "Grouping data\nIt is often useful to group data by categories to compute some summary statistics.\nFor instance, we can group by year and calculate the total numbers of infections:\n\nus_contagious_diseases |&gt;\n  group_by(year) |&gt;\n  summarise(total = sum(count))\n\n# A tibble: 84 × 2\n    year  total\n   &lt;dbl&gt;  &lt;dbl&gt;\n 1  1928 524563\n 2  1929 380196\n 3  1930 439289\n 4  1931 482886\n 5  1932 404683\n 6  1933 391485\n 7  1934 739509\n 8  1935 739224\n 9  1936 292530\n10  1937 314425\n# ℹ 74 more rows\n\n\nAlternatively, we can group by state and get the totals:\n\nus_contagious_diseases |&gt;\n  group_by(state) |&gt; \n  summarise(total = sum(count))\n\n# A tibble: 51 × 2\n   state                  total\n   &lt;fct&gt;                  &lt;dbl&gt;\n 1 Alabama               257979\n 2 Alaska                 29136\n 3 Arizona               240233\n 4 Arkansas              177556\n 5 California           1906067\n 6 Colorado              322845\n 7 Connecticut           463148\n 8 Delaware               44427\n 9 District Of Columbia   77012\n10 Florida               268383\n# ℹ 41 more rows\n\n\nWe can also group by year and state and get the totals:\n\nus_contagious_diseases |&gt;\n  group_by(year, state) |&gt; \n  summarise(total = sum(count))\n\n`summarise()` has grouped output by 'year'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 4,284 × 3\n# Groups:   year [84]\n    year state                total\n   &lt;dbl&gt; &lt;fct&gt;                &lt;dbl&gt;\n 1  1928 Alabama               9246\n 2  1928 Alaska                   0\n 3  1928 Arizona               1268\n 4  1928 Arkansas              9157\n 5  1928 California            4960\n 6  1928 Colorado              2510\n 7  1928 Connecticut          10247\n 8  1928 Delaware               607\n 9  1928 District Of Columbia  2609\n10  1928 Florida               1892\n# ℹ 4,274 more rows",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Data extraction"
    ]
  },
  {
    "objectID": "r/hss_publish.html",
    "href": "r/hss_publish.html",
    "title": "Publishing",
    "section": "",
    "text": "You might have heard of R Markdown: a way to intertwine code and prose in a single scientific document. The company behind R Markdown has now developed its successor: Quarto.\n\nQuarto allows the creation of webpages, websites, presentations, books, pdf, etc. from code in R, Python, or Julia and markdown text.\nIf you are interested in an introduction to this tool, you can have a look at our workshop or our webinar on Quarto.\nBy the way, this entire website was created with Quarto 🙂",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Publishing"
    ]
  },
  {
    "objectID": "r/hss_run.html",
    "href": "r/hss_run.html",
    "title": "Running R",
    "section": "",
    "text": "This section covers the various ways R can be run, then shows you how to access our temporary RStudio server for this course.",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Running R"
    ]
  },
  {
    "objectID": "r/hss_run.html#running-r",
    "href": "r/hss_run.html#running-r",
    "title": "Running R",
    "section": "Running R",
    "text": "Running R\nR being an interpreted language, it can be run non-interactively or interactively.\n\nRunning R non-interactively\nIf you write code in a text file (called a script), you can then execute it with:\nRscript my_script.R\n\nThe command to execute scripts is Rscript rather than R.\nBy convention, R scripts take the extension .R.\n\n\n\nRunning R interactively\nThere are several ways to run R interactively.\n\nDirectly in the console (the name for the R shell):\n\n\n\nIn Jupyter with the R kernel (IRkernel package).\nIn another IDE (e.g. in Emacs with ESS).\nIn the RStudio IDE.\n\nThe RStudio IDE is popular and this is what we will use today. RStudio can can be run locally, but for this course, we will use an RStudio server.",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Running R"
    ]
  },
  {
    "objectID": "r/hss_run.html#accessing-our-rstudio-server",
    "href": "r/hss_run.html#accessing-our-rstudio-server",
    "title": "Running R",
    "section": "Accessing our RStudio server",
    "text": "Accessing our RStudio server\nYou do not need to install anything on your machine for this course as we will provide access to a temporary RStudio server.\n\nA username, a password, and the URL of the RStudio server will be given to you during the workshop.\n\nSign in using the username and password you will be given while ignoring the OTP entry. This will take you to the server options page of a JupyterHub.\n\nSelect the following server options:\n\nTime: 2 hours\nNumber of cores: 1\nMemory: 3700 MB\nUser interface: JupyterLab\n\n\nThen press “Start” to launch the JupyterHub. There, click on the “RStudio” button and the RStudio server will open in a new tab.\n\nNote that this temporary cluster will only be available for the duration of this course.",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Running R"
    ]
  },
  {
    "objectID": "r/hss_run.html#using-rstudio",
    "href": "r/hss_run.html#using-rstudio",
    "title": "Running R",
    "section": "Using RStudio",
    "text": "Using RStudio\nFor those unfamiliar with the RStudio IDE, you can download the following cheatsheet:\n\n\n\nfrom Posit Cheatsheets",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Running R"
    ]
  },
  {
    "objectID": "r/hss_vis.html",
    "href": "r/hss_vis.html",
    "title": "Data visualization",
    "section": "",
    "text": "To understand data, it is often extremely useful to visualize them. In this section, we will plot the US infectious disease data.",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Data visualization"
    ]
  },
  {
    "objectID": "r/hss_vis.html#load-packages",
    "href": "r/hss_vis.html#load-packages",
    "title": "Data visualization",
    "section": "Load packages",
    "text": "Load packages\nThe most popular R package for data visualization is the tidyverse package ggplot2. Let’s load it in addition to our previous packages:\n\nlibrary(dslabs)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\n\nLet’s start by plotting the total number of cases for all states and all diseases per year. We already saw in the previous section how to group and summarise the data. Let’s create a new data frame with our prepared data:\n\nus_year_totals &lt;- us_contagious_diseases |&gt;\n  group_by(year) |&gt;\n  summarise(total = sum(count))\n\nThis is what our data frame looks like:\n\nhead(us_year_totals)\n\n# A tibble: 6 × 2\n   year  total\n  &lt;dbl&gt;  &lt;dbl&gt;\n1  1928 524563\n2  1929 380196\n3  1930 439289\n4  1931 482886\n5  1932 404683\n6  1933 391485\n\n\nNow we can use it to make a first plot.",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Data visualization"
    ]
  },
  {
    "objectID": "r/hss_vis.html#the-canvas",
    "href": "r/hss_vis.html#the-canvas",
    "title": "Data visualization",
    "section": "The Canvas",
    "text": "The Canvas\nThe first component of a plot is the data:\n\nggplot(us_year_totals)\n\n\n\n\n\n\n\n\nThe second component sets the way variables are mapped on the axes. This is done with the aes() (aesthetics) function:\n\nggplot(us_year_totals, aes(year, total))",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Data visualization"
    ]
  },
  {
    "objectID": "r/hss_vis.html#geometric-representations-of-the-data",
    "href": "r/hss_vis.html#geometric-representations-of-the-data",
    "title": "Data visualization",
    "section": "Geometric representations of the data",
    "text": "Geometric representations of the data\nOnto this canvas, we can add “geoms” (geometrical objects) representing the data. The type of “geom” defines the type of representation (e.g. boxplot, histogram, bar chart).\nTo represent the data as a scatterplot, we use the geom_point() function:\n\nggplot(us_year_totals, aes(year, total)) +\n  geom_point()\n\n\n\n\n\n\n\n\nThis immediately shows that the number of contagious infections in the US has declined sharply since the early 60s.\nMultiple “geoms” can be added on top of each other. For instance, we can add a smoothed conditional means function with geom_smooth(). That will help us see patterns in the data:\n\nggplot(us_year_totals, aes(year, total)) +\n  geom_point() +\n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThere is a bump of cases in the early 40s. Due to WWII maybe?\nThe default smoothing function uses the LOESS (locally estimated scatterplot smoothing) method, which is a nonlinear regression. We can change the method to a linear model:\n\nggplot(us_year_totals, aes(year, total)) +\n  geom_point() +\n  geom_smooth(method = lm)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nLine width, color, and whether or not the standard error (se) is shown can be customized:\n\nggplot(us_year_totals, aes(year, total)) +\n  geom_point() +\n  geom_smooth(\n    method = lm,\n    se = FALSE,\n    color = \"#999999\",\n    linewidth = 0.5\n  )\n\n`geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Data visualization"
    ]
  },
  {
    "objectID": "r/hss_vis.html#colour-representations",
    "href": "r/hss_vis.html#colour-representations",
    "title": "Data visualization",
    "section": "Colour representations",
    "text": "Colour representations\nSo far, we have pooled the data for all diseases together, but maybe different diseases show different trends.\nLet’s create a new data frame with the totals per year and per disease so that we can create plots with more information:\n\nus_year_disease_totals &lt;- us_contagious_diseases |&gt;\n  group_by(year, disease) |&gt;\n  summarise(total = sum(count), .groups = 'drop')\n\nhead(us_year_disease_totals)\n\n# A tibble: 6 × 3\n   year disease   total\n  &lt;dbl&gt; &lt;fct&gt;     &lt;dbl&gt;\n1  1928 Measles  483337\n2  1928 Polio      4756\n3  1928 Smallpox  36470\n4  1929 Measles  339061\n5  1929 Polio      2746\n6  1929 Smallpox  38389\n\n\nNow we can use a different colour for each disease:\n\nggplot(us_year_disease_totals, aes(year, total)) +\n  geom_point(aes(color = disease))\n\n\n\n\n\n\n\n\nThis shows how prevalent measles was until the 70s.\nWhen plotting quickly to understand the data, aesthetics don’t matter. If you want to produce plots for publications or presentations, of course you should then spend some time tweaking their style and readability.",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Data visualization"
    ]
  },
  {
    "objectID": "r/hss_vis.html#colour-scales",
    "href": "r/hss_vis.html#colour-scales",
    "title": "Data visualization",
    "section": "Colour scales",
    "text": "Colour scales\nMany colour scales exist. scale_color_brewer(), based on color brewer 2.0, is one of many methods to change the color scale. Here is the list of available scales for this particular method:\n\n\n\n\n\nWhen choosing a colour scale, it is very important to remember that various forms of colour blindness are common. Try to choose distinctive colours. Some palettes are specifically designed to work well for everyone.\nHere, let’s try the Dark2 palette:\n\nggplot(us_year_disease_totals, aes(year, total)) +\n  geom_point(aes(color = disease)) +\n  scale_color_brewer(palette = \"Dark2\")",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Data visualization"
    ]
  },
  {
    "objectID": "r/hss_vis.html#labels",
    "href": "r/hss_vis.html#labels",
    "title": "Data visualization",
    "section": "Labels",
    "text": "Labels\nLet’s improve our axes labels and legend and add a title to the plot:\n\nggplot(us_year_disease_totals, aes(year, total)) +\n  geom_point(aes(color = disease)) +\n  scale_color_brewer(palette = \"Dark2\") +\n  labs(\n    title = \"Infectious diseases in the US\",\n    x = \"Year\",\n    y = \"Number of cases\",\n    color = \"Diseases\"\n  )",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Data visualization"
    ]
  },
  {
    "objectID": "r/hss_vis.html#themes",
    "href": "r/hss_vis.html#themes",
    "title": "Data visualization",
    "section": "Themes",
    "text": "Themes\nggplot2 comes with a number of preset themes.\nEdward Tufte developed, amongst others, the principle of data-ink ratio which emphasizes that ink should be used primarily where it communicates meaningful messages. It is indeed common to see charts where more ink is used in labels or background than in the actual representation of the data.\nThe default ggplot2 theme could be criticized as not following this principle. Let’s change it:\n\nggplot(us_year_disease_totals, aes(year, total)) +\n  geom_point(aes(color = disease)) +\n  scale_color_brewer(palette = \"Dark2\") +\n  labs(\n    title = \"Infectious diseases in the US\",\n    x = \"Year\",\n    y = \"Number of cases\",\n    color = \"Diseases\"\n  ) +\n  theme_classic()\n\n\n\n\n\n\n\n\nThe theme() function allows to tweak the theme in any number of ways. For instance, what if we don’t like the default position of the title and we would rather have it centered?\n\nggplot(us_year_disease_totals, aes(year, total)) +\n  geom_point(aes(color = disease)) +\n  scale_color_brewer(palette = \"Dark2\") +\n  labs(\n    title = \"Infectious diseases in the US\",\n    x = \"Year\",\n    y = \"Number of cases\",\n    color = \"Diseases\"\n  ) +\n  theme_classic() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\nWe can also move the legend to give more space to the actual graph:\n\nggplot(us_year_disease_totals, aes(year, total)) +\n  geom_point(aes(color = disease)) +\n  scale_color_brewer(palette = \"Dark2\") +\n  labs(\n    title = \"Infectious diseases in the US\",\n    x = \"Year\",\n    y = \"Number of cases\",\n    color = \"Diseases\"\n  ) +\n  theme_classic() +\n  theme(plot.title = element_text(hjust = 0.5), legend.position = \"bottom\")",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Data visualization"
    ]
  },
  {
    "objectID": "r/hss_vis.html#facets",
    "href": "r/hss_vis.html#facets",
    "title": "Data visualization",
    "section": "Facets",
    "text": "Facets\nInstead of plotting the data for all diseases on a single graph, we can create facets:\n\nggplot(us_year_disease_totals, aes(year, total)) +\n  geom_point(aes(color = disease), show.legend = FALSE) +\n  scale_color_brewer(palette = \"Dark2\") +\n  labs(\n    title = \"Infectious diseases in the US\",\n    x = \"Year\",\n    y = \"Number of cases\"\n  ) +\n  facet_wrap(~ disease) +\n  theme(plot.title = element_text(hjust = 0.5))",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Data visualization"
    ]
  },
  {
    "objectID": "r/hss_vis.html#saving-plots",
    "href": "r/hss_vis.html#saving-plots",
    "title": "Data visualization",
    "section": "Saving plots",
    "text": "Saving plots\nPlots can be saved to file thanks to the ggsave() function from ggplot2.\nLet’s save our last plot:\nggsave(\"us_infectious_diseases.png\")\n\nBy default, ggsave() saves the last plot and guesses the file type from the file name extension. Arguments exist to select another plot to save to file, set the height and width, the resolution, add a background, etc. See ?ggsave for a list of options.",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Data visualization"
    ]
  },
  {
    "objectID": "r/hss_vis.html#ggplot2-extensions",
    "href": "r/hss_vis.html#ggplot2-extensions",
    "title": "Data visualization",
    "section": "ggplot2 extensions",
    "text": "ggplot2 extensions\nThanks to its popularity, ggplot2 has seen a proliferation of packages extending its capabilities. A full list can be found here.",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>",
      "Data visualization"
    ]
  },
  {
    "objectID": "r/index.html",
    "href": "r/index.html",
    "title": "R",
    "section": "",
    "text": "Getting started with  \nAn intro course in R\n\n\n\n\n  for the humanities\nR course for HSS\n\n\n\n\n\n\nHigh-performance  \nA course on HPC in R\n\n\n\n\nWorkshops\nVarious R topics\n\n\n\n\n\n\n60 min webinars\nVarious R topics",
    "crumbs": [
      "R",
      "<br>&nbsp;<img src=\"img/logo_r.png\" class=\"img-fluid\" style=\"width:1.5em\" alt=\"noshadow\"><br><br>"
    ]
  },
  {
    "objectID": "r/intro_basics.html",
    "href": "r/intro_basics.html",
    "title": "First steps in R",
    "section": "",
    "text": "In this section, we take our first few steps in R: we will access the R documentation, see how to set R options, and talk about a few concepts.",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "First steps in R"
    ]
  },
  {
    "objectID": "r/intro_basics.html#help-and-documentation",
    "href": "r/intro_basics.html#help-and-documentation",
    "title": "First steps in R",
    "section": "Help and documentation",
    "text": "Help and documentation\nFor some general documentation on R, you can run:\nhelp.start()\nTo get help on a function (e.g. sum), you can run:\nhelp(sum)\nDepending on your settings, this will open a documentation for sum in a pager or in your browser.",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "First steps in R"
    ]
  },
  {
    "objectID": "r/intro_basics.html#r-settings",
    "href": "r/intro_basics.html#r-settings",
    "title": "First steps in R",
    "section": "R settings",
    "text": "R settings\nSettings are saved in a .Rprofile file. You can edit the file directly in any text editor or from within R.\nList all options:\noptions()\nReturn the value of a particular option:\n\ngetOption(\"help_type\")\n\n[1] \"text\"\n\n\nSet an option:\noptions(help_type = \"html\")",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "First steps in R"
    ]
  },
  {
    "objectID": "r/intro_basics.html#assignment",
    "href": "r/intro_basics.html#assignment",
    "title": "First steps in R",
    "section": "Assignment",
    "text": "Assignment\nR can accept the equal sign (=) for assignments, but it is more idiomatic to use the assignment sign (&lt;-) whenever you bind a name to a value and to use the equal sign everywhere else.\n\na &lt;- 3\n\nOnce you have bound a name to a value, you can recall the value with that name:\n\na  # Note that you do not need to use a print() function in R\n\n[1] 3\n\n\nYou can remove an object from the environment by deleting its name:\n\nrm(a)\n\nLet’s confirm that a doesn’t exist anymore in the environment:\n\na\n\nError in eval(expr, envir, enclos): object 'a' not found\n\n\nThe garbage collector will take care of deleting the object itself from memory.",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "First steps in R"
    ]
  },
  {
    "objectID": "r/intro_basics.html#comments",
    "href": "r/intro_basics.html#comments",
    "title": "First steps in R",
    "section": "Comments",
    "text": "Comments\nAnything to the left of # is a comment and is ignored by R:\n\n# This is an inline comment\n\na &lt;- 3  # This is also a comment",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "First steps in R"
    ]
  },
  {
    "objectID": "r/intro_data_structure.html",
    "href": "r/intro_data_structure.html",
    "title": "Data types and structures",
    "section": "",
    "text": "This section covers the various data types and structures available in R.",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "Data types and structures"
    ]
  },
  {
    "objectID": "r/intro_data_structure.html#summary-of-structures",
    "href": "r/intro_data_structure.html#summary-of-structures",
    "title": "Data types and structures",
    "section": "Summary of structures",
    "text": "Summary of structures\n\n\n\nDimension\nHomogeneous\nHeterogeneous\n\n\n\n\n1 d\nAtomic vector\nList\n\n\n2 d\nMatrix\nData frame\n\n\n3 d\nArray",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "Data types and structures"
    ]
  },
  {
    "objectID": "r/intro_data_structure.html#atomic-vectors",
    "href": "r/intro_data_structure.html#atomic-vectors",
    "title": "Data types and structures",
    "section": "Atomic vectors",
    "text": "Atomic vectors\n\nWith a single element\n\na &lt;- 2\na\n\n[1] 2\n\ntypeof(a)\n\n[1] \"double\"\n\nstr(a)\n\n num 2\n\nlength(a)\n\n[1] 1\n\ndim(a)\n\nNULL\n\n\nThe dim attribute of a vector doesn’t exist (hence the NULL). This makes vectors different from one-dimensional arrays which have a dim of 1.\nYou might have noticed that 2 is a double (double precision floating point number, equivalent of “float” in other languages). In R, this is the default, even if you don’t type 2.0. This prevents the kind of weirdness you can find in, for instance, Python.\nIn Python:\n&gt;&gt;&gt; 2 == 2.0\nTrue\n&gt;&gt;&gt; type(2) == type(2.0)\nFalse\n&gt;&gt;&gt; type(2)\n&lt;class 'int'&gt;\n&gt;&gt;&gt; type(2.0)\n&lt;class 'float'&gt;\nIn R:\n&gt; 2 == 2.0\n[1] TRUE\n&gt; typeof(2) == typeof(2.0)\n[1] TRUE\n&gt; typeof(2)\n[1] \"double\"\n&gt; typeof(2.0)\n[1] \"double\"\nIf you want to define an integer variable, you use:\n\nb &lt;- 2L\nb\n\n[1] 2\n\ntypeof(b)\n\n[1] \"integer\"\n\nmode(b)\n\n[1] \"numeric\"\n\nstr(b)\n\n int 2\n\n\nThere are six vector types:\n\nlogical\ninteger\ndouble\ncharacter\ncomplex\nraw\n\n\n\nWith multiple elements\n\nc &lt;- c(2, 4, 1)\nc\n\n[1] 2 4 1\n\ntypeof(c)\n\n[1] \"double\"\n\nmode(c)\n\n[1] \"numeric\"\n\nstr(c)\n\n num [1:3] 2 4 1\n\n\n\nd &lt;- c(TRUE, TRUE, NA, FALSE)\nd\n\n[1]  TRUE  TRUE    NA FALSE\n\ntypeof(d)\n\n[1] \"logical\"\n\nstr(d)\n\n logi [1:4] TRUE TRUE NA FALSE\n\n\n\nNA (“Not Available”) is a logical constant of length one. It is an indicator for a missing value.\n\nVectors are homogeneous, so all elements need to be of the same type.\nIf you use elements of different types, R will convert some of them to ensure that they become of the same type:\n\ne &lt;- c(\"This is a string\", 3, \"test\")\ne\n\n[1] \"This is a string\" \"3\"                \"test\"            \n\ntypeof(e)\n\n[1] \"character\"\n\nstr(e)\n\n chr [1:3] \"This is a string\" \"3\" \"test\"\n\n\n\nf &lt;- c(TRUE, 3, FALSE)\nf\n\n[1] 1 3 0\n\ntypeof(f)\n\n[1] \"double\"\n\nstr(f)\n\n num [1:3] 1 3 0\n\n\n\ng &lt;- c(2L, 3, 4L)\ng\n\n[1] 2 3 4\n\ntypeof(g)\n\n[1] \"double\"\n\nstr(g)\n\n num [1:3] 2 3 4\n\n\n\nh &lt;- c(\"string\", TRUE, 2L, 3.1)\nh\n\n[1] \"string\" \"TRUE\"   \"2\"      \"3.1\"   \n\ntypeof(h)\n\n[1] \"character\"\n\nstr(h)\n\n chr [1:4] \"string\" \"TRUE\" \"2\" \"3.1\"\n\n\nThe binary operator : is equivalent to the seq() function and generates a regular sequence of integers:\n\ni &lt;- 1:5\ni\n\n[1] 1 2 3 4 5\n\ntypeof(i)\n\n[1] \"integer\"\n\nstr(i)\n\n int [1:5] 1 2 3 4 5\n\nidentical(2:8, seq(2, 8))\n\n[1] TRUE",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "Data types and structures"
    ]
  },
  {
    "objectID": "r/intro_data_structure.html#matrices",
    "href": "r/intro_data_structure.html#matrices",
    "title": "Data types and structures",
    "section": "Matrices",
    "text": "Matrices\n\nj &lt;- matrix(1:12, nrow = 3, ncol = 4)\nj\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    4    7   10\n[2,]    2    5    8   11\n[3,]    3    6    9   12\n\ntypeof(j)\n\n[1] \"integer\"\n\nstr(j)\n\n int [1:3, 1:4] 1 2 3 4 5 6 7 8 9 10 ...\n\nlength(j)\n\n[1] 12\n\ndim(j)\n\n[1] 3 4\n\n\nThe default is byrow = FALSE. If you want the matrix to be filled in by row, you need to set this argument to TRUE:\n\nk &lt;- matrix(1:12, nrow = 3, ncol = 4, byrow = TRUE)\nk\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    2    3    4\n[2,]    5    6    7    8\n[3,]    9   10   11   12",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "Data types and structures"
    ]
  },
  {
    "objectID": "r/intro_data_structure.html#arrays",
    "href": "r/intro_data_structure.html#arrays",
    "title": "Data types and structures",
    "section": "Arrays",
    "text": "Arrays\n\nl &lt;- array(as.double(1:24), c(3, 2, 4))\nl\n\n, , 1\n\n     [,1] [,2]\n[1,]    1    4\n[2,]    2    5\n[3,]    3    6\n\n, , 2\n\n     [,1] [,2]\n[1,]    7   10\n[2,]    8   11\n[3,]    9   12\n\n, , 3\n\n     [,1] [,2]\n[1,]   13   16\n[2,]   14   17\n[3,]   15   18\n\n, , 4\n\n     [,1] [,2]\n[1,]   19   22\n[2,]   20   23\n[3,]   21   24\n\ntypeof(l)\n\n[1] \"double\"\n\nstr(l)\n\n num [1:3, 1:2, 1:4] 1 2 3 4 5 6 7 8 9 10 ...\n\nlength(l)\n\n[1] 24\n\ndim(l)\n\n[1] 3 2 4",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "Data types and structures"
    ]
  },
  {
    "objectID": "r/intro_data_structure.html#lists",
    "href": "r/intro_data_structure.html#lists",
    "title": "Data types and structures",
    "section": "Lists",
    "text": "Lists\n\nm &lt;- list(2, 3)\nm\n\n[[1]]\n[1] 2\n\n[[2]]\n[1] 3\n\ntypeof(m)\n\n[1] \"list\"\n\nstr(m)\n\nList of 2\n $ : num 2\n $ : num 3\n\nlength(m)\n\n[1] 2\n\ndim(m)\n\nNULL\n\n\nAs with atomic vectors, lists do not have a dim attribute. Lists are in fact a different type of vectors.\nLists can be heterogeneous:\n\nn &lt;- list(2L, 3, c(2, 1), FALSE, \"string\")\nn\n\n[[1]]\n[1] 2\n\n[[2]]\n[1] 3\n\n[[3]]\n[1] 2 1\n\n[[4]]\n[1] FALSE\n\n[[5]]\n[1] \"string\"\n\ntypeof(n)\n\n[1] \"list\"\n\nstr(n)\n\nList of 5\n $ : int 2\n $ : num 3\n $ : num [1:2] 2 1\n $ : logi FALSE\n $ : chr \"string\"\n\nlength(n)\n\n[1] 5",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "Data types and structures"
    ]
  },
  {
    "objectID": "r/intro_data_structure.html#data-frames",
    "href": "r/intro_data_structure.html#data-frames",
    "title": "Data types and structures",
    "section": "Data frames",
    "text": "Data frames\nData frames contain tabular data. Under the hood, a data frame is a list of vectors.\n\no &lt;- data.frame(\n  country = c(\"Canada\", \"USA\", \"Mexico\"),\n  var = c(2.9, 3.1, 4.5)\n)\no\n\n  country var\n1  Canada 2.9\n2     USA 3.1\n3  Mexico 4.5\n\ntypeof(o)\n\n[1] \"list\"\n\nstr(o)\n\n'data.frame':   3 obs. of  2 variables:\n $ country: chr  \"Canada\" \"USA\" \"Mexico\"\n $ var    : num  2.9 3.1 4.5\n\nlength(o)\n\n[1] 2\n\ndim(o)\n\n[1] 3 2",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "Data types and structures"
    ]
  },
  {
    "objectID": "r/intro_functions.html",
    "href": "r/intro_functions.html",
    "title": "Function definition",
    "section": "",
    "text": "R comes with a number of built-in functions. Packages can provide additional ones. In many cases however, you will want to create your own functions to perform exactly the computations that you need.\nIn this section, we will see how to define new functions.",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "Function definition"
    ]
  },
  {
    "objectID": "r/intro_functions.html#syntax",
    "href": "r/intro_functions.html#syntax",
    "title": "Function definition",
    "section": "Syntax",
    "text": "Syntax\nHere is the syntax to define a new function:\nname &lt;- function(arguments) {\n  body\n}",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "Function definition"
    ]
  },
  {
    "objectID": "r/intro_functions.html#example",
    "href": "r/intro_functions.html#example",
    "title": "Function definition",
    "section": "Example",
    "text": "Example\nLet’s define a function that we call compare which will compare the value between 2 numbers:\n\ncompare &lt;- function(x, y) {\n  x == y\n}\n\n\ncompare is the name of our function.\nx and y are the placeholders for the arguments that our function will accept (our function will need 2 arguments to run successfully).\nx == y is the body of the function, that is, the computation performed by our function.\n\nWe can now use our function:\n\ncompare(2, 3)\n\n[1] FALSE",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "Function definition"
    ]
  },
  {
    "objectID": "r/intro_functions.html#what-is-returned-by-a-function",
    "href": "r/intro_functions.html#what-is-returned-by-a-function",
    "title": "Function definition",
    "section": "What is returned by a function?",
    "text": "What is returned by a function?\nIn R, the result of the last statement is printed automatically:\n\ntest &lt;- function(x, y) {\n  x\n  y\n}\ntest(2, 3)\n\n[1] 3\n\n\nIf you want to also print other results, you need to explicitly use the print() function:\n\ntest &lt;- function(x, y) {\n  print(x)\n  y\n}\ntest(2, 3)\n\n[1] 2\n\n\n[1] 3\n\n\nNote that, unlike print(), the function return() exits the function:\n\ntest &lt;- function(x, y) {\n  return(x)\n  y\n}\ntest(2, 3)\n\n[1] 2\n\n\n\ntest &lt;- function(x, y) {\n  return(x)\n  return(y)\n}\ntest(2, 3)\n\n[1] 2",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "Function definition"
    ]
  },
  {
    "objectID": "r/intro_indexing.html",
    "href": "r/intro_indexing.html",
    "title": "Indexing",
    "section": "",
    "text": "This section covers indexing from the various data structures.",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "Indexing"
    ]
  },
  {
    "objectID": "r/intro_indexing.html#indexing-atomic-vectors",
    "href": "r/intro_indexing.html#indexing-atomic-vectors",
    "title": "Indexing",
    "section": "Indexing atomic vectors",
    "text": "Indexing atomic vectors\n\nHere is an example with an atomic vector of size one:\n\nIndexing in R starts at 1 and is done with square brackets next to the element to index:\n\nx &lt;- 2\nx\n\n[1] 2\n\nx[1]\n\n[1] 2\n\n\nWhat happens if we index out of range?\n\nx[2]\n\n[1] NA\n\n\n\nExample for an atomic vector with multiple elements:\n\n\nx &lt;- c(2, 4, 1)\nx\n\n[1] 2 4 1\n\nx[2]\n\n[1] 4\n\nx[2:4]\n\n[1]  4  1 NA\n\n\n\nModifying mutable objects\nIndexing also allows to modify some of the values of mutable objects:\n\nx\n\n[1] 2 4 1\n\nx[2] &lt;- 0\nx\n\n[1] 2 0 1\n\n\n\n\nCopy-on-modify\nNot all languages behave the same when you assign the same mutable object to several variables, then modify one of them.\n\nIn Python: no copy-on-modify\n\nDon’t try to run this code in R. This is for information only.\n\n\n\nPython\n\na = [1, 2, 3]\nb = a\nb\n\n[1, 2, 3]\n\n\nPython\n\na[0] = 4           # In Python, indexing starts at 0\na\n\n[4, 2, 3]\n\n\nPython\n\nb\n\n[4, 2, 3]\nModifying a also modifies b: this is because no copy is made when you modify a. If you want to keep b unchanged, you need to assign an explicit copy of a to it with b = copy.copy(a).\n\n\nIn R: copy-on-modify\n\na &lt;- c(1, 2, 3)\nb &lt;- a\nb\n\n[1] 1 2 3\n\na[1] &lt;- 4          # In R, indexing starts at 1\na\n\n[1] 4 2 3\n\nb\n\n[1] 1 2 3\n\n\nHere, the default is to create a new copy in memory when a is transformed so that b remains unchanged.",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "Indexing"
    ]
  },
  {
    "objectID": "r/intro_indexing.html#indexing-matrices-and-arrays",
    "href": "r/intro_indexing.html#indexing-matrices-and-arrays",
    "title": "Indexing",
    "section": "Indexing matrices and arrays",
    "text": "Indexing matrices and arrays\n\nx &lt;- matrix(1:12, nrow = 3, ncol = 4)\nx\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    4    7   10\n[2,]    2    5    8   11\n[3,]    3    6    9   12\n\nx[2, 3]\n\n[1] 8\n\nx &lt;- array(as.double(1:24), c(3, 2, 4))\nx\n\n, , 1\n\n     [,1] [,2]\n[1,]    1    4\n[2,]    2    5\n[3,]    3    6\n\n, , 2\n\n     [,1] [,2]\n[1,]    7   10\n[2,]    8   11\n[3,]    9   12\n\n, , 3\n\n     [,1] [,2]\n[1,]   13   16\n[2,]   14   17\n[3,]   15   18\n\n, , 4\n\n     [,1] [,2]\n[1,]   19   22\n[2,]   20   23\n[3,]   21   24\n\nx[2, 1, 3]\n\n[1] 14",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "Indexing"
    ]
  },
  {
    "objectID": "r/intro_indexing.html#indexing-lists",
    "href": "r/intro_indexing.html#indexing-lists",
    "title": "Indexing",
    "section": "Indexing lists",
    "text": "Indexing lists\n\nx &lt;- list(2L, 3:8, c(2, 1), FALSE, \"string\")\nx\n\n[[1]]\n[1] 2\n\n[[2]]\n[1] 3 4 5 6 7 8\n\n[[3]]\n[1] 2 1\n\n[[4]]\n[1] FALSE\n\n[[5]]\n[1] \"string\"\n\n\nIndexing a list returns a list:\n\nx[3]\n\n[[1]]\n[1] 2 1\n\ntypeof(x[3])\n\n[1] \"list\"\n\n\nTo extract elements of a list, double square brackets are required:\n\nx[[3]]\n\n[1] 2 1\n\ntypeof(x[[3]])\n\n[1] \"double\"\n\n\n\n\nYour turn:\n\nTry to extract the number 7 from this list.",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "Indexing"
    ]
  },
  {
    "objectID": "r/intro_indexing.html#indexing-data-frames",
    "href": "r/intro_indexing.html#indexing-data-frames",
    "title": "Indexing",
    "section": "Indexing data frames",
    "text": "Indexing data frames\n\nx &lt;- data.frame(\n  country = c(\"Canada\", \"USA\", \"Mexico\"),\n  var = c(2.9, 3.1, 4.5)\n)\nx\n\n  country var\n1  Canada 2.9\n2     USA 3.1\n3  Mexico 4.5\n\n\nIndexing dataframes can be done by using indices, as we saw for matrices:\n\nx[2, 1]\n\n[1] \"USA\"\n\n\nIt can also be done using column names thanks to the $ symbol (a column is a vector, so indexing from a column is the same as indexing from a vector):\n\nx$country[2]\n\n[1] \"USA\"\n\n\nA data frame is actually a list of vectors representing the various columns:\n\ntypeof(x)\n\n[1] \"list\"\n\n\nIndexing a column can thus also be done by indexing the element of the list with double square brackets (although this is a slower method).\nWe get the same result with:\n\nx[[1]][2]\n\n[1] \"USA\"",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "Indexing"
    ]
  },
  {
    "objectID": "r/intro_packages.html",
    "href": "r/intro_packages.html",
    "title": "Packages",
    "section": "",
    "text": "Packages are a set of functions, constants, and/or data developed by the community that add functionality to R.\nIn this section, we look at where to find packages and how to install them.",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "Packages"
    ]
  },
  {
    "objectID": "r/intro_packages.html#looking-for-packages",
    "href": "r/intro_packages.html#looking-for-packages",
    "title": "Packages",
    "section": "Looking for packages",
    "text": "Looking for packages\n\nPackage finder.\nYour peers and the literature.\nList of CRAN packages.\nList of CRAN task views (list of packages with information for a large number of wide topics).",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "Packages"
    ]
  },
  {
    "objectID": "r/intro_packages.html#managing-r-packages",
    "href": "r/intro_packages.html#managing-r-packages",
    "title": "Packages",
    "section": "Managing R packages",
    "text": "Managing R packages\n\nFor this course, you won’t have to install any package as they have already been installed in our RStudio server.\n\nR packages can be installed, updated, and removed from within R:\ninstall.packages(\"&lt;package_name&gt;\", repos=\"&lt;url-cran-mirror&gt;\")\nremove.packages(\"&lt;package-name&gt;\")\nupdate_packages()\n\nExample:\n\ninstall.packages(\"rvest\", repos=\"https://mirror.rcg.sfu.ca/mirror/CRAN/\")\n\nrepos argument: chose a CRAN mirror close to the location of your cluster or use https://cloud.r-project.org/.\n\n\nThe first time you install a package, R will ask you whether you want to create a personal library in your home directory. Answer yes to both questions. Your packages will now install under ~/.\n\n\nSome packages require additional modules to be loaded before they can be installed. Other packages need additional R packages as dependencies. In either case, you will get explicit error messages. Adding the argument dependencies = T helps in the second case, but you will still have to add packages manually from time to time.",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "Packages"
    ]
  },
  {
    "objectID": "r/intro_packages.html#loading-packages",
    "href": "r/intro_packages.html#loading-packages",
    "title": "Packages",
    "section": "Loading packages",
    "text": "Loading packages\nTo make a package available in an R session, you load it with the library() function.\n\nExample:\n\nlibrary(readxl)\nAlternatively, you can access a function from a package without loading it with the syntax: package::function().\n\nExample:\n\nreadxl::read_excel(\"file.xlsx\")",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "Packages"
    ]
  },
  {
    "objectID": "r/intro_packages.html#package-documentation",
    "href": "r/intro_packages.html#package-documentation",
    "title": "Packages",
    "section": "Package documentation",
    "text": "Package documentation\n\nSelect a package from the list of CRAN packages.\nGoogle “cran” and the name of your package (e.g. “cran dplyr”).\nLook up a package in the package documentation.\nGet a list of functions within a package with the help() function (installed, but not loaded in session):\n\n\nExample to get a list of functions in the dplyr package:\n\nhelp(package = \"dplyr\")\n\nGet help on a function within a package:\n\nIf you are using RStudio or the HTML format for your R help and you already ran the command to get the list of functions within a package (e.g. help(package = \"dplyr\")), you can get help on any function by clicking on its name.\nIf you are using the text format for help (for instance, if you are running R remotely on the command line), you can get help for any function by adding its name at as the first argument of the previous command.\n\nExample to get help on the function bind() of the package dplyr:\n\nhelp(bind, package = \"dplyr\")\nOf course, if the dplyr package is already loaded in your session, you can simply run help(bind).\n\nGet a list of all help files with alias or concept or title matching a regular expression in all installed packages:\n\n\nExample to get a list of all help files with alias or concept or title matching bind:\n\n??bind\nYou can then open those help files as seen previously.\n\nGet a list of all vignettes for all installed packages:\n\nIf you are using RStudio or the HTML help format:\nbrowseVignettes()\nIf you are using the text help format:\nvignette()\n\nGet a list of vignettes available for a package (not all packages have vignettes):\n\n\nExample to get a list of vignettes for the package dplyr:\n\nIf you are using RStudio or the HTML help format:\nvignette(package = \"dplyr\")\nIf you are using the text help format:\nbrowseVignettes(package = \"dplyr\")\nYou can then open those help vignettes as seen previously.",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "Packages"
    ]
  },
  {
    "objectID": "r/intro_publishing.html",
    "href": "r/intro_publishing.html",
    "title": "Publishing",
    "section": "",
    "text": "You might have heard of R Markdown: a way to intertwine code and prose in a single scientific document. The company behind R Markdown has now developed its successor: Quarto.\n\nQuarto allows the creation of webpages, websites, presentations, books, pdf, etc. from code in R, Python, or Julia and markdown text.\nIf you are interested in an introduction to this tool, you can have a look at our workshop or our webinar on Quarto.\nBy the way, this entire website was created with Quarto 🙂",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "Publishing"
    ]
  },
  {
    "objectID": "r/intro_run.html",
    "href": "r/intro_run.html",
    "title": "Running R",
    "section": "",
    "text": "This section covers the various ways R can be run, then shows you how to access our temporary RStudio server for this course.",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "Running R"
    ]
  },
  {
    "objectID": "r/intro_run.html#running-r",
    "href": "r/intro_run.html#running-r",
    "title": "Running R",
    "section": "Running R",
    "text": "Running R\nR being an interpreted language, it can be run non-interactively or interactively.\n\nRunning R non-interactively\nIf you write code in a text file (called a script), you can then execute it with:\nRscript my_script.R\n\nThe command to execute scripts is Rscript rather than R.\nBy convention, R scripts take the extension .R.\n\n\n\nRunning R interactively\nThere are several ways to run R interactively.\n\nDirectly in the console (the name for the R shell):\n\n\n\nIn Jupyter with the R kernel (IRkernel package).\nIn another IDE (e.g. in Emacs with ESS).\nIn the RStudio IDE.\n\nThe RStudio IDE is popular and this is what we will use today. RStudio can can be run locally, but for this course, we will use an RStudio server.",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "Running R"
    ]
  },
  {
    "objectID": "r/intro_run.html#accessing-our-rstudio-server",
    "href": "r/intro_run.html#accessing-our-rstudio-server",
    "title": "Running R",
    "section": "Accessing our RStudio server",
    "text": "Accessing our RStudio server\nYou do not need to install anything on your machine for this course as we will provide access to a temporary RStudio server.\n\nA username, a password, and the URL of the RStudio server will be given to you during the workshop.\n\nSign in using the username and password you will be given while ignoring the OTP entry. This will take you to the server options page of a JupyterHub.\n\nSelect the following server options:\n\nTime: 2 hours\nNumber of cores: 1\nMemory: 3700 MB\nUser interface: JupyterLab\n\n\nThen press “Start” to launch the JupyterHub. There, click on the “RStudio” button and the RStudio server will open in a new tab.\n\nNote that this temporary cluster will only be available for the duration of this course.",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "Running R"
    ]
  },
  {
    "objectID": "r/intro_run.html#using-rstudio",
    "href": "r/intro_run.html#using-rstudio",
    "title": "Running R",
    "section": "Using RStudio",
    "text": "Using RStudio\nFor those unfamiliar with the RStudio IDE, you can download the following cheatsheet:\n\n\n\nfrom Posit Cheatsheets",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "Running R"
    ]
  },
  {
    "objectID": "r/intro_why.html",
    "href": "r/intro_why.html",
    "title": "R: why and for whom?",
    "section": "",
    "text": "There are other high level programming languages such as Python or Julia, so when might it make sense for you to turn to R?",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "R: why and for whom?"
    ]
  },
  {
    "objectID": "r/intro_why.html#why-r",
    "href": "r/intro_why.html#why-r",
    "title": "R: why and for whom?",
    "section": "Why R?",
    "text": "Why R?\nHere are a number of reasons why you might want to consider using R:\n\nFree and open source\nHigh-level and easy to learn\nLarge community\nVery well documented\nUnequalled number of statistics and modelling packages\nIntegrated package manager\nEasy connection with fast compiled languages such as C and C++\nPowerful IDEs (e.g. RStudio, ESS, Jupyter)",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "R: why and for whom?"
    ]
  },
  {
    "objectID": "r/intro_why.html#for-whom",
    "href": "r/intro_why.html#for-whom",
    "title": "R: why and for whom?",
    "section": "For whom?",
    "text": "For whom?\nFor whom is R particularly well suited?\n\nFields with heavy statistics, modelling, or Bayesian analysis such as biology, linguistics, economics, or statistics\nData science using a lot of tabular data",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "R: why and for whom?"
    ]
  },
  {
    "objectID": "r/intro_why.html#downsides-of-r",
    "href": "r/intro_why.html#downsides-of-r",
    "title": "R: why and for whom?",
    "section": "Downsides of R",
    "text": "Downsides of R\nOf course, R also has its downsides:\n\nInconsistent syntax full of quirks\nSlow\nLarge memory usage",
    "crumbs": [
      "R",
      "<em><b>Getting started</b></em>",
      "R: why and for whom?"
    ]
  },
  {
    "objectID": "r/top_hss.html",
    "href": "r/top_hss.html",
    "title": "R for the humanities",
    "section": "",
    "text": "R is a free and open-source programming language for statistical computing, modelling, and graphics, with an unbeatable collection of statistical packages. It is extremely popular in the humanities and social sciences.\nThis course does not assume any programming knowledge.\n\n Start course ➤",
    "crumbs": [
      "R",
      "<em><b>R for the humanities</b></em>"
    ]
  },
  {
    "objectID": "r/top_wb.html",
    "href": "r/top_wb.html",
    "title": "R webinars",
    "section": "",
    "text": "Intro to GIS mapping in R\n\n\n\n\nHigh-performance computing in R",
    "crumbs": [
      "R",
      "<em><b>Webinars</b></em>"
    ]
  },
  {
    "objectID": "r/wb_gis_mapping.html",
    "href": "r/wb_gis_mapping.html",
    "title": "GIS mapping with R",
    "section": "",
    "text": "In this webinar, we will see how to create all sorts of GIS maps with the packages sf, tmap, raster, leaflet, ggplot2, grid (part of Base R), and mapview:\n\nsimple maps\ninset maps\nfaceted maps\nanimated maps\ninteractive maps\nraster maps\n\nFinally, we will learn how to add basemaps from OpenStreetMap and Google Maps.\nSlides (Click and wait: this reveal.js presentation may take a little time to load.)",
    "crumbs": [
      "R",
      "<em><b>Webinars</b></em>",
      "Intro to GIS mapping in R"
    ]
  },
  {
    "objectID": "r/wb_gis_mapping.html#gis-concepts",
    "href": "r/wb_gis_mapping.html#gis-concepts",
    "title": "GIS mapping with R",
    "section": "GIS concepts",
    "text": "GIS concepts\n\nTypes of spatial data\n\nVector data\nVector data represent discrete objects.\nThey contain:\n\na geometry: the shape and location of the objects,\nattributes: additional variables (e.g. name, year, type).\n\nCommon file formats include GeoJSON and shapefile.\n\nExamples: countries, roads, rivers, towns.\n\n\n\nRaster data\nRaster data represent continuous phenomena or spatial fields.\nCommon file formats include TIFF, GeoTIFF, NetCDF, and Esri grid.\n\nExamples: temperature, air quality, elevation, water depth.\n\n\n\n\nVector data\nVector data come in multiple types:\n\npoint:     single set of coordinates,\nmulti-point:   multiple sets of coordinates,\npolyline:    multiple sets for which the order matters,\nmulti-polyline:  multiple of the above,\npolygon:    same as polyline but first and last sets are the same,\nmulti-polygon:  multiple of the above.\n\n\n\nRaster data\nGrid of equally sized rectangular cells containing values for some variables.\nSize of cells = resolution.\nFor computing efficiency, rasters do not have coordinates of each cell, but the bounding box and the number of rows and columns.\n\n\nCoordinate Reference Systems (CRS)\nA location on Earth’s surface can be identified by its coordinates and some reference system called CRS.\nThe coordinates (x, y) are called longitude and latitude.\nThere can be a 3rd coordinate (z) for elevation or other measurement—usually a vertical one.\nAnd a 4th (m) for some other data attribute—usually a horizontal measurement.\nIn 3D, longitude and latitude are expressed in angular units (e.g. degrees) and the reference system needed is an angular CRS or geographic coordinate system (GCS).\nIn 2D, they are expressed in linear units (e.g. meters) and the reference system needed is a planar CRS or projected coordinate system (PCS).\n\n\nDatums\nSince the Earth is not a perfect sphere, we use spheroidal models to represent its surface. Those are called geodetic datums.\nSome datums are global, others local (more accurate in a particular area of the globe, but only useful there).\n\nExamples of commonly used global datums:\n\nWGS84 (World Geodesic System 1984),\nNAD83 (North American Datum of 1983).\n\n\n\n\nAngular CRS\nAn angular CRS contains a datum, an angular unit and references such as a prime meridian (e.g. the Royal Observatory, Greenwich, England).\nIn an angular CRS or GCS:\n\nLongitude (\\(\\lambda\\)) represents the angle between the prime meridian and the meridian that passes through that location.\nLatitude (\\(\\phi\\)) represents the angle between the line that passes through the center of the Earth and that location and its projection on the equatorial plane.\n\nLongitude and latitude are thus angular coordinates.\n\n\nProjections\nTo create a two-dimensional map, you need to project this 3D angular CRS into a 2D one.\nVarious projections offer different characteristics. For instance:\n\nsome respect areas (equal-area),\nsome respect the shape of geographic features (conformal),\nsome almost respect both for small areas.\n\nIt is important to choose one with sensible properties for your goals.\n\nExamples of projections:\n\nMercator,\nUTM,\nRobinson.\n\n\n\n\nPlanar CRS\nA planar CRS is defined by a datum, a projection and a set of parameters such as a linear unit and the origins.\nCommon planar CRS have been assigned a unique ID called EPSG code which is much more convenient to use.\nIn a planar CRS, coordinates will not be in degrees anymore but in meters (or other length unit).\n\n\nProjecting into a new CRS\nYou can change the projection of your data.\nVector data won’t suffer any loss of precision, but raster data will.\n→ best to try to avoid reprojecting rasters: if you want to combine various datasets which have different projections, reproject vector data instead.",
    "crumbs": [
      "R",
      "<em><b>Webinars</b></em>",
      "Intro to GIS mapping in R"
    ]
  },
  {
    "objectID": "r/wb_gis_mapping.html#gis-in-r",
    "href": "r/wb_gis_mapping.html#gis-in-r",
    "title": "GIS mapping with R",
    "section": "GIS in R",
    "text": "GIS in R\n\nResources\n\nOpen GIS data\nFree GIS Data: list of free GIS datasets.\n\n\nBooks\nGeocomputation with R by Robin Lovelace, Jakub Nowosad and Jannes Muenchow.\nSpatial Data Science by Edzer Pebesma and Roger Bivand.\nSpatial Data Science with R by Robert J. Hijmans.\nUsing Spatial Data with R by Claudia A. Engel.\n\n\nTutorial\nAn Introduction to Spatial Data Analysis and Visualisation in R by the CDRC.\n\n\n\nResources\n\nWebsite\nr-spatial by Edzer Pebesma, Marius Appel and Daniel Nüst.\n\n\nCRAN package list\nAnalysis of Spatial Data.\n\n\nMailing list\nR Special Interest Group on using Geographical data and Mapping.",
    "crumbs": [
      "R",
      "<em><b>Webinars</b></em>",
      "Intro to GIS mapping in R"
    ]
  },
  {
    "objectID": "r/wb_gis_mapping.html#packages",
    "href": "r/wb_gis_mapping.html#packages",
    "title": "GIS mapping with R",
    "section": "Packages",
    "text": "Packages\nThere is now a rich ecosystem of GIS packages in R1.\n\nData manipulation\n\nOlder packages\n\nsp\nraster\nrgdal\nrgeos\n\n\n\nNewer generation\n\nsf:    vector data,\nterra:  raster data (also has vector data capabilities).\n\n\n\n\nMapping\n\nStatic maps\n\nggplot2 + ggspatial\ntmap\n\n\n\nDynamic maps\n\nleaflet\nggplot2 + gganimate\nmapview\nggmap\ntmap\n\n\n\n\nsf: Simple Features in R\nGeospatial vectors: points, lines, polygons.\nSimple Features—defined by the Open Geospatial Consortium (OGC) and formalized by ISO—is a set of standards now used by most GIS libraries.\nWell-known text (WKT) is a markup language for representing vector geometry objects according to those standards.\nA compact computer version also exists—well-known binary (WKB)—used by spatial databases.\nThe package sp predates Simple Features.\nsf—launched in 2016—implements these standards in R in the form of sf objects: data.frames (or tibbles) containing the attributes, extended by sfc objects or simple feature geometries list-columns.\n\n\nsf\nSome useful links:\n\nGitHub repo,\nPaper,\nResources,\nCheatsheet,\n6 vignettes: 1, 2, 3, 4, 5, 6.\n\nAnd the cheatsheet:\nxxxxx\n\n\n\nsf objects\n\n\n\n\n\n\n\nsf functions\nMost functions start with st_ (which refers to “spatial type”).\n\n\nterra: Geospatial rasters\nFaster and simpler replacement for the raster package by the same team.\nMostly implemented in C++.\nCan work with datasets too large to be loaded into memory.\n\n\nterra\nSome useful links:\n\nGitHub repo,\nResources,\nFull manual.\n\n\n\ntmap: Layered grammar of graphics GIS maps\nSome useful links:\n\nGitHub repo,\nResources.\n\n\nHelp pages and vignettes\n?tmap-element\nvignette(\"tmap-getstarted\")\n# All the usual help pages, e.g.:\n?tm_layout\n\n\ntmap functions\nMain functions start with tmap_\nFunctions creating map elements start with tm_\n\n\ntmap functioning\nVery similar to ggplot2\nTypically, a map contains:\n\nOne or multiple layer(s) (the order matters as they stack on top of each other)\nSome layout (e.g. customization of title, background, margins): tm_layout\nA compass: tm_compass\nA scale bar: tm_scale_bar\n\nEach layer contains:\n\nSome data: tm_shape\nHow that data will be represented: e.g. tm_polygons, tm_lines, tm_raster\n\n\n\ntmap example\n\n\n\n\n\n\n\n\n\n\n\nggplot2 (the standard in R plots)\nSome useful links:\n\nGitHub repo,\nResources,\nCheatsheet.\n\n\n\ngeom_sf allows to plot sf objects (i.e. make maps).",
    "crumbs": [
      "R",
      "<em><b>Webinars</b></em>",
      "Intro to GIS mapping in R"
    ]
  },
  {
    "objectID": "r/wb_gis_mapping.html#example-glaciers-melt-in-north-america",
    "href": "r/wb_gis_mapping.html#example-glaciers-melt-in-north-america",
    "title": "GIS mapping with R",
    "section": "Example: Glaciers melt in North America",
    "text": "Example: Glaciers melt in North America\n\nData\nFor this webinar, we will use:\n\nthe Alaska as well as the Western Canada and USA subsets of the Randolph Glacier Inventory version 6.02,\nthe USGS time series of the named glaciers of Glacier National Park3,\nthe Alaska as well as the Western Canada and USA subsets of the consensus estimate for the ice thickness distribution of all glaciers on Earth dataset4.\n\nThe datasets can be downloaded as zip files from these websites\n\n\nPackages\nPackages need to be installed before they can be loaded in a session.\nPackages on CRAN can be installed with:\ninstall.packages(\"&lt;package-name&gt;\")\nbasemaps is not on CRAN and needs to be installed from GitHub thanks to devtools:\ninstall.packages(\"devtools\")\ndevtools::install_github(\"16EAGLE/basemaps\")\nWe load all the packages that we will need at the top of the script:\nlibrary(sf)                 # spatial vector data manipulation\nlibrary(tmap)               # map production and tiled web map\nlibrary(dplyr)              # non GIS specific (tabular data manipulation)\nlibrary(magrittr)           # non GIS specific (pipes)\nlibrary(purrr)              # non GIS specific (functional programming)\nlibrary(rnaturalearth)      # basemap data access functions\nlibrary(rnaturalearthdata)  # basemap data\nlibrary(mapview)            # tiled web map\nlibrary(grid)               # (part of base R) used to create inset map\nlibrary(ggplot2)            # alternative to tmap for map production\nlibrary(ggspatial)          # spatial framework for ggplot2\nlibrary(terra)              # gridded spatial data manipulation\nlibrary(ggmap)              # download basemap data\nlibrary(basemaps)           # download basemap data\nlibrary(magick)             # wrapper around ImageMagick STL\nlibrary(leaflet)            # integrate Leaflet JS in R",
    "crumbs": [
      "R",
      "<em><b>Webinars</b></em>",
      "Intro to GIS mapping in R"
    ]
  },
  {
    "objectID": "r/wb_gis_mapping.html#reading-and-preparing-data",
    "href": "r/wb_gis_mapping.html#reading-and-preparing-data",
    "title": "GIS mapping with R",
    "section": "Reading and preparing data",
    "text": "Reading and preparing data\n\nRandolph Glacier Inventory\nThis dataset contains the contour of all glaciers on Earth.\nWe will focus on glaciers in Western North America.\nYou can download and unzip 02_rgi60_WesternCanadaUS and 01_rgi60_Alaska from the Randolph Glacier Inventory version 6.0.\n\n\nReading in data\nData get imported and turned into sf objects with the function sf::st_read:\nak &lt;- st_read(\"data/01_rgi60_Alaska\")\n\nMake sure to use the absolute paths or the paths relative to your working directory (which can be obtained with getwd).\n\nak &lt;- st_read(\"data/01_rgi60_Alaska\")\nReading layer `01_rgi60_Alaska' from data source `./data/01_rgi60_Alaska'\n               using driver `ESRI Shapefile'\nSimple feature collection with 27108 features and 22 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -176.1425 ymin: 52.05727 xmax: -126.8545 ymax: 69.35167\nGeodetic CRS:  WGS 84\n\n\nYour turn:\n\nRead in the data for the rest of north western America (from 02_rgi60_WesternCanadaUS) and create an sf object called wes.\n\n\n\nFirst look at the data\nak\nSimple feature collection with 27108 features and 22 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -176.1425 ymin: 52.05727 xmax: -126.8545 ymax: 69.35167\nGeodetic CRS:  WGS 84\nFirst 10 features:\n           RGIId        GLIMSId  BgnDate  EndDate    CenLon   CenLat O1Region\n1  RGI60-01.00001 G213177E63689N 20090703 -9999999 -146.8230 63.68900        1\n2  RGI60-01.00002 G213332E63404N 20090703 -9999999 -146.6680 63.40400        1\n3  RGI60-01.00003 G213920E63376N 20090703 -9999999 -146.0800 63.37600        1\n4  RGI60-01.00004 G213880E63381N 20090703 -9999999 -146.1200 63.38100        1\n5  RGI60-01.00005 G212943E63551N 20090703 -9999999 -147.0570 63.55100        1\n6  RGI60-01.00006 G213756E63571N 20090703 -9999999 -146.2440 63.57100        1\n7  RGI60-01.00007 G213771E63551N 20090703 -9999999 -146.2295 63.55085        1\n8  RGI60-01.00008 G213704E63543N 20090703 -9999999 -146.2960 63.54300        1\n9  RGI60-01.00009 G212400E63659N 20090703 -9999999 -147.6000 63.65900        1\n10 RGI60-01.00010 G212830E63513N 20090703 -9999999 -147.1700 63.51300        1\nO2Region   Area Zmin Zmax Zmed Slope Aspect  Lmax Status Connect Form\n1         2  0.360 1936 2725 2385    42    346   839      0       0    0\n2         2  0.558 1713 2144 2005    16    162  1197      0       0    0\n3         2  1.685 1609 2182 1868    18    175  2106      0       0    0\n4         2  3.681 1273 2317 1944    19    195  4175      0       0    0\n5         2  2.573 1494 2317 1914    16    181  2981      0       0    0\n6         2 10.470 1201 3547 1740    22     33 10518      0       0    0\n7         2  0.649 1918 2811 2194    23    151  1818      0       0    0\n8         2  0.200 2826 3555 3195    45     80   613      0       0    0\n9         2  1.517 1750 2514 1977    18    274  2255      0       0    0\n10        2  3.806 1280 1998 1666    17     35  3332      0       0    0\nTermType Surging Linkages Name                       geometry\n1         0       9        9 &lt;NA&gt; POLYGON ((-146.818 63.69081...\n2         0       9        9 &lt;NA&gt; POLYGON ((-146.6635 63.4076...\n3         0       9        9 &lt;NA&gt; POLYGON ((-146.0723 63.3834...\n4         0       9        9 &lt;NA&gt; POLYGON ((-146.149 63.37919...\n5         0       9        9 &lt;NA&gt; POLYGON ((-147.0431 63.5502...\n6         0       9        9 &lt;NA&gt; POLYGON ((-146.2436 63.5562...\n7         0       9        9 &lt;NA&gt; POLYGON ((-146.2495 63.5531...\n8         0       9        9 &lt;NA&gt; POLYGON ((-146.2992 63.5443...\n9         0       9        9 &lt;NA&gt; POLYGON ((-147.6147 63.6643...\n10        0       9        9 &lt;NA&gt; POLYGON ((-147.1494 63.5098...\n\n\nStructure of the data\nstr(ak)\nClasses ‘sf’ and 'data.frame':  27108 obs. of  23 variables:\n$ RGIId   : chr  \"RGI60-01.00001\" \"RGI60-01.00002\" \"RGI60-01.00003\" ...\n$ GLIMSId : chr  \"G213177E63689N\" \"G213332E63404N\" \"G213920E63376N\" ...\n$ BgnDate : chr  \"20090703\" \"20090703\" \"20090703\" \"20090703\" ...\n$ EndDate : chr  \"-9999999\" \"-9999999\" \"-9999999\" \"-9999999\" ...\n$ CenLon  : num  -147 -147 -146 -146 -147 ...\n$ CenLat  : num  63.7 63.4 63.4 63.4 63.6 ...\n$ O1Region: chr  \"1\" \"1\" \"1\" \"1\" ...\n$ O2Region: chr  \"2\" \"2\" \"2\" \"2\" ...\n$ Area    : num  0.36 0.558 1.685 3.681 2.573 ...\n$ Zmin    : int  1936 1713 1609 1273 1494 1201 1918 2826 1750 1280 ...\n$ Zmax    : int  2725 2144 2182 2317 2317 3547 2811 3555 2514 1998 ...\n$ Zmed    : int  2385 2005 1868 1944 1914 1740 2194 3195 1977 1666 ...\n$ Slope   : num  42 16 18 19 16 22 23 45 18 17 ...\n$ Aspect  : int  346 162 175 195 181 33 151 80 274 35 ...\n$ Lmax    : int  839 1197 2106 4175 2981 10518 1818 613 2255 3332 ...\n$ Status  : int  0 0 0 0 0 0 0 0 0 0 ...\n$ Connect : int  0 0 0 0 0 0 0 0 0 0 ...\n$ Form    : int  0 0 0 0 0 0 0 0 0 0 ...\n$ TermType: int  0 0 0 0 0 0 0 0 0 0 ...\n$ Surging : int  9 9 9 9 9 9 9 9 9 9 ...\n$ Linkages: int  9 9 9 9 9 9 9 9 9 9 ...\n$ Name    : chr  NA NA NA NA ...\n$ geometry:sfc_POLYGON of length 27108; first list element: List of 1\n..$ : num [1:65, 1:2] -147 -147 -147 -147 -147 ...\n..- attr(*, \"class\")= chr [1:3] \"XY\" \"POLYGON\" \"sfg\"\n- attr(*, \"sf_column\")= chr \"geometry\"\n- attr(*, \"agr\")= Factor w/ 3 levels \"constant\",\"aggregate\",..: NA NA NA ...\n..- attr(*, \"names\")= chr [1:22] \"RGIId\" \"GLIMSId\" \"BgnDate\" \"EndDate\" ...\n\n\nInspect your data\n\n\nYour turn:\n\nInspect the wes object you created.\n\n\n\nGlacier National Park dataset\nThis dataset contains a time series of the retreat of 39 glaciers of Glacier National Park, MT, USA for the years 1966, 1998, 2005 and 2015.\nYou can download and unzip the 4 sets of files from the USGS website.\n\n\nRead in and clean datasets\nCreate a function that reads and cleans the data:\nprep &lt;- function(dir) {\n  g &lt;- st_read(dir)\n  g %&lt;&gt;% rename_with(~ tolower(gsub(\"Area....\", \"area\", .x)))\n  g %&lt;&gt;% dplyr::select(\n    year,\n    objectid,\n    glacname,\n    area,\n    shape_leng,\n    x_coord,\n    y_coord,\n    source_sca,\n    source\n  )\n}\nCreate a vector of dataset names:\ndirs &lt;- grep(\"data/GNPglaciers_.*\", list.dirs(), value = T)\nPass each element of that vector through prep() thanks to map():\ngnp &lt;- map(dirs, prep)\n\nWe use dplyr::select because terra also has a select function.\n\n\n\nCombine datasets into one sf object\nCheck that the CRS are all the same:\nall(sapply(\n  list(st_crs(gnp[[1]]),\n       st_crs(gnp[[2]]),\n       st_crs(gnp[[3]]),\n       st_crs(gnp[[4]])),\n  function(x) x == st_crs(gnp[[1]])\n))\n[1] TRUE\nWe can rbind the elements of our list:\ngnp &lt;- do.call(\"rbind\", gnp)\nYou can inspect your new sf object by calling it or with str.\n\n\nEstimate for ice thickness\nThis dataset contains an estimate for the ice thickness of all glaciers on Earth.\nThe nomenclature follows the Randolph Glacier Inventory.\nIce thickness being a spatial field, this is raster data.\nWe will use data in RGI60-02.16664_thickness.tif from the ETH Zürich Research Collection which corresponds to one of the glaciers (Agassiz) of Glacier National Park.\n\n\nLoad raster data\nRead in data and create a SpatRaster object:\nras &lt;- rast(\"data/RGI60-02/RGI60-02.16664_thickness.tif\")\n\n\nInspect our SpatRaster object\nras\nclass       : SpatRaster \ndimensions  : 93, 74, 1  (nrow, ncol, nlyr)\nresolution  : 25, 25  (x, y)\nextent      : 707362.5, 709212.5, 5422962, 5425288  (xmin, xmax, ymin, ymax)\ncoord. ref. : +proj=utm +zone=11 +datum=WGS84 +units=m +no_defs \nsource      : RGI60-02.16664_thickness.tif \nname        : RGI60-02.16664_thickness \nnlyr gives us the number of bands (a single one here). You can also run str(ras).\n\n\nOur data\nWe now have 3 sf objects and 1 SpatRaster object:\n\nak: contour of glaciers in AK,\nwes: contour of glaciers in the rest of Western North America,\ngnp: time series of 39 glaciers in Glacier National Park, MT, USA,\nras: ice thickness of the Agassiz Glacier from Glacier National Park.",
    "crumbs": [
      "R",
      "<em><b>Webinars</b></em>",
      "Intro to GIS mapping in R"
    ]
  },
  {
    "objectID": "r/wb_gis_mapping.html#making-maps",
    "href": "r/wb_gis_mapping.html#making-maps",
    "title": "GIS mapping with R",
    "section": "Making maps",
    "text": "Making maps\n\nLet’s map our sf object ak\nAt a bare minimum, we need tm_shape with the data and some info as to how to represent that data:\ntm_shape(ak) +\n  tm_polygons()\n\n\n\nWe need to label and customize it\ntm_shape(ak) +\n  tm_polygons() +\n  tm_layout(\n    title = \"Glaciers of Alaska\",\n    title.position = c(\"center\", \"top\"),\n    title.size = 1.1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.06, 0.01, 0.09, 0.01),\n    outer.margins = 0,\n    frame.lwd = 0.2\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    size = 1.2,\n    text.size = 0.6\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 500, 1000),\n    position = c(\"right\", \"BOTTOM\")\n  )\n\n\n\nMake a map of the wes object\n\n\nYour turn:\n\nMake a map with the wes object you created with the data for Western North America excluding AK.\n\n\n\n\nNow, let’s make a map with ak and wes\nThe Coordinate Reference Systems (CRS) must be the same.\nsf has a function to retrieve the CRS of an sf object: st_crs.\nst_crs(ak) == st_crs(wes)\n[1] TRUE\nSo we’re good (we will see later what to do if this is not the case).\n\n\nOur combined map\nLet’s start again with a minimum map without any layout to test things out:\ntm_shape(ak) +\n  tm_polygons() +\n  tm_shape(wes) +\n  tm_polygons()\n\nUh … oh …\n\n\nWhat went wrong?\nMaps are bound by “bounding boxes”. In tmap, they are called bbox.\ntmap sets the bbox the first time tm_shape is called. In our case, the bbox was thus set to the bbox of the ak object.\nWe need to create a new bbox for our new map.\n\n\nRetrieving bounding boxes\nsf has a function to retrieve the bbox of an sf object: st_bbox\nThe bbox of ak is:\nst_bbox(ak)\nxmin         ymin       xmax         ymax\n-176.14247   52.05727   -126.85450   69.35167\n\n\nCombining bounding boxes\nbbox objects can’t be combined directly.\nHere is how we can create a new bbox encompassing both of our bboxes:\n\nfirst, we transform our bboxes to sfc objects with st_as_sfc,\nthen we combine those objects into a new sfc object with st_union,\nfinally, we retrieve the bbox of that object with st_bbox:\n\nnwa_bbox &lt;- st_bbox(\n  st_union(\n    st_as_sfc(st_bbox(wes)),\n    st_as_sfc(st_bbox(ak))\n  )\n)\n\n\nBack to our map\nWe can now use our new bounding box for the map of Western North America:\ntm_shape(ak, bbox = nwa_bbox) +\n  tm_polygons() +\n  tm_shape(wes) +\n  tm_polygons() +\n  tm_layout(\n    title = \"Glaciers of Western North America\",\n    title.position = c(\"center\", \"top\"),\n    title.size = 1.1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.06, 0.01, 0.09, 0.01),\n    outer.margins = 0,\n    frame.lwd = 0.2\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    size = 1.2,\n    text.size = 0.6\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 1000, 2000),\n    position = c(\"right\", \"BOTTOM\")\n  )\n\n\n\nLet’s add a basemap\nWe will use data from Natural Earth, a public domain map dataset.\nThere are much more fancy options, but they usually involve creating accounts (e.g. with Google) to access some API.\nIn addition, this dataset can be accessed direction from within R thanks to the rOpenSci packages:\n\nrnaturalearth: provides the functions,\nrnaturalearthdata: provides the data.\n\n\n\nCreate an sf object with states/provinces\nstates_all &lt;- ne_states(\n  country = c(\"canada\", \"united states of america\"),\n  returnclass = \"sf\"\n)\n\nne_ stands for “Natural Earth”.\n\n\n\nSelect relevant states/provinces\nstates &lt;- states_all %&gt;%\n  filter(name_en == \"Alaska\" |\n           name_en == \"British Columbia\" |\n           name_en == \"Yukon\" |\n           name_en == \"Northwest Territories\" |\n           name_en ==  \"Alberta\" |\n           name_en == \"California\" |\n           name_en == \"Washington\" |\n           name_en == \"Oregon\" |\n           name_en == \"Idaho\" |\n           name_en == \"Montana\" |\n           name_en == \"Wyoming\" |\n           name_en == \"Colorado\" |\n           name_en == \"Nevada\" |\n           name_en == \"Utah\"\n         )\n\n\nAdd the basemap to our map\n\nWhat do we need to make sure of first?\n\nst_crs(states) == st_crs(ak)\n[1] TRUE\nWe add the basemap as a 3rd layer.\nMind the order! If you put the basemap last, it will cover your data.\nOf course, we will use our nwa_bbox bounding box again.\nWe will also break tm_polygons into tm_borders and tm_fill for ak and wes in order to colourise them with slightly different colours:\ntm_shape(states, bbox = nwa_bbox) +\n  tm_polygons(col = \"#f2f2f2\", lwd = 0.2) +\n  tm_shape(ak) +\n  tm_borders(col = \"#3399ff\") +\n  tm_fill(col = \"#86baff\") +\n  tm_shape(wes) +\n  tm_borders(col = \"#3399ff\") +\n  tm_fill(col = \"#86baff\") +\n  tm_layout(\n    title = \"Glaciers of Western North America\",\n    title.position = c(\"center\", \"top\"),\n    title.size = 1.1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.06, 0.01, 0.09, 0.01),\n    outer.margins = 0,\n    frame.lwd = 0.2\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    size = 1.2,\n    text.size = 0.6\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 1000, 2000),\n    position = c(\"right\", \"BOTTOM\")\n  )\n\n\n\ntmap styles\ntmap has a number of styles that you can try.\nFor instance, to set the style to “classic”, run the following before making your map:\ntmap_style(\"classic\")\n\nOther options are:\n“white” (default), “gray”, “natural”, “cobalt”, “col_blind”, “albatross”, “beaver”, “bw”, and “watercolor”.\n\n\nTo return to the default, you need to run:\ntmap_style(\"white\")\nor:\ntmap_options_reset()\nwhich will reset every tmap option.",
    "crumbs": [
      "R",
      "<em><b>Webinars</b></em>",
      "Intro to GIS mapping in R"
    ]
  },
  {
    "objectID": "r/wb_gis_mapping.html#inset-maps",
    "href": "r/wb_gis_mapping.html#inset-maps",
    "title": "GIS mapping with R",
    "section": "Inset maps",
    "text": "Inset maps\nNow, how can we combine this with our gnp object?\nWe could add it as an inset of our Western North America map.\n\nFirst, let’s map it\nLet’s use the same tm_borders and tm_fill we just used:\ntm_shape(gnp) +\n  tm_borders(col = \"#3399ff\") +\n  tm_fill(col = \"#86baff\") +\n  tm_layout(\n    title = \"Glaciers of Glacier National Park\",\n    title.position = c(\"center\", \"top\"),\n    legend.title.color = \"#fcfcfc\",\n    legend.text.size = 1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.07, 0.03, 0.07, 0.03),\n    outer.margins = 0\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    text.size = 0.7\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 10, 20),\n    position = c(\"right\", \"BOTTOM\"),\n    text.size = 1\n  )\n\n\n\nCreate an inset map\nAs always, first we check that the CRS are the same:\nst_crs(gnp) == st_crs(ak)\n[1] FALSE\nAH!\n\n\nCRS transformation\nWe need to reproject gnp into the CRS of our other sf objects (e.g. ak):\ngnp &lt;- st_transform(gnp, st_crs(ak))\nWe can verify that the CRS are now the same:\nst_crs(gnp) == st_crs(ak)\n[1] TRUE\n\n\nInset maps: first step\nAdd a rectangle showing the location of the GNP map in the main North America map.\nWe need to create a new sfc object from the gnp bbox so that we can add it to our previous map as a new layer:\ngnp_zone &lt;- st_bbox(gnp) %&gt;%\n  st_as_sfc()\n\n\nInset maps: second step\nCreate a tmap object of the main map. Of course, we need to edit the title. Also, note the presence of our new layer:\nmain_map &lt;- tm_shape(states, bbox = nwa_bbox) +\n  tm_polygons(col = \"#f2f2f2\", lwd = 0.2) +\n  tm_shape(ak) +\n  tm_borders(col = \"#3399ff\") +\n  tm_fill(col = \"#86baff\") +\n  tm_shape(wes) +\n  tm_borders(col = \"#3399ff\") +\n  tm_fill(col = \"#86baff\") +\n  tm_shape(gnp_zone) +\n  tm_borders(lwd = 1.5, col = \"#ff9900\") +\n  tm_layout(\n    title = \"Glaciers of Glacier National Park\",\n    title.position = c(\"center\", \"top\"),\n    title.size = 1.1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.06, 0.01, 0.09, 0.01),\n    outer.margins = 0,\n    frame.lwd = 0.2\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    size = 1.2,\n    text.size = 0.6\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 500, 1000),\n    position = c(\"right\", \"BOTTOM\")\n  )\n\n\nInset maps: third step\nCreate a tmap object of the inset map.\nWe make sure to matching colours and edit the layouts for better readability:\ninset_map &lt;- tm_shape(gnp) +\n  tm_borders(col = \"#3399ff\") +\n  tm_fill(col = \"#86baff\") +\n  tm_layout(\n    legend.show = F,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.03, 0.03, 0.03, 0.03),\n    outer.margins = 0,\n    frame = \"#ff9900\",\n    frame.lwd = 3\n  )\n\n\nInset maps: final step\nCombine the two tmap objects.\nWe print the main map and add the inset map with grid::viewport:\nmain_map\nprint(inset_map, vp = viewport(0.41, 0.26, width = 0.5, height = 0.5))",
    "crumbs": [
      "R",
      "<em><b>Webinars</b></em>",
      "Intro to GIS mapping in R"
    ]
  },
  {
    "objectID": "r/wb_gis_mapping.html#mapping-a-subset-of-the-data",
    "href": "r/wb_gis_mapping.html#mapping-a-subset-of-the-data",
    "title": "GIS mapping with R",
    "section": "Mapping a subset of the data",
    "text": "Mapping a subset of the data\nTo see the retreat of the ice, we need to zoom in.\nLet’s focus on a single glacier: Agassiz Glacier.\n\nMap of the Agassiz Glacier\nSelect the data points corresponding to the Agassiz Glacier:\nag &lt;- gnp %&gt;% filter(glacname == \"Agassiz Glacier\")\ntm_shape(ag) +\n  tm_polygons() +\n  tm_layout(\n    title = \"Agassiz Glacier\",\n    title.position = c(\"center\", \"top\"),\n    legend.position = c(\"left\", \"bottom\"),\n    legend.title.color = \"#fcfcfc\",\n    legend.text.size = 1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.07, 0.03, 0.07, 0.03),\n    outer.margins = 0\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    text.size = 0.7\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 0.5, 1),\n    position = c(\"right\", \"BOTTOM\"),\n    text.size = 1\n  )\n\nNot great …\n\n\nMap based on attribute variables\ntm_shape(ag) +\n  tm_polygons(\"year\", palette = \"Blues\") +\n  tm_layout(\n    title = \"Agassiz Glacier\",\n    title.position = c(\"center\", \"top\"),\n    legend.position = c(\"left\", \"bottom\"),\n    legend.title.color = \"#fcfcfc\",\n    legend.text.size = 1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.07, 0.03, 0.07, 0.03),\n    outer.margins = 0\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    text.size = 0.7\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 0.5, 1),\n    position = c(\"right\", \"BOTTOM\"),\n    text.size = 1\n  )\n\n\n\nUsing ggplot2 instead of tmap\nAs an alternative to tmap, ggplot2 can plot maps with the geom_sf function:\nggplot(ag) +\n  geom_sf(aes(fill = year)) +\n  scale_fill_brewer(palette = \"Blues\") +\n  labs(title = \"Agassiz Glacier\") +\n  annotation_scale(location = \"bl\", width_hint = 0.4) +\n  annotation_north_arrow(location = \"tr\", which_north = \"true\",\n                         pad_x = unit(0.75, \"in\"), pad_y = unit(0.5, \"in\"),\n                         style = north_arrow_fancy_orienteering) +\n  theme_bw() +\n  theme(plot.title = element_text(hjust = 0.5))\nThe package ggspatial adds a lot of functionality to ggplot2 for spatial data.",
    "crumbs": [
      "R",
      "<em><b>Webinars</b></em>",
      "Intro to GIS mapping in R"
    ]
  },
  {
    "objectID": "r/wb_gis_mapping.html#faceted-maps",
    "href": "r/wb_gis_mapping.html#faceted-maps",
    "title": "GIS mapping with R",
    "section": "Faceted maps",
    "text": "Faceted maps\n\nFaceted map of the retreat of Agassiz\ntm_shape(ag) +\n  tm_polygons(col = \"#86baff\") +\n  tm_layout(\n    main.title = \"Agassiz Glacier\",\n    main.title.position = c(\"center\", \"top\"),\n    main.title.size = 1.2,\n    legend.position = c(\"left\", \"bottom\"),\n    legend.title.color = \"#fcfcfc\",\n    legend.text.size = 1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0, 0.03, 0, 0.03),\n    outer.margins = 0,\n    panel.label.bg.color = \"#fcfcfc\",\n    frame = F,\n    asp = 0.6\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    size = 1,\n    text.size = 0.6\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 0.5, 1),\n    position = c(\"right\", \"BOTTOM\"),\n    text.size = 0.6\n  ) +\n  tm_facets(\n    by = \"year\",\n    free.coords = F,\n    ncol = 4\n  )",
    "crumbs": [
      "R",
      "<em><b>Webinars</b></em>",
      "Intro to GIS mapping in R"
    ]
  },
  {
    "objectID": "r/wb_gis_mapping.html#animated-maps",
    "href": "r/wb_gis_mapping.html#animated-maps",
    "title": "GIS mapping with R",
    "section": "Animated maps",
    "text": "Animated maps\n\nAnimated map of the Retreat of Agassiz\nFirst, we need to create a tmap object with facets:\nagassiz_anim &lt;- tm_shape(ag) +\n  tm_polygons(col = \"#86baff\") +\n  tm_layout(\n    title = \"Agassiz Glacier\",\n    title.position = c(\"center\", \"top\"),\n    legend.position = c(\"left\", \"bottom\"),\n    legend.title.color = \"#fcfcfc\",\n    legend.text.size = 1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.08, 0, 0.08, 0),\n    outer.margins = 0,\n    panel.label.bg.color = \"#fcfcfc\"\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    text.size = 0.7\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 0.5, 1),\n    position = c(\"right\", \"BOTTOM\"),\n    text.size = 1\n  ) +\n  tm_facets(\n    along = \"year\",\n    free.coords = F\n  )\nThen we can pass that object to tmap_animation:\ntmap_animation(\n  agassiz_anim,\n  filename = \"ag.gif\",\n  dpi = 300,\n  inner.margins = c(0.08, 0, 0.08, 0),\n  delay = 100\n)\n\n\n\nMap of ice thickness of Agassiz\nNow, let’s map the estimated ice thickness on Agassiz Glacier.\nThis time, we use tm_raster:\ntm_shape(ras) +\n  tm_raster(title = \"\") +\n  tm_layout(\n    title = \"Ice thickness (m) of Agassiz Glacier\",\n    title.position = c(\"center\", \"top\"),\n    legend.position = c(\"left\", \"bottom\"),\n    legend.bg.color = \"#ffffff\",\n    legend.text.size = 1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.07, 0.03, 0.07, 0.03),\n    outer.margins = 0\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    text.size = 0.7\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 0.5, 1),\n    position = c(\"right\", \"BOTTOM\"),\n    text.size = 1\n  )\n\n\n\nCombining with Randolph data\nAs always, we check whether the CRS are the same:\nst_crs(ag) == st_crs(ras)\n[1] FALSE\nWe need to reproject ag (remember that it is best to avoid reprojecting raster data):\nag %&lt;&gt;% st_transform(st_crs(ras))\nThe retreat and ice thickness layers will hide each other (the order matters!). One option is to use tm_borders for one of them, but we can also use transparency (alpha). We also adjust the legend:\ntm_shape(ras) +\n  tm_raster(title = \"Ice (m)\") +\n  tm_shape(ag) +\n  tm_polygons(\"year\", palette = \"Blues\", alpha = 0.2, title = \"Contour\") +\n  tm_layout(\n    title = \"Ice thickness (m) and retreat of Agassiz Glacier\",\n    title.position = c(\"center\", \"top\"),\n    legend.position = c(\"left\", \"bottom\"),\n    legend.bg.color = \"#ffffff\",\n    legend.text.size = 0.7,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.07, 0.03, 0.07, 0.03),\n    outer.margins = 0\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    text.size = 0.7\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 0.5, 1),\n    position = c(\"right\", \"BOTTOM\"),\n    text.size = 1\n  )\n\n\n\nRefining raster maps\nLet’s go back to our ice thickness map:\n\nWe can change the palette to blue with tm_raster(palette = \"Blues\"):\n\n\nWe can create a more suitable interval scale\nFirst, let’s see what the maximum value is:\nglobal(ras, \"max\")\nmax\nRGI60-02.16664_thickness 70.10873\nThen we can set the breaks with tm_raster(breaks = seq(0, 80, 5))\nWe also need to tweak the layout, legend, etc.:\ntm_shape(ras) +\n  tm_raster(title = \"\", palette = \"Blues\", breaks = seq(0, 80, 5)) +\n  tm_layout(\n    title = \"Ice thickness (m) of Agassiz Glacier\",\n    title.position = c(\"center\", \"top\"),\n    legend.position = c(\"left\", \"bottom\"),\n    legend.bg.color = \"#ffffff\",\n    legend.text.size = 0.7,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.07, 0.03, 0.07, 0.03),\n    outer.margins = 0\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    text.size = 0.7\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 0.5, 1),\n    position = c(\"right\", \"BOTTOM\"),\n    text.size = 1\n  )\n\nOr we can use a continuous colour scheme with tm_raster(style = \"cont\"):",
    "crumbs": [
      "R",
      "<em><b>Webinars</b></em>",
      "Intro to GIS mapping in R"
    ]
  },
  {
    "objectID": "r/wb_gis_mapping.html#basemaps",
    "href": "r/wb_gis_mapping.html#basemaps",
    "title": "GIS mapping with R",
    "section": "Basemaps",
    "text": "Basemaps\n\nBasemap with ggmap\nbasemap &lt;- get_map(\n  bbox = c(\n    left = st_bbox(ag)[1],\n    bottom = st_bbox(ag)[2],\n    right = st_bbox(ag)[3],\n    top = st_bbox(ag)[4]\n  ),\n  source = \"osm\"\n)\n\nggmap is a powerful package, but Google now requires an API key obtained through registration\n\n\n\nBasemap with basemaps\nThe package basemaps allows to download open source basemap data from several sources, but those cannot easily be combined with sf objects\nThis plots a satellite image of the Agassiz Glacier:\nbasemap_plot(ag, map_service = \"esri\", map_type = \"world_imagery\")\n\n\nSatellite image of the Agassiz Glacier",
    "crumbs": [
      "R",
      "<em><b>Webinars</b></em>",
      "Intro to GIS mapping in R"
    ]
  },
  {
    "objectID": "r/wb_gis_mapping.html#tiled-web-maps-with-leaflet-js",
    "href": "r/wb_gis_mapping.html#tiled-web-maps-with-leaflet-js",
    "title": "GIS mapping with R",
    "section": "Tiled web maps with Leaflet JS",
    "text": "Tiled web maps with Leaflet JS\n\nmapview\nmapview(gnp)\n\n\n\nCartoDB.Positron\n\n\n\n\n\nOpenStreetMap\n\n\n\n\n\nOpenTopoMap\n\n\n\n\n\nEsri.WorldImagery\n\n\n\n\ntmap\nSo far, we have used the plot mode of tmap. There is also a view mode which allows interactive viewing in a browser through Leaflet\nChange to view mode:\ntmap_mode(\"view\")\n\nYou can also toggle between modes with ttm\n\nRe-plot the last map we plotted with tmap:\ntmap_last()\n\n\nleaflet\nleaflet creates a map widget to which you add layers\nmap &lt;- leaflet()\naddTiles(map)",
    "crumbs": [
      "R",
      "<em><b>Webinars</b></em>",
      "Intro to GIS mapping in R"
    ]
  },
  {
    "objectID": "r/wb_gis_mapping.html#spatial-data-analysis",
    "href": "r/wb_gis_mapping.html#spatial-data-analysis",
    "title": "GIS mapping with R",
    "section": "Spatial data analysis",
    "text": "Spatial data analysis\n\nResources\nHere are some resources on the topic to get started.\n\nR companion to Geographic Information Analysis\nSpatial data analysis",
    "crumbs": [
      "R",
      "<em><b>Webinars</b></em>",
      "Intro to GIS mapping in R"
    ]
  },
  {
    "objectID": "r/wb_gis_mapping.html#image-credits",
    "href": "r/wb_gis_mapping.html#image-credits",
    "title": "GIS mapping with R",
    "section": "Image credits",
    "text": "Image credits\nSzűcs Róbert, Grasshopper Geography",
    "crumbs": [
      "R",
      "<em><b>Webinars</b></em>",
      "Intro to GIS mapping in R"
    ]
  },
  {
    "objectID": "r/wb_gis_mapping.html#footnotes",
    "href": "r/wb_gis_mapping.html#footnotes",
    "title": "GIS mapping with R",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nBivand, R.S. Progress in the R ecosystem for representing and handling spatial data. J Geogr Syst (2020). https://doi.org/10.1007/s10109-020-00336-0.↩︎\nRGI Consortium (2017). Randolph Glacier Inventory – A Dataset of Global Glacier Outlines: Version 6.0: Technical Report, Global Land Ice Measurements from Space, Colorado, USA. Digital Media. DOI: https://doi.org/10.7265/N5-RGI-60.↩︎\nFagre, D.B., McKeon, L.A., Dick, K.A. and Fountain, A.G., 2017, Glacier margin time series (1966, 1998, 2005, 2015) of the named glaciers of Glacier National Park, MT, USA: U.S. Geological Survey data release. DOI: https://doi.org/10.5066/F7P26WB1.↩︎\nFarinotti, Daniel, 2019, A consensus estimate for the ice thickness distribution of all glaciers on Earth - dataset, Zurich. ETH Zurich. DOI: https://doi.org/10.3929/ethz-b-000315707.↩︎",
    "crumbs": [
      "R",
      "<em><b>Webinars</b></em>",
      "Intro to GIS mapping in R"
    ]
  },
  {
    "objectID": "r/wb_hpc.html",
    "href": "r/wb_hpc.html",
    "title": "High-performance research computing in R",
    "section": "",
    "text": "R is not famous for its speed. With code optimization and parallelization, it can however be used for heavy computations.\nThis webinar will introduce you to working with R from the command line on the Alliance clusters with a focus on performance. We will discuss code benchmarking and various parallelization techniques.\n\nSlides (Click and wait: this reveal.js presentation may take a little time to load.)",
    "crumbs": [
      "R",
      "<em><b>Webinars</b></em>",
      "High-performance computing"
    ]
  },
  {
    "objectID": "r/wb_package.html",
    "href": "r/wb_package.html",
    "title": "Creating R packages",
    "section": "",
    "text": "Coming up in spring 2024."
  },
  {
    "objectID": "r/ws_demo.html",
    "href": "r/ws_demo.html",
    "title": "A little demo of programming in R",
    "section": "",
    "text": "R is a free and open-source programming language with a large collection of packages for statistical computing, modeling, and graphics. It is extremely popular in several academic fields including statistics, biology, economics, data mining, data analysis, and linguistics.\nThis high-level presentation will give you a sense of what R can be used for through a series of examples (data visualization, web scraping, and GIS). I will also talk about the strengths and weaknesses of R and who would benefit most from learning it.\n\nSlides (Click and wait: the presentation might take a few instants to load)",
    "crumbs": [
      "R",
      "<b><em>Workshops</em></b>",
      "A little demo of R programming"
    ]
  },
  {
    "objectID": "r/ws_gis_intro.html",
    "href": "r/ws_gis_intro.html",
    "title": "Introduction to GIS with R",
    "section": "",
    "text": "This workshop is an introduction to GIS in R. We will learn how to import GIS data, explore it, and map it.\nIn particular, we will create maps (inset maps, faceted maps, animated maps, interactive maps, and raster maps), thanks to the packages sf, tmap, raster, leaflet, ggplot2, grid (part of Base R), and mapview.\nWe will also learn how to add basemaps from OpenStreetMap and Google Maps.",
    "crumbs": [
      "R",
      "<b><em>Workshops</em></b>",
      "Intro to GIS mapping with R"
    ]
  },
  {
    "objectID": "r/ws_gis_intro.html#getting-the-data",
    "href": "r/ws_gis_intro.html#getting-the-data",
    "title": "Introduction to GIS with R",
    "section": "Getting the data",
    "text": "Getting the data\n\nDatasets\nFor this webinar, we will use:\n\nthe Alaska as well as the Western Canada and USA subsets of the Randolph Glacier Inventory version 6.01\nthe USGS time series of the named glaciers of Glacier National Park2 The datasets can be downloaded as zip files from these websites.\n\n\n\nBasemaps\nFor our basemaps, we will use data from:\n\nNatural Earth: this dataset can be accessed direction from within R thanks to the packages rnaturalearth (which provides the functions) and rnaturalearthdata (which provides the data)",
    "crumbs": [
      "R",
      "<b><em>Workshops</em></b>",
      "Intro to GIS mapping with R"
    ]
  },
  {
    "objectID": "r/ws_gis_intro.html#loading-and-exploring-data",
    "href": "r/ws_gis_intro.html#loading-and-exploring-data",
    "title": "Introduction to GIS with R",
    "section": "Loading and exploring data",
    "text": "Loading and exploring data\nFirst, let’s load the necessary packages for this webinar:\nlibrary(sf)\nlibrary(tmap)\nlibrary(dplyr)\nlibrary(magrittr)\nlibrary(purrr)\nlibrary(rnaturalearth)\nlibrary(rnaturalearthdata)\nlibrary(mapview)\nlibrary(grid) # part of base R (already installed), but needs to be explicitly loaded\nWe will start by mapping all the glaciers of Western North America thanks to:\n\nthe Alaska subset of the Randolph Glacier Inventory\nthe Western Canada and USA subset of the Randolph Glacier Inventory\n\nDownload and unzip 02_rgi60_WesternCanadaUS and 01_rgi60_Alaska from the Randolph Glacier Inventory version 6.0.\nData get imported and turned into sf objects by the function sf::st_read():\nak &lt;- st_read(\"01_rgi60_Alaska\")\nwes &lt;- st_read(\"02_rgi60_WesternCanadaUS\")\n\nMake sure to use the absolute paths or the proper paths relative to your working directory (which can be obtained with getwd() and modified with setwd()).\n\nYou can print and explore your new objects:\nak\nwes\n\nstr(ak)\nstr(wes)\nsf objects are data.frame-like objects with a geometry list-column as their last column. That column is itself an object of class sfc (simple feature geometry list column).",
    "crumbs": [
      "R",
      "<b><em>Workshops</em></b>",
      "Intro to GIS mapping with R"
    ]
  },
  {
    "objectID": "r/ws_gis_intro.html#mapping-with-tmap",
    "href": "r/ws_gis_intro.html#mapping-with-tmap",
    "title": "Introduction to GIS with R",
    "section": "Mapping with tmap",
    "text": "Mapping with tmap\ntmap follows a grammar of graphic similar to that of ggplot2: you first need to set a shape (a spatial data object) by passing an sf object to tm_shape(). Then you plot one or several layers with one of several tmap functions and you use the + sign between each element.\nTo see the available options, run:\n?tmap-element\nWe could thus plot the glaciers of Alaska with any of the options below:\ntm_shape(ak) +\n  tm_borders()\n\ntm_shape(ak) +\n  tm_fill()\n\ntm_shape(ak) +\n  tm_polygons()      # shows both borders and fill\nHere, we will use tm_polygons() which combines tm_borders() and tm_fill().",
    "crumbs": [
      "R",
      "<b><em>Workshops</em></b>",
      "Intro to GIS mapping with R"
    ]
  },
  {
    "objectID": "r/ws_gis_intro.html#layout-elements-and-attribute-layers",
    "href": "r/ws_gis_intro.html#layout-elements-and-attribute-layers",
    "title": "Introduction to GIS with R",
    "section": "Layout elements and attribute layers",
    "text": "Layout elements and attribute layers\nA map without title, compass, or scale bars is not very useful though. We need to add layout elements and attribute layers to the map.\nYou can loop up the many arguments of the tmap functions in the help pages to see how you can customize your maps:\n?tm_layout\n?tm_compass\n?tm_scale_bar\nLet’s now map the glaciers of Alaska:\ntm_shape(ak) +\n  tm_polygons() +\n  tm_layout(\n    title = \"Glaciers of Alaska\",\n    title.position = c(\"center\", \"top\"),\n    title.size = 1.1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.06, 0.01, 0.09, 0.01),\n    outer.margins = 0,\n    frame.lwd = 0.2\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    size = 1.2,\n    text.size = 0.6\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 500, 1000),\n    position = c(\"right\", \"BOTTOM\")\n  )",
    "crumbs": [
      "R",
      "<b><em>Workshops</em></b>",
      "Intro to GIS mapping with R"
    ]
  },
  {
    "objectID": "r/ws_gis_intro.html#union-of-bounding-boxes",
    "href": "r/ws_gis_intro.html#union-of-bounding-boxes",
    "title": "Introduction to GIS with R",
    "section": "Union of bounding boxes",
    "text": "Union of bounding boxes\nNow, if we want to plot all the glaciers of Western North America, we want to combine both sf objects in the same map. A map can contain multiple shapes: you only need to “add” a tm_shape and its element(s). Before doing so however, it is very important to ensure that they have the same coordinate reference system (CRS):\nst_crs(ak)\nst_crs(wes)\n\nst_crs(ak) == st_crs(wes)\nThey do, so we are good to go.\n\nAs with ggplot2 or GIS graphical user interfaces, the order matters since the layers stack up on top of each other.\n\ntm_shape(ak) +\n  tm_polygons() +\n  tm_shape(wes) +\n  tm_polygons()\nIf you run the code above however, you may be surprised that you are still only plotting the map of Alaska.\nThis is because each map comes with a spatial bounding box (bbox).\nst_bbox(ak)\nst_bbox(wes)\nIn the code above, the bbox is set by the first shape, i.e. our entire map uses the bbox of the Alaska sf object.\nWe first need to create a new bounding box encompassing both bounding boxes:\nnwa_bbox &lt;- st_bbox(\n  st_union(\n    st_as_sfc(st_bbox(wes)),\n    st_as_sfc(st_bbox(ak))\n  )\n)\nWe can now plot the glaciers of Western North America:\ntm_shape(ak, bbox = nwa_bbox) +\n  tm_polygons() +\n  tm_shape(wes) +\n  tm_polygons() +\n  tm_layout(\n    title = \"Glaciers of Western North America\",\n    title.position = c(\"center\", \"top\"),\n    title.size = 1.1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.06, 0.01, 0.09, 0.01),\n    outer.margins = 0,\n    frame.lwd = 0.2\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    size = 1.2,\n    text.size = 0.6\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 1000, 2000),\n    position = c(\"right\", \"BOTTOM\")\n  )",
    "crumbs": [
      "R",
      "<b><em>Workshops</em></b>",
      "Intro to GIS mapping with R"
    ]
  },
  {
    "objectID": "r/ws_gis_intro.html#maps-based-on-an-attribute-variable",
    "href": "r/ws_gis_intro.html#maps-based-on-an-attribute-variable",
    "title": "Introduction to GIS with R",
    "section": "Maps based on an attribute variable",
    "text": "Maps based on an attribute variable\nWhat is interesting about glacier maps is to see their evolution through time as glaciers retreat due to climate change. While the Randolph Glacier Inventory (RGI) has an amazing map in terms of spacial coverage, it doesn’t yet have much temporal data.\nTo look at glacier retreat, we will look at the USGS time series of the named glaciers of Glacier National Park3. These 4 datasets have the contour lines of 39 glaciers for the years 1966, 1998, 2005, and 2015.\nWe could load and clean these datasets one by one. Copying and pasting code however is inefficient and error-prone. A better approach is to do this in a functional programming framework: create a function which does all the data loading and cleaning, then pass each element of a vector of the paths of all 4 datasets to it using purrr::map().\n“Cleaning” here consists of selecting the variables we are interested in, putting them in the same order in each dataset (they were not initially) and giving the exact same name across all datasets (there were case inconsistencies between datasets and R is case sensitive).\n# create a function that reads and cleans the data\nprep &lt;- function(dir) {\n  g &lt;- st_read(dir)\n  g %&lt;&gt;% rename_with(~ tolower(gsub(\"Area....\", \"area\", .x)))\n  g %&lt;&gt;% select(\n    year,\n    objectid,\n    glacname,\n    area,\n    shape_leng,\n    x_coord,\n    y_coord,\n    source_sca,\n    source\n  )\n}\n\n# create a vector of dataset names\ndirs &lt;- grep(\"GNPglaciers_.*\", list.dirs(), value = T)\n\n# pass each element of that vector through prep() thanks to map()\ngnp &lt;- map(dirs, prep)\nmap() returns a list, so we now have a list (gnp) of 4 elements: the 4 sf objects containing our cleaned datasets. A list is not really convenient and we will turn it into a single sf object.\nBefore doing so however, we want to make sure that they all have the same CRS:\nst_crs(gnp[[1]]) == st_crs(gnp[[2]])\nst_crs(gnp[[1]]) == st_crs(gnp[[3]])\nst_crs(gnp[[1]]) == st_crs(gnp[[4]])\nThey do, so we can turn gnp into a single sf object:\ngnp &lt;- do.call(\"rbind\", gnp)\n\ngnp\nstr(gnp)\nWe can now map the data:\ntm_shape(gnp) +\n  tm_polygons(\"year\", palette = \"Blues\") +\n  tm_layout(\n    title = \"Glaciers of Glacier National Park\",\n    title.position = c(\"center\", \"top\"),\n    legend.title.color = \"#fcfcfc\",\n    legend.text.size = 1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.07, 0.03, 0.07, 0.03),\n    outer.margins = 0\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    text.size = 0.7\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 10, 20),\n    position = c(\"right\", \"BOTTOM\"),\n    text.size = 1\n  )\n\nI didn’t want to show the legend title and because there is no option to remove it, I set its color to that of the background.",
    "crumbs": [
      "R",
      "<b><em>Workshops</em></b>",
      "Intro to GIS mapping with R"
    ]
  },
  {
    "objectID": "r/ws_gis_intro.html#crs-transformation",
    "href": "r/ws_gis_intro.html#crs-transformation",
    "title": "Introduction to GIS with R",
    "section": "CRS transformation",
    "text": "CRS transformation\nWouldn’t it be nice to have this map as an inset of the previous map so that we can situate it within North America?\nBefore we can do this, we need to make sure that both maps use the same CRS:\nst_crs(ak)\nst_crs(gnp)\n\nWe could use wes instead of ak since we know that both sf objects have the same CRS.\n\nThey don’t have the same CRS, so we reproject gnp by transforming its data from its current CRS to that of ak.\ngnp &lt;- st_transform(gnp, st_crs(ak))\nst_crs(gnp)",
    "crumbs": [
      "R",
      "<b><em>Workshops</em></b>",
      "Intro to GIS mapping with R"
    ]
  },
  {
    "objectID": "r/ws_gis_intro.html#inset-map",
    "href": "r/ws_gis_intro.html#inset-map",
    "title": "Introduction to GIS with R",
    "section": "Inset map",
    "text": "Inset map\nNow we can create our map with an inset: the map of the Western North America glaciers (from the sf object nwa) will be our main map and the map of Glacier National Park (from the sf object gnp) will be the inset.\nIf the goal of this new map is to show the location of the gnp map within the nwa one, we need to add a rectangle showing the bounding box of gnp in the nwa map as a new layer.\nFor this, we create a new sfc_POLYGON from the bounding box of gnp:\ngnp_zone &lt;- st_bbox(gnp) %&gt;%\n  st_as_sfc()\nWe will use it as the following layer within the new map:\ntm_shape(gnp_zone) +\n  tm_borders(lwd = 1.5, col = \"#ff9900\")\nWe assign our new map (with an updated suitable title) to the object main_map:\nmain_map &lt;- tm_shape(ak, bbox = nwa_bbox) +\n  tm_polygons() +\n  tm_shape(wes) +\n  tm_polygons() +\n  tm_shape(gnp_zone) +\n  tm_borders(lwd = 1.5, col = \"#ff9900\") +\n  tm_layout(\n    title = \"Glaciers of Glacier National Park\",\n    title.position = c(\"center\", \"top\"),\n    title.size = 1.1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.06, 0.01, 0.09, 0.01),\n    outer.margins = 0,\n    frame.lwd = 0.2\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    size = 1.2,\n    text.size = 0.6\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 500, 1000),\n    position = c(\"right\", \"BOTTOM\")\n  )\nNext, we will change the frame of the gnp inset to match the color of this new rectangle (to make it visually clear that this is a close-up view of that rectangle). We can also remove the title, compass and scale bar since this is an inset within a map which already have them. We assign this new map to the object inset_map:\ninset_map &lt;- tm_shape(gnp) +\n  tm_polygons(\"year\", palette = \"Blues\") +\n  tm_layout(\n    legend.title.color = \"#fcfcfc\",\n    legend.text.size = 0.7,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.03, 0.03, 0.03, 0.03),\n    outer.margins = 0,\n    frame = \"#ff9900\",\n    frame.lwd = 3\n  )\nFinally, we combine the two maps with grid::viewport():\nmain_map\nprint(inset_map, vp = viewport(0.41, 0.26, width = 0.5, height = 0.5))",
    "crumbs": [
      "R",
      "<b><em>Workshops</em></b>",
      "Intro to GIS mapping with R"
    ]
  },
  {
    "objectID": "r/ws_gis_intro.html#tiled-web-maps-with-leaflet",
    "href": "r/ws_gis_intro.html#tiled-web-maps-with-leaflet",
    "title": "Introduction to GIS with R",
    "section": "Tiled web maps with Leaflet",
    "text": "Tiled web maps with Leaflet\nTiled web maps are interactive maps in a browser using web servers such as Google Maps or OpenStreetMap. Several packages allow to use Leaflet (an open-source JavaScript library for interactive maps) to create tile maps.\n\nWith mapview\nThe simplest option is to use mapview::mapview():\nmapview(gnp)\nThis will open a page in your browser in which you can pan, zoom, select/deselect data layers, and choose from a number of basemap layer options:\n CartoDB.Positron\n OpenTopoMap\n OpenStreetMap\n Esri.WorldImagery\n\n\nWith tmap\ntmap has similar capabilities.\nThe package has 2 modes:\n\nplot is the default mode for static maps that we used earlier.\nview is an interactive viewing mode using Leaflet in a browser. There, as with mapview, you can zoom in/out, select/deselect the different layers, and choose to display one of Esri.WorldGrayCanvas, OpenStreetMap, or Esri.WorldTopoMap basemaps.\n\nYou can toggle between the plot and view modes with ttm(), after which you can re-plot your last plot in the new mode with tmap_last(). You can also do both of these at once with ttmp().\nAlternatively, you can switch to either mode with tmap_mode(\"view\") and tmap_mode(\"plot\").\n\nExample:\n\nEarlier, we plotted all the glaciers of Western North America using tmap:\n\nAfter displaying this map, we could have run:\ntmap_mode(\"view\")\ntmap_last()\nAnd Leaflet would have open the following interactive map in our browser:\n\n\nAfterwards, if you want to create new static plots, don’t forget to get back to plot mode with tmap_mode(\"plot\").",
    "crumbs": [
      "R",
      "<b><em>Workshops</em></b>",
      "Intro to GIS mapping with R"
    ]
  },
  {
    "objectID": "r/ws_gis_intro.html#mapping-a-subset-of-the-data",
    "href": "r/ws_gis_intro.html#mapping-a-subset-of-the-data",
    "title": "Introduction to GIS with R",
    "section": "Mapping a subset of the data",
    "text": "Mapping a subset of the data\nEach glacier has 4 borders: one for each year of survey. They are however quite hard to see on such a large map.\nLet’s zoom on the Agassiz glacier:\n# select the data points corresponding to the Agassiz Glacier\nag &lt;- g %&gt;% filter(glacname == \"Agassiz Glacier\")\nAnd map it:\ntm_shape(ag) +\n  tm_polygons(\"year\", palette = \"Blues\") +\n  tm_layout(\n    title = \"Agassiz Glacier\",\n    title.position = c(\"center\", \"top\"),\n    legend.position = c(\"left\", \"bottom\"),\n    legend.title.color = \"#fcfcfc\",\n    legend.text.size = 1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.07, 0.03, 0.07, 0.03),\n    outer.margins = 0\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    text.size = 0.7\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 0.5, 1),\n    position = c(\"right\", \"BOTTOM\"),\n    text.size = 1\n  )\n\nNow we can clearly see the retreat of the Agassiz Glacier between 1966 and 2015.",
    "crumbs": [
      "R",
      "<b><em>Workshops</em></b>",
      "Intro to GIS mapping with R"
    ]
  },
  {
    "objectID": "r/ws_gis_intro.html#faceted-map",
    "href": "r/ws_gis_intro.html#faceted-map",
    "title": "Introduction to GIS with R",
    "section": "Faceted map",
    "text": "Faceted map\nInstead of having all temporal data in a single map however, it can be split across facets:\ntm_shape(ag) +\n  tm_polygons(col = \"#86baff\") +\n  tm_layout(\n    main.title = \"Agassiz Glacier\",\n    main.title.position = c(\"center\", \"top\"),\n    main.title.size = 1.2,\n    legend.position = c(\"left\", \"bottom\"),\n    legend.title.color = \"#fcfcfc\",\n    legend.text.size = 1,\n    bg.color = \"#fcfcfc\",\n    # inner.margins = c(0, 0.03, 0, 0.03),\n    outer.margins = 0,\n    panel.label.bg.color = \"#fcfcfc\",\n    frame = F,\n    asp = 0.6\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    size = 1,\n    text.size = 0.6\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 0.5, 1),\n    position = c(\"right\", \"BOTTOM\"),\n    text.size = 0.6\n  ) +\n  tm_facets(\n    by = \"year\",\n    free.coords = F,\n    ncol = 4\n  )",
    "crumbs": [
      "R",
      "<b><em>Workshops</em></b>",
      "Intro to GIS mapping with R"
    ]
  },
  {
    "objectID": "r/ws_gis_intro.html#animated-map",
    "href": "r/ws_gis_intro.html#animated-map",
    "title": "Introduction to GIS with R",
    "section": "Animated map",
    "text": "Animated map\nThe temporal data of the Agassiz Glacier retreat can also be conveyed through an animation:\nagassiz_anim &lt;- tm_shape(ag) +\n  tm_borders() +\n  tm_fill(col = \"#86baff\") +\n  tm_layout(\n    title = \"Agassiz Glacier\",\n    title.position = c(\"center\", \"top\"),\n    legend.position = c(\"left\", \"bottom\"),\n    legend.title.color = \"#fcfcfc\",\n    legend.text.size = 1,\n    bg.color = \"#fcfcfc\",\n    inner.margins = c(0.08, 0, 0.08, 0),\n    outer.margins = 0\n  ) +\n  tm_compass(\n    type = \"arrow\",\n    position = c(\"right\", \"top\"),\n    text.size = 0.7\n  ) +\n  tm_scale_bar(\n    breaks = c(0, 0.5, 1),\n    position = c(\"right\", \"BOTTOM\"),\n    text.size = 1\n  ) +\n  tm_facets(\n    along = \"year\",\n    free.coords = F\n  )\n\ntmap_animation(\n  agassiz_anim,\n  filename = \"ag.gif\",\n  dpi = 300,\n  inner.margins = c(0.08, 0, 0.08, 0),\n  delay = 100\n)",
    "crumbs": [
      "R",
      "<b><em>Workshops</em></b>",
      "Intro to GIS mapping with R"
    ]
  },
  {
    "objectID": "r/ws_gis_intro.html#additional-resources",
    "href": "r/ws_gis_intro.html#additional-resources",
    "title": "Introduction to GIS with R",
    "section": "Additional resources",
    "text": "Additional resources\nOpen GIS data:\nFree GIS Data: list of free GIS datasets\nBooks\nGeocomputation with R by Robin Lovelace, Jakub Nowosad, and Jannes Muenchow\nSpatial Data Science by Edzer Pebesma, Roger Bivand\nSpatial Data Science with R by Robert J. Hijmans\nUsing Spatial Data with R by Claudia A. Engel\nTutorial\nAn Introduction to Spatial Data Analysis and Visualisation in R by the CDRC\nWebsite\nr-spatial by Edzer Pebesma, Marius Appel, and Daniel Nüst\nCRAN package list\nAnalysis of Spatial Data\nMailing list\nR Special Interest Group on using Geographical data and Mapping",
    "crumbs": [
      "R",
      "<b><em>Workshops</em></b>",
      "Intro to GIS mapping with R"
    ]
  },
  {
    "objectID": "r/ws_gis_intro.html#footnotes",
    "href": "r/ws_gis_intro.html#footnotes",
    "title": "Introduction to GIS with R",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nRGI Consortium (2017). Randolph Glacier Inventory – A Dataset of Global Glacier Outlines: Version 6.0: Technical Report, Global Land Ice Measurements from Space, Colorado, USA. Digital Media. DOI: https://doi.org/10.7265/N5-RGI-60.↩︎\nFagre, D.B., McKeon, L.A., Dick, K.A., and Fountain, A.G., 2017, Glacier margin time series (1966, 1998, 2005, 2015) of the named glaciers of Glacier National Park, MT, USA: U.S. Geological Survey data release, https://doi.org/10.5066/F7P26WB1.↩︎\nFagre, D.B., McKeon, L.A., Dick, K.A., and Fountain, A.G., 2017, Glacier margin time series (1966, 1998, 2005, 2015) of the named glaciers of Glacier National Park, MT, USA: U.S. Geological Survey data release, https://doi.org/10.5066/F7P26WB1.↩︎",
    "crumbs": [
      "R",
      "<b><em>Workshops</em></b>",
      "Intro to GIS mapping with R"
    ]
  },
  {
    "objectID": "r/ws_webscraping.html",
    "href": "r/ws_webscraping.html",
    "title": "Web scraping with R",
    "section": "",
    "text": "The internet is a trove of information. A lot of it is publicly available and thus suitable for use in research. Extracting that information and putting it in an organized format for analysis can, however, be extremely tedious. Web scraping tools allow to automate parts of that process and R is a popular language for the task.\nIn this workshop, we will guide you through a simple example using the package rvest.",
    "crumbs": [
      "R",
      "<b><em>Workshops</em></b>",
      "Web scraping with rvest"
    ]
  },
  {
    "objectID": "r/ws_webscraping.html#html-and-css",
    "href": "r/ws_webscraping.html#html-and-css",
    "title": "Web scraping with R",
    "section": "HTML and CSS",
    "text": "HTML and CSS\nHyperText Markup Language (HTML) is the standard markup language for websites: it encodes the information related to the formatting and structure of webpages. Additionally, some of the customization can be stored in Cascading Style Sheets (CSS) files.\nHTML uses tags of the form:\n&lt;some_tag&gt;Your content&lt;/some_tag&gt;\nSome tags have attributes:\n&lt;some_tag attribute_name=\"attribute value\"&gt;Your content&lt;/some_tag&gt;\n\nExamples:\n\nSite structure:\n\n&lt;h2&gt;This is a heading of level 2&lt;/h2&gt;\n&lt;p&gt;This is a paragraph&lt;/p&gt;\n\nFormatting:\n\n&lt;b&gt;This is bold&lt;/b&gt;\n&lt;a href=\"https://some.url\"&gt;This is the text for a link&lt;/a&gt;",
    "crumbs": [
      "R",
      "<b><em>Workshops</em></b>",
      "Web scraping with rvest"
    ]
  },
  {
    "objectID": "r/ws_webscraping.html#web-scrapping",
    "href": "r/ws_webscraping.html#web-scrapping",
    "title": "Web scraping with R",
    "section": "Web scrapping",
    "text": "Web scrapping\nWeb scraping is a general term for a set of tools which allow for the extraction of data from the web automatically.\nWhile most of the data on the internet is publicly available, it is illegal to scrape some sites and you should always look into the policy of a site before attempting to scrape it. Some sites will also block you if you submit too many requests in a short amount of time, so if you plan on scraping sites at a fairly large scale, you should look into the polite package which will help you scrape responsibly.",
    "crumbs": [
      "R",
      "<b><em>Workshops</em></b>",
      "Web scraping with rvest"
    ]
  },
  {
    "objectID": "r/ws_webscraping.html#example-for-this-workshop",
    "href": "r/ws_webscraping.html#example-for-this-workshop",
    "title": "Web scraping with R",
    "section": "Example for this workshop",
    "text": "Example for this workshop\nWe will use a website from the University of Tennessee containing a database of PhD theses from that university.\nOur goal is to scrape data from this site to produce a dataframe with the date, major, and advisor for each dissertation.\n\nWe will only do this for the first page which contains the links to the 100 most recent theses. If you really wanted to gather all the data, you would have to do this for all pages.",
    "crumbs": [
      "R",
      "<b><em>Workshops</em></b>",
      "Web scraping with rvest"
    ]
  },
  {
    "objectID": "r/ws_webscraping.html#lets-look-at-the-sites",
    "href": "r/ws_webscraping.html#lets-look-at-the-sites",
    "title": "Web scraping with R",
    "section": "Let’s look at the sites",
    "text": "Let’s look at the sites\nFirst of all, let’s have a close look at the websites we want to scrape to think carefully about what we want to do. Before starting to write code, it is always a good idea to think about what you are trying to achieve with your code.\nTo create a dataframe with the data for all the dissertations on that first page, we need to do two things:\n\nStep 1: from the dissertations database first page, we want to scrape the list of URLs for the dissertation pages.\nStep 2: once we have the URLs, we want to scrape those pages too to get the date, major, and advisor for each dissertation.",
    "crumbs": [
      "R",
      "<b><em>Workshops</em></b>",
      "Web scraping with rvest"
    ]
  },
  {
    "objectID": "r/ws_webscraping.html#package",
    "href": "r/ws_webscraping.html#package",
    "title": "Web scraping with R",
    "section": "Package",
    "text": "Package\nTo do all this, we will use the package rvest, part of the tidyverse (a modern set of R packages). It is a package influenced by the popular Python package Beautiful Soup and it makes scraping websites with R really easy.\nLet’s load it:\n\nlibrary(rvest)",
    "crumbs": [
      "R",
      "<b><em>Workshops</em></b>",
      "Web scraping with rvest"
    ]
  },
  {
    "objectID": "r/ws_webscraping.html#read-in-html-from-main-site",
    "href": "r/ws_webscraping.html#read-in-html-from-main-site",
    "title": "Web scraping with R",
    "section": "Read in HTML from main site",
    "text": "Read in HTML from main site\nAs mentioned above, our site is the database of PhD dissertations from the University of Tennessee.\nLet’s create a character vector with the URL:\n\nurl &lt;- \"https://trace.tennessee.edu/utk_graddiss/index.html\"\n\nFirst, we read in the html data from that page:\n\nhtml &lt;- read_html(url)\n\nLet’s have a look at the raw data:\n\nhtml\n\n{html_document}\n&lt;html lang=\"en\"&gt;\n[1] &lt;head&gt;\\n&lt;meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8 ...\n[2] &lt;body&gt;\\n&lt;!-- FILE /srv/sequoia/main/data/trace.tennessee.edu/assets/heade ...",
    "crumbs": [
      "R",
      "<b><em>Workshops</em></b>",
      "Web scraping with rvest"
    ]
  },
  {
    "objectID": "r/ws_webscraping.html#test-run",
    "href": "r/ws_webscraping.html#test-run",
    "title": "Web scraping with R",
    "section": "Test run",
    "text": "Test run\n\nIdentify the relevant HTML markers\nThe html code for this webpage contains the data we are interested in, but it is mixed in with a lot of HTML formatting and data we don’t care about. We need to extract the data relevant to us and turn it into a workable format.\nThe first step is to find the HTML markers that contain our data. One option is to use a web inspector or—even easier—the SelectorGadget, a JavaScript bookmarklet built by Andrew Cantino.\nTo use this tool, go to the SelectorGadget website and drag the link of the bookmarklet to your bookmarks bar.\nNow, go to the dissertations database first page and click on the bookmarklet in your bookmarks bar. You will see a floating box at the bottom of your screen. As you move your mouse across the screen, an orange rectangle appears around each element over which you pass.\nClick on one of the dissertation links: now, there is an a appearing in the box at the bottom as well as the number of elements selected. The selected elements are highlighted in yellow. Those elements are links (in HTML, a tags define hyperlinks).\nAs you can see, all the links we want are selected. However, there are many other links we don’t want that are also highlighted. In fact, all links in the document are selected. We need to remove the categories of links that we don’t want. To do this, hover above any of the links we don’t want. You will see a red rectangle around it. Click on it: now all similar links are gone. You might have to do this a few times until only the relevant links (i.e. those that lead to the dissertation information pages) remain highlighted.\nAs there are 100 such links per page, the count of selected elements in the bottom floating box should be down to 100.\nIn the main section of the floating box, you can now see: .article-listing a. This means that the data we want are under the HTML elements .article-listing a (the class .article-listing and the tag a).\n\n\nExtract test URL\nIt is a good idea to test things out on a single element before doing a massive batch scraping of a site, so let’s test our method for the first dissertation.\nTo start, we need to extract the first URL. The function html_element() from the package rvest extracts the first element matching some character. Let’s pass to this function our html object and the character \".article-listing a\" and assign the result to an object that we will call test:\n\ntest &lt;- html %&gt;% html_element(\".article-listing a\")\n\n\n%&gt;% is a pipe from the magrittr tidyverse package. It passes the output from the left-hand side expression as the first argument of the right-hand side expression. We could have written this as:\ntest &lt;- html_element(html, \".article-listing a\")\n\nOur new object is a list:\n\ntypeof(test)\n\n[1] \"list\"\n\n\nLet’s print it:\n\ntest\n\n{html_node}\n&lt;a href=\"https://trace.tennessee.edu/utk_graddiss/10400\"&gt;\n\n\nThe URL is in there, so we successfully extracted the correct element, but we need to do more cleaning.\na is one of the HTML tags that have an attribute (href) as you can see when you print test. It is actually the value of that attribute that we want. To extract an attribute value, we use the function html_attr():\n\nurl_test &lt;- test %&gt;% html_attr(\"href\")\nurl_test\n\n[1] \"https://trace.tennessee.edu/utk_graddiss/10400\"\n\n\nThis is our URL.\n\nstr(url_test)\n\n chr \"https://trace.tennessee.edu/utk_graddiss/10400\"\n\n\nIt is saved in a character vector, which is perfect.\n\nInstead of creating the intermediate objects html and test, we could have chained the functions:\n\nurl_test &lt;- read_html(url) %&gt;%\n  html_element(\".article-listing a\") %&gt;%\n  html_attr(\"href\")\n\n\n\n\nRead in HTML data for test URL\nNow that we have the URL for the first dissertation information page, we want to extract the date, major, and advisor for that dissertation.\nWe just saw that url_test is a character vector representing a URL. We know how to deal with this.\nThe first thing to do—as we did earlier with the database site—is to read in the html data. Let’s assign it to a new object that we will call html_test:\n\nhtml_test &lt;- read_html(url_test)\nhtml_test\n\n{html_document}\n&lt;html lang=\"en\"&gt;\n[1] &lt;head&gt;\\n&lt;meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8 ...\n[2] &lt;body&gt;\\n&lt;!-- FILE /srv/sequoia/main/data/trace.tennessee.edu/assets/heade ...\n\n\n\n\nGet data for test URL\nNow, we want to extract the publication date. Thanks to the SelectorGadget, following the method we saw earlier, we can see that we now need the element marked by #publication_date p.\nWe start by extracting the data as we did earlier by passing our object html_test and the character \"#publication_date p\" to html_element().\nWhile earlier we wanted the value of a tag attribute (i.e. part of the metadata), here we want the actual text (i.e. part of the actual content). To extract text from a snippet of HTML, we pass it to html_text2().\nLet’s run both operations at once to save the creation of an intermediate object:\n\ndate_test &lt;- html_test %&gt;%\n  html_element(\"#publication_date p\") %&gt;%\n  html_text2()\n\n\nNote the difference with what we did earlier to extract the URL: if we had used html_text2() then we would have gotten the text part of the link (\"The Novel Chlorination of Zirconium Metal and Its Application to a Recycling Protocol for Zircaloy Cladding from Spent Nuclear Fuel Rods\") rather than the URL (\"https://trace.tennessee.edu/utk_graddiss/7600\").\n\nLet’s verify that our date object indeed contains the date:\n\ndate_test\n\n[1] \"8-2024\"\n\n\nWe also want the major for this thesis. The SelectorGadget allows us to find that this time, it is the #department p element that we need. Let’s extract it in the same fashion:\n\nmajor_test &lt;- html_test %&gt;%\n  html_element(\"#department p\") %&gt;%\n  html_text2()\nmajor_test\n\n[1] \"History\"\n\n\nAnd for the advisor, we need the #advisor1 p element:\n\nadvisor_test &lt;- html_test %&gt;%\n  html_element(\"#advisor1 p\") %&gt;%\n  html_text2()\nadvisor_test\n\n[1] \"Jay Rubenstein\"\n\n\n\n\nYour turn:\n\nTry using the SelectorGadget to identify the element necessary to extract the abstract of this dissertation.\nNow, write the code to extract it and make sure you actually get what you want.\n\nWe now have the date, major, and advisor for the first dissertation. We can create a matrix by passing them as arguments to cbind():\n\nresult_test &lt;- cbind(date_test, major_test, advisor_test)\nresult_test\n\n     date_test major_test advisor_test    \n[1,] \"8-2024\"  \"History\"  \"Jay Rubenstein\"",
    "crumbs": [
      "R",
      "<b><em>Workshops</em></b>",
      "Web scraping with rvest"
    ]
  },
  {
    "objectID": "r/ws_webscraping.html#full-run",
    "href": "r/ws_webscraping.html#full-run",
    "title": "Web scraping with R",
    "section": "Full run",
    "text": "Full run\n\nExtract all URLs\nNow that we have tested our code on the first dissertation, we can apply it on all 100 dissertations of the first page of the database.\nInstead of using html_element(), this time we will use html_elements() which extracts all matching elements (instead of just the first one):\n\ndat &lt;- html %&gt;% html_elements(\".article-listing a\")\ndat\n\n{xml_nodeset (100)}\n [1] &lt;a href=\"https://trace.tennessee.edu/utk_graddiss/10400\"&gt;The Sons of Mel ...\n [2] &lt;a href=\"https://trace.tennessee.edu/utk_graddiss/10081\"&gt;Checking the Bo ...\n [3] &lt;a href=\"https://trace.tennessee.edu/utk_graddiss/10401\"&gt;Data-Driven Mod ...\n [4] &lt;a href=\"https://trace.tennessee.edu/utk_graddiss/10082\"&gt;Computational S ...\n [5] &lt;a href=\"https://trace.tennessee.edu/utk_graddiss/10424\"&gt;Multi-Proxy Stu ...\n [6] &lt;a href=\"https://trace.tennessee.edu/utk_graddiss/10402\"&gt;Mechanisms of G ...\n [7] &lt;a href=\"https://trace.tennessee.edu/utk_graddiss/10425\"&gt;Applying Minori ...\n [8] &lt;a href=\"https://trace.tennessee.edu/utk_graddiss/10426\"&gt;Alchemy of the  ...\n [9] &lt;a href=\"https://trace.tennessee.edu/utk_graddiss/10083\"&gt;Topics in Energ ...\n[10] &lt;a href=\"https://trace.tennessee.edu/utk_graddiss/10427\"&gt;Unraveling the  ...\n[11] &lt;a href=\"https://trace.tennessee.edu/utk_graddiss/10084\"&gt;Someone to Keep ...\n[12] &lt;a href=\"https://trace.tennessee.edu/utk_graddiss/10085\"&gt;Perceptions of  ...\n[13] &lt;a href=\"https://trace.tennessee.edu/utk_graddiss/10403\"&gt;The Influence o ...\n[14] &lt;a href=\"https://trace.tennessee.edu/utk_graddiss/10086\"&gt;Motor Control Q ...\n[15] &lt;a href=\"https://trace.tennessee.edu/utk_graddiss/10428\"&gt;Laser Directed  ...\n[16] &lt;a href=\"https://trace.tennessee.edu/utk_graddiss/10087\"&gt;Music to their  ...\n[17] &lt;a href=\"https://trace.tennessee.edu/utk_graddiss/10429\"&gt;CodonT5: A Mult ...\n[18] &lt;a href=\"https://trace.tennessee.edu/utk_graddiss/10088\"&gt;Who Benefits fr ...\n[19] &lt;a href=\"https://trace.tennessee.edu/utk_graddiss/10089\"&gt;Applying a One  ...\n[20] &lt;a href=\"https://trace.tennessee.edu/utk_graddiss/10430\"&gt;The World of Lé ...\n...\n\n\n\ntypeof(dat)\n\n[1] \"list\"\n\nlength(dat)\n\n[1] 100\n\ntypeof(dat[[1]])\n\n[1] \"list\"\n\n\nWe now have a list of lists.\nAs we did for a single URL in the test run, we now want to extract all the URLs. We will do this using a loop.\nBefore running for loops, it is important to initialize empty loops. It is much more efficient than growing the result at each iteration.\nSo let’s initialize an empty list that we call list_urls of the appropriate size:\n\nlist_urls &lt;- vector(\"list\", length(dat))\n\nNow we can run a loop to fill in our list:\n\nfor (i in seq_along(dat)) {\n  list_urls[[i]] &lt;- dat[[i]] %&gt;% html_attr(\"href\")\n}\n\nLet’s print again the first element of list_urls to make sure all looks good:\n\nlist_urls[[1]]\n\n[1] \"https://trace.tennessee.edu/utk_graddiss/10400\"\n\n\nWe now have a list of URLs (in the form of character vectors) as we wanted.\n\n\nExtract data from each page\nWe will now extract the data (date, major, and advisor) for all URLs in our list.\nAgain, before running a for loop, we need to allocate memory first by creating an empty container (here a list):\n\nlist_data &lt;- vector(\"list\", length(list_urls))\n\nWe move the code we tested for a single URL inside a loop and we add one result to the list_data list at each iteration until we have all 100 dissertation sites scraped. Because there are quite a few of us running the code at the same time, we don’t want the site to block our request. To play safe, we will add a little delay (0.1 second) at each iteration (many sites will block requests if they are too frequent):\n\nfor (i in seq_along(list_urls)) {\n  html &lt;- read_html(list_urls[[i]])\n  date &lt;- html %&gt;%\n    html_element(\"#publication_date p\") %&gt;%\n    html_text2()\n  major &lt;- html %&gt;%\n    html_element(\"#department p\") %&gt;%\n    html_text2()\n  advisor &lt;- html %&gt;%\n    html_element(\"#advisor1 p\") %&gt;%\n    html_text2()\n  Sys.sleep(0.1)  # Add a little delay\n  list_data[[i]] &lt;- cbind(date, major, advisor)\n}\n\nLet’s make sure all looks good by printing the first element of list_data:\n\nlist_data[[1]]\n\n     date     major     advisor         \n[1,] \"8-2024\" \"History\" \"Jay Rubenstein\"",
    "crumbs": [
      "R",
      "<b><em>Workshops</em></b>",
      "Web scraping with rvest"
    ]
  },
  {
    "objectID": "r/ws_webscraping.html#store-results-in-dataframe",
    "href": "r/ws_webscraping.html#store-results-in-dataframe",
    "title": "Web scraping with R",
    "section": "Store results in DataFrame",
    "text": "Store results in DataFrame\nWe can turn this big list into a dataframe:\n\nresult &lt;- do.call(rbind.data.frame, list_data)\n\nresult is a long dataframe, so we will only print the first few elements:\n\nhead(result)\n\n    date                                           major              advisor\n1 8-2024                                         History       Jay Rubenstein\n2 5-2024                         Business Administration      Larry A. Fauver\n3 8-2024                          Electrical Engineering           Dan Wilson\n4 5-2024                            Chemical Engineering       Steven M. Abel\n5 8-2024                                       Geography         Sally P Horn\n6 8-2024 Biochemistry and Cellular and Molecular Biology Dr. Mariano Labrador\n\n\nIf you like the tidyverse, you can turn it into a tibble:\n\nresult &lt;- result %&gt;% tibble::as_tibble()\n\n\nThe notation tibble::as_tibble() means that we are using the function as_tibble() from the package tibble. A tibble is the tidyverse version of a dataframe. One advantage is that it will only print the first 10 rows by default instead of printing the whole dataframe, so you don’t have to use head() when printing long dataframes:\n\nresult\n\n# A tibble: 100 × 3\n   date   major                                           advisor             \n   &lt;chr&gt;  &lt;chr&gt;                                           &lt;chr&gt;               \n 1 8-2024 History                                         Jay Rubenstein      \n 2 5-2024 Business Administration                         Larry A. Fauver     \n 3 8-2024 Electrical Engineering                          Dan Wilson          \n 4 5-2024 Chemical Engineering                            Steven M. Abel      \n 5 8-2024 Geography                                       Sally P Horn        \n 6 8-2024 Biochemistry and Cellular and Molecular Biology Dr. Mariano Labrador\n 7 8-2024 Psychology                                      Kristina C. Gordon  \n 8 8-2024 Counselor Education                             Joel F. Diambra     \n 9 5-2024 Economics                                       James Scott Holladay\n10 8-2024 Chemistry                                       Thanh D. Do         \n# ℹ 90 more rows\n\n\n\nWe can capitalize the headers:\n\nnames(result) &lt;- c(\"Date\", \"Major\", \"Advisor\")\n\nThis is what our final result looks like:\n\nresult\n\n# A tibble: 100 × 3\n   Date   Major                                           Advisor             \n   &lt;chr&gt;  &lt;chr&gt;                                           &lt;chr&gt;               \n 1 8-2024 History                                         Jay Rubenstein      \n 2 5-2024 Business Administration                         Larry A. Fauver     \n 3 8-2024 Electrical Engineering                          Dan Wilson          \n 4 5-2024 Chemical Engineering                            Steven M. Abel      \n 5 8-2024 Geography                                       Sally P Horn        \n 6 8-2024 Biochemistry and Cellular and Molecular Biology Dr. Mariano Labrador\n 7 8-2024 Psychology                                      Kristina C. Gordon  \n 8 8-2024 Counselor Education                             Joel F. Diambra     \n 9 5-2024 Economics                                       James Scott Holladay\n10 8-2024 Chemistry                                       Thanh D. Do         \n# ℹ 90 more rows",
    "crumbs": [
      "R",
      "<b><em>Workshops</em></b>",
      "Web scraping with rvest"
    ]
  },
  {
    "objectID": "r/ws_webscraping.html#save-results-to-file",
    "href": "r/ws_webscraping.html#save-results-to-file",
    "title": "Web scraping with R",
    "section": "Save results to file",
    "text": "Save results to file\nAs a final step, we will save our data to a CSV file:\nwrite.csv(result, \"dissertations_data.csv\", row.names = FALSE)",
    "crumbs": [
      "R",
      "<b><em>Workshops</em></b>",
      "Web scraping with rvest"
    ]
  },
  {
    "objectID": "r/ws_webscraping.html#functions-recap",
    "href": "r/ws_webscraping.html#functions-recap",
    "title": "Web scraping with R",
    "section": "Functions recap",
    "text": "Functions recap\nBelow is a recapitulation of the rvest functions we have used today:\n\n\n\nFunctions\nUsage\n\n\n\n\nread_html()\nRead in HTML from URL\n\n\nhtml_element()\nExtract first matching element\n\n\nhtml_elements()\nExtract all matching elements\n\n\nhtml_attr()\nExtract the value of an attribute\n\n\nhtml_text2()\nExtract text",
    "crumbs": [
      "R",
      "<b><em>Workshops</em></b>",
      "Web scraping with rvest"
    ]
  },
  {
    "objectID": "r/ws_webscraping.html#recording",
    "href": "r/ws_webscraping.html#recording",
    "title": "Web scraping with R",
    "section": "Recording",
    "text": "Recording\n\nVideo of this workshop for the Digital Research Alliance of Canada HSS Winter Series 2023:",
    "crumbs": [
      "R",
      "<b><em>Workshops</em></b>",
      "Web scraping with rvest"
    ]
  },
  {
    "objectID": "talks/2023_driconnect_slides.html#topics",
    "href": "talks/2023_driconnect_slides.html#topics",
    "title": "The instruments for advanced research computing are here, but are researchers ready?",
    "section": "Topics",
    "text": "Topics\n\nUnix shell\nHPC\nVersion control with Git/DataLad\nScientific programming in R/Python/Julia\nParallel computing in R/Julia/Chapel\nDeep learning with PyTorch\nScientific visualization\nContainers/Alliance clouds/VMs\nWebscraping in R/Python\nGIS in R\nScientific publishing with Quarto"
  },
  {
    "objectID": "talks/2023_driconnect_slides.html#fast",
    "href": "talks/2023_driconnect_slides.html#fast",
    "title": "The instruments for advanced research computing are here, but are researchers ready?",
    "section": "Fast",
    "text": "Fast"
  },
  {
    "objectID": "talks/2024_bccai_part1_slides.html#simon-fraser-university",
    "href": "talks/2024_bccai_part1_slides.html#simon-fraser-university",
    "title": "Harnessing big datafor agricultural excellence",
    "section": "Simon Fraser University",
    "text": "Simon Fraser University\n\nSFU hosts the Cedar supercomputer—a cluster of 100,400 CPUs and 1,352 GPUs soon to be replaced by an even larger computer cluster"
  },
  {
    "objectID": "talks/2024_bccai_part1_slides.html#simon-fraser-university-1",
    "href": "talks/2024_bccai_part1_slides.html#simon-fraser-university-1",
    "title": "Harnessing big datafor agricultural excellence",
    "section": "Simon Fraser University",
    "text": "Simon Fraser University\nSFU also works with the Digital Research Alliance of Canada to offer researchers large amounts of computing power to solve challenging data and technology problems, as well as training to optimize their solutions"
  },
  {
    "objectID": "talks/2024_bccai_part1_slides.html#sfus-big-data-hub",
    "href": "talks/2024_bccai_part1_slides.html#sfus-big-data-hub",
    "title": "Harnessing big datafor agricultural excellence",
    "section": "SFU’s Big Data Hub",
    "text": "SFU’s Big Data Hub\n\nSince 2016, Simon Fraser University’s Big Data Hub has been offering workshops, events, and consulting services to researchers and industry partners helping them remain at the top of the fast evolving data landscape"
  },
  {
    "objectID": "talks/2024_bccai_part1_slides.html#bc-centre-for-agritech-innovation",
    "href": "talks/2024_bccai_part1_slides.html#bc-centre-for-agritech-innovation",
    "title": "Harnessing big datafor agricultural excellence",
    "section": "BC Centre for Agritech Innovation",
    "text": "BC Centre for Agritech Innovation\n\nSince 2022, SFU BCCAI has been helping small and medium enterprises in the farming industry to embrace technology driven solutions"
  },
  {
    "objectID": "talks/2024_bccai_part1_slides.html#bc-centre-for-agritech-innovation-1",
    "href": "talks/2024_bccai_part1_slides.html#bc-centre-for-agritech-innovation-1",
    "title": "Harnessing big datafor agricultural excellence",
    "section": "BC Centre for Agritech Innovation",
    "text": "BC Centre for Agritech Innovation\n\n\n\n\n\n\n\n\n\n\nsupport\n\nAgritech projects\n\n\n\ntraining\n\nTraining & upscaling\n\n\n\n\nnetwork\n\nAgritech network"
  },
  {
    "objectID": "talks/2024_bccai_part1_slides.html#session-1",
    "href": "talks/2024_bccai_part1_slides.html#session-1",
    "title": "Harnessing big datafor agricultural excellence",
    "section": "Session 1",
    "text": "Session 1\n\n\n\nToday\n\nA (hopefully) friendly lecture to:\n\nDemystify big data\nDemonstrate the critical importance of big data in agriculture and farming"
  },
  {
    "objectID": "talks/2024_bccai_part1_slides.html#session-2",
    "href": "talks/2024_bccai_part1_slides.html#session-2",
    "title": "Harnessing big datafor agricultural excellence",
    "section": "Session 2",
    "text": "Session 2\n\n\n\nTomorrow at 11am in the Mount Baker Room\n\nAn interactive workshop to:\n\nBrainstorm on how big data can benefit your operation\nHelp you make the transition to smart farming"
  },
  {
    "objectID": "talks/2024_bccai_part1_slides.html#the-3-v-volume",
    "href": "talks/2024_bccai_part1_slides.html#the-3-v-volume",
    "title": "Harnessing big datafor agricultural excellence",
    "section": "The 3 “V”: Volume",
    "text": "The 3 “V”: Volume\n\n\nBefore\nFarmers were taking measurements (e.g. on soil moisture) manually creating low volumes of data"
  },
  {
    "objectID": "talks/2024_bccai_part1_slides.html#the-3-v-volume-1",
    "href": "talks/2024_bccai_part1_slides.html#the-3-v-volume-1",
    "title": "Harnessing big datafor agricultural excellence",
    "section": "The 3 “V”: Volume",
    "text": "The 3 “V”: Volume\n\n\nBefore\nFarmers were taking measurements (e.g. on soil moisture) manually creating low volumes of data\nNow\nInternet of Things (IoT) (e.g. hundreds of soil moisture sensors) collects large volumes of data"
  },
  {
    "objectID": "talks/2024_bccai_part1_slides.html#the-3-v-variety",
    "href": "talks/2024_bccai_part1_slides.html#the-3-v-variety",
    "title": "Harnessing big datafor agricultural excellence",
    "section": "The 3 “V”: Variety",
    "text": "The 3 “V”: Variety\n\n\nBefore\nThere was a limited set of data a producer could collect"
  },
  {
    "objectID": "talks/2024_bccai_part1_slides.html#the-3-v-variety-1",
    "href": "talks/2024_bccai_part1_slides.html#the-3-v-variety-1",
    "title": "Harnessing big datafor agricultural excellence",
    "section": "The 3 “V”: Variety",
    "text": "The 3 “V”: Variety\n\n\nBefore\nThere was a limited set of data a producer could collect\nNow\nThere are so many different types of data (e.g. satellite images, market data gathered from internet browsing…)"
  },
  {
    "objectID": "talks/2024_bccai_part1_slides.html#the-3-v-velocity",
    "href": "talks/2024_bccai_part1_slides.html#the-3-v-velocity",
    "title": "Harnessing big datafor agricultural excellence",
    "section": "The 3 “V”: Velocity",
    "text": "The 3 “V”: Velocity\n\n\nBefore\nA farmer could only gather so much data, even with a lot of employees"
  },
  {
    "objectID": "talks/2024_bccai_part1_slides.html#the-3-v-velocity-1",
    "href": "talks/2024_bccai_part1_slides.html#the-3-v-velocity-1",
    "title": "Harnessing big datafor agricultural excellence",
    "section": "The 3 “V”: Velocity",
    "text": "The 3 “V”: Velocity\n\n\nBefore\nA farmer could only gather so much data, even with a lot of employees\nNow\nData is generated in real time and accumulates at high speed"
  },
  {
    "objectID": "talks/2024_bccai_part1_slides.html#why-has-big-data-become-so-essential",
    "href": "talks/2024_bccai_part1_slides.html#why-has-big-data-become-so-essential",
    "title": "Harnessing big datafor agricultural excellence",
    "section": "Why has big data become so essential?",
    "text": "Why has big data become so essential?\nAll this data is key to the development of artificial intelligence (AI)\n\nso…\n\n\nWhat is AI?"
  },
  {
    "objectID": "talks/2024_bccai_part1_slides.html#ai",
    "href": "talks/2024_bccai_part1_slides.html#ai",
    "title": "Harnessing big datafor agricultural excellence",
    "section": "AI",
    "text": "AI\nVery loosely, you can think of neural networks (the most powerful form of AI) as an attempt to create a computer model that mimics the brain\n\n\n\n\n\nBiological neurons\n\n\n\n\n\n\n\n\nNeural network"
  },
  {
    "objectID": "talks/2024_bccai_part1_slides.html#ai-1",
    "href": "talks/2024_bccai_part1_slides.html#ai-1",
    "title": "Harnessing big datafor agricultural excellence",
    "section": "AI",
    "text": "AI\nIn traditional computing, a programmer writes code that gives a computer detailed instructions of what to do\nThese instructions are called a program\n\n\n\n\n \n\n Some action"
  },
  {
    "objectID": "talks/2024_bccai_part1_slides.html#ai-2",
    "href": "talks/2024_bccai_part1_slides.html#ai-2",
    "title": "Harnessing big datafor agricultural excellence",
    "section": "AI",
    "text": "AI\n\n\nWith neural networks, instead of writing a program, a programmer writes a model, then feeds it lots of data and the model changes little by little over time\nThe model “learns” thanks to this data\n\n\n\nSimplilearn has a video explaining how neural networks work in 5 min"
  },
  {
    "objectID": "talks/2024_bccai_part1_slides.html#ai-3",
    "href": "talks/2024_bccai_part1_slides.html#ai-3",
    "title": "Harnessing big datafor agricultural excellence",
    "section": "AI",
    "text": "AI\n\n\nThis learning is nothing magical: some numbers in the model get tweaked a tiny bit, with each new piece of data, to make the model a little bit better\n\n\n\n\n\n\nFrom xkcd.com"
  },
  {
    "objectID": "talks/2024_bccai_part1_slides.html#ai-4",
    "href": "talks/2024_bccai_part1_slides.html#ai-4",
    "title": "Harnessing big datafor agricultural excellence",
    "section": "AI",
    "text": "AI\nBasically, we start with a model, train it with data, and we end up with a trained model that can be used as a traditional computer program\n\nThat trained model can be used to get predictions, generate art or speech, identify objects in images or spams in emails…\nThe only difference from traditional computing is that we don’t write the program ourselves. Instead, we write a starting point (the untrained model), then train it with A LOT of data and let it get better by itself"
  },
  {
    "objectID": "talks/2024_bccai_part1_slides.html#ai-5",
    "href": "talks/2024_bccai_part1_slides.html#ai-5",
    "title": "Harnessing big datafor agricultural excellence",
    "section": "AI",
    "text": "AI\n\n\nTo get a very good model at the end—one that can write human language like ChatGPT or voice assistants for instance—you really need A LOT OF DATA"
  },
  {
    "objectID": "talks/2024_bccai_part1_slides.html#ai-an-example",
    "href": "talks/2024_bccai_part1_slides.html#ai-an-example",
    "title": "Harnessing big datafor agricultural excellence",
    "section": "AI: an example",
    "text": "AI: an example\nImagine that you want a program able to detect tomatoes in pictures\nThis could be very useful to get real time data on your upcoming crop so that you can plan adequately (hiring staff, setting price, looking for markets)\nFor humans, this is straightforward\nYet, this is impossible to achieve with traditional programming because there are too many factors (location of the tomatoes in the image, quality of the picture, colour of the tomatoes…)"
  },
  {
    "objectID": "talks/2024_bccai_part1_slides.html#ai-an-example-1",
    "href": "talks/2024_bccai_part1_slides.html#ai-an-example-1",
    "title": "Harnessing big datafor agricultural excellence",
    "section": "AI: an example",
    "text": "AI: an example\n\n\nHowever, by feeding a very large number of images with and without tomatoes along with labels that give the number of tomatoes for each image to a neural network, we can train it to recognize tomatoes in images that it has never seen\nWith each pair of image/label (e.g. “Picture 34, label: 56 tomatoes”), the model gets better"
  },
  {
    "objectID": "talks/2024_bccai_part1_slides.html#ai-an-example-2",
    "href": "talks/2024_bccai_part1_slides.html#ai-an-example-2",
    "title": "Harnessing big datafor agricultural excellence",
    "section": "AI: an example",
    "text": "AI: an example\n\n\nWe don’t write the program to do this. We write the starting model, then let it adjust by itself based on the data\nIt is a form of learning by experience, which is exactly what happens to us as we grow up. It is a form of programming that is much closer to how brains work than traditional programming\n\n\n\n\n\n\nLawal, M. O. (2021). Tomato detection based on modified YOLOv3 framework. Scientific Reports, 11(1), 1-11."
  },
  {
    "objectID": "talks/2024_bccai_part1_slides.html#why-now",
    "href": "talks/2024_bccai_part1_slides.html#why-now",
    "title": "Harnessing big datafor agricultural excellence",
    "section": "Why now?",
    "text": "Why now?\nThe idea is not new, but it is only recently that we have had enough computing power, internet connectivity, and storage capacity to implement it"
  },
  {
    "objectID": "talks/2024_bccai_part1_slides.html#smart-farming",
    "href": "talks/2024_bccai_part1_slides.html#smart-farming",
    "title": "Harnessing big datafor agricultural excellence",
    "section": "Smart farming",
    "text": "Smart farming\n\n\nWe already talked about data collection thanks to the Internet of Things (e.g. moisture sensors)"
  },
  {
    "objectID": "talks/2024_bccai_part1_slides.html#smart-farming-1",
    "href": "talks/2024_bccai_part1_slides.html#smart-farming-1",
    "title": "Harnessing big datafor agricultural excellence",
    "section": "Smart farming",
    "text": "Smart farming\n\n\nBut this goes much further as AI algorithms can be used for “precision agriculture”\n\n\n\n\nKarunathilake, E. M. B. M., Le, A. T., Heo, S., Chung, Y. S., & Mansoor, S. (2023). The path to smart farming: Innovations and opportunities in precision agriculture. Agriculture, 13(8), 1593."
  },
  {
    "objectID": "talks/2024_bccai_part1_slides.html#smart-farming-2",
    "href": "talks/2024_bccai_part1_slides.html#smart-farming-2",
    "title": "Harnessing big datafor agricultural excellence",
    "section": "Smart farming",
    "text": "Smart farming\n\n\nMany AI tools are involved in improving all domains of agriculture, from irrigation management to supply chain and demand forecasting\nThe benefits are huge for both farmers (increased yields, reduced costs, better planning) and the environment (optimization of resources and pesticide use)\n\n\n\n\nKarunathilake, E. M. B. M., Le, A. T., Heo, S., Chung, Y. S., & Mansoor, S. (2023). The path to smart farming: Innovations and opportunities in precision agriculture. Agriculture, 13(8), 1593."
  },
  {
    "objectID": "talks/2024_bccai_part1_slides.html#decision-making",
    "href": "talks/2024_bccai_part1_slides.html#decision-making",
    "title": "Harnessing big datafor agricultural excellence",
    "section": "Decision making",
    "text": "Decision making\n\n\nBefore\nFarmers had to make decisions as best they could based on their experience and their limited data"
  },
  {
    "objectID": "talks/2024_bccai_part1_slides.html#decision-making-1",
    "href": "talks/2024_bccai_part1_slides.html#decision-making-1",
    "title": "Harnessing big datafor agricultural excellence",
    "section": "Decision making",
    "text": "Decision making\n\n\nBefore\nFarmers had to take decisions as best they could based on their experience and their limited data\nNow\nFarmers can use powerful models to make informed decision in real time. This can be followed by the automation of some action (e.g. watering)"
  },
  {
    "objectID": "talks/2024_bccai_part1_slides.html#livestock-monitoring",
    "href": "talks/2024_bccai_part1_slides.html#livestock-monitoring",
    "title": "Harnessing big datafor agricultural excellence",
    "section": "Livestock monitoring",
    "text": "Livestock monitoring\n\n\nA case study\nLivestock successfully monitored remotely via sound sensors and algorithms for background noise filtering\nAnimal welfare and efficiency improvements\n\n\n\n\n\n\nJung, D. H., Kim, N. Y., Moon, S. H., Jhin, C., Kim, H. J., Yang, J. S., … & Park, S. H. (2021). Deep learning-based cattle vocal classification model and real-time livestock monitoring system with noise filtering. Animals, 11(2), 357."
  },
  {
    "objectID": "talks/2024_bccai_part1_slides.html#market-analysis-and-supply-chain-optimization",
    "href": "talks/2024_bccai_part1_slides.html#market-analysis-and-supply-chain-optimization",
    "title": "Harnessing big datafor agricultural excellence",
    "section": "Market analysis and supply chain optimization",
    "text": "Market analysis and supply chain optimization\nA review\n\n\nSystematic literature review of peer-reviewed articles and conference papers published between 2014 and 20241 showed large improvements of demand forecasting accuracy and supply chain optimization\nReal time data analysis helped with predictive maintenance, market volatility, resource constraints, and climate variability\n\n\n\n\n\nElufioye, O. A., Ike, C. U., Odeyemi, O., Usman, F. O., & Mhlongo, N. Z. (2024). Ai-Driven predictive analytics in agricultural supply chains: a review: assessing the benefits and challenges of ai in forecasting demand and optimizing supply in agriculture. Computer Science & IT Research Journal, 5(2), 473-497."
  },
  {
    "objectID": "talks/2024_bccai_part1_slides.html#challenges",
    "href": "talks/2024_bccai_part1_slides.html#challenges",
    "title": "Harnessing big datafor agricultural excellence",
    "section": "Challenges",
    "text": "Challenges\nThere are challenges to the implementation of such transformative methods\n\nInfrastructure development\nSkill gaps among agricultural professionals\n\n\nWe are here to help!\n\n\n\n\n\n\n\n\n\n\nCome to session 2 tomorrow!"
  },
  {
    "objectID": "talks/2024_bccai_part1_slides.html#session-2-1",
    "href": "talks/2024_bccai_part1_slides.html#session-2-1",
    "title": "Harnessing big datafor agricultural excellence",
    "section": "Session 2",
    "text": "Session 2\nJoin us tomorrow at 11am in the Mount Baker Room for our 2nd session:\n\nDiagnosing and implementing big data solutions\n\nWe will have an interactive workshop to:\n\nBrainstorm on how big data can benefit your operation\nHelp you make the transition to smart farming\n\n\n\nIf you are unable to attend, you can find the slides here, but it will be an interactive clinic with most of the material covered in the activity"
  },
  {
    "objectID": "talks/2024_bccai_part1_slides.html#getting-in-touch",
    "href": "talks/2024_bccai_part1_slides.html#getting-in-touch",
    "title": "Harnessing big datafor agricultural excellence",
    "section": "Getting in touch",
    "text": "Getting in touch\n\nSFU’s Big Data Hub\nWebsite\nContact us\nConsultation services\nPartnerships\n\nBCCAI\nWebsite\nContact us\nAgritech development program\nTraining"
  },
  {
    "objectID": "talks/2024_bccai_part1_slides.html#understanding-neural-networks",
    "href": "talks/2024_bccai_part1_slides.html#understanding-neural-networks",
    "title": "Harnessing big datafor agricultural excellence",
    "section": "Understanding neural networks",
    "text": "Understanding neural networks\nTo go a bit further than the video mentioned earlier, 3Blue1Brown by Grant Sanderson has a series of 4 videos on neural networks which is easy to watch, fun, and does an excellent job at introducing the functioning of a simple neural network"
  },
  {
    "objectID": "talks/2024_bccai_part1_slides.html#literature",
    "href": "talks/2024_bccai_part1_slides.html#literature",
    "title": "Harnessing big datafor agricultural excellence",
    "section": "Literature",
    "text": "Literature\nOpen-access preprints:\nArxiv Sanity Preserver by Andrej Karpathy\nML papers in the computer science category on arXiv\nML papers in the stats category on arXiv\nDistill ML research online journal"
  },
  {
    "objectID": "talks/2024_bccai_part1_slides.html#acknowledgements",
    "href": "talks/2024_bccai_part1_slides.html#acknowledgements",
    "title": "Harnessing big datafor agricultural excellence",
    "section": "Acknowledgements",
    "text": "Acknowledgements\n\n\n\n\n\n\n\n Carson Li (BCCAI) suggested an outline for this talk\n Ian Chan (BCCAI) provided copious feedback"
  },
  {
    "objectID": "talks/2024_bccai_part1_slides.html#feedback",
    "href": "talks/2024_bccai_part1_slides.html#feedback",
    "title": "Harnessing big datafor agricultural excellence",
    "section": "Feedback",
    "text": "Feedback\nPlease give us feedback by scanning the QR code:\n\n\n\nThank you!"
  },
  {
    "objectID": "talks/index.html",
    "href": "talks/index.html",
    "title": "Talks",
    "section": "",
    "text": "Advanced research instruments\nDRI (Digital Research Infrastructure) Connect\n\n\n\n\nBig data in the agrotech industry\nAgricultural Excellence Conference",
    "crumbs": [
      "<br>&nbsp;<em><b>Conference talks</b></em><br><br>"
    ]
  },
  {
    "objectID": "tools/top_wb.html",
    "href": "tools/top_wb.html",
    "title": "Tools webinars",
    "section": "",
    "text": ": the new R Markdown\n\n\n\n\nA great Git UI: Lazygit\n\n\n\n\nData version control with\n\n\n\n\nFun tools for the command line\n\n\n\n\n\n\nMore command line tools\n\n\n\n\nModern shell utilities\n\n\n\n\nGetting online help & asking questions",
    "crumbs": [
      "Tools",
      "<b><em>Webinars</em></b>"
    ]
  },
  {
    "objectID": "tools/wb_dvc_slides.html#on-version-control",
    "href": "tools/wb_dvc_slides.html#on-version-control",
    "title": "Version control for data science & machine learning with DVC",
    "section": "On version control",
    "text": "On version control\nI won’t introduce here the benefits of using a good version control system such as Git\n\n\n\nOn the benefits of VCS"
  },
  {
    "objectID": "tools/wb_dvc_slides.html#extending-git-for-data",
    "href": "tools/wb_dvc_slides.html#extending-git-for-data",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Extending Git for data",
    "text": "Extending Git for data\nWhile Git is a wonderful tool for text files versioning (code, writings in markup formats), it isn’t a tool to manage changes to datasets\nSeveral open source tools—each with a different structure and functioning—extend Git capabilities to track data: Git LFS, git-annex, lakeFS, Dolt, DataLad"
  },
  {
    "objectID": "tools/wb_dvc_slides.html#extending-git-for-models-and-experiments",
    "href": "tools/wb_dvc_slides.html#extending-git-for-models-and-experiments",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Extending Git for models and experiments",
    "text": "Extending Git for models and experiments\nReproducible research and collaboration on data science and machine learning projects involve more than datasets management:\nExperiments and the models they produce also need to be tracked"
  },
  {
    "objectID": "tools/wb_dvc_slides.html#many-moving-parts",
    "href": "tools/wb_dvc_slides.html#many-moving-parts",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Many moving parts",
    "text": "Many moving parts\n\n*hp = hyperparameter\n\n\n\n\n\n\n\n\n\n\ndata1\n\ndata1\n\n\n\nmodel1\n\nmodel1\n\n\n\ndata1-&gt;model1\n\n\n\n\n\nmodel2\n\nmodel2\n\n\n\ndata1-&gt;model2\n\n\n\n\n\nmodel3\n\nmodel3\n\n\n\ndata1-&gt;model3\n\n\n\n\n\ndata2\n\ndata2\n\n\n\ndata2-&gt;model1\n\n\n\n\n\ndata2-&gt;model2\n\n\n\n\n\ndata2-&gt;model3\n\n\n\n\n\ndata3\n\ndata3\n\n\n\ndata3-&gt;model1\n\n\n\n\n\ndata3-&gt;model2\n\n\n\n\n\ndata3-&gt;model3\n\n\n\n\n\nhp1\n\nhp1\n\n\n\nhp1-&gt;model1\n\n\n\n\n\nhp1-&gt;model2\n\n\n\n\n\nhp1-&gt;model3\n\n\n\n\n\nhp2\n\nhp2\n\n\n\nhp2-&gt;model1\n\n\n\n\n\nhp2-&gt;model2\n\n\n\n\n\nhp2-&gt;model3\n\n\n\n\n\nhp3\n\nhp3\n\n\n\nhp3-&gt;model1\n\n\n\n\n\nhp3-&gt;model2\n\n\n\n\n\nhp3-&gt;model3\n\n\n\n\n\nperformance\n\nperformance1 ... performance27\n\n\n\nmodel1-&gt;performance\n\n\n\n\n\nmodel1-&gt;performance\n\n\n\n\n\nmodel1-&gt;performance\n\n\n\n\n\nmodel1-&gt;performance\n\n\n\n\n\nmodel1-&gt;performance\n\n\n\n\n\nmodel1-&gt;performance\n\n\n\n\n\nmodel1-&gt;performance\n\n\n\n\n\nmodel1-&gt;performance\n\n\n\n\n\nmodel1-&gt;performance\n\n\n\n\n\nmodel2-&gt;performance\n\n\n\n\n\nmodel2-&gt;performance\n\n\n\n\n\nmodel2-&gt;performance\n\n\n\n\n\nmodel2-&gt;performance\n\n\n\n\n\nmodel2-&gt;performance\n\n\n\n\n\nmodel2-&gt;performance\n\n\n\n\n\nmodel2-&gt;performance\n\n\n\n\n\nmodel2-&gt;performance\n\n\n\n\n\nmodel2-&gt;performance\n\n\n\n\n\nmodel3-&gt;performance\n\n\n\n\n\nmodel3-&gt;performance\n\n\n\n\n\nmodel3-&gt;performance\n\n\n\n\n\nmodel3-&gt;performance\n\n\n\n\n\nmodel3-&gt;performance\n\n\n\n\n\nmodel3-&gt;performance\n\n\n\n\n\nmodel3-&gt;performance\n\n\n\n\n\nmodel3-&gt;performance\n\n\n\n\n\nmodel3-&gt;performance\n\n\n\n\n\n\n\n\n\n\n\nHow did we get performance17 again? 🤯"
  },
  {
    "objectID": "tools/wb_dvc_slides.html#dvc-principles",
    "href": "tools/wb_dvc_slides.html#dvc-principles",
    "title": "Version control for data science & machine learning with DVC",
    "section": "DVC principles",
    "text": "DVC principles\nLarge files (datasets, models…) are kept outside Git\nEach large file or directory put under DVC tracking has an associated .dvc file\nGit only tracks the .dvc files (metadata)\n\nWorkflows can be tracked for collaboration and reproducibility\n\n\nDVC functions as a Makefile and allows to only rerun what is necessary"
  },
  {
    "objectID": "tools/wb_dvc_slides.html#installation",
    "href": "tools/wb_dvc_slides.html#installation",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Installation",
    "text": "Installation\nFor Linux (other OSes, refer to the doc):\n\npip:\npip install dvc\nconda\npipx (if you want dvc available everywhere without having to activate virtual envs):\npipx install dvc\n\n\nOptional dependencies [s3], [gdrive], etc. for remote storage"
  },
  {
    "objectID": "tools/wb_dvc_slides.html#how-to-run",
    "href": "tools/wb_dvc_slides.html#how-to-run",
    "title": "Version control for data science & machine learning with DVC",
    "section": "How to run",
    "text": "How to run\n\nTerminal\ndvc ...\nVS Code extension\nPython library if installed via pip or conda\nimport dvc.api\n\n\nIn this webinar, I will use DVC through the command line"
  },
  {
    "objectID": "tools/wb_dvc_slides.html#acknowledgements",
    "href": "tools/wb_dvc_slides.html#acknowledgements",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nCode and data for this webinar modified from:\n\nReal Python\nDataLad handbook\nDVC documentation"
  },
  {
    "objectID": "tools/wb_dvc_slides.html#the-project",
    "href": "tools/wb_dvc_slides.html#the-project",
    "title": "Version control for data science & machine learning with DVC",
    "section": "The project",
    "text": "The project\ntree -L 3\n├── LICENSE\n├── data\n│   ├── prepared\n│   └── raw\n│       ├── train\n│       └── val\n├── metrics\n├── model\n├── requirements.txt\n└── src\n    ├── evaluate.py\n    ├── prepare.py\n    └── train.py"
  },
  {
    "objectID": "tools/wb_dvc_slides.html#initialize-git-repo",
    "href": "tools/wb_dvc_slides.html#initialize-git-repo",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Initialize Git repo",
    "text": "Initialize Git repo\ngit init\nInitialized empty Git repository in dvc/.git/\n\nThis creates the .git directory\n\n\ngit status\nOn branch main\n\nNo commits yet\n\nUntracked files:\n    LICENSE\n    data/\n    requirements.txt\n    src/"
  },
  {
    "objectID": "tools/wb_dvc_slides.html#initialize-dvc-project",
    "href": "tools/wb_dvc_slides.html#initialize-dvc-project",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Initialize DVC project",
    "text": "Initialize DVC project\ndvc init\nInitialized DVC repository.\n\nYou can now commit the changes to git.\n\nYou will also see a note about usage analytics collection and info on how to opt out\n\n\nA .dvc directory and a .dvcignore file got created"
  },
  {
    "objectID": "tools/wb_dvc_slides.html#commit-dvc-system-files",
    "href": "tools/wb_dvc_slides.html#commit-dvc-system-files",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Commit DVC system files",
    "text": "Commit DVC system files\n\nDVC automatically staged its system file for us:\ngit status\nOn branch main\n\nNo commits yet\n\nChanges to be committed:\n    new file:   .dvc/.gitignore\n    new file:   .dvc/config\n    new file:   .dvcignore\n\nUntracked files:\n    LICENSE\n    data/\n    requirements.txt\n    src/\n\nSo we can directly commit:\ngit commit -m \"Initialize DVC\""
  },
  {
    "objectID": "tools/wb_dvc_slides.html#prepare-repo",
    "href": "tools/wb_dvc_slides.html#prepare-repo",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Prepare repo",
    "text": "Prepare repo\nLet’s work in a virtual environment:\n# Create venv and add to .gitignore\npython -m venv venv && echo venv &gt; .gitignore\n\n# Activate venv\nsource venv/bin/activate\n\n# Update pip\npython -m pip install --upgrade pip\n\n# Install packages needed\npython -m pip install -r requirements.txt"
  },
  {
    "objectID": "tools/wb_dvc_slides.html#clean-working-tree",
    "href": "tools/wb_dvc_slides.html#clean-working-tree",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Clean working tree",
    "text": "Clean working tree\ngit add .gitignore LICENSE requirements.txt\ngit commit -m \"Add general files\"\ngit add src\ngit commit -m \"Add scripts\"\ngit status\nOn branch main\nUntracked files:\n    data/\n\n\nNow, it is time to deal with the data"
  },
  {
    "objectID": "tools/wb_dvc_slides.html#put-data-under-dvc-tracking",
    "href": "tools/wb_dvc_slides.html#put-data-under-dvc-tracking",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Put data under DVC tracking",
    "text": "Put data under DVC tracking\nWe are still not tracking any data:\ndvc status\nThere are no data or pipelines tracked in this project yet.\nYou can choose what to track as a unit (i.e. each picture individually, the whole data directory as a unit)\nLet’s break it down by set:\ndvc add data/raw/train\ndvc add data/raw/val"
  },
  {
    "objectID": "tools/wb_dvc_slides.html#section",
    "href": "tools/wb_dvc_slides.html#section",
    "title": "Version control for data science & machine learning with DVC",
    "section": "",
    "text": "This adds data to .dvc/cache/files and created 3 files in data/raw:\n\n.gitignore\ntrain.dvc\nval.dvc\n\nThe .gitignore tells Git not to track the data:\ncat data/raw/.gitignore\n/train\n/val\nThe .dvc files contain the metadata for the cached directories"
  },
  {
    "objectID": "tools/wb_dvc_slides.html#tracked-data",
    "href": "tools/wb_dvc_slides.html#tracked-data",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Tracked data",
    "text": "Tracked data\nWe are all good:\ndvc status\nData and pipelines are up to date."
  },
  {
    "objectID": "tools/wb_dvc_slides.html#data-deduplication",
    "href": "tools/wb_dvc_slides.html#data-deduplication",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Data (de)duplication",
    "text": "Data (de)duplication\nLink between checked-out version of a file/directory and the cache:\n\nCache ⟷ working directory\n\n\n\n\n\n\n\n\nDuplication\nEditable\n\n\n\n\nReflinks*\nOnly when needed\nYes\n\n\nHardlinks/Symlinks\nNo\nNo\n\n\nCopies\nYes\nYes\n\n\n\n*Reflinks only available for a few file systems (Btrfs, XFS, OCFS2, or APFS)"
  },
  {
    "objectID": "tools/wb_dvc_slides.html#commit-the-metafiles",
    "href": "tools/wb_dvc_slides.html#commit-the-metafiles",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Commit the metafiles",
    "text": "Commit the metafiles\nThe metafiles should be put under Git version control\n\nYou can configure DVC to automatically stage its newly created system files:\ndvc config [--system] [--global] core.autostage true\n\nYou can then commit directly:\ngit commit -m \"Initial version of data\"\ngit status\nOn branch main\nnothing to commit, working tree clean"
  },
  {
    "objectID": "tools/wb_dvc_slides.html#track-changes-to-the-data",
    "href": "tools/wb_dvc_slides.html#track-changes-to-the-data",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Track changes to the data",
    "text": "Track changes to the data\nLet’s make some change to the data:\nrm data/raw/val/n03445777/ILSVRC2012_val*\n\nRemember that Git is not tracking the data:\ngit status\nOn branch main\nnothing to commit, working tree clean\n\n\nBut DVC is:\ndvc status\ndata/raw/val.dvc:\n    changed outs:\n            modified:           data/raw/val"
  },
  {
    "objectID": "tools/wb_dvc_slides.html#add-changes-to-dvc",
    "href": "tools/wb_dvc_slides.html#add-changes-to-dvc",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Add changes to DVC",
    "text": "Add changes to DVC\ndvc add data/raw/val\ndvc status\nData and pipelines are up to date.\n\nNow we need to commit the changes to the .dvc file to Git:\ngit status\nOn branch main\nChanges to be committed:\n    modified:   data/raw/val.dvc\n\nStaging happened automatically because I have set the autostage option to true on my system\n\ngit commit -m \"Delete data/raw/val/n03445777/ILSVRC2012_val*\""
  },
  {
    "objectID": "tools/wb_dvc_slides.html#check-out-older-versions",
    "href": "tools/wb_dvc_slides.html#check-out-older-versions",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Check out older versions",
    "text": "Check out older versions\nWhat if we want to go back to the 1st version of our data?\nFor this, we first use Git to checkout the proper commit, then run dvc checkout to have the data catch up to the .dvc file\nTo avoid forgetting to run the commands that will make DVC catch up to Git, we can automate this process by installing Git hooks:\ndvc install"
  },
  {
    "objectID": "tools/wb_dvc_slides.html#git-workflows",
    "href": "tools/wb_dvc_slides.html#git-workflows",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Git workflows",
    "text": "Git workflows\ngit checkout is ok to have a look, but a detached HEAD is not a good place to create new commits\nLet’s create a new branch and switch to it:\ngit switch -c alternative\nSwitched to a new branch 'alternative'\nGoing back and forth between both versions of our data is now as simple as switching branch:\ngit switch main\ngit switch alternative"
  },
  {
    "objectID": "tools/wb_dvc_slides.html#classic-workflow",
    "href": "tools/wb_dvc_slides.html#classic-workflow",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Classic workflow",
    "text": "Classic workflow\nThe Git project (including .dvc files) go to a Git remote (GitHub/GitLab/Bitbucket/server)\nThe data go to a DVC remote (AWS/Azure/Google Drive/server/etc.)"
  },
  {
    "objectID": "tools/wb_dvc_slides.html#dvc-remotes",
    "href": "tools/wb_dvc_slides.html#dvc-remotes",
    "title": "Version control for data science & machine learning with DVC",
    "section": "DVC remotes",
    "text": "DVC remotes\nDVC can use many cloud storage or remote machines/server via SSH, WebDAV, etc.\nLet’s create a local remote here:\n# Create a directory outside the project\nmkdir ../remote\n\n# Setup default (-d) remote\ndvc remote add -d local_remote ../remote\nSetting 'local_remote' as a default remote.\ncat .dvc/config\n[core]\n    remote = local_remote\n['remote \"local_remote\"']\n    url = ../../remote"
  },
  {
    "objectID": "tools/wb_dvc_slides.html#commit-remote-config",
    "href": "tools/wb_dvc_slides.html#commit-remote-config",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Commit remote config",
    "text": "Commit remote config\nThe new remote configuration should be committed:\ngit status\nOn branch alternative\n\nChanges not staged for commit:\n    modified:   .dvc/config\ngit add .\ngit commit -m \"Config remote\""
  },
  {
    "objectID": "tools/wb_dvc_slides.html#push-to-remotes",
    "href": "tools/wb_dvc_slides.html#push-to-remotes",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Push to remotes",
    "text": "Push to remotes\nLet’s push the data from the cache (.dvc/cache) to the remote:\ndvc push\n2702 files pushed\n\nWith Git hooks installed, dvc push is automatically run after git push\n(But the data is pushed to the DVC remote while the files tracked by Git get pushed to the Git remote)\n\nBy default, the entire data cache gets pushed to the remote, but there are many options\n\nExample: only push data corresponding to a certain .dvc files\ndvc push data/raw/val.dvc"
  },
  {
    "objectID": "tools/wb_dvc_slides.html#pull-from-remotes",
    "href": "tools/wb_dvc_slides.html#pull-from-remotes",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Pull from remotes",
    "text": "Pull from remotes\ndvc fetch downloads data from the remote into the cache. To have it update the working directory, follow by dvc checkout\nYou can do these 2 commands at the same time with dvc pull"
  },
  {
    "objectID": "tools/wb_dvc_slides.html#dvc-pipelines",
    "href": "tools/wb_dvc_slides.html#dvc-pipelines",
    "title": "Version control for data science & machine learning with DVC",
    "section": "DVC pipelines",
    "text": "DVC pipelines\nDVC pipelines create reproducible workflows and are functionally similar to Makefiles\nEach step in a pipeline is created with dvc stage add and add an entry to a dvc.yaml file\n\ndvc stage add options:\n-n: name of stage\n-d: dependency\n-o: output\n\n\nEach stage contains:\n\ncmd: the command executed\ndeps: the dependencies\nouts: the outputs\n\nThe file is then used to visualize the pipeline and run it"
  },
  {
    "objectID": "tools/wb_dvc_slides.html#example",
    "href": "tools/wb_dvc_slides.html#example",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Example",
    "text": "Example\nLet’s create a pipeline to run a classifier on our data\nThe pipeline contains 3 steps:\n\nprepare\ntrain\nevaluate"
  },
  {
    "objectID": "tools/wb_dvc_slides.html#create-a-pipeline",
    "href": "tools/wb_dvc_slides.html#create-a-pipeline",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Create a pipeline",
    "text": "Create a pipeline\n1st stage (data preparation):\ndvc stage add -n prepare -d src/prepare.py -d data/raw \\\n    -o data/prepared/train.csv -o data/prepared/test.csv \\\n    python src/prepare.py\nAdded stage 'prepare' in 'dvc.yaml'\n\n2nd stage (training)\ndvc stage add -n train -d src/train.py -d data/prepared/train.csv \\\n    -o model/model.joblib \\\n    python src/train.py\nAdded stage `train` in 'dvc.yaml'\n\n\n3rd stage (evaluation)\ndvc stage add -n evaluate -d src/evaluate.py -d model/model.joblib \\\n    -M metrics/accuracy.json \\\n    python src/evaluate.py\nAdded stage `evaluate` in 'dvc.yaml'"
  },
  {
    "objectID": "tools/wb_dvc_slides.html#commit-pipeline",
    "href": "tools/wb_dvc_slides.html#commit-pipeline",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Commit pipeline",
    "text": "Commit pipeline\ngit commit -m \"Define pipeline\"\nprepare:\n    changed deps:\n            modified:           data/raw\n            modified:           src/prepare.py\n    changed outs:\n            deleted:            data/prepared/test.csv\n            deleted:            data/prepared/train.csv\ntrain:\n    changed deps:\n            deleted:            data/prepared/train.csv\n            modified:           src/train.py\n    changed outs:\n            deleted:            model/model.joblib\nevaluate:\n    changed deps:\n            deleted:            model/model.joblib\n            modified:           src/evaluate.py\n    changed outs:\n            deleted:            metrics/accuracy.json\n[main 4aa331b] Define pipeline\n 3 files changed, 27 insertions(+)\n create mode 100644 data/prepared/.gitignore\n create mode 100644 dvc.yaml\n create mode 100644 model/.gitignore"
  },
  {
    "objectID": "tools/wb_dvc_slides.html#visualize-pipeline-in-a-dag",
    "href": "tools/wb_dvc_slides.html#visualize-pipeline-in-a-dag",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Visualize pipeline in a DAG",
    "text": "Visualize pipeline in a DAG\ndvc dag\n+--------------------+         +------------------+\n| data/raw/train.dvc |         | data/raw/val.dvc |\n+--------------------+         +------------------+\n                  ***           ***\n                     **       **\n                       **   **\n                    +---------+\n                    | prepare |\n                    +---------+\n                          *\n                          *\n                          *\n                      +-------+\n                      | train |\n                      +-------+\n                          *\n                          *\n                          *\n                    +----------+\n                    | evaluate |\n                    +----------+"
  },
  {
    "objectID": "tools/wb_dvc_slides.html#run-pipeline",
    "href": "tools/wb_dvc_slides.html#run-pipeline",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Run pipeline",
    "text": "Run pipeline\ndvc repro\n'data/raw/train.dvc' didn't change, skipping\n'data/raw/val.dvc' didn't change, skipping\nRunning stage 'prepare':\n&gt; python src/prepare.py\nGenerating lock file 'dvc.lock'\nUpdating lock file 'dvc.lock'\n\nRunning stage 'train':\n&gt; python src/train.py\nUpdating lock file 'dvc.lock'\n\nRunning stage 'evaluate':\n&gt; python src/evaluate.py\nUpdating lock file 'dvc.lock'\nUse `dvc push` to send your updates to remote storage."
  },
  {
    "objectID": "tools/wb_dvc_slides.html#dvc-repro-breakdown",
    "href": "tools/wb_dvc_slides.html#dvc-repro-breakdown",
    "title": "Version control for data science & machine learning with DVC",
    "section": "dvc repro breakdown",
    "text": "dvc repro breakdown\n\ndvc repro runs the dvc.yaml file in a Makefile fashion\nFirst, it looks at the dependencies: the data didn’t change\nThen it ran the commands to produce the outputs (since it is our first run, we had no outputs)\nWhen the 1st stage is run, a dvc.lock is created with information on that part of the run\nWhen the 2nd and 3rd stages are run, dvc.lock is updated\nAt the end of the run dvc.lock contains all the info about the run we just did (version of the data used, etc.)\nA new directory called runs is created in .dvc/cache with cached data for this run"
  },
  {
    "objectID": "tools/wb_dvc_slides.html#results-of-the-run",
    "href": "tools/wb_dvc_slides.html#results-of-the-run",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Results of the run",
    "text": "Results of the run\n\nThe prepared data was created in data/prepared (with a .gitignore to exclude it from Git—you don’t want to track results in Git, but the scripts that can reproduce them)\nA model was saved in model (with another .gitignore file)\nThe accuracy of this run was created in metrics"
  },
  {
    "objectID": "tools/wb_dvc_slides.html#clean-working-tree-1",
    "href": "tools/wb_dvc_slides.html#clean-working-tree-1",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Clean working tree",
    "text": "Clean working tree\nNow, we definitely want to create a commit with the dvc.lock\nWe could add the metrics resulting from this run in the same commit:\ngit add metrics\ngit commit -m \"First pipeline run and results\"\n\nOur working tree is now clean and our data/pipeline up to date:\ngit status\nOn branch alternative\nnothing to commit, working tree clean\ndvc status\nData and pipelines are up to date."
  },
  {
    "objectID": "tools/wb_dvc_slides.html#modify-pipeline",
    "href": "tools/wb_dvc_slides.html#modify-pipeline",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Modify pipeline",
    "text": "Modify pipeline\nFrom now on, if we edit one of the scripts, or one of the dependencies, dvc status will tell us what changed and dvc repro will only rerun the parts of the pipeline to update the result, pretty much as a Makefile would"
  },
  {
    "objectID": "tools/wb_dvc_slides.html#going-further-next-time",
    "href": "tools/wb_dvc_slides.html#going-further-next-time",
    "title": "Version control for data science & machine learning with DVC",
    "section": "Going further … next time",
    "text": "Going further … next time\n DVC is a sophisticated tool with many additional features:\n\nCreation of data registries\nDVCLive\nA Python library to log experiment metrics\nVisualize the performance logs as plots\nContinuous integration\nWith the sister project CML (Continuous Machine Learning)"
  },
  {
    "objectID": "tools/wb_help_slides.html#when-you-are-stuck-1",
    "href": "tools/wb_help_slides.html#when-you-are-stuck-1",
    "title": "So, you are stuck … now what?",
    "section": "When you are stuck",
    "text": "When you are stuck\n\nFirst, look for information that is already out there\n\n\nThen, ask for help"
  },
  {
    "objectID": "tools/wb_help_slides.html#look-for-information",
    "href": "tools/wb_help_slides.html#look-for-information",
    "title": "So, you are stuck … now what?",
    "section": "Look for information",
    "text": "Look for information\n\nRead carefully any error message\nRead the documentation (local or online)\nMake sure you have up-to-date versions\nGoogle (using carefully selected keywords or the error message)\nLook for open issues & bug reports"
  },
  {
    "objectID": "tools/wb_help_slides.html#error-messages",
    "href": "tools/wb_help_slides.html#error-messages",
    "title": "So, you are stuck … now what?",
    "section": "Error messages",
    "text": "Error messages\nRead them!\nFamiliarise yourself with the error types in the languages you use\n\nExample: Python’s syntax errors vs exceptions\n\nWarnings ≠ errors\nLook for bits you understand (don’t get put off by what you don’t understand)\nIdentify the locations of the errors to go investigate that part of the code"
  },
  {
    "objectID": "tools/wb_help_slides.html#documentation",
    "href": "tools/wb_help_slides.html#documentation",
    "title": "So, you are stuck … now what?",
    "section": "Documentation",
    "text": "Documentation\n\nYou need to find it\n\n\nYou need to understand it"
  },
  {
    "objectID": "tools/wb_help_slides.html#finding-documentation",
    "href": "tools/wb_help_slides.html#finding-documentation",
    "title": "So, you are stuck … now what?",
    "section": "Finding documentation",
    "text": "Finding documentation\n\nOnline:\nTake the time to look for the official documentation & other high quality sources for the languages & tools you use.\n\n\n\nExamples:\nPython: Reference manual, Standard library manual, Tutorial\nNumPy: Tutorial\nR: Open source book “R for Data Science”, Open source book “Advanced R”\nJulia: Documentation\nBash: Manual\nGit: Manual, Open source book\n\n\n\nIn the program itself\n\n\nUnderstanding the documentation"
  },
  {
    "objectID": "tools/wb_help_slides.html#up-to-date-versions",
    "href": "tools/wb_help_slides.html#up-to-date-versions",
    "title": "So, you are stuck … now what?",
    "section": "Up-to-date versions",
    "text": "Up-to-date versions\n\nFirst, you need to know what needs to be updated.\n\n\nKeeping a system up to date includes updating:\n\nthe OS\nthe program\n(any potential IDE)\npackages\n\n\n\nThen, you need to update regularly."
  },
  {
    "objectID": "tools/wb_help_slides.html#google",
    "href": "tools/wb_help_slides.html#google",
    "title": "So, you are stuck … now what?",
    "section": "Google",
    "text": "Google\nGoogle’s algorithms are great at guessing what we are looking for.\n\nBut there is a frequency problem:\nSearches relating to programming-specific questions represent too small a fraction of the overall searches for results to be relevant unless you use key vocabulary.\n\n\nBe precise.\n\n\nLearn the vocabulary of your language/tool to know what to search for."
  },
  {
    "objectID": "tools/wb_help_slides.html#open-issues-bug-reports",
    "href": "tools/wb_help_slides.html#open-issues-bug-reports",
    "title": "So, you are stuck … now what?",
    "section": "Open issues & bug reports",
    "text": "Open issues & bug reports\nIf the tool you are using is open source, look for issues matching your problem in the source repository (e.g. on GitHub or GitLab)."
  },
  {
    "objectID": "tools/wb_help_slides.html#what-if-the-answer-isnt-out-there",
    "href": "tools/wb_help_slides.html#what-if-the-answer-isnt-out-there",
    "title": "So, you are stuck … now what?",
    "section": "What if the answer isn’t out there?",
    "text": "What if the answer isn’t out there?\nWhen everything has failed & you have to ask for help, you need to know:\n\n\nWhere to ask\n\n\n\n\nHow to ask"
  },
  {
    "objectID": "tools/wb_help_slides.html#where-to-ask-1",
    "href": "tools/wb_help_slides.html#where-to-ask-1",
    "title": "So, you are stuck … now what?",
    "section": "Where to ask",
    "text": "Where to ask\nQ&A sites\nMostly, Stack Overflow & the Stack Exchange network.\nCo-founded in 2008 & 2009 by Jeff Atwood & Joel Spolsky.\nForums\nMostly, Discourse.\nCo-founded in 2013 by Jeff Atwood, Robin Ward & Sam Saffron.\nA few other older forums."
  },
  {
    "objectID": "tools/wb_help_slides.html#where-to-ask-2",
    "href": "tools/wb_help_slides.html#where-to-ask-2",
    "title": "So, you are stuck … now what?",
    "section": "Where to ask",
    "text": "Where to ask\nWhich one to choose is a matter of personal preference.\nPossible considerations:\n\nSome niche topics have very active communities on Discourse\nStack Overflow & some older forums can be intimidating with higher expectations for the questions quality & a more direct handling of mistakes\nFor conversations, advice, or multiple step questions, go to Discourse\nStack Overflow has over 13 million users\nStack Overflow & co have a very efficient approach"
  },
  {
    "objectID": "tools/wb_help_slides.html#stack-overflow-co",
    "href": "tools/wb_help_slides.html#stack-overflow-co",
    "title": "So, you are stuck … now what?",
    "section": "Stack Overflow & co",
    "text": "Stack Overflow & co\nPick the best site to ask your question.\nA few of the Stack Exchange network sites:\nStack Overflow: programming\nSuper User: computer hardware & software\nUnix & Linux: *nix OS TEX: TeX/LaTeX\nCross Validated: stats; data mining, collecting, analysis & visualization; ML\nData Science: focus on implementation & processes\nOpen Data\nGIS"
  },
  {
    "objectID": "tools/wb_help_slides.html#how-to-ask-1",
    "href": "tools/wb_help_slides.html#how-to-ask-1",
    "title": "So, you are stuck … now what?",
    "section": "How to ask",
    "text": "How to ask\n\nFamiliarize yourself with the site by reading posts\n\n\nRead the “Tour” page (SO/SE) or take the “New user tutorial” (Discourse)\n\n\nMake sure the question has not already been asked\n\n\nFormat the question properly\n\n\nGive a minimum reproducible example\n\n\nDo not share sensitive data\n\n\nShow your attempts\n\n\nAvoid cross-posting. If you really have to, make sure to cross-reference"
  },
  {
    "objectID": "tools/wb_help_slides.html#how-to-ask-so-co",
    "href": "tools/wb_help_slides.html#how-to-ask-so-co",
    "title": "So, you are stuck … now what?",
    "section": "How to ask: SO & co",
    "text": "How to ask: SO & co\n\nDon’t ask opinion-based questions\n\n\nDon’t ask for package, tool, or service recommendations\n\n\nDon’t ask more than one question in a single post\n\n\nCheck your spelling, grammar, punctuation, capitalized sentences, etc.\n\n\nAvoid greetings, signatures, thank-yous; keep it to the point\n\n\nAvoid apologies about being a beginner, this being your first post, the question being stupid, etc: do the best you can & skip the personal, self-judgmental & irrelevant bits"
  },
  {
    "objectID": "tools/wb_help_slides.html#formatting-your-question",
    "href": "tools/wb_help_slides.html#formatting-your-question",
    "title": "So, you are stuck … now what?",
    "section": "Formatting your question",
    "text": "Formatting your question\nNowadays, most sites (including Stack Overflow & Discourse) allow markdown rendering.\nSome older forums implement other markup languages (e.g. BBCode).\nThe information is always easy to find. Spend the time to format your question properly. People will be much less inclined to help you if you don’t show any effort & if your question is a nightmare to read."
  },
  {
    "objectID": "tools/wb_help_slides.html#example-of-a-typical-downvoted-question",
    "href": "tools/wb_help_slides.html#example-of-a-typical-downvoted-question",
    "title": "So, you are stuck … now what?",
    "section": "Example of a typical downvoted question",
    "text": "Example of a typical downvoted question\nCode:\nhowdy!!\ni am new to R sorry for a very silly question.i looked all oever the itnernwet, but i dint find\nanyanswer. i tried to use ggplot i get the error: Error in loadNamespace(i, c(lib.loc, .libPaths()),\nversionCheck = vI[[i]]) : there is no package called 'stringi'\nthank youu very much!!!!!\nmarie\nRendered output:"
  },
  {
    "objectID": "tools/wb_help_slides.html#same-question-fixed",
    "href": "tools/wb_help_slides.html#same-question-fixed",
    "title": "So, you are stuck … now what?",
    "section": "Same question, fixed",
    "text": "Same question, fixed\nWhen I try to load the package `ggplot2` with:\n\n```{r}\nlibrary(ggplot2)\n```\nI get the error:\n\n&gt; Error in loadNamespace(i, c(lib.loc, .libPaths()), versionCheck = vI[[i]]) :\nthere is no package called 'stringi'\n\nWhat am I doing wrong?"
  },
  {
    "objectID": "tools/wb_help_slides.html#still-not-good-enough",
    "href": "tools/wb_help_slides.html#still-not-good-enough",
    "title": "So, you are stuck … now what?",
    "section": "Still not good enough",
    "text": "Still not good enough\nThis question is actually a duplicate of a question asked which is itself a duplicate of another question."
  },
  {
    "objectID": "tools/wb_help_slides.html#creating-a-minimal-reproducible-example",
    "href": "tools/wb_help_slides.html#creating-a-minimal-reproducible-example",
    "title": "So, you are stuck … now what?",
    "section": "Creating a minimal reproducible example",
    "text": "Creating a minimal reproducible example\nThere are great posts on how to create a good minimal reproducible example. In particular:\nHow to create a Minimal, Reproducible Example\nFor R (but concepts apply to any language):\nHow to make a great R reproducible example\nWhat’s a reproducible example (reprex) and how do I do one?"
  },
  {
    "objectID": "tools/wb_help_slides.html#creating-a-minimal-reproducible-example-1",
    "href": "tools/wb_help_slides.html#creating-a-minimal-reproducible-example-1",
    "title": "So, you are stuck … now what?",
    "section": "Creating a minimal reproducible example",
    "text": "Creating a minimal reproducible example\n\nLoad all necessary packages\nLoad or create necessary data\nSimplify the data & the code as much as possible while still reproducing the problem\nUse simple variable names"
  },
  {
    "objectID": "tools/wb_help_slides.html#data-for-your-example-your-own-data",
    "href": "tools/wb_help_slides.html#data-for-your-example-your-own-data",
    "title": "So, you are stuck … now what?",
    "section": "Data for your example: your own data",
    "text": "Data for your example: your own data\nDo not upload data somewhere on the web to be downloaded.\nMake sure that the data is anonymised.\nDon’t keep more variables & more data points than are necessary to reproduce the problem.\nSimplify the variable names.\nIn R, you can use functions such as dput() to turn your reduced, anonymised data into text that is easy to copy/paste & can then be used to recreate the data."
  },
  {
    "objectID": "tools/wb_help_slides.html#data-for-your-example-create-a-toy-dataset",
    "href": "tools/wb_help_slides.html#data-for-your-example-create-a-toy-dataset",
    "title": "So, you are stuck … now what?",
    "section": "Data for your example: create a toy dataset",
    "text": "Data for your example: create a toy dataset\nYou can also create a toy dataset.\nFunctions that create random data, series, or repetitions are very useful here."
  },
  {
    "objectID": "tools/wb_help_slides.html#data-for-your-example-pre-packaged-datasets",
    "href": "tools/wb_help_slides.html#data-for-your-example-pre-packaged-datasets",
    "title": "So, you are stuck … now what?",
    "section": "Data for your example: pre-packaged datasets",
    "text": "Data for your example: pre-packaged datasets\nSome languages/packages come with pre-packaged datasets. If your code involves such languages/packages, you can make use of these datasets to create your reproducible example.\nFor example, R comes with many datasets directly available, including iris, mtcars, trees, airquality. In the R console, try:\n?iris\n?mtcars"
  },
  {
    "objectID": "tools/wb_help_slides.html#additional-considerations",
    "href": "tools/wb_help_slides.html#additional-considerations",
    "title": "So, you are stuck … now what?",
    "section": "Additional considerations",
    "text": "Additional considerations\nEven if you always find answers to your questions without having to post yourself, consider signing up to these sites:\n\nIt allows you to upvote (SO/SE) or like (Discourse) the questions & answers that help you—and why not thank in this fashion those that are making your life easier?\nIt makes you a part of these communities.\nOnce you are signed up, maybe you will start being more involved & contribute with questions & answers of your own."
  },
  {
    "objectID": "tools/wb_help_slides.html#a-last-word",
    "href": "tools/wb_help_slides.html#a-last-word",
    "title": "So, you are stuck … now what?",
    "section": "A last word",
    "text": "A last word\nWhile it takes some work to ask a good question, do not let this discourage you from posting on Stack Overflow: if you ask a good question, you will get many great answers.\nYou will learn in the process of developing your question (you may actually find the answer in that process) & you will learn from the answers.\nIt is forth the effort.\nHere is the Stack Overflow documentation on how to ask a good question."
  },
  {
    "objectID": "tools/wb_lazygit_slides.html#git-interfaces",
    "href": "tools/wb_lazygit_slides.html#git-interfaces",
    "title": "A great Git UI: lazygit",
    "section": "Git interfaces",
    "text": "Git interfaces\nThere are 3 main ways to use Git:\n\nThrough a Git GUI\nFrom the command line\nIntegrated within IDE"
  },
  {
    "objectID": "tools/wb_lazygit_slides.html#git-interfaces-1",
    "href": "tools/wb_lazygit_slides.html#git-interfaces-1",
    "title": "A great Git UI: lazygit",
    "section": "Git interfaces",
    "text": "Git interfaces\nThey all have downsides:\n\nThrough a Git GUI      ➔  Slow and buggy\nFrom the command line   ➔  Austere and unintuitive\nIntegrated within IDE    ➔  Limited"
  },
  {
    "objectID": "tools/wb_lazygit_slides.html#on-the-beauty-of-tuis",
    "href": "tools/wb_lazygit_slides.html#on-the-beauty-of-tuis",
    "title": "A great Git UI: lazygit",
    "section": "On the beauty of TUIs",
    "text": "On the beauty of TUIs\nTerminal user interfaces (TUIs) were precursors to graphical user interfaces (GUIs), but they did not disappear\nPeople continue to build TUIs because they uniquely provide the speed of the command line and the easy of use of GUIs\nGitHub is full of sleek, modern, open source TUIs for all sorts of applications\nSeveral of them provide an interface to Git\nMy personal TUIs of choice are ranger as file manager and lazygit for Git"
  },
  {
    "objectID": "tools/wb_lazygit_slides.html#lazygit",
    "href": "tools/wb_lazygit_slides.html#lazygit",
    "title": "A great Git UI: lazygit",
    "section": "lazygit",
    "text": "lazygit\nWith over 52k stars on GitHub, lazygit, created and maintained by Jesse Duffield is probably the most polished Git TUI\nI followed it as it grew and developed over the past 5 years. It was great from the start, but by now, it is a truly beautiful mature tool\nIt is cross-platform. You can find installation instructions in the README"
  },
  {
    "objectID": "tools/wb_lazygit_slides.html#lazygit-1",
    "href": "tools/wb_lazygit_slides.html#lazygit-1",
    "title": "A great Git UI: lazygit",
    "section": "lazygit",
    "text": "lazygit\nGet command options:\nlazygit -h\nPrint default configurations with:\nlazygit -c\n\nlazygit is fully customizable"
  },
  {
    "objectID": "tools/wb_lazygit_slides.html#resources",
    "href": "tools/wb_lazygit_slides.html#resources",
    "title": "A great Git UI: lazygit",
    "section": "Resources",
    "text": "Resources\n\nRepo\nDefault kbds\nConfiguration options"
  },
  {
    "objectID": "tools/wb_lazygit_slides.html#time-for-a-demo",
    "href": "tools/wb_lazygit_slides.html#time-for-a-demo",
    "title": "A great Git UI: lazygit",
    "section": "Time for a demo!",
    "text": "Time for a demo!\nI will spend the rest of this webinar showing you how to use Git through lazygit"
  },
  {
    "objectID": "tools/wb_quarto_slides.html#markup-languages",
    "href": "tools/wb_quarto_slides.html#markup-languages",
    "title": "The new R Markdown:",
    "section": "Markup languages",
    "text": "Markup languages\n\nControl the formatting of text documents\nPowerful but the unrendered text is visually cluttered and hard to read"
  },
  {
    "objectID": "tools/wb_quarto_slides.html#markup-languages-1",
    "href": "tools/wb_quarto_slides.html#markup-languages-1",
    "title": "The new R Markdown:",
    "section": "Markup languages",
    "text": "Markup languages\n\nControl the formatting of text documents\nPowerful but the unrendered text is visually cluttered and hard to read\n\n\nExample: Tex—often with macro package LaTeX—to create pdfs\n\n\\documentclass{article}\n\\title{My title}\n\\author{My name}\n\\usepackage{datetime}\n\\newdate{date}{24}{11}{2022}\n\\date{\\displaydate{date}}\n\\begin{document}\n \\maketitle\n \\section{First section}\n Some text in the first section.\n\\end{document}"
  },
  {
    "objectID": "tools/wb_quarto_slides.html#markup-languages-2",
    "href": "tools/wb_quarto_slides.html#markup-languages-2",
    "title": "The new R Markdown:",
    "section": "Markup languages",
    "text": "Markup languages\n\nControl the formatting of text documents\nPowerful but the unrendered text is visually cluttered and hard to read\n\n\nExample: HTML—often with css/scss files—to create webpages\n\n&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en-US\"&gt;\n  &lt;head&gt;\n    &lt;meta charset=\"utf-8\" /&gt;\n    &lt;meta name=\"viewport\" content=\"width=device-width\" /&gt;\n    &lt;title&gt;My title&lt;/title&gt;\n    &lt;address class=\"author\"&gt;My name&lt;/address&gt;\n    &lt;input type=\"date\" value=\"2022-11-24\" /&gt;\n  &lt;/head&gt;\n  &lt;h1&gt;First section&lt;/h1&gt;\n  &lt;body&gt;\n    Some text in the first section.\n  &lt;/body&gt;\n&lt;/html&gt;"
  },
  {
    "objectID": "tools/wb_quarto_slides.html#markdown",
    "href": "tools/wb_quarto_slides.html#markdown",
    "title": "The new R Markdown:",
    "section": "Markdown",
    "text": "Markdown\n\nRemoves the visual clutter and makes texts readable prior to rendering\nCreated in 2004\nBy now quasi-ubiquitous\nInitially created for webpages\nRaw HTML can be inserted when easy syntax falls short\n\n\nPandoc’s extended Markdown\nPandoc (free and open-source markup formats converter) supports an extended Markdown syntax with functionality for figures, tables, callout blocks, LaTeX equations, citations…\nRemains as readable as basic Markdown, but can be rendered in any format (pdf, books, entire websites, Word documents…)"
  },
  {
    "objectID": "tools/wb_quarto_slides.html#markdown-1",
    "href": "tools/wb_quarto_slides.html#markdown-1",
    "title": "The new R Markdown:",
    "section": "Markdown",
    "text": "Markdown\n\nRemoves the visual clutter and makes texts readable prior to rendering\nCreated in 2004\nBy now quasi-ubiquitous\nInitially created for webpages\nRaw HTML can be inserted when easy syntax falls short\n\n\nPrevious example using Pandoc’s Markdown:\n\n---\ntitle: My title\nauthor: My name\ndate: 2022-11-24\n---\n# First section\nSome text in the first section."
  },
  {
    "objectID": "tools/wb_quarto_slides.html#how-it-works",
    "href": "tools/wb_quarto_slides.html#how-it-works",
    "title": "The new R Markdown:",
    "section": "How it works",
    "text": "How it works\nCode blocks are executed by Jupyter (Python or Julia) or knitr (R), then pandoc renders the document into any format\n\nJulia/Python:\n From Quarto documentation\nR:\n From Quarto documentation"
  },
  {
    "objectID": "tools/wb_quarto_slides.html#how-it-works-1",
    "href": "tools/wb_quarto_slides.html#how-it-works-1",
    "title": "The new R Markdown:",
    "section": "How it works",
    "text": "How it works\nCode blocks are executed by Jupyter (Python or Julia) or knitr (R), then pandoc renders the document into any format\nCan be used from .qmd text files or directly from RStudio or Jupyter notebooks."
  },
  {
    "objectID": "tools/wb_quarto_slides.html#supported-languages",
    "href": "tools/wb_quarto_slides.html#supported-languages",
    "title": "The new R Markdown:",
    "section": "Supported languages",
    "text": "Supported languages\nSyntax highlighting in pretty much any language\n\nExecutable code blocks in Python, R, Julia, Observable JS\n\n\nOutput formats\n- HTML\n- PDF\n- MS Word\n- OpenOffice\n- ePub\n- Revealjs\n- PowerPoint\n- Beamer\n- GitHub Markdown\n- CommonMark\n- Hugo\n- Docusaurus\n- Markua\n- MediaWiki\n- DokuWiki\n- ZimWiki\n- Jira Wiki\n- XWiki\n- JATS\n- Jupyter\n- ConTeXt\n- RTF\n- reST\n- AsciiDoc\n- Org-Mode\n- Muse\n- GNU\n- Groff"
  },
  {
    "objectID": "tools/wb_quarto_slides.html#document-structure-syntax-front-matter",
    "href": "tools/wb_quarto_slides.html#document-structure-syntax-front-matter",
    "title": "The new R Markdown:",
    "section": "Document structure & syntax: front matter",
    "text": "Document structure & syntax: front matter\nWritten in YAML\nSets the options for the document. Let’s see a few examples.\n\n\nCan be very basic:\n\n---\ntitle: \"My title\"\nauthor: \"My name\"\nformat: html\n---"
  },
  {
    "objectID": "tools/wb_quarto_slides.html#document-structure-syntax-front-matter-1",
    "href": "tools/wb_quarto_slides.html#document-structure-syntax-front-matter-1",
    "title": "The new R Markdown:",
    "section": "Document structure & syntax: front matter",
    "text": "Document structure & syntax: front matter\nWritten in YAML\nSets the options for the document. Let’s see a few examples.\n\nOr more sophisticated:\n\n---\ntitle: \"Some title\"\nsubtitle: \"Some subtitle\"\ninstitute: \"Simon Fraser University\"\ndate: \"2022-11-24\"\nexecute:\n  error: true\n  echo: true\nformat:\n  revealjs:\n    theme: [default, custom.scss]\n    highlight-style: monokai\n    code-line-numbers: false\n    embed-resources: true\n---"
  },
  {
    "objectID": "tools/wb_quarto_slides.html#document-structure-syntax-text",
    "href": "tools/wb_quarto_slides.html#document-structure-syntax-text",
    "title": "The new R Markdown:",
    "section": "Document structure & syntax: text",
    "text": "Document structure & syntax: text\nWritten in Pandoc’s extended Markdown"
  },
  {
    "objectID": "tools/wb_quarto_slides.html#document-structure-syntax-code-blocks",
    "href": "tools/wb_quarto_slides.html#document-structure-syntax-code-blocks",
    "title": "The new R Markdown:",
    "section": "Document structure & syntax: code blocks",
    "text": "Document structure & syntax: code blocks\nSyntax highlighting only:\n{.language} code\n\nSyntax highlighting and code execution:\n```{language}\ncode\n```\n\n\nOptions can be added to individual blocks:\n```{language}\n#| option: value\n\ncode\n```"
  },
  {
    "objectID": "tools/wb_quarto_slides.html#rendering",
    "href": "tools/wb_quarto_slides.html#rendering",
    "title": "The new R Markdown:",
    "section": "Rendering",
    "text": "Rendering\nTwo commands:\nquarto render file.qmd     # Renders the document\nquarto preview file.qmd    # Displays a live preview"
  },
  {
    "objectID": "tools/wb_quarto_slides.html#some-advantages-of-quarto",
    "href": "tools/wb_quarto_slides.html#some-advantages-of-quarto",
    "title": "The new R Markdown:",
    "section": "Some advantages of Quarto",
    "text": "Some advantages of Quarto\nGeneral considerations\n\nExtremely well documented\nSolid team behind the work\nFree and open source\nUses only well established and well tested tools"
  },
  {
    "objectID": "tools/wb_quarto_slides.html#some-advantages-of-quarto-1",
    "href": "tools/wb_quarto_slides.html#some-advantages-of-quarto-1",
    "title": "The new R Markdown:",
    "section": "Some advantages of Quarto",
    "text": "Some advantages of Quarto\nWebpages/websites\n\nFast, easy, and clean\nSites work on screens of any size out of the box (uses Bootstrap 5)\nCan be customized with CSS/SCSS, but good out of the box\nCode blocks can have a copy button\nGreat search functionality\nSite/pages can be hosted anywhere easily"
  },
  {
    "objectID": "tools/wb_quarto_slides.html#some-advantages-of-quarto-2",
    "href": "tools/wb_quarto_slides.html#some-advantages-of-quarto-2",
    "title": "The new R Markdown:",
    "section": "Some advantages of Quarto",
    "text": "Some advantages of Quarto\nAdvantages of code execution\n\nPeople can see code outputs without running code\nForces to test every bit of code\nNo need for a complex system linking code scripts with publishing documents"
  },
  {
    "objectID": "tools/wb_quarto_slides.html#resources",
    "href": "tools/wb_quarto_slides.html#resources",
    "title": "The new R Markdown:",
    "section": "Resources",
    "text": "Resources\nOfficial sites\nWebsite\nRepo\nDocumentation index\nInstallation\nYou can find information in the Quarto documentation or in our previous workshop on Quarto\nBasic examples\nYou can find several examples in our previous workshop on Quarto"
  },
  {
    "objectID": "tools/wb_tools2.html",
    "href": "tools/wb_tools2.html",
    "title": "A few more of our favourite tools",
    "section": "",
    "text": "In a previous webinar, we presented three of our favourite command line tools. Today, we will introduce other tools we find really useful in our daily workflow:\n\nlazygit: a wonderful terminal UI for Git,\nbat: a great syntax highlighter,\nripgrep: a fast alternative to grep,\nfd: a /really/ fast alternative to find,\npass: a command line password manager.\n\nAlong the way, I will use a few other neat command line tools such as hyperfine—for sophisticated benchmarking—and diff-so-fancy—which makes your diffs a lot more readable.\nFor the Emacs users among you, we will finish the workshop with two Emacs utilities:\n\nTRAMP: a remote file access system,\nHelm: a “framework for incremental completions and narrowing selections”.",
    "crumbs": [
      "Tools",
      "<b><em>Webinars</em></b>",
      "More command line tools"
    ]
  },
  {
    "objectID": "tools/wb_tools3_slides.html#how-to-choose-tools",
    "href": "tools/wb_tools3_slides.html#how-to-choose-tools",
    "title": "Modern shell utilities",
    "section": "How to choose tools?",
    "text": "How to choose tools?\n\nPopularity (GitHub stars)\nIs it maintained? (date of last commit)\nHow polished is the documentation?\nHow fast is it? (what language is it written in?)\n\nShell/Python will be slower\nCompiled languages (Rust, C, Go) will be faster"
  },
  {
    "objectID": "tools/wb_tools3_slides.html#what-is-eza",
    "href": "tools/wb_tools3_slides.html#what-is-eza",
    "title": "Modern shell utilities",
    "section": "What is eza?",
    "text": "What is eza?\neza is a replacement for ls\n\nAdds colours\nBetter default options\nAdd tree feature"
  },
  {
    "objectID": "tools/wb_tools3_slides.html#installation",
    "href": "tools/wb_tools3_slides.html#installation",
    "title": "Modern shell utilities",
    "section": "Installation",
    "text": "Installation\nOn your machine\nInstructions here\nOn the Alliance clusters\neza is not installed on the Alliance clusters, so you have to install it locally under your own user. This is easy to do because it is written in Rust and can be installed with the Rust package manager\nLoad a Rust module, install eza, and make sure ~/.cargo/bin is in your path:\nmodule load rust/1.76.0\ncargo install eza\n\nYou only need to do this once. Once installed, eza will be accessible on subsequent sessions"
  },
  {
    "objectID": "tools/wb_tools3_slides.html#usage",
    "href": "tools/wb_tools3_slides.html#usage",
    "title": "Modern shell utilities",
    "section": "Usage",
    "text": "Usage\neza\n➔ Different colours for directories, symlinks, and different types of files and better defaults (compare ls -al with eza -al)\neza by default shows the output in a human readable format and without the group\nThe flags are similar to those of ls with the additional -T, equivalent to running the tree utility:\neza -T python/"
  },
  {
    "objectID": "tools/wb_tools3_slides.html#alias",
    "href": "tools/wb_tools3_slides.html#alias",
    "title": "Modern shell utilities",
    "section": "Alias",
    "text": "Alias\nYou can alias it to ls by adding to your .bashrc or .zshrc file:\nalias ls=eza\nIf you ever want to use the true ls utility, you can do so with \\ls"
  },
  {
    "objectID": "tools/wb_tools3_slides.html#alternative",
    "href": "tools/wb_tools3_slides.html#alternative",
    "title": "Modern shell utilities",
    "section": "Alternative",
    "text": "Alternative\nIf you want a simpler and more lightweight way to add colours to your ls outputs, you can look at LS_COLORS (I did this for years until I found eza)\nTo install it locally in the Alliance clusters, you download and uncompress a script, and copy it to a proper location:\nmkdir ./LS_COLORS &&\n    curl -L https://api.github.com/repos/trapd00r/LS_COLORS/tarball/master |\n        tar xzf - --directory=./LS_COLORS --strip=1 &&\n    mkdir -p ~/.local/share &&\n    cp ~/LS_COLORS/lscolors.sh ~/.local/share &&\n    rm -r ~/LS_COLORS\nThen you add to your .bashrc/.zshrc file the sourcing of the script and an alias to ls:\nsource ~/.local/share/lscolors.sh\nalias ls='ls --color'"
  },
  {
    "objectID": "tools/wb_tools3_slides.html#what-is-bat",
    "href": "tools/wb_tools3_slides.html#what-is-bat",
    "title": "Modern shell utilities",
    "section": "What is bat?",
    "text": "What is bat?\nbat is a replacement for cat\n\nAdds syntax highlighting for most programming languages\nAdds line numbers\nAdds pager-like search\nAdds pager-like navigation"
  },
  {
    "objectID": "tools/wb_tools3_slides.html#installation-1",
    "href": "tools/wb_tools3_slides.html#installation-1",
    "title": "Modern shell utilities",
    "section": "Installation",
    "text": "Installation\nOn your machine\nInstructions here\nOn the Alliance clusters\nbat is already installed on the Alliance clusters"
  },
  {
    "objectID": "tools/wb_tools3_slides.html#usage-1",
    "href": "tools/wb_tools3_slides.html#usage-1",
    "title": "Modern shell utilities",
    "section": "Usage",
    "text": "Usage\nUse bat as you would use cat:\nbat /home/marie/parvus/prog/progpy/pydoc/basics.py\nthen you are in your default pager\nAmong other options, you can disable the frame with -n\nand also remove the line numbers with -p"
  },
  {
    "objectID": "tools/wb_tools3_slides.html#what-is-fd",
    "href": "tools/wb_tools3_slides.html#what-is-fd",
    "title": "Modern shell utilities",
    "section": "What is fd?",
    "text": "What is fd?\nfd is a replacement for find\n\nWritten in Rust, automatic parallelism ➔ with vastly improved performance\nMore friendly syntax\nBy default excludes binaries as well as hidden files and directories\nBy default excludes patterns from .gitignore or other .ignore files"
  },
  {
    "objectID": "tools/wb_tools3_slides.html#installation-2",
    "href": "tools/wb_tools3_slides.html#installation-2",
    "title": "Modern shell utilities",
    "section": "Installation",
    "text": "Installation\nOn your machine\nInstructions here\nOn the Alliance clusters\nfd is already installed on the Alliance clusters"
  },
  {
    "objectID": "tools/wb_tools3_slides.html#basic-usage",
    "href": "tools/wb_tools3_slides.html#basic-usage",
    "title": "Modern shell utilities",
    "section": "Basic usage",
    "text": "Basic usage\nSearch file names for a pattern recursively in current directory:\nfd jx\n\nfd uses regexp by default, so you can use pattern symbols:\nfd jx.*txt\n\n Search file names recursively in another directory:\nfd top bash/"
  },
  {
    "objectID": "tools/wb_tools3_slides.html#print-all-files-in-some-directory-to-stdout",
    "href": "tools/wb_tools3_slides.html#print-all-files-in-some-directory-to-stdout",
    "title": "Modern shell utilities",
    "section": "Print all files in some directory to stdout",
    "text": "Print all files in some directory to stdout\nCurrent directory:\nfd\nAnother directory:\nfd . bash/"
  },
  {
    "objectID": "tools/wb_tools3_slides.html#options",
    "href": "tools/wb_tools3_slides.html#options",
    "title": "Modern shell utilities",
    "section": "Options",
    "text": "Options\nSearch for files with a particular file extension:\nfd -e txt\nUse a globbing pattern instead of regexp:\nfd -g wb* bash/\nExecute command for each result of fd in parallel:\nfd top bash/ -x rg layout\nExecute command once with all results of fd as arguments:\nfd top bash/ -X rg layout"
  },
  {
    "objectID": "tools/wb_tools3_slides.html#excluded-files-and-directories",
    "href": "tools/wb_tools3_slides.html#excluded-files-and-directories",
    "title": "Modern shell utilities",
    "section": "Excluded files and directories",
    "text": "Excluded files and directories\nBy default, fd excludes hidden files/directories and patterns in .gitignore (you can disable this with -H and -I respectively)\nThis makes fd combined with tree sometimes more useful than tree alone\nCompare tree bash/ with:\nfd . bash/ | tree --fromfile\n\nYou can make this a function:\nft () { fd $@ | tree --fromfile }\n\nExclude additional directories or patterns:\nfd -E *.txt -E img/ . bash/"
  },
  {
    "objectID": "tools/wb_tools3_slides.html#my-personal-alias",
    "href": "tools/wb_tools3_slides.html#my-personal-alias",
    "title": "Modern shell utilities",
    "section": "My personal alias",
    "text": "My personal alias\nI prefer to disable the default settings and exclude patterns based on a file I created:\nalias fd='fd -u --ignore-file /home/marie/.fdignore'"
  },
  {
    "objectID": "tools/wb_tools3_slides.html#what-is-ripgrep",
    "href": "tools/wb_tools3_slides.html#what-is-ripgrep",
    "title": "Modern shell utilities",
    "section": "What is ripgrep?",
    "text": "What is ripgrep?\nripgrep provides the rg utility—a replacement for grep\n\nWritten in Rust, automatic parallelism ➔ with vastly improved performance\nBy default excludes patterns from .gitignore or other .ignore files\nBy default excludes binaries as well as hidden files and directories\nBy default doesn’t follow symlinks"
  },
  {
    "objectID": "tools/wb_tools3_slides.html#installation-3",
    "href": "tools/wb_tools3_slides.html#installation-3",
    "title": "Modern shell utilities",
    "section": "Installation",
    "text": "Installation\nOn your machine\nInstructions here\nOn the Alliance clusters\nrg is already installed on the Alliance clusters"
  },
  {
    "objectID": "tools/wb_tools3_slides.html#usage-2",
    "href": "tools/wb_tools3_slides.html#usage-2",
    "title": "Modern shell utilities",
    "section": "Usage",
    "text": "Usage\nSearch lines in a file matching a pattern:\nrg colour /home/marie/parvus/prog/mint/bash/wb_tools3_slides.qmd\nSearch lines matching pattern in all files in current directory (recursively):\nrg colour\nrg and fd follow the same principles:\n\nUse regexp by default\nUse globbing pattern instead with -g\nSearch recursively by default\nSame excluded files"
  },
  {
    "objectID": "tools/wb_tools3_slides.html#what-is-zoxide",
    "href": "tools/wb_tools3_slides.html#what-is-zoxide",
    "title": "Modern shell utilities",
    "section": "What is zoxide?",
    "text": "What is zoxide?\nzoxide allows to easily jump to any directory"
  },
  {
    "objectID": "tools/wb_tools3_slides.html#installation-4",
    "href": "tools/wb_tools3_slides.html#installation-4",
    "title": "Modern shell utilities",
    "section": "Installation",
    "text": "Installation\nOn your machine\nInstructions here\n\nfzf (see below) adds cool functionality to it, so you might want to install it as well"
  },
  {
    "objectID": "tools/wb_tools3_slides.html#choose-a-different-command-name",
    "href": "tools/wb_tools3_slides.html#choose-a-different-command-name",
    "title": "Modern shell utilities",
    "section": "Choose a different command name",
    "text": "Choose a different command name\nUse this instead to use the command of your choice (e.g. j and ji)\ninstead of the default z and zi:\neval \"$(zoxide init --cmd j bash)\""
  },
  {
    "objectID": "tools/wb_tools3_slides.html#usage-3",
    "href": "tools/wb_tools3_slides.html#usage-3",
    "title": "Modern shell utilities",
    "section": "Usage",
    "text": "Usage\nType z (or whatever command you chose) instead of cd\nYou can simplify the path to just a few characters\nIf there are multiple locations matching your entry, the algorithm will chose the highest ranking one based on your visit frequency and how recently you visited a path\nThis means that you can visit your usual places with a few key strokes. For less frequent places, add more info\nFinally, if you want to choose amongst all possible options in a completion framework, use zi instead and zoxide will open fzf"
  },
  {
    "objectID": "tools/wb_tools3_slides.html#alternative-1",
    "href": "tools/wb_tools3_slides.html#alternative-1",
    "title": "Modern shell utilities",
    "section": "Alternative",
    "text": "Alternative\nA tool that served me well until someone pointed zoxide to me is autojump\nInstallation\nInstructions here for your machine\nautojump is installed on the Alliance clusters, but you need add to your .bashrc or .zshrc:\n[[ -s $EPREFIX/etc/profile.d/autojump.sh ]] && source $EPREFIX/etc/profile.d/autojump.sh\nUsage\nSimilar to zoxide but you first need to visit directories so that they get entered in a database\nj is a wrapper for autojump, jc jumps to subdirectories of current directory"
  },
  {
    "objectID": "tools/wb_tools3_slides.html#what-is-fzf",
    "href": "tools/wb_tools3_slides.html#what-is-fzf",
    "title": "Modern shell utilities",
    "section": "What is fzf?",
    "text": "What is fzf?\nfzf allows to find elements of any list through incremental completion and fuzzy matching. It can be paired with any number of commands"
  },
  {
    "objectID": "tools/wb_tools3_slides.html#installation-6",
    "href": "tools/wb_tools3_slides.html#installation-6",
    "title": "Modern shell utilities",
    "section": "Installation",
    "text": "Installation\nOn your machine\nInstructions here\nOn the Alliance clusters\nfzf is already installed on the Alliance clusters\nTo get fzf kbds and fuzzy completion in your shell, add to your .bashrc:\neval \"$(fzf --bash)\"\nand/or your .zshrc:\nsource &lt;(fzf --zsh)"
  },
  {
    "objectID": "tools/wb_tools3_slides.html#direct-usage",
    "href": "tools/wb_tools3_slides.html#direct-usage",
    "title": "Modern shell utilities",
    "section": "Direct usage",
    "text": "Direct usage\nIf you run fzf directly, it will search the current directory recursively, do a narrowing selection, and print the result:\nfzf\nYou can make use of fd to remove unnecessary entries:\nexport FZF_DEFAULT_COMMAND='fd -u --ignore-file /home/marie/.fdignore'"
  },
  {
    "objectID": "tools/wb_tools3_slides.html#fzf-kbds",
    "href": "tools/wb_tools3_slides.html#fzf-kbds",
    "title": "Modern shell utilities",
    "section": "fzf kbds",
    "text": "fzf kbds\nThere are 3 default kbds:\n\nCtl+t ➔ paste selected file/dir into the command\nCtl+r ➔ paste selected command from history into the command\nAlt+c ➔ cd into selected dir"
  },
  {
    "objectID": "tools/wb_tools3_slides.html#pipe-to-fzf",
    "href": "tools/wb_tools3_slides.html#pipe-to-fzf",
    "title": "Modern shell utilities",
    "section": "Pipe to fzf",
    "text": "Pipe to fzf\nYou can also pipe the output of any command that returns a list of elements into fzf\nLook for a file/directory:\nls | fzf\n Many flags to select order of entries, type of completion, preview, case-sensitivity, and more\nLook for a running process:\nps -ef | fzf --cycle -i -e +s --tac --reverse"
  },
  {
    "objectID": "tools/wb_tools3_slides.html#what-is-a-tui",
    "href": "tools/wb_tools3_slides.html#what-is-a-tui",
    "title": "Modern shell utilities",
    "section": "What is a TUI?",
    "text": "What is a TUI?\nTerminal user interfaces (TUIs) are the predecessors to graphical user interfaces (GUIs) which are entirely text based and run in terminals\nThey have remained very popular among command line aficionados because they are fast, efficient, powerful, and keyboard-driven, while being friendly and visual\nFantastic modern ones keep being built for tasks as diverse as interfaces to Git, music players, games, emails, dashboards, and, for our purpose here, file system management"
  },
  {
    "objectID": "tools/wb_tools3_slides.html#formerly-most-popular-file-system-tuis",
    "href": "tools/wb_tools3_slides.html#formerly-most-popular-file-system-tuis",
    "title": "Modern shell utilities",
    "section": "Formerly most popular file system TUIs",
    "text": "Formerly most popular file system TUIs\nThere are many file system TUIs and all of them are actually really good. The two most notable ones used to be:\n\nranger\n\nExtremely sophisticated, easy to customize, tons of features\n\nBuilt in Python, it can be slow for operations in directories with thousands of files\n\n\nnnn\n\nMinimalist and very fast (written in C)\n\nNot easy to customize (many customizations require compiling from source). Most functionalities rely on plugins that need to be installed. Not easy to get started with"
  },
  {
    "objectID": "tools/wb_tools3_slides.html#the-new-kid-yazi",
    "href": "tools/wb_tools3_slides.html#the-new-kid-yazi",
    "title": "Modern shell utilities",
    "section": "The new kid: yazi",
    "text": "The new kid: yazi\nyazi is a brand new fs TUI that has quickly become the most popular\nIt is extremely modern, very fast (written in Rust), very well documented, intuitive, easy to customize, and integrates with modern utilities such as fd, rg, zoxide, and fzf out of the box\nOnly at version 0.4, it is not fully mature yet, but it has already more stars on GitHub than ranger and nnn because it combines ease of customization and sophistication with speed"
  },
  {
    "objectID": "tools/wb_tools3_slides.html#alternatives",
    "href": "tools/wb_tools3_slides.html#alternatives",
    "title": "Modern shell utilities",
    "section": "Alternatives",
    "text": "Alternatives\nIn decreasing number of stars on GitHub:\n\nbroot\nsuperfile\nlf\nxplr\nfff (now archived)\nvifm\nmc"
  },
  {
    "objectID": "tools/wb_tools3_slides.html#my-3-favourite-plugins",
    "href": "tools/wb_tools3_slides.html#my-3-favourite-plugins",
    "title": "Modern shell utilities",
    "section": "My 3 favourite plugins",
    "text": "My 3 favourite plugins\nThere are many plugins for Z shell and the (very bloated) Oh My Zsh, but I am sticking to 3 great plugins inspired or directly coming from the Fish shell:\n\nSyntax highlighting\nAutosuggestions\nHistory substring search"
  },
  {
    "objectID": "tools/wb_tools3_slides.html#installation-7",
    "href": "tools/wb_tools3_slides.html#installation-7",
    "title": "Modern shell utilities",
    "section": "Installation",
    "text": "Installation\nAll plugins can be installed (info in their README) or simply Git cloned. zsh-syntax-highlighting is already installed on the Alliance clusters, so you only need to clone the other two:\n# create a directory to store the scripts\nmkdir ~/.zsh_plugins\n# autosuggestions\ngit clone https://github.com/zsh-users/zsh-autosuggestions.git ~/.zsh_plugins/zsh-autosuggestions\n# history substring search\ngit clone https://github.com/zsh-users/zsh-history-substring-search.git ~/.zsh_plugins/zsh-history-substring-search\nThen you need to source them (including zsh-syntax-highlighting), so add to your .zshrc file:\nsource $EPREFIX/usr/share/zsh/site-functions/zsh-syntax-highlighting.zsh\nsource ~/.zsh_plugins/zsh-history-substring-search/zsh-history-substring-search.zsh\nsource ~/.zsh_plugins/zsh-autosuggestions/zsh-autosuggestions.zsh"
  },
  {
    "objectID": "tools/wb_tools3_slides.html#usage-5",
    "href": "tools/wb_tools3_slides.html#usage-5",
    "title": "Modern shell utilities",
    "section": "Usage",
    "text": "Usage\nYou now have syntax highlighting in your shell inputs\nTo use the history substring search, start typing some command\nthen press Alt+p or Alt+n\nIt will cycle through all entries in your history that start that way\nFinally, the autosuggestion will suggest commands based on your history and/or classic suggestions\nAccept the whole command with Ctl+e or a single word with Alt+f"
  }
]