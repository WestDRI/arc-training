<!DOCTYPE html>
<html lang="en"><head>
<link href="..//img/sfu_favicon.png" rel="icon" type="image/png">
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-html/tabby.min.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-html.min.css" rel="stylesheet" data-mode="light">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.2.335">

  <meta name="author" content="Marie-Hélène Burle">
  <meta name="dcterms.date" content="2023-02-14">
  <title>WestDRI - Introduction to machine learning for the humanities</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="../site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="../site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="../site_libs/revealjs/dist/theme/quarto.css" id="theme">
  <link href="../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-pointer/pointer.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">

  .callout {
    margin-top: 1em;
    margin-bottom: 1em;  
    border-radius: .25rem;
  }

  .callout.callout-style-simple { 
    padding: 0em 0.5em;
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
    display: flex;
  }

  .callout.callout-style-default {
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
  }

  .callout .callout-body-container {
    flex-grow: 1;
  }

  .callout.callout-style-simple .callout-body {
    font-size: 1rem;
    font-weight: 400;
  }

  .callout.callout-style-default .callout-body {
    font-size: 0.9rem;
    font-weight: 400;
  }

  .callout.callout-captioned.callout-style-simple .callout-body {
    margin-top: 0.2em;
  }

  .callout:not(.callout-captioned) .callout-body {
      display: flex;
  }

  .callout:not(.no-icon).callout-captioned.callout-style-simple .callout-content {
    padding-left: 1.6em;
  }

  .callout.callout-captioned .callout-header {
    padding-top: 0.2em;
    margin-bottom: -0.2em;
  }

  .callout.callout-captioned .callout-caption  p {
    margin-top: 0.5em;
    margin-bottom: 0.5em;
  }
    
  .callout.callout-captioned.callout-style-simple .callout-content  p {
    margin-top: 0;
  }

  .callout.callout-captioned.callout-style-default .callout-content  p {
    margin-top: 0.7em;
  }

  .callout.callout-style-simple div.callout-caption {
    border-bottom: none;
    font-size: .9rem;
    font-weight: 600;
    opacity: 75%;
  }

  .callout.callout-style-default  div.callout-caption {
    border-bottom: none;
    font-weight: 600;
    opacity: 85%;
    font-size: 0.9rem;
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-default div.callout-content {
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-simple .callout-icon::before {
    height: 1rem;
    width: 1rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 1rem 1rem;
  }

  .callout.callout-style-default .callout-icon::before {
    height: 0.9rem;
    width: 0.9rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 0.9rem 0.9rem;
  }

  .callout-caption {
    display: flex
  }
    
  .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  .callout.no-icon::before {
    display: none !important;
  }

  .callout.callout-captioned .callout-body > .callout-content > :last-child {
    margin-bottom: 0.5rem;
  }

  .callout.callout-captioned .callout-icon::before {
    margin-top: .5rem;
    padding-right: .5rem;
  }

  .callout:not(.callout-captioned) .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  /* Callout Types */

  div.callout-note {
    border-left-color: #4582ec !important;
  }

  div.callout-note .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEU0lEQVRYCcVXTWhcVRQ+586kSUMMxkyaElstCto2SIhitS5Ek8xUKV2poatCcVHtUlFQk8mbaaziwpWgglJwVaquitBOfhQXFlqlzSJpFSpIYyXNjBNiTCck7x2/8/LeNDOZxDuEkgOXe++553zfefee+/OYLOXFk3+1LLrRdiO81yNqZ6K9cG0P3MeFaMIQjXssE8Z1JzLO9ls20MBZX7oG8w9GxB0goaPrW5aNMp1yOZIa7Wv6o2ykpLtmAPs/vrG14Z+6d4jpbSKuhdcSyq9wGMPXjonwmESXrriLzFGOdDBLB8Y6MNYBu0dRokSygMA/mrun8MGFN3behm6VVAwg4WR3i6FvYK1T7MHo9BK7ydH+1uurECoouk5MPRyVSBrBHMYwVobG2aOXM07sWrn5qgB60rc6mcwIDJtQrnrEr44kmy+UO9r0u9O5/YbkS9juQckLed3DyW2XV/qWBBB3ptvI8EUY3I9p/67OW+g967TNr3Sotn3IuVlfMLVnsBwH4fsnebJvyGm5GeIUA3jljERmrv49SizPYuq+z7c2H/jlGC+Ghhupn/hcapqmcudB9jwJ/3jvnvu6vu5lVzF1fXyZuZZ7U8nRmVzytvT+H3kilYvH09mLWrQdwFSsFEsxFVs5fK7A0g8gMZjbif4ACpKbjv7gNGaD8bUrlk8x+KRflttr22JEMRUbTUwwDQScyzPgedQHZT0xnx7ujw2jfVfExwYHwOsDTjLdJ2ebmeQIlJ7neo41s/DrsL3kl+W2lWvAga0tR3zueGr6GL78M3ifH0rGXrBC2aAR8uYcIA5gwV8zIE8onoh8u0Fca/ciF7j1uOzEnqcIm59sEXoGc0+z6+H45V1CvAvHcD7THztu669cnp+L0okAeIc6zjbM/24LgGM1gZk7jnRu1aQWoU9sfUOuhrmtaPIO3YY1KLLWZaEO5TKUbMY5zx8W9UJ6elpLwKXbsaZ4EFl7B4bMtDv0iRipKoDQT2sNQI9b1utXFdYisi+wzZ/ri/1m7QfDgEuvgUUEIJPq3DhX/5DWNqIXDOweC2wvIR90Oq3lDpdMIgD2r0dXvGdsEW5H6x6HLRJYU7C69VefO1x8Gde1ZFSJLfWS1jbCnhtOPxmpfv2LXOA2Xk2tvnwKKPFuZ/oRmwBwqRQDcKNeVQkYcOjtWVBuM/JuYw5b6isojIkYxyYAFn5K7ZBF10fea52y8QltAg6jnMqNHFBmGkQ1j+U43HMi2xMar1Nv0zGsf1s8nUsmUtPOOrbFIR8bHFDMB5zL13Gmr/kGlCkUzedTzzmzsaJXhYawnA3UmARpiYj5ooJZiUoxFRtK3X6pgNPv+IZVPcnwbOl6f+aBaO1CNvPW9n9LmCp01nuSaTRF2YxHqZ8DYQT6WsXT+RD6eUztwYLZ8rM+rcPxamv1VQzFUkzFXvkiVrySGQgJNvXHJAxiU3/NwiC03rSf05VBaPtu/Z7/B8Yn/w7eguloAAAAAElFTkSuQmCC');
  }

  div.callout-note.callout-style-default .callout-caption {
    background-color: #dae6fb
  }

  div.callout-important {
    border-left-color: #d9534f !important;
  }

  div.callout-important .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEKklEQVRYCcVXTWhcVRS+575MJym48A+hSRFr00ySRQhURRfd2HYjk2SSTokuBCkU2o0LoSKKraKIBTcuFCoidGFD08nkBzdREbpQ1EDNIv8qSGMFUboImMSZd4/f9zJv8ibJMC8xJQfO3HPPPef7zrvvvnvviIkpC9nsw0UttFunbUhpFzFtarSd6WJkStVMw5xyVqYTvkwfzuf/5FgtkVoB0729j1rjXwThS7Vio+Mo6DNnvLfahoZ+i/o32lULuJ3NNiz7q6+pyAUkJaFF6JwaM2lUJlV0MlnQn5aTRbEu0SEqHUa0A4AdiGuB1kFXRfVyg5d87+Dg4DL6m2TLAub60ilj7A1Ec4odSAc8X95sHh7+ZRPCFo6Fnp7HfU/fBng/hi10CjCnWnJjsxvDNxWw0NfV6Rv5GgP3I3jGWXumdTD/3cbEOP2ZbOZp69yniG3FQ9z1jD7bnBu9Fc2tKGC2q+uAJOQHBDRiZX1x36o7fWBs7J9ownbtO+n0/qWkvW7UPIfc37WgT6ZGR++EOJyeQDSb9UB+DZ1G6DdLDzyS+b/kBCYGsYgJbSQHuThGKRcw5xdeQf8YdNHsc6ePXrlSYMBuSIAFTGAtQo+VuALo4BX83N190NWZWbynBjhOHsmNfFWLeL6v+ynsA58zDvvAC8j5PkbOcXCMg2PZFk3q8MjI7WAG/Dp9AwP7jdGBOOQkAvlFUB+irtm16I1Zw9YBcpGTGXYmk3kQIC/Cds55l+iMI3jqhjAuaoe+am2Jw5GT3Nbz3CkE12NavmzN5+erJW7046n/CH1RO/RVa8lBLozXk9uqykkGAyRXLWlLv5jyp4RFsG5vGVzpDLnIjTWgnRy2Rr+tDKvRc7Y8AyZq10jj8DqXdnIRNtFZb+t/ZRtXcDiVnzpqx8mPcDWxgARUqx0W1QB9MeUZiNrV4qP+Ehc+BpNgATsTX8ozYKL2NtFYAHc84fG7ndxUPr+AR/iQSns7uSUufAymwDOb2+NjK27lEFocm/EE2WpyIy/Hi66MWuMKJn8RvxIcj87IM5Vh9663ziW36kR0HNenXuxmfaD8JC7tfKbrhFr7LiZCrMjrzTeGx+PmkosrkNzW94ObzwocJ7A1HokLolY+AvkTiD/q1H0cN48c5EL8Crkttsa/AXQVDmutfyku0E7jShx49XqV3MFK8IryDhYVbj7Sj2P2eBxwcXoe8T8idsKKPRcnZw1b+slFTubwUwhktrfnAt7J++jwQtLZcm3sr9LQrjRzz6cfMv9aLvgmnAGvpoaGLxM4mAEaLV7iAzQ3oU0IvD5x9ix3yF2RAAuYAOO2f7PEFWCXZ4C9Pb2UsgDeVnFSpbFK7/IWu7TPTvBqzbGdCHOJQSxiEjt6IyZmxQyEJHv6xyQsYk//moVFsN2zP6fRImjfq7/n/wFDguUQFNEwugAAAABJRU5ErkJggg==');
  }

  div.callout-important.callout-style-default .callout-caption {
    background-color: #f7dddc
  }

  div.callout-warning {
    border-left-color: #f0ad4e !important;
  }

  div.callout-warning .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAETklEQVRYCeVWW2gcVRg+58yaTUnizqbipZeX4uWhBEniBaoUX1Ioze52t7sRq6APio9V9MEaoWlVsFasRq0gltaAPuxms8lu0gcviE/FFOstVbSIxgcv6SU7EZqmdc7v9+9mJtNks51NTUH84ed889/PP+cmxP+d5FIbMJmNbpREu4WUkiTtCicKny0l1pIKmBzovF2S+hIJHX8iEu3hZJ5lNZGqyRrGSIQpq15AzF28jgpeY6yk6GVdrfFqdrD6Iw+QlB8g0YS2g7dyQmXM/IDhBhT0UCiRf59lfqmmDvzRt6kByV/m4JjtzuaujMUM2c5Z2d6JdKrRb3K2q6mA+oYVz8JnDdKPmmNthzkAk/lN63sYPgevrguc72aZX/L9C6x09GYyxBgCX4NlvyGUHOKELlm5rXeR1kchuChJt4SSwyddZRXgvwMGvYo4QSlk3/zkHD8UHxwVJA6zjZZqP8v8kK8OWLnIZtLyCAJagYC4rTGW/9Pqj92N/c+LUaAj27movwbi19tk/whRCIE7Q9vyI6yvRpftAKVTdUjOW40X3h5OXsKCdmFcx0xlLJoSuQngnrJe7Kcjm4OMq9FlC7CMmScQANuNvjfP3PjGXDBaUQmbp296S5L4DrpbrHN1T87ZVEZVCzg1FF0Ft+dKrlLukI+/c9ENo+TvlTDbYFvuKPtQ9+l052rXrgKoWkDAFnvh0wTOmYn8R5f4k/jN/fZiCM1tQx9jQQ4ANhqG4hiL0qIFTGViG9DKB7GYzgubnpofgYRwO+DFjh0Zin2m4b/97EDkXkc+f6xYAPX0KK2I/7fUQuwzuwo/L3AkcjugPNixC8cHf0FyPjWlItmLxWw4Ou9YsQCr5fijMGoD/zpdRy95HRysyXA74MWOnscpO4j2y3HAVisw85hX5+AFBRSHt4ShfLFkIMXTqyKFc46xdzQM6XbAi702a7sy04J0+feReMFKp5q9esYLCqAZYw/k14E/xcLLsFElaornTuJB0svMuJINy8xkIYuL+xPAlWRceH6+HX7THJ0djLUom46zREu7tTkxwmf/FdOZ/sh6Q8qvEAiHpm4PJ4a/doJe0gH1t+aHRgCzOvBvJedEK5OFE5jpm4AGP2a8Dxe3gGJ/pAutug9Gp6he92CsSsWBaEcxGx0FHytmIpuqGkOpldqNYQK8cSoXvd+xLxXADw0kf6UkJNFtdo5MOgaLjiQOQHcn+A6h5NuL2s0qsC2LOM75PcF3yr5STuBSAcGG+meA14K/CI21HcS4LBT6tv0QAh8Dr5l93AhZzG5ZJ4VxAqdZUEl9z7WJ4aN+svMvwHHL21UKTd1mqvChH7/Za5xzXBBKrUcB0TQ+Ulgkfbi/H/YT5EptrGzsEK7tR1B7ln9BBwckYfMiuSqklSznIuoIIOM42MQO+QnduCoFCI0bpkzjCjddHPN/F+2Yu+sd9bKNpVwHhbS3LluK/0zgfwD0xYI5dXuzlQAAAABJRU5ErkJggg==');
  }

  div.callout-warning.callout-style-default .callout-caption {
    background-color: #fcefdc
  }

  div.callout-tip {
    border-left-color: #02b875 !important;
  }

  div.callout-tip .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAADr0lEQVRYCe1XTWgTQRj9ZjZV8a9SPIkKgj8I1bMHsUWrqYLVg4Ue6v9BwZOxSYsIerFao7UiUryIqJcqgtpimhbBXoSCVxUFe9CTiogUrUp2Pt+3aUI2u5vdNh4dmMzOzHvvezuz8xNFM0mjnbXaNu1MvFWRXkXEyE6aYOYJpdW4IXuA4r0fo8qqSMDBU0v1HJUgVieAXxzCsdE/YJTdFcVIZQNMyhruOMJKXYFoLfIfIvVIMWdsrd+Rpd86ZmyzzjJmLStqRn0v8lzkb4rVIXvnpScOJuAn2ACC65FkPzEdEy4TPWRLJ2h7z4cArXzzaOdKlbOvKKX25Wl00jSnrwVxAg3o4dRxhO13RBSdNvH0xSARv3adTXbBdTf64IWO2vH0LT+cv4GR1DJt+DUItaQogeBX/chhbTBxEiZ6gftlDNXTrvT7co4ub5A6gp9HIcHvzTa46OS5fBeP87Qm0fQkr4FsYgVQ7Qg+ZayaDg9jhg1GkWj8RG6lkeSacrrHgDaxdoBiZPg+NXV/KifMuB6//JmYH4CntVEHy/keA6x4h4CU5oFy8GzrBS18cLJMXcljAKB6INjWsRcuZBWVaS3GDrqB7rdapVIeA+isQ57Eev9eCqzqOa81CY05VLd6SamW2wA2H3SiTbnbSxmzfp7WtKZkqy4mdyAlGx7ennghYf8voqp9cLSgKdqNfa6RdRsAAkPwRuJZNbpByn+RrJi1RXTwdi8RQF6ymDwGMAtZ6TVE+4uoKh+MYkcLsT0Hk8eAienbiGdjJHZTpmNjlbFJNKDVAp2fJlYju6IreQxQ08UJDNYdoLSl6AadO+fFuCQqVMB1NJwPm69T04Wv5WhfcWyfXQB+wXRs1pt+nCknRa0LVzSA/2B+a9+zQJadb7IyyV24YAxKp2Jqs3emZTuNnKxsah+uabKbMk7CbTgJx/zIgQYErIeTKRQ9yD9wxVof5YolPHqaWo7TD6tJlh7jQnK5z2n3+fGdggIOx2kaa2YI9QWarc5Ce1ipNWMKeSG4DysFF52KBmTNMmn5HqCFkwy34rDg05gDwgH3bBi+sgFhN/e8QvRn8kbamCOhgrZ9GJhFDgfcMHzFb6BAtjKpFhzTjwv1KCVuxHvCbsSiEz4CANnj84cwHdFXAbAOJ4LTSAawGWFn5tDhLMYz6nWeU2wJfIhmIJBefcd/A5FWQWGgrWzyORZ3Q6HuV+Jf0Bj+BTX69fm1zWgK7By1YTXchFDORywnfQ7GpzOo6S+qECrsx2ifVQAAAABJRU5ErkJggg==');
  }

  div.callout-tip.callout-style-default .callout-caption {
    background-color: #ccf1e3
  }

  div.callout-caution {
    border-left-color: #fd7e14 !important;
  }

  div.callout-caution .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAACV0lEQVRYCdVWzWoUQRCuqp2ICBLJXgITZL1EfQDBW/bkzUMUD7klD+ATSHBEfAIfQO+iXsWDxJsHL96EHAwhgzlkg8nBg25XWb0zIb0zs9muYYWkoKeru+vn664fBqElyZNuyh167NXJ8Ut8McjbmEraKHkd7uAnAFku+VWdb3reSmRV8PKSLfZ0Gjn3a6Xlcq9YGb6tADjn+lUfTXtVmaZ1KwBIvFI11rRXlWlatwIAAv2asaa9mlB9wwygiDX26qaw1yYPzFXg2N1GgG0FMF8Oj+VIx7E/03lHx8UhvYyNZLN7BwSPgekXXLribw7w5/c8EF+DBK5idvDVYtEEwMeYefjjLAdEyQ3M9nfOkgnPTEkYU+sxMq0BxNR6jExrAI31H1rzvLEfRIdgcv1XEdj6QTQAS2wtstEALLG1yEZ3QhH6oDX7ExBSFEkFINXH98NTrme5IOaaA7kIfiu2L8A3qhH9zRbukdCqdsA98TdElyeMe5BI8Rs2xHRIsoTSSVFfCFCWGPn9XHb4cdobRIWABNf0add9jakDjQJpJ1bTXOJXnnRXHRf+dNL1ZV1MBRCXhMbaHqGI1JkKIL7+i8uffuP6wVQAzO7+qVEbF6NbS0LJureYcWXUUhH66nLR5rYmva+2tjRFtojkM2aD76HEGAD3tPtKM309FJg5j/K682ywcWJ3PASCcycH/22u+Bh7Aa0ehM2Fu4z0SAE81HF9RkB21c5bEn4Dzw+/qNOyXr3DCTQDMBOdhi4nAgiFDGCinIa2owCEChUwD8qzd03PG+qdW/4fDzjUMcE1ZpIAAAAASUVORK5CYII=');
  }

  div.callout-caution.callout-style-default .callout-caption {
    background-color: #ffe5d0
  }

  </style>
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="center">
  <h1 class="title">Introduction to machine learning for the humanities</h1>
  <p class="author">Marie-Hélène Burle</p>
  <p class="date">February 14, 2023</p>
  <img src="../img/sfudrac.png" alt="frontlogo">
</section>
<section>
<section id="definitions" class="title-slide slide level1 center">
<h1>Definitions</h1>

</section>
<section class="slide level2">

<h3 id="artificial-intelligence-ai">Artificial intelligence (AI)</h3>
<p>Any human-made system mimicking animal intelligence. This is a large and very diverse field.</p>
<h3 id="machine-learning-ml">Machine learning (ML)</h3>
<p>A subfield of AI that can be defined as computer programs whose performance at a task improves with experience.</p>
<h3 id="deep-learning-dl">Deep learning (DL)</h3>
<p>A subfield of ML using artificial neural networks with two or more hidden layers.</p>
<h3 id="natural-language-processing-nlp">Natural language processing (NLP)</h3>
<p>A subfield of AI focused on human languages.</p>
</section></section>
<section>
<section id="why-has-ml-become-so-popular" class="title-slide slide level1 center">
<h1>Why has ML become so popular?</h1>

</section>
<section id="ml-allows-to-achieve-previously-impossible-tasks" class="slide level2">
<h2>ML allows to achieve previously impossible tasks</h2>
<div class="example">
<p>Let’s take the example of image recognition:</p>
</div>
<p>In typical computing, a programmer writes code that gives a computer detailed instructions of what to do.</p>
<p>Coding all the possible ways—pixel by pixel—that an image can represent, say, a dog is an impossibly large task: there are many breeds of dogs, the image can be a picture, a blurred picture, a drawing, a cartoon, the dog can be in all sorts of positions, wearing clothes, etc.</p>
<p>There just aren’t enough resources to make the traditional programming approach able to create a computer program that can identify a dog in images.</p>
<p>By feeding a very large number of dog images to a neural network however, we can train that network to recognize dogs in images that it has never seen (without explicitly programming how it does this!).</p>
</section>
<section id="old-concept-new-computing-power" class="slide level2">
<h2>Old concept … new computing power</h2>
<p>The concept is everything but new: <a href="https://en.wikipedia.org/wiki/Arthur_Samuel_(computer_scientist)">Arthur Samuel</a> came up with it in 1949 and built a self-learning Checkers-playing program in 1959.</p>
<div class="columns">
<div class="column" style="width:60%;">
<p>Machine learning consists of feeding vast amounts of data to algorithms to strengthen <em>pathways</em>, so the excitement for the approach became somewhat dormant due to the lack of computing power and the lack of training data at the time.</p>
<p>The advent of powerful computers, GPUs, and massive amounts of data have brought the old concept to the forefront.</p>
</div><div class="column" style="width:7%;">

</div><div class="column" style="width:33%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="https://imgs.xkcd.com/comics/machine_learning.png" alt="noshadow"></p>
<p></p><figcaption>From <a href="https://xkcd.com/">xkcd.com</a></figcaption><p></p>
</figure>
</div>
</div>
</div>
</section></section>
<section>
<section id="so-how-does-it-all-work" class="title-slide slide level1 center">
<h1>So how does it all work?</h1>

</section>
<section id="decide-on-an-architecture" class="slide level2">
<h2>Decide on an architecture</h2>

<img data-src="img/diag_01.png" alt="noshadow" class="r-stretch"><p>The architecture won’t change during training.</p>
<p>The type of architecture you choose (e.g.&nbsp;CNN, Transformer) depends on the type of data you have (e.g.&nbsp;vision, textual). The depth and breadth of your network depend on the amount of data and computing resource you have.</p>
</section>
<section id="set-some-initial-parameters" class="slide level2">
<h2>Set some initial parameters</h2>

<img data-src="img/diag_02.png" alt="noshadow" class="r-stretch"><p>You can initialize them randomly or get much better ones through transfer learning.</p>
<p>While the parameters are also part of the model, those will change during training.</p>
</section>
<section id="get-some-labelled-data" class="slide level2">
<h2>Get some labelled data</h2>

<img data-src="img/diag_03.png" alt="noshadow" class="r-stretch"><p>When we say that we need a lot of data for machine learning, we mean “lots of labelled data” as this is what gets used for training models.</p>
</section>
<section id="make-sure-to-keep-some-data-for-testing" class="slide level2">
<h2>Make sure to keep some data for testing</h2>

<img data-src="img/diag_04.png" alt="noshadow" class="r-stretch"><p>Those data won’t be used for training the model. Often people keep around 20% of their data for testing.</p>
</section>
<section id="pass-data-and-parameters-through-the-architecture" class="slide level2">
<h2>Pass data and parameters through the architecture</h2>

<img data-src="img/diag_05.png" alt="noshadow" class="r-stretch"><p>The train data are the inputs and the process of calculating the outputs is the forward pass.</p>
</section>
<section id="the-outputs-of-the-model-are-predictions" class="slide level2">
<h2>The outputs of the model are predictions</h2>

<img data-src="img/diag_06.png" alt="noshadow" class="r-stretch"></section>
<section id="compare-those-predictions-to-the-train-labels" class="slide level2">
<h2>Compare those predictions to the train labels</h2>

<img data-src="img/diag_07.png" alt="noshadow" class="r-stretch"><p>Since our data was labelled, we know what the true outputs are.</p>
</section>
<section id="calculate-train-loss" class="slide level2">
<h2>Calculate train loss</h2>

<img data-src="img/diag_08.png" alt="noshadow" class="r-stretch"><p>The deviation of our predictions from the true outputs gives us a measure of training loss.</p>
</section>
<section id="adjust-parameters" class="slide level2">
<h2>Adjust parameters</h2>

<img data-src="img/diag_09.png" alt="noshadow" class="r-stretch"><p>The parameters get automatically adjusted to reduce the training loss through the mechanism of backpropagation. This is the actual training part.</p>
<p>This process is repeated many times. Training models is pretty much a giant for loop.</p>
</section>
<section id="from-model-to-program" class="slide level2">
<h2>From model to program</h2>

<img data-src="img/diag_10.png" alt="noshadow" class="r-stretch"><p>Remember that the model architecture is fixed, but that the parameters change at each iteration of the training process.</p>
</section>
<section class="slide level2">

<p>While the labelled data are key to training, what we are really interested in is the combination of architecture + final parameters.</p>

<img data-src="img/diag_11.png" alt="noshadow" class="r-stretch"></section>
<section class="slide level2">

<p>When the training is over, the parameters become fixed. Which means that our model now behaves like a classic program.</p>

<img data-src="img/diag_12.png" alt="noshadow" class="r-stretch"></section>
<section id="evaluate-the-model" class="slide level2">
<h2>Evaluate the model</h2>

<img data-src="img/diag_13.png" alt="noshadow" class="r-stretch"><p>We can now use the testing set (which was never used to train the model) to evaluate our model: if we pass the test inputs through our program, we get some predictions that we can compare to the test labels (which are the true outputs).</p>
<p>This gives us the test loss: a measure of how well our model performs.</p>
</section>
<section id="use-the-model" class="slide level2">
<h2>Use the model</h2>

<img data-src="img/diag_14.png" alt="noshadow" class="r-stretch"><p>Now that we have a program, we can use it on unlabelled inputs to get what people ultimately want: unknown outputs.</p>
<p>This is when we put our model to actual use to solve some problem.</p>
</section></section>
<section>
<section id="artificial-neural-networks" class="title-slide slide level1 center">
<h1>Artificial neural networks</h1>

</section>
<section class="slide level2">

<div class="columns">
<div class="column" style="width:45%;">
<p>In biological networks, the information consists of action potentials (neuron membrane rapid depolarizations) propagating through the network. In artificial ones, the information consists of tensors (multidimensional arrays) of weights and biases: each unit passes a weighted sum of an input tensor with an additional—possibly weighted—bias through an activation function before passing on the output tensor to the next layer of units.</p>
</div><div class="column" style="width:10%;">

</div><div class="column" style="width:45%;">
<p>Artificial neural networks are a series of layered units mimicking the concept of biological neurons: inputs are received by every unit of a layer, computed, then transmitted to units of the next layer. In the process of learning, experience strengthens some connections between units and weakens others.</p>
</div>
</div>
</section>
<section class="slide level2">

<div class="columns">
<div class="column" style="width:50%;">
<br>
<center>
<em>Schematic of a biological neuron:</em>
</center>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="https://upload.wikimedia.org/wikipedia/commons/b/b5/Neuron.svg" alt="noshadow"></p>
<p></p><figcaption>From <a href="https://commons.wikimedia.org/w/index.php?curid=1474927">Dhp1080, Wikipedia</a></figcaption><p></p>
</figure>
</div>
</div><div class="column" style="width:50%;">
<br>
<center>
<em>Schematic of an artificial neuron:</em>
</center>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/artificial_neuron.png" style="width:70.0%" alt="noshadow"></p>
<p></p><figcaption>Modified from <a href="https://royalsocietypublishing.org/doi/10.1098/rsta.2019.0163">O.C. Akgun &amp; J. Mei 2019</a></figcaption><p></p>
</figure>
</div>
</div>
</div>
</section>
<section class="slide level2">

<p>While biological neurons are connected in extremely intricate patterns, artificial ones follow a layered structure. Another difference in complexity is in the number of units: the human brain has 65–90 billion neurons. ANN have much fewer units.</p>
<div class="columns">
<div class="column" style="width:50%;">
<br>
<center>
<em>Neurons in mouse cortex:</em>
</center>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/brain_neurons.jpg" style="width:77.0%" alt="noshadow"></p>
<p></p><figcaption>Neurons are in green, the dark branches are blood vessels. Image by <a href="https://news.berkeley.edu/2020/03/19/high-speed-microscope-captures-fleeting-brain-signals/">Na Ji, UC Berkeley</a></figcaption><p></p>
</figure>
</div>
</div><div class="column" style="width:50%;">
<br>
<center>
<em>Neural network with 2 hidden layers:</em>
</center>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/nn_multi_layer.png" style="width:73.0%" alt="noshadow"></p>
<p></p><figcaption>From <a href="https://themaverickmeerkat.com/2020-01-10-TicTacToe/">The Maverick Meerkat</a></figcaption><p></p>
</figure>
</div>
</div>
</div>
</section>
<section class="slide level2">

<p>The information in biological neurons is an all-or-nothing electrochemical pulse or action potential. Greater stimuli don’t produce stronger signals but increase firing frequency. In contrast, artificial neurons pass the computation of their inputs through an activation function and the output can take any of the values possible with that function.</p>
<div class="columns">
<div class="column" style="width:50%;">
<center>
<em>Threshold potential in biological neurons:</em>
</center>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/all_none_law.png" style="width:70.0%" alt="noshadow"></p>
<p></p><figcaption>Modified from <a href="https://commons.wikimedia.org/w/index.php?curid=78013076">Blacktc, Wikimedia</a></figcaption><p></p>
</figure>
</div>
</div><div class="column" style="width:50%;">
<center>
<em>Some common activation functions in ANNs:</em>
</center>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/act_func.png" style="width:69.0%" alt="noshadow"></p>
<p></p><figcaption>From <a href="https://arxiv.org/abs/1908.08681">Diganta Misra 2019</a></figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>Which activation function to use depends on the type of problem and the available computing budget. Some early functions have fallen out of use while new ones have emerged (e.g.&nbsp;sigmoid got replaced by ReLU which is easier to train).</p>
</section>
<section class="slide level2">

<div class="columns">
<div class="column" style="width:45%;">
<p>The process of learning in biological NN happens through neuron death or growth and the creation or loss of synaptic connections between neurons.</p>
</div><div class="column" style="width:10%;">

</div><div class="column" style="width:45%;">
<p>In ANN, learning happens through optimization algorithms such as gradient descent which minimize cross entropy loss functions by adjusting the weights and biases connecting each layer of neurons over many iterations.</p>
</div>
</div>
</section></section>
<section>
<section id="types-of-ann" class="title-slide slide level1 center">
<h1>Types of ANN</h1>

</section>
<section id="fully-connected-neural-networks" class="slide level2">
<h2>Fully connected neural networks</h2>
<div class="columns">
<div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/nn.png" style="width:60.0%" alt="noshadow"></p>
<p></p><figcaption>From <a href="https://commons.wikimedia.org/w/index.php?curid=24913461">Glosser.ca, Wikipedia</a></figcaption><p></p>
</figure>
</div>
</div><div class="column" style="width:50%;">
<p>Each neuron receives inputs from every neuron of the previous layer and passes its output to every neuron of the next layer.</p>
</div>
</div>
</section>
<section id="convolutional-neural-networks" class="slide level2">
<h2>Convolutional neural networks</h2>

<img data-src="img/cnn.png" alt="noshadow" class="r-stretch"><div class="caption">
<p>From <a href="https://codetolight.wordpress.com/2017/11/29/getting-started-with-pytorch-for-deep-learning-part-3-neural-network-basics/">Programming Journeys by Rensu Theart</a></p>
</div>
<p>Convolutional neural networks (CNN) are used for spatially structured data (e.g.&nbsp;images).</p>
<p>Images have huge input sizes and would require a very large number of neurons in a fully connected neural net. In convolutional layers, neurons receive input from a subarea (called <em>local receptive field</em>) of the previous layer. This greatly reduces the number of parameters. Optionally, pooling (combining the outputs of neurons in a subarea) reduces the data dimensions.</p>
</section>
<section id="recurrent-neural-networks" class="slide level2">
<h2>Recurrent neural networks</h2>

<img data-src="img/rnn.png" alt="noshadow" class="r-stretch"><div class="caption">
<p>From <a href="https://commons.wikimedia.org/wiki/File:Recurrent_neural_network_unfold.svg">fdeloche, Wikipedia</a></p>
</div>
<p>Recurrent neural networks (RNN) such as Long Short-Term Memory (LSTM) are used for chain structured data (e.g.&nbsp;text).</p>
<p>They are not feedforward networks (i.e.&nbsp;networks for which the information moves only in the forward direction without any loop).</p>
</section>
<section id="transformers" class="slide level2">
<h2>Transformers</h2>
<p>A combination of two RNNs (the <em>encoder</em> and the <em>decoder</em>) is used in sequence to sequence models for translation or picture captioning.</p>
<p>In 2014 the concept of <em>attention</em> (giving added weight to important words) was developed, greatly improving the ability of such models to process a lot of data.</p>
<p>The problem with recurrence is that it is not easily to parallelize (and thus to run fast on GPUs).</p>
<p>In 2017, a new model—the <a href="https://arxiv.org/abs/1706.03762">transformer</a>—was proposed: by using only attention mechanisms and no recurrence, the transformer achieves better results in an easily parallelizable fashion.</p>
<p>With the addition of transfer learning, powerful transformers emerged in the field of <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP</a> (e.g.&nbsp;<a href="https://arxiv.org/abs/1810.04805">Bidirectional Encoder Representations from Transformers (BERT)</a> from Google and <a href="https://arxiv.org/pdf/2005.14165">Generative Pre-trained Transformer-3 (GPT-3)</a> from <a href="https://openai.com/">OpenAI</a>).</p>
<!-- # Types of learning -->
<!-- ## Supervised learning -->
<!-- - *Regression* is a form of supervised learning with continuous outputs -->
<!-- - *Classification* is supervised learning with discrete outputs -->
<!-- Supervised learning uses training data in the form of example input/output pairs. -->
<!-- ### Goal -->
<!-- Find the relationship between inputs and outputs. -->
<!-- ## Unsupervised learning -->
<!-- Clustering, social network analysis, market segmentation, PCA ... are all forms of unsupervised learning. -->
<!-- Unsupervised learning uses unlabelled data. -->
<!-- ### Goal -->
<!-- Find structure within the data. -->
<!-- ## Reinforcement learning -->
<!-- ## Transfer learning -->
</section></section>
<section>
<section id="limitations" class="title-slide slide level1 center">
<h1>Limitations</h1>

</section>
<section id="data-bias" class="slide level2">
<h2>Data bias</h2>
<p><strong>Bias is always present in data.</strong></p>
<p><em>Document the limitations and scope of your data as best as possible.</em></p>
<p>Problems to watch for:</p>
<ul>
<li><em>Out of domain data</em>: data used for training are not relevant to the model application.</li>
<li><em>Domain shift</em>: model becoming inadapted as conditions evolve.</li>
<li><em>Feedback loop</em>: initial bias exacerbated over the time.</li>
</ul>
</section>
<section class="slide level2">

<div class="columns">
<div class="column" style="width:47%;">
<p>The last one is particularly problematic whenever the model outputs the next round of data based on interactions of the current round of data with the real world.</p>
<p><span class="emph">Solution: ensure there are human circuit breakers and oversight.</span></p>
</div><div class="column" style="width:6%;">

</div><div class="column" style="width:47%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="https://imgs.xkcd.com/comics/ai_hiring_algorithm.png" alt="noshadow"></p>
<p></p><figcaption>From <a href="https://xkcd.com/">xkcd.com</a></figcaption><p></p>
</figure>
</div>
</div>
</div>
</section>
<section id="transformation-of-subjects" class="slide level2">
<h2>Transformation of subjects</h2>
<p>Algorithms are supposed to help us, not transform us (e.g.&nbsp;YouTube recommendation algorithms).</p>
</section>
<section id="bugs" class="slide level2">
<h2>Bugs</h2>
<p><a href="https://www.theverge.com/2018/3/21/17144260/healthcare-medicaid-algorithm-arkansas-cerebral-palsy.">Example of bug with real life consequences</a></p>
</section></section>
<section>
<section id="tools" class="title-slide slide level1 center">
<h1>Tools</h1>

</section>
<section id="many-options" class="slide level2">
<h2>Many options</h2>
<p>Here are just a few:</p>
<ul>
<li><a href="https://www.nltk.org/">Natural Language Toolkit (NLTK)</a>: a suite of Python libraries geared towards teaching and research.</li>
<li><a href="https://spacy.io/">spaCy</a>: Python library geared towards production.</li>
<li><a href="https://pytorch.org/text/stable/index.html">torchtext</a>, part of the <a href="https://pytorch.org/">PyTorch</a> project (and many options of added layers on top such as <a href="https://pytorchnlp.readthedocs.io/en/latest/">PyTorch-NLP</a>): Python library.</li>
<li><a href="https://radimrehurek.com/gensim/">GenSim</a>: Python library.</li>
<li><a href="https://stanfordnlp.github.io/CoreNLP/">Stanford CoreNLP</a>: Java library.</li>
<li><a href="https://juliapackages.com/c/machine-learning">Many libraries in the Julia programming language</a>.</li>
</ul>
</section>
<section id="which-one-to-choose" class="slide level2">
<h2>Which one to choose?</h2>
<p><a href="https://mint.westdri.ca/ml/choosing_frameworks.html" target="_blank">Choose an open source tool</a> (i.e.&nbsp;stay away from proprietary software such as MATLAB).</p>
<ul>
<li>Researchers who do not have access to the tool cannot reproduce your methods (open tools = open equitable research).</li>
<li>Once you graduate, you may not have access to the tool anymore</li>
<li>Your university may stop paying for a license</li>
<li>You may get locked-in</li>
<li>Proprietary tools are often black boxes</li>
<li>Long-term access is not guaranty (problem to replicate studies)</li>
<li>The licenses you have access to may be limiting and a cause of headache</li>
<li>Proprietary tools fall behind popular open-source tools</li>
<li>Proprietary tools often fail to address specialized edge cases needed in research</li>
</ul>
</section></section>
<section>
<section id="blue1brown-by-grant-sanderson" class="title-slide slide level1 center">
<h1><a href="https://www.3blue1brown.com/">3Blue1Brown by Grant Sanderson</a></h1>
<p><br> <a href="https://www.3blue1brown.com/">3Blue1Brown by Grant Sanderson</a> has a series of 4 videos on neural networks which is easy to watch, fun, and does an excellent job at introducing the functioning of a simple neural network.</p>
<div class="note">
<p>As you develop your own ML models, if you find that your mathematical background is shaky, 3blue1brown also has <a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab">an excellent series of videos on linear algebra</a> and <a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr">an equally great one on calculus</a>.</p>
</div>
</section>
<section class="slide level2">

<div class="columns">
<div class="column" style="width:50%;">
<p>What are NN? (19 min)</p>
<iframe src="https://www.youtube.com/embed/aircAruvnKk" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
</div><div class="column" style="width:50%;">
<p>How do NN learn? (21 min)</p>
<iframe src="https://www.youtube.com/embed/IHZwWFHWa-w" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
</div>
</div>
<div class="columns">
<div class="column" style="width:50%;">
<p>What is backpropagation? (14 min)</p>
<iframe src="https://www.youtube.com/embed/Ilg3gGewQ5U" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
</div><div class="column" style="width:50%;">
<p>How does backpropagation work? (10 min)</p>
<iframe src="https://www.youtube.com/embed/tIeHLnjs5U8" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
</div>
</div>
<!-- ## Gradient descent -->
<!-- There are several gradient descent methods: -->
<!-- *Batch gradient descent* uses all examples in each iteration and is thus slow for large datasets (the parameters are adjusted only after all the samples have been processed). -->
<!-- *Mini-batch gradient descent* is an intermediate approach: it uses mini-batch sized examples in each iteration. This allows a vectorized approach (and hence parallelization). The [Adam optimization algorithm](https://arxiv.org/abs/1412.6980) is a popular variation of mini-batch gradient descent. -->
<!-- *Stochastic gradient descent* uses one example in each iteration. It is thus much faster than batch gradient descent (the parameters are adjusted after each example). But it does not allow any vectorization. -->
<!-- ![From [Imad Dabbura](https://imaddabbura.github.io/posts/optimization-algorithms/gradient-descent.html)](img/types_gradient_descent.png){fig-alt="noshadow" width=60%} -->


<img src="../img/sfudrac_logo.png" class="slide-logo r-stretch"><div class="footer footer-default">
<p><a href="upscaling.html"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="rgb(153, 153, 153)" class="bi bi-arrow-90deg-up" viewbox="0 0 16 16"><path fill-rule="evenodd" d="M4.854 1.146a.5.5 0 0 0-.708 0l-4 4a.5.5 0 1 0 .708.708L4 2.707V12.5A2.5 2.5 0 0 0 6.5 15h8a.5.5 0 0 0 0-1h-8A1.5 1.5 0 0 1 5 12.5V2.707l3.146 3.147a.5.5 0 1 0 .708-.708l-4-4z"></path></svg>&nbsp;Back to workshop page</a></p>
</div>
</section></section>
    </div>
  </div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="../site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="../site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-pointer/pointer.js"></script>
  <script src="../site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="../site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="../site_libs/revealjs/plugin/search/search.js"></script>
  <script src="../site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="../site_libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'smaller': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'pointer': {"key":"q","color":"#b5111b","pointerSize":32,"alwaysVisible":false},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: false,

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealPointer, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const clipboard = new window.ClipboardJS('.code-copy-button', {
        target: function(trigger) {
          return trigger.previousElementSibling;
        }
      });
      clipboard.on('success', function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        let tooltip;
        if (window.bootstrap) {
          button.setAttribute("data-bs-toggle", "tooltip");
          button.setAttribute("data-bs-placement", "left");
          button.setAttribute("data-bs-title", "Copied!");
          tooltip = new bootstrap.Tooltip(button, 
            { trigger: "manual", 
              customClass: "code-copy-button-tooltip",
              offset: [0, -8]});
          tooltip.show();    
        }
        setTimeout(function() {
          if (tooltip) {
            tooltip.hide();
            button.removeAttribute("data-bs-title");
            button.removeAttribute("data-bs-toggle");
            button.removeAttribute("data-bs-placement");
          }
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      });
      function tippyHover(el, contentFn) {
        const config = {
          allowHTML: true,
          content: contentFn,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'quarto-reveal',
          placement: 'bottom-start'
        };
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          return note.innerHTML;
        });
      }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
        var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
          var filterRegex = new RegExp(/https:\/\/mint\.westdri\.ca\//);
        var isInternal = (href) => {
            return filterRegex.test(href) || localhostRegex.test(href);
        }
        // Inspect non-navigation links and adorn them if external
        var links = window.document.querySelectorAll('a:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item)');
        for (var i=0; i<links.length; i++) {
          const link = links[i];
          if (!isInternal(link.href)) {
              // target, if specified
              link.setAttribute("target", "_blank");
          }
        }
    });
    </script>
    

</body></html>